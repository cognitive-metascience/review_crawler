<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01489</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004141</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The Equivalence of Information-Theoretic and Likelihood-Based Methods for Neural Dimensionality Reduction</article-title>
<alt-title alt-title-type="running-head">Equating Neural Dimensionality Reduction Methods</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Williamson</surname> <given-names>Ross S.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="fn" rid="currentaff001"><sup>¤</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Sahani</surname> <given-names>Maneesh</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Pillow</surname> <given-names>Jonathan W.</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Gatsby Computational Neuroscience Unit, University College London, London, UK</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Centre for Mathematics and Physics in the Life Sciences and Experimental Biology, University College London, London, UK</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Princeton Neuroscience Institute, Department of Psychology, Princeton University, Princeton, New Jersey, USA</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname> <given-names>Matthias</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: RSW MS JWP. Performed the experiments: RSW MS JWP. Analyzed the data: RSW JWP. Contributed reagents/materials/analysis tools: RSW JWP. Wrote the paper: RSW MS JWP.</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label> <p>Current Affiliation: Eaton-Peabody Laboratories, Massachusetts Eye and Ear Infirmary, Boston, Massachusetts, USA &amp; Center for Computational Neuroscience and Neural Technology, Boston University, Boston, Massachusetts, USA</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">ross_williamson@meei.harvard.edu</email> (RSW); <email xlink:type="simple">pillow@princeton.edu</email> (JWP)</corresp>
</author-notes>
<pub-date pub-type="collection">
<month>4</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>1</day>
<month>4</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>4</issue>
<elocation-id>e1004141</elocation-id>
<history>
<date date-type="received">
<day>21</day>
<month>8</month>
<year>2013</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>1</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Williamson et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004141" xlink:type="simple"/>
<abstract>
<p>Stimulus dimensionality-reduction methods in neuroscience seek to identify a low-dimensional space of stimulus features that affect a neuron’s probability of spiking. One popular method, known as maximally informative dimensions (MID), uses an information-theoretic quantity known as “single-spike information” to identify this space. Here we examine MID from a model-based perspective. We show that MID is a maximum-likelihood estimator for the parameters of a linear-nonlinear-Poisson (LNP) model, and that the empirical single-spike information corresponds to the normalized log-likelihood under a Poisson model. This equivalence implies that MID does not necessarily find maximally informative stimulus dimensions when spiking is not well described as Poisson. We provide several examples to illustrate this shortcoming, and derive a lower bound on the information lost when spiking is Bernoulli in discrete time bins. To overcome this limitation, we introduce model-based dimensionality reduction methods for neurons with non-Poisson firing statistics, and show that they can be framed equivalently in likelihood-based or information-theoretic terms. Finally, we show how to overcome practical limitations on the number of stimulus dimensions that MID can estimate by constraining the form of the non-parametric nonlinearity in an LNP model. We illustrate these methods with simulations and data from primate visual cortex.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>A popular approach to the neural coding problem is to identify a low-dimensional linear projection of the stimulus space that preserves the aspects of the stimulus that affect a neuron’s probability of spiking. Previous work has focused on both information-theoretic and likelihood-based estimators for finding such projections. Here, we show that these two approaches are in fact equivalent. We show that maximally informative dimensions (MID), a popular information-theoretic method for dimensionality reduction, is identical to the maximum-likelihood estimator for a particular linear-nonlinear encoding model with Poisson spiking. One implication of this equivalence is that MID may not find the information-theoretically optimal stimulus projection when spiking is non-Poisson, which we illustrate with a few simple examples. Using these insights, we propose novel dimensionality-reduction methods that incorporate non-Poisson spiking, and suggest new parametrizations that allow for tractable estimation of high-dimensional subspaces.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by the Engineering and Physical Sciences Research Council (support for RSW through the CoMPLEX Doctoral Training Program), the Gatsby Charitable Foundation (Gatsby Computational Neuroscience Unit support to RSW and MS) and Sloan Fellowship, McKnight Scholar’s award, and NSF CAREER award IIS-1150186 (support to JWP). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="0"/>
<page-count count="31"/>
</counts>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The neural coding problem, an important topic in systems and computational neuroscience, concerns the probabilistic relationship between environmental stimuli and neural spike responses. Characterizing this relationship is difficult in general because of the high dimensionality of natural signals. A substantial literature therefore has focused on dimensionality reduction methods for identifying which stimuli affect a neuron’s probability of firing. The basic idea is that many neurons compute their responses in a low dimensional subspace, spanned by a small number of stimulus features. By identifying this subspace, we can more easily characterize the nonlinear mapping from stimulus features to spike responses [<xref ref-type="bibr" rid="pcbi.1004141.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004141.ref005">5</xref>].</p>
<p>Neural dimensionality-reduction methods can be coarsely divided into three classes: (1) moment-based estimators, such as spike-triggered average (STA) and covariance (STC) [<xref ref-type="bibr" rid="pcbi.1004141.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1004141.ref008">8</xref>]; (2) model-based estimators, which rely on explicit forward encoding models [<xref ref-type="bibr" rid="pcbi.1004141.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1004141.ref016">16</xref>]; and (3) information and divergence-based estimators, which seek to reduce dimensionality using an information-theoretic cost function [<xref ref-type="bibr" rid="pcbi.1004141.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1004141.ref022">22</xref>]. For all such methods, the goal is to find a set of linear filters, specified by the columns of a matrix <italic>K</italic>, such that the probability of response <italic>r</italic> given a stimulus <bold>s</bold> depends only on the linear projection of <bold>s</bold> onto these filters, i.e., <italic>p</italic>(<italic>r</italic>|<bold>s</bold>) ≈ <italic>p</italic>(<italic>r</italic>|<italic>K</italic><sup>⊤</sup><bold>s</bold>). Existing methods differ in computational complexity, modeling assumptions, and stimulus requirements. Typically, moment-based estimators have low computational cost but succeed only for restricted classes of stimulus distributions, whereas information-theoretic and likelihood-based estimators allow for arbitrary stimuli but have high computational cost. Previous work has established theoretical connections between moment-based and likelihood-based estimators [<xref ref-type="bibr" rid="pcbi.1004141.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref023">23</xref>], and between some classes of likelihood-based and information-theoretic estimators [<xref ref-type="bibr" rid="pcbi.1004141.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref024">24</xref>].</p>
<p>Here we focus on maximally informative dimensions (MID), a well-known information-theoretic estimator introduced by Sharpee, Rust &amp; Bialek [<xref ref-type="bibr" rid="pcbi.1004141.ref018">18</xref>]. We show that this estimator is formally identical to the maximum likelihood (ML) estimator for the parameters of a linear-nonlinear-Poisson (LNP) encoding model. Although previous work has demonstrated an asymptotic equivalence between these methods [<xref ref-type="bibr" rid="pcbi.1004141.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref025">25</xref>], we show that the correspondence is exact, regardless of time bin size or the amount of data. This equivalence follows from the fact that the plugin estimate for the single-spike information [<xref ref-type="bibr" rid="pcbi.1004141.ref026">26</xref>], the quantity that MID optimizes, is equal to a normalized Poisson log-likelihood.</p>
<p>The connection between the MID estimator and the LNP model makes clear that MID does not incorporate information carried by non-Poisson statistics of the response. We illustrate this shortcoming by showing that MID can fail to find information-maximizing filters for simulated neurons with binary or other non-Poisson spike count distributions. To overcome this limitation, we introduce new dimensionality-reduction estimators based on non-Poisson noise models, and show that they can be framed equivalently in information-theoretic or likelihood-based terms.</p>
<p>Finally, we show that a model-based perspective leads to strategies for overcoming a limitation of traditional MID, that it cannot tractably estimate more than two or three filters. The difficulty arises from the intractability of using histograms to estimate densities in high-dimensional subspaces. However, the single-spike information depends only on the ratio of densities, which is proportional to the nonlinearity in the LNP model. We show that by restricting the parametrization of this nonlinearity so that the number of parameters does not grow exponentially with the number of dimensions, we can obtain flexible yet computationally tractable estimators for models with many filters or dimensions.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Background</title>
<sec id="sec004">
<title>Linear-nonlinear-Poisson (LNP) encoding model</title>
<p>Linear-nonlinear cascade models provide a useful framework for describing neural responses to high-dimensional stimuli. These models define the response in terms of a cascade of linear, nonlinear, and probabilistic spiking stages (see <xref ref-type="fig" rid="pcbi.1004141.g001">Fig. 1</xref>). The linear stage reduces the dimensionality by projecting the high-dimensional stimulus onto a set of linear filters, and a nonlinear function then converts the output of these filters to a non-negative spike rate.</p>
<fig id="pcbi.1004141.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004141.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The linear-nonlinear-Poisson (LNP) encoding model formalizes the neural encoding process in terms of a cascade of three stages.</title>
<p>First, the high-dimensional stimulus <bold>s</bold> projects onto bank of filters contained in the columns of a matrix <italic>K</italic>, resulting in a point in a low-dimensional neural feature space <italic>K</italic><sup>⊤</sup><bold>s</bold>. Second, an instantaneous nonlinear function <italic>f</italic> maps the filtered stimulus to an instantaneous spike rate <italic>λ</italic>. Third, spikes <italic>r</italic> are generated according to an inhomogeneous Poisson process.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004141.g001"/>
</fig>
<p>Let <italic>θ</italic> = {<italic>K</italic>,<italic>α</italic>} denote the parameters of the LNP model, where <italic>K</italic> is a (tall, skinny) matrix whose columns contain the stimulus filters (for cases with a single filter, we will denote the filter with a vector <bold>k</bold> instead of the matrix <italic>K</italic>), and <italic>α</italic> are parameters governing the nonlinear function <italic>f</italic> from feature space to instantaneous spike rate. Under this model, the probability of a spike response <italic>r</italic> given stimulus <bold>s</bold> is governed by a Poisson distribution:
<disp-formula id="pcbi.1004141.e001"><alternatives><graphic id="pcbi.1004141.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>λ</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>|</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>r</mml:mi> <mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mstyle> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>r</mml:mi></mml:msup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>λ</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>λ</italic> denotes the stimulus-driven spike rate (or “conditional intensity”) and Δ denotes a time bin size. The defining feature of a Poisson process is that responses in non-overlapping time bins are conditionally independent given the spike rate. In a discrete-time LNP model, the conditional probability of a dataset <italic>D</italic> = {(<bold>s</bold><sub><italic>t</italic></sub>,<italic>r</italic><sub><italic>t</italic></sub>)}, consisting of stimulus-response pairs indexed by <italic>t</italic> ∈ {1,…,<italic>N</italic>}, is the product of independent terms. The log-likelihood is therefore a sum over time bins:
<disp-formula id="pcbi.1004141.e002"><alternatives><graphic id="pcbi.1004141.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e002"/><mml:math id="M2" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:msub><mml:mi mathvariant="bold">s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:msub><mml:mi mathvariant="bold">s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="0.277778em"/><mml:mo>-</mml:mo> <mml:mspace width="0.277778em"/><mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>!</mml:mo></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where −(∑log<italic>r</italic><sub><italic>t</italic></sub>!) is a constant that does not depend on <italic>θ</italic>. The ML estimate for <italic>θ</italic> is simply the maximizer of the log-likelihood: <inline-formula id="pcbi.1004141.e003"><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo>̂</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mspace width="1pt"/><mml:mtext>max</mml:mtext></mml:mrow><mml:mi>θ</mml:mi></mml:munder><mml:mspace width="1pt"/><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
</sec>
<sec id="sec005">
<title>Maximally informative dimensions (MID)</title>
<p>The maximally informative dimensions (MID) estimator seeks to find an informative low-dimensional projection of the stimulus by maximizing an information-theoretic quantity known as the single-spike information [<xref ref-type="bibr" rid="pcbi.1004141.ref026">26</xref>]. This quantity, which we denote <italic>I</italic><sub><italic>ss</italic></sub>, is the the average information that the time of a single spike (considered independently of other spikes) carries about the stimulus</p>
<p>Although first introduced as a quantity that can be computed from the peri-stimulus time histogram (PSTH) measured in response to a repeated stimulus, the single-spike information can also be expressed as the Kullback-Leibler (KL) divergence between two distributions over the stimulus (see [<xref ref-type="bibr" rid="pcbi.1004141.ref026">26</xref>], appendix B):
<disp-formula id="pcbi.1004141.e004"><alternatives><graphic id="pcbi.1004141.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e004"/><mml:math id="M4" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mi>p</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi> <mml:mi>e</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mi>p</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi> <mml:mi>e</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mi>p</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi> <mml:mi>e</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>p</italic>(<bold>s</bold>) denotes the marginal or “raw” distribution over stimuli, and <italic>p</italic>(<bold>s</bold>|<italic>spike</italic>) is the distribution over stimuli conditioned on observing a spike, also known as the “spike-triggered” stimulus distribution. Note that <italic>p</italic>(<bold>s</bold>|<italic>spike</italic>) is <italic>not</italic> the same as <italic>p</italic>(<bold>s</bold>|<italic>r</italic> = 1), the distribution of stimuli conditioned on a spike count of <italic>r</italic> = 1, since a stimulus that elicits two spikes will contribute twice as much to the spike-triggered distribution as a stimulus that elicits only one spike.</p>
<p>The MID estimator [<xref ref-type="bibr" rid="pcbi.1004141.ref018">18</xref>] seeks to find the linear projection that preserves maximal single-spike information:
<disp-formula id="pcbi.1004141.e005"><alternatives><graphic id="pcbi.1004141.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e005"/><mml:math id="M5" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>K</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mi>p</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi> <mml:mi>e</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>p</italic>(<italic>K</italic><sup>⊤</sup><bold>s</bold>) and <italic>p</italic>(<italic>K</italic><sup>⊤</sup><bold>s</bold>|<italic>spike</italic>) are the raw and spike-triggered stimulus distributions projected onto the subspace defined by the columns of <italic>K</italic>, respectively. In practice, the MID estimator maximizes an estimate of the projected single-spike information:
<disp-formula id="pcbi.1004141.e006"><alternatives><graphic id="pcbi.1004141.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e006"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>arg</mml:mtext><mml:munder><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mi>K</mml:mi></mml:munder><mml:mspace width="1pt"/><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
where <italic>Î</italic><sub><italic>ss</italic></sub>(<italic>K</italic>) denotes an empirical estimate of <italic>I</italic><sub><italic>ss</italic></sub>(<italic>K</italic>). The columns of <inline-formula id="pcbi.1004141.e007"><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> can be conceived as “directions” or “axes” in stimulus space that are most informative about a neuron’s probability of spiking, as quantified by single-spike information. <xref ref-type="fig" rid="pcbi.1004141.g002">Fig. 2</xref> shows a simulated example illustrating the MID estimate for a single linear filter in a two-dimensional stimulus space.</p>
<fig id="pcbi.1004141.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004141.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Geometric illustration of maximally-informative-dimensions (MID).</title>
<p><bold>Left:</bold> A two-dimensional stimulus space, with points indicating the location of raw stimuli (black) and spike-eliciting stimuli (red). For this simulated example, the probability of spiking depended only on the projection onto a filter <bold>k</bold><sub><italic>true</italic></sub>, oriented at 45<sup>∘</sup>. Histograms (inset) show the one-dimensional distributions of raw (black) and spike-triggered stimuli (red) projected onto <bold>k</bold><sub><italic>true</italic></sub> (lower right) and its orthogonal complement (lower left). <bold>Right:</bold> Estimated single-spike information captured by a 1D subspace, as a function of the axis of projection. The MID estimate <inline-formula id="pcbi.1004141.e008"><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>I</mml:mi> <mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (dotted) corresponds to the axis maximizing single-spike information, which converges asymptotically to <bold>k</bold><sub><italic>true</italic></sub> with dataset size.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004141.g002"/>
</fig>
</sec>
</sec>
<sec id="sec006">
<title>Equivalence of MID and maximum-likelihood LNP</title>
<p>Previous work has shown that MID converges asymptotically to the maximum-likelihood (ML) estimator for an LNP model in the limit of small time bins [<xref ref-type="bibr" rid="pcbi.1004141.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref024">24</xref>]. Here we present a stronger result, showing that the equivalence is not merely asymptotic. We show that standard MID, using histogram-based estimators for raw and spike-triggered stimulus densities <italic>p</italic>(<bold>s</bold>) and <italic>p</italic>(<bold>s</bold>|<italic>spike</italic>), is exactly the ML estimator for the parameters of an LNP model, regardless of spike rate, the time bins used to count spikes, or the amount of data.</p>
<p>The standard implementation of MID [<xref ref-type="bibr" rid="pcbi.1004141.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref020">20</xref>] uses histograms to estimate the projected stimulus densities <italic>p</italic>(<italic>K</italic><sup>⊤</sup><bold>s</bold>) and <italic>p</italic>(<italic>K</italic><sup>⊤</sup><bold>s</bold>|<italic>spike</italic>). These density estimates are then used to compute <italic>Î</italic><sub><italic>ss</italic></sub>(<italic>K</italic>), the plug-in estimate of single-spike information in a subspace defined by <italic>K</italic> (<xref ref-type="disp-formula" rid="pcbi.1004141.e005">Equation 4</xref>). We will now unpack the details of this estimate in order to show its relationship to the LNP model log-likelihood.</p>
<p>Let {<italic>B</italic><sub>1</sub>, …, <italic>B</italic><sub><italic>m</italic></sub>} denote a group of sets (“histogram bins”) that partition the range of the projected stimuli <italic>K</italic><sup>⊤</sup><bold>s</bold>. In the one-dimensional case, we typically choose these sets to be intervals <italic>B</italic><sub><italic>i</italic></sub> = [<italic>b</italic><sub><italic>i</italic>−1</sub>,<italic>b</italic><sub><italic>i</italic></sub>), defined by bin edges {<italic>b</italic><sub>0</sub>, …, <italic>b</italic><sub><italic>m</italic></sub>}, where <italic>b</italic><sub>0</sub> = −∞ and <italic>b</italic><sub><italic>m</italic></sub> = +∞. Then let <inline-formula id="pcbi.1004141.e009"><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">p</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="pcbi.1004141.e010"><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">q</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denote histogram-based estimates of <italic>p</italic>(<italic>K</italic><sup>⊤</sup><bold>s</bold>) and <italic>p</italic>(<italic>K</italic><sup>⊤</sup><bold>s</bold>|<italic>spike</italic>), respectively, given by:
<disp-formula id="pcbi.1004141.e011"><alternatives><graphic id="pcbi.1004141.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e011"/><mml:math id="M11" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="1.em"/></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mspace width="1.em"/><mml:mfrac><mml:mrow><mml:mo>#</mml:mo> <mml:mtext>stimuli in</mml:mtext><mml:mspace width="1pt"/><mml:msub><mml:mi mathvariant="normal">B</mml:mi> <mml:mi mathvariant="normal">i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mo>#</mml:mo> <mml:mtext>stimuli</mml:mtext></mml:mrow></mml:mfrac> <mml:mspace width="1.em"/><mml:mo>=</mml:mo> <mml:mspace width="1.em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="1.em"/></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mspace width="1.em"/><mml:mfrac><mml:mrow><mml:mo>#</mml:mo> <mml:mtext>stimuli in</mml:mtext><mml:mspace width="1pt"/> <mml:msub><mml:mi mathvariant="normal">B</mml:mi> <mml:mi mathvariant="normal">i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>spike</mml:mi></mml:mrow></mml:mrow> <mml:mrow><mml:mo>#</mml:mo> <mml:mi>spikes</mml:mi></mml:mrow></mml:mfrac> <mml:mspace width="1.em"/><mml:mo>=</mml:mo> <mml:mspace width="1.em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where <bold>x</bold><sub><italic>t</italic></sub> = <italic>K</italic><sup>⊤</sup><bold>s</bold><sub><italic>t</italic></sub> denotes the linear projection of the stimulus <bold>s</bold><sub><italic>t</italic></sub>, <inline-formula id="pcbi.1004141.e012"><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the total number of spikes, and <bold>1</bold><sub><italic>B</italic><sub><italic>i</italic></sub></sub>(⋅) is the indicator function for the set <italic>B</italic><sub><italic>i</italic></sub>, defined as:
<disp-formula id="pcbi.1004141.e013"><alternatives><graphic id="pcbi.1004141.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e013"/><mml:math id="M13" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∈</mml:mo> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∉</mml:mo> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>The estimates <inline-formula id="pcbi.1004141.e014"><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">p</mml:mtext> <mml:mo>̂</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <inline-formula id="pcbi.1004141.e015"><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">q</mml:mtext> <mml:mo>̂</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> are also known as the “plug-in” estimates, and correspond to maximum likelihood estimates for the densities in question. These estimates give us a plug-in estimate for projected single-spike information:
<disp-formula id="pcbi.1004141.e016"><alternatives><graphic id="pcbi.1004141.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e016"/><mml:math id="M16" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
where the function <italic>ĝ</italic>(<bold>x</bold>) denotes the ratio of density estimates:
<disp-formula id="pcbi.1004141.e017"><alternatives><graphic id="pcbi.1004141.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e017"/><mml:math id="M17" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≜</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula></p>
<p>Note that <italic>ĝ</italic>(<bold>x</bold>) is a piece-wise constant function that takes the value <italic>q̂</italic><sub><italic>i</italic></sub>/<italic>p̂</italic><sub><italic>i</italic></sub> over the <italic>i</italic>th histogram bin <italic>B</italic><sub><italic>i</italic></sub>.</p>
<p>Now, consider an LNP model in which the nonlinearity <italic>f</italic> is parametrized as a piece-wise constant function, taking the value <italic>f</italic><sub><italic>i</italic></sub> over histogram bin <italic>B</italic><sub><italic>i</italic></sub>. Given a projection matrix <italic>K</italic>, the ML estimate for the parameter vector <italic>α</italic> = (<italic>f</italic><sub>1</sub>, …, <italic>f</italic><sub><italic>m</italic></sub>) is the average number of spikes per stimulus in each histogram bin, divided by time bin width Δ, that is:
<disp-formula id="pcbi.1004141.e018"><alternatives><graphic id="pcbi.1004141.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e018"/><mml:math id="M18" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mo>Δ</mml:mo></mml:mfrac> <mml:mo>·</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfenced separators="" open="(" close=")"><mml:mspace width="-0.166667em"/><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>Δ</mml:mo></mml:mrow></mml:mfrac></mml:mstyle> <mml:mspace width="-0.166667em"/></mml:mfenced> <mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
Note that functions <inline-formula id="pcbi.1004141.e123"><mml:math id="M123" display="inline" overflow="scroll">
<mml:mrow>
<mml:mover accent="true">
<mml:mi>f</mml:mi>
<mml:mo stretchy="true">^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math></inline-formula> and <inline-formula id="pcbi.1004141.e124"><mml:math id="M124" display="inline" overflow="scroll">
<mml:mrow>
<mml:mover accent="true">
<mml:mi>g</mml:mi>
<mml:mo stretchy="true">^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math></inline-formula> are related by <inline-formula id="pcbi.1004141.e019"><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mo stretchy="false">(</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>Δ</mml:mo></mml:mrow></mml:mfrac> <mml:mo stretchy="true">)</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mo stretchy="false">(</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and that the sum <inline-formula id="pcbi.1004141.e020"><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. We can therefore rewrite the LNP model log-likelihood (<xref ref-type="disp-formula" rid="pcbi.1004141.e002">Equation 2</xref>):
<disp-formula id="pcbi.1004141.e021"><alternatives><graphic id="pcbi.1004141.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e021"/><mml:math id="M21" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mi>N</mml:mi></mml:mfrac> <mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="0.277778em"/><mml:mo>-</mml:mo> <mml:mspace width="0.277778em"/><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>!</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mi>N</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mfenced> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>!</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
This allows us to directly relate the empirical single-spike information (<xref ref-type="disp-formula" rid="pcbi.1004141.e016">Equation 8</xref>) with the LNP model log-likelihood, normalized by the spike count as follows:
<disp-formula id="pcbi.1004141.e022"><alternatives><graphic id="pcbi.1004141.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e022"/><mml:math id="M22" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>K</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle> <mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>-</mml:mo> <mml:mspace width="0.277778em"/><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle> <mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mi>N</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mo>∑</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>!</mml:mo></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula> <disp-formula id="pcbi.1004141.e023"><alternatives><graphic id="pcbi.1004141.e023g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e023"/><mml:math id="M23" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle> <mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>-</mml:mo> <mml:mspace width="0.277778em"/><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle> <mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
where ℒ<sub><italic>lnp</italic></sub>(<italic>θ</italic><sub>0</sub>,<italic>D</italic>) denotes the Poisson log-likelihood under a “null” model in which spike rate does not depend on the stimulus, but takes constant rate <inline-formula id="pcbi.1004141.e024"><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>λ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>Δ</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> across the entire stimulus space. In fact, the quantity −ℒ<sub><italic>lnp</italic></sub>(<italic>θ</italic><sub>0</sub>,<italic>D</italic>) can be considered an estimate for the marginal entropy of the response distribution, <inline-formula id="pcbi.1004141.e125"><mml:math id="M125" display="inline" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="1pt"/><mml:mtext>p</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>, since it is the average log-probability of the response under a Poisson model, independent of the stimulus. This makes it clear that the single-spike information <italic>I</italic><sub><italic>ss</italic></sub> can be equally regarded as “LNP information”.</p>
<p>Empirical single-spike information is therefore equal to LNP model log-likelihood per spike, plus a constant that does not depend on model parameters. This equality holds independent of time bin size Δ, the number of samples <italic>N</italic> and the number of spikes <italic>n</italic><sub><italic>sp</italic></sub>. From this relationship, it is clear that the linear projection <italic>K</italic> that maximizes <italic>Î</italic><sub><italic>ss</italic></sub> also maximizes the LNP log-likelihood ℒ<sub><italic>lnp</italic></sub>(<italic>θ</italic>;<italic>D</italic>), meaning that the MID estimate is the same as an ML estimate for the filters in an LNP model:
<disp-formula id="pcbi.1004141.e025"><alternatives><graphic id="pcbi.1004141.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e025"/><mml:math id="M25" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>I</mml:mi> <mml:mi>D</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>L</mml:mi></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula></p>
<p>Moreover, the histogram-based estimates of the raw and spike-triggered stimulus densities <inline-formula id="pcbi.1004141.e026"><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">p</mml:mtext> <mml:mo>̂</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <inline-formula id="pcbi.1004141.e027"><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">q</mml:mtext> <mml:mo>̂</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, which are used for computing the empirical single-spike information <italic>Î</italic><sub><italic>ss</italic></sub>, correspond to a particular parametrization of the LNP model nonlinearity <italic>f</italic> as a piece-wise constant function over histogram bins. The ratio of these plug-in estimates gives rise to the ML estimate for <italic>f</italic>. MID is thus formally equivalent to an ML estimator for both the linear filters and the nonlinearity of an LNP model.</p>
<p>Previous literature has not emphasized that the MID estimator implicitly provides an estimate of the LNP model nonlinearity, or that the number of histogram bins corresponds to the number of parameters governing the nonlinearity. Selecting the number of parameters for the nonlinearity is important both for accurately estimating single-spike information from finite data and for successfully finding the most informative filter or filters. <xref ref-type="fig" rid="pcbi.1004141.g003">Fig. 3</xref> illustrates this point using data from a simulated neuron with a single filter in a two-dimensional stimulus space. For small datasets, the MID estimate computed with many histogram bins (i.e., many parameters for the nonlinearity) substantially overestimates the true <italic>I</italic><sub><italic>ss</italic></sub> and yields large errors in the filter estimate. Even with 1000 stimuli and 200 spikes, a 20-bin histogram gives substantial upward bias in the estimate of single-spike information (<xref ref-type="fig" rid="pcbi.1004141.g003">Fig. 3D</xref>). Parametrization of the nonlinearity is therefore an important problem that should be addressed explicitly when using MID, e.g., by cross-validation or other model selection methods.</p>
<fig id="pcbi.1004141.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004141.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Effects of the number of histogram bins on empirical single-spike information and MID performance.</title>
<p><bold>(A)</bold> Scatter plot of raw stimuli (black) and spike-triggered stimuli (gray) from a simulated experiment using two-dimensional stimuli to drive a linear-nonlinear-Bernoulli neuron with sigmoidal nonlinearity. Arrow indicates the direction of the true filter <bold>k</bold>. <bold>(B)</bold> Plug-In estimates of <italic>p</italic>(<bold>k</bold><sup>⊤</sup><bold>s</bold>|<italic>spike</italic>), the spike-triggered stimulus distribution along the true filter axis, from 1000 stimuli and 200 spikes, using 5 (blue), 20 (green) or 80 (red) histogram bins. Black traces show estimates of raw distribution <italic>p</italic>(<bold>k</bold><sup>⊤</sup><bold>s</bold>) along the same axis. <bold>(C)</bold> True nonlinearity (black) and ML estimates of the nonlinearity (derived from the ratio of the density estimates shown in B). Roughness of the 80-bin estimate (red) arises from undersampling, or (equivalently) overfitting of the nonlinearity. <bold>(D)</bold> Empirical single-spike information vs. direction, calculated using 5, 20 or 80 histogram bins. Note that the 80-bin model overestimates the true asymptotic single-spike information at the peak by a factor of more than 1.5. <bold>(E)</bold> Convergence of empirical single-spike information along the true filter axis as a function of sample size. With small amounts of data, all three models overfit, leading to upward bias in estimated information. For large amounts of data, the 5-bin model underfits and therefore under-estimates information, since it lacks the smoothness to adequately describe the shape of the sigmoidal nonlinearity. <bold>(F)</bold> Filter error as a function of the number of stimuli, showing that the optimal number of histogram bins depends on the amount of data.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004141.g003"/>
</fig>
</sec>
<sec id="sec007">
<title>Models with Bernoulli spiking</title>
<p>Under the discrete-time inhomogeneous Poisson model considered above, spikes are modeled as conditionally independent given the stimulus, and the spike count in a discrete time bin has a Poisson distribution. However, real spike trains may exhibit more or less variability than a Poisson process [<xref ref-type="bibr" rid="pcbi.1004141.ref027">27</xref>]. In particular, the Poisson assumption breaks down when the time bin in which the data are analyzed approaches the length of the refractory period, since in that case each bin can contain at most one spike. In that case, a Bernoulli model provides a more accurate description of neural data, since it allows only 0 or 1 spike per bin. The Bernoulli and discrete-time Poisson models approach the same limiting Poisson process as the bin size (and single-bin spike probability) approaches zero while the average spike rate remains constant. However, as long as single-bin spike probabilities are above zero, the two models differ.</p>
<p>Here we show that the standard “Poisson” MID estimator does not necessarily maximize information between stimulus and response when spiking is non-Poisson. That is, if the spike count <italic>r</italic> given stimulus <bold>s</bold> is not a Poisson random variable, then MID does not necessarily find the subspace preserving maximal information between stimulus and response. To show this, we derive the mutual information between the stimulus and a Bernoulli distributed spike count, and show that this quantity is closely related to the log-likelihood under a linear-nonlinear-Bernoulli encoding model.</p>
<sec id="sec008">
<title>Linear-nonlinear-Bernoulli (LNB) model</title>
<p>We can define the linear-nonlinear-Bernoulli (LNB) model by analogy to the LNP model, but with Bernoulli instead of Poisson spiking. The parameters <italic>θ</italic> = {<italic>K</italic>,<italic>α</italic>} consist of a matrix <italic>K</italic> that determines a linear projection of the stimulus space, and a set of parameters <italic>α</italic> that govern the nonlinearity <italic>f</italic>. Here, the output of <italic>f</italic> is spike probability <italic>λ</italic> in the range [0, 1]. The probability of a spike response <italic>r</italic> ∈ {0,1} given stimulus <bold>s</bold> is governed by a Bernoulli distribution. We can express this model as
<disp-formula id="pcbi.1004141.e028"><alternatives><graphic id="pcbi.1004141.e028g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e028"/><mml:math id="M28" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>λ</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula> <disp-formula id="pcbi.1004141.e029"><alternatives><graphic id="pcbi.1004141.e029g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e029"/><mml:math id="M29" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>|</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>λ</mml:mi> <mml:mi>r</mml:mi></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>r</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
and the log-likelihood for a dataset <italic>D</italic> = {(<bold>s</bold><sub><italic>t</italic></sub>,<italic>r</italic><sub><italic>t</italic></sub>)} is
<disp-formula id="pcbi.1004141.e030"><alternatives><graphic id="pcbi.1004141.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e030"/><mml:math id="M30" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:msub><mml:mi mathvariant="bold">s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:msub><mml:mi mathvariant="bold">s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
If <italic>K</italic> has a single filter and the nonlinearity is restricted to be a logistic function, <italic>f</italic>(<italic>x</italic>) = 1/(1+exp(−<italic>x</italic>)), this reduces to the logistic regression model. Note that the spike probability <italic>λ</italic> is analogous to the single-bin Poisson rate <italic>λ</italic>Δ from the LNP model (<xref ref-type="disp-formula" rid="pcbi.1004141.e001">Equation 1</xref>), and the two models become identical in the small-bin limit where the probability of spiking <italic>p</italic>(<italic>r</italic> = 1) goes to zero [<xref ref-type="bibr" rid="pcbi.1004141.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref026">26</xref>].</p>
</sec>
<sec id="sec009">
<title>Bernoulli information</title>
<p>We can derive an equivalent dimensionality-reduction estimator in information-theoretic terms. The mutual information between the projected stimulus <bold>x</bold> = <italic>K</italic><sup>⊤</sup><bold>s</bold> and a Bernoulli spike response <italic>r</italic> ∈ {0,1} is given by:
<disp-formula id="pcbi.1004141.e031"><alternatives><graphic id="pcbi.1004141.e031g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e031"/><mml:math id="M31" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mtext>​</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mrow><mml:mo>|</mml:mo> <mml:mtext>​</mml:mtext> <mml:mo>|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula>
If we normalize by the probability of observing a spike, we obtain a quantity with units of bits-per-spike that can be directly compared to single-spike information. We refer to this as the Bernoulli information:
<disp-formula id="pcbi.1004141.e032"><alternatives><graphic id="pcbi.1004141.e032g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e032"/><mml:math id="M32" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>B</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
where <inline-formula id="pcbi.1004141.e033"><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.167em"/></mml:mrow></mml:msub> <mml:mo minsize="1.6" maxsize="1.6" stretchy="true">(</mml:mo> <mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo stretchy="false">|</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo stretchy="false">)</mml:mo> <mml:mspace width="0.167em"/><mml:mo minsize="1.6" maxsize="1.6" stretchy="true">|</mml:mo> <mml:mspace width="-0.167em"/><mml:mo minsize="1.6" maxsize="1.6" stretchy="true">|</mml:mo> <mml:mspace width="0.167em"/><mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo stretchy="false">)</mml:mo> <mml:mo minsize="1.6" maxsize="1.6" stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> is the information (per spike) carried by silences, and <italic>I</italic><sub><italic>ss</italic></sub> is the single-spike information (<xref ref-type="disp-formula" rid="pcbi.1004141.e005">Equation 4</xref>). Thus, where single-spike information quantifies the information conveyed by each spike alone (no matter how many spikes might co-occur in the same time bin) but neglects the information conveyed by silences, the Bernoulli information reflects the information conveyed by both spikes and silences.</p>
<p>Let <italic>Î</italic><sub><italic>Ber</italic></sub> = <italic>Î</italic><sub>0</sub>+<italic>Î</italic><sub><italic>ss</italic></sub> denote the empirical or plug-in estimate of the Bernoulli information, where <italic>Î</italic><sub><italic>ss</italic></sub> is the empirical single-spike information (<xref ref-type="disp-formula" rid="pcbi.1004141.e016">Equation 8</xref>), and <italic>Î</italic><sub>0</sub> is a plug-in estimate of the KL divergence between <italic>p</italic>(<bold>x</bold>|<italic>r</italic> = 0) and <italic>p</italic>(<bold>x</bold>), weighted by (<italic>N</italic>−<italic>n</italic><sub><italic>sp</italic></sub>)/<italic>n</italic><sub><italic>sp</italic></sub>, the ratio of the number of silences to the number of spikes. It is straightforward to show that empirical Bernoulli information equals the LNB model log-likelihood per spike plus a constant:
<disp-formula id="pcbi.1004141.e034"><alternatives><graphic id="pcbi.1004141.e034g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e034"/><mml:math id="M34" display="block" overflow="scroll">
<mml:mrow>
<mml:mtable>
<mml:mtr>
<mml:mtd>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mi>I</mml:mi>
<mml:mo stretchy="true">^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo><mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:msub>
<mml:mi>n</mml:mi>
<mml:mrow>
<mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
<mml:msub>
<mml:mi>ℒ</mml:mi>
<mml:mrow>
<mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mi>b</mml:mi></mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo><mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mover accent="true">
<mml:mi>r</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover>

</mml:mfrac>
<mml:mover accent="true">
<mml:mi>H</mml:mi>
<mml:mo stretchy="true">^</mml:mo>
</mml:mover>
<mml:mrow><mml:mo>[</mml:mo> <mml:mi>r</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:mtd>
</mml:mtr>

</mml:mtable></mml:mrow>
</mml:math>
</alternatives> <label>(20)</label></disp-formula>
where <inline-formula id="pcbi.1004141.e035"><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>‾</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> denotes mean spike count per bin and <inline-formula id="pcbi.1004141.e036"><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mo stretchy="false">[</mml:mo> <mml:mi>r</mml:mi> <mml:mo stretchy="false">]</mml:mo> <mml:mo>=</mml:mo> <mml:mo>−</mml:mo> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mi>N</mml:mi></mml:mfrac> <mml:mi>log</mml:mi> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mi>N</mml:mi></mml:mfrac> <mml:mo>−</mml:mo> <mml:mfrac><mml:mrow><mml:mi>N</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mi>N</mml:mi></mml:mfrac> <mml:mi>log</mml:mi> <mml:mfrac><mml:mrow><mml:mi>N</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> is the plug-in estimate for the marginal response entropy. Because the second term is independent of <italic>θ</italic>, the maximum of the empirical Bernoulli information is identical to the maximum of the LNB model likelihood, meaning that once again, we have an exact equivalence between likelihood-based and information-based estimators.</p>
</sec>
<sec id="sec010">
<title>Failure modes for MID under Bernoulli spiking</title>
<p>The empirical Bernoulli information is strictly greater than the estimated single-spike (or “Poisson”) information for a binary spike train that is not all zeros or ones, since <italic>Î</italic><sub>0</sub> &gt; 0 and these spike absences are neglected by the single-spike information measure. Only in the limit of infinitesimal time bins, where <italic>p</italic>(<italic>r</italic> = 1) → 0, does <italic>Î</italic><sub><italic>Ber</italic></sub> converge to <italic>Î</italic><sub><italic>ss</italic></sub>[<xref ref-type="bibr" rid="pcbi.1004141.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref026">26</xref>]. As a result, standard MID can fail to identify the most informative subspace when applied to a neuron with Bernoulli spiking. We illustrate this phenomenon with two (admittedly toy) simulated examples. For both examples, we compute the standard MID estimate <inline-formula id="pcbi.1004141.e037"><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>I</mml:mi> <mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> by maximizing <italic>Î</italic><sub><italic>ss</italic></sub>, and the LNB filter estimate <inline-formula id="pcbi.1004141.e038"><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>B</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> which maximizes the LNB likelihood Squashed hats.</p>
<p>The first example (<xref ref-type="fig" rid="pcbi.1004141.g004">Fig. 4</xref>) uses stimuli uniformly distributed on the right half of the unit circle. The Bernoulli spike probability <italic>λ</italic> increases linearly as a function of stimulus angle: <italic>λ</italic> = (<bold>s</bold>−<italic>π</italic>/2)/<italic>π</italic>, for <bold>s</bold> ∈ (−<italic>π</italic>/2,<italic>π</italic>/2]. For this neuron, the most informative 1D axis is the vertical axis, which is closely matched by the estimate <inline-formula id="pcbi.1004141.e039"><mml:math id="M39" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>B</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. By contrast, <inline-formula id="pcbi.1004141.e040"><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>I</mml:mi> <mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> exhibits a substantial clockwise bias, resulting from its failure to take into account the information from silences (which are more informative when spike rate is high). <xref ref-type="fig" rid="pcbi.1004141.g004">Fig. 4B</xref> shows the breakdown of total Bernoulli information into <italic>Î</italic><sub><italic>ss</italic></sub> (spikes) and <italic>Î</italic><sub>0</sub> (silences) as a function of projection angle, which illustrates the relative biases of the two quantities.</p>
<fig id="pcbi.1004141.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004141.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Illustration of MID failure mode due to non-Poisson spiking.</title>
<p><bold>(A)</bold> Stimuli were drawn uniformly on the unit half-circle, <italic>θ</italic> ∼ Unif(−<italic>π</italic>/2,<italic>π</italic>/2). The simulated neuron had Bernoulli (i.e., binary) spiking, where the probability of a spike increased linearly from 0 to 1 as <italic>θ</italic> varied from -<italic>π</italic>/2 to <italic>π</italic>/2, that is: <italic>p</italic>(<italic>spike</italic>|<italic>θ</italic>) = <italic>θ</italic>/<italic>π</italic>+1/2. Stimuli eliciting “spike” and “no-spike” are indicated by gray and black circles, respectively. For this neuron, the most informative one-dimensional linear projection corresponds to the vertical axis (<inline-formula id="pcbi.1004141.e041"><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>B</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), but the MID estimator (<inline-formula id="pcbi.1004141.e042"><mml:math id="M42" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>I</mml:mi> <mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) exhibits a 16<sup>∘</sup> clockwise bias. <bold>(B)</bold> Information from spikes (black), silences (gray), and both (red), as a function of projection angle. The peak of the Bernoulli information (which defines <inline-formula id="pcbi.1004141.e043"><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>B</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) lies close to <italic>π</italic>/2, while the peak of single-spike information (which defines <inline-formula id="pcbi.1004141.e044"><mml:math id="M44" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>I</mml:mi> <mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) exhibits the clockwise bias shown in A. Note that <inline-formula id="pcbi.1004141.e045"><mml:math id="M45" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>I</mml:mi> <mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> does not converge to the optimal direction even in the limit of infinite data, due to its lack of sensitivity to information from silences. Although this figure is framed in an information-theoretic sense, equations (19) and (20) detail the equivalence between <italic>I</italic><sub><italic>Ber</italic></sub> and ℒ<sub><italic>lnb</italic></sub>, so that this figure can be viewed from either an information-theoretic or likelihood-based perspective.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004141.g004"/>
</fig>
<p>A second example (<xref ref-type="fig" rid="pcbi.1004141.g005">Fig. 5</xref>) uses stimuli drawn from a standard bivariate Gaussian (0 mean and identity covariance), in which standard MID makes a 90 deg error in identifying the most informative one-dimensional subspace. The neuron’s nonlinearity (<xref ref-type="fig" rid="pcbi.1004141.g005">Fig. 5A</xref>) is excitatory in stimulus axis <italic>s</italic><sub>1</sub> and suppressive in stimulus axis <italic>s</italic><sub>2</sub> (indicating that a large projection onto <italic>s</italic><sub>1</sub> increases spike probability, while a large projection onto <italic>s</italic><sub>2</sub> decreases spike probability). For this neuron, both stimulus axes are clearly informative, but the (suppressive, vertical) axis <italic>s</italic><sub>2</sub> carries 13% more information than the (excitatory, horizontal) axis <italic>s</italic><sub>1</sub>. However, the standard MID estimator identifies <italic>s</italic><sub>1</sub> as the most informative axis (<xref ref-type="fig" rid="pcbi.1004141.g005">Fig. 5C</xref>), due once again to the failure to account for the information carried by silences.</p>
<fig id="pcbi.1004141.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004141.g005</object-id>
<label>Fig 5</label>
<caption>
<title>A second example Bernoulli neuron for which <inline-formula id="pcbi.1004141.e046"><mml:math id="M46" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> fails to identify the most-informative one-dimensional subspace.</title>
<p>The stimulus space has two dimensions, denoted <italic>s</italic><sub>1</sub> and <italic>s</italic><sub>2</sub>, and stimuli were drawn <italic>iid</italic> from a standard Gaussian <inline-formula id="pcbi.1004141.e127"><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1004141.e127" xlink:type="simple"/></inline-formula>(0,1). <bold>(A)</bold> The nonlinearity <italic>f</italic>(<italic>s</italic><sub>1</sub>,<italic>s</italic><sub>2</sub>) = <italic>p</italic>(<italic>spike</italic>|<italic>s</italic><sub>1</sub>,<italic>s</italic><sub>2</sub>) is excitatory in <italic>s</italic><sub>1</sub> and suppressive in <italic>s</italic><sub>2</sub>; brighter intensity indicates higher spike probability. <bold>(B)</bold> Contour plot of the stimulus-conditional densities given the two possible responses: “spike” (red) or “no-spike” (blue), along with the raw stimulus distribution (black). <bold>(C)</bold> Information carried by silences (<italic>I</italic><sub>0</sub>), single spikes (<italic>I</italic><sub><italic>ss</italic></sub>), and total Bernoulli information (<italic>I</italic><sub><italic>Ber</italic></sub> = <italic>I</italic><sub>0</sub>+<italic>I</italic><sub><italic>ss</italic></sub>) as a function of subspace orientation. The MID estimate <inline-formula id="pcbi.1004141.e047"><mml:math id="M47" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>90</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the maximum of <italic>I</italic><sub><italic>ss</italic></sub>, but the total Bernoulli information is in fact 13% higher at <inline-formula id="pcbi.1004141.e048"><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>B</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mn>0</mml:mn> <mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> due to the incorporation of no-spike information. Although both stimulus axes are clearly relevant to the neuron, MID identifies the less informative one. As with the previous figure, equations (19) and (20) detail the equivalence between <italic>I</italic><sub><italic>Ber</italic></sub> and ℒ<sub><italic>lnb</italic></sub>, so that this figure can be viewed from either an information-theoretic or likelihood-based perspective.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004141.g005"/>
</fig>
<p>These artificial examples were designed to emphasize the information carried by missing spikes, and we do not expect such stark differences between Bernoulli and Poisson estimators to arise in typical neural data. However, it is clear that the assumption of Poisson firing can lead the standard MID estimator to make mistakes when spiking is actually Bernoulli. In general, we suggest that the question of which estimator performs better is an empirical one, and depends on which model describes the true spiking process more accurately.</p>
</sec>
<sec id="sec011">
<title>Quantifying MID information loss for binary spike trains</title>
<p>In the limit of infinitesimal time bins, the information carried by silences goes to zero, and the plug-in estimates for Bernoulli and single-spike information converge: <italic>I</italic><sub>0</sub> → 0 and <italic>Î</italic><sub><italic>Ber</italic></sub> → <italic>Î</italic><sub><italic>ss</italic></sub>. However, for finite time bins, the Bernoulli information can substantially exceed single-spike information. In the previous section, we showed that this mismatch can lead to errors in subspace identification. Here we derive a lower bound on the information lost due to the neglect of <italic>I</italic><sub>0</sub>, the information (per spike) carried by silences, as a function of marginal probability of a spike, <italic>p</italic>(<italic>r</italic> = 1).</p>
<p>In the limit of rare spiking, <italic>p</italic>(<italic>r</italic> = 1) → 0, we find that:
<disp-formula id="pcbi.1004141.e049"><alternatives><graphic id="pcbi.1004141.e049g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e049"/><mml:math id="M49" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>B</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>≥</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula>
The fraction of lost information is at least half the marginal spike probability. Thus, for example, if 20% of the bins in a binary spike train contain a spike, the standard MID estimator will necessarily neglect at least 10% of the total mutual information. We show this bound holds in the asymptotic limit of small <italic>p</italic>(<italic>r</italic> = 1) (see <xref ref-type="sec" rid="sec024">Methods</xref> for details), but conjecture that it holds for all <italic>p</italic>(<italic>r</italic> = 1). The bound is tight in the Poisson limit, <italic>p</italic>(<italic>r</italic> = 1) → 0, but is substantially loose in the limit where spiking is common <italic>p</italic>(<italic>r</italic> = 1) → 1, in which all information is carried by silences. <xref ref-type="fig" rid="pcbi.1004141.g006">Fig. 6</xref> shows our bound compared to the actual (numerical) lower bound for an example with a binary stimulus.</p>
<fig id="pcbi.1004141.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004141.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Lower bound on the fraction of total information neglected by MID for a Bernoulli neuron, as a function of the marginal spike probability <italic>p</italic>(<italic>spike</italic>) = <italic>p</italic>(<italic>r</italic> = 1), for the special case of a binary stimulus.</title>
<p>Information loss is quantified as the ratio <italic>I</italic><sub>0</sub>/(<italic>I</italic><sub>0</sub>+<italic>I</italic><sub><italic>ss</italic></sub>), the information due to no-spike events, <italic>I</italic><sub>0</sub>, divided by the total information due to spikes and silences, <italic>I</italic><sub>0</sub>+<italic>I</italic><sub><italic>ss</italic></sub>. The dashed gray line shows the lower bound derived in the limit <italic>p</italic>(<italic>spike</italic>) → 0. The solid black line shows the actual minimum achieved for binary stimuli <italic>s</italic> ∈ {0,1} with <italic>p</italic>(<italic>s</italic> = 1) = <italic>q</italic>, computed via a numerical search over the parameter <italic>q</italic> ∈ [0, 1] for each value of <italic>p</italic>(<italic>spike</italic>). The lower bound is substantially loose for <italic>p</italic>(<italic>spike</italic>) &gt; 0, since as <italic>p</italic>(<italic>spike</italic>) → 1, the fraction of information due to silences goes to 1.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004141.g006"/>
</fig>
</sec>
</sec>
<sec id="sec012">
<title>Models with arbitrary spike count distributions</title>
<p>For neural responses binned at the stimulus refresh rate (e.g., 100 Hz), it is not uncommon to observe multiple spikes in a single bin. For the general case, then, we must consider an arbitrary distribution over counts conditioned on a stimulus. As we will see, maximizing the mutual information based on histogram estimators is once again equivalent to maximizing the likelihood of an LN model with piece-wise constant mappings from the linear stimulus projection to count probabilities.</p>
<sec id="sec013">
<title>Linear-nonlinear-count (LNC) model</title>
<p>Suppose that a neuron responds to a stimulus <bold>s</bold> with a spike count <italic>r</italic> ∈ {0,…,<italic>r</italic><sub>max</sub>}, where <italic>r</italic><sub>max</sub> is the maximum possible number of spikes within the time bin (constrained by the refractory period or firing rate saturation). The linear-nonlinear-count (LNC) model, which includes LNB as a special case, is defined by a linear dimensionality reduction matrix <italic>K</italic> and a set of nonlinear functions {<italic>f</italic><sup>(0)</sup> …, <italic>f</italic><sup>(<italic>r</italic><sub>max</sub>)</sup>} that map the projected stimulus to the probability of observing {0, …, <italic>r</italic><sub>max</sub>} spikes, respectively. We can write the probability of a spike response <italic>r</italic> given projected stimulus <bold>x</bold> = <italic>K</italic><sup>⊤</sup><bold>s</bold> as:
<disp-formula id="pcbi.1004141.e050"><alternatives><graphic id="pcbi.1004141.e050g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e050"/><mml:math id="M50" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>λ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="1.em"/><mml:mi>for</mml:mi><mml:mspace width="1pt"/><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>|</mml:mo> <mml:msup><mml:mi>λ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>λ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
Note that there is a constraint on the functions <italic>f</italic> requiring that ∑<sub><italic>j</italic></sub> <italic>f</italic><sup>(<italic>j</italic>)</sup>(<bold>x</bold>) = 1,∀<bold>x</bold>, since the probabilities over possible counts must add to 1 for each stimulus.</p>
<p>The LNC model log-likelihood for parameters <italic>θ</italic> = (<italic>K</italic>, <italic>α</italic><sup>(0)</sup>, … <italic>α</italic><sup>(<italic>r</italic><sub>max</sub>)</sup>) given data <italic>D</italic> = {(<bold>s</bold><sub><italic>t</italic></sub>,<italic>r</italic><sub><italic>t</italic></sub>)} can be written:
<disp-formula id="pcbi.1004141.e051"><alternatives><graphic id="pcbi.1004141.e051g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e051"/><mml:math id="M51" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:munderover> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:msub><mml:mi mathvariant="bold">s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula>
where <bold>1</bold><sub><italic>j</italic></sub>(<italic>r</italic><sub><italic>t</italic></sub>) is an indicator function selecting time bins <italic>t</italic> in which the spike count is <italic>j</italic>. As before, we consider the case where <italic>f</italic><sup>(<italic>j</italic>)</sup> takes a constant value in each of <italic>m</italic> histogram bins {<italic>B</italic><sub><italic>i</italic></sub>}, so that the parameters are just those constant values: <inline-formula id="pcbi.1004141.e052"><mml:math id="M52" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>α</mml:mi> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>j</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:msubsup><mml:mi>f</mml:mi> <mml:mn>0</mml:mn> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>j</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>f</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>j</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The maximum-likelihood estimates for the values can be given in terms of the histogram probabilities:
<disp-formula id="pcbi.1004141.e053"><alternatives><graphic id="pcbi.1004141.e053g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e053"/><mml:math id="M53" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mspace width="0.166667em"/><mml:mo>=</mml:mo> <mml:mspace width="0.166667em"/><mml:mfrac><mml:msubsup><mml:mi>n</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mi>n</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>=</mml:mo> <mml:mspace width="0.166667em"/><mml:mfrac><mml:msubsup><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mfrac><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mi>N</mml:mi></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula>
where <italic>n</italic><sub><italic>i</italic></sub> is the number of stimuli in bin <italic>B</italic><sub><italic>i</italic></sub>, <inline-formula id="pcbi.1004141.e054"><mml:math id="M54" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>n</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>j</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the number of stimuli in bin <italic>B</italic><sub><italic>i</italic></sub> that elicited <italic>j</italic> spikes, <italic>N</italic><sup>(<italic>j</italic>)</sup> is the number of stimuli in all bins that elicited <italic>j</italic> spikes, and <italic>N</italic> is the total number of stimuli. The histogram fractions of the projected raw spike counts <italic>p̂</italic><sub><italic>i</italic></sub> are defined as in <xref ref-type="disp-formula" rid="pcbi.1004141.e011">Equation 6</xref>, with the <italic>j</italic>-spike conditioned histograms defined analogously:
<disp-formula id="pcbi.1004141.e055"><alternatives><graphic id="pcbi.1004141.e055g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e055"/><mml:math id="M55" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>t</mml:mi></mml:munder> <mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mfrac><mml:msubsup><mml:mi>n</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(25)</label></disp-formula></p>
<p>Thus, the log-likelihood for projection matrix <italic>K</italic>, having already maximized with respect to the nonlinearities by using their plug-in estimates, is
<disp-formula id="pcbi.1004141.e056"><alternatives><graphic id="pcbi.1004141.e056g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e056"/><mml:math id="M56" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>K</mml:mi> <mml:mo>;</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:munderover> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mn mathvariant="bold">1</mml:mn> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:msub><mml:mi mathvariant="bold">s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(26)</label></disp-formula> <disp-formula id="pcbi.1004141.e057"><alternatives><graphic id="pcbi.1004141.e057g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e057"/><mml:math id="M57" display="block" overflow="scroll"><mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(27)</label></disp-formula> <disp-formula id="pcbi.1004141.e058"><alternatives><graphic id="pcbi.1004141.e058g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e058"/><mml:math id="M58" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>n</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>n</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mi>n</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(28)</label></disp-formula> <disp-formula id="pcbi.1004141.e059"><alternatives><graphic id="pcbi.1004141.e059g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e059"/><mml:math id="M59" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msubsup><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo form="prefix">log</mml:mo> <mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:msubsup><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mfrac><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mi>N</mml:mi></mml:mfrac></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(29)</label></disp-formula> <disp-formula id="pcbi.1004141.e060"><alternatives><graphic id="pcbi.1004141.e060g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e060"/><mml:math id="M60" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msubsup><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo form="prefix">log</mml:mo> <mml:mfenced open="(" close=")"><mml:mfrac><mml:msubsup><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mfenced></mml:mfenced> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:munderover> <mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo form="prefix">log</mml:mo> <mml:mfenced open="(" close=")"><mml:mfrac><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mi>N</mml:mi></mml:mfrac></mml:mfenced></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(30)</label></disp-formula></p>
</sec>
<sec id="sec014">
<title>Information in spike counts</title>
<p>If the binned spike-counts <italic>r</italic><sub><italic>t</italic></sub> measured in response to stimuli <bold>s</bold><sub><italic>t</italic></sub> are not Poisson distributed, the projection matrix <italic>K</italic> which maximizes the mutual information between <italic>K</italic><sup>⊤</sup><bold>s</bold> and <italic>r</italic> can be found as follows. Recalling that <italic>r</italic><sub>max</sub> is the maximal spike count possible in the time bin and writing <bold>x</bold> = <italic>K</italic><sup>⊤</sup><bold>s</bold>, we have:
<disp-formula id="pcbi.1004141.e061"><alternatives><graphic id="pcbi.1004141.e061g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e061"/><mml:math id="M61" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mi>H</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>H</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(31)</label></disp-formula> <disp-formula id="pcbi.1004141.e062"><alternatives><graphic id="pcbi.1004141.e062g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e062"/><mml:math id="M62" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold">x</mml:mi> <mml:mspace width="0.277778em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold">x</mml:mi> <mml:mspace width="0.277778em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(32)</label></disp-formula> <disp-formula id="pcbi.1004141.e063"><alternatives><graphic id="pcbi.1004141.e063g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e063"/><mml:math id="M63" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold">x</mml:mi> <mml:mspace width="0.277778em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(33)</label></disp-formula> <disp-formula id="pcbi.1004141.e064"><alternatives><graphic id="pcbi.1004141.e064g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e064"/><mml:math id="M64" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(34)</label></disp-formula>
To ease comparison with the single-spike information, which is measured in bits per spike, we normalize the mutual information by the mean spike count to obtain:
<disp-formula id="pcbi.1004141.e065"><alternatives><graphic id="pcbi.1004141.e065g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e065"/><mml:math id="M65" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>u</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mfrac> <mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(35)</label></disp-formula>
where <italic>r̄</italic> = ∑<sub><italic>t</italic></sub> <italic>r</italic><sub><italic>t</italic></sub>/<italic>N</italic> is the mean spike count, and <inline-formula id="pcbi.1004141.e066"><mml:math id="M66" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>‾</mml:mo></mml:mover></mml:mfrac> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.167em"/></mml:mrow></mml:msub> <mml:mo minsize="1.6" maxsize="1.6" stretchy="true">(</mml:mo> <mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo stretchy="false">|</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mspace width="0.167em"/><mml:mo minsize="1.6" maxsize="1.6" stretchy="true">|</mml:mo> <mml:mspace width="-0.167em"/><mml:mo minsize="1.6" maxsize="1.6" stretchy="true">|</mml:mo> <mml:mspace width="0.167em"/><mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo stretchy="false">)</mml:mo> <mml:mo minsize="1.6" maxsize="1.6" stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> is the normalized information carried by the <italic>j</italic>-spike responses. Note that <italic>I</italic><sub>1</sub>, the information carried by single-spike responses, is <italic>not</italic> the same as the single-spike information <italic>I</italic><sub><italic>ss</italic></sub>, since the latter combines information from all responses with 1 or more spikes, by assuming that each spike is conditionally independent of all other spikes.</p>
<p>We can estimate the mutual information from data using a histogram based plug-in estimator:
<disp-formula id="pcbi.1004141.e067"><alternatives><graphic id="pcbi.1004141.e067g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e067"/><mml:math id="M67" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>u</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:munderover> <mml:mfrac><mml:mn>1</mml:mn> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mfrac> <mml:mfrac><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mi>N</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:msubsup><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:msubsup><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(36)</label></disp-formula>
Comparison with the LNC model log-likelihood (<xref ref-type="disp-formula" rid="pcbi.1004141.e060">Equation 30</xref>) reveals that:
<disp-formula id="pcbi.1004141.e068"><alternatives><graphic id="pcbi.1004141.e068g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e068"/><mml:math id="M68" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>u</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac> <mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mfrac> <mml:mover accent="true"><mml:mi>H</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>r</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(37)</label></disp-formula>
where <inline-formula id="pcbi.1004141.e069"><mml:math id="M69" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mo stretchy="false">[</mml:mo> <mml:mi>r</mml:mi> <mml:mo stretchy="false">]</mml:mo> <mml:mo>=</mml:mo> <mml:mo>−</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mtext mathvariant="normal">max</mml:mtext></mml:msub></mml:msubsup> <mml:mfrac><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>j</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup> <mml:mi>N</mml:mi></mml:mfrac> <mml:mi>log</mml:mi> <mml:mfrac><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>j</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup> <mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> is the plug-in estimate for the marginal entropy of the observed spike counts. Note that this reduces to the relationship between Bernoulli information and LNB model log-likelihood (<xref ref-type="disp-formula" rid="pcbi.1004141.e034">Equation 20</xref>) in the special case where <italic>r</italic><sub>max</sub> = 1.</p>
<p>Thus, even in the most general case of an arbitrary distribution over spike counts given a stimulus, the subspace projection <italic>K</italic> that maximizes the histogram-based estimate of mutual information is identical to the maximum-likelihood <italic>K</italic> for an LN model with a corresponding piece-wise constant parametrization of the nonlinearities.</p>
</sec>
<sec id="sec015">
<title>Failures of MID under non-Poisson count distributions</title>
<p>We formulate two simple examples to illustrate the sub-optimality of standard MID for neurons whose stimulus-conditioned count distributions are not Poisson. For both examples, the neuron was sensitive to a one-dimensional projection along the horizontal axis and emitted either 0, 1, or 2 spikes in response to a stimulus.</p>
<p>Both are illustrated in <xref ref-type="fig" rid="pcbi.1004141.g007">Fig. 7</xref>. The first example (A) involves a deterministic neuron, where spike count is 0, 1, or 2 according to a piece-wise constant nonlinear function of the projected stimulus. Here, MID does not use the information from zero or two-spike bins optimally; it ignores information from zero-spike responses entirely, and treats stimuli eliciting two spikes as two independent samples from <italic>p</italic>(<bold>x</bold>|<italic>spike</italic>). The <italic>I</italic><sub><italic>count</italic></sub> estimator, by contrast, is sensitive to the non-Poisson statistics of the response, and combines information from all spike counts (<xref ref-type="disp-formula" rid="pcbi.1004141.e065">Equation 35</xref>), yielding both higher information and faster convergence to the true filter.</p>
<fig id="pcbi.1004141.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004141.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Two examples illustrating sub-optimality of MID under discrete (non-Poisson) spiking.</title>
<p>In both cases, stimuli were uniformly distributed within the unit circle and the simulated neuron’s response depended on a 1D projection of the stimulus onto the horizontal axis (<italic>θ</italic> = 0). Each stimulus evoked 0, 1, or 2 spikes. <bold>(A)</bold> Deterministic neuron. <italic>Left</italic>: Scatter plot of stimuli labelled by number of spikes evoked, and the piece-wise constant nonlinearity governing the response (below). The nonlinearity sets the response count deterministically, thus dramatically violating Poisson expectations. <italic>Middle</italic>: information vs. axis of projection. The total information <italic>I</italic><sub><italic>count</italic></sub> reflects the information from 0-, 1-, and 2-spike responses (treated as distinct symbols), while the single-spike information <italic>I</italic><sub><italic>ss</italic></sub> ignores silences and treats 2-spike responses as two samples from <italic>p</italic>(<bold>s</bold>|<italic>spike</italic>). <italic>Right</italic>: Average absolute error in <inline-formula id="pcbi.1004141.e070"><mml:math id="M70" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>I</mml:mi> <mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="pcbi.1004141.e071"><mml:math id="M71" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>u</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as a function of sample size; the latter achieves 18% lower error due to its sensitivity to the non-Poisson structure of the response. <bold>(B)</bold> Stochastic neuron with sigmoidal nonlinearity controlling the stochasticity of responses. The neuron transitions from almost always emitting 1 spike for large negative stimulus projections, to generating either 0 or 2 spikes with equal probability at large positive projections. Here, the nonlinearity does not modulate the mean spike rate, so <italic>Î</italic><sub><italic>ss</italic></sub> is approximately zero for all stimulus projections (middle) and the MID estimator does not converge (right). However, the <inline-formula id="pcbi.1004141.e072"><mml:math id="M72" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold">k</mml:mtext> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>u</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> estimate converges because the LNC model is sensitive to the change in conditional response distribution. <xref ref-type="disp-formula" rid="pcbi.1004141.e068">Equation (37)</xref> details the relationship between <italic>I</italic><sub><italic>count</italic></sub> and ℒ<sub><italic>lnc</italic></sub>, so that this figure can be interpreted from either an information-theoretic or likelihood-based perspective.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004141.g007"/>
</fig>
<p>Our second example (<xref ref-type="fig" rid="pcbi.1004141.g007">Fig. 7B</xref>) involves a model neuron in which a sigmoidal nonlinearity determines the probability that it fires exactly 1 spike (high at negative stimulus projections) or stochastically emits either 0 or 2 spikes, each with probability 0.5 at positive stimulus projections). Thus, the nonlinearity does not change the mean spike rate, but strongly affects its variance. Because the probability of observing a single spike is not affected by the stimulus, single-spike information is zero for all projections, and the MID estimate does not converge to the true filter even with infinite data. However, the full count information <italic>Î</italic><sub><italic>count</italic></sub> correctly weights the information carried by different spike counts and provides a consistent estimator for <italic>K</italic>.</p>
</sec>
</sec>
<sec id="sec016">
<title>Identifying high-dimensional subspaces</title>
<p>A significant drawback to standard MID is that it does not scale tractably to high-dimensional subspaces; that is, to the simultaneous estimation of many filters. MID has usually been limited to estimation of only one or two filters, and we are unaware of a practical setting in which it has been used to recover more than three. This stands in contrast to methods like spike-triggered covariance (STC) [<xref ref-type="bibr" rid="pcbi.1004141.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref007">7</xref>], information-theoretic spike-triggered average and covariance (iSTAC) [<xref ref-type="bibr" rid="pcbi.1004141.ref019">19</xref>], projection-pursuit regression [<xref ref-type="bibr" rid="pcbi.1004141.ref028">28</xref>], Bayesian spike-triggered covariance [<xref ref-type="bibr" rid="pcbi.1004141.ref014">14</xref>], and quadratic variants of MID [<xref ref-type="bibr" rid="pcbi.1004141.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref022">22</xref>], all of which can tractably estimate ten or more filters. This capability may be important, given that V1 neurons exhibit sensitivity to as many as 15 dimensions [<xref ref-type="bibr" rid="pcbi.1004141.ref029">29</xref>], and many canonical neural computations (e.g., motion estimation) require a large number of stimulus dimensions [<xref ref-type="bibr" rid="pcbi.1004141.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref030">30</xref>].</p>
<p>Before we continue, it is helpful to consider <italic>why</italic> MID is impractical for high-dimensional feature spaces. The problem isn’t the number of filter parameters: these scale linearly with dimensionality, since a <italic>p</italic>-filter model with <italic>D</italic>-dimensional stimuli requires only <italic>Dp</italic> parameters, or indeed only <inline-formula id="pcbi.1004141.e073"><mml:math id="M73" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>D</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>−</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> parameters to specify the subspace naively after accounting for degeneracies. The problem is instead the number of parameters needed to specify the densities <italic>p</italic>(<bold>x</bold>) and <italic>p</italic>(<bold>x</bold>|<italic>spike</italic>). For histogram-based density estimators, the number of parameters grows exponentially with dimension: a histogram with <italic>m</italic> bins along each of <italic>p</italic> filter axes requires <italic>m</italic><sup><italic>p</italic></sup> parameters, a phenomenon sometimes called the “curse of dimensionality”.</p>
<sec id="sec017">
<title>Density vs. nonlinearity estimation</title>
<p>A key benefit of the LNP model likelihood framework is that it shifts the focus of estimation away from the separate densities <italic>p</italic>(<bold>x</bold>|<italic>spike</italic>) and <italic>p</italic>(<bold>x</bold>) to a single nonlinear function <italic>f</italic>. This change in focus makes it easier to scale the likelihood approach to high dimensions for a few different reasons. First, direct estimation of a single nonlinearity in place of two densities immediately halves the number of parameters required to achieve a similarly detailed picture of the neuron’s response to the filtered stimulus. Second, the dependence of the MID cost function on the logarithm of the ratio <italic>p</italic>(<bold>x</bold>|<italic>spike</italic>)/<italic>p</italic>(<bold>x</bold>) makes it very sensitive to noise in the estimated value of the denominator <italic>p</italic>(<bold>x</bold>) when that value is near 0. Unfortunately, as <italic>p</italic>(<bold>x</bold>) is also the probability with which samples are generated, these low-value regions are precisely where the fewest samples are available. This is a common difficulty in the empirical estimation of information-theoretic quantities, and others working in more general machine-learning settings have suggested direct estimation of the ratio rather than its parts [<xref ref-type="bibr" rid="pcbi.1004141.ref031">31</xref>–<xref ref-type="bibr" rid="pcbi.1004141.ref033">33</xref>]. In LN neural modeling such direct estimation of the ratio is equivalent to direct estimation of the nonlinearity.</p>
<p>Third, … as the nonlinearity is a property of the neuron rather than the stimulus, it may be more straightforward to construct a smoothed or structured parametrization for <italic>f</italic> (or to regularize its estimate based on prior beliefs about neuronal properties) instead of regularizing the estimates of stimulus densities. For example, consider an experiment using natural visual images. While natural images presumably form a smooth manifold within the space of all possible pixel patterns, the structure of this manifold is neither simple nor known. The natural distribution of images does not factor over disjoint sets of pixels, nor over linear projections of pixel values. A small random perturbation in all pixels makes a natural image appear unnaturally noisy, violating the underlying presumption of kernel density estimators that local perturbations do not alter the density much. Indeed the question of how best to model the distribution of natural stimuli is a matter of active research. By contrast, we might expect to be able to develop better parametric forms to describe the non-linearities employed by neural systems. For instance, we might expect the neural nonlinearity to vary smoothly in the space of photoreceptor activation, and thus of filter outputs. Thus, locally kernel-smoothed estimates of the non-linear mapping—or even parametric choices of function class, such as low-order polynomials—might be valid, even if the stimulus density changes abruptly. Alternatively, subunits within the neural receptive field might lead to additively or multiplicatively separable components of the nonlinearity that act on the outputs of different filters. In this case, it would be possible to factor <italic>f</italic> between two subsets of filter outputs, say to give <italic>f</italic>(<bold>x</bold>) = <italic>f</italic><sub>1</sub>(<bold>x</bold><sub>1</sub>)<italic>f</italic><sub>2</sub>(<bold>x</bold><sub>2</sub>), even though there is no reason for the stimulus distribution to factor <italic>p</italic>(<bold>x</bold>) ≠ <italic>p</italic>(<bold>x</bold><sub>1</sub>)<italic>p</italic>(<bold>x</bold><sub>2</sub>). This reduction of <italic>f</italic> to two (or more) lower-dimensional functions would avoid the exponential parameter explosion implied by the curse of dimensionality.</p>
<p>Indeed, such strategies for parametrization of the nonlinear mapping are already implicit in likelihood-based estimators inspired by the spike-triggered average and covariance. In many such cases, <italic>f</italic> is parametrized by a quadratic form embedded in a 1D nonlinearity [<xref ref-type="bibr" rid="pcbi.1004141.ref014">14</xref>], so that the number of parameters scales only quadratically with the number of filters. A similar approach has been formulated in information-theoretic terms using a quadratic logistic Bernoulli model [<xref ref-type="bibr" rid="pcbi.1004141.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref024">24</xref>]. Another method, known as extended projection pursuit regression (ePPR) [<xref ref-type="bibr" rid="pcbi.1004141.ref028">28</xref>], parametrizes <italic>f</italic> as a sum of one-dimensional nonlinearities, in which case the number of parameters grows only linearly with the number of filters.</p>
</sec>
<sec id="sec018">
<title>Parametrizing the many-filter LNP model</title>
<p>Here we provide a general formulation that encompasses both standard MID and constrained methods that scale to high-dimensional subspaces. We can rewrite the LNP model (<xref ref-type="disp-formula" rid="pcbi.1004141.e001">Equation 1</xref>) as follows:
<disp-formula id="pcbi.1004141.e074"><alternatives><graphic id="pcbi.1004141.e074g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e074"/><mml:math id="M74" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mspace width="2.em"/><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>dimensionality reduction</mml:mtext> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(38)</label></disp-formula> <disp-formula id="pcbi.1004141.e075"><alternatives><graphic id="pcbi.1004141.e075g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e075"/><mml:math id="M75" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>λ</mml:mi> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mi>ϕ</mml:mi></mml:msub></mml:munderover> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="2.em"/><mml:mrow><mml:mo>(</mml:mo> <mml:mi>nonlinearity</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(39)</label></disp-formula> <disp-formula id="pcbi.1004141.e076"><alternatives><graphic id="pcbi.1004141.e076g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e076"/><mml:math id="M076" display="inline" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>r</mml:mi><mml:mo>|</mml:mo><mml:mi>λ</mml:mi><mml:mo>∼</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mi>Δ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(40)</label></disp-formula>
The nonlinearity <italic>f</italic> is parametrized using basis functions {<italic>φ</italic><sub><italic>i</italic></sub>(⋅)}, <italic>i</italic> = 1,…,<italic>n</italic><sub><italic>φ</italic></sub>, which are linearly combined with weights <italic>α</italic><sub><italic>i</italic></sub> and then passed through a scalar nonlinearity <italic>g</italic>. We refer to <italic>g</italic> as the output nonlinearity; its primary role is to ensure the spike rate <italic>λ</italic> is positive regardless of weights <italic>α</italic><sub><italic>i</italic></sub>. This can also be considered a special case of an LNLN model [<xref ref-type="bibr" rid="pcbi.1004141.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref035">35</xref>].</p>
<p>If we fix <italic>g</italic> and the basis functions {<italic>φ</italic><sub><italic>i</italic></sub>} in advance, fitting the nonlinearity simply involves estimating the parameters <italic>α</italic><sub><italic>i</italic></sub> from the projected stimuli and associated spike counts. If <italic>g</italic> is convex and log-concave, then the log-likelihood is concave in {<italic>α</italic><sub><italic>i</italic></sub>} given <italic>K</italic>, meaning the parameters governing <italic>f</italic> can be fit without getting stuck in non-global maxima [<xref ref-type="bibr" rid="pcbi.1004141.ref011">11</xref>].</p>
<p>Standard MID can be seen as a special case of this general framework: it sets <italic>g</italic> to the identity function and the basis functions <italic>φ</italic><sub><italic>i</italic></sub> to histogram-bin indicator functions (denoted <bold>1</bold><sub><italic>B</italic><sub><italic>i</italic></sub></sub>(⋅) in <xref ref-type="disp-formula" rid="pcbi.1004141.e013">Equation 7</xref>). The maximum-likelihood weights <inline-formula id="pcbi.1004141.e126"><mml:math id="M126" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are proportional to the ratio between the number of spikes and number of stimuli in the <italic>i</italic>’th histogram bin (<xref ref-type="disp-formula" rid="pcbi.1004141.e018">Equation 10</xref>). As discussed above, the number of basis functions <italic>n</italic><sub><italic>φ</italic></sub> scales exponentially with the number of filters, making this parametrization impractical for high-dimensional feature spaces.</p>
<p>Another special case of this framework corresponds to Bayesian spike-triggered covariance analysis [<xref ref-type="bibr" rid="pcbi.1004141.ref014">14</xref>], in which the basis functions <italic>φ</italic><sub><italic>i</italic></sub> are taken to be linear and quadratic functions of the projected stimulus. If the stimulus is Gaussian, then standard STC and iSTAC provide an asymptotically optimal fit to this model under the assumption that <italic>g</italic> is exponential [<xref ref-type="bibr" rid="pcbi.1004141.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref019">19</xref>].</p>
<p>In principle, we can select any set of basis functions. Other reasonable choices include polynomials, sigmoids, sinusoids (i.e., Fourier components), cubic splines, radial basis functions, or any mixture of these bases. Alternatively, we could use non-parametric models such as Gaussian processes, which have been used to model low-dimensional tuning curves and firing rate maps [<xref ref-type="bibr" rid="pcbi.1004141.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref037">37</xref>]. Theoretical convergence for arbitrary high-dimensional nonlinearities requires a scheme for increasing the complexity of the basis or non-parametric model as we increase the amount of data recorded [<xref ref-type="bibr" rid="pcbi.1004141.ref038">38</xref>–<xref ref-type="bibr" rid="pcbi.1004141.ref041">41</xref>]. We do not examine such theoretical details here, focusing instead on the problem of choosing a particular basis that is well suited to the dataset at hand. Below, we introduce basis functions {<italic>φ</italic><sub><italic>i</italic></sub>} that provide a reasonable tradeoff between flexibility and tractability for parametrizing high-dimensional nonlinear functions.</p>
</sec>
<sec id="sec019">
<title>Cylindrical basis functions for the LNP nonlinearity</title>
<p>We propose to parametrize the nonlinearity for many-filter LNP models using cylindrical basis functions (CBFs), which we introduce by analogy to radial basis functions (RBFs). These functions are restricted in some directions of the feature space (like RBFs), but are constant along other dimensions. They are therefore the function-domain analogues to the probability “experts” used in product-of-experts models [<xref ref-type="bibr" rid="pcbi.1004141.ref042">42</xref>] in that they constrain a high-dimensional function along only a small number of dimensions, while imposing no structure on the others.</p>
<p>We define a “first-order” CBF as a Gaussian bump in one direction of the feature space, parametrized by center location <italic>μ</italic> and a characteristic width <italic>σ</italic>:
<disp-formula id="pcbi.1004141.e077"><alternatives><graphic id="pcbi.1004141.e077g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e077"/><mml:math id="M77" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mrow><mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(41)</label></disp-formula>
which affects the function along vector component <italic>x</italic><sub><italic>i</italic></sub> and is invariant along <italic>x</italic><sub><italic>j</italic> ≠ <italic>i</italic></sub>. Parametrizing <italic>f</italic> with first-order CBFs is tantamount to assuming <italic>f</italic> can be parametrized as the sum of 1D functions along each filter axis, that is <italic>f</italic>(<bold>x</bold>) = <italic>g</italic>(<italic>f</italic><sub>1</sub>(<italic>x</italic><sub>1</sub>) + … <italic>f</italic><sub><italic>m</italic></sub>(<italic>x</italic><sub><italic>m</italic></sub>)), where each function <italic>f</italic><sub><italic>i</italic></sub> is parametrized with a linear combination of “bump” functions. This setup resembles the parametrization used in the extended projection-pursuit regression (ePPR) model [<xref ref-type="bibr" rid="pcbi.1004141.ref028">28</xref>], although the nonlinear transformation <italic>g</italic> confers some added flexibility. For example, we can have multiplicative combination when <italic>g</italic>(⋅) = exp(⋅), resulting in a separable <italic>f</italic>, or rectified additive combination when <italic>g</italic>(⋅) = max(⋅,0), which is closer to ePPR. If we use <italic>d</italic> basis functions along each filter axis, the resulting nonlinearity requires <italic>kd</italic> parameters for a <italic>k</italic>-filter LNP model.</p>
<p>We can define second-order CBFs as functions that have Gaussian dependence on two dimensions of the input space and that are insensitive to all others:
<disp-formula id="pcbi.1004141.e078"><alternatives><graphic id="pcbi.1004141.e078g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e078"/><mml:math id="M78" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>n</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mrow><mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(42)</label></disp-formula>
where <italic>μ</italic><sub><italic>i</italic></sub> and <italic>μ</italic><sub><italic>j</italic></sub> determine the center of the basis function in the (<italic>x</italic><sub><italic>i</italic></sub>,<italic>x</italic><sub><italic>j</italic></sub>) plane. A second-order basis represents <italic>f</italic> as a (transformed) sum of these bivariate functions, giving <inline-formula id="pcbi.1004141.e079"><mml:math id="M79" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mi>k</mml:mi></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="center"><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo stretchy="true">)</mml:mo></mml:mrow> <mml:msup><mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> parameters if we use <italic>d</italic><sup>2</sup> basis functions for each of the <inline-formula id="pcbi.1004141.e080"><mml:math id="M80" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mi>k</mml:mi></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="center"><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> possible pairs of <italic>k</italic> filter outputs, or merely <inline-formula id="pcbi.1004141.e081"><mml:math id="M81" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>k</mml:mi> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> if we instead partition the <italic>k</italic> filters into disjoint pairs. Higher-order CBFs can be defined analogously: <italic>k</italic><sup>′</sup>th order CBFs are Gaussian RBFs in a <italic>k</italic><sup>′</sup>-dimensional subspace while remaining constant in the remaining <italic>k</italic>−<italic>k</italic><sup>′</sup> dimensions. Of course, there is no need to represent the entire nonlinearity using CBFs of the same order. It might make sense, for example, to represent the nonlinear combination of the first two filter responses with second order CBFs (which is comparable to standard MID with a 2D histogram representation of the nonlinearity), and then use first order CBFs to represent the contributions of additional (less-informative) filter outputs.</p>
<p>To illustrate the feasibility of this approach, we applied dimensionality reduction methods to a previously published dataset from macaque V1 [<xref ref-type="bibr" rid="pcbi.1004141.ref029">29</xref>]. This dataset contains extracellular single unit recordings of simple and complex cells driven by an oriented 1D binary white noise stimulus sequence (i.e., “flickering bars”). For each neuron, we fit an LNP model using: (1) the information-theoretic spike-triggered average and covariance (iSTAC) estimator [<xref ref-type="bibr" rid="pcbi.1004141.ref019">19</xref>]; and (2) the maximum likelihood estimator for an LNP model with nonlinearity parametrized by first-order CBFs. The iSTAC estimator, which combines information from the STA and STC, returns a list of filters ordered by informativeness about the neural response. It models the nonlinearity as an exponentiated quadratic function (an instance of a generalized quadratic model [<xref ref-type="bibr" rid="pcbi.1004141.ref023">23</xref>]), and yields asymptotically optimal performance under the condition that stimuli are Gaussian. For comparison, we also implemented a model with a less-constrained nonlinearity, using Gaussian RBFs sensitive to all filter outputs (rbf-LNP). This approach is comparable to “classic” MID, although it exploits the LNP formulation to allow local smoothing of the nonlinearity (rather than square histogram bins.). Even so, the number of parameters in the nonlinearity still grows exponentially with the number of filters so, computational concerns prevented us from recovering more than four filters with this method.</p>
<p><xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8</xref> compares the performance of these estimators on neural data, and illustrates the ability to tractably recover high-dimensional feature spaces using maximum likelihood methods, provided that the nonlinearity is parametrized appropriately. We used 3 CBFs per filter output for the cbf-LNP model (resulting in 3<italic>p</italic> parameters for the nonlinearity), and a grid with 3 RBFs per dimension for the rbf-LNP model (3<sup><italic>p</italic></sup> parameters). By contrast, the exponentiated-quadratic nonlinearity underlying the iSTAC estimator requires <italic>O</italic>(<italic>p</italic><sup>2</sup>) parameters.</p>
<fig id="pcbi.1004141.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004141.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Estimation of high-dimensional subspaces using a nonlinearity parametrized with cylindrical basis functions (CBFs).</title>
<p><bold>(A)</bold> Eight most informative filters for an example complex cell, estimated with iSTAC (<italic>top row</italic>) and cbf-LNP (<italic>bottom row</italic>). For the cbf-LNP model, the nonlinearity was parametrized with three first-order CBFs for the output of each filter (see <xref ref-type="sec" rid="sec024">Methods</xref>). <bold>(B)</bold> Estimated 1D nonlinearity along each filter axis, for the filters shown in (A). Note that third and fourth iSTAC filters are suppressive while third and fourth cbf-LNP filter are excitatory. <bold>(C)</bold> Cross-validated single-spike information for iSTAC, cbf-LNP, and rbf-LNP, as a function of the number of filters, averaged over a population of 16 neurons (selected from [<xref ref-type="bibr" rid="pcbi.1004141.ref029">29</xref>] for having ≥ 8 informative filters). The cbf-LNP estimate outperformed iSTAC in all cases, while rbf-LNP yielded a slight further increase for the first four dimensions. <bold>(D)</bold> Computation time for the numerical optimization of the cbf-LNP likelihood for up to 8 filters. Even for 30 minutes of data and 8 filters, optimisation took about 4 hours. <bold>(E)</bold> Average number of excitatory filters as a function of total number of filters, for each method. <bold>(F)</bold> Information gain from excitatory filters, for each method, averaged across neurons. Each point represents the average amount of information gained from adding an excitatory filter, as a function of the number of filters.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004141.g008"/>
</fig>
<p>To compare performance, we analyzed the growth in empirical single-spike information (computed on a “test” dataset) as a function of the number of filters. Note that this is equivalent to computing test log-likelihood under the LNP model. For a subset of neurons determined to have 8 or more informative filters (16/59 cells), the cbf-LNP filters captured more information than the iSTAC filters (<xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8C</xref>). This indicates that the CBF nonlinearity captures the nonlinear mapping from filter outputs to spike rate more accurately than an exponentiated quadratic, and that this flexibility confers advantages in identifying the most informative stimulus dimensions. The first four filters estimated under the rbf-LNP model captured slightly more information again than the cbf-LNP filters, indicating that first-order CBFs provide slightly too restrictive a parametrization for these neurons. Due to computational considerations, we did not attempt to fit the rbf-LNP model with &gt; 4 filters, but note that the cbf-LNP model scaled easily to 8 filters (<xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8D</xref>).</p>
<p>In addition to its quantitative performance, the cbf-LNP estimate exhibited a qualitative difference from iSTAC with regard to the ordering of filters by informativeness. In particular, the cbf-LNP fit reveals that excitatory filters provide more information than iSTAC attributes to them, and that excitatory filters should come earlier relative to suppressive filters when ordering by informativeness. <xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8A-B</xref>, which shows the first 8 filters and associated marginal one-dimensional nonlinearities for an example V1 complex cell, provides an illustration of this discrepancy. Under the iSTAC estimate (<xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8A</xref>, top row), the first two most informative filters are excitatory but the third and fourth are suppressive (see nonlinearities in <xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8B</xref>). However, the cbf-LNP estimate (and rbf-LNP estimate, not shown) indicates that the four most informative filters are all excitatory. This tendency holds across the population of neurons. We can quantify it in terms of the number of excitatory filters within the first <italic>n</italic> filters identified (<xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8E</xref>) or the total amount of information (i.e., log-likelihood) contributed by excitatory filters (<xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8F</xref>). This shows that iSTAC, which nevertheless provides a computationally inexpensive initialization for the cbf-LNP estimate, does not accurately quantify the information contributed by excitatory filters. Most likely, this reflects the fact that an exponentiated-quadratic does not provide as accurate a description of the nonlinearity along excitatory stimulus dimensions as can be obtained with a non-parametric estimator.</p>
</sec>
</sec>
<sec id="sec020">
<title>Relationship to previous work</title>
<p>A variety of neural dimensionality reduction methods have been proposed previously. Here, we consider the relationship of the methods described in this study to these earlier approaches. Rapela <italic>et al</italic> [<xref ref-type="bibr" rid="pcbi.1004141.ref028">28</xref>] introduced a technique known as extended Projection Pursuit Regression (ePPR), where the high-dimensional estimation problem is reduced to a sequence of simpler low-dimensional ones. The approach is iterative. A one-dimensional model is found first, and the dimensionality is then progressively increased to optimize a cost function, but with the search for filters restricted to dimensions orthogonal to all the filters already identified. From a theoretical perspective this assumes that the spiking probability can be defined as a sum of functions of the different stimulus components; that is,
<disp-formula id="pcbi.1004141.e082"><alternatives><graphic id="pcbi.1004141.e082g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e082"/><mml:math id="M82" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">k</mml:mi> <mml:mn>1</mml:mn> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">k</mml:mi> <mml:mn>2</mml:mn> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>⋯</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>N</mml:mi> <mml:mi>⊤</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(43)</label></disp-formula>
Rowekamp <italic>et al</italic> [<xref ref-type="bibr" rid="pcbi.1004141.ref043">43</xref>] compared such an approach to the joint optimization more common in MID analysis (as in [<xref ref-type="bibr" rid="pcbi.1004141.ref018">18</xref>]), and derived the bias that results from sequential optimization and its implicit additivity. By contrast, we have focused here on parametrization rather than sequential optimization. In all cases, we optimized the log-likelihood simultaneously over all filter dimensions. For high-dimensional models, we advocate parametrization of the nonlinearity so as to avoid the curse of dimensionality. However, the CBF form we have introduced is more flexible than that of ePPR, both in that two- or more dimensional components are easily included, and in that the outputs of the components can be combined non-linearly.</p>
<p>Other proposals can be seen as assuming specific quadratic-based parametrizations for the nonlinearity, that are more restrictive than the CBF form. The iSTAC estimator, introduced by Pillow &amp; Simoncelli [<xref ref-type="bibr" rid="pcbi.1004141.ref019">19</xref>], is based on maximization of the KL divergence between Gaussian approximations to the spike-triggered and stimulus ensembles—thus finding the feature space that maximizes the single-spike information under a Gaussian model of both the spike-triggered and stimulus ensembles. Park &amp; Pillow [<xref ref-type="bibr" rid="pcbi.1004141.ref044">44</xref>] showed its relationship to an LNP model with an exponentiated quadratic spike rate, which takes the form:
<disp-formula id="pcbi.1004141.e083"><alternatives><graphic id="pcbi.1004141.e083g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e083"/><mml:math id="M83" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>K</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi>C</mml:mi> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(44)</label></disp-formula>
Such a nonlinearity readily yields maximum likelihood estimators based on the STA and STC. Moreover, they proposed a new model, known as “elliptical LNP”, which allowed estimation of a non-parametric nonlinearity around the quadratic function (instead of assuming an exponential form). Rajan et al. [<xref ref-type="bibr" rid="pcbi.1004141.ref024">24</xref>] considered a similar model within an information-theoretic framework and proposed extending it to nonlinear combinations of outputs from multiple quadratic functions. In a similar vein, Sharpee <italic>et al</italic>[<xref ref-type="bibr" rid="pcbi.1004141.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref046">46</xref>] used
<disp-formula id="pcbi.1004141.e084"><alternatives><graphic id="pcbi.1004141.e084g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e084"/><mml:math id="M84" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>+</mml:mo> <mml:mi>K</mml:mi> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi>C</mml:mi> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(45)</label></disp-formula>
This model corresponds to quadratic logistic regression, and thus assumes Bernoulli output noise (and a binary response). The authors also proposed a “nonlinear MID” in which the standard MID estimator is extended to by setting the firing rate to be a quadratic function of the form <italic>f</italic>(<bold>k</bold><sup>⊤</sup><bold>s</bold>+<bold>s</bold><sup>⊤</sup><italic>C</italic> <bold>s</bold>). This method is one-dimensional in a quadratic stimulus space (unlike multidimensional linear MID) and therefore avoids the curse of dimensionality. Other work has used independent component analysis to find directions in stimulus space in which the spike-triggered distribution has maximal deviations from Gaussianity [<xref ref-type="bibr" rid="pcbi.1004141.ref008">8</xref>].</p>
</sec>
</sec>
<sec id="sec021" sec-type="conclusions">
<title>Discussion</title>
<sec id="sec022">
<title>Distributional assumptions implicit in MID</title>
<p>We have studied the estimator known as maximally informative dimensions (MID), [<xref ref-type="bibr" rid="pcbi.1004141.ref018">18</xref>] a popular approach for estimating informative dimensions of stimulus space from spike-train data. Although the MID estimator was originally described in information-theoretic language, we have shown that, when used with plug-in estimators for information-theoretic quantities, it is mathematically identical to the maximum likelihood estimator for a linear-nonlinear-Poisson (LNP) encoding model. This equivalence holds irrespective of spike rate, the amount of data, or the size of time bins used to count spikes. We have shown that this follows from the fact that the plug-in estimate for single-spike information is equal (up to an additive constant) to the log-likelihood per spike of the data under an LNP model.</p>
<p>Estimators defined by the optima of information-theoretic functionals have attractive theoretical properties, including that they provide well-defined and (theoretically) distribution-agnostic characterizations of data. In practice, however, such agnosticism can be difficult to achieve, as the need to estimate information-theoretic quantities from data requires the choice of a particular estimator. MID has the virtue of using a non-parametric estimator for raw and spike-triggered stimulus densities, meaning that the number of parameters (i.e., the number of histogram bins) can grow flexibly with the amount of data. This allows it to converge for arbitrary densities, in the limit of infinite data. However, for a finite dataset, the choice of number of bins is critical for obtaining an accurate estimate. As we show in <xref ref-type="fig" rid="pcbi.1004141.g003">Fig. 3</xref>, a poor choice can lead to a systematic under- or over-estimate of the single-spike information, and in turn, a poor estimate of the most informative stimulus dimensions. Determining the number of histogram bins should therefore be considered a model selection problem, validated with a statistical procedure such as cross-validation.</p>
<p>A second kind of distributional assumption arises from MID’s reliance on single-spike information, which is tantamount to an assumption of Poisson spiking. To be clear, the single-spike information represents a valid information-theoretic quantity that does not explicitly assume any model. As noted in [<xref ref-type="bibr" rid="pcbi.1004141.ref026">26</xref>], it is simply the information carried by a single spike time, independent of all other spike times. However, conditionally independent spiking is also the fundamental assumption underlying the Poisson model and, as we have shown, the standard MID estimator (based on the KL-divergence between histograms) is mathematically identical to the maximum likelihood estimator for an LNP model with piece-wise constant nonlinearity. Thus, MID achieves no more and no less than a maximum likelihood estimator for a Poisson response model. As we illustrate in <xref ref-type="fig" rid="pcbi.1004141.g004">Fig. 4</xref>, MID does not maximize the mutual information between the projected stimulus and the spike response when the distribution of spikes conditioned on stimuli is not Poisson; it is an inconsistent estimator for the relevant stimulus subspace in such cases.</p>
<p>The distributional-dependence of MID should therefore be considered when interpreting its estimates of filters and nonlinearities. MID makes different, but not necessarily fewer, assumptions when compared to other LN estimators. For instance, although the maximum-likelihood estimator for a generalized linear model assumes a less-flexible model for the neural nonlinearity than does MID, it readily permits estimation of certain forms of spike-interdependence that MID neglects. In particular, MID-derived estimates are subject to concerns regarding model mismatch that arise whenever the true generative family is unknown [<xref ref-type="bibr" rid="pcbi.1004141.ref047">47</xref>].</p>
<p>In light of the danger that these distributional assumptions be obscured by the information-theoretic framing of MID, our belief is that the safer approach is to specify a model explicitly and adopt a likelihood-based estimation framework. Where the information theoretic and likelihood-based estimators are identical, nothing is lost by this approach. However, besides making assumptions explicit, the likelihood-based framework also readily facilitates the introduction of suitable priors for regularization of suitable priors, or hierarchical models [<xref ref-type="bibr" rid="pcbi.1004141.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref049">49</xref>], or more structured models of the type discussed here.</p>
</sec>
<sec id="sec023">
<title>Generalizations</title>
<p>Having clarified the relationship between MID and LNP model the, we introduced two generalizations designed to recover a maximally informative stimulus projection when neural response variability is not well described as Poisson. From a model-based perspective, the generalizations correspond to maximum likelihood estimators for a linear-nonlinear-Bernoulli (LNB) model (for binary spike counts), and the linear-nonlinear-Count (LNC) model (for arbitrary discrete spike counts). For both models, we obtained an equivalent relationship between log-likelihood and an estimate of mutual information between stimulus and response. This correspondence extends previous work that showed only approximate or asymptotic relationships between between information-theoretic and maximum-likelihood estimators [<xref ref-type="bibr" rid="pcbi.1004141.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref025">25</xref>]. The LNC model is the most general of the models we have considered. It requires the fewest assumptions, since it allows for arbitrary distributions over spike count given the stimulus. It includes both LNB and LNP as special cases (i.e., when the count distribution is Bernoulli or Poisson, respectively).</p>
<p>We could analogously define arbitrary “LNX” models, where <italic>X</italic> stands in for any probability distribution over the neural response (analog or discrete), and perform dimensionality reduction by maximizing likelihood for the filter parameters under this model. The log-likelihood under any such model can be associated with an information-theoretic quantity, analogous to single-spike, Bernoulli, and count information, using the difference of log-likelihoods (see also [<xref ref-type="bibr" rid="pcbi.1004141.ref035">35</xref>]):
<disp-formula id="pcbi.1004141.e085"><alternatives><graphic id="pcbi.1004141.e085g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e085"/><mml:math id="M85" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>≜</mml:mo> <mml:mspace width="0.277778em"/><mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>r</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>p</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>-</mml:mo> <mml:mspace width="0.277778em"/><mml:munder><mml:mo>∑</mml:mo> <mml:mi>r</mml:mi></mml:munder> <mml:msub><mml:mi>p</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(46)</label></disp-formula>
where <italic>p</italic><sub><italic>x</italic></sub>(<italic>r</italic>|<bold>s</bold>,<italic>θ</italic>) denotes the conditional response distribution associated with the LNX model with parameters <italic>θ</italic>, and <italic>p</italic><sub><italic>x</italic></sub>(<italic>r</italic>|<italic>θ</italic><sub>0</sub>) describes the marginal distribution over <italic>r</italic> under the stimulus distribution <italic>p</italic>(<bold>s</bold>). The empirical or plug-in estimate of this information is equal to the LNX model log-likelihood plus the estimated marginal entropy:
<disp-formula id="pcbi.1004141.e086"><alternatives><graphic id="pcbi.1004141.e086g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e086"/><mml:math id="M86" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac></mml:mstyle> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(47)</label></disp-formula>
where <italic>n</italic> denotes the number of samples and <italic>θ</italic><sub>0</sub> depends only on the marginal response distribution. The maximum likelihood estimate is therefore equally a maximal-information estimate.</p>
<p>Note that all of the dimensionality-reduction methods we have discussed treat neural responses as conditionally independent given the stimulus, meaning that they do not capture dependencies between spike counts in different time bins (e.g., due to refractoriness, bursting, adaptation, etc.). Spike-history dependencies can influence the single-bin spike count distribution—for example, a Bernoulli model is more accurate than a Poisson model when the bin size is smaller than or equal to the refractory period, since the Poisson model assigns positive probability to the event of having ≥ 2 two spikes in a single bin. The models we have considered can all be extended to capture spike history dependencies by augmenting the stimulus with a vector representation of spike history, as in both conditional renewal models and generalized linear models [<xref ref-type="bibr" rid="pcbi.1004141.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref050">50</xref>–<xref ref-type="bibr" rid="pcbi.1004141.ref052">52</xref>].</p>
<p>Lastly, we have shown that viewing MID from a model-based perspective provides insight into how to overcome practical limitations on the number of filters that can be estimated. Standard implementations of MID employ histogram-based density estimators for <italic>p</italic>(<italic>K</italic><sup>⊤</sup><bold>s</bold>) and <italic>p</italic>(<italic>K</italic><sup>⊤</sup><bold>s</bold>|<italic>spike</italic>). However, dimensionality and parameter count can be a crippling issue given limited data, and density estimation becomes intractable in dimensionalities &gt; 3. Furthermore, the dependence of the information on the logarithm of the ratio of these densities amplifies sensitivity to errors in these estimates. The LNP-likelihood view suggests direct estimation of the nonlinearity <italic>f</italic>, rather than of the densities. Such estimates are naturally more robust, and are more sensibly regularized based on expectations about neuronal responses without reference to any regularities in the stimulus distribution. We have proposed a flexible yet tractable form for the nonlinearity in terms of linear combinations of basis functions cascaded with a second <italic>output</italic> nonlinearity. This approach yielded a flexible, computationally efficient, constrained version of MID that is able to estimate high-dimensional feature spaces. It is also general in the sense that it encompasses standard MID, generalized linear and quadratic models, and other constrained models that scale tractably to high-dimensional subspaces. Future work might seek to extend this flexible likelihood-based approach further, for example by including priors over the weights with which basis functions are combined to improve regularization, or perhaps by adjusting hyperparameters in a hierarchical model as has been successful with linear approaches [<xref ref-type="bibr" rid="pcbi.1004141.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref049">49</xref>].</p>
<p>In recent years, the ability to successfully characterize low-dimensional neural feature spaces using MID has proved useful to address questions relating to multidimensional feature selectivity [<xref ref-type="bibr" rid="pcbi.1004141.ref053">53</xref>–<xref ref-type="bibr" rid="pcbi.1004141.ref056">56</xref>]. In all of these examples however, issues with dimensionality have prevented the estimation of feature spaces with more than two dimensions. The methods presented within this paper will help to overcome these issues, opening access to further important questions regarding the relationship between stimuli and their neural representation.</p>
</sec>
</sec>
<sec id="sec024" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec025">
<title>Bound on lost information under MID</title>
<p>Here we present a derivation of the lower bound on the fraction of total information carried by silences for a Bernoulli neuron, in the limit of rare spiking. For notational convenience, let <italic>ρ</italic> = <italic>p</italic>(<italic>r</italic> = 1) denote the marginal probability of a spike, so the probability of silence is <italic>p</italic>(<italic>r</italic> = 0) = 1−<italic>ρ</italic>. Let <italic>Q</italic><sub>1</sub> = <italic>p</italic>(<bold>s</bold>|<italic>r</italic> = 1) and <italic>Q</italic><sub>0</sub> = <italic>p</italic>(<bold>s</bold>|<italic>r</italic> = 0) denote the spike-triggered and silence-triggered stimulus distributions, respectively. Let <italic>P</italic><sub><bold>s</bold></sub> = <italic>p</italic>(<bold>s</bold>) denote the raw stimulus distribution. Note that we have the <italic>P</italic><sub><bold>s</bold></sub> = <italic>ρQ</italic><sub>1</sub>+(1−<italic>ρ</italic>)<italic>Q</italic><sub>0</sub>. The mutual information between the stimulus and one bin of the response (<xref ref-type="disp-formula" rid="pcbi.1004141.e031">Equation 18</xref>) can then be written
<disp-formula id="pcbi.1004141.e087"><alternatives><graphic id="pcbi.1004141.e087g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e087"/><mml:math id="M87" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>ρ</mml:mi> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>P</mml:mi> <mml:mi mathvariant="bold">s</mml:mi></mml:msub></mml:mfenced> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>P</mml:mi> <mml:mi mathvariant="bold">s</mml:mi></mml:msub></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(48)</label></disp-formula>
Note that this is a generalized form of the Jensen-Shannon (JS) divergence; the standard JS-divergence between <italic>Q</italic><sub>0</sub> and <italic>Q</italic><sub>1</sub> is obtained when <inline-formula id="pcbi.1004141.e088"><mml:math id="M88" display="inline" overflow="scroll"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p>
<p>In the limit of small <italic>ρ</italic> (i.e., the Poisson limit), the mutual information is dominated by the first (<italic>Q</italic><sub>1</sub>) term. Here we wish to show a bound on the fraction of information carried by the <italic>Q</italic><sub>0</sub> term. We can do this by computing a second-order Taylor expansion of <inline-formula id="pcbi.1004141.e089"><mml:math id="M89" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mrow><mml:mrow/><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic>I</italic>(<bold>s</bold>,<italic>r</italic>) around <italic>ρ</italic> = 0, and show that their ratio is bounded below by <italic>ρ</italic>/2. Expanding in <italic>ρ</italic>, we have
<disp-formula id="pcbi.1004141.e090"><alternatives><graphic id="pcbi.1004141.e090g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e090"/><mml:math id="M90" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Q</mml:mi> <mml:mi>o</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>P</mml:mi> <mml:mi mathvariant="bold">s</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mstyle> <mml:msup><mml:mi>ρ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>ρ</mml:mi> <mml:mn>3</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>and</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(49)</label></disp-formula> <disp-formula id="pcbi.1004141.e091"><alternatives><graphic id="pcbi.1004141.e091g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e091"/><mml:math id="M91" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>ρ</mml:mi> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfenced> <mml:mo>-</mml:mo> <mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mstyle> <mml:msup><mml:mi>ρ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>ρ</mml:mi> <mml:mn>3</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(50)</label></disp-formula>
where
<disp-formula id="pcbi.1004141.e092"><alternatives><graphic id="pcbi.1004141.e092g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e092"/><mml:math id="M92" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>Ω</mml:mi></mml:msub> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfrac> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(51)</label></disp-formula>
which is a an upper bound on the KL-divergence: <inline-formula id="pcbi.1004141.e093"><mml:math id="M93" display="inline" overflow="scroll"><mml:mrow><mml:mi>V</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mo>≥</mml:mo> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.167em"/></mml:mrow></mml:msub> <mml:mo minsize="1.6" maxsize="1.6" stretchy="true">(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mspace width="0.167em"/><mml:mo minsize="1.6" maxsize="1.6" stretchy="true">|</mml:mo> <mml:mspace width="-0.167em"/><mml:mo minsize="1.6" maxsize="1.6" stretchy="true">|</mml:mo> <mml:mspace width="0.167em"/><mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo minsize="1.6" maxsize="1.6" stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>, since (<italic>z</italic>−1) ≥ log(<italic>z</italic>). We therefore have
<disp-formula id="pcbi.1004141.e094"><alternatives><graphic id="pcbi.1004141.e094g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e094"/><mml:math id="M94" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Q</mml:mi> <mml:mi>o</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>P</mml:mi> <mml:mi mathvariant="bold">s</mml:mi></mml:msub></mml:mfenced></mml:mrow> <mml:mrow><mml:mi>I</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mi>ρ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>ρ</mml:mi> <mml:mn>3</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>ρ</mml:mi> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfenced> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mi>ρ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>ρ</mml:mi> <mml:mn>3</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>≥</mml:mo> <mml:mfrac><mml:mrow><mml:mi>ρ</mml:mi> <mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfenced></mml:mrow></mml:mfrac> <mml:mo>≥</mml:mo> <mml:mfrac><mml:mi>ρ</mml:mi> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(52)</label></disp-formula>
in the limit <italic>ρ</italic> → 0.</p>
<p>We conjecture that the bound holds for all values of <italic>ρ</italic>. For the case of <inline-formula id="pcbi.1004141.e095"><mml:math id="M95" display="inline" overflow="scroll"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>, this corresponds to an assertion about the relative contribution of each of the two terms in the JS divergence, that is:
<disp-formula id="pcbi.1004141.e096"><alternatives><graphic id="pcbi.1004141.e096g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e096"/><mml:math id="M96" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow> <mml:mrow><mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>K</mml:mi> <mml:mi>L</mml:mi> <mml:mspace width="-0.166667em"/></mml:mrow></mml:msub> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="-0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac> <mml:mo>≥</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(53)</label></disp-formula>
for any choice of distributions <italic>Q</italic><sub>0</sub> and <italic>Q</italic><sub>1</sub>. We have been unable to find any counter-examples to this (or to the more general conjecture), but have so far been unable to find a general proof.</p>
</sec>
<sec id="sec026">
<title>Single-spike information and Poisson log-likelihood</title>
<p>An important general corollary to the equivalence between MID and an LNP maximum likelihood estimate is that the standard single-spike information estimate <italic>Î</italic><sub><italic>ss</italic></sub> based on a PSTH measured in response to repeated stimuli is also a Poisson log-likelihood per spike (plus a constant). Specifically, the empirical single-spike information is equal to the log-likelihood ratio between an inhomogeneous and homogeneous Poisson model of the repeat data (normalized by spike count):
<disp-formula id="pcbi.1004141.e097"><alternatives><graphic id="pcbi.1004141.e097g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e097"/><mml:math id="M97" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle> <mml:mfenced separators="" open="(" close=")"><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>L</mml:mi></mml:mrow></mml:msub> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mo>ℒ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold">λ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(54)</label></disp-formula>
where <inline-formula id="pcbi.1004141.e098"><mml:math id="M98" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the maximum-likelihood or plug-in estimate of the time-varying spike rate (i.e., the PSTH itself), <inline-formula id="pcbi.1004141.e099"><mml:math id="M99" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> is the mean spike rate across time, and ℒ(<bold>λ</bold>;<bold>r</bold>) denotes the log-likelihood of the repeat data <bold>r</bold> under a Poisson model with time-varying rate <bold>λ</bold>.</p>
<p>We can derive this equivalence as follows. Let {<italic>r</italic><sub><italic>jt</italic></sub>} denote spike counts collected during a “frozen noise” experiment, with repeat index <italic>j</italic> ∈ {1,…,<italic>n</italic><sub><italic>rpt</italic></sub>} and index <italic>t</italic> ∈ {1,…,<italic>n</italic><sub><italic>t</italic></sub>} over time bins of width Δ. Then <italic>T</italic> = <italic>n</italic><sub><italic>t</italic></sub>Δ is the duration of the stimulus and <italic>N</italic> = <italic>n</italic><sub><italic>t</italic></sub> <italic>n</italic><sub><italic>rpt</italic></sub> is the total number of time bins in the entire experiment. The single-spike information can be estimated with a discrete version of the formula for single-spike information provided in [<xref ref-type="bibr" rid="pcbi.1004141.ref026">26</xref>] (see eq. 2.5):
<disp-formula id="pcbi.1004141.e100"><alternatives><graphic id="pcbi.1004141.e100g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e100"/><mml:math id="M100" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:munderover> <mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(55)</label></disp-formula>
where <inline-formula id="pcbi.1004141.e101"><mml:math id="M101" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mo stretchy="false">(</mml:mo> <mml:mi>t</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:msubsup> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is an estimate of the spike rate in the <italic>t</italic>’th time bin in response to the stimulus sequence <italic>s</italic>, and <inline-formula id="pcbi.1004141.e102"><mml:math id="M102" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>‾</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:msubsup> <mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mo stretchy="false">(</mml:mo> <mml:mi>t</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo> <mml:mo>/</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the mean spike rate across the experiment. Note that this formulation assumes (as in [<xref ref-type="bibr" rid="pcbi.1004141.ref026">26</xref>]) that <italic>T</italic> is long enough that an average over stimulus sequences is well approximated by the average across time.</p>
<p>The plug-in (ML) estimator for spike rate can be read off from the peri-stimulus time histogram (PSTH). It results from averaging the response across repeats for each time bin:
<disp-formula id="pcbi.1004141.e103"><alternatives><graphic id="pcbi.1004141.e103g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e103"/><mml:math id="M103" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>Δ</mml:mo></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:munderover> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(56)</label></disp-formula>
Clearly, <inline-formula id="pcbi.1004141.e104"><mml:math id="M104" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>‾</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>Δ</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <italic>n</italic><sub><italic>sp</italic></sub> = ∑<sub><italic>j</italic>,<italic>t</italic></sub> <italic>r</italic><sub><italic>jt</italic></sub> is the total spike count. This allows us to rewrite single-spike information (<xref ref-type="disp-formula" rid="pcbi.1004141.e100">Equation 55</xref>) as:
<disp-formula id="pcbi.1004141.e105"><alternatives><graphic id="pcbi.1004141.e105g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e105"/><mml:math id="M105" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>Δ</mml:mo></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:munderover> <mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>Δ</mml:mo></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(57)</label></disp-formula></p>
<p>Now, consider the Poisson log-likelihood ℒ evaluated at the ML estimate <inline-formula id="pcbi.1004141.e106"><mml:math id="M106" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, i.e., the conditional probability of the response data <bold>r</bold> = {<italic>r</italic><sub><italic>jt</italic></sub>} given rate vector <inline-formula id="pcbi.1004141.e107"><mml:math id="M107" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>. This is given by:
<disp-formula id="pcbi.1004141.e108"><alternatives><graphic id="pcbi.1004141.e108g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e108"/><mml:math id="M108" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>ℒ</mml:mo> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold">λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:munderover> <mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mfenced separators="" open="(" close=")"><mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo></mml:mfenced> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo> <mml:mo>-</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>!</mml:mo></mml:mfenced></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:munderover> <mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:munderover> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfenced> <mml:mo form="prefix">log</mml:mo> <mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>!</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>Δ</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:munderover> <mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mover accent="true"><mml:mi>λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>!</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mi>N</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>!</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mo>ℒ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold">λ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(58)</label></disp-formula>
which is identical to relationship between single-spike information and Poisson log-likelihood expressed in <xref ref-type="disp-formula" rid="pcbi.1004141.e023">Equation 13</xref>. Thus, even when estimated from raster data, <italic>I</italic><sub><italic>ss</italic></sub> is equal to the difference between Poisson log-likelihoods under an inhomogeneous (rate-varying) and a homogeneous (constant rate) Poisson model, divided by spike count (see also [<xref ref-type="bibr" rid="pcbi.1004141.ref057">57</xref>]). These normalized log-likelihoods can be conceived as entropy estimates, with <inline-formula id="pcbi.1004141.e109"><mml:math id="M109" display="inline" overflow="scroll"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>ℒ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> providing an estimate for prior entropy, measuring the prior uncertainty about spike times given the mean rate, and <inline-formula id="pcbi.1004141.e110"><mml:math id="M110" display="inline" overflow="scroll"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>ℒ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> corresponding to posterior entropy, measuring the posterior uncertainty once we know the time-varying spike rate.</p>
<p>A similar quantity has been used to report the cross-validation performance of conditionally Poisson models, including the GLM [<xref ref-type="bibr" rid="pcbi.1004141.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1004141.ref058">58</xref>]. To penalize over-fitting, the empirical single-spike information is evaluated using the rate estimate <inline-formula id="pcbi.1004141.e111"><mml:math id="M111" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> obtained with parameters fit to training data and responses <bold>r</bold> from unseen test data. This results in the “cross-validated” single-spike information:
<disp-formula id="pcbi.1004141.e112"><alternatives><graphic id="pcbi.1004141.e112g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e112"/><mml:math id="M112" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>x</mml:mi> <mml:mi>v</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mfrac> <mml:mfenced separators="" open="(" close=")"><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">λ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>t</mml:mi> <mml:mi>r</mml:mi> <mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mo>ℒ</mml:mo> <mml:mo>(</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">λ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(59)</label></disp-formula>
This can be interpreted as the predictive information (in bits-per-spike) that the model captures about test data, above and beyond that captured by a homogeneous Poisson model with correct mean rate. Note that this quantity can be negative in cases of extremely poor model fit, that is, when the model prediction on test data is worse than of the best constant-spike-rate Poisson model. Cross-validated single-spike information provides a useful measure for comparing models with different numbers of parameters (e.g., a 1-filter vs. 2-filter LNP model), since units of “bits” are more interpretable than raw log-likelihood of test data. Generally, <inline-formula id="pcbi.1004141.e113"><mml:math id="M113" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>̂</mml:mo></mml:mover> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow> <mml:mrow><mml:mo stretchy="false">[</mml:mo> <mml:mi>x</mml:mi> <mml:mi>v</mml:mi> <mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> can be considered to a lower bound on the model’s true predictive power, due to stochasticity in both training and test data. By contrast, the empirical <italic>I</italic><sub><italic>ss</italic></sub> evaluated on training data tends to over-estimate information due to over-fitting.</p>
</sec>
<sec id="sec027">
<title>Computation of model-based information quantities</title>
<p>To gain intuition for the different information measures we have considered (Poisson, Bernoulli, and categorical or “count”), it is useful to consider how they differ for a simple idealized example. Consider a world with two stimuli, ‘<italic>A</italic>’ and ‘<italic>B</italic>’, and two possible discrete stimulus sequences, <italic>s</italic><sub>1</sub> = <italic>AB</italic> and <italic>s</italic><sub>2</sub> = <italic>BA</italic>, each of which occurs with equal probability, so <italic>p</italic>(<italic>s</italic><sub>1</sub>) = <italic>p</italic>(<italic>s</italic><sub>2</sub>) = 0.5. Assume each sequence lasts <italic>T</italic> = 2s, so the natural time bin size for considering the spike response is Δ = 1s. Suppose that stimulus <italic>A</italic> always elicits 3 spikes, while <italic>B</italic> always elicits 1 spike. Thus, when sequence <italic>s</italic><sub>1</sub> is presented, we observe 3 spikes in the first time interval and 1 spike in the second interval; when <italic>s</italic><sub>2</sub> is presented, we observe 1 spike in the first time interval and 3 spikes in the second.</p>
<p>Single-spike information can be computed exactly from <italic>λ</italic><sub>1</sub>(<italic>t</italic>) and <italic>λ</italic><sub>2</sub>(<italic>t</italic>), the spike rate in response to stimulus sequence <italic>s</italic><sub>1</sub> and <italic>s</italic><sub>2</sub>, respectively. For this example, <italic>λ</italic><sub>1</sub>(<italic>t</italic>), takes the value 3 during (0,1] and 1 during (1,2], while <italic>λ</italic><sub>2</sub>(<italic>t</italic>) takes values 1 and 3 during the corresponding intervals. The mean spike rate for both stimuli is <italic>λ̄</italic> = 2 sp/s. Plugging these into <xref ref-type="disp-formula" rid="pcbi.1004141.e097">Equation 54</xref> gives single-spike information of <italic>I</italic><sub><italic>ss</italic></sub> = 0.19 bits/spike. This result is slightly easier to grasp using an equivalent definition of single-spike information as the mutual information between the stimulus <italic>s</italic> and a single spike time <italic>τ</italic> (see [<xref ref-type="bibr" rid="pcbi.1004141.ref026">26</xref>]). If one were told that a spike, sampled at random from the four spikes present during every trial, occurred during [0, 1], then the posterior <italic>p</italic>(<italic>s</italic>|<italic>τ</italic> = 1) attaches 3/4 probability to <italic>s</italic> = <italic>s</italic><sub>1</sub> and 1/4 to <italic>s</italic> = <italic>s</italic><sub>2</sub>. The posterior entropy is therefore −0.25 log 0.25−0.75 log 0.75 = 0.81 bits. We obtain the same entropy if the spike occurs in the second interval, so <italic>H</italic>(<italic>s</italic>|<italic>τ</italic>) = 0.81. The prior entropy is <italic>H</italic>(<italic>s</italic>) = 1 bit, so once again we have <italic>I</italic><sub><italic>ss</italic></sub> = 1−0.81 = 0.19 bits/spike.</p>
<p>The Bernoulli information, by contrast, is undefined, since <italic>r</italic> takes values outside the set {0,1}, and therefore cannot have a Bernoulli distribution. To make Bernoulli information well defined, we would need to either truncate spike counts above 1 (<italic>e.g.</italic>, [<xref ref-type="bibr" rid="pcbi.1004141.ref059">59</xref>]), or else use smaller bin size so that no bin contains more than one spike. In the latter case, we would need to provide more information about the distribution of spike times within these finer bins. If, for example, the three spikes elicited by <italic>A</italic> are evenly spaced within the interval and we use bins equal to 1/3<italic>s</italic>, then the Bernoulli information will clearly exceed single-spike information, since the time of a no-spike response (<italic>r</italic> = 0, a term neglected by single-spike information) provides perfect information about the stimulus, since it occurs only in response to <italic>B</italic>.</p>
<p>Lastly, the count information is easy to compute from the fact that count <italic>r</italic> carries perfect information about the stimulus, so the mutual information between stimulus (<italic>A</italic> or <italic>B</italic>) and <italic>r</italic> is 1 bit. We defined <italic>I</italic><sub><italic>count</italic></sub> to be the mutual information normalized by the mean spike count per bin (<xref ref-type="disp-formula" rid="pcbi.1004141.e065">Equation 35</xref>). Thus, <italic>I</italic><sub><italic>count</italic></sub> = 0.5 bits/spike, which is more than double the single-spike information.</p>
</sec>
<sec id="sec028">
<title>Gradient and Hessian of LNP log-likelihood</title>
<p>Here we provide formulas useful for fitting the the many-filter LNP model with cylindrical basis function (CBF) nonlinearity. We performed joint optimization of filter parameters <italic>K</italic> and basis function weights {<italic>α</italic><sub><italic>i</italic></sub>} using MATLAB’s <monospace>fminunc</monospace> function. We found this approach to converge much more rapidly than alternating coordinate ascent. We used analytically computed gradient and Hessian of the joint-likelihood to speed up performance, which we provide here.</p>
<p>Given a dataset <inline-formula id="pcbi.1004141.e114"><mml:math id="M114" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, define <bold>r</bold> = (<italic>r</italic><sub>1</sub>,…,<italic>r</italic><sub><italic>n</italic><sub><italic>t</italic></sub></sub>)<sup>⊤</sup> and <bold>λ</bold> = (<italic>f</italic>(<italic>K</italic><sup>⊤</sup><bold>s</bold><sub>1</sub>),…,<italic>f</italic>(<italic>K</italic><sup>⊤</sup><bold>s</bold><sub><italic>n</italic><sub><italic>t</italic></sub></sub>))<sup>⊤</sup>, where nonlinearity <italic>f</italic> = <italic>g</italic>(∑<italic>α</italic><sub><italic>i</italic></sub> <italic>φ</italic><sub><italic>i</italic></sub>) depends on basis function <bold>Φ</bold> = {<italic>φ</italic><sub><italic>i</italic></sub>} and weights <bold>α</bold> = {<italic>α</italic><sub><italic>i</italic></sub>} (<xref ref-type="disp-formula" rid="pcbi.1004141.e075">Equation 39</xref>). We can write the log-likelihood for the many-filter LNP model (from Equations <xref ref-type="disp-formula" rid="pcbi.1004141.e074">38</xref>–<xref ref-type="disp-formula" rid="pcbi.1004141.e076">40</xref>) as:
<disp-formula id="pcbi.1004141.e115"><alternatives><graphic id="pcbi.1004141.e115g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e115"/><mml:math id="M115" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>ℒ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow> <mml:mi>⊤</mml:mi></mml:msup> <mml:mo form="prefix">log</mml:mo> <mml:mi mathvariant="bold">λ</mml:mi> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mn mathvariant="bold">1</mml:mn> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi mathvariant="bold">λ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(60)</label></disp-formula>
where <italic>θ</italic> = {<italic>K</italic>,<bold>α</bold>} are the model parameters, Δ is the time bin size, and <bold>1</bold> denotes a vector of ones. The first and second derivatives of the log-likelihood are given by
<disp-formula id="pcbi.1004141.e116"><alternatives><graphic id="pcbi.1004141.e116g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e116"/><mml:math id="M116" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mo>ℒ</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced> <mml:mi>⊤</mml:mi></mml:msup> <mml:mspace width="0.277778em"/><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mi mathvariant="bold">r</mml:mi> <mml:mi mathvariant="bold">λ</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(61)</label></disp-formula> <disp-formula id="pcbi.1004141.e117"><alternatives><graphic id="pcbi.1004141.e117g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e117"/><mml:math id="M117" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:msup><mml:mi>∂</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>ℒ</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mrow><mml:msup><mml:mi>∂</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced> <mml:mi>⊤</mml:mi></mml:msup> <mml:mspace width="0.277778em"/><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mi mathvariant="bold">r</mml:mi> <mml:mi mathvariant="bold">λ</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mn>1</mml:mn></mml:mfenced> <mml:mo>+</mml:mo> <mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced> <mml:mi>⊤</mml:mi></mml:msup> <mml:mspace width="0.277778em"/><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mi mathvariant="bold">r</mml:mi> <mml:msup><mml:mrow><mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(62)</label></disp-formula>
where multiplication, division, and exponentiation operations on vector quantities indicate component-wise operations.</p>
<p>Let <bold>k</bold><sub>1</sub>,…,<bold>k</bold><sub><italic>m</italic></sub> denote the linear filters, i.e., the <italic>m</italic> columns of <italic>K</italic>. Then the required gradients of <bold>λ</bold> with respect to the model parameters can be written:
<disp-formula id="pcbi.1004141.e118"><alternatives><graphic id="pcbi.1004141.e118g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e118"/><mml:math id="M118" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mo>'</mml:mo></mml:msup> <mml:mo>∘</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mi>α</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(63)</label></disp-formula> <disp-formula id="pcbi.1004141.e119"><alternatives><graphic id="pcbi.1004141.e119g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e119"/><mml:math id="M119" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mo>Φ</mml:mo> <mml:mi>⊤</mml:mi></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mo>'</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(64)</label></disp-formula>
where <italic>S</italic> denotes the (<italic>n</italic><sub><italic>t</italic></sub>×<italic>D</italic>) stimulus design matrix, Φ denotes the (<italic>n</italic><sub><italic>t</italic></sub>×<italic>n</italic><sub><italic>φ</italic></sub>) matrix whose (<italic>t</italic>,<italic>j</italic>)’th entry is <italic>φ</italic><sub><italic>j</italic></sub>(<italic>K</italic><sup>⊤</sup><bold>s</bold><sub><italic>t</italic></sub>), and Φ<sup>(<italic>i</italic>)</sup> denotes a matrix of the same size, formed by the point-wise derivative of Φ with respect to its <italic>i</italic>’th input component, evaluated at each projected stimulus <italic>K</italic><sup>⊤</sup><bold>s</bold><sub><italic>t</italic></sub>. Finally, <bold>λ</bold><sup>′</sup> = <italic>g</italic><sup>′</sup>(Φ<bold>α</bold>) is a (<italic>n</italic><sub><italic>t</italic></sub>×1) vector composed of the point-wise derivatives of the inverse-link function <italic>g</italic> at its input, and ‘∘’ denotes Hadamard or component-wise vector product.</p>
<p>Lastly, second derivative blocks, which can be plugged into <xref ref-type="disp-formula" rid="pcbi.1004141.e117">Equation 62</xref> to form the Hessian, are given by
<disp-formula id="pcbi.1004141.e120"><alternatives><graphic id="pcbi.1004141.e120g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e120"/><mml:math id="M120" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:msup><mml:mi>∂</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi>diag</mml:mi> <mml:mfenced separators="" open="(" close=")"><mml:mfenced separators="" open="[" close="]"><mml:msup><mml:mrow><mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mo>'</mml:mo> <mml:mo>'</mml:mo></mml:mrow></mml:msup> <mml:mo>∘</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mi>α</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∘</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mi>α</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo> <mml:mfenced separators="" open="[" close="]"><mml:msup><mml:mrow><mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mo>'</mml:mo></mml:msup> <mml:mo>∘</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mi>α</mml:mi></mml:mfenced></mml:mfenced> <mml:mi>S</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(65)</label></disp-formula> <disp-formula id="pcbi.1004141.e121"><alternatives><graphic id="pcbi.1004141.e121g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e121"/><mml:math id="M121" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:msup><mml:mi>∂</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:msup><mml:mrow><mml:mi>α</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mo>Φ</mml:mo> <mml:mi>⊤</mml:mi></mml:msup> <mml:mi>diag</mml:mi> <mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mo>'</mml:mo> <mml:mo>'</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>Φ</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(66)</label></disp-formula> <disp-formula id="pcbi.1004141.e122"><alternatives><graphic id="pcbi.1004141.e122g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004141.e122"/><mml:math id="M122" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:msup><mml:mi>∂</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mi>∂</mml:mi> <mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mi>⊤</mml:mi></mml:msup> <mml:mfenced separators="" open="(" close=")"><mml:mi>diag</mml:mi> <mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mrow><mml:mo>'</mml:mo> <mml:mo>'</mml:mo></mml:mrow></mml:msup> <mml:mo>∘</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mi>α</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>Φ</mml:mo> <mml:mo>+</mml:mo> <mml:mi>diag</mml:mi> <mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mi mathvariant="bold">λ</mml:mi></mml:mrow> <mml:mo>'</mml:mo></mml:msup></mml:mfenced> <mml:msup><mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(67)</label></disp-formula>
where <bold>λ</bold><sup>′′</sup> = <italic>g</italic><sup>′′</sup>(Φ<bold>α</bold>) and Φ<sup>(<italic>i</italic>,<italic>j</italic>)</sup> is a matrix of point-wise second-derivatives of Φ with respect to <italic>i</italic>’th and <italic>j</italic>’th inputs, evaluated for each projected stimulus <italic>K</italic><sup>⊤</sup><bold>s</bold><sub><italic>t</italic></sub>.</p>
</sec>
<sec id="sec029">
<title>V1 data analysis</title>
<p>To examine performance in recovering high-dimensional subspaces, we analyzed data from macaque V1 cells, driven by 1D binary white noise “flickering bars” stimulus, presented at a frame rate of 100 Hz (data published in [<xref ref-type="bibr" rid="pcbi.1004141.ref029">29</xref>]). The spatiotemporal stimulus had between 8 and 32 spatial bars and we considered 10 time bins for the temporal integration window. This made for a stimulus space with dimensionality ranging from 80 to 320.</p>
<p>The cbf-LNP model was implemented with a cylindrical basis function (CBF) nonlinearity using three first-order CBFs per filter. For a <italic>k</italic>-filter model, this resulted in 3<italic>k</italic> parameters for the nonlinearity, and (240+3)<italic>k</italic> parameters in total for a stimulus with 24 bars.</p>
<p>The traditional MID estimator (rbf-LNP) was implemented using radial basis functions (RBFs) to represent the nonlinearity. Unlike the histogram-based parametrization discussed in the manuscript (which produces a piece-wise constant nonlinearity), this results in a smooth nonlinearity and, more importantly, a smooth log-likelihood with tractable analytic gradients. We defined a grid of RBFs with three grid points per dimension, so that CBF and RBF models were identical for a 1-filter model. For a <italic>k</italic>-filter model, this resulted in 3<sup><italic>k</italic></sup> parameters for the nonlinearity, and 240<italic>k</italic>+3<sup><italic>k</italic></sup> parameters in total.</p>
<p>For both models, the basis function responses were combined linearly and transformed by a “soft-rectification” function: <italic>g</italic>(⋅) = log(1+exp(⋅)), to ensure positive spike rates. We also evaluated the performance of an exponential function, <italic>g</italic>(⋅) = exp(⋅), which yielded slightly worse performance (reducing single-spike information by ∼ 0.02 bits/spike).</p>
<p>The cbf- and rbf-LNP models were both fit by maximizing the likelihood for the model parameters <italic>θ</italic> = {<italic>K</italic>,<bold>α</bold>}. Both models were fit incrementally, with the <italic>N</italic>+1 dimensional model being initialized with the parameters of the <italic>N</italic> dimensional model, plus one additional filter (initialized with the iSTAC filter that provided the greatest increase in log-likelihood). The joint likelihood in <italic>K</italic> and <bold>α</bold> was ascended using MATLAB’s <monospace>fminunc</monospace> optimization function, which exploits analytic gradients and Hessians. The models were fit to 80% of the data, with the remaining 20% used for validation.</p>
<p>In order to calculate information contributed by excitatory filters under the cbf-LNP model (<xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8F</xref>), we removed each filter from the model and refit the nonlinearity (using the training data) using just the other filters. We quantified the information contributed by each filter as the difference between log-likelihood of the full model and log-likelihood of the reduced model (on test data). We sorted the filters by informativeness and computed the cumulative sum of information loss to obtain the trace shown in (<xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8F</xref>).</p>
<p>Measurements of computation time (<xref ref-type="fig" rid="pcbi.1004141.g008">Fig. 8D</xref>) were averaged over 100 repetitions using different random seeds. For each cell, four segments of activity were chosen randomly with fixed lengths of 5, 10, 20 and 30 minutes, which contained between about 22000 and 173000 spikes. Even with 30 minutes of data, 8 filters could be identified within about 4 hours on a desktop computer, making the approach tractable even for large numbers of filters.</p>
<p>Code will be provided at <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://pillowlab.princeton.edu/code.html">http://pillowlab.princeton.edu/code.html</ext-link>.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank J. M. Beck and P. E. Latham for insightful discussions and providing scientific input during the course of this project. We thank M. Day, B. Dichter, D. Goodman, W. Guo, and L. Meshulam for providing comments on an early version of this manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004141.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>de Ruyter van Steveninck</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name> (<year>1988</year>) <article-title>Real-time performance of a movement-senstivive neuron in the blowfly visual system: coding and information transmission in short spike sequences</article-title>. <source>Proceedings of the Royal Society B</source> <volume>234</volume>: <fpage>379</fpage>–<lpage>414</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.1988.0055" xlink:type="simple">10.1098/rspb.1988.0055</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Aguera y Arcas</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name> (<year>2003</year>) <article-title>What causes a neuron to spike?</article-title> <source>Neural Computation</source> <volume>15</volume>: <fpage>1789</fpage>–<lpage>1807</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/08997660360675044" xlink:type="simple">10.1162/08997660360675044</ext-link></comment> <object-id pub-id-type="pmid">14511513</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Aguera y Arcas</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name> (<year>2003</year>) <article-title>Computation in a single neuron: Hodgkin and Huxley revisited</article-title>. <source>Neural Computation</source> <volume>15</volume>: <fpage>1715</fpage>–<lpage>1749</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/08997660360675017" xlink:type="simple">10.1162/08997660360675017</ext-link></comment> <object-id pub-id-type="pmid">14511510</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name> (<year>2004</year>) <chapter-title>Characterization of neural responses with stochastic stimuli</chapter-title>. In: <name name-style="western"><surname>Gazzaniga</surname> <given-names>M</given-names></name>, editor, <source>The Cognitive Neurosciences, III</source>, <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <volume>chapter 23</volume> pp. <fpage>327</fpage>–<lpage>338</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="other">Bialek W, de Ruyter van Steveninck RR (2005). Features and dimensions: Motion estimation in fly vision. arXiv:q-bio.NC/0505003.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chichilnisky</surname> <given-names>EJ</given-names></name> (<year>2001</year>) <article-title>A simple white noise analysis of neuronal light responses</article-title>. <source>Network: Computation in Neural Systems</source> <volume>12</volume>: <fpage>199</fpage>–<lpage>213</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/net.12.2.199.213" xlink:type="simple">10.1080/net.12.2.199.213</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name> (<year>2006</year>) <article-title>Spike-triggered neural characterization</article-title>. <source>Journal of Vision</source> <volume>6</volume>: <fpage>484</fpage>–<lpage>507</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/6.4.13" xlink:type="simple">10.1167/6.4.13</ext-link></comment> <object-id pub-id-type="pmid">16889482</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Saleem</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Krapp</surname> <given-names>HG</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>SR</given-names></name> (<year>2008</year>) <article-title>Receptive field characterization by spike-triggered independent component analysis</article-title>. <source>Journal of Vision</source> <volume>8</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/8.13.2" xlink:type="simple">10.1167/8.13.2</ext-link></comment> <object-id pub-id-type="pmid">19146332</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brillinger</surname> <given-names>DR</given-names></name> (<year>1988</year>) <article-title>Maximum likelihood analysis of spike trains of interacting nerve cells</article-title>. <source>Biological Cybernetics</source> <volume>59</volume>: <fpage>189</fpage>–<lpage>200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00318010" xlink:type="simple">10.1007/BF00318010</ext-link></comment> <object-id pub-id-type="pmid">3179344</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Ventura</surname> <given-names>V</given-names></name> (<year>2001</year>) <article-title>A spike-train probability model</article-title>. <source>Neural Computation</source> <volume>13</volume>: <fpage>1713</fpage>–<lpage>1720</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/08997660152469314" xlink:type="simple">10.1162/08997660152469314</ext-link></comment> <object-id pub-id-type="pmid">11506667</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name> (<year>2004</year>) <article-title>Maximum likelihood estimation of cascade point-process neural encoding models</article-title>. <source>Network: Computation in Neural Systems</source> <volume>15</volume>: <fpage>243</fpage>–<lpage>262</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0954-898X/15/4/002" xlink:type="simple">10.1088/0954-898X/15/4/002</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Truccolo</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Eden</surname> <given-names>UT</given-names></name>, <name name-style="western"><surname>Fellows</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Donoghue</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name> (<year>2005</year>) <article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble and extrinsic covariate effects</article-title>. <source>Journal of Neurophysiology</source> <volume>93</volume>: <fpage>1074</fpage>–<lpage>1089</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00697.2004" xlink:type="simple">10.1152/jn.00697.2004</ext-link></comment> <object-id pub-id-type="pmid">15356183</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>AM</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Spatio-temporal correlations and visual signaling in a complete neuronal population</article-title>. <source>Nature</source> <volume>454</volume>: <fpage>995</fpage>–<lpage>999</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature07140" xlink:type="simple">10.1038/nature07140</ext-link></comment> <object-id pub-id-type="pmid">18650810</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Park</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name> (<year>2011</year>) <chapter-title>Bayesian spike-triggered covariance analysis</chapter-title>. In: <name name-style="western"><surname>Shawe-Taylor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zemel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bartlett</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pereira</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>K</given-names></name>, editors, <source>Advances in Neural Information Processing Systems 24</source>. <publisher-name>Curran Associates, Inc.</publisher-name> pp. <fpage>1692</fpage>–<lpage>1700</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>McFarland</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Cui</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Butts</surname> <given-names>DA</given-names></name> (<year>2013</year>) <article-title>Inferring nonlinear neuronal computation based on physiologically plausible inputs</article-title>. <source>PLoS Computational Biology</source> <volume>9</volume>: <fpage>e1003143</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003143" xlink:type="simple">10.1371/journal.pcbi.1003143</ext-link></comment> <object-id pub-id-type="pmid">23874185</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cui</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>LD</given-names></name>, <name name-style="western"><surname>Khawaja</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Pack</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Butts</surname> <given-names>DA</given-names></name> (<year>2013</year>) <article-title>Diverse suppressive influences in area MT and selectivity to complex motion features</article-title>. <source>Journal of Neuroscience</source> <volume>33</volume>: <fpage>16715</fpage>–<lpage>16728</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0203-13.2013" xlink:type="simple">10.1523/JNEUROSCI.0203-13.2013</ext-link></comment> <object-id pub-id-type="pmid">24133273</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name> (<year>2003</year>) <article-title>Convergence properties of some spike-triggered analysis techniques</article-title>. <source>Network: Computation in Neural Systems</source> <volume>14</volume>: <fpage>437</fpage>–<lpage>464</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0954-898X/14/3/304" xlink:type="simple">10.1088/0954-898X/14/3/304</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sharpee</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name> (<year>2004</year>) <article-title>Analyzing neural responses to natural signals: maximally informative dimensions</article-title>. <source>Neural Computation</source> <volume>16</volume>: <fpage>223</fpage>–<lpage>250</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976604322742010" xlink:type="simple">10.1162/089976604322742010</ext-link></comment> <object-id pub-id-type="pmid">15006095</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name> (<year>2006</year>) <article-title>Dimensionality reduction in neural models: An information-theoretic generalization of spike-triggered average and covariance analysis</article-title>. <source>Journal of Vision</source> <volume>6</volume>: <fpage>414</fpage>–<lpage>428</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/6.4.9" xlink:type="simple">10.1167/6.4.9</ext-link></comment> <object-id pub-id-type="pmid">16889478</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kouh</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name> (<year>2009</year>) <article-title>Estimating linear-nonlinear models using renyi divergences</article-title>. <source>Network: Computation in Neural Systems</source> <volume>20</volume>: <fpage>49</fpage>–<lpage>68</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/09548980902950891" xlink:type="simple">10.1080/09548980902950891</ext-link></comment> <object-id pub-id-type="pmid">19568981</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fitzgerald</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Rowekamp</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Sincich</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name> (<year>2011</year>) <article-title>Second order dimensionality reduction using minimum and maximum mutual information models</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002249</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002249" xlink:type="simple">10.1371/journal.pcbi.1002249</ext-link></comment> <object-id pub-id-type="pmid">22046122</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rajan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name> (<year>2013</year>) <article-title>Maximally informative stimulus energies in the analysis of neural responses to natural signals</article-title>. <source>PLoS ONE</source> <volume>8</volume>: <fpage>e71959</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0071959" xlink:type="simple">10.1371/journal.pone.0071959</ext-link></comment> <object-id pub-id-type="pmid">24250780</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Park</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Archer</surname> <given-names>EW</given-names></name>, <name name-style="western"><surname>Priebe</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name> (<year>2013</year>) <article-title>Spectral methods for neural characterization using generalized quadratic models</article-title>. In: <name name-style="western"><surname>Burges</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Welling</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>K</given-names></name>, editors, <source>Advances in Neural Information Processing Systems 26</source>, <publisher-name>Curran Associates, Inc.</publisher-name> pp. <fpage>2454</fpage>–<lpage>2462</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rajan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Marre</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name> (<year>2013</year>) <article-title>Learning quadratic receptive fields from neural responses to natural stimuli</article-title>. <source>Neural Computation</source> <volume>25</volume>: <fpage>1661</fpage>–<lpage>1692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00463" xlink:type="simple">10.1162/NECO_a_00463</ext-link></comment> <object-id pub-id-type="pmid">23607557</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kinney</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Callan</surname> <given-names>C</given-names></name> (<year>2007</year>) <article-title>Precise physical models of protein–DNA interaction from high-throughput data</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>104</volume>: <fpage>501</fpage>–<lpage>506</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0609908104" xlink:type="simple">10.1073/pnas.0609908104</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brenner</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Strong</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Koberle</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>de Ruyter van Steveninck</surname> <given-names>RR</given-names></name> (<year>2000</year>) <article-title>Synergy in a neural code</article-title>. <source>Neural Computation</source> <volume>12</volume>: <fpage>1531</fpage>–<lpage>1552</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976600300015259" xlink:type="simple">10.1162/089976600300015259</ext-link></comment> <object-id pub-id-type="pmid">10935917</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Maimon</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Assad</surname> <given-names>JA</given-names></name> (<year>2009</year>) <article-title>Beyond Poisson: increased spike-time regularity across primate parietal cortex</article-title>. <source>Neuron</source> <volume>62</volume>: <fpage>426</fpage>–<lpage>440</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.03.021" xlink:type="simple">10.1016/j.neuron.2009.03.021</ext-link></comment> <object-id pub-id-type="pmid">19447097</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rapela</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Felsen</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Touryan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mendel</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Grzywacz</surname> <given-names>N</given-names></name> (<year>2010</year>) <article-title>ePPR: a new strategy for the characterization of sensory cells from input/output data</article-title>. <source>Network: Computation in Neural Systems</source> <volume>21</volume>: <fpage>35</fpage>–<lpage>90</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name> (<year>2005</year>) <article-title>Spatiotemporal elements of macaque V1 receptive fields</article-title>. <source>Neuron</source> <volume>46</volume>: <fpage>945</fpage>–<lpage>956</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2005.05.021" xlink:type="simple">10.1016/j.neuron.2005.05.021</ext-link></comment> <object-id pub-id-type="pmid">15953422</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name> (<year>2006</year>) <article-title>How MT cells analyze the motion of visual patterns</article-title>. <source>Nature Neuroscience</source> <volume>9</volume>: <fpage>1421</fpage>–<lpage>1431</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1786" xlink:type="simple">10.1038/nn1786</ext-link></comment> <object-id pub-id-type="pmid">17041595</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sugiyama</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kawanabe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chui</surname> <given-names>PL</given-names></name> (<year>2010</year>) <article-title>Dimensionality reduction for density ratio estimation in high-dimensional spaces</article-title>. <source>Neural Networks</source> <volume>23</volume>: <fpage>44</fpage>–<lpage>59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2009.07.007" xlink:type="simple">10.1016/j.neunet.2009.07.007</ext-link></comment> <object-id pub-id-type="pmid">19631506</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Sugiyama</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Suzuki</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kanamori</surname> <given-names>T</given-names></name> (<year>2012</year>) <source>Density ratio estimation in machine learning</source>. <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Suzuki</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Sugiyama</surname> <given-names>M</given-names></name> (<year>2013</year>) <article-title>Sufficient dimension reduction via squared-loss mutual information estimation</article-title>. <source>Neural Computation</source> <volume>25</volume>: <fpage>725</fpage>–<lpage>758</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00407" xlink:type="simple">10.1162/NECO_a_00407</ext-link></comment> <object-id pub-id-type="pmid">23272920</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Vintch</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Zaharia</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name> (<year>2012</year>) <chapter-title>Efficient and direct estimation of a neural subunit model for sensory coding</chapter-title>. In: <name name-style="western"><surname>Bartlett</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pereira</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Burges</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>K</given-names></name>, editors, <source>Advances in Neural Information Processing Systems 25</source>, <publisher-name>Curran Associates, Inc.</publisher-name>, pp. <fpage>3104</fpage>–<lpage>3112</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Theis</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Chagas</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Arnstein</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Schwarz</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name> (<year>2013</year>) <article-title>Beyond GLMs: A generative mixture modeling approach to neural system identification</article-title>. <source>PLoS Computational Biology</source> <volume>9</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003356" xlink:type="simple">10.1371/journal.pcbi.1003356</ext-link></comment> <object-id pub-id-type="pmid">24278006</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rad</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name> (<year>2010</year>) <article-title>Efficient, adaptive estimation of two-dimensional firing rate surfaces via Gaussian process methods</article-title>. <source>Network: Computation in Neural Systems</source> <volume>21</volume>: <fpage>142</fpage>–<lpage>168</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Park</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Horwitz</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name> (<year>2011</year>) <chapter-title>Active learning of neural response functions with Gaussian processes</chapter-title>. In: <name name-style="western"><surname>Shawe-Taylor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zemel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bartlett</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pereira</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>K</given-names></name>, editors, <source>Advances in Neural Information Processing Systems 24</source>, <publisher-name>Curran Associates, Inc.</publisher-name>, pp. <fpage>2043</fpage>–<lpage>2051</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Rice</surname> <given-names>J</given-names></name> (<year>1964</year>) <source>The approximation of functions: linear theory</source>, <volume>volume 1</volume>. <publisher-name>Addison-Wesley</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Park</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sandberg</surname> <given-names>I</given-names></name> (<year>1991</year>) <article-title>Universal approximation using radial-basis-function networks</article-title>. <source>Neural Computation</source> <volume>3</volume>: <fpage>246</fpage>–<lpage>257</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1991.3.2.246" xlink:type="simple">10.1162/neco.1991.3.2.246</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Korenberg</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bruder</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mcllroy</surname> <given-names>P</given-names></name> (<year>1988</year>) <article-title>Exact orthogonal kernel estimation from finite data records: Extending Weiner’s identification of nonlinear systems</article-title>. <source>Annals of Biomedical Engineering</source> <volume>16</volume>: <fpage>201</fpage>–<lpage>214</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF02364581" xlink:type="simple">10.1007/BF02364581</ext-link></comment> <object-id pub-id-type="pmid">3382067</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Victor</surname> <given-names>J</given-names></name> (<year>1991</year>) <article-title>Asymptotic approach of generalized orthogonal functional expansions to Wiener kernels</article-title>. <source>Annals of Biomedical Engineering</source> <volume>19</volume>: <fpage>383</fpage>–<lpage>399</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF02584317" xlink:type="simple">10.1007/BF02584317</ext-link></comment> <object-id pub-id-type="pmid">1741523</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name> (<year>2002</year>) <article-title>Training products of experts by minimizing contrastive divergence</article-title>. <source>Neural Computation</source> <volume>14</volume>: <fpage>1771</fpage>–<lpage>1800</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976602760128018" xlink:type="simple">10.1162/089976602760128018</ext-link></comment> <object-id pub-id-type="pmid">12180402</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rowekamp</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>T</given-names></name> (<year>2011</year>) <article-title>Analyzing multicomponent receptive fields from neural responses to natural stimuli</article-title>. <source>Network: Computation in Neural Systems</source> <volume>7</volume>: <fpage>1</fpage>–<lpage>29</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Park</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name> (<year>2011</year>) <chapter-title>Bayesian Spike-Triggered Covariance Analysis</chapter-title>. In: <name name-style="western"><surname>Shawe-Taylor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zemel</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Bartlett</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pereira</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>K</given-names></name>, editors, <source>Advances in Neural Information Processing Systems 24</source>, <publisher-name>Curran Associates, Inc.</publisher-name> pp. <fpage>1692</fpage>–<lpage>1700</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fitzgerald</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Sincich</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name> (<year>2011</year>) <article-title>Minimal models of multidimensional computations</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1001111</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1001111" xlink:type="simple">10.1371/journal.pcbi.1001111</ext-link></comment> <object-id pub-id-type="pmid">21455284</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fitzgerald</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Rowekamp</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Sincich</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name> (<year>2011</year>) <article-title>Second order dimensionality reduction using minimum and maximum mutual information models</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002249</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002249" xlink:type="simple">10.1371/journal.pcbi.1002249</ext-link></comment> <object-id pub-id-type="pmid">22046122</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Christianson</surname> <given-names>GB</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Linden</surname> <given-names>JF</given-names></name> (<year>2008</year>) <article-title>The consequences of response nonlinearities for interpretation of spectrotemporal receptive fields</article-title>. <source>Journal of Neuroscience</source> <volume>28</volume>: <fpage>446</fpage>–<lpage>455</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1775-07.2007" xlink:type="simple">10.1523/JNEUROSCI.1775-07.2007</ext-link></comment> <object-id pub-id-type="pmid">18184787</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Linden</surname> <given-names>J</given-names></name> (<year>2003</year>) <article-title>Evidence optimization techniques for estimating stimulus-response functions</article-title>. In: <name name-style="western"><surname>Becker</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Thrun</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Obermayer</surname> <given-names>K</given-names></name>, editors, <source>Advances in Neural Information Processing Systems 15</source> <publisher-name>MIT Press</publisher-name>. pp. <fpage>317</fpage>–<lpage>324</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Park</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name> (<year>2011</year>) <article-title>Receptive field inference with localized priors</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002219</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002219" xlink:type="simple">10.1371/journal.pcbi.1002219</ext-link></comment> <object-id pub-id-type="pmid">22046110</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Reich</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Mechler</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Purpura</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Victor</surname> <given-names>JD</given-names></name> (<year>2000</year>) <article-title>Interspike intervals, receptive fields, and information encoding in primary visual cortex</article-title>. <source>Journal of Neuroscience</source> <volume>20</volume>: <fpage>1964</fpage>–<lpage>1974</lpage>. <object-id pub-id-type="pmid">10684897</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barbieri</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Quirk</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name> (<year>2001</year>) <article-title>Construction and analysis of non-poisson stimulus-response models of neural spiking activity</article-title>. <source>Journal of Neuroscience Methods</source> <volume>105</volume>: <fpage>25</fpage>–<lpage>37</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0165-0270(00)00344-7" xlink:type="simple">10.1016/S0165-0270(00)00344-7</ext-link></comment> <object-id pub-id-type="pmid">11166363</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name> (<year>2009</year>) <chapter-title>Time-rescaling methods for the estimation and assessment of non-poisson neural encoding models</chapter-title>. In: <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schuurmans</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>La erty</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>CKI</given-names></name>, <name name-style="western"><surname>Culotta</surname> <given-names>A</given-names></name>, editors, <source>Advances in Neural Information Processing Systems 22</source>. <publisher-name>Curran Associates, Inc.</publisher-name>, pp. <fpage>1473</fpage>–<lpage>1481</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Atencio</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name> (<year>2008</year>) <article-title>Cooperative nonlinearities in auditory cortical neurons</article-title>. <source>Neuron</source> <volume>58</volume>: <fpage>956</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.04.026" xlink:type="simple">10.1016/j.neuron.2008.04.026</ext-link></comment> <object-id pub-id-type="pmid">18579084</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Atencio</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name> (<year>2009</year>) <article-title>Hierarchical computation in the canonical auditory cortical circuit</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>106</volume>: <fpage>21894</fpage>–<lpage>21899</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0908383106" xlink:type="simple">10.1073/pnas.0908383106</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>, <name name-style="western"><surname>Atencio</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name> (<year>2011</year>) <article-title>Hierarchical representations in the auditory cortex</article-title>. <source>Current Opinion in Neurobiology</source> <volume>21</volume>: <fpage>761</fpage>–<lpage>767</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2011.05.027" xlink:type="simple">10.1016/j.conb.2011.05.027</ext-link></comment> <object-id pub-id-type="pmid">21704508</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Atencio</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name> (<year>2012</year>) <article-title>Receptive field dimensionality increases from the auditory midbrain to cortex</article-title>. <source>Journal of Neurophysiology</source> <volume>107</volume>: <fpage>2594</fpage>–<lpage>2603</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.01025.2011" xlink:type="simple">10.1152/jn.01025.2011</ext-link></comment> <object-id pub-id-type="pmid">22323634</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fernandes</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Stevenson</surname> <given-names>IH</given-names></name>, <name name-style="western"><surname>Phillips</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Segraves</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>KP</given-names></name> (<year>2013</year>) <article-title>Saliency and saccade encoding in the frontal eye field during natural scene search</article-title>. <source>Cerebral Cortex</source> <volume>24</volume>: <fpage>3232</fpage>–<lpage>3245</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004141.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Fellows</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shoham</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hatsopoulos</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Donoghue</surname> <given-names>J</given-names></name> (<year>2004</year>) <article-title>Superlinear population encoding of dynamic hand trajectory in primary motor cortex</article-title>. <source>Journal of Neuroscience</source> <volume>24</volume>: <fpage>8551</fpage>–<lpage>8561</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0919-04.2004" xlink:type="simple">10.1523/JNEUROSCI.0919-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15456829</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004141.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schneidman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Segev</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name> (<year>2006</year>) <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title>. <source>Nature</source> <volume>440</volume>: <fpage>1007</fpage>–<lpage>1012</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature04701" xlink:type="simple">10.1038/nature04701</ext-link></comment> <object-id pub-id-type="pmid">16625187</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>