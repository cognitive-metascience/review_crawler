{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this file is for managing file directories containing corpuses, built with MDPI in mind\n",
    "\n",
    "the idea is to scan the folder containing all scraped articles (a_dir), then move folders with articles that do have reviews to a different folder (r_dir, by default it's named 'reviewed')\n",
    "\n",
    "this notebook is also used to fix issues with the mdpi corpus: missing 'sub_articles' field in the metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getj(a):\n",
    "    \"\"\"\n",
    "    Takes a path to a directory and returns the metadata.json file as a dictionary.\n",
    "\n",
    "    :param a: path to an article's folder.\n",
    "    :return: A json object as a dict.\n",
    "    :rtype: dict\n",
    "\n",
    "    \"\"\"\n",
    "    path = os.path.join(a, \"metadata.json\")\n",
    "    if not os.path.exists(path):\n",
    "        print(\"no meta for a =\", a)\n",
    "        return pd.NA\n",
    "    fp = open(path, encoding='utf-8')\n",
    "    j = json.load(fp)\n",
    "    fp.close()\n",
    "    return j\n",
    "\n",
    "# count number of files in sub-articles for each reviewed\n",
    "def count_suba_files(x):\n",
    "    if not x.has_suba: return 0\n",
    "    else: return len(os.listdir(x.path+\"/sub-articles\"))\n",
    "\n",
    "def load_arts_dir(dirpath, load_meta = False):\n",
    "    \"\"\"\n",
    "    The load_arts_dir function takes a directory path and returns a dataframe df with the following columns:\n",
    "        doi - the name of each subdirectory in dirpath, which should be dois\n",
    "        path - the full path to each subdirectory in dirpath\n",
    "        has_suba - whether or not there is a 'sub-articles' folder within each article's folder\n",
    "        num_suba_files - how many files are contained within 'sub-articles' if it exists\n",
    "        meta and meta.has_reviews - only if load_meta is True\n",
    "\n",
    "    Setting the load_meta parameter to True will mean that all JSON files will be loaded into df as dicts and stored in the 'meta' column.\n",
    "    This could take Extremely long to run, depending on the number of directories in the provided path.\n",
    "    \n",
    "    :param dirpath: Used to Specify the directory path of the articles.\n",
    "    :param load_meta=False: Whether to Load the metadata from the articles into the column 'meta'.\n",
    "    :return: A pandas dataframe\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'doi': os.listdir(dirpath)})  # folder names should be dois\n",
    "    df = df.loc[df.doi.map(lambda x: os.path.isdir(os.path.join(dirpath, x)))]\n",
    "    df['path'] = df.doi.map(lambda x: os.path.join(dirpath, x))  # path to the folder relative to cwd\n",
    "    # check if articles have a sub-articles folder\n",
    "    df['has_suba'] = df.path.map(lambda p: os.path.exists(os.path.join(p, \"sub-articles\")))\n",
    "    df['num_suba_files'] = df.apply(count_suba_files, axis=1)\n",
    "    if load_meta:\n",
    "        df['meta'] = df.path.apply(getj)  # dangerous line\n",
    "        df['meta.has_reviews'] = df.meta.map(lambda a: a['has_reviews'])\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dir = os.path.join('output', 'mdpi')  # change 'mdpi' to plos/elife to work with others\n",
    "r_dir = os.path.join(a_dir, \"reviewed\")  # this is where the reviewed articles should be stored. could be different for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Run at your own risk: this cell loads ALL metadata files from a_dir into the dataframe `arts`\n",
    "# on my PC this took 42 minutes to run!\n",
    "arts = load_arts_dir(a_dir, load_meta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: same as above, but for the folder with reviewed articles\n",
    "# this takes ~ 11 minutes on my PC\n",
    "rarts = load_arts_dir(r_dir, load_meta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 493681 articles in the root (all) folder and\t 135772 in the 'reviewed' folder\n"
     ]
    }
   ],
   "source": [
    "print(\"we have\",len(arts),\"articles in the root (all) folder and\\t\",len(rarts),\"in the 'reviewed' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(arts['meta.has_reviews']), \"<- this many articles from \" + a_dir + \" have reviews!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 493681 arts have a sub-articles folder\n",
      "151302 out of 151817 rarts have a sub-articles folder\n"
     ]
    }
   ],
   "source": [
    "# count how many articles have a sub-articles folder\n",
    "print(sum(arts.has_suba),  \"out of\",len(arts),\"arts have a sub-articles folder\")\n",
    "print(sum(rarts.has_suba), \"out of\",len(rarts),\"rarts have a sub-articles folder\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move reviewed to a separate folder\n",
    "\n",
    "reviewed_arts = arts.loc[arts['meta.has_reviews']]\n",
    "\n",
    "# first check if already in rarts:\n",
    "reviewed_arts.loc[:, 'in_rarts'] = reviewed_arts.loc[:, 'doi'].isin(rarts.loc[:, 'doi'])\n",
    "\n",
    "print(f\"{reviewed_arts['in_rarts'].mean()*100}% of dois are already in rarts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not reviewed_arts['in_rarts'].all():\n",
    "    reviewed_arts['new_path'] = reviewed_arts['path'].apply(lambda x: '/'.join(str(x).split('/')[0:2]) + '/reviewed/' + str(x).split('/')[2])\n",
    "    \n",
    "    # do the new paths exist?\n",
    "    # print(f\"{reviewed_arts['new_path'].apply(os.path.exists).mean()*100}% of new_paths already exist\")\n",
    "\n",
    "    reviewed_arts.apply(lambda x: shutil.copytree(x.path, x.new_path), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove reviewed articles from root folder\n",
    "\n",
    "reviewed_arts.loc[:, 'path_exists'] = reviewed_arts.loc[:, \"path\"].apply(os.path.exists)\n",
    "\n",
    "reviewed_arts = reviewed_arts.loc[reviewed_arts['path_exists']]\n",
    "\n",
    "# print(reviewed_arts['path_exists'].sum(), len(reviewed_arts))\n",
    "\n",
    "if reviewed_arts['path_exists'].any():\n",
    "    reviewed_arts['path'].apply(shutil.rmtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_arts.loc[:, 'path_exists'] = reviewed_arts.loc[:, \"path\"].apply(os.path.exists)\n",
    "reviewed_arts['path_exists'].any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bind new rows to rarts\n",
    "reviewed_arts['path'] = reviewed_arts['new_path']\n",
    "rarts = pd.concat([rarts, reviewed_arts.loc[~reviewed_arts['in_rarts']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16045 <- this many reviewed articles have 0 files in their sub-articles folder\n",
      "That is 11 percent of all rarts\n"
     ]
    }
   ],
   "source": [
    "lacking_suba = rarts[rarts['num_suba_files'] == 0]\n",
    "print(len(lacking_suba), \"<- this many reviewed articles have 0 files in their sub-articles folder\")\n",
    "print(\"That is\",round(len(lacking_suba)*100/len(rarts)),\"percent of all rarts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move reviewed, but without sub-articles to a working dump_dir\n",
    "\n",
    "dump_dir = \"output/mdpi-to-scrape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lacking_suba['new_path'] = lacking_suba.loc[:, 'doi'].map(lambda x: dump_dir + \"/\" + x)\n",
    "lacking_suba.apply(lambda x: shutil.copytree(x.path, x.new_path), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove reviewed articles without sub-articles from 'reviewed'\n",
    "\n",
    "# first remove those form rarts\n",
    "rarts = rarts.loc[~rarts.doi.isin(lacking_suba.doi)]\n",
    "\n",
    "# check if path exists, if yes remove those files\n",
    "lacking_suba.loc[:, 'path_exists'] = lacking_suba.loc[:, \"path\"].apply(os.path.exists)\n",
    "\n",
    "lacking_suba = lacking_suba.loc[lacking_suba['path_exists']]\n",
    "\n",
    "# print(lacking_suba['path_exists'].sum(), len(lacking_suba))\n",
    "\n",
    "if lacking_suba['path_exists'].any():\n",
    "    lacking_suba['path'].apply(shutil.rmtree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the mdpi review crawler on the folder `dump_dir` specified above to attempt to scrape the missing sub-articles files\n",
    "\n",
    "then run the cells below to load the updated articles from that directory, and then update the 'reviewed' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-articles were scraped for 479 out of 16524 articles from output/mdpi-dump-dir/\n"
     ]
    }
   ],
   "source": [
    "lacking_suba2 = load_arts_dir(dump_dir, load_meta=True)\n",
    "\n",
    "# check for how many articles we managed to get sub-articles:\n",
    "print(\"sub-articles were scraped for\",len(lacking_suba2[lacking_suba2['num_suba_files'] > 0]),\n",
    "      \"out of\",len(lacking_suba2),\"articles from\",dump_dir)\n",
    "\n",
    "lacking_suba2 = lacking_suba2[lacking_suba2['num_suba_files'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move these articles back to the 'reviewed' folder\n",
    "lacking_suba2['new_path'] = lacking_suba2['doi'].apply(lambda x: os.path.join(r_dir, x))\n",
    "\n",
    "lacking_suba2['path'] = lacking_suba2.apply(lambda x: shutil.copytree(x.path, x.new_path, dirs_exist_ok=True), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally update rarts:\n",
    "assert len(rarts.loc[rarts.doi.isin(lacking_suba2.doi)] == len(lacking_suba2))\n",
    "\n",
    "rarts = pd.concat([rarts, lacking_suba2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135772 articles have a sub-articles directory\n"
     ]
    }
   ],
   "source": [
    "## add sub-articles to metadata.json files\n",
    "\n",
    "rarts = rarts.loc[rarts['num_suba_files'] > 0]\n",
    "print(len(rarts), \"articles have a sub-articles directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rarts['meta.has_suba_obj'] = rarts['meta'].apply(lambda x: 'sub_articles' in x.keys())\n",
    "\n",
    "# working copy to update\n",
    "temp = rarts.loc[-rarts['meta.has_suba_obj']]\n",
    "print(len(temp),\"JSON files should end up with the key 'sub-articles'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metadata_add_suba(rart):\n",
    "    \"\"\"\n",
    "    The update_metadata_add_suba function takes an article and updates the metadata.json file\n",
    "    with sub-article information if it exists.\n",
    "    \n",
    "    :return: A metadata object.\n",
    "    \"\"\"\n",
    "    meta = rart.meta\n",
    "    if not rart.has_suba: return meta\n",
    "    if rart.num_suba_files == 0: return meta  # shouldn't happen really \n",
    "\n",
    "    sub_a_path = os.path.join(rart.path, 'sub-articles')\n",
    "    \n",
    "    sub_articles = []\n",
    "    for json_file in [f for f in os.listdir(sub_a_path) if f.endswith(\".json\")]:\n",
    "        filepath = os.path.join(sub_a_path, json_file)\n",
    "        j = json.load(open(filepath, 'rb'))\n",
    "        sub_articles.append(j)\n",
    "    \n",
    "    if len(sub_articles) > 0: \n",
    "        meta['sub_articles'] = sub_articles\n",
    "        with open(os.path.join(rart.path, \"metadata.json\"), 'w', encoding=\"utf-8\") as fp:\n",
    "            json.dump(meta, fp, ensure_ascii=False)\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this cell will probably also take very long to run, depending on how many articles are in `temp`\n",
    "# 17 minutes on my PC\n",
    "temp['meta'] = temp.apply(update_metadata_add_suba, axis=1)\n",
    "temp['meta.has_suba_obj'] = temp['meta'].apply(lambda x: 'sub_articles' in x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally update rarts:\n",
    "assert len(rarts) == len(temp) + len(rarts.loc[rarts['num_suba_files'] > 0])\n",
    "\n",
    "rarts = rarts.loc[~rarts.doi.isin(temp.doi)]\n",
    "rarts = pd.concat([rarts, temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% of rarts now have sub-articles in their metadata.json\n"
     ]
    }
   ],
   "source": [
    "rarts['meta.has_suba_obj'] = rarts['meta'].apply(lambda x: 'sub_articles' in x.keys())\n",
    "print(f\"{round(rarts['meta.has_suba_obj'].mean()*100)}% of rarts now have sub-articles in their metadata.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
