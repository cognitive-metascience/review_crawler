<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">28158</article-id><article-id pub-id-type="doi">10.7554/eLife.28158</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Rapid whole brain imaging of neural activity in freely behaving larval zebrafish (<italic>Danio rerio</italic>)</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-87625"><name><surname>Cong</surname><given-names>Lin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-87626"><name><surname>Wang</surname><given-names>Zeguan</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-20618"><name><surname>Chai</surname><given-names>Yuming</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-87627"><name><surname>Hang</surname><given-names>Wei</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-87628"><name><surname>Shang</surname><given-names>Chunfeng</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-87740"><name><surname>Yang</surname><given-names>Wenbin</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-87630"><name><surname>Bai</surname><given-names>Lu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-87631"><name><surname>Du</surname><given-names>Jiulin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-86951"><name><surname>Wang</surname><given-names>Kai</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7858-944X</contrib-id><email>wangkai@ion.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-87624"><name><surname>Wen</surname><given-names>Quan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0268-8403</contrib-id><email>qwen@ustc.edu.cn</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Institute of Neuroscience, State Key Laboratory of Neuroscience, CAS Center for Excellence in Brain Science and Intelligence Technology</institution><institution>Shanghai Institutes for Biological Sciences, Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Center for Integrative Imaging, Hefei National Laboratory for Physical Sciences at Microscale, CAS Center for Excellence in Brain Science and Intelligence Technology, School of Life Sciences</institution><institution>University of Science and Technology of China</institution><addr-line><named-content content-type="city">Hefei</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution>University of Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-1056"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>20</day><month>09</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e28158</elocation-id><history><date date-type="received" iso-8601-date="2017-04-28"><day>28</day><month>04</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2017-09-11"><day>11</day><month>09</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Cong et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Cong et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-28158-v2.pdf"/><related-object ext-link-type="url" xlink:href="https://elifesciences.org/articles/e28158v1"><date date-type="v1" iso-8601-date="2017-09-20"><day>20</day><month>09</month><year>2017</year></date></related-object><abstract><object-id pub-id-type="doi">10.7554/eLife.28158.001</object-id><p>The internal brain dynamics that link sensation and action are arguably better studied during natural animal behaviors. Here, we report on a novel volume imaging and 3D tracking technique that monitors whole brain neural activity in freely swimming larval zebrafish (<italic>Danio rerio</italic>). We demonstrated the capability of our system through functional imaging of neural activity during visually evoked and prey capture behaviors in larval zebrafish.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.28158.002</object-id><title>eLife digest</title><p>How do neurons in the brain process information from the senses and drive complex behaviors? This question has fascinated neuroscientists for many years. It is currently not possible to record the electrical activities of all of the 100 billion neurons in a human brain. Yet, in the last decade, it has become possible to genetically engineer some neurons in animals to produce fluorescence reporters that change their brightness in response to brain activity and then monitor them under a microscope. In small animals such as zebrafish larvae, this method makes it possible to monitor the activities of all the neurons in the brain if the animal’s head is held still. However, many behaviors – for example, catching prey – require movement, and no existing technique could image brain activity in enough detail if the animal’s head was moving.</p><p>Cong, Wang, Chai, Hang et al. have now made progress towards this goal by developing a new technique to image neural activity across the whole brain of a zebrafish larva as it swims freely in a small water-filled chamber. The technique uses high-speed cameras and computer software to track the movements of the fish in three dimensions, and then automatically moves the chamber under the microscope such that the animal’s brain is constantly kept in focus. The newly developed microscope can capture changes in neural activity across a large volume all at the same time. It is then further adapted to overcome problems caused by sudden or swift movements, which would normally result in motion blur. With this microscope set up, Cong et al. were able to capture, for the first time, activity from all the neurons in a zebrafish larva’s brain as it pursued and caught its prey.</p><p>This technique provides a new window into how brain activity changes when animals are behaving naturally. In the future, this technique could help link the activities of neurons to different behaviors in several popular model organisms including fish, worms and fruit flies.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>freely behaving larval zebrafish</kwd><kwd>whole brain Imaging</kwd><kwd>3D tracking system</kwd><kwd>light field microscope</kwd><kwd>prey capture behavior</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Zebrafish</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Strategic Priority Research Program of the Chinese Academy of Sciences</institution></institution-wrap></funding-source><award-id>XDB02060012</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Kai</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>National Science Foundation of China</institution></institution-wrap></funding-source><award-id>NSFC-31471051</award-id><principal-award-recipient><name><surname>Wen</surname><given-names>Quan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>China Thousand Talents Program</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Wang</surname><given-names>Kai</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>CAS Pioneer Hundred Talents Program</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Wen</surname><given-names>Quan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The integration of a novel high spatiotemporal resolution volume imaging technique and a fast 3D tracking system allows capturing whole brain neural activities in a freely behaving larval zebrafish.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A central goal in systems neuroscience is to understand how distributed neural circuitry dynamics drive animal behaviors. The emerging field of optical neurophysiology allows monitoring (<xref ref-type="bibr" rid="bib25">Kerr and Denk, 2008</xref>; <xref ref-type="bibr" rid="bib17">Dombeck et al., 2007</xref>) and manipulating (<xref ref-type="bibr" rid="bib53">Wyart et al., 2009</xref>; <xref ref-type="bibr" rid="bib11">Boyden et al., 2005</xref>; <xref ref-type="bibr" rid="bib54">Zhang et al., 2007</xref>) the activities of defined populations of neurons that express genetically encoded activity indicators (<xref ref-type="bibr" rid="bib14">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib48">Tian et al., 2009</xref>) and light-activated proteins (<xref ref-type="bibr" rid="bib25">Kerr and Denk, 2008</xref>; <xref ref-type="bibr" rid="bib11">Boyden et al., 2005</xref>; <xref ref-type="bibr" rid="bib54">Zhang et al., 2007</xref>; <xref ref-type="bibr" rid="bib28">Luo et al., 2008</xref>). Larval zebrafish (<italic>Danio rerio</italic>) are an attractive model system to investigate the neural correlates of behaviors owing to their small brain size, optical transparency, and rich behavioral repertoire (<xref ref-type="bibr" rid="bib20">Friedrich et al., 2010</xref>; <xref ref-type="bibr" rid="bib5">Ahrens and Engert, 2015</xref>). Whole brain imaging of larval zebrafish using light sheet/two-photon microscopy holds considerable potential in creating a comprehensive functional map that links neuronal activities and behaviors (<xref ref-type="bibr" rid="bib3">Ahrens et al., 2012</xref>; <xref ref-type="bibr" rid="bib4">Ahrens et al., 2013</xref>; <xref ref-type="bibr" rid="bib19">Engert, 2014</xref>).</p><p>Recording neural activity maps in larval zebrafish has been successfully integrated with the virtual reality paradigm: closed-loop fictive behaviors in immobilized fish can be monitored and controlled via visual feedback that varies according to the electrical output patterns of motor neurons (<xref ref-type="bibr" rid="bib3">Ahrens et al., 2012</xref>; <xref ref-type="bibr" rid="bib18">Engert, 2012</xref>). The behavioral repertoire, however, may be further expanded in freely swimming zebrafish whose behavioral states can be directly inferred and when sensory feedback loops are mostly intact and active. For example, it is likely that vestibular as well as proprioceptive feedbacks are perturbed in immobilized zebrafish (<xref ref-type="bibr" rid="bib18">Engert, 2012</xref>; <xref ref-type="bibr" rid="bib9">Bianco et al., 2012</xref>). The crowning moment during hunting behavior (<xref ref-type="bibr" rid="bib8">Bianco et al., 2011</xref>; <xref ref-type="bibr" rid="bib38">Patterson et al., 2013</xref>; <xref ref-type="bibr" rid="bib49">Trivedi and Bollmann, 2013</xref>) — when a fish succeeds in catching a paramecium — cannot be easily replicated in a virtual reality setting. Therefore, whole brain imaging in a freely swimming zebrafish may allow optical interrogation of brain circuits underlying a range of less explored behaviors.</p><p>Although whole brain functional imaging methods are available for head-fixed larval zebrafish, imaging a speeding brain imposes many technical challenges. Current studies on freely swimming zebrafish are either limited to non-imaging optical systems (<xref ref-type="bibr" rid="bib31">Naumann et al., 2010</xref>) or wide field imaging at low resolution (<xref ref-type="bibr" rid="bib30">Muto et al., 2013</xref>). While light sheet microscopy (LSM) has demonstrated entire brain coverage and single neuron resolution in restrained zebrafish (<xref ref-type="bibr" rid="bib4">Ahrens et al., 2013</xref>), it lacks the speed to follow rapid fish movement. Moreover, in LSM, the sample is illuminated from its side, a configuration that is difficult to be integrated with a tracking system. Conventional light field microscopy (LFM) (<xref ref-type="bibr" rid="bib12">Broxton et al., 2013</xref>; <xref ref-type="bibr" rid="bib43">Prevedel et al., 2014</xref>) is a promising alternative due to its higher imaging speed; however, its spatial resolution is relatively low. Specialized LFMs for monitoring neural activity utilizing temporal information were also developed recently (<xref ref-type="bibr" rid="bib44">Pégard et al., 2016</xref>; <xref ref-type="bibr" rid="bib35">Nöbauer et al., 2017</xref>), which rely on spatiotemporal sparsity of fluorescent signals and cannot be applied to moving animals.</p><p>Here, we describe a fast 3D tracking technique and a novel volume imaging method that allows whole brain calcium imaging with high spatial and temporal resolution in freely behaving larval zebrafish. Zebrafish larvae possess extraordinary mobility. They can move at an instantaneous velocity up to 50 mm/s (<xref ref-type="bibr" rid="bib46">Severi et al., 2014</xref>) and acceleration of 1 g (9.83 m/s<sup>2</sup>). To continuously track fish motion, we developed a high-speed closed-loop system in which (1) customized machine vision software allowed rapid estimation of fish movement in both the <italic>x-y</italic> and <italic>z</italic> directions; and, (2) feedback control signals drove a high-speed motorized <italic>x-y</italic> stage (at 300 Hz) and a piezo <italic>z</italic> stage (at 100 Hz) to retain the entire fish head within the field of view of a high numerical aperture (25×, NA = 1.05) objective.</p><p>Larval zebrafish can make sudden and swift movements that easily cause motion blur and severely degrade imaging quality. To overcome this obstacle, we developed a new eXtended field of view LFM (XLFM). The XLFM can image sparse neural activity over the larval zebrafish brain at near single cell resolution and at a volume rate of 77 Hz, with the aid of genetically encoded calcium indicator GCamp6f. Furthermore, the implementation of flashed fluorescence excitation (200 μs in duration) allowed blur-free fluorescent images to be captured when a zebrafish moved at a speed up to 10 mm/s. The seamless integration of the tracking and imaging system made it possible to reveal rich whole brain neural dynamics during natural behavior with unprecedented resolution. We demonstrated the ability of our system during visually evoked and prey capture behaviors in larval zebrafish.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>The newly developed XLFM is based on the general principle of light field (<xref ref-type="bibr" rid="bib2">Adelson and Wang, 1992</xref>) and can acquire 3D information from a single camera frame. XLFM greatly relaxed the constraint imposed by the tradeoff between spatial resolution and imaging volume coverage in conventional LFM. This achievement relies on optics and in computational reconstruction techniques. First, a customized lenslet array (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) was placed at the rear pupil plane of the imaging objective, instead of at the imaging plane as in LFM. Therefore, in ideal conditions, a 2D spatially invariant point spread function (PSF) could be defined and measured; in practice, the PSF was approximately spatially invariant (see Materials and methods). Second, the aperture size of each micro-lens was decoupled from their interspacing and spatial arrangement, so that both the imaging volume and the resolution could be optimized simultaneously given the limited imaging sensor size. Third, multifocal imaging (<xref ref-type="bibr" rid="bib1">Abrahamsson et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Perwass and Wietzke, 2012</xref>) was introduced to further increase the depth of view by dividing the micro-lenses array into two groups whose focal planes were at different axial positions (<xref ref-type="fig" rid="fig1">Figure 1b and c</xref>, <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>). Fourth, a new computational algorithm based on optical wave theory was developed to reconstruct the entire 3D volume from one image (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>) captured by a fast camera (see Materials and methods).</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.28158.003</object-id><label>Figure 1.</label><caption><title>Whole brain imaging of larval zebrafish with XLFM.</title><p>(<bold>a</bold>) Schematic of XLFM. Lenslet array position was conjugated to the rear pupil plane of the imaging objective. Excitation laser (blue) provided uniform illumination across the sample. (<bold>b–c</bold>) Point sources at two different depths formed, through two different groups of micro-lenses, sharp images on the imaging sensor, with positional information reconstructed from these distinct patterns. (<bold>d</bold>) Maximum intensity projections (MIPs) on time and space of time series volume images of an agarose-restrained larval zebrafish with pan-neuronal nucleus-localized GCaMP6f (huc:h2b-gcamp6f) fluorescence labeling. (<bold>e</bold>) Normalized neuronal activities of selected neurons exhibited increasing calcium responses after the onset of light stimulation at t = 0. Neurons were ordered by the onset time when the measured fluorescence signals reached 20% of their maximum. (<bold>f</bold>) Selected neurons in (<bold>e</bold>) were color coded based on their response onset time. Scale bar is 100 μm.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-v2"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Customized lenslet array.</title><p>Customized lenslet array consisted of 27 customized micro-lenses embedded in an aluminum plate with 27 drilled holes. (<bold>a</bold>) Micro-lenses were divided into two groups (A or B), illustrated in yellow and green, respectively. (<bold>b</bold>) Micro-lens had a diameter of 1.3 mm and focal length of 26 mm. (<bold>c</bold>). The aluminum housing plate had a 1.3 mm diameter aperture on one side and 1 mm diameter aperture on the other side. Group A and Group B micro-lenses were displaced axially.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp1-v2"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.005</object-id><label>Figure 1—figure supplement 2.</label><caption><title>Experimentally measured PSF of the whole imaging system.</title><p>Maximum intensity projections (MIPs) of the measured raw PSF stack. The stack was 2048 pixels × 2048 pixels×200 pixels with a voxel size of 1.6 µm × 1.6 μm × 2 μm.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp2-v2"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.006</object-id><label>Figure 1—figure supplement 3.</label><caption><title>PSF of Group A micro-lenses: PSF_A.</title><p>Maximum intensity projections (MIP) of PSF_A. PSF_A was extracted from experimentally measured PSF (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>) according to individual micro-lens positions in group A.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp3-v2"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.007</object-id><label>Figure 1—figure supplement 4.</label><caption><title>PS F of Group B micro-lenses: PSF_B.</title><p>Maximum intensity projections (MIP) of PSF_B. PSF_B was extracted from experimentally measured PSFs (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>) according to individual micro-lens positions in group B.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp4-v2"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.008</object-id><label>Figure 1—figure supplement 5.</label><caption><title>Example of camera captured raw imaging data of larval zebrafish.</title><p>Raw fluorescence imaging data consisted of 27 sub-images of a larval zebrafish formed by 27 micro-lenses. Under the condition that the PSF is spatially invariant, which is satisfied apart from small aberrations, the algorithm can handle overlapping fish images.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp5-v2"/></fig><fig id="fig1s6" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.009</object-id><label>Figure 1—figure supplement 6.</label><caption><title>Characterization of in-plane resolution of micro-lenses.</title><p>Fourier transforms of raw images of a 0.5-<inline-formula><mml:math id="inf1"><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula> diameter fluorescent particle placed at different locations (x = −400, 0, 400 μm; z = −100, 0, 100 μm) were plotted in log scales. Dashed circles represent in-plane spatial frequency coordinates corresponding to spatial resolutions of 3.2 μm and 4 μm, respectively.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp6-v2"/></fig><fig id="fig1s7" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.010</object-id><label>Figure 1—figure supplement 7.</label><caption><title>Characterization of axial resolution of XLFM afforded by individual micro-lenses.</title><p>Characterization of axial resolution using a 0.5 μm diameter bright fluorescent particle. (<bold>a</bold>) Maximum intensity projection of an image stack consisting of the particle’s fluorescent images captured at different z positions. (<bold>b</bold>) Analysis of the images formed by micro-lenses 1 and 2, indicated by sub-regions in (<bold>a</bold>). The first and second columns are the particle’s fluorescent images captured at different z positions separated by 5 μm. The third column is the sum of columns 1 and 2. The fourth column is the Fourier analysis of column three using function: <inline-formula><mml:math id="inf2"><mml:mi mathvariant="normal">f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mfenced close="|" open="|" separators="|"><mml:mrow><mml:mi mathvariant="script">ℱ</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula>), where <inline-formula><mml:math id="inf3"><mml:mi mathvariant="script">ℱ</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> represents the Fourier transform. The fifth column is the deconvolution of column three using Wiener filtering method. Experimentally measured images of the bead at different z positions (z = −100 μm, z = 0 μm and z = 100 μm) are employed as PSFs to deconvolve different images (C1, C2 and C3), respectively.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp7-v2"/></fig><fig id="fig1s8" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.011</object-id><label>Figure 1—figure supplement 8.</label><caption><title>Characterization of magnification variation of micro-lenses in XLFM.</title><p>Magnifications of 27 micro-lenses were measured at different locations across the field of view. A fluorescent bead originally placed at the center of the field of view (x, y, z = 0) was moved to six different locations (x = 200 μm, 300 μm, 400 μm, −200 μm, −300 μm, −400 μm, y = 0, z = 0). Six classes of the bead’s image shifts, represented by different colors, were measured. Each class consisted of 27 image shifts formed by 27 micro-lenses. Within each class, image shifts were normalized to the one from the first micro-lens. The first 12 micro-lenses and the rest formed two different groups of micro-lenses: group B and group A, consistent with <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>. The magnification variation of a single micro-lens across the field of view was small (&lt;0.3%), suggesting that the spatial invariance of individual micro-lens’ PSF was well preserved across the field of view of Ø = 800 μm. The variation across different micro-lenses within one group (A/B) was more evident (~2%), suggesting that the combined PSF from different micro-lenses was not perfectly spatially invariant.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp8-v2"/></fig><fig id="fig1s9" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.012</object-id><label>Figure 1—figure supplement 9.</label><caption><title>Resolution degradation due to focal length variation of micro-lenses.</title><p>Maximum intensity projections (MIPs) of a reconstructed fluorescent bead positioned at different locations across the field of view. As the bead moved to the edge of the field of view, the reconstruction became distorted because the magnification variation of the micro-lenses led to spatial variance of total PSF. Scale bars are 10 µm.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp9-v2"/></fig><fig id="fig1s10" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.013</object-id><label>Figure 1—figure supplement 10.</label><caption><title>Characterization of axial resolution of XLFM at low SNR.</title><p>Characterization of axial resolution using densely packed fluorescent particles (0.5 μm in diameter) at low SNR. (<bold>a</bold>) Synthetic XLFM raw image (Materials and methods) formed by two layers of fluorescent particles with different z positions. (<bold>b</bold>) Axial resolution at different depths characterized by the minimum separation of two particles in <italic>z</italic>, which can be resolved using the reconstruction algorithm (Materials and methods). (<bold>c</bold>) Left, reconstructed examples of X-Z projections of two particles located at different z positions (−70 μm, −30 μm, 30 μm, 70 μm) with different axial separations (6 μm, 5 μm, 5 μm, 6 μm); right, extracted intensity profiles of these examples.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp10-v2"/></fig><fig id="fig1s11" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.014</object-id><label>Figure 1—figure supplement 11.</label><caption><title>Dependence of imaging resolution on the sparseness of the sample.</title><p>Characterization of the dependence of imaging resolution on the sparseness of the sample using computer simulation. (<bold>a</bold>) Maximum intensity projections (MIPs) of a numerically simulated (top) and reconstructed (bottom) larval zebrafish with randomly distributed active neurons. Red and green lines indicate positions where simulated (red) and reconstructed (green) cross-sections are compared. We assumed that the total number of neurons in the zebrafish brain is 80,000, and gradually increased the sparseness index <italic>ρ</italic>, the fraction of neurons activated at a given frame. (<bold>b</bold>–<bold>d</bold>) Characterization of the reconstruction results for different <italic>ρ</italic>. Insets are magnified views of rectangular regions. Red and green dots are simulated and reconstructed neurons, respectively.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp11-v2"/></fig><fig id="fig1s12" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.015</object-id><label>Figure 1—figure supplement 12.</label><caption><title>Characterization of photobleaching in fluorescence imaging by XLFM.</title><p>Photobleaching was characterized by a total fluorescence intensity change of five 5 dpf zebrafish larval with nucleus-localized GCamp6f (huc:h2b-gcamp6f). Each fish was embedded in 1% agarose and continuously exposed to 2.5 mW/mm<sup>2</sup> fluorescence excitation laser (488 nm) illumination. After ~100 min, corresponding to 300,000 volumes with a volume rate of 50 volumes/s, total fluorescence intensity dropped to half of that at the starting point. Random spikes corresponded to spontaneous neural activity. Fish were alive and swam normally when they were relieved from the agarose after imaging.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig1-figsupp12-v2"/></fig></fig-group><p>We first characterized the XLFM by imaging 0.5 μm diameter fluorescent beads. In our design, the system had ~ Ø800 μm in plane coverage (Ø is the diameter of the lateral field of view) and more than 400 μm depth of view, within which an optimal resolution of 3.4 μm × 3.4 μm × 5 μm could be achieved over a depth of 200 μm (<xref ref-type="fig" rid="fig1s6">Figure 1—figure supplements 6</xref> and <xref ref-type="fig" rid="fig1s7">7</xref>, Materials and methods). In the current implementation, however, the imaging performance suffered from the variation in the focal length of the micro-lenses (<xref ref-type="fig" rid="fig1s8">Figure 1—figure supplement 8</xref>), which led to spatial variance of the PSF. As a result, the reconstruction performance and the achievable optimal resolution were shown to degrade beyond the volume of Ø500 μm × 100 μm (<xref ref-type="fig" rid="fig1s9">Figure 1—figure supplements 9</xref> and <xref ref-type="fig" rid="fig1s10">10</xref>). To minimize the reconstruction time while assuring whole brain coverage (~250 μm thick), all imaging reconstructions were carried out over a volume of Ø800 μm × 400 μm.</p><p>We next characterized the imaging performance by considering more fluorescent light sources distributed within the imaging volume. The achievable optimal resolution depends on the sparseness of the sample, because the information captured by the image sensor was insufficient to assign independent values for all voxels in the entire reconstructed imaging volume. Given the total number of neurons (~80,000 [<xref ref-type="bibr" rid="bib21">Hill et al., 2003</xref>]) in a larval zebrafish brain, we next introduced a sparseness index <italic>ρ</italic>, defined as the fraction of neurons in the brain active at a given instant, and used numerical simulation and our reconstruction algorithm to characterize the dependence of achievable resolution on <italic>ρ</italic>. We identified a critical <italic>ρ<sub>c</sub></italic> ≈ 0.11, below which active neurons could be resolved at the optimal resolution (<xref ref-type="fig" rid="fig1s11">Figure 1—figure supplement 11b</xref>). As <italic>ρ</italic> increased, closely clustered neurons could no longer be well resolved (<xref ref-type="fig" rid="fig1s11">Figure 1—figure supplement 11c–d</xref>). Therefore, sparse neural activity is a prerequisite in XLFM for resolving individual neurons at the optimal resolution. Moreover, the above characterization assumed an aberration and scattering free environment; complex optical properties of biological tissue could also degrade the resolution (<xref ref-type="bibr" rid="bib24">Ji, 2017</xref>).</p><p>We demonstrated the capabilities of XLFM by imaging the whole brain neuronal activities of a larval zebrafish (5 d post-fertilization [dpf]) at a speed of 77 volumes/s and relatively low excitation laser exposure of 2.5 mW/mm<sup>2</sup> (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, <xref ref-type="video" rid="video1">Video 1</xref>). The fluorescent intensity loss due to photobleaching reached ~50% when the zebrafish, which expressed pan-neuronal nucleus-labelled GCamp6f (huc:h2b-gcamp6f), was imaged continuously for ~100 min and over more than 300,000 volumes (<xref ref-type="fig" rid="fig1s12">Figure 1—figure supplement 12</xref>, <xref ref-type="video" rid="video2">Video 2</xref> and <xref ref-type="video" rid="video3">3</xref>). To test whether XLFM could monitor fast changes in neuronal dynamics across the whole brain at high resolution (close to single neuron level), we first presented the larval zebrafish, restrained in low melting point agarose, with visual stimulation (~2.6 s duration). We found that different groups of neurons in the forebrain, midbrain, and hindbrain were activated at different times (<xref ref-type="fig" rid="fig1">Figure 1e–f</xref>, <xref ref-type="video" rid="video1">Video 1</xref> and <xref ref-type="video" rid="video4">4</xref>), suggesting rapid sensorimotor transformation across different brain regions.</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-28158-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.28158.016</object-id><label>Video 1.</label><caption><title>Whole brain functional imaging of larval zebrafish under light stimulation.</title><p>Whole brain XLFM imaging of a 5 dpf agarose-embedded larval zebrafish expressing nucleus-localized GCamp6f (huc:h2b-gcamp6f). Light stimulation was introduced at time point t = 0. Whole brain activity was recorded at 77 volumes/s.</p></caption></media><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-28158-video2.mp4"><object-id pub-id-type="doi">10.7554/eLife.28158.017</object-id><label>Video 2.</label><caption><title>Whole brain functional imaging of spontaneous activities of larval zebrafish.</title><p>Whole brain XLFM imaging of a 5 dpf agarose-embedded larval zebrafish expressing nucleus-localized GCamp6f (huc:h2b-gcamp6f). Spontaneous neural activity was recorded at 0.6 volumes/s.</p></caption></media><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-28158-video3.mp4"><object-id pub-id-type="doi">10.7554/eLife.28158.018</object-id><label>Video 3.</label><caption><title>Whole brain functional imaging of spontaneous activities of larval zebrafish.</title><p>Whole brain XLFM imaging of a 5 dpf agarose-embedded larval zebrafish expressing cytoplasm-labeled GCamp6s (huc:gcamp6s). Spontaneous neural activity was recorded at 0.6 volumes/s.</p></caption></media><media id="video4" mime-subtype="mp4" mimetype="video" xlink:href="elife-28158-video4.mp4"><object-id pub-id-type="doi">10.7554/eLife.28158.019</object-id><label>Video 4.</label><caption><title>Whole brain functional imaging of larval zebrafish under light stimulation.</title><p>Whole brain XLFM imaging of a 5 dpf agarose-embedded larval zebrafish expressing cytoplasm-labeled GCamp6s (huc:gcamp6s). Light stimulation was introduced at time point t = 0. Whole brain activity was recorded at 50 volumes/s.</p></caption></media><p>To track freely swimming larval zebrafish, we transferred fish into a water-filled chamber with a glass ceiling and floor. The 20 mm × 20 mm × 0.8 mm-sized chamber was coupled with a piezo actuator and mounted on a high-speed 2D motorized stage (<xref ref-type="fig" rid="fig2">Figure 2</xref>). A tracking camera monitored the lateral movement of the fish, and an autofocus camera, which captured light field images, monitored the axial movement of the fish head (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.28158.020</object-id><label>Figure 2.</label><caption><title>System schematics that integrated tracking, whole brain functional imaging, and real time behavioral analysis.</title><p>Larval zebrafish swam in a customized chamber with an optically transparent ceiling and floor. The water-filled chamber was mounted on a high-speed three-axis stage (PI M686 and PI P725KHDS). Customized LED rings generated dark field illumination of the zebrafish. The scattered light was collected by four cameras: two cameras below the chamber were used for <italic>x-y</italic> plane tracking and low magnification real-time (RT) analysis, respectively; two cameras above the chamber and after the imaging objective were used for Z autofocus and high magnification RT analysis. The positional information of the larval zebrafish, acquired from the tracking and autofocus system, was converted to feedback voltage signals to drive the three-axis stage and to compensate for fish movement. The functional imaging system, described in <xref ref-type="fig" rid="fig1">Figure 1</xref>, shared the same imaging objective placed above the swimming chamber. The 3D tracking, RT behavioral analysis, and functional imaging system were synchronized for accurate correlation between neural activity and behavioral output.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.28158.021</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Characterization of the autofocus system.</title><p>(<bold>a</bold>) Autofocus camera behind a one-dimensional lenslet array captured triplet images of the fish head (up). Its autocorrelation function was computed (bottom). (<bold>b</bold>) Central line profile of the autocorrelation function was extracted and inter-fish distance was computed as local maximums in the autocorrelation function. (<bold>c</bold>) Axial shift of the fish head, calibrated by moving the piezo at a constant interval, changed linearly (red line) with inter-fish distance.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig2-figsupp1-v2"/></fig></fig-group><p>Real-time machine vision algorithms allowed quick estimate of lateral (within 1 ms) and axial (~5 ms) head positions (see Materials and methods). The error signals in three dimensions, defined as the difference between the head position and set point, were calculated (<xref ref-type="fig" rid="fig3">Figure 3a</xref>) and converted to analog voltage signals through proportional-integral-derivative (PID) control to drive the motorized stage and z-piezo scanner. Tracking and autofocusing allowed for rapid compensation of 3D fish movement (300 Hz in x and y, 100 Hz in z, <xref ref-type="fig" rid="fig3">Figure 3a</xref>) and retainment of the fish head within the field of view of the imaging objective.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.28158.022</object-id><label>Figure 3.</label><caption><title>3D tracking of larval zebrafish.</title><p>(<bold>a</bold>) Representative time varying error signals in three dimensions, defined as the difference between real head position and set point. Inset provides magnified view at short time interval. Lateral movement can be rapidly compensated for within a few milliseconds with an instantaneous velocity of up to 10 mm/s. The axial shift was small compared with the depth coverage (200 μm) during whole brain imaging, and thereby had minor effect on brain activity reconstruction. (<bold>b</bold>) Tracking images at four time points during prey capture behavior, acquired at low (left) and high (right) magnification simultaneously. Scale bars are 1 mm (left) and 200 μm (right). (<bold>c</bold>) Kinematics of behavioral features during prey capture. Shaded region marks the beginning and end of the prey capture process.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig3-v2"/></fig><p>Our tracking system permitted high-speed and high-resolution recording of larval zebrafish behaviors. With two cameras acquiring head and whole body videos simultaneously (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig3">Figure 3b</xref>), we recorded and analyzed in real time (see Materials and methods) the kinematics of key features during larval zebrafish prey capture (<xref ref-type="fig" rid="fig3">Figure 3b and c</xref>, <xref ref-type="video" rid="video5">Video 5</xref> and <xref ref-type="video" rid="video6">6</xref>). Consistent with several earlier findings (<xref ref-type="bibr" rid="bib8">Bianco et al., 2011</xref>; <xref ref-type="bibr" rid="bib38">Patterson et al., 2013</xref>; <xref ref-type="bibr" rid="bib49">Trivedi and Bollmann, 2013</xref>), eyes converged rapidly when the fish entered the prey capture state (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). Other features that characterized tail and fin movement were also analyzed at high temporal resolution (<xref ref-type="fig" rid="fig3">Figure 3c</xref>).</p><media id="video5" mime-subtype="mp4" mimetype="video" xlink:href="elife-28158-video5.mp4"><object-id pub-id-type="doi">10.7554/eLife.28158.023</object-id><label>Video 5.</label><caption><title>Tracking of larval zebrafish during prey capture behavior at low resolution</title><p>Tracking and real time kinematic analysis of larval zebrafish during prey capture behavior at low resolution. Recorded at 190 frames/s.</p></caption></media><media id="video6" mime-subtype="mp4" mimetype="video" xlink:href="elife-28158-video6.mp4"><object-id pub-id-type="doi">10.7554/eLife.28158.024</object-id><label>Video 6.</label><caption><title>Tracking of larval zebrafish during prey capture behavior at high resolution.</title><p>Tracking and real time kinematic analysis of larval zebrafish during prey capture behavior at high resolution. Recorded at 160 frames/s.</p></caption></media><p>The integration of the XLFM and 3D tracking system allowed us to perform whole brain functional imaging of a freely behaving larval zebrafish (<xref ref-type="fig" rid="fig2">Figure 2</xref>). We first replicated the light-evoked experiment (similar to <xref ref-type="fig" rid="fig1">Figure 1</xref>), albeit in a freely behaving zebrafish with pan-neuronal cytoplasm-labeled GCaMP6s (huc:gcamp6s), which exhibited faster and more prominent calcium response (<xref ref-type="video" rid="video7">Video 7</xref>). Strong activities were observed in the neuropil of the optical tectum and the midbrain after stimulus onset. The fish tried to avoid strong light exposure and made quick tail movement at ~60 Hz. Whole brain neural activity was monitored continuously during the light-evoked behavior, except for occasional blurred frames due to the limited speed and acceleration of the tracking stage.</p><media id="video7" mime-subtype="mp4" mimetype="video" xlink:href="elife-28158-video7.mp4"><object-id pub-id-type="doi">10.7554/eLife.28158.025</object-id><label>Video 7.</label><caption><title>Whole brain functional imaging of a freely swimming larval zebrafish under light stimulation</title><p>Whole brain XLFM imaging of a 7 dpf freely swimming larval zebrafish expressing cytoplasm-labeled GCamp6s (huc:gcamp6s). Light stimulation was introduced at time point t = 0. Whole brain activities were recorded at 77 volumes/s and with a flashed excitation laser under 0.3 ms exposure time.</p></caption></media><p>Next, we captured whole brain neural activity during the entire prey capture process in freely swimming larval zebrafish (huc:gcamp6s, <xref ref-type="video" rid="video8">Video 8</xref>). When a paramecium moved into the visual field of the fish, groups of neurons, indicated as group one in <xref ref-type="fig" rid="fig4">Figure 4b</xref>, near the contralateral optical tectum of the fish were first activated (t<sub>1</sub>). The fish then converged its eyes onto the paramecium and changed its heading direction in approach (t<sub>2</sub>). Starting from t<sub>2</sub>, several groups of neurons in the hypothalamus, midbrain, and hindbrain, highlighted as groups two, three, and four in <xref ref-type="fig" rid="fig4">Figure 4b</xref>, were activated. It took the fish three attempts (<xref ref-type="fig" rid="fig4">Figure 4c</xref>) to catch and eat the paramecium. After the last try (t<sub>4</sub>), neuron activity in group one decreased gradually, whereas activities in the other groups of neurons continued to rise and persisted for ~1 s before the calcium signals decreased. The earliest tectal activity (group 1) responsible for prey detection found here is consistent with previous studies (<xref ref-type="bibr" rid="bib45">Semmelhack et al., 2014</xref>; <xref ref-type="bibr" rid="bib10">Bianco and Engert, 2015</xref>). Moreover, our data revealed interesting neural dynamics arising from other brain regions during and after successful prey capture. We also monitored similar behavior in a zebrafish expressing nucleus-localized GCamp6f (huc:h2b-gcamp6f) with better resolution but less prominent calcium response (<xref ref-type="video" rid="video9">Video 9</xref>).</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.28158.026</object-id><label>Figure 4.</label><caption><title>Whole brain imaging of larval zebrafish during prey capture behavior.</title><p>(<bold>a</bold>) Renderings of whole brain calcium activity at six time points (up) and the corresponding behavioral images (bottom). Features used to quantify behavior were: fish-paramecium azimuth <italic>α</italic>; convergence angle between eyes β; head orientation γ; and fish-paramecium distance <italic>d</italic>. (<bold>b</bold>) Maximum intensity projections of zebrafish brain with pan-neuronal cytoplasm-labeled GCaMP6s (huc:gcamp6s). Boundaries of four brain regions are color marked. (<bold>c</bold>) Neural dynamics inferred from GCaMP6 fluorescence changes in these four regions during the entire prey capture behavior (up) and the kinematics of behavioral features (bottom). Note that between t2 and t4, fish-paramecium distance <italic>d</italic> exhibits three abrupt kinks, representing the three attempts to catch prey.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-28158-fig4-v2"/></fig><media id="video8" mime-subtype="mp4" mimetype="video" xlink:href="elife-28158-video8.mp4"><object-id pub-id-type="doi">10.7554/eLife.28158.027</object-id><label>Video 8.</label><caption><title>Whole brain functional imaging of a freely swimming larval zebrafish during prey capture behavior.</title><p>Whole brain XLFM imaging of an 11 dpf freely swimming larval zebrafish expressing cytoplasm-labeled GCamp6s (huc:gcamp6s). The entire process during which the larval zebrafish caught and ate the paramecium was recorded.</p></caption></media><media id="video9" mime-subtype="mp4" mimetype="video" xlink:href="elife-28158-video9.mp4"><object-id pub-id-type="doi">10.7554/eLife.28158.028</object-id><label>Video 9.</label><caption><title>Whole brain functional imaging of a freely swimming larval zebrafish during prey capture behavior.</title><p>Whole brain XLFM imaging of a 7 dpf freely swimming larval zebrafish expressing nucleus-localized GCamp6f (huc:h2b-gcamp6f). The entire process during which the larval zebrafish caught and ate the paramecium was recorded.</p></caption></media></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Whole brain imaging in freely behaving animals has been previously reported in <italic>Caenorhabditis elegans</italic>, by integrating spinning-disk confocal microscopy with a 2D tracking system (<xref ref-type="bibr" rid="bib50">Venkatachalam et al., 2016</xref>; <xref ref-type="bibr" rid="bib33">Nguyen et al., 2016</xref>). In the more remote past, Howard Berg pioneered the use of 3D tracking microscopy to study bacteria chemotaxis (<xref ref-type="bibr" rid="bib7">Berg, 1971</xref>). However, the significant increase of animal size imposes challenges both in tracking and imaging technologies. The XLFM, derived from the general concept of light field imaging (<xref ref-type="bibr" rid="bib12">Broxton et al., 2013</xref>; <xref ref-type="bibr" rid="bib2">Adelson and Wang, 1992</xref>; <xref ref-type="bibr" rid="bib32">Ng et al., 2005</xref>; <xref ref-type="bibr" rid="bib26">Levoy et al., 2006</xref>), overcomes several critical limitations of conventional LFM and allows optimization of imaging volume, resolution, and speed simultaneously. Furthermore, it can be perfectly combined with flashed fluorescence excitation to capture blur-free images at high resolution during rapid fish movement. Taken together, we have developed a volume imaging and tracking microscopy system suitable for observing and capturing freely behaving larval zebrafish, which have ~80,000 neurons and can move two orders of magnitude faster than <italic>C. elegans</italic>.</p><p>Tracking and whole brain imaging of naturally behaving zebrafish provide an additional way to study sensorimotor transformation across the brain circuit. A large body of research suggests that sensory information processing depends strongly on the locomotor state of an animal (<xref ref-type="bibr" rid="bib34">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="bib29">Maimon et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">Chiappe et al., 2010</xref>). The ability to sense self-motion, such as proprioceptive feedback (<xref ref-type="bibr" rid="bib39">Pearson, 1995</xref>) and efferent copy (<xref ref-type="bibr" rid="bib6">Bell, 1981</xref>), can also profoundly shape the dynamics of the neural circuit and perception. To explore brain activity in swimming zebrafish, several studies have utilized an elegant tail-free embedding preparation (<xref ref-type="bibr" rid="bib46">Severi et al., 2014</xref>; <xref ref-type="bibr" rid="bib41">Portugues and Engert, 2011</xref>; <xref ref-type="bibr" rid="bib42">Portugues et al., 2014</xref>), in which only the head of the fish is restrained in agarose for functional imaging. Nevertheless, it would be ideal to have physiological access to all neurons in defined behavioral states, where all sensory feedback loops remain intact and functional. Our XLFM-3D tracking system is one step towards this goal, and could be better exploited to explore the neural basis of more sophisticated natural behaviors, such as prey capture and social interaction, where the integration of multiple sensory feedbacks becomes critical.</p><p>In the XLFM, the camera sensor size limited the number of voxels and hence the number of neurons that could be reliably reconstructed. Our simulation suggested that the sparseness of neuronal activities is critical for optimal imaging volume reconstruction. A growing body of experimental data indeed suggests that population neuronal activities are sparse (<xref ref-type="bibr" rid="bib22">Hromádka et al., 2008</xref>; <xref ref-type="bibr" rid="bib13">Buzsáki and Mizuseki, 2014</xref>) and sparse representation is useful for efficient neural computation (<xref ref-type="bibr" rid="bib36">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib37">Olshausen and Field, 2004</xref>). Given the total number of neurons in the larval zebrafish brain, we found that when the fraction of active neurons in a given imaging frame was less than <italic>ρ<sub>c</sub></italic> ≈ 0.11, individual neurons could be resolved at optimal resolution. When population neural activity was dense (<italic>e.g</italic>., neurons have high firing rate and firing patterns have large spatiotemporal correlation), we obtained a coarse-grained neural activity map with reduced resolution.</p><p>To retain the fish head within the field of view of the imaging objective, our tracking system compensated for fish movement by continuously adjusting the lateral positions of the motorized stage. As a result, self-motion perceived by the fish was not exactly the same as that during natural behaviors. The linear acceleration of the swimming fish, encoded by vestibular feedback, was significantly underestimated. The perception of angular acceleration during head orientation remained largely intact. The relative flow velocity along the fish body, which was invariant upon stage translation, can still be detected by specific hair cells in the lateral line system (<xref ref-type="bibr" rid="bib16">Coombs, 2014</xref>; <xref ref-type="bibr" rid="bib27">Liao, 2010</xref>). Together, the interpretation of brain activity associated with self-motion must consider motion compensation driven by the tracking system.</p><p>Both tracking and imaging techniques can be improved in the future. For example, the current axial displacement employed by the piezo scanner had a limited travelling range (400 µm), and our swimming chamber essentially restrained the movement of the zebrafish in two dimensions. This limitation could be relaxed by employing axial translation with larger travelling range and faster dynamics. Furthermore, to avoid any potential disturbance of animal behaviors, it would be ideal if the imaging system moved, instead of the swimming chamber.</p><p>In XLFM, the performance degradation caused by focal length variation of the micro-lenses could be resolved by higher precision machining. In addition, the capability of XLFM could be further improved with the aid of technology development in other areas. With more pixels on the imaging sensor, we could resolve more densely labelled samples, and achieve higher spatial resolution without sacrificing imaging volume coverage by introducing more than two different focal planes formed by more groups of micro-lenses. With better imaging objectives that could provide higher numerical aperture and larger field of view at the same time, we could potentially image the entire nervous system of the larval zebrafish with single neuron resolution in all three dimensions. Additionally, the fast imaging speed of XLFM holds the potential for recording electrical activity when high signal-to-noise ratio (SNR) fluorescent voltage sensors become available (<xref ref-type="bibr" rid="bib47">St-Pierre et al., 2014</xref>). Finally, the illumination-independent characteristic of XLFM is perfectly suitable for recording brain activities from bioluminescent calcium/voltage indicators in a truly natural environment, where light interference arising from fluorescence excitation can be eliminated (<xref ref-type="bibr" rid="bib31">Naumann et al., 2010</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>XLFM</title><p>The imaging system (<xref ref-type="fig" rid="fig1">Figure 1</xref>) was a customized upright microscope. Along the fluorescence excitation light path, a blue laser (Coherent, OBIS 488 nm, 100 mW, USA) was expanded and collimated into a beam with a diameter of ~25 mm. It was then focused by an achromatic lens (focal length: 125 mm) and reflected by a dichroic mirror (Semrock, Di02-R488−25×36, USA) into the back pupil of the imaging objective (Olympus, XLPLN25XWMP2, 25X, NA 1.05, WD 2 mm, Japan) to result in an illumination area of ~1.44 mm in diameter near the objective’s focal plane. In the fluorescence imaging light path, excited fluorescence was collected by the imaging objective and transmitted through the dichroic mirror. A pair of achromatic lenses (focal lengths: F1 = 180 mm and F2 = 160 mm), arranged in 2F1 +2F2, were placed after the objective and dichroic mirror to conjugate the objective’s back pupil onto a customized lenslet array (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The customized lenslet array was an aluminum plate with 27 holes (1.3 mm diameter aperture on one side and 1 mm diameter aperture on the other side, <xref ref-type="supplementary-material" rid="scode1">Source code file 1</xref>) housing 27 customized micro-lenses (1.3 mm diameter, focal length: 26 mm). The 27 micro-lenses were divided into two groups (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) and an axial displacement of 2.5 mm was introduced between them. Apertures of 1 mm diameter on the aluminum plate were placed right at the objective’s pupil plane so that all micro-lenses samples light at pupil plane even though they were displaced axially after apertures. Due to the blockage of light by the aluminum micro-lenses housing, 16% of the light after a 1.05 NA imaging objective was effectively collected by the camera. This efficiency is equivalent to using a 0.4 NA imaging objective. Finally, the imaging sensor of a sCMOS camera (Hamamatsu, Orca-Flash 4.0 v2, Japan) was placed at the middle plane between two focal planes formed by two different groups of micro-lenses. The total magnification of the imaging system was ~4, so one camera pixel (6.5 µm) corresponded to ~1.6 µm on the sample.</p><p>We developed a computational algorithm for 3D volume reconstruction, which required an accurately measured PSF (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). The PSF was measured by recording images of a 500 nm diameter fluorescent bead sitting on a motorized stage under the objective. A stack of 200 images was recorded when the bead was scanned with a step size of 2 µm in the axial direction from 200 µm below the objective’s focal plane to 200 µm above. Since the images formed by two different groups of micro-lenses were from different axial locations and had different magnifications, the measured raw PSF data were reorganized into two complementary parts: PSF_A and PSF_B (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>), according to the spatial arrangement of the micro-lenses. We took PSF_A stack, PSF_B stack, and a single frame of a raw image (2048 × 2048 pixels) as inputs, and applied a newly developed algorithm to reconstruct the 3D volume.</p></sec><sec id="s4-2"><title>Image reconstruction of XLFM</title><p>The reconstruction algorithm was derived from the Richardson-Lucy deconvolution. The goal was to reconstruct a 3D fluorescent object from a 2D image:<disp-formula id="equ1"><mml:math id="m1"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>The algorithm assumes that the real 3D object can be approximated by a discrete number of <italic>x-y</italic> planes at different <italic>z</italic> positions:<disp-formula id="equ2"><mml:math id="m2"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mfenced><mml:mo>~</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>The numbers and positions of these planes can be arbitrary, yet the Nyquist sampling rate should be chosen to optimize the speed and accuracy of the reconstruction.</p><p>As the imaging system consisted of two different groups of micro-lenses (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), their PSFs (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>) each consisted of a stack of planes that were measured at the same chosen axial positions <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ3"><mml:math id="m3"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>Although the PSF was measured in imaging space, here we denote <italic>x</italic> and <italic>y</italic> as coordinates in object space to follow conventions in optical microscopy. Here and below, the combination of <italic>PSF<sub>A</sub></italic> and <italic>PSF<sub>B</sub></italic> is the total PSF.</p><p>Additionally, the images formed by two different groups of micro-lenses had different magnifications, which could be determined experimentally. The ratio between two different magnifications can be defined as:<disp-formula id="equ4"><mml:math id="m4"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi> <mml:mi/><mml:mi>o</mml:mi><mml:mi>f</mml:mi> <mml:mi/><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi> <mml:mi/><mml:mi>A</mml:mi> <mml:mi/><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi> <mml:mi/><mml:mi>o</mml:mi><mml:mi>f</mml:mi> <mml:mi/><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi> <mml:mi/><mml:mi>B</mml:mi> <mml:mi/><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Then, the captured image on the camera can be estimated as:<disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⨂</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⨂</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ6"><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The operator <inline-formula><mml:math id="inf5"><mml:mo>⨂</mml:mo></mml:math></inline-formula> represents 2D convolution. Here, <italic>x</italic> and <italic>y</italic> on the left hand side of the equation also represent coordinates in object space so that 2D convolution was carried out in the same coordinates.</p><p>The goal of the algorithm is to estimate the <inline-formula><mml:math id="inf6"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> from the measured camera frame:<disp-formula id="equ7"><mml:math id="m7"><mml:msub><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>According to the Richardson-Lucy deconvolution algorithm, the iterative reconstruction can be expressed as:<disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⨂</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⨂</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>⨂</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>⨂</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ11"><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ12"><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>x</mml:mi><mml:mi>γ</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mi>y</mml:mi><mml:mi>γ</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf7"><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> is the weighting factor at different axial positions. The choice of <inline-formula><mml:math id="inf8"><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> can be arbitrary. Because the resolutions achieved by different groups of micro-lenses at different z positions were not the same, the weighting factor can take this effect into consideration by weighing higher quality information more than lower quality information. One simple choice is <inline-formula><mml:math id="inf9"><mml:mi>w</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, that is, to weigh information from two groups of micro-lenses equally.</p><p>The starting estimate of the object can be any non-zero value. Near the end of the iterations, <inline-formula><mml:math id="inf10"><mml:msubsup><mml:mrow><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf11"><mml:msubsup><mml:mrow><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> are interchangeable, except with different magnifications. Either can be used as the resulting estimate of the 3D object.</p><p>In XLFM, together with its reconstruction algorithm, the diffraction of the 3D light field is properly considered by experimentally measured PSF. The raw imaging data can be fed into the algorithm directly without any preprocessing. Given that the PSF is spatially invariant, which is satisfied apart from small aberrations, the algorithm can handle overlapping fish images (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>). As a result, the field of view can be increased significantly. The reconstruction algorithm was typically terminated after 30 iterations when modifications in the estimated object became very small. The computation can speed up significantly via GPU. It took about 4 min to reconstruct one 3D volume using a desktop computer with a GPU (Nvidia Titan X). In comparison, the reconstruction ran ~20 × slower using a CPU (Intel E5-2630v2) on a Dell desktop. The source code written in MATLAB can be found in the <xref ref-type="supplementary-material" rid="scode2">Source code file 2</xref>.</p><p>The 3D deconvolution method has been developed for conventional LFM (<xref ref-type="bibr" rid="bib12">Broxton et al., 2013</xref>). Our method differs from <xref ref-type="bibr" rid="bib12">Broxton et al. (2013</xref>) in several ways. (1) The optical imaging systems are different. (2) The definitions of PSFs are different. Ours defines a spatially <italic>invariant</italic> PSF (see below for detailed characterization), whereas <xref ref-type="bibr" rid="bib12">Broxton et al. (2013</xref>) defined a spatially variant PSF, leading to increased computational complexity in the deconvolution algorithm. (3) The PSF in <xref ref-type="bibr" rid="bib12">Broxton et al. (2013</xref>) was simulated based on a model derived from an ideal imaging system, whereas ours was measured experimentally. Furthermore, our system took practical conditions, such as a non-ideal imaging objective, actual positions of microlenses, the spectrum of received fluorescence signal <italic>et al</italic>., into consideration.</p></sec><sec id="s4-3"><title>Characterization of the spatial invariance of PSF in XLFM</title><p>The definition of a 2D spatially invariant PSF fundamentally means that in an ideal optical microscopy system, the resulting image can be described as a 2D convolution between object and PSF. As discussed in the previous section, this operation forms the basis of our reconstruction algorithm.</p><p>One of the fundamental differences between XLFM and conventional LFM is the location of the microlens array. In XLFM, the microlens array is placed at the pupil plane and the image sensor is at imaging plane, whereas in conventional LFM, the microlens array is placed at the image plane and the image sensor is| at pupil plane. It is possible to define a spatially invariant PSF in XLFM because:</p><list list-type="order"><list-item><p>Spatially invariant PSFs can be defined for individual sub-imaging systems consisting of different micro-lenses.</p></list-item><list-item><p>A spatially invariant PSF can be defined for the entire imaging system if the magnifications of all sub-imaging systems are the same.</p></list-item></list><p>By definition, the imaging formation in an ideal optical imaging system is linear and spatially invariant, so spatially invariant PSFs for sub-imaging systems consisting of micro-lens <italic>A1</italic> and <italic>A2</italic> can be defined as:<disp-formula id="equ13"><mml:math id="m13"><mml:msub><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>⨂</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ14"><mml:math id="m14"><mml:msub><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>⨂</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are sub-images behind individual micro-lens. If we perform the convolution in the imaging space, the coordinates of <inline-formula><mml:math id="inf13"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo> <mml:mi/></mml:math></inline-formula> should be scaled by the magnification factors of their sub-image systems, respectively. Now if the magnifications of different sub-image systems are the same, the summation of all PSFs formed by individual micro-lenses can be defined as a single PSF. In other words,<disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>⨂</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>⨂</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ16"><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Experimentally, the small variation of individual micro-lenses’ focal length (<xref ref-type="fig" rid="fig1s8">Figure 1—figure supplement 8</xref>) resulted in spatial variance of <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/></mml:math></inline-formula> but it does not affect the imaging formation theory of XLFM. The spatial variance led to degraded reconstruction performance, as shown in <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref>. This degradation was negligible near the center of the field of view, but became more evident near the edge of the field of view. This is because the PSF was measured near the center of the field of view. The reconstruction algorithm produces 27 estimates of the same object based on 27 sub-images. In the meanwhile, it tries to combine and align these estimates all together in the same coordinates. The position where the PSF is measured determines the origin of this coordinates. If the magnifications of different micro-lenses are different, the reconstruction will yield an image that is clear near the origin of the coordinates but blurred at the edge, as shown in in <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref>.</p></sec><sec id="s4-4"><title>Resolution characterization of XLFM</title><p>Unlike conventional microscopy, where the performance of the imaging system is fully characterized by the PSF at the focal plane, the capability of XLFM is better characterized as a function of positions throughout the imaging volume.</p><p>We first characterized the spatial resolution in the <italic>x-y</italic> plane by analyzing the spatial frequency support of the experimentally measured PSF from individual micro-lenses using a 0.5 µm diameter fluorescent bead. The optical transfer function (OTF), which is the Fourier transform of the PSF in the <italic>x-y</italic> plane, was extended to a spatial frequency of ~1/3.4 µm<sup>−1</sup> (<xref ref-type="fig" rid="fig1s6">Figure 1—figure supplement 6</xref>), a result that agreed well with the designed resolution at 3.4 μm, given that the equivalent NA of individual micro-lenses was 0.075.</p><p>The lateral resolution, measured from the raw PSF behind individual micro-lenses, was preserved across the designed cylindrical imaging volume of Ø800 μm × 200 μm (<xref ref-type="fig" rid="fig1s6">Figure 1—figure supplement 6</xref>). However, the reconstruction results (<xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref>), which used total PSF (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), exhibited resolution degradation when the fluorescent bead was placed more than 250 μm away from the center (<xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref>). This discrepancy resulted from the variation in focal length of the micro-lenses (<xref ref-type="fig" rid="fig1s8">Figure 1—figure supplement 8</xref>), which, in turn, led to spatial variance of the defined <italic>PSF<sub>A</sub></italic> and <italic>PSF<sub>B</sub></italic>. In principle, the designed lateral resolution of 3.4 µm could be preserved over a volume of Ø800 μm × 200 μm by reducing focal length variation to below 0.3%</p><p>We next characterized the axial resolution of the XLFM. The XLFM gained axial resolution by viewing the object from large projection angles achieved by micro-lenses sitting near the edge of the objective’s back pupil plane. For example, if two points of light source were located at the same position in the <italic>x-y</italic> plane, but were separated by <inline-formula><mml:math id="inf16"><mml:mo>∆</mml:mo><mml:mi>z</mml:mi></mml:math></inline-formula> in the axial direction, then one micro-lens in the XLFM could capture an image of these two points with a shift between them. The shift can be determined as:<disp-formula id="equ17"><mml:math id="m17"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mo>∆</mml:mo><mml:mi>z</mml:mi><mml:mi>*</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>θ</mml:mi></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf17"><mml:mi>θ</mml:mi></mml:math></inline-formula> is the inclination angle inferred from the measured PSF (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). If the two points in the image can be resolved, the two points separated by <inline-formula><mml:math id="inf18"><mml:mo>∆</mml:mo><mml:mi>z</mml:mi></mml:math></inline-formula> can be resolved by the imaging system. Since a micro-lens sitting in the outer layer of the array offered the largest inclination angle of 40 degree in our system, the axial resolution <inline-formula><mml:math id="inf19"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">z</mml:mi></mml:math></inline-formula> can be directly calculated as:<disp-formula id="equ18"><mml:math id="m18"><mml:mi>d</mml:mi><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>3.4</mml:mn> <mml:mi/><mml:mi>μ</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mn>40</mml:mn><mml:mo>°</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>4</mml:mn> <mml:mi/><mml:mi>μ</mml:mi><mml:mi>m</mml:mi></mml:math></disp-formula></p><p>The best way to confirm the theoretical estimate is to image two fluorescent beads with precisely controlled axial separations. However, this is technically very challenging. Instead, we pursued an alternative method that is equivalent to imaging two beads simultaneously:</p><list list-type="order"><list-item><p>We took a z stack of images of fluorescent beads, as done in measuring the PSF.</p></list-item><list-item><p>In post processing, we added two images from different z positions to mimic the beads being present simultaneously at two different <italic>z</italic> positions.</p></list-item></list><p>The above method allowed us to experimentally characterize the axial resolution afforded by individual micro-lenses focusing at different z positions. We used a single fluorescent bead (0.5 <inline-formula><mml:math id="inf20"><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula> in diameter) with a high SNR (<xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7a</xref>). We imaged at different axial positions: <italic>z</italic> = −100 <inline-formula><mml:math id="inf21"> <mml:mi mathvariant="normal"/><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula>, <italic>z</italic> = 0 <inline-formula><mml:math id="inf22"><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula>, and <italic>z</italic> = 100<inline-formula><mml:math id="inf23"> <mml:mi mathvariant="normal"/><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7b</xref>). The third column is the combined images in column 1 and 2. The capability of resolving the two beads in the third column can be demonstrated by spatial frequency analysis (fourth column in <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7b</xref>). The two line dips, indicating the existence of two beads instead of one rod in the fourth column, were confirmations of the resolving capability. This becomes more evident after deconvolution of the raw images (fifth column in <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7b</xref>). Micro-lenses 1 and 2 could resolve two beads, separated by 5 <inline-formula><mml:math id="inf24"><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula>, within the range of <inline-formula><mml:math id="inf25"><mml:mo>-</mml:mo><mml:mn>100</mml:mn><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/></mml:math></inline-formula> μm≤z ≤ 0 and <inline-formula><mml:math id="inf26"><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>z</mml:mi><mml:mo>≤</mml:mo><mml:mn>100</mml:mn> <mml:mi mathvariant="normal"/><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mo>,</mml:mo> <mml:mi mathvariant="normal"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:math></inline-formula>. In other words, the complementary information provided by the two micro-lenses allowed the system to maintain a high axial resolution at 5 <inline-formula><mml:math id="inf27"> <mml:mi/><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula> across a <inline-formula><mml:math id="inf28"><mml:mn>200</mml:mn> <mml:mi mathvariant="normal"/><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula> depth.</p><p>Next, we imaged densely packed fluorescent beads (0.5 <inline-formula><mml:math id="inf29"><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula> in diameter) with a low SNR (<xref ref-type="fig" rid="fig1s10">Figure 1—figure supplement 10a</xref>), and used our reconstruction algorithm to determine the minimum axial separation between beads that could be resolved (<xref ref-type="fig" rid="fig1s10">Figure 1—figure supplement 10b–c</xref>). In this case, 5 <inline-formula><mml:math id="inf30"> <mml:mi/><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula> axial resolution could be preserved across a depth of 100 <inline-formula><mml:math id="inf31"> <mml:mi/><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula>. The resolution decayed gradually to ~10 <inline-formula><mml:math id="inf32"> <mml:mi mathvariant="normal"/><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula> at the edge of an imaging volume with a 400 <inline-formula><mml:math id="inf33"> <mml:mi/><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula> axial coverage (<xref ref-type="fig" rid="fig1s10">Figure 1—figure supplement 10b</xref>). We believe that the optimal axial resolution at 5 µm could be achieved over an axial coverage of 200 μm by minimizing micro-lens focal length variation (<xref ref-type="fig" rid="fig1s8">Figure 1—figure supplement 8</xref>).</p><p>Finally, we characterized how the imaging performance depended upon the sparseness of the sample. Given the total number of neurons (~80,000) in a larval zebrafish brain, we introduced a sparseness index <italic>ρ</italic>, defined as the fraction of neurons in the brain active at an imaging frame, and used numerical simulation to characterize the dependence of achievable resolution on <italic>ρ</italic>. To this end, we simulated a zebrafish larva with uniformly distributed firing neurons (red dots in <xref ref-type="fig" rid="fig1s11">Figure 1—figure supplement 11a</xref>). By convolving the simulated zebrafish with the experimentally measured PSFs (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>), we generated an image that mimicked the raw data captured by the camera. We then reconstructed the simulated neurons from this image, represented by green dots. When <italic>ρ</italic> was equal to or less than 0.11, which corresponded to ~9000 neurons activated at a given instant, all active neurons, including those closely clustered, could be reconstructed with optimal resolution (<xref ref-type="fig" rid="fig1s11">Figure 1—figure supplement 11b</xref> inset). As the sparseness index <italic>ρ</italic> increased, the resolution degraded: nearby neurons merged laterally and elongated axially (<xref ref-type="fig" rid="fig1s11">Figure 1—figure supplement 11c–d</xref>). In all calculations, the Poisson noise was properly considered by assuming that each active neuron emitted 20,000 photons, 2.2% of which were collected by our imaging system.</p><p>In vivo resolution characterization is challenging due to a lack of bright and spot-like features in living animals. Additionally, achievable resolution depends on the optical properties of biological tissues, which can be highly heterogeneous and difficult to infer. The light scattering and aberration induced by biological tissue usually leads to degraded imaging performance (<xref ref-type="bibr" rid="bib24">Ji, 2017</xref>; <xref ref-type="bibr" rid="bib23">Ji et al., 2010</xref>; <xref ref-type="bibr" rid="bib51">Wang et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Wang et al., 2015</xref>).</p></sec><sec id="s4-5"><title><italic>XY</italic> tracking system</title><p>To compensate for lateral fish movement and retain the entire fish head within the field of view of a high NA objective (25×, NA = 1.05), a high-speed camera was used to capture fish motion (2 ms exposure time, 300 fps or higher, Basler aca2000-340kmNIR, Germany). We developed an FPGA-based RT system in LabVIEW that could rapidly identify the head position by processing the pixel stream data within the Cameralink card before the whole image was transferred to RAM. The error signal between the actual head position and the set point was then fed into the PID to generate output signals and control the movement of a high-speed motorized stage (PI M687 ultrasonic linear motor stage, Germany). In the case of large background noise, we alternatively performed conventional imaging processing in C/C++ (within 1 ms delay). The rate-limiting factor of our lateral tracking system was the response time of the stage (~300 Hz).</p></sec><sec id="s4-6"><title>Autofocus system</title><p>We applied the principle of LFM to determine the axial movement of larval zebrafish. The autofocus camera (100 fps or higher, Basler aca2000-340kmNIR, Germany) behind a one-dimensional micro-lens array captured triplet images of the fish from different perspectives (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a</xref>). <italic>Z</italic> motion caused an extension or contraction between the centroids of the fish head in the left and right sub-images, an inter-fish distance (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>) that can be accurately computed from image autocorrelation. The inter-fish distance, multiplied by a pre-factor, can be used to estimate the z position of the fish, as it varies linearly with axial movement (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1c</xref>). The error signal between the actual axial position of the fish head and the set point was then fed into the PID to generate an output signal to drive a piezo-coupled fish container. The feedback control system was written in LabVIEW. The code was further accelerated by parallel processing and the closed loop delay was ~5 ms. The rate-limiting factor of the autofocus system was the settling time of the piezo scanner (PI P725KHDS, Germany, 400 μm travelling distance), which was about 10 ms.</p></sec><sec id="s4-7"><title>Real-time behavioral analysis</title><p>Two high-speed cameras acquired dark-field images at high and low magnification, respectively, and customized machine vision software written in C/C ++ with the aid of OpenCV library was used to perform real-time behavioral analysis of freely swimming larval zebrafish. At high magnification, eye positions, their orientation, and convergence angle were computed; at low magnification, the contour of the whole fish, centerline, body curvature, and bending angle of the tail were computed. The high mag RT analysis was run at ~120 fps and the low mag RT analysis was run at ~180 fps. The source code can be found in the <xref ref-type="supplementary-material" rid="scode3">Source code file 3</xref>.</p></sec><sec id="s4-8"><title>Ethics statement and animal handling</title><p>All animal handling and care were conducted in strict accordance with the guidelines and regulations set forth by the Institute of Neuroscience, Chinese Academy of Sciences, University of Science and Technology of China (USTC) Animal Resources Center, and University Animal Care and Use Committee. The protocol was approved by the Committee on the Ethics of Animal Experiments of the USTC (permit number: USTCACUC1103013).</p><p>All larval zebrafish (huc:h2b-gcamp6f and huc:gcamp6s) were raised in embryo medium under 28.5°C and a 14/10 hr light/dark cycle. Zebrafish were fed with paramecium from 4 dpf. For restrained experiments, 4–6 dpf zebrafish were embedded in 1% low melting point agarose. For freely moving experiments, 7–11 dpf zebrafish with 10% Hank’s solution were transferred to a customized chamber (20 mm in diameter, 0.8 mm in depth), and 10–20 paramecia were added before the chamber was covered by a coverslip.</p></sec><sec id="s4-9"><title>Neural activity analysis</title><p>To extract neural activity induced by visual stimuli (<xref ref-type="fig" rid="fig1">Figure 1e and f</xref>), time series 3D volume stacks were first converted to a single 3D volume stack, in which each voxel represented variance of voxel values over time. Candidate neurons were next extracted by identifying local maxima in the converted 3D volume stack. The region-of-interest (ROI) was set according to the empirical size of a neuron. The voxels around the local maxima were selected to represent neurons. The fluorescence intensity over each neuron’s ROI was integrated and extracted as neural activity. Relative fluorescent changes <inline-formula><mml:math id="inf34"><mml:mo>∆</mml:mo><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> were normalized to their maximum calcium response <inline-formula><mml:math id="inf35"><mml:mo>∆</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> over time, and sorted according to their onset time when <inline-formula><mml:math id="inf36"><mml:mo>∆</mml:mo><mml:mi>F</mml:mi></mml:math></inline-formula> first reached 20% of its <inline-formula><mml:math id="inf37"><mml:mo>∆</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1e and f</xref>) after the visual stimulus was presented.</p></sec><sec id="s4-10"><title>Visual stimulation</title><p>A short wavelength LED was optically filtered (short-pass optical filter with cut-off wavelength at 450 nm, Edmund #84–704) to avoid light interference with fluorescence. It was then focused by a lens into a spot 2 ~ 3 mm in diameter. The zebrafish was illuminated from its side. The total power of the beam was roughly 3 mW.</p></sec><sec id="s4-11"><title>Statement of replicates and repeats in experiments</title><p>Each experiment was repeated at least three times with similar experimental conditions. Imaging and video data acquired from behaviorally active larval zebrafish with normal huc:h2b-gcamp6f or huc:gcamp6s expression were used in the main figures and videos.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Misha B Ahrens for the zebrafish lines. We thank Yong Jiang, Tongzhou Zhao, WenKai Han, Shenqi Fan for assistance in building the 3D tracking system, real time behavioral analysis, and larval zebrafish experiments. We thank Dr Bing Hu and Dr Jie He for his support in zebrafish handling and helpful discussions.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Designed and built the XLFM, Designed and built the autofocus system, Did experiments under the supervision of Chunfeng Shang, Jiulin Du, Kai Wang and Quan Wen, Worked collaboratively to integrate the XLFM and the tracking system</p></fn><fn fn-type="con" id="con2"><p>Designed and built the XLFM, Designed and built the X-Y tracking and the real-time behavioral analysis system, Designed and built the autofocus system, Did experiments under the supervision of Chunfeng Shang, Jiulin Du, Kai Wang and Quan Wen, Worked collaboratively to integrate the XLFM and the tracking system</p></fn><fn fn-type="con" id="con3"><p>Designed and built the X-Y tracking and the real-time behavioral analysis system, Did experiments under the supervision of Chunfeng Shang, Jiulin Du, Kai Wang and Quan Wen, Worked collaboratively to integrate the XLFM and the tracking system</p></fn><fn fn-type="con" id="con4"><p>Designed and built the XLFM, Designed and built the autofocus system, Did experiments under the supervision of Chunfeng Shang, Jiulin Du, Kai Wang and Quan Wen, Worked collaboratively to integrate the XLFM and the tracking system</p></fn><fn fn-type="con" id="con5"><p>Designed zebrafish behavioral experiments, Worked collaboratively to integrate the XLFM and the tracking system</p></fn><fn fn-type="con" id="con6"><p>Designed and built the X-Y tracking and the real-time behavioral analysis system, Worked collaboratively to integrate the XLFM and the tracking system</p></fn><fn fn-type="con" id="con7"><p>Designed and built the XLFM, Worked collaboratively to integrate the XLFM and the tracking system</p></fn><fn fn-type="con" id="con8"><p>Designed zebrafish behavioral experiments, Worked collaboratively to integrate the XLFM and the tracking system</p></fn><fn fn-type="con" id="con9"><p>Conceived the project, Conceived the idea of XLFM, Designed and built the XLFM, Designed and built the autofocus system, Wrote the paper with inputs from all authors, Worked collaboratively to integrate the XLFM and the tracking system</p></fn><fn fn-type="con" id="con10"><p>Conceived the project, Designed and built the X-Y tracking and the real-time behavioral analysis system, Designed zebrafish behavioral experiments, Wrote the paper with inputs from all authors, Worked collaboratively to integrate the XLFM and the tracking system</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: Zebrafish handling procedures were approved by the Institute of Neuroscience, Shanghai Institutes for Biological Sciences, Chinese Academy of Sciences.(permit number: USTCACUC1103013).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><object-id pub-id-type="doi">10.7554/eLife.28158.029</object-id><label>Source code 1.</label><caption><title>Computer-Aided design files of mounting plates for micro-lenses array.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-28158-code1-v2.zip"/></supplementary-material><supplementary-material id="scode2"><object-id pub-id-type="doi">10.7554/eLife.28158.030</object-id><label>Source code 2.</label><caption><title>Source code for XLFM reconstruction.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-28158-code2-v2.zip"/></supplementary-material><supplementary-material id="scode3"><object-id pub-id-type="doi">10.7554/eLife.28158.031</object-id><label>Source code 3.</label><caption><title>Source code for Real-Time behavioral analysis.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-28158-code3-v2.zip"/></supplementary-material><supplementary-material id="supp1"><object-id pub-id-type="doi">10.7554/eLife.28158.032</object-id><label>Supplement file 1</label><caption><title>Acquisition parameters for fluorescence imaging.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-28158-fig4-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.28158.033</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-28158-transrepform-v2.docx"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abrahamsson</surname> <given-names>S</given-names></name><name><surname>Chen</surname> <given-names>J</given-names></name><name><surname>Hajj</surname> <given-names>B</given-names></name><name><surname>Stallinga</surname> <given-names>S</given-names></name><name><surname>Katsov</surname> <given-names>AY</given-names></name><name><surname>Wisniewski</surname> <given-names>J</given-names></name><name><surname>Mizuguchi</surname> <given-names>G</given-names></name><name><surname>Soule</surname> <given-names>P</given-names></name><name><surname>Mueller</surname> <given-names>F</given-names></name><name><surname>Dugast Darzacq</surname> <given-names>C</given-names></name><name><surname>Darzacq</surname> <given-names>X</given-names></name><name><surname>Wu</surname> <given-names>C</given-names></name><name><surname>Bargmann</surname> <given-names>CI</given-names></name><name><surname>Agard</surname> <given-names>DA</given-names></name><name><surname>Dahan</surname> <given-names>M</given-names></name><name><surname>Gustafsson</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Fast multicolor 3D imaging using aberration-corrected multifocus microscopy</article-title><source>Nature Methods</source><volume>10</volume><fpage>60</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2277</pub-id><pub-id pub-id-type="pmid">23223154</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname> <given-names>EH</given-names></name><name><surname>Wang</surname> <given-names>JYA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Single lens stereo with a plenoptic camera</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>14</volume><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1109/34.121783</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname> <given-names>MB</given-names></name><name><surname>Li</surname> <given-names>JM</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Robson</surname> <given-names>DN</given-names></name><name><surname>Schier</surname> <given-names>AF</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name><name><surname>Portugues</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Brain-wide neuronal dynamics during motor adaptation in zebrafish</article-title><source>Nature</source><volume>485</volume><fpage>471</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1038/nature11057</pub-id><pub-id pub-id-type="pmid">22622571</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname> <given-names>MB</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Robson</surname> <given-names>DN</given-names></name><name><surname>Li</surname> <given-names>JM</given-names></name><name><surname>Keller</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whole-brain functional imaging at cellular resolution using light-sheet microscopy</article-title><source>Nature Methods</source><volume>10</volume><fpage>413</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2434</pub-id><pub-id pub-id-type="pmid">23524393</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname> <given-names>MB</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Large-scale imaging in small brains</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>78</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.01.007</pub-id><pub-id pub-id-type="pmid">25636154</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>An efference copy which is modified by reafferent input</article-title><source>Science</source><volume>214</volume><fpage>450</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1126/science.7291985</pub-id><pub-id pub-id-type="pmid">7291985</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname> <given-names>HC</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>How to track bacteria</article-title><source>Review of Scientific Instruments</source><volume>42</volume><fpage>868</fpage><lpage>871</lpage><pub-id pub-id-type="doi">10.1063/1.1685246</pub-id><pub-id pub-id-type="pmid">4940742</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname> <given-names>IH</given-names></name><name><surname>Kampff</surname> <given-names>AR</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prey capture behavior evoked by simple visual stimuli in larval zebrafish</article-title><source>Frontiers in Systems Neuroscience</source><volume>5</volume><elocation-id>101</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2011.00101</pub-id><pub-id pub-id-type="pmid">22203793</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname> <given-names>IH</given-names></name><name><surname>Ma</surname> <given-names>LH</given-names></name><name><surname>Schoppik</surname> <given-names>D</given-names></name><name><surname>Robson</surname> <given-names>DN</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Beck</surname> <given-names>JC</given-names></name><name><surname>Li</surname> <given-names>JM</given-names></name><name><surname>Schier</surname> <given-names>AF</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name><name><surname>Baker</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The tangential nucleus controls a gravito-inertial vestibulo-ocular reflex</article-title><source>Current Biology</source><volume>22</volume><fpage>1285</fpage><lpage>1295</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.05.026</pub-id><pub-id pub-id-type="pmid">22704987</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname> <given-names>IH</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visuomotor transformations underlying hunting behavior in zebrafish</article-title><source>Current Biology</source><volume>25</volume><fpage>831</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.01.042</pub-id><pub-id pub-id-type="pmid">25754638</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyden</surname> <given-names>ES</given-names></name><name><surname>Zhang</surname> <given-names>F</given-names></name><name><surname>Bamberg</surname> <given-names>E</given-names></name><name><surname>Nagel</surname> <given-names>G</given-names></name><name><surname>Deisseroth</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Millisecond-timescale, genetically targeted optical control of neural activity</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1263</fpage><lpage>1268</lpage><pub-id pub-id-type="doi">10.1038/nn1525</pub-id><pub-id pub-id-type="pmid">16116447</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broxton</surname> <given-names>M</given-names></name><name><surname>Grosenick</surname> <given-names>L</given-names></name><name><surname>Yang</surname> <given-names>S</given-names></name><name><surname>Cohen</surname> <given-names>N</given-names></name><name><surname>Andalman</surname> <given-names>A</given-names></name><name><surname>Deisseroth</surname> <given-names>K</given-names></name><name><surname>Levoy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Wave optics theory and 3-D deconvolution for the light field microscope</article-title><source>Optics Express</source><volume>21</volume><fpage>25418</fpage><lpage>25439</lpage><pub-id pub-id-type="doi">10.1364/OE.21.025418</pub-id><pub-id pub-id-type="pmid">24150383</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Mizuseki</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The log-dynamic brain: how skewed distributions affect network operations</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>264</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1038/nrn3687</pub-id><pub-id pub-id-type="pmid">24569488</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>TW</given-names></name><name><surname>Wardill</surname> <given-names>TJ</given-names></name><name><surname>Sun</surname> <given-names>Y</given-names></name><name><surname>Pulver</surname> <given-names>SR</given-names></name><name><surname>Renninger</surname> <given-names>SL</given-names></name><name><surname>Baohan</surname> <given-names>A</given-names></name><name><surname>Schreiter</surname> <given-names>ER</given-names></name><name><surname>Kerr</surname> <given-names>RA</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name><name><surname>Looger</surname> <given-names>LL</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Kim</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title><source>Nature</source><volume>499</volume><elocation-id>295</elocation-id><lpage>300</lpage><pub-id pub-id-type="doi">10.1038/nature12354</pub-id><pub-id pub-id-type="pmid">23868258</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chiappe</surname> <given-names>ME</given-names></name><name><surname>Seelig</surname> <given-names>JD</given-names></name><name><surname>Reiser</surname> <given-names>MB</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Walking modulates speed sensitivity in <italic>Drosophila</italic> motion vision</article-title><source>Current Biology</source><volume>20</volume><fpage>1470</fpage><lpage>1475</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2010.06.072</pub-id><pub-id pub-id-type="pmid">20655222</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Coombs</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>The Lateral Line System</source><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name><fpage>xiv, 347</fpage></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dombeck</surname> <given-names>DA</given-names></name><name><surname>Khabbaz</surname> <given-names>AN</given-names></name><name><surname>Collman</surname> <given-names>F</given-names></name><name><surname>Adelman</surname> <given-names>TL</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Imaging large-scale neural activity with cellular resolution in awake, mobile mice</article-title><source>Neuron</source><volume>56</volume><fpage>43</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.08.003</pub-id><pub-id pub-id-type="pmid">17920014</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fish in the matrix: motor learning in a virtual world</article-title><source>Frontiers in neural circuits</source><volume>6</volume><elocation-id>125</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2012.00125</pub-id><pub-id pub-id-type="pmid">23355810</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The big data problem: turning maps into knowledge</article-title><source>Neuron</source><volume>83</volume><fpage>1246</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.008</pub-id><pub-id pub-id-type="pmid">25233305</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname> <given-names>RW</given-names></name><name><surname>Jacobson</surname> <given-names>GA</given-names></name><name><surname>Zhu</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Circuit neuroscience in zebrafish</article-title><source>Current Biology</source><volume>20</volume><fpage>R371</fpage><lpage>R381</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2010.02.039</pub-id><pub-id pub-id-type="pmid">21749961</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname> <given-names>A</given-names></name><name><surname>Howard</surname> <given-names>CV</given-names></name><name><surname>Strahle</surname> <given-names>U</given-names></name><name><surname>Cossins</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neurodevelopmental defects in zebrafish (Danio rerio) at environmentally relevant dioxin (TCDD) concentrations</article-title><source>Toxicological Sciences</source><volume>76</volume><fpage>392</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1093/toxsci/kfg241</pub-id><pub-id pub-id-type="pmid">14600291</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hromádka</surname> <given-names>T</given-names></name><name><surname>Deweese</surname> <given-names>MR</given-names></name><name><surname>Zador</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Sparse representation of sounds in the unanesthetized auditory cortex</article-title><source>PLoS Biology</source><volume>6</volume><elocation-id>e16</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060016</pub-id><pub-id pub-id-type="pmid">18232737</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname> <given-names>N</given-names></name><name><surname>Milkie</surname> <given-names>DE</given-names></name><name><surname>Betzig</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Adaptive optics via pupil segmentation for high-resolution imaging in biological tissues</article-title><source>Nature Methods</source><volume>7</volume><fpage>141</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1411</pub-id><pub-id pub-id-type="pmid">20037592</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adaptive optical fluorescence microscopy</article-title><source>Nature Methods</source><volume>14</volume><fpage>374</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1038/nmeth.4218</pub-id><pub-id pub-id-type="pmid">28362438</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerr</surname> <given-names>JN</given-names></name><name><surname>Denk</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Imaging in vivo: watching the brain in action</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>195</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1038/nrn2338</pub-id><pub-id pub-id-type="pmid">18270513</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levoy</surname> <given-names>M</given-names></name><name><surname>Ng</surname> <given-names>R</given-names></name><name><surname>Adams</surname> <given-names>A</given-names></name><name><surname>Footer</surname> <given-names>M</given-names></name><name><surname>Horowitz</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Light field microscopy</article-title><source>ACM Transactions on Graphics</source><volume>25</volume><fpage>924</fpage><lpage>934</lpage><pub-id pub-id-type="doi">10.1145/1141911.1141976</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Organization and physiology of posterior lateral line afferent neurons in larval zebrafish</article-title><source>Biology Letters</source><volume>6</volume><fpage>402</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1098/rsbl.2009.0995</pub-id><pub-id pub-id-type="pmid">20181553</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>L</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Genetic dissection of neural circuits</article-title><source>Neuron</source><volume>57</volume><fpage>634</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.01.002</pub-id><pub-id pub-id-type="pmid">18341986</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maimon</surname> <given-names>G</given-names></name><name><surname>Straw</surname> <given-names>AD</given-names></name><name><surname>Dickinson</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Active flight increases the gain of visual motion processing in <italic>Drosophila</italic></article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>393</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1038/nn.2492</pub-id><pub-id pub-id-type="pmid">20154683</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muto</surname> <given-names>A</given-names></name><name><surname>Ohkura</surname> <given-names>M</given-names></name><name><surname>Abe</surname> <given-names>G</given-names></name><name><surname>Nakai</surname> <given-names>J</given-names></name><name><surname>Kawakami</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Real-time visualization of neuronal activity during perception</article-title><source>Current Biology</source><volume>23</volume><fpage>307</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.12.040</pub-id><pub-id pub-id-type="pmid">23375894</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naumann</surname> <given-names>EA</given-names></name><name><surname>Kampff</surname> <given-names>AR</given-names></name><name><surname>Prober</surname> <given-names>DA</given-names></name><name><surname>Schier</surname> <given-names>AF</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Monitoring neural activity with bioluminescence during natural behavior</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>513</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1038/nn.2518</pub-id><pub-id pub-id-type="pmid">20305645</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Ng</surname> <given-names>R</given-names></name><name><surname>Levoy</surname> <given-names>M</given-names></name><name><surname>Bredif</surname> <given-names>M</given-names></name><name><surname>Duval</surname> <given-names>G</given-names></name><name><surname>Horowitz</surname> <given-names>M</given-names></name><name><surname>Hanrahan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Light Field Photography with a Hand-Held Plenoptic Camera</source><publisher-loc>Stanford, United States</publisher-loc><publisher-name>Stanford University</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname> <given-names>JP</given-names></name><name><surname>Shipley</surname> <given-names>FB</given-names></name><name><surname>Linder</surname> <given-names>AN</given-names></name><name><surname>Plummer</surname> <given-names>GS</given-names></name><name><surname>Liu</surname> <given-names>M</given-names></name><name><surname>Setru</surname> <given-names>SU</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Leifer</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Whole-brain calcium imaging with cellular resolution in freely behaving <italic>Caenorhabditis elegans</italic></article-title><source>PNAS</source><volume>113</volume><fpage>E1074</fpage><lpage>E1081</lpage><pub-id pub-id-type="doi">10.1073/pnas.1507110112</pub-id><pub-id pub-id-type="pmid">26712014</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname> <given-names>CM</given-names></name><name><surname>Stryker</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nöbauer</surname> <given-names>T</given-names></name><name><surname>Skocek</surname> <given-names>O</given-names></name><name><surname>Pernía-Andrade</surname> <given-names>AJ</given-names></name><name><surname>Weilguny</surname> <given-names>L</given-names></name><name><surname>Traub</surname> <given-names>FM</given-names></name><name><surname>Molodtsov</surname> <given-names>MI</given-names></name><name><surname>Vaziri</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Video rate volumetric Ca(2+) imaging across cortex using seeded iterative demixing (SID) microscopy</article-title><source>Nature Methods</source><volume>14</volume><fpage>811</fpage><lpage>818</lpage><pub-id pub-id-type="doi">10.1038/nmeth.4341</pub-id><pub-id pub-id-type="pmid">28650477</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Sparse coding of sensory inputs</article-title><source>Current Opinion in Neurobiology</source><volume>14</volume><fpage>481</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2004.07.007</pub-id><pub-id pub-id-type="pmid">15321069</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname> <given-names>BW</given-names></name><name><surname>Abraham</surname> <given-names>AO</given-names></name><name><surname>MacIver</surname> <given-names>MA</given-names></name><name><surname>McLean</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visually guided gradation of prey capture movements in larval zebrafish</article-title><source>Journal of Experimental Biology</source><volume>216</volume><fpage>3071</fpage><lpage>3083</lpage><pub-id pub-id-type="doi">10.1242/jeb.087742</pub-id><pub-id pub-id-type="pmid">23619412</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname> <given-names>KG</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Proprioceptive regulation of locomotion</article-title><source>Current Opinion in Neurobiology</source><volume>5</volume><fpage>786</fpage><lpage>791</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(95)80107-3</pub-id><pub-id pub-id-type="pmid">8805415</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Perwass</surname> <given-names>C</given-names></name><name><surname>Wietzke</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title><italic>Single Lens 3D-Camera with Extended Depth-of-Field</italic></article-title><conf-name>Human Vision and Electronic Imaging XVII</conf-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portugues</surname> <given-names>R</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Adaptive locomotor behavior in larval zebrafish</article-title><source>Frontiers in Systems Neuroscience</source><volume>5</volume><elocation-id>72</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2011.00072</pub-id><pub-id pub-id-type="pmid">21909325</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portugues</surname> <given-names>R</given-names></name><name><surname>Feierstein</surname> <given-names>CE</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Whole-brain activity maps reveal stereotyped, distributed networks for visuomotor behavior</article-title><source>Neuron</source><volume>81</volume><fpage>1328</fpage><lpage>1343</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.019</pub-id><pub-id pub-id-type="pmid">24656252</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prevedel</surname> <given-names>R</given-names></name><name><surname>Yoon</surname> <given-names>YG</given-names></name><name><surname>Hoffmann</surname> <given-names>M</given-names></name><name><surname>Pak</surname> <given-names>N</given-names></name><name><surname>Wetzstein</surname> <given-names>G</given-names></name><name><surname>Kato</surname> <given-names>S</given-names></name><name><surname>Schrödel</surname> <given-names>T</given-names></name><name><surname>Raskar</surname> <given-names>R</given-names></name><name><surname>Zimmer</surname> <given-names>M</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name><name><surname>Vaziri</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Simultaneous whole-animal 3D imaging of neuronal activity using light-field microscopy</article-title><source>Nature Methods</source><volume>11</volume><fpage>727</fpage><lpage>730</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2964</pub-id><pub-id pub-id-type="pmid">24836920</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pégard</surname> <given-names>NC</given-names></name><name><surname>Liu</surname> <given-names>H-Y</given-names></name><name><surname>Antipa</surname> <given-names>N</given-names></name><name><surname>Gerlock</surname> <given-names>M</given-names></name><name><surname>Adesnik</surname> <given-names>H</given-names></name><name><surname>Waller</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title><italic>Compressive light-field microscopy for</italic> 3<italic>D neural activity recording</italic></article-title><source>Optica</source><volume>3</volume><fpage>517</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1364/OPTICA.3.000517</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Semmelhack</surname> <given-names>JL</given-names></name><name><surname>Donovan</surname> <given-names>JC</given-names></name><name><surname>Thiele</surname> <given-names>TR</given-names></name><name><surname>Kuehn</surname> <given-names>E</given-names></name><name><surname>Laurell</surname> <given-names>E</given-names></name><name><surname>Baier</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A dedicated visual pathway for prey detection in larval zebrafish</article-title><source>eLife</source><volume>3</volume><elocation-id>e04878</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04878</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Severi</surname> <given-names>KE</given-names></name><name><surname>Portugues</surname> <given-names>R</given-names></name><name><surname>Marques</surname> <given-names>JC</given-names></name><name><surname>O'Malley</surname> <given-names>DM</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural control and modulation of swimming speed in the larval zebrafish</article-title><source>Neuron</source><volume>83</volume><fpage>692</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.06.032</pub-id><pub-id pub-id-type="pmid">25066084</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>St-Pierre</surname> <given-names>F</given-names></name><name><surname>Marshall</surname> <given-names>JD</given-names></name><name><surname>Yang</surname> <given-names>Y</given-names></name><name><surname>Gong</surname> <given-names>Y</given-names></name><name><surname>Schnitzer</surname> <given-names>MJ</given-names></name><name><surname>Lin</surname> <given-names>MZ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>High-fidelity optical reporting of neuronal electrical activity with an ultrafast fluorescent voltage sensor</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>884</fpage><lpage>889</lpage><pub-id pub-id-type="doi">10.1038/nn.3709</pub-id><pub-id pub-id-type="pmid">24755780</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>L</given-names></name><name><surname>Hires</surname> <given-names>SA</given-names></name><name><surname>Mao</surname> <given-names>T</given-names></name><name><surname>Huber</surname> <given-names>D</given-names></name><name><surname>Chiappe</surname> <given-names>ME</given-names></name><name><surname>Chalasani</surname> <given-names>SH</given-names></name><name><surname>Petreanu</surname> <given-names>L</given-names></name><name><surname>Akerboom</surname> <given-names>J</given-names></name><name><surname>McKinney</surname> <given-names>SA</given-names></name><name><surname>Schreiter</surname> <given-names>ER</given-names></name><name><surname>Bargmann</surname> <given-names>CI</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Looger</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Imaging neural activity in worms, flies and mice with improved GCaMP calcium indicators</article-title><source>Nature Methods</source><volume>6</volume><fpage>875</fpage><lpage>881</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1398</pub-id><pub-id pub-id-type="pmid">19898485</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trivedi</surname> <given-names>CA</given-names></name><name><surname>Bollmann</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visually driven chaining of elementary swim patterns into a goal-directed motor sequence: a virtual reality study of zebrafish prey capture</article-title><source>Frontiers in Neural Circuits</source><volume>7</volume><elocation-id>86</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2013.00086</pub-id><pub-id pub-id-type="pmid">23675322</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venkatachalam</surname> <given-names>V</given-names></name><name><surname>Ji</surname> <given-names>N</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Clark</surname> <given-names>C</given-names></name><name><surname>Mitchell</surname> <given-names>JK</given-names></name><name><surname>Klein</surname> <given-names>M</given-names></name><name><surname>Tabone</surname> <given-names>CJ</given-names></name><name><surname>Florman</surname> <given-names>J</given-names></name><name><surname>Ji</surname> <given-names>H</given-names></name><name><surname>Greenwood</surname> <given-names>J</given-names></name><name><surname>Chisholm</surname> <given-names>AD</given-names></name><name><surname>Srinivasan</surname> <given-names>J</given-names></name><name><surname>Alkema</surname> <given-names>M</given-names></name><name><surname>Zhen</surname> <given-names>M</given-names></name><name><surname>Samuel</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Pan-neuronal imaging in roaming <italic>Caenorhabditis elegans</italic></article-title><source>PNAS</source><volume>113</volume><fpage>E1082</fpage><lpage>E1088</lpage><pub-id pub-id-type="doi">10.1073/pnas.1507109113</pub-id><pub-id pub-id-type="pmid">26711989</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>K</given-names></name><name><surname>Milkie</surname> <given-names>DE</given-names></name><name><surname>Saxena</surname> <given-names>A</given-names></name><name><surname>Engerer</surname> <given-names>P</given-names></name><name><surname>Misgeld</surname> <given-names>T</given-names></name><name><surname>Bronner</surname> <given-names>ME</given-names></name><name><surname>Mumm</surname> <given-names>J</given-names></name><name><surname>Betzig</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rapid adaptive optical recovery of optimal resolution over large volumes</article-title><source>Nature Methods</source><volume>11</volume><fpage>625</fpage><lpage>628</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2925</pub-id><pub-id pub-id-type="pmid">24727653</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>K</given-names></name><name><surname>Sun</surname> <given-names>W</given-names></name><name><surname>Richie</surname> <given-names>CT</given-names></name><name><surname>Harvey</surname> <given-names>BK</given-names></name><name><surname>Betzig</surname> <given-names>E</given-names></name><name><surname>Ji</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Direct wavefront sensing for high-resolution in vivo imaging in scattering tissue</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>7276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms8276</pub-id><pub-id pub-id-type="pmid">26073070</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyart</surname> <given-names>C</given-names></name><name><surname>Del Bene</surname> <given-names>F</given-names></name><name><surname>Warp</surname> <given-names>E</given-names></name><name><surname>Scott</surname> <given-names>EK</given-names></name><name><surname>Trauner</surname> <given-names>D</given-names></name><name><surname>Baier</surname> <given-names>H</given-names></name><name><surname>Isacoff</surname> <given-names>EY</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Optogenetic dissection of a behavioural module in the vertebrate spinal cord</article-title><source>Nature</source><volume>461</volume><fpage>407</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1038/nature08323</pub-id><pub-id pub-id-type="pmid">19759620</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>F</given-names></name><name><surname>Wang</surname> <given-names>LP</given-names></name><name><surname>Brauner</surname> <given-names>M</given-names></name><name><surname>Liewald</surname> <given-names>JF</given-names></name><name><surname>Kay</surname> <given-names>K</given-names></name><name><surname>Watzke</surname> <given-names>N</given-names></name><name><surname>Wood</surname> <given-names>PG</given-names></name><name><surname>Bamberg</surname> <given-names>E</given-names></name><name><surname>Nagel</surname> <given-names>G</given-names></name><name><surname>Gottschalk</surname> <given-names>A</given-names></name><name><surname>Deisseroth</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Multimodal fast optical interrogation of neural circuitry</article-title><source>Nature</source><volume>446</volume><fpage>633</fpage><lpage>639</lpage><pub-id pub-id-type="doi">10.1038/nature05744</pub-id><pub-id pub-id-type="pmid">17410168</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.28158.034</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Rapid whole brain imaging of neural activities in freely behaving larval zebrafish&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom, Ronald L Calabrese (Reviewer #1), is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Eve Marder as the Senior Editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This is an exciting manuscript, which reports a new development in Light Field Microscopy (LFM). The authors developed a Light Field Microscope: eXtended (XLFM) field of view seamlessly integrated with an X-Y tracking system and an auto focus. XLFM can simultaneously image intact brain neural activities (over a volume of 800 µm X 800 µm X 200 µm) at ~ 3.4 µm X 3.4 µm X 5 µm spatial resolution and at 77 Hz volume rate, with the aid of genetically encoded calcium indicator GCamp6f in a freely moving larval zebrafish during visual stimulation and prey capture. They provide stunning videos and enough processed data to show the value of the new development for imaging activity across the brain during real behavior.</p><p>The work is nicely illustrated with exemplar data. This is not a full report on the science behind the experiments illustrated but rather a proof of principle. Exciting science is in the offing but a new technology is showcased, as is appropriate for a Tools and Resources paper.</p><p>Essential revisions:</p><p>1) We find this technology to be a significant advance. There are several technical issues, however, that must be resolved. Further clarifications to the text are needed about precisely what was done and how it was done. Some claims need to be more carefully worded to recognize the limitations of the technique and recognize other contributions. The writing should be improved. The expert reviews provide a detailed list of all the points that should be considered in revision. Rather than paraphrasing those reports, they are included in full to ensure that the detailed technological issues are well-stated.</p><p>2) As stated explicitly in the expert reviews, software and design features must be made fully available to the scientific community with publication.</p><p><italic>Reviewer #1:</italic> </p><p>1) The chamber in which the freely moving larva swims is ONLY 0.8 mm deep. Thus the animal is sandwiched between glass plates with no real ability to move in the z-direction. Essentially, it moves in two dimensions. The authors should address this limitation in their approach.</p><p><italic>Reviewer #2:</italic> </p><p>In Cong et al., two advances are reported. First, a tracking system is introduced capable of keeping a freely swimming larval zebrafish in one location most of the time. Second, a new form of light field microscopy is reported capable of fast 3D imaging. Putting these together constitutes a system for whole-brain imaging in freely swimming zebrafish larvae, with a resolution slightly below single-cell.</p><p>In my opinion this is a major advance and I am supportive of publication in <italic>eLife</italic> with a few improvements.</p><p>For background, previous efforts to perform whole-brain imaging in behaving animals consisted of light-sheet (slower than light-field) in head-restrained animals, or light-field (a variant with, I believe, lower spatial resolution) in head-restrained animals. Imaging in freely behaving animals has been done in <italic>C. elegans</italic>, which move more slowly than zebrafish. Thus, compared to previous work, the advances of this manuscript are considerable. Furthermore, imaging most of the brain in freely swimming animals in really impressive.</p><p>Points that should be addressed:</p><p>1) The authors claim that the point spread function (PSF) is spatially invariant. This appears to be true if one considers the microscope an ideal optical system, but with non-ideal optics, it's unlikely. Even with a good objective, the entire system contributes to the PSF, and it's unlikely that all microlenses are diffraction limited over the entire field of view. Moreover, differential distortion between the sub-images would cause the full camera PSF to warp as the point source moves in the sample. So if Richardson-Lucy deconvolution only works with a spatially invariant PSF, and the true PSF is not fully spatially invariant, the question arises, What image artifacts do you get?</p><p>There may be multiple ways to answer this question. One path might be to (a) move a bead around a few x-y locations, including the extreme ones, and check how spatially invariant the PSF really is (and include the raw PSF volumes in the manuscript, e.g. by measuring them at two extreme points, shifting one by the predicted amount, and overlaying the two in different colors). (b) Next, assuming a spatially invariant PSF derived from one of the bead locations (e.g. the center), reconstruct a bead positioned at various points, including the edges of the volume, and quantify the spread of the point source in the reconstructed volume (this should have high brightness at the original bead position, plus dimmer pixel values spread at other locations, which should be quantified).</p><p>2) A follow-up: if the PSF is not fully spatially invariant, what does this mean for the statement that overlapping sub-images are permitted (subsection “Image reconstruction of XLFM”)? My understanding is that the overlap is fine so long as the PSF is fully x-y invariant, and if not, then some artifacts will be introduced. The reasons and assumptions underlying this statement should be clarified in the text.</p><p>For clarity, points (<xref ref-type="bibr" rid="bib25">1</xref>) and (<xref ref-type="bibr" rid="bib17">2</xref>) are not criticisms of the system, only a call for characterization of the artifacts that the reconstruction algorithm introduces when using simplifying assumptions.</p><p>3) The reconstruction algorithm (subsection “Image reconstruction of XLFM”) contains confusing notation (if I understand it correctly). The coordinates (x,y) on the left hand side of Equation 5, refer to image coordinates. But (x,y,z_k) on the right hand side refers to coordinates in the 3D volume. That's confusing, x and y should not be used for both. Moreover, I believe that PSF_A,B(x,y,z_k) are each 2 dimensional objects. So in reality, spelled out with all the indices, using ^superscript for volume coordinates and _subscript for image coordinates, I believe the equation is</p><p>ImgEst_(x',y') = sum {ObjA^(x,y,z_k) conv^(x,y,z_k) PSFA^(x,y,z_k)_(x',y') +.…}</p><p>Explaining this equation better, e.g. by writing it out as above or stating that Img_est is a 2D object in image coordinates, and PSF_A,B(x,y,z_k) is 2D in image space contingent on x,y,z_k in volume space, will make this section more understandable.</p><p>4) Can the lateral resolution be measured instead of estimated?</p><p>5) The manuscript says the reconstruction algorithm is based on optical wave theory. What do the authors mean by this? The algorithm is based on the assumption of a spatially invariant PSF and observations of how to apply Richardon-Lucy to sets of microlenses of different focal lengths. Where does this rely on optical wave theory instead of just classical optics?</p><p>6) I assume CAD models of the microlens holder and the autofocus system exists, can these files be made available?</p><p>7) Most of the code is said to be available (e.g. real-time behavioral analysis and 3D reconstruction), but in some cases it is not mentioned. Can the code for the tracking and autofocus system be made available?</p><p><italic>Reviewer #3:</italic> </p><p>My overall opinion of the manuscript is positive. I think being able to image neuronal activity in a freely moving larval zebrafish is an advance and the current paper serves as a satisfactory proof of principle.</p><p>I have some issues regarding the term &quot;whole-brain&quot; and the resolution claimed by the authors. The authors claim, or at least imply, that they can simultaneously (within 1 Orca camera frame which has a 2048 x 2048 pixel sensor) image 800 x 800 x 200 microns at 3.4 x 3.4 x 5 micron resolution. I find this very difficult to believe. Imaging with this resolution requires imaging 800/(3.4/2) = 470 pixels in both x and y for 200/(5/2) = 80 planes (the factor of 2 arise from Nyquist sampling). Given the sensor dimensions one can fit at most 25 planes into it (5 x 5). The authors show that they are able to use their microlens distribution to image 27. I do not believe there is enough information on the chip to have the claimed resolution. The authors may be able to distinguish 2 fluorescent particles 6 microns apart as in <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7</xref>, but these are still sparse particles appearing in the center of their CMOS chip, not a densely fluorescent tissue as a pan-neuronally fluorescent larval zebrafish. I think my argument is corroborated by the data shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>: this data does not have the resolution claimed and does not show the whole brain of the larva.</p><p>The above argument assumes that the fish's head is also perfectly in focus. The z extent of a larval zebrafish head at this age is ~ 250 microns, which will already be larger than the z field of view. The axial shift shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, typically 20 microns but up to 80 will greatly affect this. The authors mention they use a 500 fps camera for the lateral tracking, but do not (or I missed) the speed of their auto-focus camera for axial tracking: how fast is this?</p><p>I do not think that these are &quot;deal-breakers&quot;, but I think it is important for the authors to rewrite their claims and be explicit about what their system can't and can do (which is a lot). In the Discussion the authors claim to have developed a whole brain imaging setup: I am not sure what this means.</p><p><xref ref-type="fig" rid="fig1">Figure 1</xref> looks at an agarose restrained larval zebrafish. The authors should be explicit about this in the text and the Figure caption (for example the name of the figure). I do not think that panel d presents maximum intensity projections – they are far too clean for this (the bottom panel looks more like a snapshot of a 3d rendering of the stack). Can the authors correct this in the caption or be explicit about what they are showing?</p><p>Closed-loop systems have also been implemented in restrained fish with their tail free to move (e.g. Portugues and Engert 2011) which can remove the issue the authors mention relating to proprioception (Introduction paragraph two). The authors also mention improper vestibular feedback when fish are restrained, but in their setup, due to the closed look, the fish would also experience a reduced vestibular feedback: if the closed-loop was perfect the head would not move at all and the same vestibular deficits would be observed. If this is correct then the authors should comment on this.</p><p>The authors talk about &quot;visual stimulation. What does this visual stimulation consist of? This should be explained in the Materials and methods clearly.</p><p>The claim in Results paragraph seven is a strong one and I am not sure it is fully warranted. Given the resolution and the data shown I would omit the phrase &quot;for the first time&quot; and again, explain carefully what is meant (here and in other places) by the term &quot;whole brain&quot;.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Rapid whole brain imaging of neural activity in freely behaving larval zebrafish (<italic>Danio rerio</italic>)&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Eve Marder (Senior editor), a Reviewing editor, and two reviewers.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>This is an exciting manuscript, which reports a new development in Light Field Microscopy (LFM). The authors have made a strong effort at revising the manuscript in response to the last review. They have answered forthrightly almost every point raised and the manuscript is much stronger. There is still one major concern that must be addressed, and there some more minor concerns. The reviews are reproduced in their entirety to aid revision.</p><p>Major Concern</p><p>1) As brought up by reviewer #3 the definition and technical details of the claimed resolution are not adequately documented and explained. The detailed comments of the reviewer should be fully addressed.</p><p><italic>Reviewer #2:</italic> </p><p>The manuscript has improved and I think that most of our comments have been addressed.</p><p>The changes include a new piece of useful information: the non-idealness of the point spread function (PSF) has been measured, and attributed, in part, to small differences in focal lengths of the microlenses.</p><p>I think this is great work and the revised version is even better. There are a few final comments I'd like to make,</p><p>In the subsection “Image reconstruction of XLFM”: &quot;Furthermore, our system took practical conditions, such as imaging system and light properties, into consideration.&quot; What does this mean; can this be explained better?</p><p>About the reconstruction algorithm: The authors opt for sticking with the conventions and use the same indices x,y on both sides of the equation. This is ok, but in that case, some explanation should be added. For example,</p><p>&quot;The 2D convolution is over x and y&quot; and &quot;Per convention in optics, x,y on both sides represent object space, even though in practice, x,y at the left will refer to image space on the camera chip and x,y on the right to the sample coordinates.&quot;</p><p>In the same section the authors seem to agree that the statement that the algorithm can deal with overlapping fish images depends on the invariance of the PSF, this information should be explicitly stated, i.e. include a statement like &quot;under the condition that the PSF is spatially invariant, which is satisfied apart from small aberrations, the algorithm can handle overlapping fish images&quot;.</p><p>Results paragraph one: &quot;Therefore, a spatially invariant point spread function (PSF) of the entire optical imaging system could be defined and measured (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>)&quot;. Here also it would be good to mention that it's an approximately spatially invariant PSF.</p><p>A reference to a recept paper from the Vaziri lab, Nobauer et al., 2017, should be included. (This is also about light field microscopy, but I want to emphasize that this does in no way diminish the impact of the current manuscript.)</p><p><italic>Reviewer #3:</italic> </p><p>As I mentioned previously, I like the paper, and the authors have addressed most of the minor issues appropriately. There are two points which I am still not sure have been resolved.</p><p>1) I do not believe the authors have fully addressed my previous point relating to resolution, which I reproduce below. This argument may be wrong, and I would be very happy if the authors could explain to me where my logic fails.</p><p>&quot;I have some issues regarding the term &quot;whole-brain&quot; and the resolution claimed by the authors. The authors claim, or at least imply, that they can simultaneously (within 1 Orca camera frame which has a 2048 x 2048 pixel sensor) image 800 x 800 x 200 microns at 3.4 x 3.4 x 5 micron resolution. I find this very difficult to believe. Imaging with this resolution requires imaging 800/(3.4/2) = 470 pixels in both x and y for 200/(5/2) = 80 planes (the factor of 2 arise from Nyquist sampling). Given the sensor dimensions one can fit at most 25 planes into it (5 x 5). The authors show that they are able to use their microlens distribution to image 27.”</p><p>I am not worried about reconstructing the volume; that is not the point here. The issue is that of resolution and discriminability of points. This involves two aspects: the &quot;optical resolution&quot; of the imaging system and the sampling. The Rayleigh resolution criterion states the minimal distance resolvable is half the width of the first diffraction order. This depends on the wavelength of the light and the NA of the system. Ideal sampling is then obtained by using the Nyquist criterion: the inverse image of a pixel should be half the optical resolution of the system, so that there is a &quot;negative dip&quot; in between the two bright pixels.</p><p>Using this last sampling definition, the number of resolvable points can be estimated as follows:</p><p>- every bright pixel has to be surrounded (in the chip) by a dark set of pixels.</p><p>- the number of bright pixels you can have on the chip in this configuration is the number of resolvable points. This is a quarter of the number of pixels on the chip.</p><p>- ideally, the inverse image of a pixel should be a quarter of the first diffraction order peak width, for sampling and optical resolution to be perfectly matched.</p><p>With a 2048 by 2048 chip, there are at most ~ 1 million resolvable points.</p><p>The authors claim they can resolve ~ 2.2 million (800 x 800 x 200 microns at 3.4 x 3.4 x 5 micron resolution).</p><p>Deconvolution is a linear process, so XLFM may &quot;shuffle or combine&quot; the intensities of different pixels, but must do so in a linear way.</p><p>In addition, as the authors state, the NA of the objective (nominally 1.05) is greatly decreased (to an effective 0.4) because a lot of light is blocked by the array casing. The optical resolution of the system is bound to be significantly affected.</p><p><xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7</xref> does try to address this issue, but I remain unconvinced because column 3 of panel b shows no dip whatsoever in between the two particles. This indicates to me that the authors are not using the Rayleigh criterion. It is definitely possible to separate two identical Gaussians whose positions differ by less than two SDs (situation which approximates to the Rayleigh criterion), but this is not what is usually called resolution.</p><p>2) I am not sure I understand the authors' claims of a spatially invariant PSF. In <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref> they in fact show it is not spatially invariant even within the focal plane (which they attribute to variation in the magnification across microlenses).</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Rapid whole brain imaging of neural activity in freely behaving larval zebrafish (<italic>Danio rerio</italic>)&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Eve Marder (Senior editor), a Reviewing editor, and one reviewer.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below. Given that this would be a third round, we must now bring this process to a close with no possibility of further revisions.</p><p>Reviewer #3 concerns must be addressed more fully.</p><p>I appreciate the authors' comments to the points I raised, but unfortunately I do not feel they have been addressed. I still believe there are two points which need to be addressed: the resolution and the PSF. I think both these points could be addressed by rewriting the manuscript and making claims that are both theoretically sound and supported by the data presented. Given that the manuscript is a proof of concept about an exciting and promising new imaging technique I think it is fundamental to be precise, as this paper will set the baseline for all future work involving this technique and other freely swimming whole brain imaging approaches.</p><p>1) Resolution.</p><p>The authors have not addressed my argument or provided a counterargument. I still believe my argument is correct. I think the authors are under a misconception. As I mentioned in my previous report, the resolution of an optical system depends on only two things:</p><p>a) The optical resolution of the system that results in the Rayleigh criterion.b) The sampling resolution that results in the Nyquist criterion.</p><p>Specifically, the resolution does not depend on either the reconstruction (as suggested by the authors in answer to my first review) or the sparseness (as the authors argue in the answer to my second and third review).</p><p>I repeat the basis of my argument. Given the dimensions of the chip, the number of resolvable points cannot be more than ~ (2000/2)^2 which is about 1 million. This assumes that a diffraction-limited point in the sample is images onto 1 pixel in the camera chip and that the whole chip is used. The first point is not true for the current system (see <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref> in which a 0.5um bead results in at least 6 reconstructed pixels in the image). In fact, a one such diffraction limited spot will automatically be imaged onto 27 points just by construction). The second point is also not true for this system (see <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref> in which only about half the chip is used). My estimate is that at most one fifth of this upper limit of the number of points can be resolved, about 200,000, and probably much closer to 100,000.</p><p>Paragraph two of the Results is now very confusing. In the second sentence, the authors state that they can image a field of view of ~ 800 x 800 x 200 μm with 3.4 x 3.4 x 5 μm resolution. This corresponds to 2 million resolvable points. This is probably a factor of 20 out. Then later in the paragraph it is claimed that it is 500 x 500 x 100 μm at this resolution. This corresponds to 400,000 points. First of all I think this is still way too high. And secondly I do not understand the contradictory statements in this paragraph. The resolution does not depend on the sparseness. In addition, one can interpolate and reconstruct at whatever desired resolution to make pretty images, so this does not play a role either.</p><p>2) PSF.</p><p>The authors have developed a deconvolution algorithm that to my understanding calculates an effective PSF which they assume to be spatially invariant (in 3 dimensions) and which they then use to deconvolve their data. If this is true I think this is a statement that I am very happy with and support. In traditional lightfield microscopy it can be shown theoretically that the PSF is not spatially invariant. I cannot see why the setup presented by the authors would result in a spatially invariant PSF. If they claim this it should be shown theoretically (this is a methods paper). I agree that inhomogeneities arising from unequal microlens magnifications will contribute to worsen the spatial invariance of the PSF, as argued in <xref ref-type="fig" rid="fig1s8">Figure 1—figure supplement 8</xref>. But I do not believe this is the only source for them, so I do not agree with the statement in the figure legend. In <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref> the authors show that the PSF is not spatially invariant. And the data here relates to the &quot;focal plane z=0&quot; How does the PSF look at x=400um and z =100um? It will most likely look &quot;worse&quot; than that in the last panel of this figure. The authors seem to propose that the PSF of their imaging system is spatially invariant until it is not, away from the center of the field of view and the focal plane. This is not a rigorous scientific statement and not one which can be made in a methods paper that proposes a new imaging technique in a highly regarded journal such as <italic>eLife</italic>. I can definitely stand and support the argument put forward in the first sentence of this paragraph but if the authors claim spatial invariance they will have to either theoretically prove it or measure the PSF throughout the field of view and show it (and <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref> contradicts this).</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.28158.035</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>1) The chamber in which the freely moving larva swims is ONLY 0.8 mm deep. Thus the animal is sandwiched between glass plates with no real ability to move in the z-direction. Essentially, it moves in two dimensions. The authors should address this limitation in their approach.</p></disp-quote><p>We agree with the reviewer that leaving more space in the z direction would be beneficial. Nevertheless, another important factor we must consider is the tracking speed. The traveling range and the moving speed in z-direction are two parameters that <italic>cannot</italic> be easily optimized simultaneously in commercially available products. Here, we chose to use a piezo scanner (Physik Instrumente, P-725KHDS) with a good combination of traveling range (400 μm) and moving speed (330 Hz resonance frequency in the absence of load). In our experimental setup, larva zebrafish, which is typically ~400 μm thick, was swimming in an 800 μm deep chamber, and it had 400 μm free space to explore along the z direction, which can be covered by the 400 μm traveling range moving stage. The prey capture behavior in the larval zebrafish appeared to be normal in such a semi-2D environment.</p><p>Tracking in the z direction can be improved in the future. A traveling range beyond 1 mm with sufficiently fast dynamics along the z direction requires a new motion control system. So far, we haven’t explored in this direction. We have added related discussion in the manuscript and addressed the limitation of our approach (Discussion, fifth paragraph).</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>In Cong et al., two advances are reported. First, a tracking system is introduced capable of keeping a freely swimming larval zebrafish in one location most of the time. Second, a new form of light field microscopy is reported capable of fast 3D imaging. Putting these together constitutes a system for whole-brain imaging in freely swimming zebrafish larvae, with a resolution slightly below single-cell.</p><p>In my opinion this is a major advance and I am supportive of publication in eLife with a few improvements.</p><p>For background, previous efforts to perform whole-brain imaging in behaving animals consisted of light-sheet (slower than light-field) in head-restrained animals, or light-field (a variant with, I believe, lower spatial resolution) in head-restrained animals. Imaging in freely behaving animals has been done in C. elegans, which move more slowly than zebrafish. Thus, compared to previous work, the advances of this manuscript are considerable. Furthermore, imaging most of the brain in freely swimming animals in really impressive.</p><p>Points that should be addressed:</p><p>1) The authors claim that the point spread function (PSF) is spatially invariant. This appears to be true if one considers the microscope an ideal optical system, but with non-ideal optics, it's unlikely. Even with a good objective, the entire system contributes to the PSF, and it's unlikely that all microlenses are diffraction limited over the entire field of view. Moreover, differential distortion between the sub-images would cause the full camera PSF to warp as the point source moves in the sample. So if Richardson-Lucy deconvolution only works with a spatially invariant PSF, and the true PSF is not fully spatially invariant, the question arises, What image artifacts do you get?</p><p>There may be multiple ways to answer this question. One path might be to (a) move a bead around a few x-y locations, including the extreme ones, and check how spatially invariant the PSF really is (and include the raw PSF volumes in the manuscript, e.g. by measuring them at two extreme points, shifting one by the predicted amount, and overlaying the two in different colors). (b) Next, assuming a spatially invariant PSF derived from one of the bead locations (e.g. the center), reconstruct a bead positioned at various points, including the edges of the volume, and quantify the spread of the point source in the reconstructed volume (this should have high brightness at the original bead position, plus dimmer pixel values spread at other locations, which should be quantified).</p></disp-quote><p>We agree with the reviewer that only an ideal imaging system would have a spatially invariant PSF and it is informative to characterize the imaging performance under realistic conditions. We performed calibration in the way the reviewer suggested and confirmed that the spatial invariance could not be perfectly conserved across the entire imaging volume. We further confirmed that the spatial variance of PSF was mainly due to the focal length variation of the customized micro-lenses (<xref ref-type="fig" rid="fig1s8">Figure 1—figure supplement 8</xref>). We have clarified this point in the Results, second paragraph.</p><p>This problem can be solved either by employing more precisely machined micro-lenses or a generalized reconstruction algorithm. The generalized reconstruction will take 27 PSFs measured from 27 micro-lenses, instead of 2 PSFs measured from two groups of micro-lenses, as in our current implementation. For accurate reconstruction, the magnification factor for each micro-lens should be characterized experimentally to account for the focal length variation. In this way, the reconstruction will be more accurate. However, the increased computational complexity cannot be handled by our current computing platform. Further optimization of XLFM will be under future investigation.</p><p>We also reconstructed beads that were placed at extreme points in the imaging volume using a PSF that was measured by placing a fluorescent particle at the center of the field of view. We found that reconstructions resulted in nicely localized spots within a field of view of 500 μm in diameter but were distorted near the edge of the imaging volume. This is apparently due to the spatial variance of the PSF, as suggested by the reviewer.</p><p>In summary, we appreciate the reviewer’s comments, which help us identify the focal length variation of the customized micro-lenses as the major contribution to the spatial variance of PSF. We envisioned two ways to solve this problem: (1) a more precisely machined microlenses array; (2) employing generalized reconstruction algorithm in which each micro-lens is characterized individually. Both directions will be investigated in the future.</p><disp-quote content-type="editor-comment"><p>2) A follow-up: if the PSF is not fully spatially invariant, what does this mean for the statement that overlapping sub-images are permitted (subsection “Image reconstruction of XLFM”)? My understanding is that the overlap is fine so long as the PSF is fully x-y invariant, and if not, then some artifacts will be introduced. The reasons and assumptions underlying this statement should be clarified in the text.</p></disp-quote><p>We agree that spatial invariance is important for correct reconstruction over the overlapping regions. However, our current implementation cannot produce a perfectly spatially invariant PSF. If the sample is not sparse, spatially variant PSF could lead to reconstruction artifacts, which is not easy to quantify. The best way to solve this problem would be to build the next generation XLFM and to make a direct comparison with the current one.</p><disp-quote content-type="editor-comment"><p>For clarity, points (1) and (2) are not criticisms of the system, only a call for characterization of the artifacts that the reconstruction algorithm introduces when using simplifying assumptions.</p><p>3) The reconstruction algorithm (subsection “Image reconstruction of XLFM”) contains confusing notation (if I understand it correctly). The coordinates (x,y) on the left hand side of Equation 5, refer to image coordinates. But (x,y,z_k) on the right hand side refers to coordinates in the 3D volume. That's confusing, x and y should not be used for both. Moreover, I believe that PSF_A,B(x,y,z_k) are each 2 dimensional objects. So in reality, spelled out with all the indices, using ^superscript for volume coordinates and _subscript for image coordinates, I believe the equation is</p><p>ImgEst_(x',y') = sum {ObjA^(x,y,z_k) conv^(x,y,z_k) PSFA^(x,y,z_k)_(x',y') +.…}</p><p>Explaining this equation better, e.g. by writing it out as above or stating that Img_est is a 2D object in image coordinates, and PSF_A,B(x,y,z_k) is 2D in image space contingent on x,y,z_k in volume space, will make this section more understandable.</p></disp-quote><p>Thanks for the comment. We believe that the reviewer has fully understood the equation. The notations may sound confusing, but we would like to follow the convention in the field of optical imaging: PSF is conventionally defined in object space even though it is actually measured in imaging space, so that the size of the PSF reflects the imaging resolution. The image captured on the camera is also conventionally transformed and interpreted as in object space because the actual pixel size and the factor of magnification are not important in above equations. In this way, the imaging formation can be conveniently written as the convolution between object and PSF in object space.</p><disp-quote content-type="editor-comment"><p>4) Can the lateral resolution be measured instead of estimated?</p></disp-quote><p>We have added experimental characterization (<xref ref-type="fig" rid="fig1s6">Figure 1—figure supplement 6</xref>).</p><disp-quote content-type="editor-comment"><p>5) The manuscript says the reconstruction algorithm is based on optical wave theory. What do the authors mean by this? The algorithm is based on the assumption of a spatially invariant PSF and observations of how to apply Richardon-Lucy to sets of microlenses of different focal lengths. Where does this rely on optical wave theory instead of just classical optics?</p></disp-quote><p>The optical wave theory we referred here is to distinguish it from classical optics, which is often called light ray optics. In conventional light field microscopy (LFM), the reconstruction is based on the light ray assumption, which is implied in the name of “light field”. However, the light ray assumption cannot account for the limitations of resolution and the depth of view in LFM. To take these effects into account accurately, optical diffraction described by the optical wave theory needs to be incorporated into the reconstruction algorithm. Actually, M. Broxton et al. had introduced a way of doing so in the conventional LFM, as seen in Broxton et al., 2013. In XLFM, we defined a spatially invariant PSF (assuming an ideal imaging system), which reflects the resolution limit and the beam diffraction effect. For this reason, we claimed that the XLFM reconstruction algorithm was based on optical wave theory to distinguish it from conventional LFM reconstruction algorithm.</p><disp-quote content-type="editor-comment"><p>6) I assume CAD models of the microlens holder and the autofocus system exists, can these files be made available?</p></disp-quote><p>We have added CAD models of the microlens holder for fluorescence imaging and autofocus.</p><disp-quote content-type="editor-comment"><p>7) Most of the code is said to be available (e.g. real-time behavioral analysis and 3D reconstruction), but in some cases it is not mentioned. Can the code for the tracking and autofocus system be made available?</p></disp-quote><p>The code is available in the supplementary software.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>My overall opinion of the manuscript is positive. I think being able to image neuronal activity in a freely moving larval zebrafish is an advance and the current paper serves as a satisfactory proof of principle.</p><p>I have some issues regarding the term &quot;whole-brain&quot; and the resolution claimed by the authors. The authors claim, or at least imply, that they can simultaneously (within 1 Orca camera frame which has a 2048 x 2048 pixel sensor) image 800 x 800 x 200 microns at 3.4 x 3.4 x 5 micron resolution. I find this very difficult to believe. Imaging with this resolution requires imaging 800/(3.4/2) = 470 pixels in both x and y for 200/(5/2) = 80 planes (the factor of 2 arise from Nyquist sampling). Given the sensor dimensions one can fit at most 25 planes into it (5 x 5). The authors show that they are able to use their microlens distribution to image 27. I do not believe there is enough information on the chip to have the claimed resolution. The authors may be able to distinguish 2 fluorescent particles 6 microns apart as in <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7</xref>, but these are still sparse particles appearing in the center of their CMOS chip, not a densely fluorescent tissue as a pan-neuronally fluorescent larval zebrafish. I think my argument is corroborated by the data shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>: this data does not have the resolution claimed and does not show the whole brain of the larva.</p><p>The above argument assumes that the fish's head is also perfectly in focus. The z extent of a larval zebrafish head at this age is ~ 250 microns, which will already be larger than the z field of view. The axial shift shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, typically 20 microns but up to 80 will greatly affect this. The authors mention they use a 500 fps camera for the lateral tracking, but do not (or I missed) the speed of their auto-focus camera for axial tracking: how fast is this?</p></disp-quote><p>1) XLFM can cover a volume larger than 200 μm in the z direction. All data shown in the manuscript were reconstructed over a volume of 800 μm x 800 μm x 400 μm and were cropped later to remove empty space for better display. As shown in the reconstruction algorithm, there was no constraint on the number of z planes to be reconstructed. Since we measured PSF over 200 planes with 2 μm interspacing, the reconstruction was done over the same z range of 400 μm. Therefore, the whole brain of the larval zebrafish was indeed covered by XLFM. We have clarified this point in the Results, second paragraph.</p><p>2) We appreciate the reviewer’s comment on the imaging resolution. Indeed, the sparseness of neuronal activities (or sparsely labeled neurons) is a prerequisite for obtaining both high resolution and large field of view. The relationship between resolution and sparseness of neural activity was discussed and added to the manuscript and summarized in <xref ref-type="fig" rid="fig1s11">Figure 1—figure supplement 11</xref>. In short, we introduced a sparseness index ρ, defined as the fraction of neurons that areactivated at a given instant. Given the total number of neurons (~ 80,000) in the larval zebrafish brain, we performed computer simulation and identified a critical ρ = 0.11, below which neuronal activities can be resolved at optimal resolution (<xref ref-type="fig" rid="fig1s11">Figure 1—figure supplement 11B</xref>). When population activity is denser, XLFM would obtain a more coarse-grained neural activity map with reduced resolution (<xref ref-type="fig" rid="fig1s11">Figure 1— figure supplement 11C-D</xref>). We have clarified this point in the Results, third paragraph, and in Discussion, third paragraph.</p><p>3) Thanks for pointing out the error. The correct acquisition parameters for the lateral tracking camera should be 2 ms exposure time and 300 Hz (or higher) frame rate, which is consistent with the claimed lateral tracking update rate of 300 Hz. The axial tracking camera ran at 10 ms exposure time and 100 Hz frame rate, which is also consistent with claimed axial tracking update rate of 100 Hz. We have corrected and included more information in the manuscript, see Materials and methods.</p><disp-quote content-type="editor-comment"><p>I do not think that these are &quot;deal-breakers&quot;, but I think it is important for the authors to rewrite their claims and be explicit about what their system can't and can do (which is a lot). In the Discussion the authors claim to have developed a whole brain imaging setup: I am not sure what this means.</p><p><xref ref-type="fig" rid="fig1">Figure 1</xref> looks at an agarose restrained larval zebrafish. The authors should be explicit about this in the text and the Figure caption (for example the name of the figure). I do not think that panel d presents maximum intensity projections – they are far too clean for this (the bottom panel looks more like a snapshot of a 3d rendering of the stack). Can the authors correct this in the caption or be explicit about what they are showing?</p></disp-quote><p>Thanks for the comment. We explicitly mentioned the agarose-restrained condition in the main text as well as in the <xref ref-type="fig" rid="fig1">Figure 1</xref> caption. Because the main theme of <xref ref-type="fig" rid="fig1">Figure 1</xref> is to introduce the principle of XLFM and to demonstrate its capabilities for volume imaging in larval zebrafish, we think it might be better to keep the figure title unchanged.</p><p>Panel D in <xref ref-type="fig" rid="fig1">Figure 1</xref> shows maximum intensity projections of time series 3D volume images. In other words, we performed maximum intensity projections on space (top, top view; bottom, side view) and time. We have clarified this point in the figure caption accordingly.</p><disp-quote content-type="editor-comment"><p>Closed-loop systems have also been implemented in restrained fish with their tail free to move (e.g. Portugues and Engert 2011) which can remove the issue the authors mention relating to proprioception (Introduction paragraph two). The authors also mention improper vestibular feedback when fish are restrained, but in their setup, due to the closed look, the fish would also experience a reduced vestibular feedback: if the closed-loop was perfect the head would not move at all and the same vestibular deficits would be observed. If this is correct then the authors should comment on this.</p></disp-quote><p>We agree with the reviewer that the head-restrained and tail-free setting is a simple and elegant behavioral paradigm for incorporating multiple sensory cues, such as proprioception, and for studying sensorimotor transformation in larval zebrafish. We have included references to related works and added discussion on these in the manuscript (see Discussion, second paragraph).</p><p>We also agree that in our closed-loop setup, any interpretation of behaviors and neural activity associated with self-motion must take into account motion compensation driven by the tracking system. Indeed, the perception of linear acceleration, encoded by the vestibular feedback, would be significantly reduced. The perceptions of angular acceleration and the relative velocity of water flow may remain intact. We have added one paragraph discussing the limitation of our approach and future improvement of the tracking system. See Discussion, fourth paragraph.</p><disp-quote content-type="editor-comment"><p>The authors talk about &quot;visual stimulation. What does this visual stimulation consist of? This should be explained in the Materials and methods clearly.</p></disp-quote><p>Thanks! Detailed information about visual stimulation has been added in the Materials and methods section.</p><disp-quote content-type="editor-comment"><p>The claim in Results paragraph seven is a strong one and I am not sure it is fully warranted. Given the resolution and the data shown I would omit the phrase &quot;for the first time&quot; and again, explain carefully what is meant (here and in other places) by the term &quot;whole brain&quot;.</p></disp-quote><p>Thanks! We have deleted this phrase. We agree that our writing may be misleading. “Whole brain” means that our imaging volume reconstruction can cover the entire larval zebrafish head. However, to achieve close to single neuron resolution, the population neural activity must be sparse. We have clarified our essential claims in the introduction of XLFM.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The manuscript has improved and I think that most of our comments have been addressed.</p><p>The changes include a new piece of useful information: the non-idealness of the point spread function (PSF) has been measured, and attributed, in part, to small differences in focal lengths of the microlenses.</p><p>I think this is great work and the revised version is even better. There are a few final comments I'd like to make,</p><p>In the subsection “Image reconstruction of XLFM”, &quot;Furthermore, our system took practical conditions, such as imaging system and light properties, into consideration.&quot; What does this mean; can this be explained better?</p></disp-quote><p>Thanks for comment. As compared with a theoretically derived PSF, the experimentally measured one account for practically conditions, such as a non-ideal imaging objective, actual positions of individual micro-lenses, the actual spectrum of received fluorescence signal et al. We have added more description in the manuscript.</p><disp-quote content-type="editor-comment"><p>About the reconstruction algorithm: The authors opt for sticking with the conventions and use the same indices x,y on both sides of the equation. This is ok, but in that case, some explanation should be added. For example,</p><p>&quot;The 2D convolution is over x and y&quot; and &quot;Per convention in optics, x,y on both sides represent object space, even though in practice, x,y at the left will refer to image space on the camera chip and x,y on the right to the sample coordinates.&quot;</p></disp-quote><p>Thanks for the comment. We have modified the manuscript accordingly.</p><disp-quote content-type="editor-comment"><p>In the same section the authors seem to agree that the statement that the algorithm can deal with overlapping fish images depends on the invariance of the PSF, this information should be explicitly stated, i.e. include a statement like &quot;under the condition that the PSF is spatially invariant, which is satisfied apart from small aberrations, the algorithm can handle overlapping fish images&quot;.</p></disp-quote><p>Thanks for the comment. We have modified the manuscript accordingly.</p><disp-quote content-type="editor-comment"><p>Results paragraph one: &quot;Therefore, a spatially invariant point spread function (PSF) of the entire optical imaging system could be defined and measured (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>)&quot;. Here also it would be good to mention that it's an approximately spatially invariant PSF.</p></disp-quote><p>Thanks for the comment. We have modified the manuscript accordingly.</p><disp-quote content-type="editor-comment"><p>A reference to a recent paper from the Vaziri lab, Nobauer et al., 2017, should be included. (This is also about light field microscopy, but I want to emphasize that this does in no way diminish the impact of the current manuscript.)</p></disp-quote><p>Thanks for the comment. We have updated the manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>As I mentioned previously, I like the paper, and the authors have addressed most of the minor issues appropriately. There are two points which I am still not sure have been resolved.</p><p>1) I do not believe the authors have fully addressed my previous point relating to resolution, which I reproduce below. This argument may be wrong, and I would be very happy if the authors could explain to me where my logic fails.</p><p>&quot;I have some issues regarding the term &quot;whole-brain&quot; and the resolution claimed by the authors. The authors claim, or at least imply, that they can simultaneously (within 1 Orca camera frame which has a 2048 x 2048 pixel sensor) image 800 x 800 x 200 microns at 3.4 x 3.4 x 5 micron resolution. I find this very difficult to believe. Imaging with this resolution requires imaging 800/(3.4/2) = 470 pixels in both x and y for 200/(5/2) = 80 planes (the factor of 2 arise from Nyquist sampling). Given the sensor dimensions one can fit at most 25 planes into it (5 x 5). The authors show that they are able to use their microlens distribution to image 27.&quot;</p><p>I am not worried about reconstructing the volume; that is not the point here. The issue is that of resolution and discriminability of points. This involves two aspects: the &quot;optical resolution&quot; of the imaging system and the sampling. The Rayleigh resolution criterion states the minimal distance resolvable is half the width of the first diffraction order. This depends on the wavelength of the light and the NA of the system. Ideal sampling is then obtained by using the Nyquist criterion: the inverse image of a pixel should be half the optical resolution of the system, so that there is a &quot;negative dip&quot; in between the two bright pixels.</p><p>Using this last sampling definition, the number of resolvable points can be estimated as follows:</p><p>- every bright pixel has to be surrounded (in the chip) by a dark set of pixels.</p><p>- the number of bright pixels you can have on the chip in this configuration is the number of resolvable points. This is a quarter of the number of pixels on the chip.</p><p>- ideally, the inverse image of a pixel should be a quarter of the first diffraction order peak width, for sampling and optical resolution to be perfectly matched.</p><p>With a 2048 by 2048 chip, there are at most ~ 1 million resolvable points.</p><p>The authors claim they can resolve ~ 2.2 million (800 x 800 x 200 microns at 3.4 x 3.4 x 5 micron resolution).</p><p>Deconvolution is a linear process, so XLFM may &quot;shuffle or combine&quot; the intensities of different pixels, but must do so in a linear way.</p><p>In addition, as the authors state, the NA of the objective (nominally 1.05) is greatly decreased (to an effective 0.4) because a lot of light is blocked by the array casing. The optical resolution of the system is bound to be significantly affected.</p><p><xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7</xref> does try to address this issue, but I remain unconvinced because column 3 of panel b shows no dip whatsoever in between the two particles. This indicates to me that the authors are not using the Rayleigh criterion. It is definitely possible to separate two identical Gaussians whose positions differ by less than two SDs (situation which approximates to the Rayleigh criterion), but this is not what is usually called resolution.</p></disp-quote><p>Thanks for the comment.</p><p>We agree with reviewer’s comment that that a total number of resolvable points is limited by the total number of pixels on image sensor. We thus responded that the sparsity was a prerequisite for obtaining both high resolution and large field of view. By doing simulation, we found that when the sparseness index ρ, defined as the fraction of total neuron population (~80,000 in larval zebrafish), was less than 0.11, corresponding to 8,800 neurons distributed over a larval zebrafish brain, individual neurons could be resolved with an optimal resolution of 3.4 μm x 3.4 μm x 5 μm, as shown in <xref ref-type="fig" rid="fig1s11">Figure 1—figure supplement 11</xref>.</p><p>In the extreme case when there were only two particles separated in z direction, as shown in <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7B</xref>, <xref ref-type="fig" rid="fig1s2">2</xref>7 sub-images of the same two particles were captured, which provided partially redundant information for these two particles. As a result, these two particles could be resolved at optimal resolution. The theoretical analysis of this optimal resolution was provided in the Materials and methods section of “Resolution characterization of XLFM”. It was also experimentally confirmed, as shown in <xref ref-type="fig" rid="fig1s6">Figure 1—figure supplement 6</xref> and <xref ref-type="fig" rid="fig1s7">7</xref>.</p><p>To respond to reviewer’s concern on <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7</xref>, we characterized resolution by spatial frequency analysis (<xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7b</xref> column 4), which is an important and precise way to characterize the resolution. But we agree that it would be more convincing to see a dip between two particles. Because higher spatial frequency component has much lower signal to noise ratio than lower spatial frequency component, as shown in <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7B</xref> column 4, it may not be easy to directly see a dip in raw images. Therefore, raw images are conventionally deconvolved to assist visualization. In the updated <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7B</xref>, we added an extra column to show the results after deconvolution using linear Wiener filtering method. The expected dips between two particles were much more evident in the deconvolved images than that before deconvolution.</p><p>Our reconstruction method, which was developed from Richardson-Lucy deconvolution, inherited the same property that there was no limit on the number of voxels to be reconstructed: all voxels were estimated with maximum likelihood. However, as the reviewer correctly pointed out, there was no information gain during the reconstruction. As a result, although we could reconstruct 2.2 million voxels of 1.7 μm × 1.7 μm × 2 μm over the imaging volume of Ф 800 μm × 200 μm, these voxels were not completely independent variables. However, when sample was sparse, the small fraction of nonzero voxels can be treated independently, and by using Nyquist sampling and keeping the voxel small, we could achieve the optimal resolution at 3.4 μm x 3.4 μm x 5 μm. An extreme case was when there were only two particles separated in z direction, as shown in <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7B</xref> and discussed above.</p><p>When sample was dense, nearby voxels would not be independent anymore because the captured information was insufficient to assign independent value for each voxel. This resulted in degraded resolution, as shown in <xref ref-type="fig" rid="fig1s11">Figure 1—figure supplement 11</xref>. Together, given the limited number of pixels on the image sensor implemented in our setup, the optimal resolution could only be achieved when the sample was sparse. We apologize for possible misleading statements. We have clarified this point in the main text.</p><p>Throughout the manuscript, we used Abbe limitd=0.5λNA for resolution characterization, which differs slightly from Rayleigh criteriond=0.61λNA in an optical system with circular apertures.</p><p>The effective NA of 0.4 we mentioned in the manuscript is defined based on light collection efficiency. It means the light collection efficiency of this system is equivalent to the one using an objective of 0.4 NA. This collection efficiency could be improved by using more micro-lenses in the array, but it also requires more camera pixels to ensure certain field of view for each micro-lens. This collection efficiency argument is not applicable for resolution comparison.</p><disp-quote content-type="editor-comment"><p>2) I am not sure I understand the authors' claims of a spatially invariant PSF. In <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref> they in fact show it is not spatially invariant even within the focal plane (which they attribute to variation in the magnification across microlenses).</p></disp-quote><p>Our statement actually is that a spatially invariant PSF can be defined if the optical system is ideal. Our current implementation, however, is not perfect as shown in <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref>. We have modified the text to clarify this point.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #3 concerns must be addressed more fully.</p><p>I appreciate the authors' comments to the points I raised, but unfortunately I do not feel they have been addressed. I still believe there are two points which need to be addressed: the resolution and the PSF. I think both these points could be addressed by rewriting the manuscript and making claims that are both theoretically sound and supported by the data presented. Given that the manuscript is a proof of concept about an exciting and promising new imaging technique I think it is fundamental to be precise, as this paper will set the baseline for all future work involving this technique and other freely swimming whole brain imaging approaches.</p><p>1) Resolution.</p><p>The authors have not addressed my argument or provided a counterargument. I still believe my argument is correct. I think the authors are under a misconception. As I mentioned in my previous report, the resolution of an optical system depends on only two things:</p><p>a) The optical resolution of the system that results in the Rayleigh criterion.b) The sampling resolution that results in the Nyquist criterion.</p><p>Specifically, the resolution does not depend on either the reconstruction (as suggested by the authors in answer to my first review) or the sparseness (as the authors argue in the answer to my second and third review).</p><p>I repeat the basis of my argument. Given the dimensions of the chip, the number of resolvable points cannot be more than ~ (2000/2)^2 which is about 1 million. This assumes that a diffraction-limited point in the sample is images onto 1 pixel in the camera chip and that the whole chip is used. The first point is not true for the current system (see <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref> in which a 0.5um bead results in at least 6 reconstructed pixels in the image). In fact, a one such diffraction limited spot will automatically be imaged onto 27 points just by construction). The second point is also not true for this system (see <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref> in which only about half the chip is used). My estimate is that at most one fifth of this upper limit of the number of points can be resolved, about 200,000, and probably much closer to 100,000.</p><p>Paragraph two of the Results is now very confusing. In the second sentence, the authors state that they can image a field of view of ~ 800 x 800 x 200 μm with 3.4 x 3.4 x 5 μm resolution. This corresponds to 2 million resolvable points. This is probably a factor of 20 out. Then later in the paragraph it is claimed that it is 500 x 500 x 100 μm at this resolution. This corresponds to 400,000 points. First of all I think this is still way too high. And secondly I do not understand the contradictory statements in this paragraph. The resolution does not depend on the sparseness. In addition, one can interpolate and reconstruct at whatever desired resolution to make pretty images, so this does not play a role either.</p></disp-quote><p>Thanks for the comment. We would like to try our best to clarify our statement and avoid possible misunderstanding.</p><p>We believe that the major disagreement between us might be due to our different understanding on the claim, that is, XLFM could achieve optimal resolution of 3.4 x 3.4 x 5 μm within the volume of Ø 800 x 200 μm when the sample is sparse.</p><p>The reviewer argued that resolution is independent of sparsity and our claim thus meant that all <italic>N</italic> ≈ 2 million voxels in this volume should be measured independently, as imposed by Nyquist sampling theorem. On the other hand, we argued that sparsity constraint was central to this claim. If there is no sparsity constraint, we would agree that the Nyquist sampling is required. However, if sparsity constraint is imposed, which means the number of non-zero voxels is far less than the total number of voxels in the imaging volume, our reconstruction algorithm can achieve the claimed resolution of 3.4 x 3.4 x 5 μm within the volume of Ø 800 x 200 um. This claim can be illustrated by the following simplified example:</p><p>The designed XLFM has 27 micro-lenses. These micro-lenses image the same object from different view angles and form 27 sub-images on a single image sensor chip. Here, we simplify the imaging formation model and assume that each sub-image is captured by an individual camera (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>). Please note that this model is not a precise description of the actual XLFM system; but it draws a close analogy with XLFM and serves as a valid model to illustrate the design principle of XLFM. Additionally, the example is illustrated in two dimensions, but can be easily generalized to three dimensions.</p><p>As shown in <xref ref-type="fig" rid="respfig1">Author response image 1A</xref>, Camera A captures a top view of three pairs of dots within a gray square area. The imaging system of camera A is designed to have relatively high resolution in x direction, but very poor resolution in z direction. The poor resolution in z direction actually means very large depth of view in this direction. As a result, camera A can resolve two laterally aligned and closely spaced particles anywhere within the imaging field of view (gray square). This system, however, do not provide any capability to resolve two particles in z direction.</p><p>To make the system capable of resolving particles in z direction, we can add camera B to get a side view of the same object over the same field of view. The camera B provides high resolution in z direction, but poor resolution in x direction. If we combine information from both camera A and B, which are complementary to each other, we can see that this system can resolve two closely spaced particles anywhere within the field of view, as shown in <xref ref-type="fig" rid="respfig1">Author response image 1B</xref>.</p><fig id="respfig1"><label>Author response image 1.</label><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-28158-resp-fig1-v2"/></fig><p>Therefore, we claim that the imaging system, which consists of two cameras A and B oriented orthogonal to each other, provides high resolution both in x and z directions over the entire field of view when the sample is sparse.</p><p>In the above claim, the sparsity constraint is important because this system has problems to resolve denser samples. As shown in <xref ref-type="fig" rid="respfig1">Author response image 1C</xref>, three different sample distributions i, ii, and iii can generate exactly the same observation on camera A and B. In this case, it is not possible to distinguish whether four particles or two particles are present within the field of view.</p><p>The above problem can be alleviated by adding more cameras sampling from different perspectives. By adding camera C, as shown in <xref ref-type="fig" rid="respfig1">Author response image 1D</xref>, the images captured by camera C can provide useful information to distinguish all three different cases.</p><p>In the above example, each camera is analogous to one micro-lens in XLFM. In our XLFM, we have 27 micro-lenses sampling from 27 different view angles. As a result, the designed XLFM can handle much more dense samples than that illustrated in <xref ref-type="fig" rid="respfig1">Author response image 1C</xref>.</p><p>To summarize, our claim of “XLFM could achieve optimal resolution of 3.4 x 3.4 x 5 μm within the volume of Ø 800 x 200 μm when the sample is sparse” is based on following facts:</p><p>1) We showed that each micro-lens with NA of 0.075 provided an in-plane resolution of 3.4 um. This resolution was experimentally confirmed to be preserved throughout the imaging volume of Ø 800 x 200 um, as shown in <xref ref-type="fig" rid="fig1s6">Figure 1—figure supplement 6</xref>.</p><p>2) We theoretically showed that the micro-lenses on the edge of the objective’s pupil provide resolution in z direction because the PSFs generated by these micro-lenses are tilted, as shown in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> and discussed in subsection “Resolution characterization of XLFM” in manuscript. Combing this theoretical analysis and result (1), we expected that the z resolution of 4um can be achieved throughout the imaging volume of Ø 800 x 200 um. Due to the limited signal to noise ratio in practical conditions, however, we experimentally obtained a resolution of 5 μm in z direction, as shown in <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7</xref>.</p><p>3) Based on the results (1) and (2), we concluded that our designed XLFM can resolve two closely spaced particles anywhere in the imaging volume of Ø 800 x 200 μm with optimal resolution of 3.4 x 3.4 x 5 um.</p><p>4) We expected that the capability of resolving objects with optimal resolution can be generalized from the simple case of having only two particles within the field of view to more complicated cases. A detailed theoretical analysis of such generalization is out of the scope of this work. Instead, we did computer simulation and found that the sample sparseness can be a proper indicator of when this optimal resolution can be achieved, as shown in <xref ref-type="fig" rid="fig1s11">Figure 1—figure supplement 11</xref>.</p><p>5) By combining the above results, we claimed that our designed XLFM, in the absence of micro-lenses’ focal length variation, can achieve a resolution of 3.4 x 3.4 x 5 μm within the volume of Ø 800 x 200 μm when sample is sparse.</p><p>6) Due to the focal length variation, the PSF of the optical system is not fully spatially invariant (see below). As a result, the reconstruction performance is degraded comparing to our initial design (<xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref>). Thus we claimed that our current implementation of XLFM achieved optimal resolution of 3.4 x 3.4 x 5 μm within a reduced imaging volume of Ø 500 x 100 um.</p><p>Now we have streamlined the structure of the main text based on the logical flow from (1)-(6).</p><p>As a side note, our designed XLFM is not the only one that makes use of the sparsity constraint. In the field of compressed sensing, it has been demonstrated that image signals can be recovered with fewer number of measurements than that required by the Shannon-Nyquist sampling theorem when sample is sparse. Strictly speaking, our method is not compressed sensing, but our developed XLFM can also recover high resolution information of sparse objects within a large field of view in the way illustrated in <xref ref-type="fig" rid="respfig1">Author response image 1</xref>.</p><disp-quote content-type="editor-comment"><p>2) PSF.</p><p>The authors have developed a deconvolution algorithm that to my understanding calculates an effective PSF which they assume to be spatially invariant (in 3 dimensions) and which they then use to deconvolve their data. If this is true I think this is a statement that I am very happy with and support. In traditional lightfield microscopy it can be shown theoretically that the PSF is not spatially invariant. I cannot see why the setup presented by the authors would result in a spatially invariant PSF. If they claim this it should be shown theoretically (this is a methods paper). I agree that inhomogeneities arising from unequal microlens magnifications will contribute to worsen the spatial invariance of the PSF, as argued in <xref ref-type="fig" rid="fig1s8">Figure 1—figure supplement 8</xref>. But I do not believe this is the only source for them, so I do not agree with the statement in the figure legend. In <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref> the authors show that the PSF is not spatially invariant. And the data here relates to the &quot;focal plane z=0&quot; How does the PSF look at x=400um and z =100um? It will most likely look &quot;worse&quot; than that in the last panel of this figure. The authors seem to propose that the PSF of their imaging system is spatially invariant until it is not, away from the center of the field of view and the focal plane. This is not a rigorous scientific statement and not one which can be made in a methods paper that proposes a new imaging technique in a highly regarded journal such as eLife. I can definitely stand and support the argument put forward in the first sentence of this paragraph but if the authors claim spatial invariance they will have to either theoretically prove it or measure the PSF throughout the field of view and show it (and <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref> contradicts this).</p></disp-quote><p>Thanks for the comment. We would like to make our best effort to explain the underlying theory of XLFM. We have added a new section discussing the spatial invariance of the PSF in the Materials and methods of the manuscript.</p><p>Since the raw image is in 2D, the spatially invariance of PSF is only required in 2D as well. It’s implied in the reconstruction algorithm. To avoid misunderstanding, we have clarified this in the manuscript.</p><p>The definition of a spatially invariant PSF fundamentally means that in an ideal optical microscopy system, the resulting image can be described as a convolution between object and PSF (Introduction to Fourier Optics, Goodman):<disp-formula id="equ23"><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>⊗</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This equation forms the basis of our reconstruction algorithm, as shown in subsection “Image reconstruction of XLFM” of the manuscript. If the PSF is far from spatially invariant, the imaging reconstruction wouldn’t yield any meaningful result.</p><p>One of the fundamental differences between XLFM and conventional LFM is the location of the microlens array. In XLFM, the microlens array is placed at the pupil plane and the image sensor is at imaging plane, whereas in conventional LFM, the microlens array is placed at the image plane and the image sensor is at pupil plane. Only in XLFM, it is possible to define and measure a spatially invariant PSF. The reasons are as follows:</p><p>1) Spatially invariant PSFs can be defined for individual sub-imaging systems consisting of different micro-lenses.</p><p>As shown in <xref ref-type="fig" rid="respfig2">Author response image 2</xref>, which is a simplified version of <xref ref-type="fig" rid="respfig1">Author response image 1A</xref> in the manuscript, the object under the imaging objective is firstly imaged onto an intermediate imaging plane by tube lens 1, and then relayed by tube lens 2 and individual micro-lens A1 and A2 onto the camera image sensor. By definition, the imaging process in an ideal imaging system is linear and spatially invariant, so spatially invariant PSFs for sub-imaging systems consisting of micro-lens A1 and A2 can be defined as:<disp-formula id="equ24"><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊗</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⊗</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>As discussed in the Materials and methods, the convolution can be performed either in the object space or imaging space. If we perform the convolution in the imaging space, then the coordinates of <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> should be scaled by the magnification factors of their sub-image systems.</p><fig id="respfig2"><label>Author response image 2.</label><caption><title>Point spread function of XLFM.</title></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-28158-resp-fig2-v2"/></fig><p>2) A spatially invariant PSF can be defined for the entire imaging system if the magnifications of all sub-imaging systems are the same.</p><p>Because the camera captured image is the summation of all sub-images, the summation of all PSFs formed by individual micro-lenses can be defined as a single PSF if the magnifications of different sub-images are the same. Below is the simple proof:<disp-formula id="equ25"><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊗</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⊗</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>If magnifications of the sub-imaging systems A1 and A2 are the same, then <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and the above equation can be rewritten as:<disp-formula id="equ26"><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>⊗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>⊗</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The variation of individual micro-lenses’ focal length indicate that 𝑃𝑆𝐹<sub>𝐴</sub> or 𝑃𝑆𝐹<sub>𝐵</sub> (see Materials and methods) are not fully spatially invariant, but it does not affect the imaging formation theory of XLFM. The spatial variance, as measured in <xref ref-type="fig" rid="fig1s8">Figure 1—figure supplement 8</xref>, leads to degraded reconstruction performance, as shown in <xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref>. This degradation is negligible near the center of the field of view, but becomes more evident at the edge of the field of view. This is because the PSF is measured near the center of the field of view. The reconstruction algorithm will produce 27 estimates of the same object based on 27 sub-images. In the meanwhile, it tries to combine and align these estimates all together in the same coordinates. The position where the PSF is measured determines the origin of this coordinates. If the magnifications of all sub-images are the same, all estimates can be combined coherently to produce an accurate reconstruction. If magnifications of different micro-lenses are different, the reconstruction will yield an image that is clear near the origin of the coordinates but blurred at the edge, as shown in <xref ref-type="fig" rid="respfig3">Author response image 3</xref>.</p><fig id="respfig3"><label>Author response image 3.</label><caption><title>Resolution degradation caused by magnification variation of micro-lenses in XLFM.</title></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-28158-resp-fig3-v2"/></fig></body></sub-article></article>