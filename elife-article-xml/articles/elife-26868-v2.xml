<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">26868</article-id><article-id pub-id-type="doi">10.7554/eLife.26868</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Task-dependent recurrent dynamics in visual cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-33385"><name><surname>Tajima</surname><given-names>Satohiro</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9597-1381</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-84517"><name><surname>Koida</surname><given-names>Kowa</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0156-3406</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-84518"><name><surname>Tajima</surname><given-names>Chihiro I</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-84519"><name><surname>Suzuki</surname><given-names>Hideyuki</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-33145"><name><surname>Aihara</surname><given-names>Kazuyuki</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4602-9816</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-48548"><name><surname>Komatsu</surname><given-names>Hidehiko</given-names></name><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Basic Neuroscience</institution>, <institution>University of Geneva</institution>, <addr-line><named-content content-type="city">Geneva</named-content></addr-line>, <country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">JST PRESTO</institution>, <institution>Japan Science and Technology Agency</institution>, <addr-line><named-content content-type="city">Kawaguchi</named-content></addr-line>, <country>Japan</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">EIIRIS</institution>, <institution>Toyohashi University of Technology</institution>, <addr-line><named-content content-type="city">Toyohashi</named-content></addr-line>, <country>Japan</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Graduate School of Information Science and Technology</institution>, <institution>University of Tokyo</institution>, <addr-line><named-content content-type="city">Tokyo</named-content></addr-line>, <country>Japan</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of Information and Physical Sciences</institution>, <institution>Graduate School of Information Science and Technology, Osaka University</institution>, <addr-line><named-content content-type="city">Suita</named-content></addr-line>, <country>Japan</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Institute of Industrial Science</institution>, <institution>University of Tokyo</institution>, <addr-line><named-content content-type="city">Tokyo</named-content></addr-line>, <country>Japan</country></aff><aff id="aff7"><label>7</label><institution>National Institute for Physiological Sciences</institution>, <addr-line><named-content content-type="city">Okazaki</named-content></addr-line>, <country>Japan</country></aff><aff id="aff8"><label>8</label><institution content-type="dept">Brain Science Institute</institution>, <institution>Tamagawa University</institution>, <addr-line><named-content content-type="city">Machida</named-content></addr-line>, <country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing editor</role><aff><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>satohiro.tajima@gmail.com</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>24</day><month>07</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e26868</elocation-id><history><date date-type="received"><day>16</day><month>03</month><year>2017</year></date><date date-type="accepted"><day>10</day><month>07</month><year>2017</year></date></history><permissions><copyright-statement>Â© 2017, Tajima et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Tajima et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-26868-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.26868.001</object-id><p>The capacity for flexible sensory-action association in animals has been related to context-dependent attractor dynamics outside the sensory cortices. Here, we report a line of evidence that flexibly modulated attractor dynamics during task switching are already present in the higher visual cortex in macaque monkeys. With a nonlinear decoding approach, we can extract the particular aspect of the neural population response that reflects the task-induced emergence of bistable attractor dynamics in a neural population, which could be obscured by standard unsupervised dimensionality reductions such as PCA. The dynamical modulation selectively increases the information relevant to task demands, indicating that such modulation is beneficial for perceptual decisions. A computational model that features nonlinear recurrent interaction among neurons with a task-dependent background input replicates the key properties observed in the experimental data. These results suggest that the context-dependent attractor dynamics involving the sensory cortex can underlie flexible perceptual abilities.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.001">http://dx.doi.org/10.7554/eLife.26868.001</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>population coding</kwd><kwd>task demand</kwd><kwd>color vision</kwd><kwd>dynamical systems</kwd><kwd>category</kwd><kwd>central visual pathways</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002241</institution-id><institution>Japan Science and Technology Agency</institution></institution-wrap></funding-source><award-id>PRESTO JPMJPR16E6</award-id><principal-award-recipient><name><surname>Tajima</surname><given-names>Satohiro</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007884</institution-id><institution>Hoso Bunka Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Tajima</surname><given-names>Satohiro</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002241</institution-id><institution>Japan Science and Technology Agency</institution></institution-wrap></funding-source><award-id>CREST</award-id><principal-award-recipient><name><surname>Aihara</surname><given-names>Kazuyuki</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>KAKENHI 15H05707</award-id><principal-award-recipient><name><surname>Aihara</surname><given-names>Kazuyuki</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002241</institution-id><institution>Japan Science and Technology Agency</institution></institution-wrap></funding-source><award-id>Center of Innovation Program from Japan</award-id><principal-award-recipient><name><surname>Komatsu</surname><given-names>Hidehiko</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A decoding-based, state-space reconstruction reveals that neurons in macaque IT cortex change the structure of their collective attractor dynamics depending on task contexts.</meta-value></custom-meta><custom-meta><meta-name>eLife Digest</meta-name><meta-value>2.5</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Animals are able to adapt their behavior flexibly depending on task contexts, even when the physical stimuli presented to them are identical. The physiological mechanisms underlying this flexible translation of sensory information into behaviorally relevant signals are largely unknown. Recent studies indicate that context-dependent behavior is accounted for by adaptive attractor-like dynamics in the prefrontal areas (<xref ref-type="bibr" rid="bib36">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib59">Stokes et al., 2013</xref>), which associate sensory representation with behavioral responses depending on task contexts (<xref ref-type="bibr" rid="bib15">Freedman et al., 2001</xref>, <xref ref-type="bibr" rid="bib16">2002</xref>, <xref ref-type="bibr" rid="bib17">2003</xref>; <xref ref-type="bibr" rid="bib68">Wallis et al., 2001</xref>; <xref ref-type="bibr" rid="bib69">Wallis and Miller, 2003</xref>; <xref ref-type="bibr" rid="bib39">Meyers et al., 2012</xref>). In contrast to the prefrontal cortex, the visual areas have been suggested to show no or only modest task-related modulations of neural responses (<xref ref-type="bibr" rid="bib52">Sasaki and Uka, 2009</xref>; <xref ref-type="bibr" rid="bib38">McKee et al., 2014</xref>). This supports the view that sensory information is processed sequentially across the cortical hierarchy; that is, the physical properties of stimuli are encoded by the sensory cortex, and read out by the higher areas such as the prefrontal cortex.</p><p>An alternative to this sequential processing model is a view that the sensory cortex is dynamically involved in the neural mechanisms for the flexible sensory-action association. Unlike the former model, the latter does not assume a strong differentiation between sensory and higher areas, which is described in the âencoding-vs.-readoutâ framework, but allows the decision process to arise from the mutual interactions among them. In particular, assuming the involvement of sensory areas in the task-dependent behavior predicts that the neural representations in those areas are modulated by task contexts. Indeed, some studies report that neurons in the sensory areas can change their activities depending on task demands (<xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>; <xref ref-type="bibr" rid="bib40">Mirabella et al., 2007</xref>; <xref ref-type="bibr" rid="bib5">Brouwer and Heeger, 2013</xref>). For example, it is reported that performing a color categorization task modulates the neural responses to color stimuli in the ventral visual pathway, including macaque inferior temporal (IT) cortex (<xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>) and human V4 and VO1 (<xref ref-type="bibr" rid="bib5">Brouwer and Heeger, 2013</xref>).</p><p>However, no clear consensus has been reached on the functional interpretations of those context-dependent sensory modulations (i.e., the effects of task contexts that alter the sensory neural responses). Some researchers suggest that the task-dependent modulation of neural activities could reflect multiple confounding factors (<xref ref-type="bibr" rid="bib52">Sasaki and Uka, 2009</xref>). For example, although the task demands can modulate the neuronal response amplitudes in the IT cortex (<xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>), the response amplitudes in individual neurons could be affected by the changes in arousal levels (<xref ref-type="bibr" rid="bib21">Greenberg et al., 2008</xref>), visual awareness (<xref ref-type="bibr" rid="bib32">Lamme et al., 1998</xref>, <xref ref-type="bibr" rid="bib33">2002</xref>), task difficulty (<xref ref-type="bibr" rid="bib6">Chen et al., 2008</xref>) and feature-based attention (<xref ref-type="bibr" rid="bib64">Treue and MartÃ­nez Trujillo, 1999</xref>; <xref ref-type="bibr" rid="bib27">Kastner and Ungerleider, 2000</xref>; <xref ref-type="bibr" rid="bib47">Reynolds and Heeger, 2009</xref>).</p><p>To understand the functions and mechanisms of the task-dependent modulations in the sensory neurons, we need to elucidate the structures of collective dynamics in the neural populationâin particular, the dynamical structures reflecting the perceptual functions to accomplish the tasks. To this end, in the present study we analyze the spatiotemporal structures of collective neural activity recorded from the macaque IT cortex during context-dependent behavior. To focus on functional aspects of the collective dynamics, we first characterize the evolution of neuronal states within a perceptual space that is reconstructed from the neural population activities. The analysis reveals a task-dependent dynamics of sensory representation in the IT neurons, demonstrating the emergence of discrete attractors during categorical perceptions. Moreover, those attractor dynamics are found to reflect adaptive information processing and explain behavioral variabilities. Finally, through a data analysis and a computational modeling, we suggest a potential mechanism in which the task-dependent attractor structures emerge from a bifurcation in recurrent network dynamics among the sensory and downstream areas.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We analyzed the responses of color-selective neurons recorded in the macaque IT cortex, which change their activities depending on the task demands (<xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>). In the experiments, the monkeys made saccadic responses based on either of two different rules (categorization or discrimination) that associate the stimulus with different behavior. In both tasks, the monkeys were presented a sample color stimulus for 500 ms. In the categorization task, the monkeys then classified the sample color into one of two color categories, âRedâ or âGreenâ (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). In the discrimination task (also known as âmatching to sampleâ), the monkeys discriminated precise color differences by reporting which of two choice stimuli was the same color as the sample stimulus (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). We then analyzed the neural responses to the sample colors in the two tasksâwhere the visual stimuli were physically identical between those tasks.<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.26868.002</object-id><label>Figure 1.</label><caption><title>Color selectivity of IT neurons and the decoding-based stimulus reconstruction.</title><p>(<bold>a</bold>) In the categorization task, subjects classified sample colors into either a âreddishâ or âgreenishâ group. (<bold>b</bold>) In the discrimination task, they selected the physically identical colors. (<bold>c</bold>) Color tuning curves of four representative neurons in the categorization and discrimination tasks. The color selectivity and task effect varied across neurons. The average firing rates during the period spanning 100â500 ms after stimulus onset are shown. The error bars indicate the s.e.m across trials. (<bold>d</bold>) The likelihood-based decoding for reconstructing the stimulus representation by the neural population.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.002">http://dx.doi.org/10.7554/eLife.26868.002</ext-link></p><p><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.26868.003</object-id><label>Figure 1âsource data 1.</label><caption><title>Neural tuning data.</title><p>Each neuron's response evoked by each color stimulus, where the firing rates were computed within each 50 ms time windows sliding with 10 ms time steps.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.003">http://dx.doi.org/10.7554/eLife.26868.003</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-fig1-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-26868-fig1-v2"/></fig></p><p>A previous study reported that about 64% of recorded IT cells changed their response magnitudes significantly depending on the task demands (<xref ref-type="fig" rid="fig1">Figure 1c</xref>) (<xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>). Although the earlier reports have demonstrated that the modulations in individual sensory neurons could be correlated to the hypothetical models that encode categorical information (<xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>; <xref ref-type="bibr" rid="bib60">Tajima et al., 2016</xref>), the mechanisms and functional impacts of the neural population-response modulation remain to be understood. To elucidate the functional impacts of neural activity modulations, the present study directly investigates the dynamical structure of the collective responses of large numbers of neurons from a decoding perspective.</p><sec id="s2-1"><title>Reconstructing population activity dynamics from a decoding perspective</title><p>To reconstruct the stimulus representation by the neural population, we projected the population activity to stimulus space by extending the idea of likelihood-based decoding (<xref ref-type="bibr" rid="bib24">Jazayeri and Movshon, 2006</xref>; <xref ref-type="bibr" rid="bib34">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib4">Brouwer and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib20">Graf et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">Fetsch et al., 2011</xref>) such that it captures cross-conditional differences and time-varying properties in neural population representations (Materials and methods). To obtain the joint distribution of neural activities, we generated âpseudo-populationâ activities from the collection of single-neuron firing-rate distributions by randomly resampling the trials (<xref ref-type="bibr" rid="bib13">Fetsch et al., 2011</xref>).We assumed no noise correlation in our main analyses although we also confirmed by additional analyses that adding noise correlation did not affect the conclusion of this study (see Discussion). The basic procedures are as follows (<xref ref-type="fig" rid="fig1">Figure 1d</xref>): to reconstruct the subjectsâ percepts, we first built a maximum-likelihood decoder of stimulus based on the spike-count statistics of the correct trials in the discrimination task, in which the subjects reported precise color identity during stimulus presentation; next, the same decoder was used to analyze the data from the categorization task. Note that the decoded values are matched to both the presented and the perceived stimuli in the discrimination task because we used only the correct trials from that task and the monkeysâ correct rates were overall high (80â90%). Including the incorrect trials did not affect our conclusion based on the subsequent analyses. On the other hand, in the categorization task the perceived stimuli could differ from the presented stimuli. Although in the categorization task we had no access to the precise percepts of the stimulus identities but the categorical reports, we could reconstruct the putative percepts in the decoded stimulus space. The relationship between the decoder outputs and subjective percepts was also supported by follow-up analyses on the choice variability.</p><p>The decoding-based approach has two major advantages for interpreting the neural population state. First, the decoding provides a way to reduce the dimensionality of neural representation effectively by mapping the high-dimensional population state to a low-dimensional space of the perceived stimuli (which is, in the present case, one-dimensional space of color varying from red to green), which enhances visualization and analysis of the dynamical structures. Second, the decoding-based method enables clear functional interpretation of neural representation because the decoded stimuli are directly related to the subjectâs judgment of stimulus identity (note that it is often difficult to interpret global distance in a reduced space in nonlinear dimensionality-reduction methods; e.g., (<xref ref-type="bibr" rid="bib48">Roweis and Saul, 2000</xref>; <xref ref-type="bibr" rid="bib62">Tenenbaum et al., 2000</xref>; <xref ref-type="bibr" rid="bib67">van der Maaten and Hinton, 2008</xref>)). In particular, the decoded stimulus identity was what the subject had to respond to in the discrimination task, and thus we can compare the decoder output and the subjectsâ behavior (see also Materials and methods). If the decoding is successful, it means that the neural population responses to different stimuli are effectively differentiated within the space of the decoder output. Indeed, cross-validation of the decoder performance (by dividing the data from the discrimination task into two non-overlapping sets of trials) showed a high correct rate (&gt;75% on average across stimuli), which was comparable to the actual subject performance in the discrimination task. We will also compare the results to those of other dimensionality reduction techniques in a later subsection.</p></sec><sec id="s2-2"><title>Task context alters the attractor dynamics of the sensory neural population</title><p>To characterize the dynamical properties of the decoder output changes for the individual stimuli, we reconstructed the time evolution of the neural states within the space of decoder-output vs. mean firing rate (<xref ref-type="fig" rid="fig2">Figure 2a</xref>; the posterior given by the decoder in each stimulus and task condition is shown in <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). The population state trajectories during the discrimination task were accurately matched the presented stimuli, confirming the successful mapping from neural representations to stimuli (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, bottom). Remember that here we used the trials in which the subjects correctly identified the sample stimuli in the subsequent fine discrimination in the discrimination task (<xref ref-type="fig" rid="fig1">Figure 1b</xref>), thus the decoded stimulus identity should also correspond to the stimuli perceived by the subjects. In contrast to the discrimination task, we found that the same analysis for the Categorization task yielded strikingly different state trajectories (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, top), which suggests that the neural representation was altered between the two tasks. In particular, the population state trajectories in the categorization task showed attractor-like dynamics in which the state relaxes toward either of two stable points respectively corresponding to the âRedâ and âGreenâ categories along the âlineâ structure (in the horizontal direction in the figure) connecting those two stable points (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, top). The relationship between the mean firing rate and the decoded stimulus identity was also kept in the discrimination task and showed a similar âlineâ structure with little bistability (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, bottom). Interestingly, green stimuli tended to evoke larger neural responses than the red stimuli, consistently in both discrimination and categorization tasks, although the reason for this is not clear. Finally, these properties of the dynamics were robust to various changes in the decoder construction and neural noise-correlation structures in the data, indicating that the present results do not rely on the specific designs of the decoder (see <bold>Discussion</bold>). We observed that the results in the eye-fixation task were similar to those of the categorization task (data not shown), replicating the previous report that the neural tunings in the eye-fixation task shared properties with the categorization task (<xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>).<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.26868.004</object-id><label>Figure 2.</label><caption><title>Population dynamics in the perceptual domain.</title><p>(<bold>a</bold>) State-space trajectories during the categorization and discrimination tasks. Small markers show the population states 100â550 ms after stimulus onset in 10 ms steps. Large markers indicate the endpoint (550 ms). The colors of the trajectories and numbers around them refer to the presented stimulus. (<bold>b</bold>) During the categorization task, the decoded stimulus was shifted toward either the âreddishâ or âgreenishâ extreme during the late responses but not during the early responses. The thickness of the curve represents the 25thâ75th percentile on resampling. The yellow arrow on the horizontal axis indicates the sample color corresponding to the categorical boundary estimated from the behavior (subjectâs 50% response threshold) in the categorization task. (<bold>c</bold>) Evolution of the task-dependent clustering of the decoded stimulus (the curve with shade), as compared to the population average firing rate (the black solid and dashed curves). The magnitude of clustering was quantified with a clustering index, CI = (mean distance within categories) / (distance between category means) (Materials and methods). The figure shows the ratio of CIs in the two tasks, categorization/discrimination (smoothed with a 100 ms boxcar kernel for visualization). The horizontal line at CI ratioÂ =Â 1 indicates the identity between the two tasks. The difference in CI ratio was larger in the late period (450â550 ms after the stimulus onset) than the early period (100â200 ms) (p=0.001, bootstrap test). The figure shows data averaged across all stimuli. The black curve and shaded area represent the medianÂ Â±25 th percentile on the resampling. (<bold>d</bold>) The time-evolution of clustering indices in the gain-modulation and recurrent models applied to the discrimination-task data compared to the actual evolution in the categorization task (black curve, the same as in <xref ref-type="fig" rid="fig3">Figure 3c</xref>). The curve and shaded areas represent the medianÂ Â±25 th percentile on the resampling. (Inset) The cross-validation errors (<inline-formula><mml:math id="inf1"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) in model fitting measured based on the individual neuronsâ firing rate (Materials and methods). The horizontal dashed line indicates the baseline variability in neural responses, which was quantified by comparing each neuronâs firing rate in the odd and even trials. In this plot, the results of the three gain modulation models are largely overlapped with each other. Error bars: the s.e.m. across neurons. G1: gain-modulation model 1; G2: gain-modulation model 2; G3: gain-modulation model 3; R: recurrent model.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.004">http://dx.doi.org/10.7554/eLife.26868.004</ext-link></p><p><supplementary-material id="SD2-data"><object-id pub-id-type="doi">10.7554/eLife.26868.005</object-id><label>Figure 2âsource data 1.</label><caption><title>Neural tuning data.</title><p>Each neuron's response evoked by each color stimulus, where the firing rates were computed within each 50 ms time windows sliding with 10 ms time steps.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.005">http://dx.doi.org/10.7554/eLife.26868.005</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-fig2-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-26868-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.26868.006</object-id><label>Figure 2âfigure supplement 1.</label><caption><title>Posterior probability distributions in each stimulus and task.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.006">http://dx.doi.org/10.7554/eLife.26868.006</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-26868-fig2-figsupp1-v2"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.26868.007</object-id><label>Figure 2âfigure supplement 2.</label><caption><title>Trajectories and weights in trained models.</title><p>(<bold>aâd</bold>) Trajectories generated by the trained models. (<bold>eâh</bold>) The learned weights in the individual models. (<bold>i</bold>) The decoded vs. presented stimuli in the individual models.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.007">http://dx.doi.org/10.7554/eLife.26868.007</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-26868-fig2-figsupp2-v2"/></fig></fig-group></p><p>Remarkably, the attraction toward stable points continued throughout the stimulus presentation period, even after the population average firing rate had stabilized (as demonstrated by the horizontal shifts in <xref ref-type="fig" rid="fig2">Figure 2a</xref>, top). This also confirms that the dynamics in decoded stimuli are not merely reflecting the changes in the overall firing rate in the population (which could be potentially concerned to affect the decoding analysis through the changes in signal-to-noise ratio in the data). The polarity of the modulation depended strongly on the presented stimulus identity (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). The modulation was not large at the beginning of the stimulus presentation (light plot along diagonal in <xref ref-type="fig" rid="fig2">Figure 2b</xref>) but was magnified in the late period to form the two distinct clusters (dark plots in âSâ shape, <xref ref-type="fig" rid="fig2">Figure 2b</xref>). The evolution of the task-dependent effect on clustering continued across the entire period of stimulus presentation, and was not directly associated with the dynamics of the mean firing rate, which became stable about 250 ms after the stimulus onset (<xref ref-type="fig" rid="fig2">Figure 2c</xref>).</p></sec><sec id="s2-3"><title>The recurrent model explains the stimulus-dependent dynamics</title><p>Standard models of a recurrent dynamical system, in which the systemâs energy function relaxes as the state evolves toward either stable point, naturally account for the dynamics converging to stable point attractors in the categorization task. In addition, the dependency on presented stimulus identity indicates that the modulation was dynamically driven by the visual input, rather than by <italic>pre-readout</italic> (i.e., stimulus-invariant) modulation of neural response gains, such as conventional feature-based attention (<xref ref-type="bibr" rid="bib64">Treue and MartÃ­nez Trujillo, 1999</xref>). These facts are more consistent with the recurrent model than conventional gain-modulation models as an explanation of the population dynamics reported here. To verify this, we next examined how gain-modulation and recurrent models could account for the quantitative aspects of modulation dynamics.</p><p>To analyze the dynamics of neural modulation quantitatively, we considered three gain-modulation models (in which neural response gains could depend on the task and either of time and stimulus; <xref ref-type="fig" rid="fig2">Figure 2d</xref>) and a recurrent model (response modulation via self-feedback through mutual connections to two hidden units, whose weights depended on the task but neither on time nor on the stimulus identity; <xref ref-type="fig" rid="fig2">Figure 2d</xref>). Note that we did not assume explicit stimulus-dependency of model parameters in any of the three models. We derived the model parameters based on the recorded neural responses, such that the modulated neural responses in the discrimination task fit the responses in the categorization task (full details of the modeling are provided in the Materials and methods). Using these four models, we determined to what extent the gain modulations and recurrent modulation predict the temporal evolution of decoder output changes in the categorization task. The model-fitting performances were assessed using cross-validation based on two separate sets of trials: the first set was used to train models, and the second was used to test each modelâs fitting performance. We computed the cross-validation errors, <inline-formula><mml:math id="inf2"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>CV</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, directly based on the difference between the predicted and actual neural population activities, thus the measure is independent of the assumptions about the decoder (Materials and methods).</p><p>Among the four models, we found that the recurrent model showed the smallest cross-validation error in terms of the individual neuronâs firing rates (<xref ref-type="fig" rid="fig2">Figure 2d</xref>, inset). Indeed, neither gain-modulation model could account for the large increase in decoder output change in the late period (aboutÂ &gt;150 ms) after the stimulus onset (<xref ref-type="fig" rid="fig2">Figure 2d</xref>, the green and blue curves, the stimulus-wise trajectories shown in <xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2</xref>). The time- or stimulus-dependency of the gain parameters did not make a major difference in the prediction performance among the three gain-modulation models, suggesting that the modulation depends both on stimulus and time. On the other hand, the recurrent model explained the large continuous increase in decoder output change (<xref ref-type="fig" rid="fig2">Figure 2d</xref>, the magenta curve). It should be noted that the parameters in the recurrent model were constant across time, in contrast to the time-variant gain-modulation model. This means that the time-invariant recurrent model is superior even to the time-variant gain-modulation model at explaining the task-dependent modulation of neural population dynamics. The reason for this is that the effective modulation signals in the recurrent model could vary across different stimuli because the recurrent architecture allowed the modulation to depend on the neuronsâ past activities evoked by stimulus, leading to an âimplicitâ dependency on stimulus and time. It is remarkable that the recurrent model is capable of describing the dynamic activity modulations without assuming any explicit parameter change across stimuli and time, even better than the time- and stimulus-dependent gain modulation, which had much more parameters than the recurrent model. The results were similar when we assumed fully-connected pairwise interactions instead of the restricted connection via the hidden units. All the results were cross-validated, making it unlikely that the difference in model performance was caused by overfitting. In addition, the superiority of the recurrent model was robustly observed with changes in the decoder construction and neural noise correlations (Discussion). These results support the idea that the task-dependency of neural dynamics originates from a recurrent mechanism, although we do not exclude the possibility of more complex gain-modulation mechanisms (that depend on both the stimulus and time) as substrates of the context-dependent dynamics observed here (see also Discussion). Note that the analysis here compares the data-fitting performance of gain-modulation and recurrent models, but does not aim at explaining how the task-dependent attractor structure can emerge. A possible mechanism underlying the task-dependent attractor dynamics is discussed later.</p></sec><sec id="s2-4"><title>Reconstructed collective dynamics explains choice variability</title><p>We also found that the neural state represented in the space of the decoded stimulus was closely related to the subjectsâ subsequent behavior. First, the locus of the behavioral classification boundary in the categorization task, which moderately prefers the âGreenâ category, was replicated by the stimulus classification based on decoder output (<xref ref-type="fig" rid="fig3">Figure 3a,b</xref>). This suggests the decoded-stimulus space used here was closely related to the behavioral response dimension. Second, the modulation of the dynamics reflected the subjectsâ trial-to-trial response variability. The subjectâs choices between the âRedâ and âGreenâ categories were variable across trials, particularly for the stimuli around the classification boundary (stimuli #4â6), even when the task condition and the presented stimulus were the same. To investigate the mechanism underlying this behavioral variability, we reanalyzed the neural responses during the categorization task using the same decoding protocol used in the previous sections, but now separated the trials into two groups according to the subsequent choice behavior. We found that the behavioral fluctuation was reflected in the preceding population dynamics in the decoded-stimulus space (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). The neural state shifted toward the âRedâ extreme before the subject classified the stimulus into the âRedâ category, whereas the state shifted toward the âGreenâ extreme before classifying it into the âGreenâ category. The difference was small at the beginning of the response but gradually increased as time elapsed (<xref ref-type="fig" rid="fig3">Figure 3d,e</xref>). Gradual amplification of small differences in the initial state is a general property of a recurrent dynamical system having two distinct stable attractors, which further supports the recurrent model. Note that the current decoding analysis shares some concept with the conventional choice-probability analysis in single neurons (<xref ref-type="bibr" rid="bib3">Britten et al., 1996</xref>), but the current decoder analysis focuses more on the collective representation by neural population. When the neuronal decoding is not linear and static, it is not necessarily straightforward to relate the dynamics of the two measures (a previous study shows modest choice probabilities at the single-neuron level in a similar color discrimination task (<xref ref-type="bibr" rid="bib37">Matsumora et al., 2008</xref>)). In addition, the decoding analysis allows us to specify not only choice polarities but also the estimated perceptual contents (color identities) at each moment.<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.26868.008</object-id><label>Figure 3.</label><caption><title>Choice-related dynamics.</title><p>(<bold>a</bold>) Actual monkey behavior. Note that the monkeysâ subjective category borders were consistent with the decoder output. The error bar is theÂ standard error of mean. The yellow arrow on the horizontal axis indicates the sample color corresponding to the putative categorical boundary based on the behavior. (<bold>b</bold>) Fraction of selecting green category predicted by the likelihood-based decoding. The shaded area indicates the 25thâ75th percentile on resampling. (<bold>c</bold>) The same analysis as <xref ref-type="fig" rid="fig2">Figure 2a</xref> (top) but with trial sets segregated based on whether the monkeys selected the âredâ or âgreenâ category. The results for stimuli #4â6 are shown. (<bold>d</bold>) The same analysis as <xref ref-type="fig" rid="fig2">Figure 2b</xref>, except that the trials were segregated based on the behavioral outcome. For stimuli #1â3 (#7â11), only the âRedâ (âGreenâ) selecting trials were analyzed because the subjects rarely selected the other option for those stimuli. (<bold>e</bold>) Evolution of difference in the decoded color. Data were averaged across stimuli #4â6. The difference in the decoded stimulus was larger during the late period (450â550 ms) than the early period (50â150 ms) (p=0.002, permutation test).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.008">http://dx.doi.org/10.7554/eLife.26868.008</ext-link></p><p><supplementary-material id="SD3-data"><object-id pub-id-type="doi">10.7554/eLife.26868.009</object-id><label>Figure 3âsource data 1.</label><caption><title>Neural tuning and population response data.</title><p>Each neuron's response evoked by each color stimulus, where the firing rates were computed within each 50 ms time windows sliding with 10 ms time steps, and the intermediate data summarizing the population responses.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.009">http://dx.doi.org/10.7554/eLife.26868.009</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-26868-fig3-v2"/></fig></p></sec><sec id="s2-5"><title>Dynamical modulation enhances task-relevant information</title><p>The evidence so far indicates that the neural population in the IT cortex flexibly modulates its recurrent dynamics depending on the task context. What is this modulation for? We hypothesized that the modulation is a consequence of stimulus information processing adapted to the changing task demands. To test this possibility, we computed the mutual information between the neural population firing and the stimulus identity (hue) or stimulus category. The mutual information was estimated by assuming no noise correlations among neurons, and those estimates provide the upper limits for the information extracted from the neural state trajectories, which indicates how the dynamical modulations could contribute to the task-relevant information processing. We found that the modulatory effect was accompanied by selective increases in the task-relevant stimulus information conveyed by the neural population (<xref ref-type="fig" rid="fig4">Figure 4aâc</xref>). Namely, category information increased in the categorization task compared with the discrimination task, whereas hue information increased in the discrimination task. The difference in category information was observed in the relatively late period of the response (<xref ref-type="fig" rid="fig4">Figure 4c</xref>), consistent with the slow clustering dynamics described in the previous sections. On the other hand, the hue information differed only in the early period of response; this could be partially due to the stronger late responses (i.e., higher signal to noise ratio) in the categorization task. The fact that the modulation of the neural dynamics increases the task-relevant information indicates that the modulation benefits the subjects switching the tasks depending on the context.<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.26868.010</object-id><label>Figure 4.</label><caption><title>Modulation increases task-relevant information.</title><p>(<bold>a</bold>) The evolution of mutual information about category. (<bold>b</bold>) The evolution of mutual information about hue. (<bold>c</bold>) The evolution of the mutual information difference after the stimulus onset. The dots on top of each panel indicate the statistical significance (p&lt;0.05, permutation test; black dots: larger category information in the categorization task; gray dots: larger hue information in the discrimination task; the dots on top of panel c just repeat the test results shown in panels a and b).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.010">http://dx.doi.org/10.7554/eLife.26868.010</ext-link></p><p><supplementary-material id="SD4-data"><object-id pub-id-type="doi">10.7554/eLife.26868.011</object-id><label>Figure 4âsource data 1.</label><caption><title>Neural tuning and population response data.</title><p>Each neuron's response evoked by each color stimulus, where the firing rates were computed within each 50 ms time windows sliding with 10 ms time steps, and the intermediate data summarizing the population responses.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.011">http://dx.doi.org/10.7554/eLife.26868.011</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-fig4-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-26868-fig4-v2"/></fig></p></sec><sec id="s2-6"><title>Comparison to other methods of dimensionality reduction</title><p>We have shown that the decoding approach captures the task-dependent attractor-like dynamics in the neural population. To examine how the other dimensionality reduction methods capture the task-dependent natures of the collective neural dynamics, we first applied the principal component analysis (PCA) to the neural responses during the stimulus presentation. <xref ref-type="fig" rid="fig5">Figure 5a</xref> shows the reconstructed trajectories and the clustering index of the neural population states in the space spanned by PCs 1â3. The trajectories for categorization and discrimination tasks largely overlapped, and the task-dependent clustering was not obvious in this space despite that these top three PCs together explained more than 60% of the total variance (<xref ref-type="fig" rid="fig5">Figure 5f</xref>). This indicates that the task-dependent components of the dynamics are hidden in the other dimensions. Similarly, it was not straightforward to demonstrate the emergence of two discrete attractors in the categorization task with other unsupervised dimensionality reduction methods such as t-stochastic neighbor embedding (tSNE) (<xref ref-type="bibr" rid="bib67">van der Maaten and Hinton, 2008</xref>) (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). Other semi-supervised and supervised linear dimensionality reduction/decoding methods, including the demixed PCA (<xref ref-type="bibr" rid="bib2">Brendel and Machens, 2011</xref>; <xref ref-type="bibr" rid="bib28">Kobak et al., 2016</xref>), a population vector (<xref ref-type="bibr" rid="bib19">Georgopoulos et al., 1986</xref>) and an optimal linear decoder (<xref ref-type="bibr" rid="bib50">Salinas and Abbott, 1994</xref>; <xref ref-type="bibr" rid="bib46">Pouget et al., 1998</xref>), did not demonstrate the clear task-dependent clustering effect. These results implicate that the task-dependent components could be obscured when visualized naively with some of those conventional methods.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.26868.012</object-id><label>Figure 5.</label><caption><title>Comparison to other dimensionality reduction methods.</title><p>(<bold>aâe</bold>) The results of five different dimensionality reduction methods. (Left) the trajectories visualized in the reduced state space. (Right) The time evolutions of the clustering index ratio in each reduced space. The conventions follow that of <xref ref-type="fig" rid="fig2">Figure 2d</xref>. In (<bold>aâc</bold>), the trajectories in 0â550 ms after stimulus onset are shown; in (<bold>d, e</bold>), the trajectories in 100â550 ms after stimulus onset are shown, as in the <xref ref-type="fig" rid="fig2">Figure 2a</xref>, because the decoder outputs were unreliable before 100 ms. The filled and open circles indicate the end point (at 550 ms) of the trajectories in the categorization and discrimination tasks, respectively. The colors of trajectories indicate the presented stimuli (#1-11). The parameters used in each dimensionality reduction method is provided are provided in Materials and methods. (<bold>a</bold>) PCA. The left panel shows the top 3 principal components (PCs). The left panel depicts the clustering indices based on the top 3 components (black, solid) and all the neural activities (purple, dashed). (<bold>b</bold>) Three-dimensional space obtained by t-stochastic neighbor embedding (tSNE). (<bold>c</bold>) Demixed PCA (dPCA). The left panel shows the top component from each of stimulus-dependent (black, solid), task-dependent (cyan, dotted), and stimulus-task-interaction (purple, dashed) components. (<bold>d</bold>) Population vector decoding. The horizontal and vertical axes show the decoder output and the average firing rate in population, respectively. (<bold>e</bold>) Optimal linear decoder. The horizontal and vertical axes show the decoder output and the average firing rate in population, respectively. (<bold>f</bold>) The fraction of data variance explained by each principal component (PC) in PCA. (<bold>g</bold>) The fraction of data variance explained by each demixed component (dPC) in dPCA. (Inset) The pie chart showing the relative contributions of stimulus, task, interaction and condition-independent components.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.012">http://dx.doi.org/10.7554/eLife.26868.012</ext-link></p><p><supplementary-material id="SD5-data"><object-id pub-id-type="doi">10.7554/eLife.26868.013</object-id><label>Figure 5âsource data 1.</label><caption><title>Neural tuning and population response data.</title><p>Each neuron's response evoked by each color stimulus, where the firing rates were computed within each 50 ms time windows sliding with 10 ms time steps, and the intermediate data summarizing the population responses.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.013">http://dx.doi.org/10.7554/eLife.26868.013</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-fig5-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-26868-fig5-v2"/></fig></p></sec><sec id="s2-7"><title>Bifurcation of attractor dynamics in a recurrent model</title><p>The analyses in the previous sections have indicated the flexible recurrent interactions that modulate the structures of attractors depending on the task context. What mechanism could explain such a dynamic change in neural dynamics? Here, we show a simple potential mechanism that accounts for the flexible changes in attractor structures in the collective neural dynamics.</p><p>We extended a model of prefrontal attractor dynamics that was proposed in the context of two-interval discrimination (<xref ref-type="bibr" rid="bib35">Machens et al., 2005</xref>) by introducing a recurrent interaction that involves a population of hue-selective neurons. <xref ref-type="fig" rid="fig6">Figure 6a</xref> illustrates a potential mechanism for the context-dependent change in attractor structure. We assume that the hue-selective neurons (hereafter referred to as âhue-neuronsâ) in the IT cortex have mutual interaction with category-selective neurons (hereafter, âcategory-neuronsâ) in the frontal or other cortical areas. The hue neurons receive sensory input from earlier visual areas. The connectivity weights between hue- and category-neurons are modeled using the functions of the preferred hues in hue-neurons such that a âredâ category-neuron exhibits excitatory interactions with hue-neurons preferring reddish hues and inhibitory interactions with neurons preferring greenish hues (similar for âgreenâ category-neuron). We assume that the category neurons also receive a common background input, and respond based on an activation function with response threshold and saturating nonlinearity, which characterizes the categorical response in cortical neurons (<xref ref-type="bibr" rid="bib15">Freedman et al., 2001</xref>) (see Materials and methods).<fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.26868.014</object-id><label>Figure 6.</label><caption><title>Bifurcation of attractor dynamics in a simple neural circuit model.</title><p>(<bold>a</bold>) Schematic of the model circuit architecture. IT hue-selective neurons (hereafter, hue-neurons), <inline-formula><mml:math id="inf3"><mml:mi>H</mml:mi></mml:math></inline-formula>, with different preferred stimuli (varying from red to green) receive sensory input, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, from the earlier visual cortex. The hue neurons interact with category neuron groups <inline-formula><mml:math id="inf5"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> through bottom-up and top-down connections with weights (<inline-formula><mml:math id="inf7"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mtext>BU</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf8"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mtext>BU</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>) and (<inline-formula><mml:math id="inf9"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mtext>TD</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf10"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mtext>TD</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>), respectively. The category neurons also receive a common background input, <inline-formula><mml:math id="inf11"><mml:mi>B</mml:mi></mml:math></inline-formula>, whose magnitude depends on task context. Note that the modeled hue-neurons covered entire hue circle, [-Ï,Â Ï], although the figure shows only the half of them, corresponding to the stimulus range from red to green. (<bold>b</bold>) Activity evolution represented in the space of category-neurons in the discrimination task (where the background input <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>). The red (dashed) and green (solid) curves represent nullclines for category-neurons 1 and 2, respectively. The black line shows a dynamical trajectory, starting from (0, 0) and ending at a filled circle. The gray arrows schematically illustrate the vector field. (<bold>c</bold>) The same analysis as in panel c but in the categorization task (where <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). The black and blue lines show two different dynamical trajectories, starting from (â0.01, 0.01) and (â0.01, 0.01), respectively (indicated as numbers â1â and â2â in the figure), and separately ending at filled circles. (<bold>d</bold>) The number of stable fixed points is controlled by the parameter <italic>B</italic>. Here, the parameter <italic>B</italic> was continuously varied as the bifurcation parameter while the other parameters were kept constant. The vertical axis shows the difference of category neuron activities, <inline-formula><mml:math id="inf14"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, corresponding to the fixed points. The solid black and blue curves show the stable fixed points; the dashed line indicates the unstable fixed point. The stimulus value was <italic>s</italic>Â =Â 0. (eâk) The model replicates recorded neural population dynamics. (<bold>e</bold>) Presented and decoded stimuli. The same analysis as in <xref ref-type="fig" rid="fig2">Figure 2b</xref> was applied to the dynamics of the modeled hue-neurons. (<bold>f</bold>) The same as panel e, except that the trials were segregated based on the choices (i.e., to which fixed point the neural states were attracted). The plot corresponds to <xref ref-type="fig" rid="fig3">Figure 3d</xref>. (<bold>g</bold>) Evolution of difference in the decoded color, corresponding to <xref ref-type="fig" rid="fig3">Figure 3e</xref>. (<bold>h</bold>) Mean activity of the entire neural population, corresponding to <xref ref-type="fig" rid="fig2">Figure 2c</xref>, inset. (<bold>i</bold>) Differences in mutual information about category and hue between the categorization and discrimination tasks, corresponding to <xref ref-type="fig" rid="fig4">Figure 4c</xref>. (<bold>j</bold>) The activity trajectories of the modeled hue-neurons population in PCA space, corresponding to <xref ref-type="fig" rid="fig5">Figure 5a</xref>. Note that the scaling of the stimulus coordinate (ranging from <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) used in the model is not necessarily identical to that of experimental stimuli (index by colors #1 â #11), and point of this modeling is to replicate the qualitative aspects of the data.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.014">http://dx.doi.org/10.7554/eLife.26868.014</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-26868-fig6-v2"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.26868.015</object-id><label>Figure 6âfigure supplement 1.</label><caption><title>Nullclines in different stimulus conditions and background input.</title><p>Under a strong background inhibition (<inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>, corresponding to the âdiscrimination taskâ), the system always have only one stable fixed point. On the other hand, under a weak background inhibition, (<inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, corresponding to the âcategorization taskâ), the system can have two stable fixed points for the neutral stimuli (<inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). This is consistent with the monkey behavior that was more variable for the neutral stimuli compared to the stimuli at the reddish/greenish category extreme. The convention follows that of <xref ref-type="fig" rid="fig6">Figure 6b and c</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.015">http://dx.doi.org/10.7554/eLife.26868.015</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-26868-fig6-figsupp1-v2"/></fig></fig-group></p><p>This system has different numbers of stable attractors depending on the strength of common inhibitory background input (parameter <inline-formula><mml:math id="inf20"><mml:mi>B</mml:mi></mml:math></inline-formula>), with the connectivity among neurons unaffected (<xref ref-type="fig" rid="fig6">Figure 6bâd</xref>). The neural state converges to a single stable equilibrium point under a strong background inhibition (<xref ref-type="fig" rid="fig6">Figure 6b</xref>) whereas two distinct stable equilibrium points emerges under a weak or no background input, yielding bistability that depends on the initial state (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). <xref ref-type="fig" rid="fig6s1">Figure 6âfigure supplement 1</xref> shows the set of nullclines for all the color presentations, demonstrating that the bistability is observed specifically for the neutral stimuli. This agrees with the actual monkeysâ behavior in which the response varied between two categories only for the neutral stimuli (#4â#6).</p><p>We confirmed that the model replicated multiple aspects of the collective neural dynamics observed in IT cortex. First, the representation of modeled hue neurons (hypothetical IT neurons) showed the gradually evolving biases toward either of two extreme stimuli (âredâ or âgreenâ; (<xref ref-type="fig" rid="fig6">Figure 6e</xref>) as well as the moderately higher mean activity in the categorization task (<xref ref-type="fig" rid="fig6">Figure 6h</xref>). Second, the recurrent dynamics replicated the gradual development of the choice-related neural variability (<xref ref-type="fig" rid="fig6">Figure 6f,g</xref>). Third, the circuit enhanced the task-relevant information (<xref ref-type="fig" rid="fig6">Figure 6i</xref>). Finally, the task-dependent components of dynamics could be obscured when visualized with PCA (<xref ref-type="fig" rid="fig6">Figure 6j</xref>), which is also consistent with the results in IT neurons (<xref ref-type="fig" rid="fig5">Figure 5a</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We demonstrated that the task context modulates the structures of collective neural dynamics in the macaque IT cortex. The neural population in the IT cortex exhibited the dynamics with two discrete attractors that respectively corresponded to the two task-relevant color categories in the categorization task. The trial-to-trial variability in the dynamics confirmed that those two stable attractors co-existed under a single stimulus, thus the observed bistability reflects an inherent property of neural circuit. Remarkably, we found that the patterns of the neural state evolution were explained by a recurrent mechanism, but not fully accounted for by conventional gain-modulation models such as the ones assumed for top-down attention (<xref ref-type="bibr" rid="bib64">Treue and MartÃ­nez Trujillo, 1999</xref>; <xref ref-type="bibr" rid="bib47">Reynolds and Heeger, 2009</xref>).</p><p>The present hierarchical recurrent model shares some features with other recent models including the recurrent interactions between top-down and bottom-up signals (<xref ref-type="bibr" rid="bib12">Engel et al., 2015</xref>; <xref ref-type="bibr" rid="bib72">Wimmer et al., 2015</xref>; <xref ref-type="bibr" rid="bib22">Haefner et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Tajima et al., 2016</xref>). Theoretically, the hierarchical recurrent circuit can approximate a probabilistic inference on categorical stimuli, in general dynamic contexts (<xref ref-type="bibr" rid="bib22">Haefner et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Tajima et al., 2016</xref>). The recurrent interactions via the top-down projection are also suggested to have significant roles in learning categorical tasks (<xref ref-type="bibr" rid="bib12">Engel et al., 2015</xref>) and causing the choice related fluctuations within single neuron activities (<xref ref-type="bibr" rid="bib12">Engel et al., 2015</xref>; <xref ref-type="bibr" rid="bib72">Wimmer et al., 2015</xref>). The present results support the model by showing the hallmarks of recurrent interactions collective neural dynamics. A unique point in the present model is that it also explains the context-dependent structure of collective neural dynamics in terms of the bifurcation of attractors caused by a simple change in the background input to the categorical neurons. Lastly, although the present results suggest a profound contribution by a recurrent mechanism to the context-dependent modulation of sensory cortex dynamics, which has not been emphasized in previous studies, we do not exclude the potential contributions of a gain-modulation mechanism; rather, it is quite possible that the brain uses a combination of both the recurrent and feedforward mechanisms.</p><p>Recent studies emphasize a variety of stimulus-dependent contextual modulations, particularly in the early visual cortex (<xref ref-type="bibr" rid="bib63">Toth et al., 1996</xref>; <xref ref-type="bibr" rid="bib54">Sceniak et al., 1999</xref>, <xref ref-type="bibr" rid="bib53">2002</xref>; <xref ref-type="bibr" rid="bib49">Sadakane et al., 2006</xref>; <xref ref-type="bibr" rid="bib61">Tajima et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">Solomon and Kohn, 2014</xref>; <xref ref-type="bibr" rid="bib7">Coen-Cagli et al., 2015</xref>). However, it is yet to be elucidated whether the same mechanisms also apply to the context-dependent categorical processing in IT cortex as studied here, and how such a modulation could be implemented in biological systems without any recurrent mechanisms. Note that, in principle, a stimulus-dependent gain modulation requires a form of self-referencing process (which is naturally implemented by recurrent mechanisms) because it implies the stimulus encoding being modulated by the encoded stimulus itself, whether the source of modulation is the fluctuations in choice-related activity (<xref ref-type="bibr" rid="bib43">Nienborg and Cumming, 2009</xref>) or attention (<xref ref-type="bibr" rid="bib11">Ecker et al., 2016</xref>). Nonetheless, the mathematically equivalent effects could be achieved by a feedforward mechanism in physiological circuits that feature an information duplication (e.g., two parallel feedforward pathways converging at IT cortex, in which one has a longer latency than others). We do not exclude this possibility. This point could be tested with physiological recordings with which we can investigate the models replicating the noise correlation structures sensitive to the task contexts or estimate the causal interactions by artificially (in)activating specific areas. Our current results demonstrate that the task-dependent neural dynamics were at least not fully accounted for by conventional forms of stimulus-invariant gain modulations such as assumed in a previous study.</p><p>As a key methodology, we took a decoding approach to reconstruct the perceptual space from neural population activity. One may concern a possibility that the results rely on the selection of decoder. To examine this point, we replicated the same analyses with different decoders, and confirmed that the results reported in this paper were robust to various changes in the decoder construction, such as introducing noise correlations in neural responses, removing the half of cells to use, assuming non-Gaussian models, and ignoring the time dependence (as summarized in <xref ref-type="fig" rid="fig7">Figure 7</xref>). This suggests that the present results do not require fine tunings of the decoder constructions or assuming the independent noises across neurons. On the other hand, the task dependence of attractor structures could be unclear when visualized with-conventional unsupervised dimensionality reduction methods, despite that PCA could extract cluster structures in a previous human neuroimaging with a color naming task (<xref ref-type="bibr" rid="bib5">Brouwer and Heeger, 2013</xref>). The effectiveness of the decoding approach shares some aspects with other recent labeled dimensionality-reduction approaches applied to neural population data (<xref ref-type="bibr" rid="bib36">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Okazawa et al., 2015</xref>). Although it is beyond the scope of the current study to compare all the possible dimensionality reduction methods, we suggest that analyzing neural-population state-space from a decoding perspective could be useful to extract the hidden dynamical properties that are relevant to the functions of collective neural responses.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.26868.016</object-id><label>Figure 7.</label><caption><title>Robustness of the results to changes in the decoder.</title><p>We replicated the main results of the paper using four different decoders. Both the stimulus-dependent clustering effect and the temporal evolution were replicated with those decoders. (Left) State-space trajectories during the categorization task (corresponding to <xref ref-type="fig" rid="fig2">Figure 2a</xref>, top). (Right) Time-evolution of clustering indices in the gain modulation and recurrent models compared to that in the categorization-task data (corresponding to <xref ref-type="fig" rid="fig2">Figure 2d</xref>). (<bold>a</bold>) Results obtained by simulating noise correlation among neurons. Here we assumed that the covariance <inline-formula><mml:math id="inf21"><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> between two different neurons, <italic>i</italic> and <italic>j</italic>, is proportional to the correlation between their mean spike counts (<xref ref-type="bibr" rid="bib8">Cohen and Kohn, 2011</xref>; <xref ref-type="bibr" rid="bib45">Pitkow et al., 2015</xref>): <inline-formula><mml:math id="inf22"><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:msqrt><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Î±</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <italic>k</italic> is a constant shared across all neuron pairs (here, <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), and <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the Fano factor for neuron <inline-formula><mml:math id="inf25"><mml:mi>i</mml:mi></mml:math></inline-formula>. (<bold>b</bold>) Results based on a subset of the recorded cell population; excluded are cells showing extremely high or low activity, as compared to the typical firing rate of the population. We only used cells whose average firing rates (the average across all stimuli and time bins) were within the 25th-75th percentile of the whole population. (<bold>c</bold>) Results with a decoder based on Poisson spike variability. The generative model of neuron <italic>i</italic>âs spike count in response to stimulus <italic>s</italic> at time <italic>t</italic> was given by <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:math></inline-formula> (i.e., the log likelihood was provided by <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msubsup><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>). (<bold>d</bold>) Results with a time-invariant decoder. The mean and variance of each neuronâs spike count were computed by pooling all the time bins during the period spanning 200â550 ms after stimulus onset.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.016">http://dx.doi.org/10.7554/eLife.26868.016</ext-link></p><p><supplementary-material id="SD6-data"><object-id pub-id-type="doi">10.7554/eLife.26868.017</object-id><label>Figure 7âsource data 1.</label><caption><title>Neural tuning and population response data.</title><p>Each neuron's response evoked by each color stimulus, where the firing rates were computed within each 50 ms time windows sliding with 10 ms time steps, and the intermediate data summarizing the population decoding results.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.017">http://dx.doi.org/10.7554/eLife.26868.017</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-fig7-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-26868-fig7-v2"/></fig></p><p>Previous studies have proposed that context-dependent decision-making is achieved through flexible modulations of recurrent attractor dynamics within the prefrontal cortex (<xref ref-type="bibr" rid="bib36">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib59">Stokes et al., 2013</xref>). The present results imply that the dynamical mechanisms of context-dependent computation can include not only the prefrontal areas but also the sensory cortex, potentially organizing the distinct representational layers such as hypothesized in the present model (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Although an earlier study reported attractor-like dynamics in the IT cortex during object categorization (<xref ref-type="bibr" rid="bib1">Akrami et al., 2009</xref>), the flexible modulation of a dynamical structure depending on task context has not been demonstrated. It should be noted that the present task design differs from those of many other task-switching studies: in contrast to the previous studies, in which the subjects switched behavioral rules between two different categorization tasks (e.g., categorizing motions, colors, or depths) (<xref ref-type="bibr" rid="bib9">Cohen and Newsome, 2008</xref>; <xref ref-type="bibr" rid="bib52">Sasaki and Uka, 2009</xref>; <xref ref-type="bibr" rid="bib36">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib55">Siegel et al., 2015</xref>), the present study is based on switching between categorization and discrimination. This difference in task design may underlie the apparent discrepancy between the present and the previous studies regarding the involvement of sensory cortex in task switching. Lastly, even when no clear contextual effect is observed in single neuron properties, the correlations among neural responses can reflect the task differences (<xref ref-type="bibr" rid="bib9">Cohen and Newsome, 2008</xref>), indicating that the task context affects the patterns of collective neural representations but can be missed at the single cell levels,</p><p>The way of neural modulation such that the population response becomes more sensitive to color around the categorical boundary in the categorization task is consistent with previous human psychophysics showing that the stimulus discriminability is higher around category boundaries (<xref ref-type="bibr" rid="bib66">Uchikawa and Sugiyama, 1993</xref>, <xref ref-type="bibr" rid="bib65">1996</xref>). Moreover, the present results add a dynamical viewpoint in neural population representations, which predicts that the perceptual illusion depends on time as well as task demands. Beyond color perception, this modulation of dynamics in sensory representation implies potential physiological substrates of task-dependent perceptual illusion. For instance, perceived motion direction is biased away from the classification boundary during a motion categorization task (<xref ref-type="bibr" rid="bib25">Jazayeri and Movshon, 2007</xref>). Theoretically, this illusion could be explained both by considering direct modulation of sensory representation (<xref ref-type="bibr" rid="bib25">Jazayeri and Movshon, 2007</xref>) and by assuming a readout mechanism without direct modulation of the sensory neural representation itself (<xref ref-type="bibr" rid="bib58">Stocker and Simoncelli, 2007</xref>). The first model would be preferred if the motion perception is based on a population coding mechanism similar to the one demonstrated in this study which suggests the neural population representation is indeed modulated at the level of the sensory cortex.</p><p>The involvement of the sensory cortex in decision-related neural dynamics is consistent with the idea that responses within the sensory cortex are not only read out by the higher areas in a feedforward manner but also affected by decision-related signals through feedback connections from areas outside the sensory cortex (<xref ref-type="bibr" rid="bib43">Nienborg and Cumming, 2009</xref>; <xref ref-type="bibr" rid="bib55">Siegel et al., 2015</xref>; <xref ref-type="bibr" rid="bib72">Wimmer et al., 2015</xref>). Unfortunately, we cannot fully conclude from the present data whether the observed choice-related attractor-dynamics are the <italic>cause</italic> or the <italic>effect</italic> of decision-making (<xref ref-type="bibr" rid="bib43">Nienborg and Cumming, 2009</xref>). Nonetheless, the fact that modulation of the neural dynamics enhances the task-relevant information in sensory neurons may hint at the potential contribution of this modulation to the task performances. In addition, our data suggest that the choice-related difference in the dynamics had already begun during the early period (&lt;250 ms; <xref ref-type="fig" rid="fig2">Figure 2</xref>), which is thought to affect the decision (<xref ref-type="bibr" rid="bib43">Nienborg and Cumming, 2009</xref>). Therefore, it is likely that the task-dependent modulation of neural dynamics (at least during the early period after the stimulus onset) contributed to improving the behavioral performance rather than merely reflected the decision signal. More generally, theoretical studies have proposed that a common recurrent neural circuit can serve as the basis for multiple functions, such as sensory information encoding, categorization and decision (<xref ref-type="bibr" rid="bib70">Wang, 2002</xref>, <xref ref-type="bibr" rid="bib71">2008</xref>; <xref ref-type="bibr" rid="bib35">Machens et al., 2005</xref>; <xref ref-type="bibr" rid="bib18">Furman and Wang, 2008</xref>), enabling a flexible use of the neural dynamics depending on context. The present findings suggest the involvement of sensory cortex in the context-dependent behavior, leading to a new view that the sensory neurons could contribute to context-dependent behavior by flexibly modulating their collective attractor dynamics.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Ethics statement</title><p>This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols of the Okazaki National Research Institutes. The protocol was approved by the Animal Experiment Committee of the Okazaki National Research Institutes (Permit Number: A16-86-29). All surgeries were performed under sodium pentobarbital anesthesia, and every effort was made to minimize suffering.</p></sec><sec id="s4-2"><title>Subjects, stimuli, and behavioral task</title><p>To study the neural basis of context-dependent behavior, we analyzed neural responses from the anterior inferior temporal (IT) cortices in two female monkeys (<italic>Macaca fuscata</italic>) performing visual tasks. Details of the experimental procedures have been previously published (<xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>).</p><p>To study the neural basis of context-dependent behavior, we analyzed neural responses from the anterior inferior temporal (IT) cortices in two female monkeys (<italic>Macaca fuscata</italic>) performing visual tasks. The monkeys were trained in categorization and discrimination tasks. In both tasks, the same 11 sample colors were used as visual stimuli. The 11 sample colors ranged from red [color 1, (xÂ =Â 0.631, yÂ =Â 0.343 in the CIE 1931 xy chromaticity diagram)] to green [color 11, (xÂ =Â 0.286, yÂ =Â 0.603)] and were spaced at equal intervals on the CIE 1931 xy chromaticity diagram. The colors all had the same luminance (30 cd/m<sup>2</sup>). Tasks were alternated in blocks in a fixed sequence that included categorization and discrimination tasks, as well as an eye-fixation task in which the monkey passively viewed the same color stimuli. There was no explicit cue to indicate the ongoing task. Each block consisted of 88 correct trialsâeight repetitions of the 11 sample color stimuli. The 11 sample colors were presented in a pseudorandom order. If a monkey made an incorrect response to a given color, the trial using that color was repeated after some intervening trials. These repeated trials and other incomplete trials, such as those with fixation errors, were excluded from the subsequent data analyses. The stimulus was usually a disk with a diameter spanning 2.0<bold>Â°</bold> of visual angle, but for cells with shape selectivity, an optimal shape was chosen from among seven geometrical shapes (<xref ref-type="bibr" rid="bib31">Komatsu and Ideura, 1993</xref>; <xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>). The background was uniform 10 cd/m<sup>2</sup> gray (xÂ =Â 0.3127, yÂ =Â 0.3290). Stimuli were calibrated using a spectrophotometer (Photo Research PR-650). Personal computers controlled the task, presented the visual stimuli and recorded neural signals and eye positions. Eye movements were recorded using the scleral search coil method (<xref ref-type="bibr" rid="bib26">Judge et al., 1980</xref>). The monkeys were required to maintain fixation within a 2.8<bold>Â°</bold> window throughout the trial, except for the saccade response. At the beginning of each trial, a small fixation spot was presented at the center of the screen. When the monkeys had gazed at the fixation spot for 500 ms, it turned off, and a sample color stimulus was presented at the center of the display for 500 ms in both categorization and discrimination tasks.</p><p>In the categorization task, after the sample stimulus was turned off, two small spots of light appeared, one at the center of the visual field, the other 5<bold>Â°</bold> to the right (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). If the sample color belonged to the âreddishâ category (sample colors 1â4), the monkeys were rewarded for maintaining fixation on the center spot for another 700 ms (âno-goâ response). If the sample color belonged to the âgreenishâ category (sample colors 8â11), the monkeys were rewarded for making a saccade to the spot on the right (âgoâ response). For the intermediate colors (sample colors 5â7), the monkeys were rewarded randomly regardless of its behavioral response. In an early phase of the recordings from one monkey (15 neurons), there were no intermediate colors; the âno-goâ response was assigned to colors 1â5, the âgoâ response to colors 6â11.</p><p>In the discrimination task, after the sample stimulus was turned off, two choice stimuli appeared 3<bold>Â°</bold> above and below the fixation position (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). The choice stimuli were the same shape and size as the sample stimulus; one was the same color as the sample stimulus, the other a slightly different color. The monkeys were required to make a saccade to the choice stimulus that was the same color as the sample. The two choice colors were three steps apart along the 11 sample colors â that is, the eight choice color pairs included colors #1â4, #2â5, #3â6, #4â7, #5â8, #6â9, #7â10 and #8â11. This color interval was chosen so as to yield a relatively high discriminability (about 80â90% correct). Throughout the present paper, the term âdiscriminationâ is used for consistency with our previous study (<xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>).</p></sec><sec id="s4-3"><title>Electrophysiological recording</title><p>Neuronal activity was recorded with single unit recording from the anterior part of the IT cortex in the monkeys. We could record from 125 neurons in total. The recording region was slightly lateral to the posterior end of the anterior middle temporal sulcus (anterior 9â14 mm in the stereotaxic coordinates, area TE), which is a region where color-selective neurons are concentrated (<xref ref-type="bibr" rid="bib30">Komatsu et al., 1992</xref>; <xref ref-type="bibr" rid="bib37">Matsumora et al., 2008</xref>). The activities of single neurons were first isolated with online monitoring during recordings, then subject to offline spike sorting using a template matching algorithm, which confirmed that all of the data reported in this paper were single neuron activities.</p><p>All data analyses were based on neural responses to the sample colors and the fact that the monkeys saw the same visual stimuli in the categorization and discrimination tasks. For this purpose, we analyzed neural spikes recorded up to 550 ms after the sample onset, taking into account the neural response delay to the visual stimuli.</p><p>Our main results are based on a collection of single unit recordings (not a simultaneous recording of multiple neurons). In the population decoding analyses, we generated âpseudo-populationâ activities from those single neuron data by randomly resampling the trials, following a procedure reported in a previous study (<xref ref-type="bibr" rid="bib13">Fetsch et al., 2011</xref>). A caveat of the analysis based on âpseudo-populationâ is that it omits the noise correlation (i.e., the correlation in trial-to-trial fluctuations) across neurons. As widely recognized, the noise correlation can have profound influences on the information coding by neural population, affecting particularly the resolution of sensory representation. From the decoding perspective, in many cases the noise correlation is generally considered to affect the accuracy of decoding (e.g., error bars added when plotting the decoder outputs) although how noise correlation actually limits the stimulus information is a subject of ongoing debate (<xref ref-type="bibr" rid="bib41">Moreno-Bote et al., 2014</xref>). In this study, we do not primarily focus on the resolution of neural coding (reflected in the lengths of error bars) but on the âbiasesâ induced by the change in the mean activity in each neuron, which is captured by the present single-unit recording. In addition, a control analysis confirmed by that artificially inducing noise correlations in the studied pseudo-population did not affect the overall results (<xref ref-type="fig" rid="fig7">Figure 7a</xref>).</p></sec><sec id="s4-4"><title>Likelihood-based decoding</title><p>To visualize and characterize high-dimensional representation by neural populations, we mapped the neural population activity in the stimulus space by decoding the neural activity. From Bayesâ rule, the posterior probability on stimulus <inline-formula><mml:math id="inf28"><mml:mi>s</mml:mi></mml:math></inline-formula> under a given neural population activity <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In a full-normative framework, the prior distribution over the stimulus could be further modeled by assuming the hierarchical model with categorical prior on stimulus, <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; that is, <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>â«</mml:mo></mml:mstyle><mml:mtext>â</mml:mtext></mml:msup><mml:mtext>d</mml:mtext><mml:mi>c</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf33"><mml:mi>c</mml:mi></mml:math></inline-formula> denotes the category information (<xref ref-type="bibr" rid="bib60">Tajima et al., 2016</xref>). In the present experiments, however, the stimulus was sampled from a uniform distribution, thus the problem reduces to maximizing the likelihood <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In our analysis, a maximum-likelihood decoder (<xref ref-type="bibr" rid="bib14">FÃ¶ldiÃ¡k, 1991</xref>; <xref ref-type="bibr" rid="bib51">Sanger, 1996</xref>; <xref ref-type="bibr" rid="bib24">Jazayeri and Movshon, 2006</xref>; <xref ref-type="bibr" rid="bib34">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib20">Graf et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">Fetsch et al., 2011</xref>) of the stimulus was constructed based on the neural responses in the discrimination task and then applied to the data for categorization task to reconstruct the neural population states in the perceptual stimulus space (<xref ref-type="fig" rid="fig1">Figure 1d</xref>; see also the later descriptions for the rationale behind this procedure). The analyses in this and the subsequent sections are implemented using MATLAB (RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001622">SCR_001622</ext-link>).</p><p>The decoder was constructed based on a standard likelihood-based population decoding approach as follows (<xref ref-type="bibr" rid="bib20">Graf et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">Fetsch et al., 2011</xref>). Let <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> be the spike counts for the cell <inline-formula><mml:math id="inf36"><mml:mi>i</mml:mi></mml:math></inline-formula> response at time bin <italic>t</italic> in a trial. The spike count was derived from a 50 ms boxcar window whose starting point moved with 10 ms step from 0 to 500 ms after the onset of a sample-color stimulus. We first estimated a probability distribution, <inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, of responses evoked by stimulus <italic>s</italic> for each cell and each time bin, based on the data obtained during the discrimination task. This is approximated by a Gaussian distribution with a mean <inline-formula><mml:math id="inf38"><mml:mrow><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf39"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, which were respectively estimated from the mean and variance in the neural spike count data. The mean responses to 11 sample stimuli (<inline-formula><mml:math id="inf40"><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mo>â¦</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mn>11</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) were converted to smooth functions <inline-formula><mml:math id="inf41"><mml:mrow><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of the stimulus parameter <inline-formula><mml:math id="inf42"><mml:mi>s</mml:mi></mml:math></inline-formula> (a real number varying from 1 to 11 with an interval of 0.2) through cubic interpolation (Matlab function âinterp1â with âcubicâ option) over the stimulus space, to obtain smooth likelihood functions in the later analysis. We used the smooth likelihood functions because the neural color tunings are generally considered toÂ be continuous functions of stimulus, and the decoder outputs were more accurate (avoiding discrete jumps in trajectories) or better-interpreted by assuming the smooth tuning functions. The variance estimate was denoised by fitting a linear function, <inline-formula><mml:math id="inf43"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>, with a stimulus- and time-invariant scalar variable (Fano factor), <inline-formula><mml:math id="inf44"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, for each cell, in order to capture the potential variability in the Fano factor across neurons. To ensure that the decoder output matches the subjectâs perception about color identity, we used the trials in which the subjects answered correctly in the task. The Gaussian model naively implies the potential for negative neural activity, the biological meaning of which is unclear. However, this does not cause a problem in the practical data analysis because the analyzed neural responses are always positive, and we can safely equate the analysis with the one based on a rectified Gaussian model that satisfies the non-negativity of the neural responses. In addition, we also tested a Poisson distribution as a generative model of spike count, and confirmed that the results were not qualitatively affected (Results).</p><p>Combining these models of spike-count distributions derived from individual neurons and time bins yielded the log-likelihood of a population response.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Î¼</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Î¼</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¼</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>Î£</mml:mi></mml:mstyle></mml:math></inline-formula> are the mean and covariance of neural population response, respectively. In the main analysis, for simplicity, we assumed independent trial-to-trial variability in the neural firing (<xref ref-type="bibr" rid="bib51">Sanger, 1996</xref>; <xref ref-type="bibr" rid="bib10">Dayan and Abbott, 2001</xref>; <xref ref-type="bibr" rid="bib24">Jazayeri and Movshon, 2006</xref>; <xref ref-type="bibr" rid="bib34">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib4">Brouwer and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib13">Fetsch et al., 2011</xref>)âwe also observed that our main results were not affected by the decoder that takes into account the correlated variability among neurons (<xref ref-type="fig" rid="fig7">Figure 7</xref>). The joint log-likelihood <inline-formula><mml:math id="inf47"><mml:mi>L</mml:mi></mml:math></inline-formula> of a population response of <italic>N</italic> neurons, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, given stimulus <inline-formula><mml:math id="inf49"><mml:mi>s</mml:mi></mml:math></inline-formula> is<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/></mml:mtd><mml:mtd><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, column vector <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> represents the population activity of all <inline-formula><mml:math id="inf51"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons at time <inline-formula><mml:math id="inf52"><mml:mi>t</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>125</mml:mn></mml:mrow></mml:math></inline-formula> in the present data).. Based on this population activity, the decoder output (stimulus estimate), <inline-formula><mml:math id="inf54"><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, is given by maximizing the aforementioned likelihood function, <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mtext>*</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mtext>Â </mml:mtext><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This equation represents a mapping from the <italic>N</italic>-dimensional population state <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> to a one-dimensional value, <inline-formula><mml:math id="inf57"><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, in the stimulus space. We iterated this decoding procedure for each time <italic>t</italic>. We used different decoders for individual time bins, by constructing a generative model of the spike counts for each time bin. The main results were unaffected when we used a single time-invariant decoder (constructed based on the average spike count statistics across 200â550 ms after stimulus onset) for all time bins (Results).</p><p>To analyze the neural responses in the categorization task, we used the same function, <inline-formula><mml:math id="inf58"><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>r</mml:mi></mml:mstyle><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> [i.e., the same mean and variance parameters, <inline-formula><mml:math id="inf59"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for each neuron] as the decoder that was constructed based on neural activity in the discrimination task. The decoder constructed based on the discrimination task data does not necessarily provide an unbiased estimate of the stimulus for the categorization task. We made use of this potential decoding bias to characterize the difference in neural population responses between the two tasks. If there were any systematic bias, it would suggest that the neural population changes the stimulus representation depending on task context. It is reasonable to construct the stimulus decoder based on neural responses in the discrimination task because the perceived stimulus identity to be decoded could be confirmed with what the subjects reported in the discrimination task. By comparing the decoder output to the subjectsâ behavior, we were able to map the neural population response to the subjectsâ perception of the stimulus identity.</p><p>The rationale behind those procedures is as follows: in the discrimination task, the subject was presented a sample color (e.g., light green), then later identified it by selecting from a pair of similar colors (e.g., the same light green vs. a slightly deeper green). When the subjects correctly identified the presented sample color, by construction, the presented color matched the chosen color, which suggests that they correctly perceived the sample color such that it could be discriminated from other similar colors in the perceptual space. Although such a correspondence between choice and perception is not always guaranteed if the subjectâs choice is nearly random, it was not the case in this study because the subject showed high correct rate (about 80â90%) in the discrimination task. Nonetheless, there were a few error trials in which the presented colors differed from the chosen colors. In those error trials, it is not straightforward to tell what color was perceived by the subjects; it could be the chosen color, but alternatively, they might have actually perceived the presented color but made a mistake in the response, or they might have simply unattended to the task. Thus, we excluded those error trials from the present analysis, and focused on the correct trials in which the presented and chosen colors were identical. We also confirmed that the overall results were qualitatively maintained when we replicated the same analysis using half the recorded neural population without extremely high or low activity (by eliminating the neurons showing average firing rates outside the 25th-75th percentile of the whole population; Results), which excluded the possibility that a small subset of strongly-responding neurons determined the results of decoding.</p><p>The magnitude of clustering in the decoder outputs was quantified with a clustering index, which was defined by the ratio between the mean distance within categories and the distance between category means:<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>â²</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">â©</mml:mo><mml:msub><mml:mtext>Â </mml:mtext><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>â²</mml:mo></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Red</mml:mtext></mml:mrow></mml:msub><mml:mo>â¨</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>â²</mml:mo></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Green</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">â©</mml:mo><mml:msub><mml:mtext>Â </mml:mtext><mml:mrow><mml:mi>s</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Red</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">â©</mml:mo><mml:msub><mml:mtext>Â </mml:mtext><mml:mrow><mml:mi>s</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Green</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Red</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mo>#</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:mo>#</mml:mo><mml:mn>3</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Green</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mo>#</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:mo>#</mml:mo><mml:mn>11</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> are the âRedâ and âGreenâ stimulus sets defined based on the subjectsâ categorization behavior.</p></sec><sec id="s4-5"><title>Fitting the task-dependent components in neural dynamics</title><p>To investigate what form of neural response modulation explains the difference between the decoded stimulus dynamics in discrimination and categorization, we fitted the population responses in the categorization task (<inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) by modulating those in the discrimination task (<inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), based on four different models: three feedforward gain-modulation models and a recurrent model.</p><sec id="s4-5-1"><title>Gain modulation model 1 (time-invariant, stimulus-independent gains)</title><p>In the time-invariant gain-modulation model, the neural response data in the discrimination task were modulated so that they simulate the categorization-task responses. The simulated categorization-task response of neuron <inline-formula><mml:math id="inf64"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, was provided as<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>Â </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf66"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>Â¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes a constant gain-modulation for each cell <italic>i</italic>. The gain <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>Â¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was estimated by the linear regression (that minimizes the squared error between the predicted and actual neural responses in the categorization task). The numbers of parameters were 125 (corresponding to the number of recorded neurons <inline-formula><mml:math id="inf68"><mml:mi>N</mml:mi></mml:math></inline-formula>) in the time-invariant model. We analyzed the simulated population activity in the same procedure used for the actual response during the categorization task.</p></sec><sec id="s4-5-2"><title>Gain modulation model 2 (time-variant, stimulus-independent gains)</title><p>Similarly, in the time-variant gain modulation model, the predicted categorization-task response, <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, was given by<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext>\Â </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The gain term <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each neuron <inline-formula><mml:math id="inf71"><mml:mi>i</mml:mi></mml:math></inline-formula> was estimated by the linear regression. In our main analysis, the number of neurons was <inline-formula><mml:math id="inf72"><mml:mi>N</mml:mi></mml:math></inline-formula>=125, thus the numbers of parameters were 125 <inline-formula><mml:math id="inf73"><mml:mo>Ã</mml:mo></mml:math></inline-formula>51Â =Â 6375 (corresponding to the number of recorded neurons <inline-formula><mml:math id="inf74"><mml:mo>Ã</mml:mo></mml:math></inline-formula> the number of time bins) in this model.</p></sec><sec id="s4-5-3"><title>Gain modulation model 3 (time-invariant, stimulus-dependent gains)</title><p>we also considered a gain modulation depending on the presented stimulus as a control (see Discussion for its biological interpretation). Note that the modulation component for each neuron can be trivially fitted by the gain modulation depending on both stimulus and time, since they are the only variables (except for the task demands) in the present experiment. Thus, here we tried to fit the data with a gain-modulation model in which the neuronal gains depend on the stimulus but not on time. In this model, the predicted categorization-task response, <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, was given by<disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext>Â </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The gain term <inline-formula><mml:math id="inf76"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each neuron <inline-formula><mml:math id="inf77"><mml:mi>i</mml:mi></mml:math></inline-formula> was estimated by the linear regression. The numbers of model parameters were 125 <inline-formula><mml:math id="inf78"><mml:mo>Ã</mml:mo></mml:math></inline-formula>11Â =Â 1375 (corresponding to the number of recorded neurons <inline-formula><mml:math id="inf79"><mml:mo>Ã</mml:mo></mml:math></inline-formula> the number of sample colors).</p></sec><sec id="s4-5-4"><title>Recurrent model</title><p>Lastly, we also fitted the neural dynamics with a model that features a recurrent feedback. In the recurrent model, a self-feedback term was added to the responses in the discrimination task so that the resulting modulated activities fit those recorded in the categorization task. We assumed a restricted recurrent circuit with a single hidden layer consisting of two nonlinear hidden units. In this model, we assumed mutual connections between the recorded IT neurons and the two hidden units (which could be interpreted as the neural activity outside IT cortex, e.g., the frontal cortex, as modeled in further details later). There was no direct connection between the hidden units, resembling two-layer restricted Boltzmann machines (<xref ref-type="bibr" rid="bib56">Smolensky, 1986</xref>; <xref ref-type="bibr" rid="bib23">Hinton, 2002</xref>). The model is a simplified version of the circuit model (<xref ref-type="fig" rid="fig6">Figure 6a</xref>) that we used for demonstrating the task-dependent change in attractor structures (see the later section); here, we use this simplified version in the purpose of the quantitative fitting.</p><p>Based on this model, the hypothetical neural activity in the categorization task, <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, was provided as<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>:=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where the <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>N</mml:mi><mml:mo>Ã</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the connectivity weights between the neurons to the two hidden units. The weights are symmetric between the bottom-up and top-down connections (from the neurons to the hidden units, and from the hidden units to the neurons, respectively), as assumed commonly in the restricted Boltzmann machine (<xref ref-type="bibr" rid="bib23">Hinton, 2002</xref>) and in our later simulation. The symmetric connections also avoid having the more parameters, leading to a parsimonious model. <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the activities of hidden units at time <inline-formula><mml:math id="inf84"><mml:mi>t</mml:mi></mml:math></inline-formula>. The function <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>â</mml:mo><mml:mo>)</mml:mo><mml:mi>ï¿½</mml:mi><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext></mml:mrow></mml:math></inline-formula> is the activation function for the hidden units. Scalar <inline-formula><mml:math id="inf86"><mml:mi>B</mml:mi></mml:math></inline-formula> is the common bias inputs to the hidden units. <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf88"><mml:mi>B</mml:mi></mml:math></inline-formula> were learned from the data, but kept constant across time and different stimuli. To optimize those parameters, we minimized the sum of squared error between the actual and predicted neural activities in the categorization task, <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, with a standard gradient descent method on <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf91"><mml:mi>B</mml:mi></mml:math></inline-formula>. The number of parameters was <inline-formula><mml:math id="inf92"><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>=</mml:mo><mml:mn>252</mml:mn></mml:mrow></mml:math></inline-formula>, corresponding to the total number of connections and the bias inputs. Note that it is not necessarily straightforward to relate those two hidden units directly to the âredâ and âgreenâ category neurons modeled because such categorical information is represented in a mixed way in the circuit learned from the real data. Nonetheless, the goodness of fitting with this model demonstrates that the recurrent network with the restricted architecture is capable of describing the neural data quantitatively. It should be also noted that we do not consider that the task switching requires changes in all the connectivity weights among the neurons. Instead, we could assume a more parsimonious mechanism that features the attractor structure in the circuit is modulated through the change in a background input to the circuit (see the later subsection).</p></sec><sec id="s4-5-5"><title>Assessment of model-fitting performances</title><p>We assessed the model-fitting performances based on the cross-validation procedure as follows: we randomly divided the data into two non-overlapping sets of trials (âtrial set 1â and âtrial set 2â having odd and even trial numbers, respectively), the first of which was used to train models, and the second of which was used to test each modelâs fitting performances. This procedure ensured that a difference in fitting performance did not reflect overfitting or a difference in the number of parameters. The model-fitting errors, <inline-formula><mml:math id="inf93"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>CV</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, were quantified by the root mean square errors between the predicted and actual neural population activities, relative to the âbaselineâ variability across trials:<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>CV</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mrow><mml:mo>â¨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext><mml:mo>,</mml:mo><mml:mo>,</mml:mo><mml:mtext>trial set</mml:mtext><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext><mml:mo>,</mml:mo><mml:mo>,</mml:mo><mml:mtext>trial set</mml:mtext><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>â©</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mtext>Â </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>Â </mml:mtext></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo>â¨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext><mml:mo>,</mml:mo><mml:mo>,</mml:mo><mml:mtext>trial set</mml:mtext><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext><mml:mo>,</mml:mo><mml:mo>,</mml:mo><mml:mtext>trial set</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>â©</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">â©</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the average over the cells, time bins, and stimuli. The numerator corresponds to the error in the model prediction, whereas the denominator represents the âbaselineâ variability within the condition due to the trial-to-trial fluctuations in neural firing. Note that this measure itself is independent of the assumptions about decoders because it is computed directly from the neural population activities.</p></sec></sec><sec id="s4-6"><title>Mutual information analysis</title><p>The amount of information about a stimulus carried by the neural population response was also evaluated using mutual information, which does not require any specific assumptions about the decoder or the models of dynamical modulations. The mutual information between the stimulus hue and the neural responses within each time bin <inline-formula><mml:math id="inf95"><mml:mi>t</mml:mi></mml:math></inline-formula> during the categorization task was given by<disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>hue</mml:mtext><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>r</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo movablelimits="false">â</mml:mo></mml:mstyle><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow/></mml:munderover><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtext>logÂ </mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mtext>logÂ </mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf96"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability distribution of the <inline-formula><mml:math id="inf97"><mml:mi>i</mml:mi></mml:math></inline-formula>th neuronâs response (spike counts) evoked by stimulus <inline-formula><mml:math id="inf98"><mml:mi>s</mml:mi></mml:math></inline-formula> during the categorization task (modeled by the same normal distributions as the ones used in the likelihood-based decoding in the above). The âhueâ in the parenthesis indicates that this is the mutual information about the stimulus hue. Similarly, the mutual information between the stimulus category <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtext>Red</mml:mtext><mml:mo>,</mml:mo><mml:mtext>Green</mml:mtext></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the neural responses within each time bin <italic>t</italic> was given by<disp-formula id="equ11"><label>(10)</label><mml:math id="m11"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>cat</mml:mtext><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>r</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo movablelimits="false">â</mml:mo></mml:mstyle><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow/></mml:munderover><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtext>logÂ </mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mtext>logÂ </mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow/></mml:msubsup><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf101"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â</mml:mo><mml:mo>{</mml:mo><mml:mtext>Red</mml:mtext><mml:mo>,</mml:mo><mml:mtext>Â Green</mml:mtext><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> denotes the stimulus category. The âcatâ in the parenthesis indicates that this is the mutual information about the stimulus category. The mutual information values for the discrimination task, <inline-formula><mml:math id="inf102"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>hue</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf103"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>cat</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, were provided by substituting <inline-formula><mml:math id="inf104"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the above equations with the corresponding spike count distributions, <inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, obtained during the discrimination task. The differential mutual information for hue and category were defined by <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi></mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>hue</mml:mtext><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>hue</mml:mtext><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>hue</mml:mtext><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi></mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>cat</mml:mtext><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>cat</mml:mtext><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>cat</mml:mtext><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. In the main text, we normalized the quantity by the neuron number and the time-bin length to show the information per cell and per second.</p></sec><sec id="s4-7"><title>Other dimensionality reduction analyses</title><p>We also applied five other dimensionality reduction methods to the same data and compared the results with that of the likelihood-based decoding. First, the standard principal component analysis (PCA) was applied to the set of trial-averaged data points (i.e., population response vectors <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mtext>Dis</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi mathvariant="normal">#</mml:mi><mml:mn>11</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mtext>Â </mml:mtext><mml:mtext>Â </mml:mtext><mml:mn>0</mml:mn><mml:mtext>Â </mml:mtext><mml:mtext>ms</mml:mtext><mml:mo>â¤</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>t</mml:mi><mml:mtext>Â </mml:mtext><mml:mo>â¤</mml:mo><mml:mtext>Â </mml:mtext><mml:mn>550</mml:mn><mml:mtext>Â </mml:mtext><mml:mtext>ms</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) that varied over time <inline-formula><mml:math id="inf109"><mml:mi>t</mml:mi></mml:math></inline-formula>. Second,, we conducted t-stochastic neighbor embedding (t-SNE) (<xref ref-type="bibr" rid="bib67">van der Maaten and Hinton, 2008</xref>) on the same data to examine the effects of nonlinearity in the unsupervised dimensionality reduction. Third, we performed the demixed PCA (dPCA) (<xref ref-type="bibr" rid="bib2">Brendel and Machens, 2011</xref>; <xref ref-type="bibr" rid="bib28">Kobak et al., 2016</xref>), which can separate the contributions of stimulus, task, and their interactions. Fourth, we applied a population vector decoding (<xref ref-type="bibr" rid="bib19">Georgopoulos et al., 1986</xref>). Here, we assumed that the presented 11 stimuli span a part (<inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> radian angles) of a circular hue space, and decoded stimulus by <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>angle</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mo movablelimits="false">â</mml:mo><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mtext>i</mml:mtext><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is neuron <inline-formula><mml:math id="inf113"><mml:mi>j</mml:mi></mml:math></inline-formula>âs preferred stimulus angle. Lastly, as a naive application of the population vector decoding is biased in non-uniform neural population, we also decoded the stimulus using a version of optimal linear decoder (<xref ref-type="bibr" rid="bib50">Salinas and Abbott, 1994</xref>; <xref ref-type="bibr" rid="bib46">Pouget et al., 1998</xref>), in which a readout matrix for the stimulus identity (#1â#11) was trained by the linear regression on mean responses to take into account the shapes of tuning curves and cell distribution.</p></sec><sec id="s4-8"><title>A model of context-dependent attractor dynamics</title><p>We introduced a simple recurrent model that provides a parsimonious explanation for the observed context-dependent change in attractor dynamics (see <xref ref-type="fig" rid="fig6">Figure 6a</xref>, Results). The model assumed bidirectional interactions between <italic>n</italic> hue-selective neurons (hereafter, <italic>hue neurons</italic>) in IT cortex and two groups of category-selective neurons (<italic>category neurons</italic>) outside IT cortex; for example, such neurons that encode category have been found in the prefrontal cortex (<xref ref-type="bibr" rid="bib15">Freedman et al., 2001</xref>; <xref ref-type="bibr" rid="bib38">McKee et al., 2014</xref>). This circuit shares the basic architecture with our previous model that was proposed for general categorical inference (<xref ref-type="bibr" rid="bib60">Tajima et al., 2016</xref>); here, we extend this model to explain the context dependent bifurcation of attractor dynamics. Note that the category- and hue-neurons in this model should not be confused with the terms âcategorization-â and âdiscrimination-task preferred cellsâ used in the previous study (<xref ref-type="bibr" rid="bib29">Koida and Komatsu, 2007</xref>), which were the labels on the IT neurons introduced to describe the polarity of task-dependent modulation for each cell, and not relevant to the current model.</p><p>The dynamics of category neurons were described by differential equations as follows:<disp-formula id="equ12"><label>(11)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mover><mml:mi>C</mml:mi><mml:mo>.</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>BU</mml:mtext></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ13"><label>(12)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mover><mml:mi>C</mml:mi><mml:mo>.</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>BU</mml:mtext></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ14"><label>(13)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>TD</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>TD</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where the dots between variables denote inner products of vectors. <inline-formula><mml:math id="inf114"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the time constant for the dynamics of category neurons, which was set as <inline-formula><mml:math id="inf115"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>75</mml:mn></mml:mrow></mml:math></inline-formula> ms in the simulation, roughly matched to the order of time constants in cortical neurons (<xref ref-type="bibr" rid="bib42">Murray et al., 2014</xref>). <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf117"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are scalar values representing mean activity of red- and green-preferring category neurons, respectively. The time constant for hue-neurons was neglected for the sake of the tractability in nullclines analysis. The faster dynamics in sensory neurons compared to those in higher-area is consistent with a previous report (<xref ref-type="bibr" rid="bib42">Murray et al., 2014</xref>). We also confirmed that assuming non-zero time constant in hue neurons did not change the qualitative behavior of the model. The activation function in the simulation was given by a sigmoid function, <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>, though the precise form of the activation function was not critical for the emergence of bistability as long as the neural activity was described by a monotonic saturating function. <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mo>:=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a vector representing the population activity of hue-neurons with different preferred stimuli (varying from red to green), which receive sensory input, <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, from the earlier visual cortex. The hue neurons interact with category-neuron groups <inline-formula><mml:math id="inf122"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf123"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> through bottom-up and top-down connections with weights (<inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>BU</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>BU</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>) and (<inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>TD</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>TD</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>), respectively, where the connectivity weights were expressed as vectors (e.g., <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>BU</mml:mtext></mml:mrow></mml:msubsup><mml:mo>:=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mrow><mml:mtext>BU</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mtext>BU</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). The category neurons also receive a common background input, <inline-formula><mml:math id="inf129"><mml:mi>B</mml:mi></mml:math></inline-formula>. We assume that this background input is the only component that depend on task demand in this circuit.</p><p>In the simulation, the numbers of hue-neurons were set to <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow></mml:math></inline-formula>, although the size of neural population did not have major effect on the results of simulation. Sensory input to hue-neuron <inline-formula><mml:math id="inf131"><mml:mi>i</mml:mi></mml:math></inline-formula> was modeled using a von Mises function, <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Îº</mml:mi><mml:mtext>Â </mml:mtext><mml:mi>cos</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>pref</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the sharpness parameter <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>Îº</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mo>â</mml:mo><mml:mtext>Ï</mml:mtext><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mtext>Â Ï</mml:mtext><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the stimulus hue, which varied from red to green, and <inline-formula><mml:math id="inf135"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>pref</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the preferred hue of neuron <inline-formula><mml:math id="inf136"><mml:mi>i</mml:mi></mml:math></inline-formula>; <inline-formula><mml:math id="inf137"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>50</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf138"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf139"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf140"><mml:mrow><mml:mi>t</mml:mi><mml:mo>â¤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The preferred hues were distributed uniformly across the entire hue circle, <inline-formula><mml:math id="inf141"><mml:mrow><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mo>â</mml:mo><mml:mtext>Ï</mml:mtext><mml:mo>,</mml:mo><mml:mtext>Â Ï</mml:mtext></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Each category-neuron group contained 150 cells, which were uniform within each group. The connectivity weight between hue neuron <inline-formula><mml:math id="inf142"><mml:mi>i</mml:mi></mml:math></inline-formula> and category-neuron group <inline-formula><mml:math id="inf143"><mml:mi>j</mml:mi></mml:math></inline-formula> was modeled by <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">U</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf145"><mml:mrow><mml:mi>a</mml:mi><mml:mtext>Â </mml:mtext><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:mn>10</mml:mn><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf146"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>Cat</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the preferred hue of <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. For simplicity, the bottom-up and top-down weights were assumed to be symmetric. We assumed that all the model parameters except for the background input <inline-formula><mml:math id="inf148"><mml:mi>B</mml:mi></mml:math></inline-formula> were the same between different task conditions. The differential equations were solved with the Euler method with a unit step size of 0.25 ms.</p><p>Note that, although this model shares the basic circuit architecture with our âfirstâ recurrent model used for fitting the data, it includes additional model parameters to demonstrate the potential mathematical principle for the attractor bifurcation in a self-contained manner. For example, the second model includes the bottom-up inputs to the individual neurons by term <inline-formula><mml:math id="inf149"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which are unknown in the actual data. (In some sense, the âfirstâ model used the discrimination task response as a proxy for this term). We did not fit this âsecondâ recurrent model directly to the data because the more model parameters can risk overfitting the data as well as make the model comparisons unstraightforward: when the model fitted the data better than the gain-modulation models, the increased model complexity can obscure whether such a model explained the data because the recurrent component was crucial or merely because it had many parameters. Nonetheless, the two models share the basic circuit architecture, and the results together demonstrate that the recurrent mechanism can describe the data as well as account for a potential mathematical principle underling the context-dependent neural dynamics.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Gouki Okazawa and Ruben Coen Cagli for helpful comments. ST was supported by JST PRESTO, Grant Number JPMJPR16E6. KA was supported by CREST, JST and JSPS KAKENHI, Grant Number 15H05707. HK was supported by the Center of Innovation Program from Japan Science and Technology Agency, JST.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>ST, Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writingâoriginal draft, Project administration, Writingâreview and editing</p></fn><fn fn-type="con" id="con2"><p>KK, Resources, Data curation, Writingâreview and editing</p></fn><fn fn-type="con" id="con3"><p>CIT, Writingâreview and editing, Discussed the results</p></fn><fn fn-type="con" id="con4"><p>HS, Validation, Writingâreview and editing, Discussed the results</p></fn><fn fn-type="con" id="con5"><p>KA, Funding acquisition, Writingâreview and editing</p></fn><fn fn-type="con" id="con6"><p>HK, Resources, Funding acquisition, Writingâreview and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols of the Okazaki National Research Institutes. The protocol was approved by the Animal Experiment Committee of the Okazaki National Research Institutes (Permit Number: A16-86-29). All surgery was performed under sodium pentobarbital anesthesia, and every effort was made to minimize suffering.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="SD7-data"><object-id pub-id-type="doi">10.7554/eLife.26868.018</object-id><label>Source code file 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig1">Figure 1</xref>: Matlab codes related to <xref ref-type="fig" rid="fig1">Figure 1</xref>.</title><p>The codes run with <xref ref-type="supplementary-material" rid="SD1-data">Figure 1âsource data 1</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.018">http://dx.doi.org/10.7554/eLife.26868.018</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-code-v2.zip"/></supplementary-material><supplementary-material id="SD8-data"><object-id pub-id-type="doi">10.7554/eLife.26868.019</object-id><label>Source code file 2.</label><caption><title>Related to <xref ref-type="fig" rid="fig2">Figure 2</xref>: Matlab codes related to <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>The codes run with <xref ref-type="supplementary-material" rid="SD2-data">Figure 2âsource data 1</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.019">http://dx.doi.org/10.7554/eLife.26868.019</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-code-v2.zip"/></supplementary-material><supplementary-material id="SD9-data"><object-id pub-id-type="doi">10.7554/eLife.26868.020</object-id><label>Source code file 3.</label><caption><title>Related to <xref ref-type="fig" rid="fig3">Figure 3</xref>: Matlab codes related to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>The codes run with <xref ref-type="supplementary-material" rid="SD3-data">Figure 3âsource data 1</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.020">http://dx.doi.org/10.7554/eLife.26868.020</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-code-v2.zip"/></supplementary-material><supplementary-material id="SD10-data"><object-id pub-id-type="doi">10.7554/eLife.26868.021</object-id><label>Source code file 4.</label><caption><title>Related to <xref ref-type="fig" rid="fig4">Figure 4</xref>: Matlab codes related to <xref ref-type="fig" rid="fig4">Figure 4</xref>.</title><p>The codes run with <xref ref-type="supplementary-material" rid="SD4-data">Figure 4âsource data 1</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.021">http://dx.doi.org/10.7554/eLife.26868.021</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-code-v2.zip"/></supplementary-material><supplementary-material id="SD11-data"><object-id pub-id-type="doi">10.7554/eLife.26868.022</object-id><label>Source code file 5.</label><caption><title>Related to <xref ref-type="fig" rid="fig5">Figure 5</xref>: Matlab codes related to <xref ref-type="fig" rid="fig5">Figure 5</xref>.</title><p>The codes run with <xref ref-type="supplementary-material" rid="SD5-data">Figure 5âsource data 1</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.022">http://dx.doi.org/10.7554/eLife.26868.022</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-code-v2.zip"/></supplementary-material><supplementary-material id="SD12-data"><object-id pub-id-type="doi">10.7554/eLife.26868.023</object-id><label>Source code file 6.</label><caption><title>Related to <xref ref-type="fig" rid="fig6">Figure 6</xref>: Matlab codes related to <xref ref-type="fig" rid="fig6">Figure 6</xref>.</title><p>The codes simulate the neural population response based on the recurrent differential equation model described in the text.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.023">http://dx.doi.org/10.7554/eLife.26868.023</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-code-v2.zip"/></supplementary-material><supplementary-material id="SD13-data"><object-id pub-id-type="doi">10.7554/eLife.26868.024</object-id><label>Source code file 7.</label><caption><title>Related to <xref ref-type="fig" rid="fig7">Figure 7</xref>: Matlab codes related to <xref ref-type="fig" rid="fig7">Figure 7</xref>.</title><p>The codes run with <xref ref-type="supplementary-material" rid="SD6-data">Figure 7âsource data 1</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.26868.024">http://dx.doi.org/10.7554/eLife.26868.024</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-26868-code-v2.zip"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Treves</surname><given-names>A</given-names></name><name><surname>Jagadeesh</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Converging neuronal activity in inferior temporal cortex during the classification of morphed stimuli</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>760</fpage><lpage>776</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn125</pub-id><pub-id pub-id-type="pmid">18669590</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brendel</surname><given-names>W</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Demixed principal Component Analysis</article-title><source>Advances in Neural Information Processing Systems</source><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Celebrini</surname><given-names>S</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A relationship between behavioral choice and the visual responses of neurons in macaque MT</article-title><source>Visual Neuroscience</source><volume>13</volume><fpage>87</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1017/S095252380000715X</pub-id><pub-id pub-id-type="pmid">8730992</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname><given-names>GJ</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding and reconstructing color from responses in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>13992</fpage><lpage>14003</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3577-09.2009</pub-id><pub-id pub-id-type="pmid">19890009</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname><given-names>GJ</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Categorical clustering of the neural representation of color</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>15454</fpage><lpage>15465</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2472-13.2013</pub-id><pub-id pub-id-type="pmid">24068814</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Martinez-Conde</surname><given-names>S</given-names></name><name><surname>Macknik</surname><given-names>SL</given-names></name><name><surname>Bereshpolova</surname><given-names>Y</given-names></name><name><surname>Swadlow</surname><given-names>HA</given-names></name><name><surname>Alonso</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Task difficulty modulates the activity of specific neuronal populations in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>974</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1038/nn.2147</pub-id><pub-id pub-id-type="pmid">18604204</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coen-Cagli</surname><given-names>R</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Flexible gating of contextual influences in natural vision</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1648</fpage><lpage>1655</lpage><pub-id pub-id-type="doi">10.1038/nn.4128</pub-id><pub-id pub-id-type="pmid">26436902</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Measuring and interpreting neuronal correlations</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>811</fpage><lpage>819</lpage><pub-id pub-id-type="doi">10.1038/nn.2842</pub-id><pub-id pub-id-type="pmid">21709677</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Context-dependent changes in functional circuitry in visual area MT</article-title><source>Neuron</source><volume>60</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.08.007</pub-id><pub-id pub-id-type="pmid">18940596</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>On the structure of Neuronal Population activity under fluctuations in Attentional state</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>1775</fpage><lpage>1789</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2044-15.2016</pub-id><pub-id pub-id-type="pmid">26843656</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>TA</given-names></name><name><surname>Chaisangmongkon</surname><given-names>W</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Choice-correlated activity fluctuations underlie learning of neuronal category representation</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>6454</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms7454</pub-id><pub-id pub-id-type="pmid">25759251</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname><given-names>CR</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural correlates of reliability-based cue weighting during multisensory integration</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>146</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1038/nn.2983</pub-id><pub-id pub-id-type="pmid">22101645</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>FÃ¶ldiÃ¡k</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Learning Invariance from transformation sequences</article-title><source>Neural Computation</source><volume>3</volume><fpage>194</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1162/neco.1991.3.2.194</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Categorical representation of visual stimuli in the primate prefrontal cortex</article-title><source>Science</source><volume>291</volume><fpage>312</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1126/science.291.5502.312</pub-id><pub-id pub-id-type="pmid">11209083</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname> <given-names>DJ</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual categorization and the primate prefrontal cortex: neurophysiology and behavior</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>929</fpage><lpage>941</lpage><pub-id pub-id-type="pmid">12163542</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A comparison of primate prefrontal and inferior temporal cortices during visual categorization</article-title><source>Journal of Neuroscience</source><volume>23</volume><fpage>5235</fpage><lpage>5246</lpage><pub-id pub-id-type="pmid">12832548</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furman</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Similarity effect and optimal control of multiple-choice decision making</article-title><source>Neuron</source><volume>60</volume><fpage>1153</fpage><lpage>1168</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.12.003</pub-id><pub-id pub-id-type="pmid">19109918</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgopoulos</surname><given-names>AP</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name><name><surname>Kettner</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Neuronal population coding of movement direction</article-title><source>Science</source><volume>233</volume><fpage>1416</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1126/science.3749885</pub-id><pub-id pub-id-type="pmid">3749885</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graf</surname><given-names>AB</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Decoding the activity of neuronal populations in macaque primary visual cortex</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>239</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1038/nn.2733</pub-id><pub-id pub-id-type="pmid">21217762</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenberg</surname><given-names>DS</given-names></name><name><surname>Houweling</surname><given-names>AR</given-names></name><name><surname>Kerr</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Population imaging of ongoing neuronal activity in the visual cortex of awake rats</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>749</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.1038/nn.2140</pub-id><pub-id pub-id-type="pmid">18552841</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haefner</surname><given-names>RM</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual Decision-Making as Probabilistic Inference by Neural sampling</article-title><source>Neuron</source><volume>90</volume><fpage>649</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.020</pub-id><pub-id pub-id-type="pmid">27146267</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Training products of experts by minimizing contrastive divergence</article-title><source>Neural Computation</source><volume>14</volume><fpage>1771</fpage><lpage>1800</lpage><pub-id pub-id-type="doi">10.1162/089976602760128018</pub-id><pub-id pub-id-type="pmid">12180402</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Optimal representation of sensory information by neural populations</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>690</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1038/nn1691</pub-id><pub-id pub-id-type="pmid">16617339</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A new perceptual illusion reveals mechanisms of sensory decoding</article-title><source>Nature</source><volume>446</volume><fpage>912</fpage><lpage>915</lpage><pub-id pub-id-type="doi">10.1038/nature05739</pub-id><pub-id pub-id-type="pmid">17410125</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Judge</surname><given-names>SJ</given-names></name><name><surname>Richmond</surname><given-names>BJ</given-names></name><name><surname>Chu</surname><given-names>FC</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Implantation of magnetic search coils for measurement of eye position: an improved method</article-title><source>Vision Research</source><volume>20</volume><fpage>535</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(80)90128-5</pub-id><pub-id pub-id-type="pmid">6776685</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Mechanisms of visual attention in the human cortex</article-title><source>Annual Review of Neuroscience</source><volume>23</volume><fpage>315</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.23.1.315</pub-id><pub-id pub-id-type="pmid">10845067</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobak</surname><given-names>D</given-names></name><name><surname>Brendel</surname><given-names>W</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Feierstein</surname><given-names>CE</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Qi</surname><given-names>XL</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Demixed principal component analysis of neural population data</article-title><source>eLife</source><volume>5</volume><fpage>1</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.7554/eLife.10989</pub-id><pub-id pub-id-type="pmid">27067378</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koida</surname><given-names>K</given-names></name><name><surname>Komatsu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Effects of task demands on the responses of color-selective neurons in the inferior temporal cortex</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>108</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/nn1823</pub-id><pub-id pub-id-type="pmid">17173044</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Komatsu</surname><given-names>H</given-names></name><name><surname>Ideura</surname><given-names>Y</given-names></name><name><surname>Kaji</surname><given-names>S</given-names></name><name><surname>Yamane</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Color selectivity of neurons in the inferior temporal cortex of the awake macaque monkey</article-title><source>Journal of Neuroscience</source><volume>12</volume><fpage>408</fpage><lpage>424</lpage><pub-id pub-id-type="pmid">1740688</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Komatsu</surname><given-names>H</given-names></name><name><surname>Ideura</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Relationships between color, shape, and pattern selectivities of neurons in the inferior temporal cortex of the monkey</article-title><source>Journal of Neurophysiology</source><volume>70</volume><fpage>677</fpage><lpage>694</lpage><pub-id pub-id-type="pmid">8410167</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VA</given-names></name><name><surname>Zipser</surname><given-names>K</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Figure-ground activity in primary visual cortex is suppressed by anesthesia</article-title><source>PNAS</source><volume>95</volume><fpage>3263</fpage><lpage>3268</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.6.3263</pub-id><pub-id pub-id-type="pmid">9501251</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VA</given-names></name><name><surname>Zipser</surname><given-names>K</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Masking interrupts figure-ground signals in V1</article-title><source>Journal of Cognitive Neuroscience</source><volume>14</volume><fpage>1044</fpage><lpage>1053</lpage><pub-id pub-id-type="doi">10.1162/089892902320474490</pub-id><pub-id pub-id-type="pmid">12419127</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian inference with probabilistic population codes</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1432</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1038/nn1790</pub-id><pub-id pub-id-type="pmid">17057707</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Flexible control of mutual inhibition: a neural model of two-interval discrimination</article-title><source>Science</source><volume>307</volume><fpage>1121</fpage><lpage>1124</lpage><pub-id pub-id-type="doi">10.1126/science.1104171</pub-id><pub-id pub-id-type="pmid">15718474</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsumora</surname><given-names>T</given-names></name><name><surname>Koida</surname><given-names>K</given-names></name><name><surname>Komatsu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Relationship between color discrimination and neural responses in the inferior temporal cortex of the monkey</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>3361</fpage><lpage>3374</lpage><pub-id pub-id-type="doi">10.1152/jn.90551.2008</pub-id><pub-id pub-id-type="pmid">18922950</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKee</surname><given-names>JL</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Task dependence of visual and category representations in prefrontal and inferior temporal cortices</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>16065</fpage><lpage>16075</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1660-14.2014</pub-id><pub-id pub-id-type="pmid">25429147</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname><given-names>EM</given-names></name><name><surname>Qi</surname><given-names>XL</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Incorporation of new information into prefrontal cortical activity after learning working memory tasks</article-title><source>PNAS</source><volume>109</volume><fpage>4651</fpage><lpage>4656</lpage><pub-id pub-id-type="doi">10.1073/pnas.1201022109</pub-id><pub-id pub-id-type="pmid">22392988</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirabella</surname><given-names>G</given-names></name><name><surname>Bertini</surname><given-names>G</given-names></name><name><surname>Samengo</surname><given-names>I</given-names></name><name><surname>Kilavik</surname><given-names>BE</given-names></name><name><surname>Frilli</surname><given-names>D</given-names></name><name><surname>Della Libera</surname><given-names>C</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neurons in area V4 of the macaque translate attended visual features into behaviorally relevant categories</article-title><source>Neuron</source><volume>54</volume><fpage>303</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.04.007</pub-id><pub-id pub-id-type="pmid">17442250</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Beck</surname><given-names>J</given-names></name><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Information-limiting correlations</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1410</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1038/nn.3807</pub-id><pub-id pub-id-type="pmid">25195105</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>JD</given-names></name><name><surname>Bernacchia</surname><given-names>A</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name><name><surname>Cai</surname><given-names>X</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Pasternak</surname><given-names>T</given-names></name><name><surname>Seo</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A hierarchy of intrinsic timescales across primate cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1661</fpage><lpage>1663</lpage><pub-id pub-id-type="doi">10.1038/nn.3862</pub-id><pub-id pub-id-type="pmid">25383900</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nienborg</surname><given-names>H</given-names></name><name><surname>Cumming</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decision-related activity in sensory neurons reflects more than a neuron's causal effect</article-title><source>Nature</source><volume>459</volume><fpage>89</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1038/nature07821</pub-id><pub-id pub-id-type="pmid">19270683</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okazawa</surname><given-names>G</given-names></name><name><surname>Tajima</surname><given-names>S</given-names></name><name><surname>Komatsu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Image statistics underlying natural texture selectivity of neurons in macaque V4</article-title><source>PNAS</source><volume>112</volume><fpage>E351</fpage><lpage>E360</lpage><pub-id pub-id-type="doi">10.1073/pnas.1415146112</pub-id><pub-id pub-id-type="pmid">25535362</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Dora</surname><given-names>E</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Deangelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>How can Single Sensory Neurons Predict Behavior?</article-title><source>Neuron</source><volume>87</volume><fpage>411</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.033</pub-id><pub-id pub-id-type="pmid">26182422</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Statistically efficient estimation using population coding</article-title><source>Neural Computation</source><volume>10</volume><fpage>373</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1162/089976698300017809</pub-id><pub-id pub-id-type="pmid">9472487</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id><pub-id pub-id-type="pmid">19186161</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roweis</surname><given-names>ST</given-names></name><name><surname>Saul</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Nonlinear dimensionality reduction by locally linear embedding</article-title><source>Science</source><volume>290</volume><fpage>2323</fpage><lpage>2326</lpage><pub-id pub-id-type="doi">10.1126/science.290.5500.2323</pub-id><pub-id pub-id-type="pmid">11125150</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadakane</surname><given-names>O</given-names></name><name><surname>Ozeki</surname><given-names>H</given-names></name><name><surname>Naito</surname><given-names>T</given-names></name><name><surname>Akasaki</surname><given-names>T</given-names></name><name><surname>Kasamatsu</surname><given-names>T</given-names></name><name><surname>Sato</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Contrast-dependent, contextual response modulation in primary visual cortex and lateral geniculate nucleus of the cat</article-title><source>European Journal of Neuroscience</source><volume>23</volume><fpage>1633</fpage><lpage>1642</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2006.04681.x</pub-id><pub-id pub-id-type="pmid">16553627</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salinas</surname><given-names>E</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Vector reconstruction from firing rates</article-title><source>Journal of Computational Neuroscience</source><volume>1</volume><fpage>89</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1007/BF00962720</pub-id><pub-id pub-id-type="pmid">8792227</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanger</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Probability density estimation for the interpretation of neural population codes</article-title><source>Journal of Neurophysiology</source><volume>76</volume><fpage>2799</fpage><lpage>2793</lpage><pub-id pub-id-type="pmid">8899646</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sasaki</surname><given-names>R</given-names></name><name><surname>Uka</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dynamic readout of behaviorally relevant signals from area MT during task switching</article-title><source>Neuron</source><volume>62</volume><fpage>147</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.019</pub-id><pub-id pub-id-type="pmid">19376074</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sceniak</surname><given-names>MP</given-names></name><name><surname>Hawken</surname><given-names>MJ</given-names></name><name><surname>Shapley</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Contrast-dependent changes in spatial frequency tuning of macaque V1 neurons: effects of a changing receptive field size</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>1363</fpage><lpage>1373</lpage><pub-id pub-id-type="doi">10.1152/jn.00967.2001</pub-id><pub-id pub-id-type="pmid">12205157</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sceniak</surname><given-names>MP</given-names></name><name><surname>Ringach</surname><given-names>DL</given-names></name><name><surname>Hawken</surname><given-names>MJ</given-names></name><name><surname>Shapley</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Contrast's effect on spatial summation by macaque V1 neurons</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>733</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1038/11197</pub-id><pub-id pub-id-type="pmid">10412063</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical information flow during flexible sensorimotor decisions</article-title><source>Science</source><volume>348</volume><fpage>1352</fpage><lpage>1355</lpage><pub-id pub-id-type="doi">10.1126/science.aab0551</pub-id><pub-id pub-id-type="pmid">26089513</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smolensky</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1986">1986</year><chapter-title>Information processing in dynamical systems: foundations of harmony theory</chapter-title><person-group person-group-type="editor"><name><surname>Rumelhart</surname> <given-names>D. E</given-names></name><name><surname>McLelland</surname> <given-names>J. L</given-names></name></person-group><source>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</source><volume>Vol. 1</volume><publisher-name>MIT Press</publisher-name><fpage>194</fpage><lpage>281</lpage></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solomon</surname><given-names>SG</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Moving sensory adaptation beyond suppressive effects in single neurons</article-title><source>Current Biology</source><volume>24</volume><fpage>R1012</fpage><lpage>R1022</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.09.001</pub-id><pub-id pub-id-type="pmid">25442850</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A Bayesian Model of Conditioned perception</article-title><source>Advances in Neural Information Processing Systems</source><volume>2007</volume><elocation-id>1409-1416</elocation-id><pub-id pub-id-type="pmid">25328364</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Sigala</surname><given-names>N</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id><pub-id pub-id-type="pmid">23562541</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tajima</surname><given-names>CI</given-names></name><name><surname>Tajima</surname><given-names>S</given-names></name><name><surname>Koida</surname><given-names>K</given-names></name><name><surname>Komatsu</surname><given-names>H</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name><name><surname>Suzuki</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Population Code Dynamics in Categorical Perception</article-title><source>Scientific Reports</source><volume>6</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/srep22536</pub-id><pub-id pub-id-type="pmid">26935275</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tajima</surname><given-names>S</given-names></name><name><surname>Watanabe</surname><given-names>M</given-names></name><name><surname>Imai</surname><given-names>C</given-names></name><name><surname>Ueno</surname><given-names>K</given-names></name><name><surname>Asamizuya</surname><given-names>T</given-names></name><name><surname>Sun</surname><given-names>P</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Cheng</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Opposing effects of contextual surround in human early visual cortex revealed by functional magnetic resonance imaging with continuously modulated visual stimuli</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>3264</fpage><lpage>3270</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4473-09.2010</pub-id><pub-id pub-id-type="pmid">20203185</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>de Silva</surname><given-names>V</given-names></name><name><surname>Langford</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A global geometric framework for nonlinear dimensionality reduction</article-title><source>Science</source><volume>290</volume><fpage>2319</fpage><lpage>2323</lpage><pub-id pub-id-type="doi">10.1126/science.290.5500.2319</pub-id><pub-id pub-id-type="pmid">11125149</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toth</surname><given-names>LJ</given-names></name><name><surname>Rao</surname><given-names>SC</given-names></name><name><surname>Kim</surname><given-names>DS</given-names></name><name><surname>Somers</surname><given-names>D</given-names></name><name><surname>Sur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Subthreshold facilitation and suppression in primary visual cortex revealed by intrinsic signal imaging</article-title><source>PNAS</source><volume>93</volume><fpage>9869</fpage><lpage>9874</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.18.9869</pub-id><pub-id pub-id-type="pmid">8790423</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>MartÃ­nez Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title><source>Nature</source><volume>399</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/21176</pub-id><pub-id pub-id-type="pmid">10376597</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uchikawa</surname><given-names>K</given-names></name><name><surname>Shinoda</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Influence of basic color categories on color memory discrimination</article-title><source>Color Research &amp; Application</source><volume>21</volume><fpage>430</fpage><lpage>439</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1520-6378(199612)21:6&lt;430::AID-COL5&gt;3.0.CO;2-X</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uchikawa</surname><given-names>K</given-names></name><name><surname>Sugiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Effects of eleven basic color categories on color memory</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><volume>34</volume><fpage>745</fpage></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visualizing Data using t-SNE</article-title><source>Journal of Machine Learning Research : JMLR</source><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname><given-names>JD</given-names></name><name><surname>Anderson</surname><given-names>KC</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Single neurons in prefrontal cortex encode abstract rules</article-title><source>Nature</source><volume>411</volume><fpage>953</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1038/35082081</pub-id><pub-id pub-id-type="pmid">11418860</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname><given-names>JD</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neuronal activity in primate dorsolateral and orbital prefrontal cortex during performance of a reward preference task</article-title><source>European Journal of Neuroscience</source><volume>18</volume><fpage>2069</fpage><lpage>2081</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2003.02922.x</pub-id><pub-id pub-id-type="pmid">14622240</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title><source>Neuron</source><volume>36</volume><fpage>955</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01092-9</pub-id><pub-id pub-id-type="pmid">12467598</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision making in recurrent neuronal circuits</article-title><source>Neuron</source><volume>60</volume><fpage>215</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.034</pub-id><pub-id pub-id-type="pmid">18957215</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>K</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name><name><surname>Roxin</surname><given-names>A</given-names></name><name><surname>Peixoto</surname><given-names>D</given-names></name><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>de la Rocha</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sensory integration dynamics in a hierarchical network explains choice probabilities in cortical area MT</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>6177</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms7177</pub-id><pub-id pub-id-type="pmid">25649611</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.26868.025</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing editor</role><aff><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Context-Dependent Attractor Dynamics in Visual Cortex&quot; for consideration by <italic>eLife</italic>. Your article has been favorably evaluated by Timothy Behrens (Senior Editor) and three reviewers, one of whom is a member of our Board of Reviewing Editors. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary of the work:</p><p>This paper presents evidence that context-dependent dynamics occur in the sensory cortex, as opposed to the view that sensory cortex encodes the stimulus information and then passes it to the prefrontal areas for task-dependent interpretation. The authors use a very clean experimental setup, where precisely the same visual stimulus is shown regardless of task context. They then use a nonlinear decoder, which maps the neural recordings to 1D stimulus color space, to analyze the dynamics of these recordings. The decoder is trained on one task (the &quot;Discrimination&quot; task) and then used to interpret the results of the other task (the &quot;Categorization&quot; task). The decoder indicates that the dynamics in the Discrimination and Categorization task are different.</p><p>Essential revisions:</p><p>Although we have a relatively large number of comments, all should be relatively straightforward.</p><p>1) The title â Attractor Dynamics in Visual Cortex â may not be correct, and in addition may actually detract from the message of the paper. Technically, there is an attractor whenever activity goes to a fixed point. However, that's kind of trivial: given that input is fixed, we wouldn't be totally surprised if activity goes to a fixed point, so the fact that there's an attractor wouldn't be an especially big deal. It also wouldn't be an especially big deal if activity didn't go to a fixed point, as we know there are long timescales in network dynamics. And, indeed, it looks like it's the latter: while the population averaged firing rate appears to asymptote (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), the decoded stimulus is still changing at the end of the trial (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>In any case, attractor dynamics is beside the point; what's really going on here is that there is different activity with identical input. This is a very nice illustration of task switching, but not really of attractor dynamics. That, rather than attractor dynamics, should be emphasized.</p><p>2) In the section &quot;The recurrent model explains the stimulus-dependent dynamics,&quot; the authors look at different explanations for the differences in activity in the categorization versus discrimination tasks. The winning explanation was a recurrent model (Equation 10). All well and good, except for two things. First, the model described by Equation 10 isn't really recurrent (could have been a typo; see point 9 below). Second, later on the authors consider a different recurrent model (Equations 14-16). Why not use the second one, which seemed better to me? I suspect there's a reason, but I don't think it was well explained (either that or I missed the explanation).</p><p>3) In the section &quot;Reconstructed collective dynamics explains choice variability,&quot; it would be a good idea to decode individual trials. If the authors really have the right model, the model should make errors on the same trial as the monkeys.</p><p>4) There are two problems with the mutual information calculation. First, it makes no sense to compute accumulated information. Even if the responses were uncorrelated across time bins, the stimulus is correlated. You can see this in the very high information differences â they're larger than the total information available. This can be partially fixed by reporting the instantaneous estimates. Here both <italic>I</italic><sub>Cat</sub> and <italic>I</italic><sub>Dis</sub> should be displayed individually, as the difference does not tell us where the information is coming from.</p><p>Second, to compute information accurately, it's necessary to have the correct noise model. And, by the author's own admission, they have the wrong noise model (they assume independent decoders). Not much they can do about this, but they should at least comment on it.</p><p>5) In the section &quot;Comparison to other methods of dimensionality reduction,&quot; the authors claim that standard dimensionality reduction methods don't capture the difference in the discrimination versus categorization tasks. They need to do two things to back up that claim. First, they need to quantify it: differences between the trajectories in the categorization and discrimination tasks seem about as big in <xref ref-type="fig" rid="fig5">Figure 5D</xref> as they do in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. And in general it is more difficult to judge separation in a single 3D plot than in 1D, so a purely visual comparison isn't so useful.</p><p>Second, they should try a similarly supervised linear approach used for the nonlinear decoder. For example they could try an approach similar to the population vectors used in motor decoding, where they weight the firing rate according to a neuron's stimulus preference. An allied more modern approach is demixed PCA (Kobak et al., <italic>eLife</italic> 2016). Both of these differ substantially from the unsupervised PCA approach the authors use, which by comparison could be seen as a bit of a straw man for linear decoding. Although maybe not so much of a straw man; see above.</p><p>Finally, <xref ref-type="fig" rid="fig5">Figures 5A and 5C</xref> should also be made easier to interpret â in particular, the dots are hard to visualize.</p><p>6) In the section &quot;Bifurcation of attractor dynamics in a recurrent model,&quot; the authors consider a second, different, recurrent model. The nullclines in <xref ref-type="fig" rid="fig6">Figures 6B and C</xref> are pretty standard. However, they are known to be somewhat fragile. Do the nullclines really keep their shape no matter what color is presented? The set of nullclines for all color presentations should be shown. It could be in Methods, and it could be all on one plot.</p><p>7) Section âLikelihood-based decodingâ: Why smooth over the stimulus space? Analysis is probably easier if <italic>s</italic> is discrete â you can just compute the posterior for the discrete values of <italic>s</italic> shown to the animal. If there's a benefit to considering <italic>s</italic> to be a continuous variable, that should be mentioned. Otherwise, it should be discrete.</p><p>8) Something seems to be wrong with Equation 10: it's not a recurrent network; instead, <inline-formula><mml:math id="inf150"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> depends on <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>and<inline-formula><mml:math id="inf152"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Is this really what the authors meant? If so, it shouldn't be called a recurrent network. If not, it needs to be corrected (there may be a hat missing somewhere).</p><p>9) The method for comparing models used in <xref ref-type="fig" rid="fig2">Figure 2C</xref>/D, while nice for presentation as the decoder results are 1D, is difficult to interpret. Because the model prediction comes from multiple steps (first fitting the firing rates, then putting it through the decoder which could strongly transform them), it is hard to judge the meaning of the results in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. It would be good to report the mean squared error between the actual firing rates in the categorization tasks and the firing rates predicted by the model. Presumably they'll show the same trends as in panel D, but it would be nice to see that.</p><p>10) Is there a simple explanation for why the recurrent model does better? It is unclear what about the recurrent model makes it work, and some more intermediate models could be used to elucidate this. Does the recurrence matter, or is it simply fitting an additive component that works? Note that the answer may be &quot;we can't find a simple explanation&quot;. If so, use your best judgment as to whether you want to include that in the paper.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.26868.026</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>Although we have a relatively large number of comments, all should be relatively straightforward.</italic> </p><p><italic>1) The title â Attractor Dynamics in Visual Cortex â may not be correct, and in addition may actually detract from the message of the paper. Technically, there is an attractor whenever activity goes to a fixed point. However, that's kind of trivial: given that input is fixed, we wouldn't be totally surprised if activity goes to a fixed point, so the fact that there's an attractor wouldn't be an especially big deal. It also wouldn't be an especially big deal if activity didn't go to a fixed point, as we know there are long timescales in network dynamics. And, indeed, it looks like it's the latter: while the population averaged firing rate appears to asymptote (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), the decoded stimulus is still changing at the end of the trial (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</italic> </p><p><italic>In any case, attractor dynamics is beside the point; what's really going on here is that there is different activity with identical input. This is a very nice illustration of task switching, but not really of attractor dynamics. That, rather than attractor dynamics, should be emphasized.</italic> </p><p>We agree with the reviewers in these points, and changed the title to âTask-dependent recurrent dynamics in visual cortex.â We are open to further changes if the new title seemed not the most appropriate one.</p><p><italic>2) In the section &quot;The recurrent model explains the stimulus-dependent dynamics,&quot; the authors look at different explanations for the differences in activity in the categorization versus discrimination tasks. The winning explanation was a recurrent model (Equation 10). All well and good, except for two things. First, the model described by Equation 10 isn't really recurrent (could have been a typo; see point 9 below). Second, later on the authors consider a different recurrent model (Equations 14-16). Why not use the second one, which seemed better to me? I suspect there's a reason, but I don't think it was well explained (either that or I missed the explanation).</italic> </p><p>On the first point, we corrected the typo in old Equation 10 as the reviewer suggested (please see the response to point 8).</p><p>With regard to the second point, for a technical reason, we did not fit our âsecondâ recurrent model directly to the data. In the âfirstâ model, we needed to keep the adjustable model parameters as few as possible to avoid overfitting the data. On the other hand, the âsecondâ model includes additional parameters to demonstrate a possible mathematical principle for attractor bifurcation in a self-contained manner. For example, in the âsecondâ model, we explicitly modeled the bottom-up inputs to the individual neurons by term ð°(ð , ð¡) (containing parameters in the number of neuronsÃstimuliÃtime bins), which were not directly accessible in the actual data. It could be theoretically possible to estimate those components from the data together with the recurrent term ð¾ (by introducing a more advanced fitting method, such as Monte Carlo sampling). However, in practice, introducing more model parameters leads</p><p>to the overfitting/instability in fitting results, as well as make the interpretation of results unstraightforward: even when the model fitted the data well, the increased model complexity can obscure whether such a model explained the data because the recurrent component was crucial or merely because it had many parameters.</p><p>Nonetheless, the two models share the basic circuit architecture, and the results together demonstrate that the recurrent mechanism both describes the data and accounts for the potential mathematical principle underling the context-dependent neural dynamics. We explained this point in the revised manuscript (subsection âA model of context-dependent attractor dynamicsâ, last paragraph). In addition, in the revised manuscript, we replicated the âfirstâ modelâs fitting results by making two minor modifications so that the âfirstâ model makes further matches to the âsecondâ model: using a sigmoid rather than the âtanhâ function for the hidden unit activation function, and assuming a single common bias input to the hidden units rather than two independent biases to them. Lastly, we also added the results showing the response trajectories generated by running the âfirst,â trained model (<xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2A</xref>), which replicates the bistable fixed points. We expect that this would help the readers see the correspondence between the first and second models more clearly.</p><p><italic>3) In the section &quot;Reconstructed collective dynamics explains choice variability,&quot; it would be a good idea to decode individual trials. If the authors really have the right model, the model should make errors on the same trial as the monkeys.</italic> </p><p>Although we are interested in this point, unfortunately we could not do this analysis because the present dataset is not based on a simultaneous multi-unit recording but based on sequentially collected single unit recordings, from which we reconstructed the âvirtual population activity,â as we have described in the text (e.g., subsection âReconstructing population activity dynamics from a decoding perspectiveâ, first paragraph). Nonetheless, we thank the reviewers for this suggestion and would like to try this analysis when we could obtain multi-unit recording data in a future study.</p><p><italic>4) There are two problems with the mutual information calculation. First, it makes no sense to compute accumulated information. Even if the responses were uncorrelated across time bins, the stimulus is correlated. You can see this in the very high information differences â they're larger than the total information available. This can be partially fixed by reporting the instantaneous estimates. Here both I<sub>Cat</sub> and I<sub>Dis</sub> should be displayed individually, as the difference does not tell us where the information is coming from.</italic> </p><p><italic>Second, to compute information accurately, it's necessary to have the correct noise model. And, by the author's own admission, they have the wrong noise model (they assume independent decoders). Not much they can do about this, but they should at least comment on it.</italic> </p><p>We agree with both the points. We changed the plots to show the instantaneous estimates of mutual information (<xref ref-type="fig" rid="fig4">Figure 4</xref>), and noted that those estimates assume no noise correlations among neurons (subsection âDynamical modulation enhances task-relevant informationâ). Interestingly, the increase in hue information in the Discrimination task is specific to the early period of response, which could be partially due to the relatively stronger responses (i.e., less firing variability) in the late period during the Categorization task.</p><p><italic>5) In the section &quot;Comparison to other methods of dimensionality reduction,&quot; the authors claim that standard dimensionality reduction methods don't capture the difference in the discrimination versus categorization tasks. They need to do two things to back up that claim. First, they need to quantify it: differences between the trajectories in the Categorization and Discrimination tasks seem about as big in <xref ref-type="fig" rid="fig5">Figure 5D</xref> as they do in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. And in general it is more difficult to judge separation in a single 3D plot than in 1D, so a purely visual comparison isn't so useful.</italic> </p><p><italic>Second, they should try a similarly supervised linear approach used for the nonlinear decoder. For example they could try an approach similar to the population vectors used in motor decoding, where they weight the firing rate according to a neuron's stimulus preference. An allied more modern approach is demixed PCA (Kobak et al., eLife 2016). Both of these differ substantially from the unsupervised PCA approach the authors use, which by comparison could be seen as a bit of a straw man for linear decoding. Although maybe not so much of a straw man; see above.</italic> </p><p><italic>Finally, <xref ref-type="fig" rid="fig5">Figures 5A and 5C</xref> should also be made easier to interpret â in particular, the dots are hard to visualize.</italic> </p><p>On the first and the second points, we quantified the task-differences in clustering effects for each dimensionality reduction method in new <xref ref-type="fig" rid="fig6">Figure 6</xref>. In the revised manuscript, we included the population vector decoding and demixed PCA (dPCA), as suggested. We additionally tested an optimal linear decoder, as the population vector decoding is known to be biased in non-uniform neural populations. We removed the second PCA analysis (PCA on the task-dependent component) because it is redundant when we have the result of dPCA. (For the consistency, we showed the same clustering index for the likelihood-based decoding in <xref ref-type="fig" rid="fig1">Figure 1C</xref>/D.) These linear methods did not demonstrate clear task-dependent clustering effect (although âtaskâ component in dPCA showed some trend toward it), suggesting an importance of nonlinear methods. We also preliminary tried other discriminative models such as nonlinear support vector machines but could not obtain a stable result due to the limitation in the data size compared to the feature dimensionality.</p><p>With regard to the third point, we improved the visualization by clarifying the end point of each trajectory.</p><p><italic>6) In the section &quot;Bifurcation of attractor dynamics in a recurrent model,&quot; the authors consider a second, different, recurrent model. The nullclines in <xref ref-type="fig" rid="fig6">Figures 6B and C</xref> are pretty standard. However, they are known to be somewhat fragile. Do the nullclines really keep their shape no matter what color is presented? The set of nullclines for all color presentations should be shown. It could be in Methods, and it could be all on one plot.</italic> </p><p>As suggested, we presented the set of nullclines for a range of the stimulus colors (<xref ref-type="fig" rid="fig6s1">Figure 6âfigure supplement 1</xref>). Note that the bistability is observed specifically for the neutral (âyellowishâ) stimuli, which is qualitatively consistent with the monkeysâ choice behavior (subsection âBifurcation of attractor dynamics in a recurrent modelâ, third paragraph). (We presented this figure as a figure supplement because the current contents already seemed relatively dense for nontheoreticians, but we can place it to the main contents if the reviewers think it is better.)</p><p><italic>7) Section âLikelihood-based decodingâ: Why smooth over the stimulus space? Analysis is probably easier if s is discrete â you can just compute the posterior for the discrete values of s shown to the animal. If there's a benefit to considering s to be a continuous variable, that should be mentioned. Otherwise, it should be discrete.</italic> </p><p>The primary benefit of the smoothing is that the actual neural tuning curves are considered be continuous functions of stimulus, and we could avoid large discrete jumps in trajectories of the decoder outputs to have more robust results. We mentioned this point in the revised manuscript (subsection âLikelihood-based decodingâ, second paragraph).</p><p><italic>8) Something seems to be wrong with Equation 10: it's not a recurrent network; instead,</italic> <inline-formula><mml:math id="inf153"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> <italic>depends on</italic> <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula><italic>and</italic><inline-formula><mml:math id="inf155"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula><italic>. Is this really what the authors meant? If so, it shouldn't be called a recurrent network. If not, it needs to be corrected (there may be a hat missing somewhere).</italic> </p><p>This was a typo and the ð in the second equation should have been ðÌ (the âhatâ was missing). We corrected this point in the revised manuscript.</p><p><italic>9) The method for comparing models used in <xref ref-type="fig" rid="fig2">Figure 2C</xref>/D, while nice for presentation as the decoder results are 1D, is difficult to interpret. Because the model prediction comes from multiple steps (first fitting the firing rates, then putting it through the decoder which could strongly transform them), it is hard to judge the meaning of the results in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. It would be good to report the mean squared error between the actual firing rates in the categorization tasks and the firing rates predicted by the model. Presumably they'll show the same trends as in panel D, but it would be nice to see that.</italic> </p><p>We agree that it should be better to visualize the cross-validation errors measured in the actual firing rates. We added the plot showing the cross-validation errors in the revised manuscript (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, inset).</p><p><italic>10) Is there a simple explanation for why the recurrent model does better? It is unclear what about the recurrent model makes it work, and some more intermediate models could be used to elucidate this. Does the recurrence matter, or is it simply fitting an additive component that works? Note that the answer may be &quot;we can't find a simple explanation&quot;. If so, use your best judgment as to whether you want to include that in the paper.</italic> </p><p>It might not be necessarily straightforward to provide a complete and simple/intuitive explanation for the fitting performances. Nonetheless, the better fitting performance in the recurrent model seems to come, at least partially, from the fact that it can replicate the âfixed point attractorsâ demonstrated in <xref ref-type="fig" rid="fig2">Figure 2A</xref> better than the feedforward models cannot have attractors. It could be intuitively seen in the predicted neural state trajectories generated by the trained models, in which the feedforward models form less clear categorical clustering in the end points of the trajectories compared to the recurrent model (quantified in the new <xref ref-type="fig" rid="fig2">Figure 2D</xref>; the trajectories visualized in <xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2B-D</xref>).</p><p>We can say at least that the better fitting performance is not merely due to the higher degree of freedom in the model (indeed, it has even less parameters than one of the feedforward models), thus we believe that the model architecture matters. We conjecture that the recurrent model works better because it describes more accurately what is happening in the actual nervous system. We also discussed possible approaches to investigating the physiological mechanisms underlying the phenomena reported in the present work (Discussion, third paragraph).</p></body></sub-article></article>