<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">12215</article-id><article-id pub-id-type="doi">10.7554/eLife.12215</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Active sensing in the categorization of visual patterns</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-33538"><name><surname>Yang</surname><given-names>Scott Cheng-Hsin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-31486"><name><surname>Lengyel</surname><given-names>Máté</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7266-0049</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-3698"><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2011-2790</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Computational and Biological Learning Lab, Department of Engineering</institution>, <institution>University of Cambridge</institution>, <addr-line><named-content content-type="city">Cambridge</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Cognitive Science</institution>, <institution>Central European University</institution>, <addr-line><named-content content-type="city">Budapest</named-content></addr-line>, <country>Hungary</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Tsao</surname><given-names>Doris Y</given-names></name><role>Reviewing editor</role><aff id="aff3"><institution>California Institute of Technology</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>schy2@eng.cam.ac.uk</email></corresp><fn fn-type="con" id="equal-contrib"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="pub" publication-format="electronic"><day>10</day><month>02</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e12215</elocation-id><history><date date-type="received"><day>10</day><month>10</month><year>2015</year></date><date date-type="accepted"><day>06</day><month>12</month><year>2015</year></date></history><permissions><copyright-statement>© 2016, Yang et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Yang et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-12215-v3.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.12215.001</object-id><p>Interpreting visual scenes typically requires us to accumulate information from multiple locations in a scene. Using a novel gaze-contingent paradigm in a visual categorization task, we show that participants' scan paths follow an active sensing strategy that incorporates information already acquired about the scene and knowledge of the statistical structure of patterns. Intriguingly, categorization performance was markedly improved when locations were revealed to participants by an optimal Bayesian active sensor algorithm. By using a combination of a Bayesian ideal observer and the active sensor algorithm, we estimate that a major portion of this apparent suboptimality of fixation locations arises from prior biases, perceptual noise and inaccuracies in eye movements, and the central process of selecting fixation locations is around 70% efficient in our task. Our results suggest that participants select eye movements with the goal of maximizing information about abstract categories that require the integration of information from multiple locations.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.001">http://dx.doi.org/10.7554/eLife.12215.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.12215.002</object-id><title>eLife digest</title><p>To interact with the world around us, we need to decide how best to direct our eyes and other senses to extract relevant information. When viewing a scene, people fixate on a sequence of locations by making fast eye movements to shift their gaze between locations. Previous studies have shown that these fixations are not random, but are actively chosen so that they depend on both the scene and the task. For example, in order to determine the gender or emotion from a face, we fixate around the eyes or the nose, respectively.</p><p>Previous studies have only analyzed whether humans choose the optimal fixation locations in very simple situations, such as searching for a square among a set of circles. Therefore, it is not known how efficient we are at optimizing our rapid eye movements to extract high-level information from visual scenes, such as determining whether an image of fur belongs to a cheetah or a zebra.</p><p>Yang, Lengyel and Wolpert developed a mathematical model that determines the amount of information that can be extracted from an image by any set of fixation locations. The model could also work out the next best fixation location that would maximize the amount of information that could be collected. This model shows that humans are about 70% efficient in planning each eye movement. Furthermore, it suggests that the inefficiencies are largely caused by imperfect vision and inaccurate eye movements.</p><p>Yang, Lengyel and Wolpert’s findings indicate that we combine information from multiple locations to direct our eye movements so that we can maximize the information we collect from our surroundings. The next challenge is to extend this mathematical model and experimental approach to even more complex visual tasks, such as judging an individual’s intentions, or working out the relationships between people in real-life settings.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.002">http://dx.doi.org/10.7554/eLife.12215.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>active sensing</kwd><kwd>eye movements</kwd><kwd>visual categorization</kwd><kwd>Bayesian analysis</kwd><kwd>ideal observer</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Yang</surname><given-names>Scott Cheng-Hsin</given-names></name><name><surname>Lengyel</surname><given-names>Máté</given-names></name><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004412</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Humans use a near-optimal eye movement strategy to efficiently extract information about high-level visual categories.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Several lines of evidence suggest that humans and other animals direct their sensors (e.g. their eyes, whiskers, or hands) so as to extract task-relevant information efficiently (<xref ref-type="bibr" rid="bib48">Yarbus, 1967</xref>; <xref ref-type="bibr" rid="bib18">Kleinfeld et al., 2006</xref>; <xref ref-type="bibr" rid="bib23">Lederman and Klatzky, 1987</xref>). Indeed, in vision, the pattern of eye movements used to scan a scene depends on the type of information sought (<xref ref-type="bibr" rid="bib10">Hayhoe and Ballard, 2005</xref>; <xref ref-type="bibr" rid="bib39">Rothkopf et al., 2007</xref>), and has been implied to follow from an active strategy (<xref ref-type="bibr" rid="bib29">Najemnik and Geisler, 2005</xref>; <xref ref-type="bibr" rid="bib37">Renninger et al., 2007</xref>; <xref ref-type="bibr" rid="bib31">Navalpakkam et al., 2010</xref>; <xref ref-type="bibr" rid="bib32">Nelson and Cottrell, 2007</xref>; <xref ref-type="bibr" rid="bib46">Toscani et al., 2013</xref>; <xref ref-type="bibr" rid="bib4">Chukoskie et al., 2013</xref>) in which each saccade depends on the information gathered about the current visual scene and prior knowledge about scene statistics. However, until now, studies of such active sensing have either been limited to search tasks or to qualitative descriptions of the active sensing process. In particular, no studies have shown whether the information acquired by each individual fixation is being optimized. Rather, the fixation patterns have either been described without a tight link to optimality (<xref ref-type="bibr" rid="bib1">Ballard et al., 1995</xref>; <xref ref-type="bibr" rid="bib5">Epelboim and Suppes, 2001</xref>) or compared to an optimal strategy only through summary statistics such as the total number of eye movements and the distribution of saccade vectors (<xref ref-type="bibr" rid="bib29">Najemnik and Geisler, 2005</xref>; <xref ref-type="bibr" rid="bib30">2008</xref>) that could have arisen through a heuristic. Therefore, these studies leave open the question as to what extent eye movements truly follow an active optimal strategy. In order to study eye movements in a more principled quantitative manner, we estimated the efficiency of eye movements in a high-level task on a fixation-by-fixation basis.</p><p>Here, we focus on a pattern categorization task that is fundamentally different from visual search, in which often there is a single location in the scene that has all the necessary information (the target), and eye movements are well described by the simple mechanism of inhibition of return (<xref ref-type="bibr" rid="bib17">Klein, 2000</xref>). In contrast, in many other tasks, such as constructing the meaning of a sentence of written text, or judging from a picture how long a visitor has been away from a family (<xref ref-type="bibr" rid="bib48">Yarbus, 1967</xref>), no single visual location has the necessary information in it and thus such tasks require more complex eye movement patterns. While some basic-level categorization tasks can be solved in a single fixation (<xref ref-type="bibr" rid="bib45">Thorpe et al., 1996</xref>; <xref ref-type="bibr" rid="bib25">Li et al., 2002</xref>), many situations require multiple fixations to extract several details at different locations to make a decision. Therefore, when people have to extract abstract information (eg., how long the visitor has been away) they need to integrate a series of detailed observations (such as facial expression, postures and gestures of the people) across the scene, relying heavily on foveal vision information with peripheral vision playing a more minor role (<xref ref-type="bibr" rid="bib24">Levi, 2008</xref>).</p><p>We illustrate the key features of active sensing in visual categorization by a situation that requires the categorization of an animal based on its fur that is partially obscured by foliage (<xref ref-type="fig" rid="fig1">Figure 1</xref>). As each individual patch of fur can be consistent with different animals, such as a zebra or a cheetah, and the foliage prevents the usage of gist information (<xref ref-type="bibr" rid="bib33">Oliva and Torralba, 2006</xref>), multiple locations have to be fixated individually, and the information accumulated across these locations, until a decision can be made with high confidence. For maximal efficiency, this requires a closed loop interaction between perception, which integrates information from the locations already fixated with prior knowledge about the prevalence of different animals and their fur patterns, and thus maintains beliefs about which animal might be present in the image, and the planning of eye movement, which should direct the next fixation at a location which has potentially the most information relevant for the categorization (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Inspired by this example, and to allow a mathematically tractable quantification of the information at any fixation location, we designed an experiment with visual patterns that were statistically well-controlled and relatively simple, while ensuring that foveal vision would dominate by using a gaze-contingent categorization task in which we tracked the eye and successively revealed small apertures of the image at each fixation location. In contrast to previous studies (<xref ref-type="bibr" rid="bib29">Najemnik and Geisler, 2005</xref>; <xref ref-type="bibr" rid="bib37">Renninger et al., 2007</xref>; <xref ref-type="bibr" rid="bib34">Peterson and Eckstein, 2012</xref>; <xref ref-type="bibr" rid="bib28">Morvan and Maloney, 2012</xref>), our task required multiple locations to be visited to extract information about abstract pattern categories.<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.12215.003</object-id><label>Figure 1.</label><caption><title>Active sensing involves an interplay between perception and action.</title><p>When trying to categorize whether a fur hidden behind foliage (left) belongs to a zebra or a cheetah, evidence from multiple fixations (blue, the visible patches of the fur, and their location in the image) needs to be integrated to generate beliefs about fur category (right, here represented probabilistically, as the posterior probability of the particular animal given the evidence). Given current beliefs, different potential locations in the scene will be expected to have different amounts of informativeness with regard to further distinguishing between the categories, and optimal sensing involves choosing the maximally informative location (red). In the example shown, after the first two fixations (blue) it is ambiguous whether the fur belongs to a zebra or a cheetah, but active sensing chooses a collinearly located revealing position (red) which should be informative and indeed reveals a zebra with high certainty. Note that this is just an illustrative example.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.003">http://dx.doi.org/10.7554/eLife.12215.003</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig1-v3"/></fig></p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Categorization performance and eye movement patterns</title><p>We generated images of three types: patchy, horizontal stripy, and vertical stripy (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Participants had to categorize each image pattern as patchy or stripy (disregarding whether a stripy image was horizontal or vertical—the inclusion of two different stripy image types prevented participants from solving the task based on one image axis alone). The images were generated by a flexible statistical model that could generate many examples from each of the three image types, so that the individual pixel values varied widely even within a type and only higher order statistical information (ie. the length scale of spatial correlations) could be used for categorization. We first presented the participants with examples of full images to familiarize them with the statistics of the image types and to ensure their categorization with full images was perfect. We then switched to an active gaze-contingent mode in which the entire pattern was initially occluded by a black mask and the underlying image was revealed with a small aperture at each fixation location (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; for visibility, the black mask is shown as white). As a control, we also used a number of passive revealing conditions in which the revealing locations were chosen by the computer rather than in a gaze-contingent manner. In all conditions, we controlled the number of revealings on each trial before requiring the participants to categorize the image (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). To ensure that participants had equal chance to extract information from all revealing locations in the passive as well as the active conditions we allowed the participants to rescan the revealed locations after the final revealing (see also Materials and methods for full rationale). Importantly, in the active revealing condition, even though rescanning was allowed after the final revealing, participants had to select all revealing locations in real time without knowing how many revealings they would be allowed on a given trial. Therefore, although rescanning could improve categorization it was unlikely to influence participants’ active revealing strategy. To confirm this we also performed a control in which rescanning was not allowed (see below).<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.12215.004</object-id><label>Figure 2.</label><caption><title>Image categorization task and participants’ performance.</title><p>(<bold>A</bold>) Example stimuli for each of the three image types sampled from two-dimensional Gaussian processes. (<bold>B</bold>) Experimental design. Participants started each trial by fixating the center cross. In the free-scan condition, an aperture of the underlying image was revealed at each fixation location. In the passive condition, revealing locations were chosen by the computer. In both conditions, after a random number of revealings, participants were required to make a category choice (patchy, P, versus stripy, S) and were given feedback. (<bold>C</bold>) Categorization performance as a function of revealing number for each of the three participants (symbols and error bars: mean <inline-formula><mml:math id="inf1"> <mml:mo>± </mml:mo></mml:math></inline-formula> SEM across trials), and their average, under the free-scan and passive conditions corresponding to different revealing strategies. Lines and shaded areas show across-trial mean <inline-formula><mml:math id="inf2"> <mml:mo>± </mml:mo></mml:math></inline-formula> SEM for the ideal observer model. <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> shows categorization performance in a control experiment in which no rescanning was allowed.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.004">http://dx.doi.org/10.7554/eLife.12215.004</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig2-v3"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.12215.005</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Performance in the no-rescanning control experiment.</title><p>Categorization performance as a function of revealing number for participants in the control (no rescanning) and the average participant in the main experiment (rescanning) in which rescanning was allowed (cf. <xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.005">http://dx.doi.org/10.7554/eLife.12215.005</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig2-figsupp1-v3"/></fig></fig-group></p><p>In the free-scan (active) condition, categorization performance improved with the number of revealings for each participant (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, red points), indicating the successful integration of information across a large number of fixations. Although we used a gaze-contingent display, the task still allowed participants to employ natural every-day eye movement strategies, consistent with the inter-saccadic intervals (mean 408 ms) and relative saccade size (0.75–3.9 normalized by the three different relevant length scales of the stimuli) that were similar though somewhat shorter than those recorded for everyday activities (e.g. tea making; mean inter-saccadic interval 497 ms, and 0.95–19 relative saccade size normalized by the size of different fixated objects; <xref ref-type="bibr" rid="bib11">Hayhoe et al., 2003</xref>; <xref ref-type="bibr" rid="bib22">Land and Tatler, 2009</xref>).</p><p>To examine whether fixation locations depended on the underlying image patterns, we constructed revealing density maps for each image type. To account for the translation-invariant statistics of the underlying images of a type, we used the relative location of revealings obtained by subtracting from the absolute location of each revealing (measured in screen-centered coordinates) the center of mass of the absolute locations within each trial (Materials and methods). In order to compare eye movement patterns across conditions and participants, we subtracted the mean density map across all images for each participant. Importantly, we found that the pattern of revealings strongly depended on image type (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, first four rows). The pattern of eye movements for images of the same type were positively correlated for each participant (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left, orange bars; p&lt;0.001 in all cases), whereas eye movements for images of different types were negatively correlated (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left, purple bars; p&lt;0.001 in all cases). We also found that eye movement patterns became increasingly differentiated over the course of the trial as progressively more of the image was revealed (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left, curves). The dependence of the eye movement patterns on the underlying image type shows that participants employed an active sensing strategy.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.12215.006</object-id><label>Figure 3.</label><caption><title>Density maps of relative revealing locations and their correlations.</title><p>(<bold>A</bold>) Revealing density maps for participants and BAS. Last three columns show mean-corrected revealing densities for each of the three underlying image types (removing the mean density across image types, first column). Bottom: color scales used for all mean densities (left), and for all mean-corrected densities (right). All density maps use the same scale, such that a density of 1 corresponds to the peak mean density across all maps. <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> shows revealing density maps obtained for participants in a control experiment in which no rescanning was allowed. <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> shows the measured saccadic noise that was incorporated into the BAS simulations. <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref> shows density maps separately for correct and incorrect trials. (<bold>B</bold>) The curves are correlations for individual participants as a function of revealing number with their own maps (left) and the maps generated by BAS (right). The bars are correlations at 25 revealing (see Materials and methods). Orange shows within image type correlation, ie. correlation between revealing densities obtained for images of the same type, and purple shows across image type correlation. Data are represented as mean <inline-formula><mml:math id="inf3"> <mml:mo>± </mml:mo></mml:math></inline-formula> SD for the curves and mean <inline-formula><mml:math id="inf4"> <mml:mo>± </mml:mo></mml:math></inline-formula> 95% confidence intervals for the bars.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.006">http://dx.doi.org/10.7554/eLife.12215.006</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig3-v3"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.12215.007</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Revealing density maps in the no-rescanning control experiment.</title><p>Revealing density maps in the control (no rescanning) and the average participant in the main experiment (rescanning) in which rescanning was allowed (cf. <xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.007">http://dx.doi.org/10.7554/eLife.12215.007</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig3-figsupp1-v3"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.12215.008</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Saccadic variability and bias.</title><p>(<bold>A</bold>) Saccadic standard deviation both along (tangential) and orthogonal to the target direction with linear regression giving SD = 0:13×distance+0:41° and 0:011 distance+0:38°, respectively. (<bold>B</bold>) Saccadic bias along the target direction (negative is undershoot) with linear regression giving bias = 0:23×distance+0:37°. Error bars are SEM across participants.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.008">http://dx.doi.org/10.7554/eLife.12215.008</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig3-figsupp2-v3"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.12215.009</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Revealing density maps of the average participant in the main experiment split into correct and incorrect trials, and the BAS revealing maps.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.009">http://dx.doi.org/10.7554/eLife.12215.009</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig3-figsupp3-v3"/></fig></fig-group></p><p>To assess whether this active strategy used by our participants contributed to performance improvement, we examined the same participants in a passive revealing condition. When the revealings were drawn randomly from an isotropic Gaussian centered on the image, performance was substantially impaired (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, blue points), indicating that participants’ decision performance benefited from their active sensing strategy. After the final revealing, participants rescanned only briefly, on average for 5.0 s in the active condition, 6.4 s in the passive random condition before making a decision. Moreover, their performance did not improve with increased rescanning time, instead it correlated negatively with rescanning time for 2 out of 3 participants, such that the probability of a correct decision for rescanning times at the 25th and 75th percentiles of the rescanning time distribution fell from 0.77 to 0.54 (p&lt;0.001) and from 0.77 to 0.66 (p&lt;0.03), respectively. This suggests that longer rescanning times indicate when participants are uncertain rather than providing the main source of information for their decisions. This is in contrast to additional revealings during the original scanning period which clearly benefit performance when chosen appropriately (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><p>To further examine whether the rescanning period after the final revealing affected the participants scanning strategy, we examined additional participants in a free-scan condition in which no rescanning was allowed after the final revealing (the display blanked). The revealing density maps for these participants were very similar to the maps of participants who were allowed to rescan (average within-type vs. across-type correlation across the two groups of participants: 0.63 vs. −0.30) and performance was also similar (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), although as expected slightly worse without rescanning. The proportion correct across all trials for the participants who were allowed to rescan was 0.65, 0.66 and 0.69 (average 0.66) and for those not allowed to rescan was 0.64, 0.58 and 0.66 (average 0.63). This confirms that allowing rescanning did not substantially change participants’ revealing strategy.</p></sec><sec id="s2-2"><title>Bayesian ideal observer</title><p>We constructed an ideal observer (<xref ref-type="bibr" rid="bib7">Geisler, 2011</xref>) which computed a posterior distribution, <inline-formula><mml:math id="inf5"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, over image category <inline-formula><mml:math id="inf6"><mml:mi>c</mml:mi></mml:math></inline-formula> given the observations <italic>D</italic> (collection of previous revealing locations and revealed pixel values in the trial) and made a choice such that the category with higher posterior probability was more likely to be selected (see Materials and methods). To construct this model, we considered three sources of suboptimality: <italic>prior biases</italic> implying imperfect knowledge of the precise correlation length scales of each pattern category, <italic>perception noise</italic> that distorts the displayed pixel values, and <italic>decision noise</italic> which occasionally results in selecting the category with the lower posterior probability (<xref ref-type="bibr" rid="bib14">Houlsby et al., 2013</xref>). We fitted six models to the individual choices of each participant. These models differed in the kind of prior bias and decision noise they included, and we selected the model with the strongest statistical evidence, as quantified by the Bayesian information criterion (<xref ref-type="table" rid="tbl1">Tables 1</xref>–<xref ref-type="table" rid="tbl2">2</xref>). The best model (4 parameters in total) provided a close match to the participants’ performance both in the active and passive random-revealing conditions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, red and blue lines). Crucially, this model also allowed us to estimate the beliefs that participants held about image categories at any point in a trial, which was necessary for determining the optimal next eye movement that could maximally disambiguate between the categories.<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.12215.010</object-id><label>Table 1.</label><caption><p>Maximum likelihood parameters of the model (see Materials and methods for details) with the best BIC score (see Table 2).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.010">http://dx.doi.org/10.7554/eLife.12215.010</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Participant</th><th rowspan="2">Perception noise, σ<sub>p</sub></th><th rowspan="2">Prior bias, Δ</th><th colspan="2">Decision noise</th></tr><tr><th>Stimulus-dependent, β</th><th>Stimulus-independent, κ</th></tr></thead><tbody><tr><td>1</td><td>0.5</td><td>0.58°</td><td>1.4</td><td>0.044</td></tr><tr><td>2</td><td>0.5</td><td>0.61°</td><td>1.9</td><td>0.12</td></tr><tr><td>3</td><td>0.3</td><td>0.54°</td><td>1.5</td><td>0.10</td></tr></tbody></table></table-wrap><table-wrap id="tbl2" position="float"><object-id pub-id-type="doi">10.7554/eLife.12215.011</object-id><label>Table 2.</label><caption><p>Model comparison results using Bayesian information criterion (BIC, lower is better). Each row is a different model using a different combination of included (+) and excluded (–) parameters (columns, see Materials and methods for details). Last column shows BIC score relative to the BIC of the best model (number 4).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.011">http://dx.doi.org/10.7554/eLife.12215.011</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Model</th><th rowspan="2">Perception noise, σ<sub>p</sub></th><th colspan="2">Prior bias</th><th colspan="2">Decision noise</th><th rowspan="2">BIC</th></tr><tr><th>Scale, α</th><th>Offset, Δ</th><th>Stimulus-dependent, β</th><th>Stimulus-independent, κ</th></tr></thead><tbody><tr><td>1</td><td>+</td><td>–</td><td>–</td><td>+</td><td>–</td><td>160</td></tr><tr><td>2</td><td>+</td><td>–</td><td>–</td><td>+</td><td>+</td><td>139</td></tr><tr><td>3</td><td>+</td><td>–</td><td>+</td><td>+</td><td>–</td><td>58</td></tr><tr><td>4</td><td>+</td><td>–</td><td>+</td><td>+</td><td>+</td><td>0</td></tr><tr><td>5</td><td>+</td><td>+</td><td>–</td><td>+</td><td>–</td><td>105</td></tr><tr><td>6</td><td>+</td><td>+</td><td>–</td><td>+</td><td>+</td><td>102</td></tr></tbody></table></table-wrap></p></sec><sec id="s2-3"><title>Predicting eye movement patterns by a Bayesian active sensor algorithm</title><p>To be able to rigorously assess how close our participants were to optimal sensing, we developed a Bayesian active sensor (BAS) algorithm which is optimal in minimizing categorization error with every single revealing. That is, for our task, the aim of BAS is to choose the next fixation location, <inline-formula><mml:math id="inf7"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, so as to maximally reduce uncertainty in the category (<xref ref-type="bibr" rid="bib26">MacKay, 1992</xref>). This objective is formalized by the BAS score function which expresses the expected information gain when choosing <inline-formula><mml:math id="inf8"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, and which can be conveniently computed as:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>Score</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>H</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mtext>H</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf9"><mml:mtext>H</mml:mtext></mml:math></inline-formula> denotes entropy (a measure of uncertainty), <inline-formula><mml:math id="inf10"><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> is the possible pixel value at <inline-formula><mml:math id="inf11"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf12"><mml:mi>D</mml:mi></mml:math></inline-formula> is the collection of revealing locations and revealed pixel values that have been observed in the trial as above, <inline-formula><mml:math id="inf13"><mml:mi>c</mml:mi></mml:math></inline-formula> is image category, and <inline-formula><mml:math id="inf14"><mml:mrow><mml:mo>⟨</mml:mo><mml:mo id="XM8">⋅</mml:mo><mml:mo>⟩</mml:mo></mml:mrow></mml:math></inline-formula> denotes averaging over the two categories weighted by their posterior probabilities, as computed by the ideal observer (for more details, see Materials and methods). This expresses a trade-off between two terms. The first term encourages the selection of locations, <inline-formula><mml:math id="inf15"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, where we have the most overall uncertainty about the pixel value, <inline-formula><mml:math id="inf16"><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, while the second term prefers locations for which our expected pixel value for each category is highly certain.</p><p><xref ref-type="fig" rid="fig4">Figure 4A</xref> shows a sequence of fixations on a representative trial. On each fixation, the BAS score is computed for all possible positions (grayscale map) based on all previous fixation locations (green dots) and the pixel values revealed there. While the BAS algorithm would choose the position with the highest BAS score as the next fixation location (blue crosses), the participant might choose a different, suboptimal, fixation location (yellow circles). Nevertheless, the informativeness of most of the participant’s fixation locations were very high as expressed by their information-percentile values (the percentage of putative fixation locations with lower BAS scores than the one chosen by the participant).<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.12215.012</object-id><label>Figure 4.</label><caption><title>Example trial of the Bayesian active sensor (BAS) and its maximum entropy variant.</title><p>(<bold>A</bold>) The operation of BAS in a representative trial for saccades 1–8 and 14 (underlying image shown top left). For each fixation (left, panels), BAS computes a score across the image (gray scale, <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). This indicates the expected informativeness of each putative fixation location based on its current belief about the image type, expressed as a posterior distribution (inset, lower left), which in turn is updated at each fixation by incorporating the new observation of the pixel value at that fixated location. Crosses show the fixation locations with maximal score for each saccade, green dots show past fixation locations chosen by the participant and yellow circle shows current fixation location. Percentage values (bottom right) show their information percentile values (the percentage of putative fixation locations with lower BAS scores than the one chosen by the participant). Histogram on the right shows distribution of percentile values across all participants, trials and fixations. (<bold>B</bold>) Predictions of the maximum entropy variant (the first term in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) as in (<bold>A</bold>). For saccades 1–3, the fixation locations with maximal score (crosses) are not shown because the maxima comprise a continuous region near the edge of the image instead of discrete points. Note that entropy can be maximal further (eg. fixation 4) or nearer the edges of the image (eg. fixation 1), depending on the tradeoff between the two additive components defining it: the BAS score, which tends to be higher near revealing locations (panel A), and uncertainty due to the stochasticity of the stimulus and perception noise, which tends to be greater away from revealing locations. <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> shows two illustrative examples for this trade-off.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.012">http://dx.doi.org/10.7554/eLife.12215.012</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig4-v3"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.12215.013</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Trade-off between the two components making up total uncertainty underlying the maximum-entropy algorithm.</title><p>Top row shows entropy (in bits) of the predictive distributions, <inline-formula><mml:math id="inf17"><mml:mi mathvariant="normal">H</mml:mi><mml:mo>[</mml:mo><mml:mi mathvariant="normal">z</mml:mi><mml:mo>*</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">D</mml:mi><mml:mo>]</mml:mo></mml:math></inline-formula>, for a grid of locations after one observation. Bottom row shows the corresponding variance decomposition (cf. <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), unexplained (blue) corresponds to “noise” entropy, <inline-formula><mml:math id="inf18"><mml:msub><mml:mfenced close="&gt;" open="&lt;"><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mo>[</mml:mo><mml:mi>z</mml:mi><mml:mo>*</mml:mo><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">D</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mfenced><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>, explained (red) correspond to the BAS score, Score<inline-formula><mml:math id="inf19"><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>*</mml:mo><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, and total (black) coresponds to total uncertainty, <inline-formula><mml:math id="inf20"><mml:mi mathvariant="normal">H</mml:mi><mml:mo>[</mml:mo><mml:mi>z</mml:mi><mml:mo>*</mml:mo><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">D</mml:mi><mml:mo>]</mml:mo></mml:math></inline-formula>. The widths of the gray regions correspond to the three length scales with which the three image types are constructed.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.013">http://dx.doi.org/10.7554/eLife.12215.013</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig4-figsupp1-v3"/></fig></fig-group></p><p>We simulated eye movement patterns derived by BAS for the same images shown to our participants. In order to take into account basic biological constraints on the accuracy of eye movements, we included saccadic variability and bias in the model based on measurements made independently in a group of participants which took into account both the standard deviation and bias of saccades both along and orthogonal (standard deviation only) to the desired saccade direction as a function of desired amplitude (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). The predicted (mean-corrected) pattern of eye movements closely matched those observed (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, last two rows): they were positively correlated with participants’ eye movements for the same image type (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, right, orange bars; p<inline-formula><mml:math id="inf21"><mml:mo>&lt;</mml:mo></mml:math></inline-formula>0.001 in all cases), but negatively correlated with those for different image types (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, right, purple bars; p&lt;0.001 in all cases). These differences increased as a function of revealing number (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, right, curves). Moreover, when we split participants’ trials into those in which they made a correct or incorrect decision, the pattern of eye movements derived from the correct trials correlated better with the BAS pattern than that derived from incorrect trials (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>, <inline-formula><mml:math id="inf22"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>correct</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.74</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>incorrect</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.20</mml:mn></mml:mrow></mml:math></inline-formula>, p&lt;0.001 for the average participant), further suggesting that following a BAS-like strategy was beneficial for performance.</p><p>For comparison, we also analyzed how well participants’ fixations could be accounted for by a strategy using a variant of the score that only included the first term in Equation 1 and thus selected locations with maximal entropy rather than maximal information gain. We found that this strategy provided a substantially poorer fit to our eye movement data than the full BAS algorithm, as measured by the distribution of the scores corresponding to actual fixation locations (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) and the anti-correlations between predicted and actual revealing maps at 25 revealings (<inline-formula><mml:math id="inf24"><mml:mi>ρ</mml:mi></mml:math></inline-formula> = <inline-formula><mml:math id="inf25"><mml:mo>-</mml:mo></mml:math></inline-formula>0.62, <inline-formula><mml:math id="inf26"><mml:mo>-</mml:mo></mml:math></inline-formula>0.51, <inline-formula><mml:math id="inf27"><mml:mo>-</mml:mo></mml:math></inline-formula>0.43; all p&lt;0.001).</p></sec><sec id="s2-4"><title>Fixation informativeness</title><p>In order to obtain a fixation-by-fixation measure of the informativeness of participants’ individual eye movements, we used the ideal observer model to quantify the amount of information accumulated about image category over subsequent revealings in a trial for different revealing strategies (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). This information-based measure is more robust than directly measuring distances between optimal and actual scan paths because multiple locations are often (nearly) equally informative when planning the next saccade. For example, in the trial shown in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, the BAS score map was clearly multimodal for several fixations, and some of the participant’s fixation locations were indeed distant from the corresponding locations that BAS would have chosen, yet their informativeness was generally very high. Therefore, categorization performance is better understood in terms of information measures on the revealed locations rather than by the geometry of individual scan paths, which has traditionally been used in previous studies (<xref ref-type="bibr" rid="bib29">Najemnik and Geisler, 2005</xref>; <xref ref-type="bibr" rid="bib37">Renninger et al., 2007</xref>).<fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.12215.014</object-id><label>Figure 5.</label><caption><title>Information gain as a function of revealing number for different strategies.</title><p>(<bold>A</bold>) Cumulative information gain of an ideal observer (matched to participants’ prior bias and perceptual noise) with different revealing strategies (black, green, and blue) and participants’ own revealings (red). Data are represented as mean <inline-formula><mml:math id="inf28"> <mml:mo>± </mml:mo></mml:math></inline-formula> SEM across trials. <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> shows a measure of efficiency extracted from these information curves across sessions. (<bold>B</bold>) Information gains for three heuristic strategies (See text for details, and Materials and methods): posterior-independent &amp; order-dependent fixations (orange), posterior-dependent &amp; order-independent fixations (purple), and posterior- &amp; order-dependent fixations (brown). The information gain curves for the three heuristics overlap in all cases. Participants’ active revealings (red lines, as in A) were 1.81 (95% CI, 1.68–1.94), 1.85 (95% CI, 1.72–1.99), and 1.92 (95% CI, 1.74–2.04) times more efficient in gathering information than these heuristics, respectively. Data are represented as mean <inline-formula><mml:math id="inf29"> <mml:mo>± </mml:mo></mml:math></inline-formula> SEM across trials.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.014">http://dx.doi.org/10.7554/eLife.12215.014</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig5-v3"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.12215.015</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Relative efficiency across free-scan sessions.</title><p>Relative efficiency computed as the ratio of the scale parameter <italic>a<sub>s</sub></italic> (see Materials and methods) of each session to that of the last session. Circles and lines show mean and SD obtained by bootstrapping trials within a session 50 times. The revealings on day 2-3 (free-scan sessions) were 0.90–1.04 (across participants) times as efficient compared to day 1 revealings (free-scan familiarization sessions). This suggests that participants were already choosing revealing locations on day 1 with near-asymptotic efficiency.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.015">http://dx.doi.org/10.7554/eLife.12215.015</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-fig5-figsupp1-v3"/></fig></fig-group></p><p>We compared the information efficiency of different revealing strategies by measuring the relative number of revealings required by them to gain the same amount of information. Participants’ active revealings (<xref ref-type="fig" rid="fig5">Figure 5A–B</xref>, red lines) were 2.93 (95% confidence interval [CI], 2.60–3.32) times more efficient than random revealings (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, blue lines). As a more stringent control than a random strategy, we also simulated eye movement patterns for three heuristic strategies that reproduced different aspects of the statistics of participants’ actual eye movement patterns but lacked the full closed-loop Bayesian interaction between belief updating and eye-movement selection (see Materials and methods for details). We first used a feed-forward strategy, which retained order-dependence, i.e. the way the statistics of revealing locations chosen by the participants depended on revealing number, but ignored participants’ inferences about the category (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, orange). The second heuristic took into account the participant’s belief about the underlying image, but not the revealing number (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, purple). The third heuristic was a partial closed-loop strategy, that thus respected both the belief- and order-dependence of revealings, but not the details of previous revealings (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, brown line). Notably, this strategy would be optimal for simpler stimuli and tasks, such as visual search for a target, but not for our spatially correlated stimuli and task requiring information integration across multiple locations. Participants’ revealings were 1.81–1.92 times more efficient than these three heuristic strategies.</p><p>However, participants’ active revealings were less efficient by a factor of 2.48 (95% CI, 2.33–2.62) than the information provided by revealings generated by the BAS algorithm with ‘idealized’ parameters: minimal perception noise (2–3 times lower than our participants’; see Materials and methods) and no prior biases or saccadic inaccuracies (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, black lines). Indeed, this efficiency in information gathering was also reflected in our participants’ performance when they viewed revealings generated by this ideal BAS (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, black points). In this condition, participants only rescanned the revealed locations for a short amount of time after the final revealing (average of 4.4 s), less than in the active and passive random conditions, thus their increased efficiency could not be attributed to longer rescanning times. The discrepancies between participants’ and BAS’s revealings may be caused by participants employing an inefficient strategy to select their fixation locations, or, alternatively, they may be due to more trivial factors upstream or downstream of the process responsible for selecting the next fixation, such as noise and variability in perception or execution, respectively. Importantly, when we computed the information gain provided by BAS when operating with participants’ prior biases, perceptual noise and the typical saccadic inaccuracies as described above, the discrepancy between the informativeness of BAS-generated revealings compared to that of participants’ revealings was markedly reduced (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, green lines, BAS/free-scan efficiency = 1.45, 95% CI, 1.37–1.53). This suggests that the central component of choosing where to fixate was around 70% efficient in our participants, and a large component of suboptimality arose due to other processes. To examine the role of learning in participants’ active sensing strategy, we computed the relative efficiency of each of the free-scan sessions of our experiment, spanning multiple days, compared to the final session (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We found that efficiency remained stable over the whole course of the experiment, suggesting there was minimal, if any, learning required in the free-scan task.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our results show that humans employ an active sensing strategy in deciding where to look in a high-level pattern categorization task. In our task, participants’ patterns of eye movements were well predicted by a Bayes-optimal algorithm seeking to maximize information gain relevant to the task with each individual eye movement. This Bayes optimal strategy involved finding the location in the scene which when fixated was most likely to lead to the greatest reduction in categorization error, rather than simply the location associated with the most uncertainty about pixel value.</p><sec id="s3-1"><title>The efficiency of active sensing in human vision</title><p>Although our participants performed better when revealings were chosen by the BAS algorithm than with their own scan paths, our results suggest that this suboptimality in participants’ performance was to a large part due to prior biases, perception noise, and saccadic inaccuracies constraining the selection of fixation locations rather than an inefficient active sensing strategy. In particular, our participants’ eye movements were substantially more efficient than heuristic strategies that only employed a subset of the elements of a fully closed-loop active sensing strategy, and were about 70% as efficient as the optimal active sensor that operated with the participants’ own prior bias, perception noise, and saccadic inaccuracies. Importantly, this estimate does not conflate the inference process with the selection process in that even if the participant’s inference is biased or inconsistent, we can measure, given those beliefs, the extent to which they select fixation locations optimally. The 30% inefficiency we found may be due to unmodeled constraints in human eye movement strategies, such as the biases for cardinal directions or fixating the centre of an image that were apparent in our data (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) and that may also be beneficial in natural environments (<xref ref-type="bibr" rid="bib43">Tatler and Vincent, 2008</xref>; <xref ref-type="bibr" rid="bib44">2009</xref>). Such suboptimalities are conceptually different from those that we factored out using the specific biases and noise included in the BAS model. The latter are suboptimalities that arise in the execution of a planned saccade (as in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>), while the former could be suboptimalities of the planning itself. Therefore, as we were interested in the degree of (sub)optimality of the planning component, we chose not to factor out potential biases for cardinal directions or locations. Should such suboptimalities turn out to be part of the execution process, then our estimate of 70% would become a lower bound on the efficiency of the planning process.</p><p>Our results contrast with recent studies suggesting that the pattern of eye movements do not follow an active sensing strategy. In one study, using a simple object localization task, it was found that the choice of fixation locations was close to random despite obvious learning of the underlying stimulus statistics (<xref ref-type="bibr" rid="bib12">Holm et al., 2012</xref>). In another study, using a simplified visual search task, participants’ eye movements were virtually unaffected by the configuration of the visual stimuli presented in the trial (<xref ref-type="bibr" rid="bib28">Morvan and Maloney, 2012</xref>). In contrast, we found that participants used both their prior knowledge of stimulus statistics as well as evidence accumulated about the current visual scene to guide their eye movements. We speculate that better performance in our case may be due to our more naturalistic task, which involves the extraction of abstract latent features, revealing more typical processing of the sensorimotor system.</p></sec><sec id="s3-2"><title>Relevance for natural vision</title><p>Although our task was designed to emulate many features of natural vision, there remain several differences. First, our gaze-contingent display involved exposing small patches of the image at each fixation whereas in natural vision information is available from the full visual field, albeit with decreasing acuity away from the fovea. If participants’ selection process was aware of and actively optimized for the changed field of vision in our task, the resulting eye movement patterns could be different from those in natural vision. Nevertheless, the basic summary statistics of (macro-)saccades in our task (average size and inter-saccadic intervals) were similar to those found in natural vision (<xref ref-type="bibr" rid="bib11">Hayhoe et al., 2003</xref>; <xref ref-type="bibr" rid="bib22">Land and Tatler, 2009</xref>), and there was a lack of adaptation to the task over the course of several days of free-scan sessions (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). These seem to suggest, at least indirectly, that participants did not depart dramatically from their natural eye movement strategies.</p><p>Second, our task focuses on the voluntary component of eye movements, that is the scan paths of fixations, rather than the more involuntary processes of micro-saccades and drift (<xref ref-type="bibr" rid="bib38">Rolfs, 2009</xref>; <xref ref-type="bibr" rid="bib19">Ko et al., 2010</xref>; <xref ref-type="bibr" rid="bib40">Rucci et al., 2007</xref>; <xref ref-type="bibr" rid="bib35">Poletti et al., 2013</xref>; <xref ref-type="bibr" rid="bib21">Kuang et al., 2012</xref>), which did not trigger new revealings. Importantly, in our task these involuntary processes are likely to be used for extracting information within each revealed patch, whereas high-level categorization required macro-saccades. This is because these involuntary processes cover an area with a standard deviation (SD) on the order of 0.22° (over the course of each fixation; <xref ref-type="bibr" rid="bib38">Rolfs, 2009</xref>), which is similar to our Gaussian apertures (SD of 0.18°), whereas the smallest length scale of our stimuli (0.91°) is 4 times larger. However, it is possible that in natural scenes with finer structure, micro-saccades and drift may contribute more to abstract feature extraction.</p><p>Third, our stimuli set had strictly stationary statistics. This is an approximation to natural image statistics that are often assumed to be spatially stationary on average (eg. <xref ref-type="bibr" rid="bib6">Field, 1987</xref>) but usually include local non-stationarities (eg. different objects with different characteristic spatial frequencies). Nevertheless, our stimuli were more naturalistic than many of the stimuli used in active sensing studies of visual search (eg. simple 1/f noise) while still allowing a rigorous control and measurement of the amount of high-level category information available at any potential revealing location which would not have been possible with natural scenes.</p><p>Finally, natural vision works under time constraints. Our task limited the number of revealings in each trial rather than the time, although the temporal aspects of the eye movements (inter-saccadic intervals) were similar to eye movements with natural scenes. We also showed in a control experiment that the additional time allowed for rescanning the revealed locations did not affect the initial scanning strategy.</p><p>A challenge for future studies will be to employ visual tasks that are more naturalistic in these respects while still retaining our ability to quantify the task-relevant information available to participants at any point in a trial. It may well be that participants are more efficient in such naturalistic settings than the 70% we found in our task.</p></sec><sec id="s3-3"><title>Relation to earlier work</title><p>Our approach makes several important contributions. First, by using a gaze-contingent display we were able to isolate the top-down strategies of eye movement selection, thereby complementing studies which examined the influence of low-level visual features and salience (<xref ref-type="bibr" rid="bib16">Itti and Koch, 2000</xref>; <xref ref-type="bibr" rid="bib47">Wismeijer and Gegenfurtner, 2012</xref>) on eye movement selection. In addition, our task focuses on integrating low-level visual information into an abstract category thus complementing studies that examine the effect of target value (<xref ref-type="bibr" rid="bib31">Navalpakkam et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Krajbich et al., 2010</xref>; <xref ref-type="bibr" rid="bib27">Markowitz et al., 2011</xref>; <xref ref-type="bibr" rid="bib41">Schutz et al., 2012</xref>) or more cognitive tasks that use eye movement (<xref ref-type="bibr" rid="bib32">Nelson and Cottrell, 2007</xref>) or button-pressing for information search (<xref ref-type="bibr" rid="bib3">Castro et al., 2009</xref>; <xref ref-type="bibr" rid="bib9">Gureckis and Markant, 2009</xref>; <xref ref-type="bibr" rid="bib2">Borji and Itti, 2013</xref>). Some of these studies used a similar Bayesian formalism to obtain the active learning strategies specific to their tasks and showed that humans perform active learning when given the opportunity to consciously deliberate from which location in a scene they should gather information next. In contrast, our work shows that high-efficiency active sensing is a natural strategy for eye movements that humans adopt without overt deliberation, as evidenced by the inter-saccadic intervals matching naturalistic tasks and the fact that our participants reached near-asymptotic efficiency already in the first session of our task.</p><p>Second, by using a pattern categorization task we could ensure that no single location was especially informative but that the task required an integration of information from multiple locations both to select the next eye movement and to solve the task, and specifically that a closed-loop strategy was necessary for solving the task efficiently. This is in contrast with studies that attempted to quantify the general informativeness of single locations in a scene (<xref ref-type="bibr" rid="bib8">Gosselin and Schyns, 2001</xref>), and showed that they are the target of fixations humans tend to choose in general (<xref ref-type="bibr" rid="bib34">Peterson and Eckstein, 2012</xref>; <xref ref-type="bibr" rid="bib46">Toscani et al., 2013</xref>; <xref ref-type="bibr" rid="bib4">Chukoskie et al., 2013</xref>), or visual search in which, by the nature of the task, the target location is fully informative by itself (<xref ref-type="bibr" rid="bib29">Najemnik and Geisler, 2005</xref>). As such, most previous studies have not addressed the important interplay between information gathering and fixation selection characteristic of naturalistic active sensing: that, in general, the most informative location is ever-changing, dependent on the history of fixations one has already performed.</p><p>Third, in contrast to active sensing for simple visual search (<xref ref-type="bibr" rid="bib29">Najemnik and Geisler, 2005</xref>; <xref ref-type="bibr" rid="bib31">Navalpakkam et al., 2010</xref>; <xref ref-type="bibr" rid="bib28">Morvan and Maloney, 2012</xref>), our formalism extends the range of active sensing to tasks which have arbitrary, not necessarily spatial, latent features (such as categories). In particular, it provides the first fixation-by-fixation analysis of information gathering under active eye movements by carefully matching our observer model to participants’ performance. As a result, for the first time, we were able to dissociate the contributions of the eye-movement selection process from those of perceptual, motor, and decision processes, identify predominant sources of apparent sub-optimality in active sensing, and quantify the efficiency of choosing each individual fixation throughout scanning a whole scene. Taken together, these features make our approach amenable to multiple tasks in different perceptual domains (<xref ref-type="bibr" rid="bib18">Kleinfeld et al., 2006</xref>), as well as high-level cognitive tasks such as estimating the age or socio-economic status of people in a scene (<xref ref-type="bibr" rid="bib48">Yarbus, 1967</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Three naive participants (aged 25–35 years, none of them were authors or neuroscientists) took part in the experiment. All participants were neurologically healthy, had normal or corrected to normal vision and gave their informed consent before participating. The study was approved by the institutional ethics committee. Each experiment took approximately 12 hr across 6 days (2 hr per day). As this experiment was particularly laborious we focus on within-participant analysis.</p></sec><sec id="s4-2"><title>Experimental apparatus and setup</title><p>Participants sat 42 cm in front of a 17” Sony Multiscan G200 FD Trinitron CRT monitor (32-bit color, 1024x768 resolution, 100 Hz refresh rate). An EyeLink 1000 eye tracker was used to track the participant’s right eye position at 1000 Hz. A chin and forehead rest was used to stabilize the head.</p></sec><sec id="s4-3"><title>Stimuli</title><p>Stimuli were generated such that the value of a pixel, <inline-formula><mml:math id="inf30"><mml:mi>z</mml:mi></mml:math></inline-formula>, depended on its two-dimensional location, <inline-formula><mml:math id="inf31"><mml:mi>x</mml:mi></mml:math></inline-formula>, through a function <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM9">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> sampled from a two-dimensional Gaussian process (<xref ref-type="bibr" rid="bib36">Rasmussen and Williams, 2006</xref>) with zero mean and covariance function <inline-formula><mml:math id="inf33"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo id="XM10">⋅</mml:mo><mml:mo>,</mml:mo><mml:mo id="XM11">⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The covariance function was squared-exponential and it was parameterized by <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub id="XM14"><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM15"><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> setting the image pattern’s horizontal and vertical correlation length scales, with the variance set to unity, such that the covariance of the pixel values at two positions, <inline-formula><mml:math id="inf35"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf36"><mml:msup><mml:mi>x</mml:mi><mml:mo>'</mml:mo></mml:msup></mml:math></inline-formula> (each two-dimensional), was:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:msub><mml:mi>K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>x</mml:mi><mml:mo>'</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>'</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mfenced close="]" open="["><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msubsup><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:msup><mml:mo>⁢</mml:mo><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>'</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></disp-formula></p><p>For the patchy (<inline-formula><mml:math id="inf37"><mml:mi>PA</mml:mi></mml:math></inline-formula>), stripy horizontal (<inline-formula><mml:math id="inf38"><mml:mi>SH</mml:mi></mml:math></inline-formula>), and stripy vertical (<inline-formula><mml:math id="inf39"><mml:mi>SV</mml:mi></mml:math></inline-formula>) pattern types the hyperparameters were <inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>PA</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msup id="XM26"><mml:mn>1.39</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup id="XM27"><mml:mn>1.39</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf41"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>SH</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msup id="XM30"><mml:mn>4.63</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup id="XM31"><mml:mn>0.91</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>SV</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msup id="XM34"><mml:mn>0.91</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup id="XM35"><mml:mn>4.63</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. Function values (<inline-formula><mml:math id="inf43"><mml:mi>z</mml:mi></mml:math></inline-formula>) in the range <inline-formula><mml:math id="inf44"><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf45"><mml:mn>4</mml:mn></mml:math></inline-formula> were mapped to image pixel colors so that the extremes corresponded to pure red (RGB value [1, 0, 0]) and blue ([0, 0, 1]), with intermediate values being linearly interpolated in RGB space. Functions which had values outside [<inline-formula><mml:math id="inf46"><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf47"><mml:mn>4</mml:mn></mml:math></inline-formula>] were discarded and a new function was generated. The images were sampled over a grid of 77<inline-formula><mml:math id="inf48"><mml:mo>×</mml:mo></mml:math></inline-formula>77 locations subtending <inline-formula><mml:math id="inf49"><mml:msup><mml:mn>27.8</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> at the eye in the horizontal and vertical dimensions and then supersampled, using 2D splines, up to a resolution of 770<inline-formula><mml:math id="inf50"><mml:mo>×</mml:mo></mml:math></inline-formula>770 pixels (this allowed images to be generated rapidly without compromising visual appearance). On each trial, a pattern category <inline-formula><mml:math id="inf51"><mml:mi>c</mml:mi></mml:math></inline-formula>, that was patchy (<inline-formula><mml:math id="inf52"><mml:mtext>P</mml:mtext></mml:math></inline-formula>) or stripy (<inline-formula><mml:math id="inf53"><mml:mtext>S</mml:mtext></mml:math></inline-formula>), was chosen with equal probability and if the stripy category was chosen then a horizontal or vertical pattern type was chosen with equal probability. Having two different types within the stripy pattern category ensured that the optimal scan path depended on the image (otherwise, always scanning in one direction where the length scales were different would be optimal).</p></sec><sec id="s4-4"><title>Task</title><p>The task on each trial was for participants to determine whether a pattern displayed on the monitor was patchy or stripy (irrespective of whether it was vertical or horizontal) under different experimental conditions. The experiment consisted of four conditions: training, free-scan familiarization, free-scan, and passive revealing.</p><sec id="s4-4-1"><title>Training (8 sessions <inline-formula><mml:math id="inf54"><mml:mo>×</mml:mo></mml:math></inline-formula> 40 trials)</title><p>Participants triggered the start of each trial by fixating (within 1.5° for 500 ms) a cross centered on the screen, at which point the cross disappeared. An image was displayed centered on the location of the fixation cross, and participants had to decide whether the image was patchy or stripy. They were allowed to scan the image freely for up to 10 s and make their decision by fixating (within 2.8° for 800 ms) on one of the two choice letters, P or S, which were displayed to the left and right of the displayed image, respectively. Participants received audio feedback as to the correctness of their choice. Twenty images from each category were presented in a randomized order. The training sessions ensured that participants could learn the statistics of the image patterns. Categorization performance was perfect in training sessions for all participants.</p></sec><sec id="s4-4-2"><title>Free-scan familiarization (5 sessions <inline-formula><mml:math id="inf55"><mml:mo>×</mml:mo></mml:math></inline-formula> 40 trials)</title><p>Each trial started with participants fixating a center cross. A randomly generated image from one of the categories was then displayed but initially completely obscured by a black mask. Participants could freely scan the display, and wherever they fixated, the underlying image was revealed by unmasking a small aperture at the fixation location. This was achieved by revealing an isotropic Gaussian region with standard deviation 0.18° at the fixation location, with the values of the Gaussian used to interpolate linearly between complete transparency (alpha=0) at the maximum of the Gaussian and black (alpha=1) where the value of the Gaussian is 0. A new fixation was detected, and hence a new revealing triggered, when the following three criteria were met: 1) a saccade had occurred since the last fixation as determined by eye speed greater than 59° s<inline-formula><mml:math id="inf56"><mml:msup><mml:mi/><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>; 2) the displacement from the previous fixation was greater than 0.59°; and 3) the standard deviation of the eye position was less than 0.28° for the last 100 ms. These parameters were based on pilot experiments with the aim of making the revealings in the free-scan session feel natural so that each location viewed was revealed and spurious locations were not revealed. Participants were required to make 25 fixations before making their decision. After the 25 revealings, the category choices appeared (P vs. S) and participants had 60 s to choose a category with the revealed locations remaining on the display. Upon answering, participants were shown the full image with audio feedback. All participants achieved an average performance of 70% accuracy or higher.</p></sec><sec id="s4-4-3"><title>Free-scan (6 sessions <inline-formula><mml:math id="inf57"><mml:mo>×</mml:mo></mml:math></inline-formula> 100 trials)</title><p>Free-scan trials were exactly the same as free-scan familiarization trials except that the number of revealings across trials was chosen randomly on each trial from 5 to 25 in steps of 5 (balanced) and was a priori unknown to the participants. The choice letters, P and S, appeared after the given number of revealings and no new revealings occurred after this point. The unknown, random stopping number of revealings served to encourage participants to be greedy in their information seeking.</p></sec><sec id="s4-4-4"><title>Passive revealing (8 sessions <inline-formula><mml:math id="inf58"><mml:mo>×</mml:mo></mml:math></inline-formula> 100 trials)</title><p>This session was the same as the free-scan session except that the revealing locations were pre-determined by an algorithm independent of participant’s eye movements and sequentially appeared at intervals of 400 ms (which was about the average interval between consecutive fixations in the free scan experiment: 408 ms, which in turn was very similar to the inter-saccadic intervals measured under natural viewing conditions in everyday tasks; <xref ref-type="bibr" rid="bib11">Hayhoe et al., 2003</xref>; <xref ref-type="bibr" rid="bib22">Land and Tatler, 2009</xref>). Participants were instructed to follow the revealings as much as possible and were allowed to scan the scene after all revealings had appeared until they made their category decision. The algorithm followed one of three strategies randomly chosen:</p><list id="I2" list-type="order"><list-item><p>Random: revealing locations were drawn from a scene-centered isotropic Gaussian with standard deviation 9.27°. For comparison, participants’ revealing locations in the free-scan condition had an average location that was 0.05° from the center of the scene and had a standard deviation 4.50°. Revealing locations that fell outside the image were resampled;</p></list-item><list-item><p>Ideal BAS: revealing locations were generated by the BAS algorithm (see below). For establishing an upper bound on the informativeness of the optimal revealing locations, the algorithm was allowed to access the displayed, as opposed to perception noise-corrupted pixel values, and did not include prior biases and saccadic inaccuracies (see below).</p></list-item><list-item><p>Anti-BAS: revealing locations were generated by the BAS algorithm as above but as if it was observing a different image than the real one, which belonged to a wrong type. For example, if the real image belonged to type <inline-formula><mml:math id="inf59"><mml:mi>SH</mml:mi></mml:math></inline-formula>, the revealing locations were generated based on an image from type <inline-formula><mml:math id="inf60"><mml:mi>PA</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf61"><mml:mi>SV</mml:mi></mml:math></inline-formula> (randomly chosen).</p></list-item></list><p>We mixed the BAS and anti-BAS trials to ensure participants could not use the pattern of revealing locations (independent of pixel values) to infer the category of the underlying image. The experiment included 8 passive revealing sessions with a total of 200, 200, and 400 trials from strategy 1, 2, and 3, respectively. The trials were first randomly mixed then divided into 8 sessions.</p><p>The eye tracker was calibrated before each session (25-point calibration for the free-scan condition and 9-point calibration for the passive-revealing conditions). Drift correction was performed at the start of each trial after fixation on the center cross was achieved. Re-calibration was performed whenever participants reported that the revealing locations did not match where they fixated, whenever they could not trigger the start of a trial, or make their category choice by fixating.</p><p>Participants ran using the following schedule: day one, 3 training sessions intermixed with 5 free-scan familiarization sessions; day two and three, 1 training session and 3 free-scan sessions each; day four to six, 1 training session and 2 or 3 passive revealing sessions each. All the free-scan sessions came before any passive revealing sessions so as to avoid influencing participants’ choice of eye movements by our choice of passive revealing strategies.</p><p>As we wished to compare the active strategy with passive revealing we allowed participants to rescan the revealed locations after the final revealing but before making a decision. This was critical as in the passive revealing conditions, although participants may detect and follow the revealings, because they are small and dispersed, they need to scan the scene to find and view them all. Therefore, the reason for allowing additional time after the final revealing was so as to make sure they had a chance to extract as much information as they liked from the revealed locations (locations and pixel values). In order to make the conditions directly comparable, we followed the same procedure in the active condition. Thus, allowing a rescanning period was the only way we could make a fair comparison across conditions using our gaze contingent design, which in turn allowed fine control over the information participants could obtain by eye movements.</p><p>Crucially, our key interest was in where participants chose to fixate during the initial revealing period and not in the perception model. During the active condition, all the selection happened prior to the rescanning phase and participants did not know how many saccades they would be allowed so they still needed to be efficient in choosing revealing locations even if they could rescan them later. Therefore, as we were analyzing the initial scanning, the final rescanning simply equalized the information that could be extracted from each revealing for all conditions but it was unlikely to influence the initial selection. Although, it is theoretically possible that participants adopted a different eye movement strategy knowing they could freely re-visit already revealed locations, the rescanning control (described below) suggests that this was not the case (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>To examine whether rescanning time had an effect on performance, we fit each participants choice accuracy as a logistic function (bounded between 0.5 and 1) of rescanning time. We allowed different shifts per condition (active, passive random, passive BAS) but the same slope parameter across conditions.</p></sec></sec><sec id="s4-5"><title>No-rescanning control</title><p>To directly examine whether the rescanning period allowed after the final revealing affected participants scanning strategy, we performed a control experiment with three additional naive participants. These participants performed the training, free-scan familiarization, and free-scan sessions as in the original experiment except that no rescanning after the final revealing was allowed. That is all revealings disappeared (i.e. display returned to a black screen) 350 ms after the saccade away from the final revealing and participants were required to indicate their choice.</p></sec><sec id="s4-6"><title>The ideal observer model of the task</title><p>The ideal observer maintains and continually (after each observation) updates a posterior distribution over categories, <inline-formula><mml:math id="inf62"><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtext>P,S</mml:mtext><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, given knowledge of the parameters defining each image type, <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub id="XM41"><mml:mi>θ</mml:mi><mml:mi>PA</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM42"><mml:mi>θ</mml:mi><mml:mi>SV</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM43"><mml:mi>θ</mml:mi><mml:mi>SH</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and data, <inline-formula><mml:math id="inf64"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mtext>, </mml:mtext><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, which is the set of perceived pixel values <inline-formula><mml:math id="inf65"><mml:mi mathvariant="bold">z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> at the <inline-formula><mml:math id="inf66"><mml:mi>L</mml:mi></mml:math></inline-formula> locations <inline-formula><mml:math id="inf67"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> revealed in the trial so far:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mfenced><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>S</mml:mtext><mml:mo>|</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mfenced><mml:mrow><mml:mi>D</mml:mi><mml:mo>|</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="script">𝒩</mml:mi><mml:mtext>⁢</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mtext>, </mml:mtext><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf68"><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>p</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is the variance of the participant’s (Gaussian) perceptual noise on the pixel value, and <inline-formula><mml:math id="inf69"><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msup><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>'</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is a matrix with element <inline-formula><mml:math id="inf70"><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM61">i</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM62">j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> being <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mo>'</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). Note that the length scales of the three pattern types as assumed by the observer, <inline-formula><mml:math id="inf72"><mml:mi>θ</mml:mi></mml:math></inline-formula>, need not necessarily be the same as those actually used to generate the images, and indeed we explore below variants of the model that differ in their assumptions about these length scales. For simplicity, the ideal observer model assumes a fixed extraction of information from each revealing and therefore does not include temporal factors such as fixation durations (i.e. perception noise in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> is constant).</p></sec><sec id="s4-7"><title>The Bayesian active sensor model of eye movements</title><p>The Bayesian active sensor (BAS) algorithm computes a score, the expected reduction in entropy of the distribution over categories as a function of a possible next revealing location, <inline-formula><mml:math id="inf73"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, and chooses the next fixation to be at the location with the highest score. The score is defined as<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mi>Score</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>H</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mtext>H</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf74"><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> is a possible (and as yet, unobserved) pixel value at <inline-formula><mml:math id="inf75"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf76"><mml:mtext>H</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mo>⋅</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> denotes entropy in bits. Using the insight that the BAS score formally expresses the mutual information between <inline-formula><mml:math id="inf77"><mml:mi>c</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> (for the given <inline-formula><mml:math id="inf79"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>), <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> can be rewritten in a different form that is computationally far more convenient, as it does not require expensive posterior updates for a continuum of imaginary data, <inline-formula><mml:math id="inf80"><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, to compute <inline-formula><mml:math id="inf81"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> for the second term (<xref ref-type="bibr" rid="bib13">Houlsby et al., 2011</xref>):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mi>Score</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>H</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mtext>H</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></disp-formula></p><p>This form (equivalent to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) is also more plausible psychologically as it is easily approximated by simple mental simulation for a few hypotheses sampled from the current posterior. Note that maximizing just the first term would be equivalent to “maximum entropy sampling” (<xref ref-type="bibr" rid="bib42">Sebastiani and Wynn, 2000</xref>) which is suboptimal in general, and in the context of our task would be similar to simple “inhibition of return” which does not account well for participants’ fixations (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The two distributions needed for evaluating <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> are the current posterior over categories, <inline-formula><mml:math id="inf82"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, and the predictive distribution of the pixel value at a location for a category, <inline-formula><mml:math id="inf83"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. (Note that the predictive distribution in the first term can also be computed using these two distributions: <inline-formula><mml:math id="inf84"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>.) The category posterior is given by <xref ref-type="disp-formula" rid="equ3 equ4 equ5">Equation 3-5</xref> and the category-specific predictive distribution is (<xref ref-type="bibr" rid="bib36">Rasmussen and Williams, 2006</xref>):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mfenced><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>PA</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>S</mml:mtext><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>SV</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>SV</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>SH</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>SV</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>SH</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>SV</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>SH</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>SH</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>with<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mi>ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">𝒩</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo rspace="4.2pt">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mtext>, </mml:mtext><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>p</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mtext mathvariant="bold">I</mml:mtext><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold">z</mml:mi><mml:mo>,</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>p</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mtext mathvariant="bold">I</mml:mtext><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>p</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>For all simulations, we computed the BAS score on a 110<inline-formula><mml:math id="inf85"><mml:mo>×</mml:mo></mml:math></inline-formula>110 grid of <inline-formula><mml:math id="inf86"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> that covered the image. The entropies of the predictive distributions (which are mixtures of Gaussians) in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> were approximated by their Jensen lower bound for efficiency (<xref ref-type="bibr" rid="bib15">Huber et al., 2008</xref>).</p><p>Importantly, at least in principle, the ideal observer and BAS algorithms can be generalised to more complex stimuli as long as their statistics are known and can be expressed as <inline-formula><mml:math id="inf87"><mml:mi>ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-8"><title>Saccadic variability and bias</title><p>Due to the gaze-contingent design of our experiment, most saccades were made towards locations without any visible target. Saccades are known to be variable and biased, and to incorporate this variability and bias into our model we measured saccadic variability and bias in an independent experiment with 6 participants (including two from the main experiment) under similar conditions to those in the free-scan session of the main experiment. Although in the main experiment participants chose the location they wished to saccade to, to reliably measure saccadic variability and bias we needed to know the target of the saccade but not display it. Therefore we first trained participants on the locations of two saccadic targets and that on each trial the color of a stimulus shown at the fixation point indicated to which of these they needed to saccade. The two targets were at equal eccentricity from the fixation point, one to the right and one above it. On each training trial, participants first fixated a central fixation cross. Then two isotropic Gaussian patches (SD 0.18° as in the main experiment) appeared, one at the fixation location and one at one of the two target locations. The color of the fixation patch determined where the eccentric target was, either to the right (red) or above (blue), and participants were informed that the relation between the fixation color and the target direction was fixed throughout the experiment. To ensure that participants paid attention to the target, they were asked to report the color of the eccentric target which could either be red or blue. Note that this task by itself did not necessarily require them to saccade to the target, to avoid overtraining on particular saccades, but ensured that they developed a strong association between the color of the central fixation stimulus and the location of the target on that trial, such that in the next phase we could instruct them to saccade to a particular target without showing it. A training session of 40 trials was performed for each target eccentricity tested (1.39, 2.78, 5.56 and 8.34° in this order) with each session followed by a test session of 100 trials with the same eccentricity.</p><p>In the test sessions, only the fixation patch appeared and its color determined which target the participant should make an eye movement to. As in the free-scan sessions, a patch was revealed wherever the participant fixated, and their task was to report the target’s color. However, as this first eye movement was often inaccurate (see below) it did not necessarily reveal the target which needed to be reported. Thus, to motivate participants by making the task feasible, they were allowed to make four additional eye movements, leading to more revealings, before reporting the color of the target. (Although they were allowed 5 revealings, they were instructed to be as accurate as possible with their first eye movement.) The entire image was then shown as feedback.</p><p>To estimate saccadic variability and bias we used only the first saccade after fixation. We removed trials in which participants did not accurately maintain fixation (drift of &gt;0.56°) or where they clearly mis-directed their saccade (saccade was closer to the non-cued target). We calculated the bias and standard deviation (robust estimate from the median absolute deviation) both along (tangential) and orthogonal (SD only) to the direction of the target. We fit the bias and SD as a linear function of target distance for both the tangential and orthogonal components for each participant (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). We averaged the model parameters across the participants and used these values to corrupt the desired saccade locations in the BAS simulations (unless otherwise noted).</p></sec><sec id="s4-9"><title>Model parameters and data fitting</title><p>To match empirical data collected from our participants, we included perception noise, decision noise, and potential prior biases in the ideal observer model.</p><list list-type="bullet"><list-item><p>To model perception noise, we added Gaussian noise (SD <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>p</mml:mtext></mml:msub></mml:math></inline-formula>) to the displayed pixel values to obtain z, which was either fit to individual participants’ categorization data (see below), or set at <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.17</mml:mn></mml:math></inline-formula> for computing the BAS score (in <xref ref-type="disp-formula" rid="equ5 equ10">Equations 5 and 10</xref>) when determining ideal BAS revealings in passive revealing sessions. The ideal observer model then received these noisy pixel values as input instead of the pixel values actually shown to the participants.</p></list-item></list><list list-type="bullet"><list-item><p>To incorporate decision noise, the category posterior of <xref ref-type="disp-formula" rid="equ3 equ4">Equation 3-4</xref> was transformed by a softmax function to obtain the probability of choosing the patchy category:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mover accent="true"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mo>|</mml:mo><mml:mi mathvariant="normal">D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>β</mml:mi><mml:mo> </mml:mo><mml:mo>⁢</mml:mo><mml:mi>LPR</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf90"><mml:mi>κ</mml:mi></mml:math></inline-formula> describes the stimulus-independent decision noise and can be interpreted as the lapse rate, <inline-formula><mml:math id="inf91"><mml:mi>β</mml:mi></mml:math></inline-formula> is the stimulus-dependent decision noise (larger values of <inline-formula><mml:math id="inf92"><mml:mi>β</mml:mi></mml:math></inline-formula> result in more deterministic behavior and lower values in more random behavior), and <inline-formula><mml:math id="inf93"><mml:mi>LPR</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>S</mml:mtext><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></inline-formula> is the log posterior ratio under the ideal observer model (<xref ref-type="disp-formula" rid="equ3 equ4">Equation 3–4</xref>).</p></list-item></list><list list-type="bullet"><list-item><p>To model prior biases, i.e. imperfect knowledge of the stimulus statistics, we considered three qualitatively different ways in which participants could misrepresent the length scales used to generate the stimuli (6 length scales for three types of stimulus and 2 directions):<list list-type="order"><list-item><p>no prior biases, i.e. the length scales used by the model were identical to those actually used to generate the stimuli;</p></list-item><list-item><p>a uniform scaling of all length scales relative to their true values, <inline-formula><mml:math id="inf94"><mml:mi>α</mml:mi></mml:math></inline-formula>;</p></list-item><list-item><p>a fixed offset of all length scales from their true values, <inline-formula><mml:math id="inf95"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula>;Thus, the last two models has a single parameter controlling the relation between the extent of misrepresentation.</p></list-item></list></p></list-item></list><p>For a systematic model comparison, we constructed a set of models which all included perception noise (<inline-formula><mml:math id="inf96"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>p</mml:mtext></mml:msub></mml:math></inline-formula>) and stimulus-dependent decision noise (<inline-formula><mml:math id="inf97"><mml:mi>β</mml:mi></mml:math></inline-formula>) and differed in whether they also included stimulus-independent decision noise (<inline-formula><mml:math id="inf98"><mml:mi>κ</mml:mi></mml:math></inline-formula>) and which of the three prior biases they had (none, <inline-formula><mml:math id="inf99"><mml:mi>α</mml:mi></mml:math></inline-formula>, or <inline-formula><mml:math id="inf100"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula>). This gave six models and for each we fit all the free parameters on the combined category choice data (from both the free-scan and passive revealing conditions) of each participant using maximum likelihood.</p><p>In order to fit the models to empirical data, we needed to take into account that the ideal observer was conditioned on the <italic>perceived</italic> pixel values, which were noisy versions of the pixel values actually displayed (corrupted by perception noise) and were thus unknown to us. Thus, we integrated out the unknown perceived pixel values in order to compute the actual choice probabilities predicted by the model (<xref ref-type="bibr" rid="bib14">Houlsby et al., 2013</xref>). This was approximated by a Monte Carlo integral, by simulating each trial 500 times, drawing random samples of the perceived pixel values given the displayed pixel values from the perceptual noise distribution for each revealing. In each simulated trial, we then computed the probability of a participant’s choice according to <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>, and finally averaged over simulations to obtain the expected probability of each response category in that trial. For optimizing the values of <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>p</mml:mtext></mml:msub></mml:math></inline-formula> and the parameters for prior bias, <inline-formula><mml:math id="inf102"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf103"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula>, we conducted a grid search with a resolution <inline-formula><mml:math id="inf104"><mml:mn>0.1</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf105"><mml:mn>0.1</mml:mn></mml:math></inline-formula>, and <inline-formula><mml:math id="inf106"><mml:msup><mml:mn>0.01</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula>, respectively. For each setting of these parameters, we optimized <inline-formula><mml:math id="inf107"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf108"><mml:mi>κ</mml:mi></mml:math></inline-formula> (when used) using gradient-based search so that all the parameters were jointly optimized. <xref ref-type="table" rid="tbl1">Table 1</xref> shows the best fit parameters. We used the Bayesian information criterion (computed across all participants) to compare the models by controlling for their differing number of free parameters and chose the best one (<xref ref-type="table" rid="tbl2">Table 2</xref>).</p><p>We used the BAS algorithm to predict the pattern of eye movements for each participant individually given the values of <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>p</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf110"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula> fitted to their categorization choices and the measured saccadic inaccuracies. This meant that once we fitted categorization choices, eye movements were predicted without any further tuning of parameters. Note that <inline-formula><mml:math id="inf111"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf112"><mml:mi>κ</mml:mi></mml:math></inline-formula> affected only choice probabilities in the categorization decision process, not the eye-movement selection process described by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>.</p></sec><sec id="s4-10"><title>Revealing densities</title><p>As the statistics of stimuli were translationally invariant, the critical features of the optimal solution was the relative locations of the revealing to each other. Therefore, for each trial we first shifted all the revealing locations in that trial equally so that their mean (centre of mass) was located at the centre of the image, thereby removing the variability caused by which part of the image the participants explored in that trial, which was irrelevant to optimality. We then computed the mean density maps by assigning the revealing locations to centers of a 770<inline-formula><mml:math id="inf113"><mml:mo>×</mml:mo></mml:math></inline-formula>770 grid of bins (the resolution of the images), normalizing the counts, and smoothing the distribution with an isotropic Gaussian filter (std. of 20 bins, ie. <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:msup><mml:mn>0.73</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>). For a balanced comparison, we computed densities as if each trial had 25 revealings and the three underlying patterns were chosen with equal probability. To achieve this we multiplied the count for the n<sup>th</sup> revealings by both the relative frequency of first revealings to n<sup>th</sup> revealings and by the inverse of the frequency of the image type. For each participant, we computed the mean density map across all trials (<xref ref-type="fig" rid="fig3">Figure 3A</xref> first column) and the mean-corrected density map for each underlying image type (<xref ref-type="fig" rid="fig3">Figure 3A</xref> last three columns) by removing the mean from those averaged for each image type. The density maps for BAS were generated with simulations that used the same trials (image and number of revealings) that the participants performed. We repeated the simulation of each trial 10 times to obtain a reliable Monte Carlo estimate of the BAS revealing density maps marginalized over the unknown perceived pixel values.</p></sec><sec id="s4-11"><title>Correlation analysis</title><p>To examine whether the participants’ pattern of eye movements depended on the underlying image, and hence what they saw at the revealing locations, we compared correlations between mean-corrected density maps within and across image types for each participant. In computing the correlation as a function of revealing number, we kept the number of samples used to construct the maps constant. This number of samples was chosen to be the maximum number that still allowed the image type and revealing number to be sampled with equal probability without replacement. To increase the sample size for all revealings we only examined revealings from 5 onwards. For the bar plots, we used about 3 times the number of samples, but weighted each revealing so that the maps were still effectively constructed with an equal number of samples from each revealing number.</p><p>To obtain statistics of the correlations, we split the revealing locations for an image type randomly into two data sets of equal size. This produced 6 different data sets (two for each image type). For within-type correlations, we computed the three correlations (using a Gaussian smoothing kernel with <inline-formula><mml:math id="inf115"><mml:msup><mml:mn>1.5</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> SD) between the mean-corrected maps of the same image types, and averaged these three correlations across the three image types to obtain a single within-type correlation value. Similarly, for across-type correlations, we computed the six correlations between the mean-corrected maps of different image types (across the two different halves of the split), and averaged these six correlations to obtain a single across-type correlation value. We repeated this procedure 1000 times, using a different random split of the data each time, to obtain the means, SD, and 95% confidence intervals (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p>We also performed the same analysis on the three participants’ pooled data, treating the ensemble as data from an 'average' participant. Here the data sets used to calculate the density maps were thus three times larger than those for the individuals. These results together are shown as <italic>participant-self</italic> correlations (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left). We applied the same approach to compute <italic>participant-BAS</italic> correlations. For this, participant- as well as BAS-generated revealing location data were each randomly split in half (as above), and correlations were always computed between a participant- and a BAS-generated data set (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, right).</p><p>The correlation between the eye movement patterns derived from correct / incorrect trials and that from BAS was calculated in the same way described above, but only for the 'average' participant as the number of incorrect trials was limited. The p-value reported is the fraction of bootstrapped samples that satisfied the condition <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>correct</mml:mtext></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>incorrect</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-12"><title>Information gain and efficiency</title><p>According to the ideal observer, at any point in a trial, the information gain associated with the set of revealings made thus far is defined as <inline-formula><mml:math id="inf117"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mtext>H</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>. As described above (Model parameters and data fitting), since we only knew the displayed but not the perceived pixel values, we used Monte Carlo integration to marginalize out the unknown perceived pixel values. To do this, we simulated each experiment 200 times using the parameters of the best fit model, drawing new samples of perceived values for the chosen revealing locations given the displayed pixel values, and then averaged the information gains across runs and trials. <xref ref-type="fig" rid="fig5">Figure 5A</xref> shows the average information gain as a function of revealing number across the 200 simulations with the average across-trial SEM for each revealing strategy. To characterize the efficiency of each strategy, <inline-formula><mml:math id="inf118"><mml:mi>s</mml:mi></mml:math></inline-formula>, we fit the information vs. revealing number curves from each simulation obtained with the above analysis using a cumulative Weibull distribution: <inline-formula><mml:math id="inf119"><mml:msub><mml:mi>I</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mfenced><mml:mi>n</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>exp</mml:mi><mml:mfenced><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mfenced><mml:mfrac bevelled="true"><mml:mi>n</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac></mml:mfenced><mml:mi>b</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula>, where <inline-formula><mml:math id="inf120"><mml:mi>n</mml:mi></mml:math></inline-formula> is the revealing number, <inline-formula><mml:math id="inf121"><mml:mi>b</mml:mi></mml:math></inline-formula> is a shape parameter shared across strategies (free-scan, passive random, passive ideal BAS, and simulated BAS), and <inline-formula><mml:math id="inf122"><mml:msub><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is a strategy-specific scale parameter, which captures the overall efficiency of the strategy. As a relative measure of efficiency for comparing any two strategies, we computed the ratio of their <inline-formula><mml:math id="inf123"><mml:msub><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>, and obtained 95% confidence intervals by using the 200 simulations as bootstrap samples.</p></sec><sec id="s4-13"><title>Heuristics</title><p>To address whether a heuristic algorithm could account for participants’ eye movement patterns, we computed the information gains achieved using several heuristics.</p><list list-type="order"><list-item><p><bold>Posterior-independent &amp; order-dependent fixations</bold> (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, orange). For each participant, the fixation location on the <italic>i</italic><sup>th</sup> revealing was obtained by sampling (with replacement) from fixation locations pooled across their <italic>i</italic><sup>th</sup> revealing of all free-scan trials, regardless of the underlying image pattern and the ensuing posterior. This feed-forward strategy respects the participant’s average order-dependent fixation map, but is otherwise based on a set of pre-determined fixation locations rather than on what was observed about the actual scene.</p></list-item><list-item><p><bold>Posterior-dependent &amp; order-independent fixations</bold> (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, purple). For each participant, the fixation location on the <italic>i</italic><sup>th</sup> revealing was obtained by first computing the likelihood <inline-formula><mml:math id="inf124"><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mfenced><mml:mrow><mml:mi>D</mml:mi><mml:mo>|</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, where <inline-formula><mml:math id="inf125"><mml:mi>D</mml:mi></mml:math></inline-formula> included the <italic>i</italic> revealing locations and pixel values observed up to this revealing, finding the type that had the maximum posterior probability, and then sampling from the fixation locations pooled across trials with that image type as the underlying image pattern. This strategy uses the observations only indirectly, through the posterior, but does not otherwise take into account previous fixation locations and the corresponding pixel values for evaluating the informativeness of potential new revealing locations.</p></list-item><list-item><p><bold>Posterior- &amp; order-dependent fixations</bold> (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, brown). This combined order-dependence, as in the 1st heuristic, with posterior-dependence, as in the 2nd, but still did not take into account previous fixation locations and the corresponding pixel values. While this strategy would be optimal for simpler stimuli, in which pixels are independent for each image category, or simpler tasks, such as visual search, in which knowledge of the task-relevant posterior (eg. target location) is sufficient for optimal action, it is suboptimal with the kind of naturalistic stimuli and task we used.</p></list-item></list></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank C Rothkopf for useful comments on the manuscript, N Houlsby and F Huszár for their contributions to the theory underlying BAS, and J Ingram for technical support. This work was supported by the Wellcome Trust (SC-HY, ML, DMW), the Human Frontier Science Program (DMW), and the Royal Society Noreen Murray Professorship in Neurobiology (to DMW).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>SC-HY, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>ML, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>DMW, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was approved by the Cambridge Psychology Research Ethics Committee. All participants gave written informed consent prior to the experiment.</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ballard</surname><given-names>DH</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name><name><surname>Pelz</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Memory representations in natural tasks</article-title><source>Journal of Cognitive Neuroscience</source><volume>7</volume><fpage>66</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1162/jocn.1995.7.1.66</pub-id><pub-id pub-id-type="pmid">23961754</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Borji</surname><given-names>A</given-names></name><name><surname>Itti</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>Bayesian optimization explains human active search</chapter-title><person-group person-group-type="editor"><name><surname>Burges</surname> <given-names>C</given-names></name><name><surname>Bottou</surname> <given-names>L</given-names></name><name><surname>Welling</surname> <given-names>M</given-names></name><name><surname>Ghahramani</surname> <given-names>Z</given-names></name><name><surname>Weinberger</surname> <given-names>K</given-names></name></person-group><source>Advances in Neural Information Processing Systems 26</source><publisher-loc>NIPS</publisher-loc><fpage>55</fpage><lpage>63</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Castro</surname><given-names>RM</given-names></name><name><surname>Kalish</surname><given-names>C</given-names></name><name><surname>Nowak</surname><given-names>R</given-names></name><name><surname>Qian</surname><given-names>R</given-names></name><name><surname>Rogers</surname><given-names>T</given-names></name><name><surname>Zhu</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2009">2009</year><chapter-title>Human active learning</chapter-title><person-group person-group-type="editor"><name><surname>Koller</surname> <given-names>D</given-names></name><name><surname>Schuurmans</surname> <given-names>D</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Bottou</surname> <given-names>L</given-names></name></person-group><source>Advances in Neural Information Processing Systems 21</source><publisher-loc>NIPS</publisher-loc><fpage>241</fpage><lpage>248</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chukoskie</surname><given-names>L</given-names></name><name><surname>Snider</surname><given-names>J</given-names></name><name><surname>Mozer</surname><given-names>MC</given-names></name><name><surname>Krauzlis</surname><given-names>RJ</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Learning where to look for a hidden target</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>110</volume><fpage>10438</fpage><lpage>10445</lpage><pub-id pub-id-type="doi">10.1073/pnas.1301216110</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epelboim</surname><given-names>J</given-names></name><name><surname>Suppes</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A model of eye movements and visual working memory during problem solving in geometry</article-title><source>Vision Research</source><volume>41</volume><fpage>1561</fpage><lpage>1574</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(00)00256-X</pub-id><pub-id pub-id-type="pmid">11343722</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Relations between the statistics of natural images and the response properties of cortical cells</article-title><source>Journal of the Optical Society of America A</source><volume>4</volume><fpage>2379</fpage><lpage>2394</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.4.002379</pub-id><pub-id pub-id-type="pmid">3430225</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contributions of ideal observer theory to vision research</article-title><source>Vision Research</source><volume>51</volume><fpage>771</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.09.027</pub-id><pub-id pub-id-type="pmid">20920517</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Bubbles: a technique to reveal the use of information in recognition tasks</article-title><source>Vision Research</source><volume>41</volume><fpage>2261</fpage><lpage>2271</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(01)00097-9</pub-id><pub-id pub-id-type="pmid">11448718</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gureckis</surname><given-names>TM</given-names></name><name><surname>Markant</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><chapter-title>Active learning strategies in a spatial concept learning game</chapter-title><person-group person-group-type="editor"><name><surname>TaatgenN</surname></name><name><surname>van Rijn</surname> <given-names>H</given-names></name><name><surname>Schomaker</surname> <given-names>L</given-names></name><name><surname>Nerbonne</surname> <given-names>J</given-names></name></person-group><source>Proceedings of the 31st Annual Conference of the Cognitive Science Society</source><publisher-loc>Austin, TX</publisher-loc><publisher-name>Cognitive Science Society</publisher-name><fpage>3145</fpage><lpage>3150</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>M</given-names></name><name><surname>Ballard</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Eye movements in natural behavior</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>188</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.02.009</pub-id><pub-id pub-id-type="pmid">15808501</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>MM</given-names></name><name><surname>Shrivastava</surname><given-names>A</given-names></name><name><surname>Mruczek</surname><given-names>R</given-names></name><name><surname>Pelz</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Visual memory and motor planning in a natural task</article-title><source>Journal of Vision</source><volume>3</volume><fpage>6</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1167/3.1.6</pub-id><pub-id pub-id-type="pmid">12678625</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holm</surname><given-names>L</given-names></name><name><surname>Engel</surname><given-names>S</given-names></name><name><surname>Schrater</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Object learning improves feature extraction but does not improve feature selection</article-title><source>PloS One</source><volume>7</volume><fpage>e51325</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0051325</pub-id><pub-id pub-id-type="pmid">23251499</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Houlsby</surname><given-names>N</given-names></name><name><surname>Huszár</surname><given-names>F</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Bayesian active learning for classification and preference learning</chapter-title><source>arXiv</source><fpage>p. 1112.5745</fpage></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houlsby</surname><given-names>NM</given-names></name><name><surname>Huszár</surname><given-names>F</given-names></name><name><surname>Ghassemi</surname><given-names>MM</given-names></name><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cognitive tomography reveals complex, task-independent mental representations</article-title><source>Current Biology</source><volume>23</volume><fpage>2169</fpage><lpage>2175</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.09.012</pub-id><pub-id pub-id-type="pmid">24354016</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>MF</given-names></name><name><surname>Bailey</surname><given-names>T</given-names></name><name><surname>Durrant-Whyte</surname><given-names>H</given-names></name><name><surname>Hanebeck</surname><given-names>UD</given-names></name></person-group><year iso-8601-date="2008">2008</year><chapter-title>On entropy approximation for Gaussian mixture random vectors.</chapter-title><source>IEEE International Conference on Multisensor Fusion and Integration forIntell. Syst</source><publisher-name>IEEE MFI</publisher-name><fpage>181</fpage><lpage>188</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A saliency-based search mechanism for overt and covert shifts of visual attention</article-title><source>Vision Research</source><volume>40</volume><fpage>1489</fpage><lpage>1506</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(99)00163-7</pub-id><pub-id pub-id-type="pmid">10788654</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Inhibition of return</article-title><source>Trends in Cognitive Sciences</source><volume>4</volume><fpage>138</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01452-2</pub-id><pub-id pub-id-type="pmid">10740278</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleinfeld</surname><given-names>D</given-names></name><name><surname>Ahissar</surname><given-names>E</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Active sensation: insights from the rodent vibrissa sensorimotor system</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>435</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.06.009</pub-id><pub-id pub-id-type="pmid">16837190</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ko</surname><given-names>HK</given-names></name><name><surname>Poletti</surname><given-names>M</given-names></name><name><surname>Rucci</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Microsaccades precisely relocate gaze in a high visual acuity task</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1549</fpage><lpage>1553</lpage><pub-id pub-id-type="doi">10.1038/nn.2663</pub-id><pub-id pub-id-type="pmid">21037583</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname><given-names>I</given-names></name><name><surname>Armel</surname><given-names>C</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual fixations and the computation and comparison of value in simple choice</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1292</fpage><lpage>1298</lpage><pub-id pub-id-type="doi">10.1038/nn.2635</pub-id><pub-id pub-id-type="pmid">20835253</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuang</surname><given-names>X</given-names></name><name><surname>Poletti</surname><given-names>M</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Rucci</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Temporal encoding of spatial information during active visual fixation</article-title><source>Current Biology</source><volume>22</volume><fpage>510</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.01.050</pub-id><pub-id pub-id-type="pmid">22342751</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Land</surname><given-names>M</given-names></name><name><surname>Tatler</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Looking and Acting: Vision and Eye Movements in Natural Behaviour</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780198570943.001.0001</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lederman</surname><given-names>SJ</given-names></name><name><surname>Klatzky</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Hand movements: a window into haptic object recognition</article-title><source>Cognitive Psychology</source><volume>19</volume><fpage>342</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(87)90008-9</pub-id><pub-id pub-id-type="pmid">3608405</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levi</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Crowding–an essential bottleneck for object recognition: a mini-review</article-title><source>Vision Research</source><volume>48</volume><fpage>635</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.12.009</pub-id><pub-id pub-id-type="pmid">18226828</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>FF</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Rapid natural scene categorization in the near absence of attention</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>99</volume><fpage>9596</fpage><lpage>9601</lpage><pub-id pub-id-type="doi">10.1073/pnas.092277599</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacKay</surname><given-names>DJC</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Information-based objective functions for active data selection</article-title><source>Neural Computation</source><volume>4</volume><fpage>590</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.1162/neco.1992.4.4.590</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>DA</given-names></name><name><surname>Shewcraft</surname><given-names>RA</given-names></name><name><surname>Wong</surname><given-names>YT</given-names></name><name><surname>Pesaran</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Competition for visual selection in the oculomotor system</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>9298</fpage><lpage>9306</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0908-11.2011</pub-id><pub-id pub-id-type="pmid">21697379</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morvan</surname><given-names>C</given-names></name><name><surname>Maloney</surname><given-names>LT</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Human visual search does not maximize the post-saccadic probability of identifying targets</article-title><source>PLoS Computational Biology</source><volume>8</volume><fpage>e1002342</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002342</pub-id><pub-id pub-id-type="pmid">22319428</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Najemnik</surname><given-names>J</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Optimal eye movement strategies in visual search</article-title><source>Nature</source><volume>434</volume><fpage>387</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1038/nature03390</pub-id><pub-id pub-id-type="pmid">15772663</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Najemnik</surname><given-names>J</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Eye movement statistics in humans are consistent with an optimal search strategy</article-title><source>Journal of Vision</source><volume>8</volume><fpage>4</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1167/8.3.4</pub-id><pub-id pub-id-type="pmid">18484810</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navalpakkam</surname><given-names>V</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Optimal reward harvesting in complex perceptual environments</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>107</volume><fpage>5232</fpage><lpage>5237</lpage><pub-id pub-id-type="doi">10.1073/pnas.0911972107</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>JD</given-names></name><name><surname>Cottrell</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A probabilistic model of eye movements in concept formation</article-title><source>Neurocomputing</source><volume>70</volume><fpage>2256</fpage><lpage>2272</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2006.02.026</pub-id><pub-id pub-id-type="pmid">22787288</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Building the gist of a scene: the role of global image features in recognition</article-title><source>Progress in Brain Research</source><volume>155</volume><fpage>23</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(06)55002-2</pub-id><pub-id pub-id-type="pmid">17027377</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>MF</given-names></name><name><surname>Eckstein</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Looking just below the eyes is optimal across face recognition tasks</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>109</volume><fpage>E3314</fpage><lpage>E3323</lpage><pub-id pub-id-type="doi">10.1073/pnas.1214269109</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poletti</surname><given-names>M</given-names></name><name><surname>Listorti</surname><given-names>C</given-names></name><name><surname>Rucci</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Microscopic eye movements compensate for nonhomogeneous vision within the fovea</article-title><source>Current Biology</source><volume>23</volume><fpage>1691</fpage><lpage>1695</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.07.007</pub-id><pub-id pub-id-type="pmid">23954428</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rasmussen</surname><given-names>CE</given-names></name><name><surname>Williams</surname><given-names>CKI</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Gaussian Processes for Machine Learning</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renninger</surname><given-names>LW</given-names></name><name><surname>Verghese</surname><given-names>P</given-names></name><name><surname>Coughlan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Where to look next? Eye movements reduce local uncertainty</article-title><source>Journal of Vision</source><volume>7</volume><fpage>6</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1167/7.3.6</pub-id><pub-id pub-id-type="pmid">17461684</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Microsaccades: small steps on a long way</article-title><source>Vision Research</source><volume>49</volume><fpage>2415</fpage><lpage>2441</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2009.08.010</pub-id><pub-id pub-id-type="pmid">19683016</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rothkopf</surname><given-names>CA</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Task and context determine where you look</article-title><source>Journal of Vision</source><volume>7</volume><fpage>16</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1167/7.14.16</pub-id><pub-id pub-id-type="pmid">18217811</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rucci</surname><given-names>M</given-names></name><name><surname>Iovin</surname><given-names>R</given-names></name><name><surname>Poletti</surname><given-names>M</given-names></name><name><surname>Santini</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Miniature eye movements enhance fine spatial detail</article-title><source>Nature</source><volume>447</volume><fpage>852</fpage><lpage>855</lpage><pub-id pub-id-type="doi">10.1038/nature05866</pub-id><pub-id pub-id-type="pmid">17568745</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schütz</surname><given-names>AC</given-names></name><name><surname>Trommershauser</surname><given-names>J</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Dynamic integration of information about salience and value for saccadic eye movements</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>109</volume><fpage>7547</fpage><lpage>7552</lpage><pub-id pub-id-type="doi">10.1073/pnas.1115638109</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sebastiani</surname><given-names>P</given-names></name><name><surname>Wynn</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Maximum entropy sampling and optimal Bayesian experimental design</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>62</volume><fpage>145</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1111/1467-9868.00225</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tatler</surname><given-names>BW</given-names></name><name><surname>Vincent</surname><given-names>BT</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Systematic tendencies in scene viewing</article-title><source>Journal of Eye Movement Research</source><volume>2(2)</volume><elocation-id>5</elocation-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tatler</surname><given-names>BW</given-names></name><name><surname>Vincent</surname><given-names>BT</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The prominence of behavioural biases in eye guidance</article-title><source>Visual Cognition</source><volume>17</volume><fpage>1029</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1080/13506280902764539</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname><given-names>S</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Marlot</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><volume>381</volume><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/381520a0</pub-id><pub-id pub-id-type="pmid">8632824</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toscani</surname><given-names>M</given-names></name><name><surname>Valsecchi</surname><given-names>M</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Optimal sampling of visual information for lightness judgments</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>110</volume><fpage>11163</fpage><lpage>11168</lpage><pub-id pub-id-type="doi">10.1073/pnas.1216954110</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wismeijer</surname><given-names>DA</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Orientation of noisy texture affects saccade direction during free viewing</article-title><source>Vision Research</source><volume>58</volume><fpage>19</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.02.003</pub-id><pub-id pub-id-type="pmid">22366079</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yarbus</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="1967">1967</year><source>Eye Movements and Vision</source><publisher-loc>Boston, MA</publisher-loc><publisher-name>Springer US</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4899-5379-7</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.12215.018</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Tsao</surname><given-names>Doris Y</given-names></name><role>Reviewing editor</role><aff id="aff4"><institution>California Institute of Technology</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your work entitled &quot;Active sensing in the categorization of visual patterns&quot; for consideration by <italic>eLife</italic>. Your article has been favourably reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors. The evaluation has been overseen by this Reviewing Editor and Eve Marder as the Senior Editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Having conferred about this paper, we agree that it is acceptable for publication pending revision. However, there was an extended discussion among the reviewers about the extent to which the current results apply to natural vision. The reviewers agree that the authors need to qualify their conclusions to clearly state that while they were trying to emulate natural vision, their task was unnatural in several important ways (e.g. very small exposures, uniform statistics, small saccades not allowed, and reviewing for up to 60s after presentation in the passive conditions), all of which may have changed the participants' strategy from that used in natural vision. Natural vision works under entirely different constrains – larger exposures, natural scene statistics, all saccades allowed, and limited time. Under these constrains the selection of targets may (or may not) be completely different. These differences between the paradigm used in the study and natural vision should be carefully discussed, and relevant parts of the Abstract and Discussion should be modified accordingly.</p><p>[Editors’ note: a previous version of this study was rejected after peer review, but the authors submitted for reconsideration. The previous decision letter after peer review is shown below.]</p><p>Thank you for choosing to send your work entitled &quot;Active sensing in the categorization of visual patterns&quot; for consideration at <italic>eLife</italic>. Your full submission has been evaluated by Eve Marder (Senior editor) and three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the decision was reached after discussions between the reviewers. Based on our discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in <italic>eLife</italic>.</p><p>While all of the reviewers appreciated the significance of the work, they were very concerned about potential confounds due to letting subjects continue viewing the display for 60s after the last revealing. Given this confound, the reviewers were not certain that a revision would adequately deal with this problem, which led to the decision to reject the paper at this time. If you feel you can adequately answer the concerns of this review, the reviewers were sufficiently potentially positive about your manuscript that <italic>eLife</italic> would allow a submission of a new manuscript that addresses these concerns.</p><p><italic>Reviewer #1:</italic> </p><p>Summary</p><p>The present work utilizes an innovative gaze-contingent paradigm where subjects have to identify the category of a texture and can actively reveal locations based on where they saccade to. The authors compare the fixation density maps for the three different stimulus categories and find that they are consistent and category-specific across participants, i.e. each stimulus category evokes a specific pattern of fixations. When revealing locations randomly controlled by the computer the categorization performance decreases indicating that active sensing does increase the information yield for the participants. Next, they construct an ideal observer model that computes the posterior probability of each category given the revealed pixel values, and fit the parameters prior bias, perception noise and decision noise to predict the subject's choice of category. Based on these parameters fitted to the subject's category choice the authors try to predict saccade choices using a Bayesian active sensor algorithm which chooses the saccade location which maximizes the expected information gain, taking into account saccade inaccuracies. They claim that the subject's saccades are predicted well by their Bayesian active sensor algorithm, as the predicted category-specific fixation density maps are significantly correlated with the actual density maps. As a better measure of the goodness of their model they compare the information (in the ideal observer sense) gained by the subject's saccades to the information gained by their simulated saccades, random saccades and saccades based on other strategies. They find that the information gained by the subject's saccades is higher than random saccades and saccades based on three heuristic strategies but lower than the information of the saccades simulated by the Bayesian active sensor, even if prior bias, perception noise and category decision noise are taken into account. They call the latter discrepancy a 30% inefficiency of saccade choice. To show that the saccade choices made by the subject are indeed not optimal in efficiency of making the category choice, they reveal the fixation locations computer-controlled based on the optimal locations predicted by the active Bayesian sensor model without biases and low noise. Thereby, the subject's categorization performance is increased. The efficiency of saccade choice does not vary over time, so there does not appear to be any learning.</p><p>Remarks</p><p>1) In <xref ref-type="fig" rid="fig1">Figure 1</xref>, it is not clear to me, why the patch surrounded by the red circle should have more information about zebra vs. cheetah than other blue patches, so maybe use a better example.</p><p>2) In <xref ref-type="fig" rid="fig2">Figure 2C</xref>, the blue condition titled Random should be called passive or computer-controlled random locations to avoid confusion. Also, please clarify the term &quot;gaze-contingent&quot;.</p><p>3) In Formula 1, D is not defined.</p><p>4) Regarding <xref ref-type="fig" rid="fig4">Figure 4B</xref>, I don't understand why the first term of the formula isn't maximal at locations as far as possible from previous locations (i.e. on the borders of the image). Given that spatial correlations of the stimulus decrease with distance, the uncertainty about pixel values at a far distance from known pixel values should be highest.</p><p>5) The correlation analysis done in <xref ref-type="fig" rid="fig3">Figure 3</xref> should be repeated with simulated entropy-maximizing saccades, to show that the correlation analysis is meaningful. Just correlating the fixation density maps might not be the best measure to evaluate fit of the simulation, since it does not consider each fixation on a case-by-case basis given the prior information from previous saccades.</p><p>6) In addition to the BIC it would be nice to know the cross-validated prediction accuracy of category choice on a testing set.</p><p>7) Also, when showing the performance decrease when revealing at random locations, I would have matched the inter-saccadic distribution (including the variance) and the saccadic distance distribution, to ensure the performance decline is not due to the variance in fixation duration or the subject needing to recover from longer saccades.</p><p><italic>Reviewer #2 (General assessment and major comments (Required)):</italic> </p><p>The paper addresses the strategy underlying the selection of gaze targets along the scanpath of fixations. Specifically, the paper compares human selections and performances with those of an optimal Bayesian algorithm. The authors conclude that while humans employ an information-based strategy, this strategy is sub-optimal. They also identify the sources of this sub-optimality and suggest that &quot;participants select eye movements with the goal of maximizing information about abstract categories that require the integration of information from multiple locations&quot;.</p><p>This is potentially an important paper, which can contribute significantly to the understanding of perceptual processes. The topic is important, the work is, for the most part, elegant and the paper is in general well written. However, there are several crucial points that question the conclusions reached by the authors – these and other comments are listed below.</p><p>1) The authors employ a reductionist method, which is indeed necessary if one wants to reveal underlying mechanisms. However, some of the reductionist choices made in this work fuel the questioning of several of its conclusions. The most concerning reductionist steps are:</p><p>A) Small saccades (&lt; 1 deg) were not allowed. The use of small (&quot;micro&quot;) saccades is task dependent and it may be the case, for example, that with images like those used here a possible strategy is scanning a continuous region. This possibility was &quot;reduced-out&quot; here.</p><p>B) The exposed area was effectively &lt; 0.5 deg and was scaled down in transparency from the center out – this reduced-out a meaningful drift-based scanning of the fixational region.</p><p>C) The images had uniform statistics, which preclude generalization to natural images.</p><p>The authors mention that &quot;the task still allowed them to employ natural every-day eye movement strategies as evidenced by the inter-saccadic intervals (mean 408 ms) that were similar to those recorded for everyday activities […]&quot; – evidently, normal ISIs do not indicate a natural strategy – a) there is much more into an active strategy and b) ISIs seems to be determined by hard-wired circuits that are only slightly modulated in different contexts – see a variety of tests of ISIs in the last decade.</p><p>2) The &quot;last 60s&quot; problem. Natural viewing involves a scanpath of fixations. However, natural viewing does not allow a re-scanning of the collection of previous fixation locations. The design of the trial is thus odd– why were the subjects given those extra 60s at the end of the trial to rescan their revealings? This is not justified in the paper and it is actually hidden, in a way, in the Materials and methods section – it took me some time to understand that this was the case. The implications of this design are significant:</p><p>A) How can the authors relate performance to either of the two different phases of the trial (the &quot;acquiring&quot; and the &quot;rescanning&quot;)?</p><p>B) Even in the &quot;acquiring&quot; phase, how can the authors rule out brief rescanning of exposed revealings for periods shorter than the threshold period?</p><p>C) If performance depended on the perception during these 60s, then what was the underlying strategy while rescanning?</p><p>The authors must describe the procedure clearly at the outset, relate to all these issues explicitly, and explain how they affect (or not) their conclusions.</p><p>3) As indicated by the above, the primary characterization of the active strategy here is about <italic>which</italic> revealing were or should be selected, and less about when or in what order. True, order was a factor during the acquiring phase, but not during the rescanning phase. Order seems to play a role in the correlations in <xref ref-type="fig" rid="fig3">Figure 3B</xref> but it is not clear by how much. The collection of revealing locations may indeed be the most relevant factor when trying to categorize an image out of several known images with known structures. This is, however, not the case in most natural cases, in which real-time information is essential.</p><p>4) From the above and from further analysis it appears that the strong statements of the Abstract are not supported by the data in the paper.</p><p>A) &quot;Categorization performance was markedly improved when participants were guided to fixate locations selected by an optimal Bayesian active sensor algorithm […]&quot; – this statement sounds as if guiding the next saccade improves performance. This cannot be concluded here because of the &quot;60 s problem&quot; – during rescanning subjects could select any pattern they wanted. Moreover, performance may even depend on the rescanning period <italic>only</italic>.</p><p>B) &quot;By using […] we show that a major portion of this apparent suboptimality of fixation locations arises from prior biases, perceptual noise and inaccuracies in eye movements[…]&quot; – you do not really <italic>show</italic> this. You show that if you add these imperfections to your model you get performance level that resembles that of the subjects. However, there are so many ways to impair the performance of your model – where do you show that these are the crucial factors?</p><p>C) &quot;The central process of selecting fixation locations is around 70% efficient&quot; – this is correct <italic>only</italic> for the specific task and context of your experiment. The statement sounds much more general than that.</p><p>D) &quot;Participants select eye movements with the goal of maximizing information about abstract categories that require the integration of information from multiple locations&quot; – again, a statement with a general flavor without such justification – the statement should be toned and tuned down to reflect the narrow context of the findings.</p><p>5) Audience and style. As written now the paper seems to address experts in Bayesian models of perception. Given that it is submitted to <italic>eLife</italic> I assume that the paper should address primarily, or at least to the same degree, biologists who are interested in understanding perception. And indeed, papers like this form a wonderful opportunity to create a productive dialogue between biologists and theoreticians. For this end, the style of writing must change. The amount of statistical details should be reduced, the biological meaning of the various strategies discussed should be provided, the rational for selecting the BAS algorithm as the optimal strategy should be explained, the meaning of each strategy in terms of actual eye movements in natural scenes should be explained. At the end, the reader should understand the biological meaning of each strategy (that is, a biologically-plausible description of the strategy employed by their subjects in this task, and a strategy they would employ in a natural task), the rational of why one strategy is better than the other, and in what sense humans are sub-optimal.</p><p>6) The discussions about active-sensing in the paper are written as if they come from a sort of a theoretical vacuum. Active sensing has been introduced, studied and discussed at various levels for various species and modalities for years, and intensively so for the last decade or two. Nevertheless, the paper sounds as if these concepts are only beginning to be addressed, and as if active vision is mostly covered by addressing saccades scanpath selections. This introduces a huge distortion of the concepts of active sensing and active vision. Active vision is much more than scanpath selections. Eye movements include saccades and drifts, and as far as we know today vision does not occur without the drift movements. Saccades include large and small (&quot;micro&quot;) saccades, making a continuous spectrum of saccade amplitudes. In this study only large (&gt; 1 deg) saccades were allowed. Thus, this study ignores a substantial portion of visually-crucial eye movements. While this is ok as a reductionist method, it is not ok to ignore the reduced-away components in the discussion and interpretation of the results. Thus:</p><p>A) There is no justification to use the term &quot;eye movements&quot; in this paper – the components of eye movements studied here should be termed properly (perhaps use terms like &quot;fixation scanpath&quot; or &quot;saccadic order&quot; or anything else that is appropriate).</p><p>B) Previous results and hypotheses about active sensing/active vision that refer to all kinds of movements must be discussed and referred to when interpreting the current results.</p><p>C) The results of the current study should be put in the context of this general active sensing context, and the generality of the conclusions should be phrased accordingly – mostly, they should be toned down and related to the specific reduced context of this study.</p><p>Minor comments:</p><p>1) Arbitrary selection of parameters – please explain the choice of every parameter you use (e.g., the criteria for saccades and fixations).</p><p>2) <xref ref-type="fig" rid="fig3">Figure 3B</xref> – please run a shuffling control analysis to show how much of the correlation is order-dependent.</p><p><italic>Reviewer #3:</italic> </p><p>The authors investigated the control of eye movements in a visual categorization task. By using a gaze-contingent paradigm, they could show that eye movement patterns became more specific to the underlying image structure with increasing fixation number. A comparison with a Bayesian active sensor (BAS) model showed that humans were quite efficient, given certain biases and inaccuracies of eye movements.</p><p>The study should be interesting to a broad audience and is well conducted. A few further analyses could strengthen the message.</p><p>1) The authors corrected for errors in saccade targeting tangential and orthogonal to saccade direction. However, as they correctly stated in the Discussion, there are also other potential sources of errors due to biases along cardinal directions or the bias to fixate the center of an image. It would be very interesting to see how much of the performance reduction is caused by these biases. It should be possible to assess these biases directly from the existing data.</p><p>2) Since there were a lot of incorrect judgments overall, it would be interesting to compare the density maps for correct and incorrect judgments. If eye movements are indeed controlled by an active strategy, the differences and correlations should be more pronounced for correct judgments.</p><p>3) The example trial in <xref ref-type="fig" rid="fig4">Figure 4</xref> provides a good illustration of BAS and the maximum entropy variant but it does not allow a quantitative comparison. It would be helpful to show the full distributions of percentile values with respect to BAS and entropy scores.</p><p>4) As I understand the task, the revealings stayed on screen after the last revealing for 60 s or an unlimited duration, depending on the condition. The behavior of the subjects after the last revealing should be reported because it could have influenced their perceptual judgment.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.12215.019</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Having conferred about this paper, we agree that it is acceptable for publication pending revision. However, there was an extended discussion among the reviewers about the extent to which the current results apply to natural vision. The reviewers agree that the authors need to qualify their conclusions to clearly state that while they were trying to emulate natural vision, their task was unnatural in several important ways (e.g. very small exposures, uniform statistics, small saccades not allowed, and reviewing for up to 60s after presentation in the passive conditions), all of which may have changed the participants' strategy from that used in natural vision. Natural vision works under entirely different constrains – larger exposures, natural scene statistics, all saccades allowed, and limited time. Under these constrains the selection of targets may (or may not) be completely different. These differences between the paradigm used in the study and natural vision should be carefully discussed, and relevant parts of the Abstract and Discussion should be modified accordingly.</italic> </p><p>Thank you for the reviewers’ comments on our revised manuscript. We have now added in a whole section in the Discussion on the relation of our task and results to natural vision and discuss in detail the four issues raised by the reviewers. Some of these issues were discussed already in other parts of the paper such as the Methods but have now been put together in this section headed “Relevance for natural vision”.</p><p>[Editors’ note: the author responses to the previous round of peer review follow.]</p><p><italic>While all of the reviewers appreciated the significance of the work, they were very concerned about potential confounds due to letting subjects continue viewing the display for 60s after the last revealing. Given this confound, the reviewers were not certain that a revision would adequately deal with this problem, which led to the decision to reject the paper at this time.</italic> </p><p>All three reviewers were concerned that allowing additional time after the last revealing (rescanning period) was a confound in the experiment and undermined our conclusions. In fact the additional time is essential for the interpretation of the experimental results and does not affect our conclusions. We realise we did not highlight this sufficiently and have now done so in the revised manuscript.</p><p>1) As we wished to compare the active strategy with passive revealing we allowed participants to rescan the revealed locations after the final revealing but before making a decision. This was critical as in the passive revealing conditions, although participants may detect and follow the revealings, because they are small and dispersed, the participants need to scan the scene to find and view them all. Therefore, the reason for allowing additional time after the final revealing was so as to make sure they had a chance to extract as much information as they like from the revealed locations (locations and pixel values). In order to make the conditions directly comparable, we followed the same procedure in the active condition. Thus, allowing a rescanning period was the only way we could make a fair comparison across conditions using our gaze contingent design, which in turn allowed us fine control over the information participants could obtain by eye movements.</p><p>2) Critically, our key interest is in where participants choose to fixate during the initial revealing period and not in the perception model. During the active condition all the selection happens prior to the rescanning phase and participants do not know how many saccades they will be allowed so they still need to be efficient in revealing locations even if they can rescan them later. Therefore, as we are analyzing the initial selection, the final rescanning simply equalizes the information that can be extracted from each revealing for all conditions but it is unlikely to influence the initial selection. Although, it is theoretically possible that participants adopted a different eye movement strategy knowing they could freely re-visit already revealed locations, given the little improvement they showed across sessions in our task (Figure 7), this remains a highly unlikely possibility.</p><p>3) Although we allowed up to 60 s after the final revealing (we wished participants to feel that they had as long as they wanted), participants took on average only 5.0 s to make a decision in the active condition, 6.4 s in the passive random and 4.4 s in the passive BAS. We expected that participants would use rescanning time so that information extracted from the revealings would have saturated by the time of choice. To examine whether rescanning time had an effect on performance, we fit each participant’s choice accuracy as a logistic function (bounded between 0.5 and 1) of rescanning time. We allowed different shifts per condition (active, passive random, passive BAS) but the same slope parameter across conditions. This showed that for two participants there was a significant effect of rescanning time on performance (i.e. slope significantly different from zero) but with a decrement in performance for longer rescanning times. The probability of a correct decision for rescanning times at the 25th and 75th percentiles of the rescanning time distribution falls from 0.77 to 0.54 (p&lt;0.001) and from 0.77 to 0.66 (p &lt;0.03) for these two participants. We have included this information in the revision. Therefore, if anything, participants did not use longer rescanning times to improve performance but may have taken a little extra time when they were uncertain. In contrast, their performance correlated quite strongly with the number of revealings (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). This indicates that the main determinant of their performance was how well they chose the revealing locations in the first place, rather than how long they rescanned the location, as our original results already showed.</p><p>4. To fully assuage the reviewers’ concerns we have now run a control experiment on 3 new participants in the active revealing condition except that no rescanning after the final revealing was allowed (all revealings returned to a black screen 350 ms after the saccade away from the final revealing). The results from this show that the revealing density maps are very similar to those from the original experiment (average within-type vs. across-type correlation across the two groups of participants: 0.63 vs. 0.30) and performance is also similar (although not surprisingly slightly worse). The proportion correct across all active revealing trials for the original participants was 0.65, 0.66 &amp; 0.69 (average 0.66) and for the new controls 0.64, 0.58 &amp; 0.66 (average 0.63). We include these new results in the paper and in a supplementary figure (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) and take them to indicate that allowing rescanning in our original design did not change participants’ revealing strategy.</p><p>In addition, we realized we could further improve our analysis of the revealing density maps. In particular, in our original analysis these maps were constructed using the absolute positions of revealings. However, the statistics of our stimuli are translationally invariant and therefore the critical features of the optimal solution are the <italic>relative</italic> locations of the revealing to each other. Therefore, for each trial we first shifted all the revealing locations in that trial equally so that their mean (centre of mass) was located at the centre of the image, thereby removing the variability caused by which part of the image the participants explored in that trial, which is irrelevant to optimality (due to the translationally invariant nature of the statistics of our images, see above). This analysis leads to much cleaner and more compelling maps and we have redone all analyses (correlations of revealing densities) based on these new densities. We checked that all conclusions (including those reported in the response to reviewers below) remained the same (although quantitatively stronger) compared to the original absolute location method.</p><p>Reviewer #1:</p><p><italic>Remarks</italic> </p><p><italic>1) In <xref ref-type="fig" rid="fig1">Figure 1</xref>, it is not clear to me, why the patch surrounded by the red circle should have more information about zebra vs. cheetah than other blue patches, so maybe use a better example.</italic> </p><p>We have modified this figure and legend to make the example more intuitive.</p><p><italic>2) In <xref ref-type="fig" rid="fig2">Figure 2C</xref>, the blue condition titled Random should be called passive or computer-controlled random locations to avoid confusion. Also, please clarify the term &quot;gaze-contingent&quot;.</italic> </p><p>We have relabelled <xref ref-type="fig" rid="fig2">Figure 2C</xref> and <xref ref-type="fig" rid="fig5">Figure 5A</xref> to explicitly show the passive and active conditions and modified the text to use consistent terminology where necessary. We have also added a phrase to clarify the term “gaze-contingent.”</p><p><italic>3) In Formula 1, D is not defined.</italic> </p><p>D was actually defined in the previous section “Bayesian ideal observer” already, but we now define it again right after Eq. 1.</p><p><italic>4) Regarding <xref ref-type="fig" rid="fig4">Figure 4B</xref>, I don't understand why the first term of the formula isn't maximal at locations as far as possible from previous locations (i.e. on the borders of the image). Given that spatial correlations of the stimulus decrease with distance, the uncertainty about pixel values at a far distance from known pixel values should be highest.</italic> </p><p>We understand that this seems counter-intuitive, but it is correct. We try here to give an intuition. However, as the maximum entropy model is not the focus of the paper we would prefer to omit the lengthy description and figure but will be happy to include them in the paper if the Reviewer deems it necessary.</p><p>We have added a short note to the caption of <xref ref-type="fig" rid="fig4">Figure 4</xref> to alert the reader to the non-intuitive nature of the MaxEnt model, and also included a new <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p><p>In more detail, the total uncertainty (equivalent to the first term on the right hand side of Eq. 1) about pixel value can be decomposed into two parts (this can be most easily understood formally if entropy is substituted by variance, by the law of total variance, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.). The first part is “unexplained variance” (equivalent to the second term on the right hand side of Eq. 1): even if we knew what the image type was, there is still uncertainty due to the stochastic nature of the stimulus and perception noise. Unexplained variance <italic>increases</italic> with distance from revealings. Because the stimulus is spatially correlated, i.e. it is not white noise, knowing about the pixel value at a particular revealing location provides some information about nearby pixel values but not about distant pixel values. The second part is “explained variance” (equivalent to our BAS score, Eq. 1) and it is due to uncertainty about image type and the fact that each type predicts different pixel values at the characteristic length scale of stimulus spatial correlations. At very short length scales, each image type is bound to predict the same pixel values as that at the revealing location (as they are constrained by this observation), and so the explained variance is diminishingly small, and beyond the stimulus correlation length scale the differences again become small as the average predictions are the same (zero) due to stochasticity. Thus, explained variance peaks around the stimulus correlation length scales (which in our case are small relative to the total image size) and fall off from there, resulting in a contribution that predominantly <italic>decreases</italic> with distance from revealings. Therefore, the total uncertainty determined as a combination of these two sources of uncertainty can peak at the borders or at intermediate distances, close to the autocorrelation length scale of the stimulus, depending on the balance of these two sources, which in turn depends on the revealed pixel values (and locations)</p><p><italic>5) The correlation analysis done in <xref ref-type="fig" rid="fig3">Figure 3</xref> should be repeated with simulated entropy-maximizing saccades, to show that the correlation analysis is meaningful. Just correlating the fixation density maps might not be the best measure to evaluate fit of the simulation, since it does not consider each fixation on a case-by-case basis given the prior information from previous saccades.</italic> </p><p>We repeated the correlation analysis simulating the maximum entropy strategy. The results show no significant correlation between the revealing locations chosen by MaxEnt and those chosen by the participants (Author response <xref ref-type="table" rid="tbl1">table 1</xref>). This suggests that the correlation shown in the original <xref ref-type="fig" rid="fig3">Figure 3</xref> is meaningful and that MaxEnt does not describe our participants’ behaviour well. We now present this analysis in the results but would prefer not to overload the paper with this additional Table.</p><p><table-wrap id="tblu1" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td valign="top"/><td valign="top"><p>within-type correlation;</p><p>p-value for correlation (ρ)</p></td><td valign="top"><p>across-type correlation; p-value for correlation (ρ)</p></td></tr><tr><td valign="top"><p>participant 1</p></td><td valign="top"><p>ρ=-0.121; p=0.13</p></td><td valign="top"><p>ρ=0.060; p=0.13</p></td></tr><tr><td valign="top"><p>participant 2</p></td><td valign="top"><p>ρ=-0.117; p=0.12</p></td><td valign="top"><p>ρ=0.056; p=0.13</p></td></tr><tr><td valign="top"><p>participant 3</p></td><td valign="top"><p>ρ=-0.024; p=0.43</p></td><td valign="top"><p>ρ=0.017; p=0.38</p></td></tr></tbody></table></table-wrap></p><p>Author response table 1. Correlation between participant’s eye movement and those derived from the MaxEnt algorithm at 25 revealing.</p><p>We also agree that the correlation is not the best way to evaluate the model but they are intuitive to understand. Critically, this is why we also analyzed the information curves which are a more principled measure that includes prior information from previous saccades.</p><p><italic>6) In addition to the BIC it would be nice to know the cross-validated prediction accuracy of category choice on a testing set.</italic> </p><p>We have now performed this analysis and the results are in Author response table 2. We computed 10-fold cross validated prediction errors by 10 times holding out a different random 10% of the data, fitting parameters to the remaining 90% and measuring prediction performance on the held out 10%. As one can see, cross validation errors show the same trend as BIC values, and thus we decided not to include this in the manuscript as the BIC is more informative for model comparison.</p><p><table-wrap id="tblu2" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td valign="top"><p>BIC difference from <xref ref-type="table" rid="tbl2">Table 2</xref>:</p></td><td valign="top"><p>Average prediction error per trial (arithmetic mean; geometric mean):</p></td></tr><tr><td valign="top"><p>160</p></td><td valign="top"><p>0.4294; 0.4262</p></td></tr><tr><td valign="top"><p>139</p></td><td valign="top"><p>0.3958; 0.4246</p></td></tr><tr><td valign="top"><p>58</p></td><td valign="top"><p>0.3671; 0.4194</p></td></tr><tr><td valign="top"><p>0</p></td><td valign="top"><p>0.3658; 0.4179</p></td></tr><tr><td valign="top"><p>105</p></td><td valign="top"><p>0.3731; 0.4237</p></td></tr><tr><td valign="top"><p>102</p></td><td valign="top"><p>0.3729; 0.4228</p></td></tr></tbody></table></table-wrap></p><p>Author response table 2. BIC values and cross-validated prediction errors.</p><p><italic>7) Also, when showing the performance decrease when revealing at random locations, I would have matched the inter-saccadic distribution (including the variance) and the saccadic distance distribution, to ensure the performance decline is not due to the variance in fixation duration or the subject needing to recover from longer saccades.</italic> </p><p>As discussed above in the response to the editors, in both the passive and active conditions, participants could rescan the revealed locations after the final revealing before making a decision. Therefore, these concerns are not relevant as the participants can fixate as long as they like each of the revealed locations. Please also see the response to the editors above for the rationale for the time that participants are allowed to rescan the scene after the last revealing.</p><p>Reviewer #2:</p><p><italic>1) The authors employ a reductionist method, which is indeed necessary if one wants to reveal underlying mechanisms. However, some the reductionist choices made in this work fuel the questioning of several of its conclusions. The most concerning reductionist steps are:</italic> </p><p><italic>A) Small saccades (&lt; 1 deg) were not allowed. The use of small (&quot;micro&quot;) saccades is task dependent and it may be the case, for example, that with images like those used here a possible strategy is scanning a continuous region. This possibility was &quot;reduced-out&quot; here.</italic> </p><p><italic>B) The exposed area was effectively &lt; 0.5 deg and was scaled down in transparency from the center out – this reduced-out a meaningful drift-based scanning of the fixational region.</italic> </p><p>The reviewer is concerned that micro-saccades and drift could play an important role in our task but that we do not account for them in our experimental protocol. As we now clarify in the revised manuscript, the key aim of the experiment was to look at a more voluntary component of movement, that is, the scan paths of fixations, rather than the more involuntary process of</p><p>micro-saccades and drift (Rolfs, 2009). Over the course of each fixation in our experiment (~350 ms), based on the work of Rolfs (2009) who systematically studied fixation variability (ibid <xref ref-type="fig" rid="fig4">Figure 4</xref>), the SD of eye position is on the order of 0.22 deg. As each of our Gaussian revealing aperture had a SD of 0.18 degree, drifts and microsaccades were in fact likely to be used for extracting information within each revealed patch. Critically, if we allowed revealings on this length scale, the revealed locations would be too close together to be informative, as the smallest length scale of the our patterns (stripy) is 0.91 deg and is more than 4 times larger than the typical distance covered by microsaccades and drift (0.22 deg, see above).</p><p>Finally, if participants are limited in the number of saccades they are allowed to make (micro or otherwise) then our model makes it very clear that micro- (or small) saccades are highly sub-optimal for the high-level category judgments we study, and in agreement with this our participants’ selection of revealing locations were on average 3.51 deg apart. Therefore, we feel that the existence of drift and micro-saccades does not undermine the main message of our paper and include a discussion of these issues, in particular, that they may play a complementary role in increasing information about low-level visual features.</p><p><italic>C) The images had uniform statistics, which preclude generalization to natural images.</italic> </p><p><italic>The authors mention that &quot;the task still allowed them to employ natural every-day eye movement strategies as evidenced by the inter-saccadic intervals (mean 408 ms) that were similar to those recorded for everyday activities […]&quot; – evidently, normal ISIs do not indicate a natural strategy – a) there is much more into an active strategy and b) ISIs seems to be determined by hard-wired circuits that are only slightly modulated in different contexts – see a variety of tests of ISIs in the last decade.</italic> </p><p>Natural image statistics are often assumed to be spatially stationary (e.g. Field, 1987), although this is undoubtedly an approximation. Our stimulus was more naturalistic than many of the stimuli used in active sensing studies of visual search (e.g. 1/f noise) while still allowing a rigorous control and measurement of the amount of high-level category information available at any potential revealing location which would have not been possible with real natural scenes. Thus, we felt our stimuli strike the right balance on the eternally debated natural-vs.-artificial stimulus scale. We agree that ISIs by themselves are only necessary but not sufficient to prove that natural eye movement strategies are at play, but – as we also mention in the manuscript – we also found little to no learning across several sessions which we take as further (albeit still only circumstantial) evidence. Importantly, at least in principle, our mathematical approach generalises to more complex stimuli as long as their statistics are known and can be expressed as P(z<sub>1</sub> z<sub>n</sub> x<sub>1</sub> x<sub>n</sub>, c) (with the notation used in the manuscript). We have included these points in the Methods to more clearly express the strengths and limitations of our stimuli and approach.</p><p><italic>2) The &quot;last 60s&quot; problem. Natural viewing involves a scanpath of fixations. However, natural viewing does not allow a re-scanning of the collection of previous fixation locations. The design of the trial is thus odd– why were the subjects given those extra 60s at the end of the trial to rescan their revealings? This is not justified in the paper and it is actually hidden, in a way, in the Methods section – it took me some time to understand that this was the case. The implications of this design are significant:</italic> </p><p><italic>A) How can the authors relate performance to either of the two different phases of the trial (the &quot;acquiring&quot; and the &quot;rescanning&quot;)?</italic> </p><p>As this point was raised by all three reviewers and highlighted as the key reason the paper was rejected, we have provided a full response to editors above (that includes an analysis of performance as a function of rescanning time as well as a new control experiment without rescanning).</p><p><italic>B) Even in the &quot;acquiring&quot; phase, how can the authors rule out brief rescanning of exposed revealings for periods shorter than the threshold period?</italic> </p><p>The revealed locations are generally far apart so it is not possible to return to a revealing without triggering an additional revealing. Even if participants could rescan during the revealing stage this still does not undermine the analysis of whether they are efficient in the selection of N discrete revealings.</p><p><italic>C) If performance depended on the perception during these 60s, then what was the underlying strategy while rescanning?</italic> </p><p>Please see point 3 and 4 in the response to the editors for this point.</p><p><italic>The authors must describe the procedure clearly at the outset, relate to all these issues explicitly, and explain how they affect (or not) their conclusions.</italic> </p><p>We have now clarified all the issues relating to rescanning in the revision.</p><p><italic>3) As indicated by the above, the primary characterization of the active strategy here is about which revealing were or should be selected, and less about when or in what order. True, order was a factor during the acquiring phase, but not during the rescanning phase. Order seems to play a role in the correlations in <xref ref-type="fig" rid="fig3">Figure 3B</xref> but it is not clear by how much (see below). The collection of revealing locations may indeed be the most relevant factor when trying to categorize an image out of several known images with known structures. This is, however, not the case in most natural cases, in which real-time information is essential.</italic></p><p>This is really the rescanning issue again. As the participant does not know how many saccades they will be allowed on each trial, they have to select them in an order that will be informative at the time they choose them and several of our analyses examine order effects. Therefore the selection process operates in real time. Please see the full response to the editors. We agree that we are not studying the “when” question (i.e. how long each fixation should be hold for) and we mention this in the Materials and methods.</p><p><italic>4) From the above and from further analysis it appears that the strong statements of the Abstract are not supported by the data in the paper.</italic> </p><p><italic>A) &quot;Categorization performance was markedly improved when participants were guided to fixate locations selected by an optimal Bayesian active sensor algorithm […]&quot; – this statement sounds as if guiding the next saccade improves performance. This cannot be concluded here because of the &quot;60 s problem&quot; – during rescanning subjects could select any pattern they wanted. Moreover, performance may even depend on the rescanning period</italic> only.</p><p>Please see the response to the editors. We do not claim the participant has to see the revealings in a specific order in the passive condition and our perceptual model does not take order into account (and the reason for this is that we allow rescanning). To reiterate, in order to equate the information on location and pixel color in the passive and active conditions we allow the rescanning period. The performance may well depend on only the rescanning period in both conditions but critically the selection in the active condition has to be done in real time. We have revised the phrasing of this sentence to: “categorization performance was markedly improved when locations were revealed to participants by an optimal Bayesian active sensor algorithm.”</p><p><italic>B) &quot;By using […] we show that a major portion of this apparent suboptimality of fixation locations arises from prior biases, perceptual noise and inaccuracies in eye movements […]&quot; – you do not really show this. You show that if you add these imperfections to your model you get performance level that resembles that of the subjects. However, there are so many ways to impair the performance of your model – where do you show that these are the crucial factors?</italic> </p><p>We chose to start with what we (and we expect most readers) would regard as the most natural sources and forms of sub-optimality – sensory and motor noise and biases. Clearly, the number of potential models is unbounded but in any given scientific study one has to consider a finite set of possible alternative models. We did do formal comparison of a set of models, so we emphasize that it is not as though we hand-picked one. Our aim was to explain the data parsimoniously and we are happy to consider alternative models if the reviewer has something particular in mind but do feel the set of models we examined forms a reasonable set for our study. We have removed the word “show” and now use “estimate” instead.</p><p><italic>C) &quot;The central process of selecting fixation locations is around 70% efficient&quot; – this is correct only for the specific task and context of your experiment. The statement sounds much more general than that.</italic> </p><p>We clarify that this is for our task. However, note that no paper can ever make a claim of anything apart from what particular task was studied in the experiment. We feel it reasonable that an Abstract then tries to abstract a message for the reader. To take just one example, Najemnik and Geisler in the Abstract of their 2005 Nature paper say “We find that humans achieve nearly optimal search performance” whereas clearly that can’t be stated unless they have examined every search task and stimuli that humans have ever used. But most readers understand what they and others mean by the statement.</p><p><italic>D) &quot;Participants select eye movements with the goal of maximizing information about abstract categories that require the integration of information from multiple locations&quot; – again, a statement with a general flavor without such justification – the statement should be toned and tuned down to reflect the narrow context of the findings.</italic> </p><p>We do use the word “estimate” earlier in the Abstract, so again we find this criticism rather unfair as it would apply to almost all papers on this topic.</p><p><italic>5) Audience and style. As written now the paper seems to address experts in Bayesian models of perception. Given that it is submitted to</italic> eLife <italic>I assume that the paper should address primarily, or at least to the same degree, biologists who are interested in understanding perception. And indeed, papers like this form a wonderful opportunity to create a productive dialogue between biologists and theoreticians. For this end, the style of writing must change. The amount of statistical details should be reduced,</italic> </p><p>We feel it is very important to back up our results with statistical rigor so would not be keen to remove statistical tests from the results. Indeed other reviewers have asked for more statistical tests which we have included where necessary. If the reviewer means the equations for the BAS, this is the single equation we have in the main text which is so central to the paper we would be reluctant to remove it.</p><p><italic>the biological meaning of the various strategies discussed should be provided, the rational for selecting the BAS algorithm as the optimal strategy should be explained, the meaning of each strategy in terms of actual eye movements in natural scenes should be explained. At the end, the reader should understand the biological meaning of each strategy (that is, a biologically-plausible description of the strategy employed by their subjects in this task, and a strategy they would employ in a natural task), the rational of why one strategy is better than the other, and in what sense humans are sub-optimal.</italic> </p><p>We have explained the rationale for selecting the BAS algorithm in the section “Predicting eye movement patterns by a Bayesian active sensor algorithm”. In terms of giving a “biologically-plausible description of the strategy employed by their subjects in this task,” we are a little unsure what the reviewer wants – the detailed description of BAS is the strategy that we propose participants are using, which is to find the location in the scene that when fixated is most likely to lead to the greatest reduction in the categorization error. We now highlight this in words in the Discussion and hope this addresses the reviewer’s comment.</p><p><italic>6) The discussions about active-sensing in the paper are written as if they come from a sort of a theoretical vacuum. Active sensing has been introduced, studied and discussed at various levels for various species and modalities for years, and intensively so for the last decade or two. Nevertheless, the paper sounds as if these concepts are only beginning to be addressed, and as if active vision is mostly covered by addressing saccades scanpath selections. This introduces a huge distortion of the concepts of active sensing and active vision. Active vision is much more than scanpath selections. Eye movements include saccades and drifts, and as far as we know today vision does not occur without the drift movements. Saccades include large and small (&quot;micro&quot;) saccades, making a continuous spectrum of saccade amplitudes. In this study only large (&gt; 1 deg) saccades were allowed. Thus, this study ignores a substantial portion of visually-crucial eye movements. While this is ok as a reductionist method, it is not ok to ignore the reduced-away components in the discussion and interpretation of the results. Thus:</italic> </p><p><italic>A) There is no justification to use the term &quot;eye movements&quot; in this paper – the components of eye movements studied here should be termed properly (perhaps use terms like &quot;fixation scanpath&quot; or &quot;saccadic order&quot; or anything else that is appropriate).</italic> </p><p>We felt it would be very awkward to replace all mention of “eye movement” in the paper and as suggested we clarify now that we only studied fixation scanpaths and not microsaccades or drift. Again there are many examples in the literature where it is common practice to refer to the kind of study we did as being about “eye movements”. If the reviewer and editors insist, we can replace all mention of eye movements but feel it would make the paper far less accessible.</p><p><italic>B) Previous results and hypotheses about active sensing/active vision that refer to all kinds of movements must be discussed and referred to when interpreting the current results.</italic> </p><p>We now reference several papers on microsaccades and drift.</p><p><italic>C) The results of the current study should be put in the context of this general active sensing context, and the generality of the conclusions should be phrased accordingly – mostly, they should be toned down and related to the specific reduced context of this study.</italic> </p><p>We have clarified that we study fixation scanpaths.</p><p><italic>Minor comments:</italic></p><p><italic>1) Arbitrary selection of parameters – please explain the choice of every parameter you use (e.g., the criteria for saccades and fixations).</italic></p><p>The parameters were based on pilot experiments on the authors with the aim of making the revealings in the free-scan session feel natural so that each location viewed was revealed and spurious locations were not revealed. We have now explained this in the Methods.</p><p><italic>2) <xref ref-type="fig" rid="fig3">Figure 3B</xref> – please run a shuffling control analysis to show how much of the correlation is order-dependent.</italic> </p><p>We have performed the shuffling control and in doing this, we realized that the magnitude of correlation depends on the number of samples used to construct the density maps. When we calculated the correlation in our original analysis, the number of samples increased with the number of revealing on the x-axis. This means that the increasing separation could have arisen from the increasing sample size. We have therefore modified the way we calculate the correlation and ensured that the number of samples used to construct the density maps remains the same as the number of revealing increases. The new results are in the updated <xref ref-type="fig" rid="fig3">Figure 3B</xref>. As one can see, the trend of the correlation curves is the same: the within- and across- type correlations increasingly separate as a function of revealing, with the within-type correlation increasing and the across-type correlation decreasing. In contrast, the shuffling analysis shows that the correlation curves remain constant as a function of revealing number. This is expected for the shuffling case because the density maps are now constructed with random subsamples of the same pool of data regardless of revealing number. The shuffled results are shown in <xref ref-type="fig" rid="fig6">Author response image 1</xref> but we don’t think they need to be included in the paper as they have to be flat by construction now that we have ensured that the sample size is independent of revealing number. We thank the reviewer for suggesting the shuffling control as it led to us improving the analysis.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.12215.016</object-id><label>Author response image 1.</label><caption><title>Correlation as a function of revealing number with the revealing number shuffled.</title><p>Orange denotes within-type correlation; purple denotes across-type (cf. <xref ref-type="fig" rid="fig3">Figure 3B</xref> in the manuscript). Line and shaded area represent mean and SD, respectively.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.016">http://dx.doi.org/10.7554/eLife.12215.016</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-resp-fig1-v3"/></fig></p><p>Reviewer #3:</p><p><italic>1) The authors corrected for errors in saccade targeting tangential and orthogonal to saccade direction. However, as they correctly stated in the Discussion, there are also other potential sources of errors due to biases along cardinal directions or the bias to fixate the center of an image. It would be very interesting to see how much of the performance reduction is caused by these biases. It should be possible to assess these biases directly from the existing data.</italic> It is important to clarify that the suboptimalities the Reviewer asks us to identify are conceptually different from those that we factored out using the specific biases and noise included in the (non-ideal) BAS model. The ones we considered are suboptimalities that arise in the execution of a planned saccade (as in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>), while the ones that the reviewer refers to could be suboptimalities of the planning itself. As we are interested in the degree of (sub)optimality of the planning component, we thought it would be misleading to factor out the biases the Reviewer is considering. We realise that our discussion of this issue was confusing in the original manuscript so we have now rewritten it.</p><p>The Reviewer asks us to assess possible fixation bias towards the center of the image and saccade directional bias towards cardinal directions. With the old analysis of the density maps that uses the absolute revealing locations (see point 4 in response to the editor), we see that the participants actually tend to fixate away from the center compared to BAS. Similarly, with the new analysis that uses relative locations, we see that the participants actually tend to have a more diffuse distribution of fixation compared to BAS (see mean revealing density maps in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). The fact that the revealings chosen by BAS are more efficient and more concentrated at the center of the field of view (both in absolute and shifted position) suggests that this type of fixation bias will actually improve the performance rather than reduce it and therefore would not account for any sub-optimality.</p><p>Comparing the saccades made by the participants and the BAS algorithm, we see that our participants tend to make more horizontal saccades and fewer vertical saccades (<xref ref-type="fig" rid="fig7">Author response image 2</xref>). The deviations from BAS may well reduce performance, but it is not obvious how to formalize or determine if this is a true bias on top of planning or part of planning itself. In particular, it is not clear to us how we can measure bias directly from the existing data (as the reviewer suggests) separate from any active strategy as we do not know where subjects are aiming. We would prefer not to add the plot below to the paper as there is no really strong message we can make about it, but are happy to add it if the reviewer wishes.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.12215.017</object-id><label>Author response image 2.</label><caption><title>Probability density of saccade angles.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.12215.017">http://dx.doi.org/10.7554/eLife.12215.017</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-12215-resp-fig2-v3"/></fig></p><p><italic>2) Since there were a lot of incorrect judgments overall, it would be interesting to compare the density maps for correct and incorrect judgments. If eye movements are indeed controlled by an active strategy, the differences and correlations should be more pronounced for correct judgments.</italic> </p><p>Thank you for this great suggestion. We have now performed this analysis and below are the density maps for correct and incorrect trials (averaged across participants, new <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). We chose to analyze only for the “average” participant as the number of incorrect trials is limited for each participant. The correlation with BAS is higher for correct compared to incorrect trials (average of 0.35 vs 0.01; p&lt;0.05). We have added this into the revised manuscript.</p><p><italic>3) The example trial in <xref ref-type="fig" rid="fig4">Figure 4</xref> provides a good illustration of BAS and the maximum entropy variant but it does not allow a quantitative comparison. It would be helpful to show the full distributions of percentile values with respect to BAS and entropy scores.</italic> </p><p>We have computed the distribution of score percentiles across revealings for the two sensing algorithms. The result makes it clear that the BAS algorithm is a much better description for human eye movement than the MaxEnt algorithm. We have added this graph as a subpanel of <xref ref-type="fig" rid="fig4">Figure 4</xref> in the revision and mentioned this in the text.</p><p><italic>4) As I understand the task, the revealings stayed on screen after the last revealing for 60 s or an unlimited duration, depending on the condition. The behavior of the subjects after the last revealing should be reported because it could have influenced their perceptual judgment.</italic> </p><p>As this point was raised by all three reviewers and highlighted as the key reason the paper was rejected, we have provided a full response to this issue in the response to the editors above.</p></body></sub-article></article>