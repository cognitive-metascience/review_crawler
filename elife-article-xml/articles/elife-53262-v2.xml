<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">53262</article-id><article-id pub-id-type="doi">10.7554/eLife.53262</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Dopamine role in learning and action inference</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-10626"><name><surname>Bogacz</surname><given-names>Rafal</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8994-1661</contrib-id><email>rafal.bogacz@ndcn.ox.ac.uk</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution>MRC Brain Networks Dynamics Unit, University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Wassum</surname><given-names>Kate M</given-names></name><role>Senior Editor</role><aff><institution>University of California, Los Angeles</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>07</day><month>07</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e53262</elocation-id><history><date date-type="received" iso-8601-date="2019-11-01"><day>01</day><month>11</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-07-06"><day>06</day><month>07</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Bogacz</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Bogacz</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-53262-v2.pdf"/><abstract><p>This paper describes a framework for modelling dopamine function in the mammalian brain. It proposes that both learning and action planning involve processes minimizing prediction errors encoded by dopaminergic neurons. In this framework, dopaminergic neurons projecting to different parts of the striatum encode errors in predictions made by the corresponding systems within the basal ganglia. The dopaminergic neurons encode differences between rewards and expectations in the goal-directed system, and differences between the chosen and habitual actions in the habit system. These prediction errors trigger learning about rewards and habit formation, respectively. Additionally, dopaminergic neurons in the goal-directed system play a key role in action planning: They compute the difference between a desired reward and the reward expected from the current motor plan, and they facilitate action planning until this difference diminishes. Presented models account for dopaminergic responses during movements, effects of dopamine depletion on behaviour, and make several experimental predictions.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>In the brain, chemicals such as dopamine allow nerve cells to ‘talk’ to each other and to relay information from and to the environment. Dopamine, in particular, is released when pleasant surprises are experienced: this helps the organism to learn about the consequences of certain actions. If a new flavour of ice-cream tastes better than expected, for example, the release of dopamine tells the brain that this flavour is worth choosing again.</p><p>However, dopamine has an additional role in controlling movement. When the cells that produce dopamine die, for instance in Parkinson’s disease, individuals may find it difficult to initiate deliberate movements. Here, Rafal Bogacz aimed to develop a comprehensive framework that could reconcile the two seemingly unrelated roles played by dopamine.</p><p>The new theory proposes that dopamine is released when an outcome differs from expectations, which helps the organism to adjust and minimise these differences. In the ice-cream example, the difference is between how good the treat is expected to taste, and how tasty it really is. By learning to select the same flavour repeatedly, the brain aligns expectation and the result of the choice. This ability would also apply when movements are planned. In this case, the brain compares the desired reward with the predicted results of the planned actions. For example, while planning to get a spoonful of ice-cream, the brain compares the pleasure expected from the movement that is currently planned, and the pleasure of eating a full spoon of the treat. If the two differ, for example because no movement has been planned yet, the brain releases dopamine to form a better version of the action plan. The theory was then tested using a computer simulation of nerve cells that release dopamine; this showed that the behaviour of the virtual cells closely matched that of their real-life counterparts.</p><p>This work offers a comprehensive description of the fundamental role of dopamine in the brain. The model now needs to be verified through experiments on living nerve cells; ultimately, it could help doctors and researchers to develop better treatments for conditions such as Parkinson’s disease or ADHD, which are linked to a lack of dopamine.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>dopamine</kwd><kwd>reinforcement learning</kwd><kwd>active inference</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Mouse</kwd><kwd>Rat</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MC_UU_12024/5</award-id><principal-award-recipient><name><surname>Bogacz</surname><given-names>Rafal</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MC_UU_00003/1</award-id><principal-award-recipient><name><surname>Bogacz</surname><given-names>Rafal</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/S006338/1</award-id><principal-award-recipient><name><surname>Bogacz</surname><given-names>Rafal</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A mathematical model describes the function of dopaminergic neurons in both learning and action planning.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neurons releasing dopamine send widespread projections to many brain regions, including basal ganglia and cortex (<xref ref-type="bibr" rid="bib6">Björklund and Dunnett, 2007</xref>), and substantially modulate information processing in the target areas. Dopaminergic neurons in the ventral tegmental area respond to unexpected rewards (<xref ref-type="bibr" rid="bib67">Schultz et al., 1997</xref>), and hence it has been proposed that they encode reward prediction error, defined as the difference between obtained and expected reward (<xref ref-type="bibr" rid="bib35">Houk et al., 1995</xref>; <xref ref-type="bibr" rid="bib56">Montague et al., 1996</xref>). According to the classical reinforcement learning theory, this prediction error triggers update of the estimates of expected rewards encoded in striatum. Indeed, it has been observed that dopaminergic activity modulates synaptic plasticity in the striatum in a way predicted by the theory (<xref ref-type="bibr" rid="bib62">Reynolds et al., 2001</xref>; <xref ref-type="bibr" rid="bib68">Shen et al., 2008</xref>). This classical reinforcement learning theory of dopamine has been one of the greatest successes of computational neuroscience, as the predicted patterns of dopaminergic activity have been seen in diverse studies in multiple species (<xref ref-type="bibr" rid="bib23">Eshel et al., 2016</xref>; <xref ref-type="bibr" rid="bib75">Tobler et al., 2005</xref>; <xref ref-type="bibr" rid="bib81">Zaghloul et al., 2009</xref>).</p><p>However, this classical theory does not account for the important role of dopamine in action planning. This role is evident from the difficulties in initiation of voluntary movements seen after the death of dopaminergic neurons in Parkinson’s disease. This role is consistent with the diversity in the activity of dopaminergic neurons, with many of them responding to movements (<xref ref-type="bibr" rid="bib13">da Silva et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Dodson et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">Howe and Dombeck, 2016</xref>; <xref ref-type="bibr" rid="bib40">Jin and Costa, 2010</xref>; <xref ref-type="bibr" rid="bib48">Lee et al., 2019</xref>; <xref ref-type="bibr" rid="bib65">Schultz et al., 1983</xref>; <xref ref-type="bibr" rid="bib72">Syed et al., 2016</xref>). The function of dopamine in energizing movements is likely to come from the effects it has on the excitability or gain of the target neurons (<xref ref-type="bibr" rid="bib47">Lahiri and Bevan, 2020</xref>; <xref ref-type="bibr" rid="bib74">Thurley et al., 2008</xref>). Understanding the role of dopamine in action planning and movement initiation is important for refining treatments for Parkinson’s disease, where the symptoms are caused by dopamine depletion.</p><p>A foundation for a framework accounting the role of dopamine in both learning and action planning may be provided by a theory called active inference (<xref ref-type="bibr" rid="bib29">Friston, 2010</xref>). This theory relies on an assumption that the brain attempts to minimize prediction errors defined as the differences between observed stimuli and expectations. In active inference, these prediction errors can be minimized in two ways: through learning – by updating expectations to match stimuli, and through action – by changing the world to match the expectations. According to the active inference theory, prediction errors may need to be minimized by actions, because the brain maintains prior expectations that are necessary for survival and so cannot be overwritten by learning, e.g. an expectation that food reserves should be at a certain level. When such predictions are not satisfied, the brain plans actions to reduce the corresponding prediction errors, for example by finding food.</p><p>This paper suggests that a more complete description of dopamine function can be gained by integrating reinforcement learning with elements of three more recent theories. First, taking inspiration from active inference, we propose that prediction errors represented by dopaminergic neurons are minimized by both learning and action planning, which gives rise to the roles of dopamine in both these processes. Second, we incorporate a recent theory of habit formation, which suggests that the habit and goal-directed systems learn on the basis of distinct prediction errors (<xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref>), and we propose that these prediction errors are encoded by distinct populations of dopaminergic neurons, giving rise to the observed diversity of their responses. Third, we assume that the most appropriate actions are identified through Bayesian inference (<xref ref-type="bibr" rid="bib69">Solway and Botvinick, 2012</xref>), and present a mathematical framework describing how this inference can be physically implemented in anatomically identified networks within the basal ganglia. Since the framework extends the description of dopamine function to action planning, we refer to it as the DopAct framework. The DopAct framework accounts for a wide range of experimental data including the diversity of dopaminergic responses, the difficulties in initiation of voluntary movements under dopamine depletion, and it makes several experimentally testable predictions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To provide an intuition for the DopAct framework, we start with giving its overview. Next, we formalize the framework, and show examples of models developed within it for two tasks commonly used in experimental studies of reinforcement learning and habit formation: selection of action intensity (such as frequency of lever pressing) and choice between two actions.</p><sec id="s2-1"><title>Overview of the framework</title><p>This section first gives an overview of computations taking place during action planning in the DopAct framework, and then summarizes how these computations could be implemented in neural circuits including dopaminergic neurons.</p><p>The DopAct framework includes two components contributing to planning of behaviour. The first component is a valuation system, which finds the value <inline-formula><mml:math id="inf1"><mml:mi>v</mml:mi></mml:math></inline-formula> of reward that the animal should aim at acquiring in a given situation. A situation of an animal can be described by two classes of factors: internal factors connected with level of reserves such as food, water, etc. to which we refer as ‘reserves’, and external factors related to the environment, such as stimuli or locations in space, to which we refer as a ‘state’ following reinforcement learning terminology. The value <inline-formula><mml:math id="inf2"><mml:mi>v</mml:mi></mml:math></inline-formula> depends on both the amount of reward available in state <inline-formula><mml:math id="inf3"><mml:mi>s</mml:mi></mml:math></inline-formula>, and the current level of reserves. For example, if animal is not hungry, the desired value is equal to <inline-formula><mml:math id="inf4"><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> even if food is available. The second component of the DopAct framework is an actor, which selects an action to obtain the desired reward. This paper focusses on describing computations in the actor. Thus, for simplicity, we assume that the valuation system is able to compute the value <inline-formula><mml:math id="inf5"><mml:mi>v</mml:mi></mml:math></inline-formula>, but this paper does not describe how that computation is performed. In simulations we mostly focus on a case of low reserves, and use a simple model similar to a critic in standard reinforcement learning, which just learns the average value <inline-formula><mml:math id="inf6"><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> of resource in state <inline-formula><mml:math id="inf7"><mml:mi>s</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib71">Sutton and Barto, 1998</xref>). Extending the description of the valuation system will be an important direction for future work and we come back to it in Discussion.</p><p>The goal of the actor is to select an action to obtain the reward set by the valuation system. This action is selected through inference in a probabilistic model, which describes relationships between states, actions and rewards, which we denote by <inline-formula><mml:math id="inf8"><mml:mi>s</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf9"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"> <mml:mi/><mml:mi>R</mml:mi></mml:math></inline-formula>. Following reinforcement learning convention, we use <inline-formula><mml:math id="inf11"><mml:mi>R</mml:mi></mml:math></inline-formula> to denote the total reward defined in Equation 1.1 of <xref ref-type="fig" rid="fig1">Figure 1A</xref>, which includes the current reward <inline-formula><mml:math id="inf12"><mml:mi>r</mml:mi></mml:math></inline-formula>, and the future reward value <inline-formula><mml:math id="inf13"><mml:mi>v</mml:mi></mml:math></inline-formula> computed by the valuation system. The DopAct framework assumes that two systems within the actor learn distinct relationships between the variables, shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. The first system, shown in orange, learns how the reward depends on the action selected in a given state, and we refer to it as ‘goal-directed’, because it can infer actions that typically lead to the desired reward. The second system, in blue, learns which actions should generally be chosen in a given state, and we refer to it as ‘habit’, because it suggests actions without considering the value of the reward currently available. Both goal-directed and habit systems propose an action, and their influence depends on their relative certainty.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of systems within the DopAct framework.</title><p>(<bold>A</bold>) Probabilistic model learned by the actor. Random variables are indicated by circles, and arrows denote dependencies learned by different systems. (<bold>B</bold>) Schematic overview of information processing in the framework at different stages of task acquisition. (<bold>C</bold>) Mapping of the systems on different parts of the cortico-basal ganglia network. Circles correspond to neural populations located in the regions indicated by labels to the left, where ‘Striatum’ denotes medium spiny neurons expressing D1 receptors, ‘GABA’ denotes inhibitory neurons located in vicinity of dopaminergic neurons, and ‘Reward’ denotes neurons providing information on the magnitude of instantaneous reward. Arrows denote excitatory projections, while lines ending with circles denote inhibitory projections. (<bold>D</bold>) Schematic illustration of the mechanism of habit formation. Notation as in panel C, but additionally shading indicates the level of activity, and thickness of lines indicates the strength of synaptic connections.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig1-v2.tif"/></fig><p><xref ref-type="fig" rid="fig1">Figure 1B</xref> gives an overview of how the systems mentioned above contribute to action planning, in a typical task. During initial trials, the valuation system (shown in red) evaluates the current state <inline-formula><mml:math id="inf14"><mml:mi>s</mml:mi></mml:math></inline-formula> and computes the value of desired reward <inline-formula><mml:math id="inf15"><mml:mi>v</mml:mi></mml:math></inline-formula>, and the goal-directed system selects the action <inline-formula><mml:math id="inf16"><mml:mi>a</mml:mi></mml:math></inline-formula>. At this stage the habit system contributes little to the planning process as its uncertainty is high. As the training progresses, the habit system learns to mimic the choices made by the goal-directed system (<xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref>). On later trials the action is jointly determined by the habit and goal-directed systems (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), and their relative contributions depend on their levels of certainty.</p><p>The details of the above computations in the framework will be described in the next section, and it will be later shown how an algorithm inferring action can be implemented in a network resembling the anatomy of the basal ganglia. But before going through a mathematical description, let us first provide an overview of this implementation (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In this implementation, the valuation, goal-directed and habit systems are mapped on the spectrum of cortico-basal ganglia loops (<xref ref-type="bibr" rid="bib1">Alexander et al., 1986</xref>), ranging from valuation in a loop including ventral striatum, to habit in a loop including the dorsolateral striatum that has been shown to be critical for habitual behaviour (<xref ref-type="bibr" rid="bib10">Burton et al., 2015</xref>). In the DopAct framework, the probability distributions learned by the actor are encoded in the strengths of synaptic connections in the corresponding loops, primarily in cortico-striatal connections. As in a standard implementation of the critic (<xref ref-type="bibr" rid="bib35">Houk et al., 1995</xref>), the parameters of the value function learned by the valuation system are encoded in cortico-striatal connections of the corresponding loop.</p><p>Analogous to classical reinforcement learning theory, dopaminergic neurons play a critical role in learning, and encode errors in predictions made by the systems in the DopAct framework. However, by contrast to the standard theory, dopaminergic neurons do not all encode the same signal, but instead dopaminergic populations in different systems compute errors in predictions made by their corresponding system. Since both valuation and goal-directed systems learn to predict reward, the dopaminergic neurons in these systems encode reward prediction errors (which slightly differ between these two systems, as will be illustrated in simulations presented later). By contrast, the habit system learns to predict action on the basis of a state, so its prediction error encodes how the currently chosen action differs from a habitual action in the given state. Thus these dopaminergic neurons respond to non-habitual actions in the DopAct framework. We denote the prediction errors in the valuation, goal-directed and habit systems by <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively. The dopaminergic neurons send these prediction errors to the striatum, where they trigger plasticity of cortico-striatal connections.</p><p>In the DopAct framework, habits are formed through a process in which the habit system learns to mimic the goal-directed system. Unlike in a previous model of habit formation (<xref ref-type="bibr" rid="bib14">Daw et al., 2005</xref>), in the DopAct framework learning in the habit system is not driven by a reward prediction error, but by a signal encoding a difference between chosen and habitual actions. At the start of training, when an action is selected mostly by the goal-directed system, the dopaminergic neurons in the habit system receive an input encoding the chosen action, but the striatal neurons in the habit system are not yet able to predict this action, resulting in a prediction error encoded in dopaminergic activity (left display in <xref ref-type="fig" rid="fig1">Figure 1D</xref>). This prediction error triggers plasticity in the striatal neurons of the habit system, so they tend to predict this action in the future (right display in <xref ref-type="fig" rid="fig1">Figure 1D</xref>).</p><p>The systems communicate through an ‘ascending spiral’ structure of striato-dopaminergic projections identified by <xref ref-type="bibr" rid="bib33">Haber et al., 2000</xref>. These Authors observed that dopaminergic neurons within a given loop project to the corresponding striatal neurons, while the striatal neurons project to the dopaminergic neurons in the corresponding and next loops, and they proposed that the projections to the next loop go via interneurons, so they are effectively excitatory (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In the DopAct framework, once the striatal neurons in the valuation system compute the value of the state <inline-formula><mml:math id="inf20"><mml:mi>v</mml:mi></mml:math></inline-formula>, they send it to the dopaminergic neurons in the goal-directed system.</p><p>In the DopAct framework, dopamine in the goal-directed system plays a role in both action planning and learning, and now an overview of this role is given. In agreement with classical reinforcement learning theory, the dopaminergic activity <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> encodes reward prediction error, namely the difference between the reward <inline-formula><mml:math id="inf22"><mml:mi>R</mml:mi></mml:math></inline-formula> (including both obtained and available reward) and the expected reward (<xref ref-type="bibr" rid="bib67">Schultz et al., 1997</xref>), but in the DopAct framework the expectation of reward in the goal-directed system is computed on the basis of the current action plan. Therefore, this reward expectation only arises from formulating a plan to achieve it. Consequently, when a reward is available, the prediction error <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can only be reduced to zero, once a plan to obtain the reward is formulated.</p><p>To gain an intuition for how the goal-directed system operates, let us consider a simple example of a hungry rat in a standard operant conditioning experiment. Assume that the rat has been trained that after pressing a lever a food pellet is delivered (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Consider a situation in which a lever is suddenly made available to the animal. Its sight allows the valuation system to predict that reward is available, and it sends an estimated value of the reward to the goal-directed system. Such input induces a reward prediction error in the goal-directed system, because this system has received information that a reward is available, but has not yet prepared actions to obtain the reward, hence it does not expect any reward for its action. The resulting prediction error triggers a process of planning actions that can get the reward. This facilitation of planning arises in the network, because the dopaminergic neurons in the goal-directed system project to striatal neurons (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), and increase their excitability. Once an appropriate action has been computed, the animal starts to expect the available reward, and the dopamine level encoding the prediction error decreases. Importantly, in this network dopamine provides a crucial feedback to striatal neurons on whether the current action plan is sufficient to obtain the available reward. If it is not, this feedback triggers changes in the action plan until it becomes appropriate. Thus the framework suggests why it is useful for the neurons encoding reward prediction error to be involved in planning, namely it suggests that this prediction error provides a useful feedback for the action planning system, informing if the plan is suitable to obtain the reward.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Schematic illustration of changes in dopaminergic activity in the goal-directed system while a hungry rat presses a lever and a food pellet is delivered.</title><p>(<bold>A</bold>) Prediction error reduced by action planning. The prediction error encoded in dopamine (bottom trace) is equal to a difference between the reward available (top trace) and the expectation of reward arising from a plan to obtain it (middle trace). (<bold>B</bold>) Prediction errors reduced by both action planning and learning.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig2-v2.tif"/></fig><p>It is worth explaining why the reward expectation in the goal-directed system arises already once an action is computed and before it is implemented. It happens in the DopAct framework, because the striatal neurons in the goal-directed system learn over trials to predict that particular pattern of activity of neurons encoding action in the basal ganglia (which subsequently triggers a motor response) leads to reward in the future. This mechanism is fully analogous to that in the temporal-difference learning model used to describe classical conditioning, where the reward expectation also arises already after a stimulus, because the striatal neurons learn that the pattern of cortical inputs to the basal ganglia encoding the state (i.e. the stimulus) will lead to a reward (<xref ref-type="bibr" rid="bib67">Schultz et al., 1997</xref>). In the goal-directed system of DopAct, an analogous reward prediction is made, but not only on the basis of a state, but on the basis of a combination of state and action.</p><p>The prediction error in the goal-directed system also allows the animal to learn about the rewards resulting from actions. In the example we considered above such learning would be necessary if the amount of reward changed, for example to two pellets (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). On the first trial after such change, a prediction error will be produced after reward delivery. This prediction error can be reduced by learning, so the animal will expect such increased reward in the future trials and no longer produce prediction error at reward delivery. In summary, the prediction errors in the goal-directed system are reduced by both planning and learning, as in active inference (<xref ref-type="bibr" rid="bib29">Friston, 2010</xref>). Namely, the prediction errors arising from rewards becoming available are reduced within trials by formulating plans to obtain them, and the prediction errors due to outcomes of actions differing from expectations are reduced across trials by changing weights of synaptic connection encoding expected reward.</p><p>The next three sections will provide the details of the DopAct framework. For clarity, we will follow Marr’s levels of description, and discuss computations, an algorithm, and its implementation in the basal ganglia network.</p></sec><sec id="s2-2"><title>Computations during planning and learning</title><p>To illustrate the computations in the framework we will consider a simple task, in which only an intensity of a single action needs to be chosen. Such choice has to be made by animals in classical experiments investigating habit formation, where the animals are offered a single lever, and need to decide how frequently to press it. Furthermore, action intensity often needs to be chosen by animals also in the wild (e.g. a tiger deciding how vigorously pounce on a prey, a chimpanzee choosing how strongly hit a nut with a stone, or a sheep selecting how quickly eat the grass). Let us denote the action intensity by <inline-formula><mml:math id="inf24"><mml:mi>a</mml:mi></mml:math></inline-formula>. Let us assume that the animal chooses it on the basis of the reward it expects <inline-formula><mml:math id="inf25"><mml:mi>R</mml:mi></mml:math></inline-formula> and the stimulus <inline-formula><mml:math id="inf26"><mml:mi>s</mml:mi></mml:math></inline-formula> (e.g. the size of prey, nut or grass). Thus the animal needs to infer an action intensity sufficient to obtain the desired reward (but not larger to avoid unnecessary effort).</p><p>Let us consider the computation in the DopAct framework during action planning. During planning, the animal has not received any reward yet <inline-formula><mml:math id="inf27"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, so according to Equation 1.1, the total reward is equal to the reward available <inline-formula><mml:math id="inf28"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>v</mml:mi></mml:math></inline-formula>. While planning to obtain this reward, the actor combines information from the goal-directed system (encoding how the reward depends on actions taken in given states), and the habit system (encoding the probability distribution of generally selecting actions in particular states). These two pieces information are combined according to Bayes’ theorem (Equation 3.1 in <xref ref-type="fig" rid="fig3">Figure 3</xref>), which states that the posterior probability of selecting a particular action given available reward is proportional to the product of a likelihood of the reward given the action, which we propose is represented in the goal-directed system, and a prior, which we propose is encoded by the habit system.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Computational level.</title><p>(<bold>A</bold>) Summary of computations performed by the actor. (<bold>B</bold>) Sample form of probability distributions. (<bold>C</bold>) An example of inference of action intensity. In this example the stimulus intensity is equal to <inline-formula><mml:math id="inf29"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, the valuation system computes desired reward <inline-formula><mml:math id="inf30"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>, and the parameters of the probability distributions encoded in the goal-directed and habit systems are listed in the panel. The blue curve shows the distribution of action intensity, which the habit system has learned to be generally suitable for this stimulus. The orange curve shows probability density of obtaining reward of 2 for a given action intensity, and this probability is estimated by the goal-directed system. For the chosen parameters, it is the probability of obtaining 2 from a normal distribution with mean <inline-formula><mml:math id="inf31"><mml:mi>a</mml:mi></mml:math></inline-formula>. Finally, the green curve shows a posterior distribution computed from Equation 3.1.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig3-v2.tif"/></fig><p>In the DopAct framework, an action <inline-formula><mml:math id="inf32"><mml:mi>a</mml:mi></mml:math></inline-formula> is selected which maximizes the probability <inline-formula><mml:math id="inf33"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. An analogous way of selecting actions has been used in models treating planning as inference (<xref ref-type="bibr" rid="bib2">Attias, 2003</xref>), and it has been nicely summarized by <xref ref-type="bibr" rid="bib69">Solway and Botvinick, 2012</xref> 'The decision process takes the occurrence of reward as a premise, and leverages the generative model to determine which course of action best explains the observation of reward.' In this paper, we make explicit the rationale for this approach: The desired amount of resources that should be acquired depends on the levels of reserves (and a given state); this value is computed by the valuation system, and the actor needs to find the action depending on this reward. Let us provide a further rationale for selecting an action <inline-formula><mml:math id="inf34"><mml:mi>a</mml:mi></mml:math></inline-formula> which maximizes <inline-formula><mml:math id="inf35"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, by analysing what this probability expresses. Let us consider the following hypothetical scenario: An animal selected an action without considering the desired reward, that is by sampling it from its default policy <inline-formula><mml:math id="inf36"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> provided by the habit system, and obtained reward <inline-formula><mml:math id="inf37"><mml:mi>R</mml:mi></mml:math></inline-formula>. In this case, <inline-formula><mml:math id="inf38"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the probability that the selected action was <inline-formula><mml:math id="inf39"><mml:mi>a</mml:mi></mml:math></inline-formula>. When an animal knows the amount of resource desired <inline-formula><mml:math id="inf40"><mml:mi>R</mml:mi></mml:math></inline-formula>, then instead of just relying on the prior, the animal should rather choose an action maximizing <inline-formula><mml:math id="inf41"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, which was the action most likely to yield this reward in the above scenario.</p><p>One may ask why it is useful to employ the habit system, instead of exclusively relying on the goal-directed system that encodes the relationship between rewards and actions. It is because there may be uncertainty in the action suggested by the goal-directed system, arising for example, from noise in the computations of the valuation system or inaccurate estimates of the parameters of the goal-directed system. According to Bayesian philosophy, in face of such uncertainty, it is useful to additionally bias the action by a prior, which here is provided by the habit system. This prior encodes an action policy that has overall worked in the situations previously experienced by the animal, so it is a useful policy to consider under the uncertainty in the goal-directed system.</p><p>To make the above computation more concrete, we need to specify the form of the prior and likelihood distributions. We first provide them for the example of choosing action intensity. They are given in <xref ref-type="fig" rid="fig3">Figure 3B</xref>, where <inline-formula><mml:math id="inf42"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> denotes the probability density of a normal distribution with mean <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf44"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula>. In a case of the prior, we assume that action intensity is normally distributed around a mean given by stimulus intensity scaled by parameter <inline-formula><mml:math id="inf45"><mml:mi>h</mml:mi></mml:math></inline-formula>, reflecting an assumption that a typical action intensity often depends on a stimulus (e.g. the larger a nut, the harder a chimpanzee must hit it). On the other hand, in a case of the probability of reward <inline-formula><mml:math id="inf46"><mml:mi>R</mml:mi></mml:math></inline-formula> maintained by the goal-directed system, the mean of the reward is equal to a product of action intensity and the stimulus size, scaled by parameter <inline-formula><mml:math id="inf47"><mml:mi>q</mml:mi></mml:math></inline-formula>. We assume that the mean reward depends on a product of <inline-formula><mml:math id="inf48"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf49"><mml:mi>s</mml:mi></mml:math></inline-formula> for three reasons. First, in many situations reward depends jointly on the size of the stimulus, and the intensity with which the action is taken, because if the action is too weak, the reward may not be obtained (e.g. a prey may escape or a nut may not crack), and the product captures this dependence of reward on a conjunction of stimulus and action. Second, in many foraging situations, the reward that can be obtained within a period of time is proportional to a product of <inline-formula><mml:math id="inf50"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf51"><mml:mi>s</mml:mi></mml:math></inline-formula> (e.g. the amount of grass eaten by a sheep is proportional to both how quickly the sheep eats it, and how high the grass is). Third, when the framework is generalized to multiple actions later in the paper, the assumption of reward being proportional to a product of <inline-formula><mml:math id="inf52"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf53"><mml:mi>s</mml:mi></mml:math></inline-formula> will highlight a link with classical reinforcement learning. We denote the variances of the distributions of the goal-directed and habit systems by <inline-formula><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The variance <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> quantifies to what extent the obtained rewards have differed from those predicted by the goal-directed system, while the variance <inline-formula><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> describes by how much the chosen actions have differed from the habitual actions.</p><p><xref ref-type="fig" rid="fig3">Figure 3C</xref> shows an example of probability distributions encoded by the two systems for sample parameters. It also shows a posterior distribution <inline-formula><mml:math id="inf58"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, and please note that its peak is in between the peaks of the distributions of the two systems, but it is closer to the peak of a system with smaller uncertainty (orange distribution is narrower). This illustrates how in the DopAct framework, the action is inferred by incorporating information from both systems, but weighting it by the certainty of the systems.</p><p>In addition to action planning, the animal needs to learn from the outcomes, to predict rewards more accurately in the future. After observing an outcome, the valuation system no longer predicts future reward <inline-formula><mml:math id="inf59"><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, so according to Equation 1.1 the total reward is equal to the reward actually obtained <inline-formula><mml:math id="inf60"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi></mml:math></inline-formula>. The parameters of the distributions should be updated to increase <inline-formula><mml:math id="inf61"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, so in the future the animal is less surprised by the reward obtained in that state (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p></sec><sec id="s2-3"><title>Algorithm for planning and learning</title><p>Let us describe an algorithm used by the actor to infer action intensity <inline-formula><mml:math id="inf62"><mml:mi>a</mml:mi></mml:math></inline-formula> that maximizes the posterior probability <inline-formula><mml:math id="inf63"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. This posterior probability could be computed from Equation 3.1, but note that <inline-formula><mml:math id="inf64"><mml:mi>a</mml:mi></mml:math></inline-formula> does not occur in the denominator of that equation, so we can simply find the action that maximizes the numerator. Hence, we define an objective function <inline-formula><mml:math id="inf65"><mml:mi>F</mml:mi></mml:math></inline-formula> equal to a logarithm of the numerator of Bayes’ theorem (Equation 4.1 in <xref ref-type="fig" rid="fig4">Figure 4</xref>). Introducing the logarithm will simplify function <inline-formula><mml:math id="inf66"><mml:mi>F</mml:mi></mml:math></inline-formula> because it will cancel with exponents present in the definition of normal density (Equation 3.3), and it does not change the position of the maximum of the numerator because the logarithm is a monotonic function. For example, the green curve in <xref ref-type="fig" rid="fig4">Figure 4B</xref> shows function <inline-formula><mml:math id="inf67"><mml:mi>F</mml:mi></mml:math></inline-formula> corresponding to the posterior probability in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. Both green curves have the maximum at the same point, so instead of searching for a maximum of a posterior probability, we can seek the maximum of a simpler function <inline-formula><mml:math id="inf68"><mml:mi>F</mml:mi></mml:math></inline-formula>.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Algorithmic level.</title><p>(<bold>A</bold>) Summary of the algorithm used by the actor. (<bold>B</bold>) Identifying an action based on a gradient of <inline-formula><mml:math id="inf69"><mml:mi>F</mml:mi></mml:math></inline-formula>. The panel shows an example of a dependence of <inline-formula><mml:math id="inf70"><mml:mi>F</mml:mi></mml:math></inline-formula> on <inline-formula><mml:math id="inf71"><mml:mi>a</mml:mi></mml:math></inline-formula>, and we wish <inline-formula><mml:math id="inf72"><mml:mi>a</mml:mi></mml:math></inline-formula> to take the value maximizing <inline-formula><mml:math id="inf73"><mml:mi>F</mml:mi></mml:math></inline-formula>. To find the action, we let <inline-formula><mml:math id="inf74"><mml:mi>a</mml:mi></mml:math></inline-formula> to change over time in proportion to the gradient of <inline-formula><mml:math id="inf75"><mml:mi>F</mml:mi></mml:math></inline-formula> over <inline-formula><mml:math id="inf76"><mml:mi>a</mml:mi></mml:math></inline-formula> (Equation 4.2, where the dot over <inline-formula><mml:math id="inf77"><mml:mi>a</mml:mi></mml:math></inline-formula> denotes derivative over time). For example, if the action is initialized to <inline-formula><mml:math id="inf78"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>.5, then the gradient of <inline-formula><mml:math id="inf79"><mml:mi>F</mml:mi></mml:math></inline-formula> at this point is positive, so <inline-formula><mml:math id="inf80"><mml:mi>a</mml:mi></mml:math></inline-formula> is increased (<bold>Equation 4.2</bold>), as indicated by a green arrow on the x-axis. These changes in <inline-formula><mml:math id="inf81"><mml:mi>a</mml:mi></mml:math></inline-formula> continue until the gradient is no longer positive, i.e. when <inline-formula><mml:math id="inf82"><mml:mi>a</mml:mi></mml:math></inline-formula> is at the maximum. Analogously, if the action is initialized to <inline-formula><mml:math id="inf83"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>3.5</mml:mn></mml:math></inline-formula>, then the gradient of <inline-formula><mml:math id="inf84"><mml:mi>F</mml:mi></mml:math></inline-formula> is negative, so <inline-formula><mml:math id="inf85"><mml:mi>a</mml:mi></mml:math></inline-formula> is decreased until it reaches the maximum of <inline-formula><mml:math id="inf86"><mml:mi>F</mml:mi></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig4-v2.tif"/></fig><p>During action planning the total reward is equal to reward available, so we set <inline-formula><mml:math id="inf87"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>v</mml:mi></mml:math></inline-formula> in Equation 4.1, and we find the action maximizing <inline-formula><mml:math id="inf88"><mml:mi>F</mml:mi></mml:math></inline-formula>. This can be achieved by initializing <inline-formula><mml:math id="inf89"><mml:mi>a</mml:mi></mml:math></inline-formula> to any value, and then changing it proportionally to the gradient of <inline-formula><mml:math id="inf90"><mml:mi>F</mml:mi></mml:math></inline-formula> (Equation 4.2). <xref ref-type="fig" rid="fig4">Figure 4B</xref> illustrates that with such dynamics, the value of <inline-formula><mml:math id="inf91"><mml:mi>a</mml:mi></mml:math></inline-formula> approaches a maximum of <inline-formula><mml:math id="inf92"><mml:mi>F</mml:mi></mml:math></inline-formula>. Once <inline-formula><mml:math id="inf93"><mml:mi>a</mml:mi></mml:math></inline-formula> converges, the animal may select the action with the corresponding intensity. In summary, this method yields a differential equation describing an evolution of a variable <inline-formula><mml:math id="inf94"><mml:mi>a</mml:mi></mml:math></inline-formula>, which converges to a value of <inline-formula><mml:math id="inf95"><mml:mi>a</mml:mi></mml:math></inline-formula> that maximizes <inline-formula><mml:math id="inf96"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>.</p><p>After obtaining a reward, <inline-formula><mml:math id="inf97"><mml:mi>R</mml:mi></mml:math></inline-formula> is equal to the reward obtained, so we set <inline-formula><mml:math id="inf98"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi></mml:math></inline-formula> in Equation 4.1, and the values of parameters are changed proportionally to the gradient of <inline-formula><mml:math id="inf99"><mml:mi>F</mml:mi></mml:math></inline-formula> (Equations 4.3). Such parameter updates allow the model to be less surprised by the rewards (as aimed for in <xref ref-type="fig" rid="fig3">Figure 3A</xref>), because under certain assumptions function <inline-formula><mml:math id="inf100"><mml:mi>F</mml:mi></mml:math></inline-formula> expresses 'negative free energy'. The negative free energy (for the inference problem considered in this paper) is defined as <inline-formula><mml:math id="inf101"><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf102"><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:math></inline-formula> is the Kullback-Leibler divergence between <inline-formula><mml:math id="inf103"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and an estimate of this distribution (a detailed definition and an explanation for why <inline-formula><mml:math id="inf104"><mml:mi>F</mml:mi></mml:math></inline-formula> given in Equation 4.1 expresses negative free energy for an analogous problem is given by <xref ref-type="bibr" rid="bib7">Bogacz, 2017</xref>). Importantly, since <inline-formula><mml:math id="inf105"><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, the negative free energy provides a lower bound on <inline-formula><mml:math id="inf106"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib28">Friston, 2005</xref>). Thus changing the parameters to increase <inline-formula><mml:math id="inf107"><mml:mi>F</mml:mi></mml:math></inline-formula>, rises the lower bound on <inline-formula><mml:math id="inf108"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, and so it tends to increase <inline-formula><mml:math id="inf109"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>.</p><p>Let us derive the details of the algorithm (general form of which is given in <xref ref-type="fig" rid="fig4">Figure 4A</xref>) for the problem of choosing action intensity. Let us start with considering a special case in which both variance parameters are fixed to <inline-formula><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, because then the form of the algorithm and its mapping on the network are particularly beautiful. Substituting probability densities of likelihood and prior distributions (Equations 3.2-3.3) for the case of unit variances into Equation 4.1 (and ignoring constants <inline-formula><mml:math id="inf111"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>), we obtain the expression for the objective function <inline-formula><mml:math id="inf112"><mml:mi>F</mml:mi></mml:math></inline-formula> in Equation 5.1 (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We see that <inline-formula><mml:math id="inf113"><mml:mi>F</mml:mi></mml:math></inline-formula> consists of two terms, which are the squared prediction errors associated with goal-directed and habit systems. The prediction error for the goal-directed system describes how the reward differs from the expected mean, while the prediction error of the habit system expresses how the chosen action differs from that typically chosen in the current state (Equations 5.2). As described in the previous section, action intensity can be found by changing its value according to a gradient of <inline-formula><mml:math id="inf114"><mml:mi>F</mml:mi></mml:math></inline-formula> (Equation 4.2). Computing the derivative of <inline-formula><mml:math id="inf115"><mml:mi>F</mml:mi></mml:math></inline-formula> over <inline-formula><mml:math id="inf116"><mml:mi>a</mml:mi></mml:math></inline-formula>, we obtain Equation 5.3, where the two colours indicate terms connected with derivatives of the corresponding prediction errors. Finally, when the reward is obtained, we modify the parameters proportionally to the derivatives of <inline-formula><mml:math id="inf117"><mml:mi>F</mml:mi></mml:math></inline-formula> over the parameters, which are equal to relatively simple expressions in Equations 5.4.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Description of a model selecting action intensity, in a case of unit variances.</title><p>(<bold>A</bold>) Details of the algorithm. (<bold>B</bold>) Mapping of the algorithm on network architecture. Notation as in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, and additionally ‘Output’ denotes the output nuclei of the basal ganglia. (<bold>C</bold>) Definition of striatal activity in the goal-directed system.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig5-v2.tif"/></fig><p><xref ref-type="fig" rid="fig5">Figure 5A</xref> illustrates the key feature of the DopAct framework, that both action planning and learning can be described by the same process. Namely in both planning and learning, certain variables (the action intensity and synaptic weights, respectively) are changed to maximize the same function <inline-formula><mml:math id="inf118"><mml:mi>F</mml:mi></mml:math></inline-formula> (Equations 5.3 and 5.4). Since <inline-formula><mml:math id="inf119"><mml:mi>F</mml:mi></mml:math></inline-formula> is a negative of the sum of prediction errors (Equation 5.1), both action planning and learning are aimed at reducing prediction errors.</p></sec><sec id="s2-4"><title>Network selecting action intensity</title><p>The key elements of the algorithm in <xref ref-type="fig" rid="fig5">Figure 5A</xref> naturally map on the known anatomy of striato-dopaminergic connections. This mapping relies on three assumptions analogous to those typically made in models of the basal ganglia: (i) the information about state <inline-formula><mml:math id="inf120"><mml:mi>s</mml:mi></mml:math></inline-formula> is provided to the striatum by cortical input, (ii) the parameters of the systems <inline-formula><mml:math id="inf121"><mml:mi>q</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf122"><mml:mi>h</mml:mi></mml:math></inline-formula> are encoded in the cortico-striatal weights, and (iii) the computed action intensity is represented in the thalamus (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Under these assumptions, Equation 5.3 describing an update of action intensity can be mapped on the circuit: The action intensity in the model is jointly determined by the striatal neurons in the goal-directed and habit systems, which compute the corresponding terms of Equation 5.3, and communicate them by projecting to the thalamus via the output nuclei of the basal ganglia. The first term <inline-formula><mml:math id="inf123"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mi>q</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula> can be provided by striatal neurons in the goal-directed system (denoted by <inline-formula><mml:math id="inf124"><mml:mi>G</mml:mi></mml:math></inline-formula> in <xref ref-type="fig" rid="fig5">Figure 5B</xref>): They receive cortical input encoding stimulus intensity <inline-formula><mml:math id="inf125"><mml:mi>s</mml:mi></mml:math></inline-formula>, which is scaled by cortico-striatal weights encoding parameter <inline-formula><mml:math id="inf126"><mml:mi>q</mml:mi></mml:math></inline-formula>, so these neurons receive synaptic input <inline-formula><mml:math id="inf127"><mml:mi>q</mml:mi><mml:mi>s</mml:mi><mml:mo>.</mml:mo></mml:math></inline-formula> To compute <inline-formula><mml:math id="inf128"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mi>q</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula>, the gain of the striatal neurons in the goal-directed system needs to be modulated by dopaminergic neurons encoding prediction error <inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (this modulation is represented in <xref ref-type="fig" rid="fig5">Figure 5B</xref> by an arrow from dopaminergic to striatal neurons). Hence, these dopaminergic neurons drive an increase in action intensity until the prediction error they represent is reduced (as discussed in <xref ref-type="fig" rid="fig2">Figure 2</xref>). The second term <inline-formula><mml:math id="inf130"><mml:mi>h</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula> in Equation 5.3 can be computed by a population of neurons in the habit system receiving cortical input via connection with the weight <inline-formula><mml:math id="inf131"><mml:mi>h</mml:mi></mml:math></inline-formula>. Finally, the last term <inline-formula><mml:math id="inf132"><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:math></inline-formula> simply corresponds to a decay.</p><p>In the DopAct framework, dopaminergic neurons within each system compute errors in the predictions about the corresponding variable, i.e. reward for the goal-directed system, and action for the habit system. Importantly, in the network on <xref ref-type="fig" rid="fig5">Figure 5B</xref> this computation can be performed locally, i.e. the dopaminergic neurons receive inputs encoding all quantities necessary to compute their corresponding errors. In the habit system, the prediction error is equal to a difference between action <inline-formula><mml:math id="inf133"><mml:mi>a</mml:mi></mml:math></inline-formula> and expectation <inline-formula><mml:math id="inf134"><mml:mi>h</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula> (blue Equation 5.2). Such error can be easily computed in a network of <xref ref-type="fig" rid="fig5">Figure 5B</xref>, where the dopaminergic neurons in the habit system receive effective input form the output nuclei equal to <inline-formula><mml:math id="inf135"><mml:mi>a</mml:mi></mml:math></inline-formula> (as they receive inhibition equal to <inline-formula><mml:math id="inf136"><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:math></inline-formula>), and inhibition <inline-formula><mml:math id="inf137"><mml:mi>h</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula> from the striatal neurons. In the goal-directed system, the expression for prediction error is more complex (orange Equation 5.2), but importantly, all terms occurring in the equation could be provided to dopaminergic neurons in the goal-directed system via connections shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref> (<inline-formula><mml:math id="inf138"><mml:mi>q</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula> could be provided by the striatum, while <inline-formula><mml:math id="inf139"><mml:mi>a</mml:mi></mml:math></inline-formula> thorough an input from the output nuclei which have been reported to project to dopaminergic neurons [<xref ref-type="bibr" rid="bib78">Watabe-Uchida et al., 2012</xref>]).</p><p>Once the actual reward is obtained, changing parameters proportionally to prediction errors (Equations 5.4) can arise due to dopaminergic modulation of the plasticity of cortico-striatal connections (represented in <xref ref-type="fig" rid="fig5">Figure 5B</xref> by arrows going from dopamine neurons to parameters). With such a modulation, learning could be achieved through local synaptic plasticity: The update of a weight encoding parameter <inline-formula><mml:math id="inf140"><mml:mi>h</mml:mi></mml:math></inline-formula> (blue Equation 5.4) is simply proportional to the product of presynaptic (<inline-formula><mml:math id="inf141"><mml:mi>s</mml:mi></mml:math></inline-formula>) and dopaminergic activity (<inline-formula><mml:math id="inf142"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). In the goal-directed system, orange Equation 5.4 corresponds to local plasticity, if at the time of reward the striatal neurons encode information about action intensity (see definition of <inline-formula><mml:math id="inf143"><mml:mi>G</mml:mi></mml:math></inline-formula> in <xref ref-type="fig" rid="fig5">Figure 5C</xref>). Such information could be provided from the thalamus during action execution. Then the update of synaptic weight encoding parameter <inline-formula><mml:math id="inf144"><mml:mi>q</mml:mi></mml:math></inline-formula> will correspond to a standard three-factor rule (<xref ref-type="bibr" rid="bib46">Kuśmierz et al., 2017</xref>) involving a product of presynaptic (<inline-formula><mml:math id="inf145"><mml:mi>s</mml:mi></mml:math></inline-formula>), postsynaptic (<inline-formula><mml:math id="inf146"><mml:mi>a</mml:mi></mml:math></inline-formula>) and dopaminergic activity (<inline-formula><mml:math id="inf147"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>).</p><p>The model can be extended so that the parameters <inline-formula><mml:math id="inf148"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf149"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> describing variances of distributions are encoded in synaptic connections or internal properties of the neurons (e.g. leak conductance). In such an extended model, the action proposals of the two systems are weighted according to their certainties. <xref ref-type="fig" rid="fig6">Figure 6A</xref> shows the general description of the algorithms which is analogous to that in <xref ref-type="fig" rid="fig5">Figure 5A</xref>. The action intensity is driven by both goal-directed and habit systems, but now their contributions are normalised by the variance parameters. For the habit system this normalization is stated explicitly in Equation 6.2, while for the goal-directed system it comes from a normalization of prediction error by variance in orange Equation 6.3 (it is not necessary to normalize habit prediction error by variance because the contribution of the habit system is already normalized in Equation 6.2).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Description of a model selecting action intensity.</title><p>(<bold>A</bold>) Details of the algorithm. The update rules for the variance parameters can be obtained by computing derivatives of <inline-formula><mml:math id="inf150"><mml:mi>F</mml:mi></mml:math></inline-formula>, giving <inline-formula><mml:math id="inf151"><mml:msubsup><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf152"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></inline-formula> but to simplify these expressions, we scale them by <inline-formula><mml:math id="inf153"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf154"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, resulting in Equations 6.5. Such scaling does not change the value to which the variance parameters converge because <inline-formula><mml:math id="inf155"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf156"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> are positive. (<bold>B</bold>) Mapping of the algorithm on network architecture. Notation as in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. This network is very similar to that shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, but now the projection to output nuclei from the habit system is weighted by its precision <inline-formula><mml:math id="inf157"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (to reflect the weighting factor in Equation 6.2), and also the rate of decay (or relaxation to baseline) in the output nuclei needs to depend on <inline-formula><mml:math id="inf158"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. One way to ensure that the prediction error in goal-directed system is scaled by <inline-formula><mml:math id="inf159"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is to encode <inline-formula><mml:math id="inf160"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the rate of decay or leak of these prediction error neurons (<xref ref-type="bibr" rid="bib7">Bogacz, 2017</xref>). Such decay is included as the last term in orange Equation 6.7 describing the dynamics of prediction error neurons. Prediction error evolving according to this equation converges to the value in orange Equation 6.3 (the value in equilibrium can be found by setting the left hand side of orange Equation 6.7 to 0, and solving for <inline-formula><mml:math id="inf161"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). In Equation 6.7, total reward <inline-formula><mml:math id="inf162"><mml:mi>R</mml:mi></mml:math></inline-formula> was replaced according to Equation 1.1 by the sum of instantaneous reward <inline-formula><mml:math id="inf163"><mml:mi>r</mml:mi></mml:math></inline-formula>, and available reward <inline-formula><mml:math id="inf164"><mml:mi>v</mml:mi></mml:math></inline-formula> computed by the valuation system. (<bold>C</bold>) Dynamics of the model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig6-v2.tif"/></fig><p>There are several ways of including the variance parameters in the network, and one of them is illustrated in <xref ref-type="fig" rid="fig6">Figure 6B</xref> (see caption for details). The updates of the variance parameters (Equations 6.5) only depend on the corresponding prediction errors and the variance parameters themselves, so they could be implemented with local plasticity, if the neurons encoding variance parameters received corresponding prediction errors. <xref ref-type="fig" rid="fig6">Figure 6C</xref> provides a complete description of the dynamics of the simulated model. It parallels that in <xref ref-type="fig" rid="fig6">Figure 6B</xref>, but now explicitly includes time constants for update of neural activity (<inline-formula><mml:math id="inf165"><mml:mi>τ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf166"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), and learning rates for synaptic weights (<inline-formula><mml:math id="inf167"><mml:mi>α</mml:mi></mml:math></inline-formula> with corresponding indices).</p><p>As described in the Materials and methods, a simple model of the valuation system based on standard temporal-difference learning was employed in simulations (because the simulations corresponded to a case of low level of animal’s reserves). Striatal neurons in the valuation system compute the reward expected in a current state on the basis of parameters <inline-formula><mml:math id="inf168"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denoting estimates of reward at time <inline-formula><mml:math id="inf169"><mml:mi>t</mml:mi></mml:math></inline-formula> after a stimulus, and following standard reinforcement learning we assume that these parameters are encoded in cortico-striatal weights. The dopaminergic neurons in the valuation system encode the prediction error similar to that in the temporal-difference learning model, and after reward delivery, they modulate plasticity of cortico-striatal connections. The Method section also provides details of the implementation and simulations of the model.</p></sec><sec id="s2-5"><title>Simulations of action intensity selection</title><p>To illustrate how the model mechanistically operates and to help relate it to experimental data, we now describe a simulation of the model inferring action intensity. On each simulated trial the model selected action intensity, after observing a stimulus, which was set to <inline-formula><mml:math id="inf170"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. The reward obtained depended on action intensity as shown in <xref ref-type="fig" rid="fig7">Figure 7A</xref>, according to <inline-formula><mml:math id="inf171"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mi mathvariant="normal">tanh</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn><mml:mi>a</mml:mi><mml:mo>/</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:math></inline-formula>. Thus, the reward was proportional to the action intensity, transformed through a saturating function, and a cost was subtracted proportional to the action intensity, that could correspond to a price for making an effort. We also added Gaussian noise to reward (with standard deviation <inline-formula><mml:math id="inf172"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>) to account for randomness in the environment, and to action intensity to account for imprecision of the motor system or exploration.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Simulation of a model selecting action intensity.</title><p>(<bold>A</bold>) Mean reward given to a simulated agent as a function of action intensity. (<bold>B</bold>) Changes in variables encoded by the valuation system in different stages of task acquisition. (<bold>C</bold>) Changes in variables encoded by the actor. (<bold>D</bold>) Changes in model parameters across trials. The green curve in the right display shows the action intensity at the end of a planning phase of each trial. (<bold>E</bold>) Action intensity inferred by the model after simulated blocking of dopaminergic neurons.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig7-v2.tif"/></fig><p><xref ref-type="fig" rid="fig7">Figure 7AB</xref> shows how the quantities encoded in the valuation system changed throughout the learning process. The pattern of prediction errors in this figure is very similar to that expected from the temporal difference model, as the valuation system was based on that model. The stimulus was presented at time <inline-formula><mml:math id="inf173"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. On the first trial (left display) the simulated animal received a positive reward at time <inline-formula><mml:math id="inf174"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> (dashed black curve) due to stochastic nature of the rewards in the simulation. As initially the expectation of reward was low (dashed red curve), the reward triggered a substantial prediction error (solid red curve). The middle and right plots show the same quantities after learning. Now the prediction error was produced after the presentation of the stimulus, because after seeing the stimulus a simulated animal expected more reward than before the stimulus. In the middle display the reward received at time <inline-formula><mml:math id="inf175"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> was very close to the expectation, so the prediction error at the time of the reward was close to 0. In the right display the reward happened to be lower than usual (due to noise in the reward), which resulted in a negative prediction error. Note that the pattern of prediction errors in the valuation system in <xref ref-type="fig" rid="fig7">Figure 7B</xref> resembles the famous figure showing the activity of dopaminergic neurons during conditioning (<xref ref-type="bibr" rid="bib67">Schultz et al., 1997</xref>).</p><p><xref ref-type="fig" rid="fig7">Figure 7C</xref> shows the prediction errors in the actor and action intensity on the same trials that were visualised in <xref ref-type="fig" rid="fig7">Figure 7B</xref>. Prediction errors in the goal-directed system follow a similar pattern as in the valuation system in the left and middle displays in <xref ref-type="fig" rid="fig7">Figure 7C</xref>, that is before the behaviour becomes habitual. The middle display in <xref ref-type="fig" rid="fig7">Figure 7C</xref> shows simulated neural activity that was schematically illustrated in <xref ref-type="fig" rid="fig2">Figure 2A</xref>: As the valuation system detected that a reward was available (see panel above), it initially resulted in a prediction error in the goal-directed system, visible as an increase in the orange curve. This prediction error triggered a process of action planning, so with time the green curve representing planned action intensity increased. Once the action plan has been formulated, it provided a reward expectation, so the orange prediction error decreased. When an action became habitual after extensive training (right display in <xref ref-type="fig" rid="fig7">Figure 7C</xref>), the prediction error in the goal-directed system started to qualitatively differ from that in the valuation system. At this stage of training, the action was rapidly computed by the habit system, and the goal-directed system was too slow to lead action planning, so the orange prediction error was lower. This illustrates that in the DopAct framework reward expectations in the goal-directed system can arise even if an action is computed by the habit system.</p><p>The prediction error in the habit system follows a very different pattern than in other systems. Before an action became habitual, the prediction errors in the habit system arose after the action has been computed (middle display in <xref ref-type="fig" rid="fig7">Figure 7C</xref>). Since the habit system has not formed significant habits on early trials, it was surprised by the action, and this high value of blue prediction error drove its learning over trials. Once the habit system was highly trained (right display in <xref ref-type="fig" rid="fig7">Figure 7C</xref>) it rapidly drove action planning, so the green curve showing planned action intensity increased more rapidly. Nevertheless, due to the dynamics in the model, the increase in action intensity was not instant, so there was a transient negative prediction error in the habit system while an action was not yet equal to the intensity predicted by the habit system. The prediction error in the habit system at the time of action execution depended on how the chosen action differed from a habitual one, rather than on the received reward (e.g. in the right display in <xref ref-type="fig" rid="fig7">Figure 7C</xref>, <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> because the executed action was stronger than the planned one due to motor noise, despite reward being lower than expected).</p><p><xref ref-type="fig" rid="fig7">Figure 7D</xref> shows how the parameters in the model evolved over the trials in the simulation. The left display shows changes in the parameters of the three systems. A parameter of the valuation system correctly converged to the maximum value of the reward available in the task <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>≈</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (i.e. the maximum of the curve in <xref ref-type="fig" rid="fig7">Figure 7A</xref>). The parameter of the habit system correctly converged to <inline-formula><mml:math id="inf178"><mml:mi>h</mml:mi><mml:mo>≈</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>, i.e. typical action intensity chosen over trials (shown by a green curve in the right display of <xref ref-type="fig" rid="fig7">Figure 7D</xref>). The parameter of the goal-directed system converged to a vicinity of <inline-formula><mml:math id="inf179"><mml:mi>q</mml:mi><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, which allows the goal-directed system to expect the reward of 2 after selecting an action with intensity 2 (according to orange Equation 3.2 the reward expected by the goal-directed system is equal to <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi><mml:mi>q</mml:mi><mml:mi>s</mml:mi><mml:mo>≈</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). The right display in <xref ref-type="fig" rid="fig7">Figure 7D</xref> shows how the variance parameters in the goal-directed and habit systems changed during the simulation. The variance of the habit system was initialised to a high value, and it decreased over time, resulting in an increased certainty of the habit system.</p><p>Dopaminergic neurons in the model are only required to facilitate planning in the goal-directed system, where they increase excitability of striatal neurons, but not in the habit system. To illustrate it, <xref ref-type="fig" rid="fig7">Figure 7E</xref> shows simulations of a complete dopamine depletion in the model. It shows action intensity produced by the model in which following training, all dopaminergic neurons were set to 0. After 119 trials of training, on the 120th trial, the model was unable to plan an action. By contrast, after 359 training trials (when the uncertainty of the habit system has decreased – see the blue curve in right display of <xref ref-type="fig" rid="fig7">Figure 7D</xref>), the model was still able to produce a habitual response, because dopaminergic neurons are not required for generating habitual responses in the model. This parallels the experimentally observed robustness of habitual responses to blocking dopaminergic modulation (<xref ref-type="bibr" rid="bib11">Choi et al., 2005</xref>).</p></sec><sec id="s2-6"><title>Simulations of effects observed in conditioning experiments</title><p>This section shows that the model is able to reproduce two key patterns of behaviour that are thought to arise from interactions between different learning systems, namely the resistance of habitual responses to reward devaluation (<xref ref-type="bibr" rid="bib18">Dickinson, 1985</xref>), and Pavlovian-instrumental transfer (<xref ref-type="bibr" rid="bib24">Estes, 1943</xref>).</p><p>In experiments investigating devaluation, animals are trained to press a level (typically multiple times) for reward, for example food. Following this training the reward is devalued in a subgroup of animals, e.g. the animals in the devaluation group are fed to satiety, so they no longer desire the reward. Top displays in <xref ref-type="fig" rid="fig8">Figure 8A</xref> replot experimental data from one such study (<xref ref-type="bibr" rid="bib19">Dickinson et al., 1995</xref>). The displays show the average number of lever presses made by trained animals during a testing period in which no reward was given for lever pressing. The dashed and solid curves correspond to devaluation and control groups, and the two displays correspond to groups of animals trained for different periods, that is trained until they received 120 or 360 rewards respectively. <xref ref-type="fig" rid="fig8">Figure 8A</xref> illustrates two key effects. First, all animals eventually reduced lever pressing with time, thus demonstrating extinction of the previously learned responses. Second, the effect of devaluation on initial testing trials depended on the amount of training. In particular, in the case of animals that received moderate amount of training (top left display) the number of responses in the first bin was much lower for the devaluation group than control group. By contrast, highly trained animals (top right display) produced more similar numbers of responses in the first bin irrespective of devaluation. Such production of actions despite their consequence being no longer desired is considered as a hallmark of habit formation.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Comparison of experimentally observed lever pressing (top displays) and action intensity produced by the model (bottom displays).</title><p>(<bold>A</bold>) Devaluation paradigm. Top displays replot data represented by open shapes in Figure 1 in a paper by <xref ref-type="bibr" rid="bib19">Dickinson et al., 1995</xref>. Bottom displays show the average action intensity produced by the model across bins of 30 trials of the testing period. Simulations were repeated 10 times, and error bars indicate standard deviation across simulations. (<bold>B</bold>) Pavlovian-instrumental transfer. Top display replots the data represented by solid line in Figure 1 in a paper by <xref ref-type="bibr" rid="bib24">Estes, 1943</xref>. Bottom displays show the average action intensity produced by the model across bins of 10 trials of the testing period. Simulations were repeated 10 times, and error bars indicate standard error across simulations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig8-v2.tif"/></fig><p>The model can also produce insensitivity to devaluation with extensive training. Although the experimental tasks involving pressing levers multiple times is not identical to choosing intensity of a single action, such tasks could be conceptualized as a choice of the frequency of pressing a lever, that could also be described by a single number <inline-formula><mml:math id="inf181"><mml:mi>a</mml:mi></mml:math></inline-formula>. Furthermore, the average reward rate experienced by an animal in paradigms typically used in studies of habit formation (variable interval schedules that will be explained in Discussion) may correspond to a non-monotonic function similar to that in <xref ref-type="fig" rid="fig7">Figure 7A</xref>, because in these paradigms the reward per unit of time increases with frequency of lever press only to a certain point, but beyond certain frequency, there is no benefit of pressing faster.</p><p>To simulate the experiment described above, the model was trained either for 120 trials (bottom left display in <xref ref-type="fig" rid="fig8">Figure 8A</xref>) or 360 trials (bottom right display). During the training the reward depended on action as in <xref ref-type="fig" rid="fig7">Figure 7A</xref>. Following this training, the model was tested on 180 trials on which reward was not delivered, so in simulations <inline-formula><mml:math id="inf182"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:math></inline-formula> reflecting just a cost connected with making an effort. To simulate devaluation, the expectation of reward was set to 0.</p><p>Bottom displays in <xref ref-type="fig" rid="fig8">Figure 8A</xref> show the average action intensity produced by the model, and they reproduce qualitatively the key two effects in the top displays. First, the action intensity decreased with time, because the valuation and goal-directed systems learned that the reward was no longer available. Second, the action intensity just after devaluation was higher in the highly trained group (bottom right display) than in moderately trained group (bottom left display). This effect was produced by the model because after 360 trials of training the variance <inline-formula><mml:math id="inf183"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the habit system was much lower than after 120 trials (right display in <xref ref-type="fig" rid="fig7">Figure 7D</xref>), so after the extended training, the action intensity was to a larger extent determined by the habit system, which was not affected by devaluation.</p><p>The model can be easily extended to capture the phenomenon of Pavlovian-instrumental transfer. This phenomenon was observed in an experiment that consisted of three stages (<xref ref-type="bibr" rid="bib24">Estes, 1943</xref>). First, animals were trained to press a lever to obtain a reward. Second, the animals were placed in a cage without levers, and trained that a conditioned stimulus predicted the reward. Third, the animals were placed back to a conditioning apparatus, but no reward was given for lever pressing. Top display in <xref ref-type="fig" rid="fig8">Figure 8B</xref> shows the numbers of responses in that third stage, and as expected they gradually decreased as animals learned that no reward was available. Importantly, in the third and fifth intervals of this testing phase the conditioned stimulus was shown (highlighted with pink background in <xref ref-type="fig" rid="fig8">Figure 8B</xref>), and then the lever pressing increased. Thus the learned association between the conditioned stimulus and reward influenced the intensity of actions produced in the presence of the stimulus.</p><p>The bottom display of <xref ref-type="fig" rid="fig8">Figure 8B</xref> shows the action intensity produced by the model in simulations of the above paradigm. As described in Materials and methods, the valuation system learned the rewards associated with two states: presence of a lever, and the conditioned stimulus. During the first stage (operant conditioning), the reward expectation computed by the valuation system drove action planning, while in the second stage (classical conditioning), no action was available, so the valuation system generated predictions for the reward without triggering action planning. In the third stage (testing), on the highlighted intervals on which the conditioned stimulus was present, the expected reward <inline-formula><mml:math id="inf184"><mml:mi>v</mml:mi></mml:math></inline-formula> was increased, because it was a sum of rewards associated with both states. Consequently, the actor computed that a higher action intensity was required to obtain a bigger reward, because the goal-directed system assumes that the action intensity is proportional to the mean reward (orange Equation 3.2). In summary, the model explains the Pavlovian-instrumental transfer by proposing that the presence of the conditioned stimulus increases the reward expected by the valuation system, which results in actor selecting higher action intensity to obtain this anticipated reward.</p></sec><sec id="s2-7"><title>Extending the model to choice between two actions</title><p>This section shows how models developed within the DopAct framework can also describe more complex tasks with multiple actions and multiple dimensions of state. We consider a task involving choice between two options, often used in experimental studies, as it allows illustrating the generalization, and at the same time results in a relatively simple model. This section will also show that the models developed in the framework can under certain assumptions be closely related to previously proposed models of reinforcement learning and habit formation.</p><p>To make dimensionality of all variables and parameters explicit, we will denote vectors with a bar and matrices with a bold font. Thus <inline-formula><mml:math id="inf185"><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> is a vector where different entries correspond to intensities of different stimuli in an environment, and <inline-formula><mml:math id="inf186"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> is a vector where different entries correspond to intensities of different actions. The model is set up such that only one action can be chosen, so following a decision, <inline-formula><mml:math id="inf187"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> for the chosen action <inline-formula><mml:math id="inf188"><mml:mi>i</mml:mi></mml:math></inline-formula>, while for other actions <inline-formula><mml:math id="inf189"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. Thus symbol <inline-formula><mml:math id="inf190"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> still denotes action intensity, but the intensity of an action only takes binary values once an action has been chosen.</p><p>Equation 9.1 in <xref ref-type="fig" rid="fig9">Figure 9A</xref> shows how the definitions of the probability distributions encoded by the goal-directed and habit systems can be generalized to multiple dimensions. Orange Equation 9.1 states that the reward expected by the goal-directed system has mean <inline-formula><mml:math id="inf191"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula>, where <inline-formula><mml:math id="inf192"><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:math></inline-formula> is now a matrix of parameters. This notation highlights the link with the standard reinforcement learning, where the expected reward for selecting action <inline-formula><mml:math id="inf193"><mml:mi>i</mml:mi></mml:math></inline-formula> in state <inline-formula><mml:math id="inf194"><mml:mi>j</mml:mi></mml:math></inline-formula> is denoted by <inline-formula><mml:math id="inf195"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>: Note that if <inline-formula><mml:math id="inf196"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf197"><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> are both binary vectors with entries <inline-formula><mml:math id="inf198"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf199"><mml:mi>j</mml:mi></mml:math></inline-formula> equal to 1 in the corresponding vectors, and all other entries equal to 0, then <inline-formula><mml:math id="inf200"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> is equal to the element <inline-formula><mml:math id="inf201"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of matrix <inline-formula><mml:math id="inf202"><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:math></inline-formula>.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Description of the model of choice between two actions.</title><p>(<bold>A</bold>) Probability distributions assumed in the model. (<bold>B</bold>) Details of the algorithm. (<bold>C</bold>) Approximation of the algorithm. In blue Equation 9.10, * indicates that the parameters are updated for all actions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig9-v2.tif"/></fig><p>In the model, the prior probability is proportional to a product of three distributions. The first of them is encoded by the habit system and given in blue Equation 9.1. The expected action intensity encoded in the habit system has mean <inline-formula><mml:math id="inf203"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula>, and this notation highlights the analogy with a recent model of habit formation (<xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref>) where a tendency to select action <inline-formula><mml:math id="inf204"><mml:mi>i</mml:mi></mml:math></inline-formula> in state <inline-formula><mml:math id="inf205"><mml:mi>j</mml:mi></mml:math></inline-formula> is also denoted by <inline-formula><mml:math id="inf206"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Additionally, we introduce another prior given in Equation 9.2, which ensures that only one action has intensity significantly deviating from 0. Furthermore, to link the framework with classical reinforcement learning, we enforce a third condition ensuring that action intensity remains between 0 and 1 (Equation 9.3). These additional priors will often result in one entry of <inline-formula><mml:math id="inf207"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> converging to 1, while all other entries decaying towards 0 due to competition. Since in our simulations we also use a binary state vector, the reward expected by the goal-directed system will often be equal to <inline-formula><mml:math id="inf208"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as in the classical reinforcement learning (see paragraph above).</p><p>Let us now derive equations describing inference and learning for the above probabilistic model. Substituting probability densities from Equations 9.1 and 9.2 into the objective function of Equation 4.1, we obtain Equation 9.4 in <xref ref-type="fig" rid="fig9">Figure 9B</xref>. To ensure that action intensity remained between 0 and 1 (Equation 9.3), <inline-formula><mml:math id="inf209"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was set to one of these values if it exceeded the range during numerical integration.</p><p>To obtain the equations describing action planning or learning, we need to compute derivatives of <inline-formula><mml:math id="inf210"><mml:mi>F</mml:mi></mml:math></inline-formula> over vectors or matrices. The rules for computing such derivatives are natural generalizations of the standard rules and they can be found in a tutorial paper (<xref ref-type="bibr" rid="bib7">Bogacz, 2017</xref>). During planning, the action intensity should change proportionally to a gradient of <inline-formula><mml:math id="inf211"><mml:mi>F</mml:mi></mml:math></inline-formula>, which is given in Equation 9.5, where the prediction errors are defined in Equations 9.6. These equations have an analogous form to those in <xref ref-type="fig" rid="fig6">Figure 6A</xref>, but are generalized to matrices. The only additional element is the last term in Equation 9.5, which ensures competition between different actions, i.e. <inline-formula><mml:math id="inf212"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> will be decreased proportionally to <inline-formula><mml:math id="inf213"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, and vice versa. During learning, the parameters need to be updated proportionally to the corresponding gradients of <inline-formula><mml:math id="inf214"><mml:mi>F</mml:mi></mml:math></inline-formula>, which are given in Equations 9.7 and 9.8. Again, these equations are fully analogous to those in <xref ref-type="fig" rid="fig6">Figure 6A</xref>.</p><p>Both action selection and learning in the above model share similarities with standard models of reinforcement learning and a recent model of habit formation (<xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref>). To see which action is most likely to be selected in the model, it is useful to consider the evolution of action intensity at the start of a trial, when <inline-formula><mml:math id="inf215"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, because the action with a largest initial input is likely to win the competition and be selected. Substituting orange Equation 9.6 into Equation 9.5 and setting <inline-formula><mml:math id="inf216"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, we obtain Equation 9.9 in <xref ref-type="fig" rid="fig9">Figure 9C</xref>. This equation suggests that probabilities of selecting actions depend on a sum of inputs form the goal-directed and habit systems weighted by their certainty, analogously as in a model by <xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref>. There are also similarities in the update rules: if only single elements of vectors <inline-formula><mml:math id="inf217"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf218"><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> have non-zero values <inline-formula><mml:math id="inf219"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf220"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, then substituting Equations 9.6 into 9.7 and ignoring constants gives Equations 9.10. These equations suggest that the parameter <inline-formula><mml:math id="inf221"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> describing expected reward for action <inline-formula><mml:math id="inf222"><mml:mi>i</mml:mi></mml:math></inline-formula> in state <inline-formula><mml:math id="inf223"><mml:mi>j</mml:mi></mml:math></inline-formula> is modified proportionally to a reward prediction error, as in classical reinforcement learning. Additionally, for every action and current state <inline-formula><mml:math id="inf224"><mml:mi>j</mml:mi></mml:math></inline-formula> the parameter describing a tendency to take this action is modified proportionally to a prediction error equal to a difference between the intensity of this action and the intensity expected by the habit system, as in a model of habit formation (<xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref>).</p><p>The similarity of a model developed in the DopAct framework to classical reinforcement learning, which has been designed to maximize resources, highlights that the model also tends to maximize resources, when animal’s reserves are sufficiently low. But the framework is additionally adaptive to the levels of reserves: If the reserves were at the desired level, then <inline-formula><mml:math id="inf225"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> during action planning, so according to Equation 9.9, the goal-directed system would not suggest any action.</p><p>Let us now consider how the inference and learning can be implemented in a generalized version of the network described previously, which is shown in <xref ref-type="fig" rid="fig10">Figure 10A</xref>. In this network, striatum, output nuclei and thalamus include neural populations selective for the two alternative actions (shown in vivid and pale colours in <xref ref-type="fig" rid="fig10">Figure 10A</xref>), as in standard models of action selection in the basal ganglia (<xref ref-type="bibr" rid="bib8">Bogacz and Gurney, 2007</xref>; <xref ref-type="bibr" rid="bib27">Frank et al., 2007</xref>; <xref ref-type="bibr" rid="bib32">Gurney et al., 2001</xref>). We assume that the connections between these nuclei are within the populations selective for a given action, as in previous models (<xref ref-type="bibr" rid="bib8">Bogacz and Gurney, 2007</xref>; <xref ref-type="bibr" rid="bib27">Frank et al., 2007</xref>; <xref ref-type="bibr" rid="bib32">Gurney et al., 2001</xref>). Additionally, we assume that sensory cortex includes neurons selective for different states (shown in black and grey in <xref ref-type="fig" rid="fig10">Figure 10A</xref>), and the parameters <inline-formula><mml:math id="inf226"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf227"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are encoded in cortico-striatal connections. Then, the orange and blue terms in Equation 9.5 can be computed by the striatal neurons in goal-directed and habit systems in exactly analogous way as in the network inferring action intensity, and these terms can be integrated in the output nuclei and thalamus. The last term in Equation 9.5 corresponds to mutual inhibition between the populations selective for the two actions, and such inhibition could be provided by inhibitory projections that are presents in many different regions of this circuit, e.g. by co-lateral projections of striatal neurons (<xref ref-type="bibr" rid="bib60">Preston et al., 1980</xref>) or via a subthalamic nucleus, which has been proposed to play role in inhibiting non-selected actions (<xref ref-type="bibr" rid="bib8">Bogacz and Gurney, 2007</xref>; <xref ref-type="bibr" rid="bib27">Frank et al., 2007</xref>; <xref ref-type="bibr" rid="bib32">Gurney et al., 2001</xref>).</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Implementation of the model of choice between two actions.</title><p>(<bold>A</bold>) Mapping of the algorithm on network architecture. Notation as in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. (<bold>B</bold>) An approximation of learning in the habit system.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig10-v2.tif"/></fig><p>The prediction error in the goal-directed system (orange Equation 9.6) could be computed locally, because the orange dopaminergic neurons in <xref ref-type="fig" rid="fig10">Figure 10A</xref> receive inputs encoding all terms in the equation. During learning, the prediction error in the goal-directed system modulates plasticity of the corresponding cortico-striatal connections according to orange Equation 9.7, which describes a standard tri-factor Hebbian rule (if following movement the striatal neurons encode chosen action, as assumed in <xref ref-type="fig" rid="fig5">Figure 5C</xref>).</p><p>The prediction error in the habit system (blue Equation 9.6) is a vector, so computing it explicitly would also require multiple populations of dopaminergic neurons in the habit system selective for available actions, but different dopaminergic neurons in the real brain may not be selective for different actions (<xref ref-type="bibr" rid="bib13">da Silva et al., 2018</xref>). Nevertheless, learning in the habit system can be approximated with a single dopaminergic population, because the prediction error <inline-formula><mml:math id="inf228"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> has a characteristic structure with large redundancy. Namely, if only one entry in the vectors <inline-formula><mml:math id="inf229"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf230"><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> is equal to 1 and other entries to 0, then only one entry in <inline-formula><mml:math id="inf231"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> corresponding to the chosen action is positive, while all other entries are negative (because parameters <inline-formula><mml:math id="inf232"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> stay in a range between 0 and 1 when initialized within this range and updated according to blue Equation 9.7). Hence, we simulated an approximate model just encoding the prediction error for the chosen action (Equation 10.1). With such a single modulatory signal, the learning rules for striatal neurons in the habit system have to be adjusted so the plasticity has opposite directions for the neurons selective for the chosen and the other actions. Such modified rule is given in Equation 10.2 and corresponds to tri-factor Hebbian learning (if striatal neurons in the habit system have activity proportional to <inline-formula><mml:math id="inf233"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> during learning, as we assumed for the goal-directed system). Thanks to this approximation, the prediction error and plasticity in the habit system take a form that is more analogous to that in the goal-directed system. When the prediction error in the habit system is a scalar, the learning rule for the variance parameter (blue Equation 9.8) becomes the same as in the model in the previous section (cf. blue Equation 6.5). Materials and method section provides the description of the valuation system in this model, and describes details of the simulations.</p></sec><sec id="s2-8"><title>Simulations of choice between two actions</title><p>To illustrate predictions made by the model, we simulated it in a probabilistic reversal task. On each trial, the model was 'presented' with one of two 'stimuli', that is one randomly chosen entry of vector <inline-formula><mml:math id="inf234"><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> was set to 1, while the other entry was set to 0. On the initial 150 trials, the correct response was to select action 1 for stimulus 1 and action 2 for stimulus 2, while on the subsequent trials, the correct responses were reversed. The mean reward was equal to 1 for a correct response and 0 for an error. In each case, a Gaussian noise (with standard deviation <inline-formula><mml:math id="inf235"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>) was added to the reward.</p><p><xref ref-type="fig" rid="fig11">Figure 11A</xref> shows changes in action intensity and inputs from goal-directed and habit systems as a function of time during planning on different trials within a simulation. On an early trial (left display) the changes in action intensity were primarily driven by the goal-directed system. The intensity of the correct action converged to 1, while it stayed at 0 for the incorrect one. After substantial training (middle display), the changes in action intensity were primarily driven by the faster habit system. Following a reversal (right display) one can observe a competition between the two systems: Although the goal-directed system had already learned the new contingency (solid orange curve), the habit system still provided larger input to the incorrect action node (dashed blue curve). Since the habit system was faster, the incorrect action had higher intensity initially, and only with time, the correct action node received input from the goal-directed system, and inhibited the incorrect one.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Simulation of the model of choice between two actions.</title><p>(<bold>A</bold>) Changes in action intensity and inputs from the goal-directed and habit systems, defined below Equation 9.9. Solid lines correspond to a correct action and dashed lines to an error. Thus selecting action 1 for stimulus 1 (or action 2 for stimulus 2) corresponds to solid lines before reversal (left and middle displays) and to dashed lines after reversal (right display). (<bold>B</bold>) Changes in model parameters across trials. Dashed black lines indicate a reversal trial. (<bold>C</bold>) Maximum values of prediction errors during action planning on each simulated trial.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig11-v2.tif"/></fig><p><xref ref-type="fig" rid="fig11">Figure 11B</xref> shows how parameters in the model changed over trials. Left display illustrates changes in sample cortico-striatal weights in the three systems. The valuation system rapidly learned the reward available, but after reversal this estimate decreased, as the model persevered in choosing the incorrect option. Once the model discovered the new rule, the estimated value of the stimulus increased. The goal-directed system learned that selecting the first action after the first stimulus gave higher rewards before reversal, but not after. The changes in the parameters of the habit system followed those in the goal-directed system. The right display shows that the variance estimated by the habit system initially decreased, but then increased several trials after the reversal, when the goal-directed system discovered the new contingency, and thus the selected actions differed from the habitual ones. <xref ref-type="fig" rid="fig11">Figure 11C</xref> shows an analogous pattern in dopaminergic activity, where the neurons in the habit system signalled higher prediction errors following a reversal. This pattern of prediction errors is unique to the habit system, as the prediction errors in the goal-directed system (orange curve) fluctuated throughout the simulation following the fluctuations in reward. The increase in dopaminergic activity in the habit system following a reversal is a key experimental prediction of the model, to which we will come back in Discussion.</p><p>Let us consider the mechanisms of reversal in the model. Since the prediction errors in the habit system do not directly depend on rewards, the habit system would not perform reversal on its own, and the goal-directed system is necessary to initiate the reversal. This feature is visible in simulations, where just after the reversal the agent was still selecting the same actions as before, so the habits were still being strengthen rather weakened (the blue curve in left display of <xref ref-type="fig" rid="fig11">Figure 11B</xref> still increased for ~20 trials after the reversal). When the goal-directed system learned that the previously selected actions were no longer rewarded, the tendency to select them decreased, and other actions had higher chances of being selected due to noise (although the amount of noise added to the choice process was constant, there was a higher chance for noise to affect behaviour, because the old actions were now suggested only by the habit rather than both systems). Once the goal-directed system found that the actions selected according to new contingency gave rewards, the probability of selecting action according to the old contingency decreased, and only then the habit system slowly unlearned the old habit.</p><p>It is worth adding that the reversal was made harder by the fact that a sudden change in reward increased the uncertainty of the goal-directed system (the orange curve in the right display of <xref ref-type="fig" rid="fig11">Figure 11B</xref> increased after reversal), which actually weakened the control by that system. Nevertheless, this increase of uncertainty was brief, because the goal-directed system quickly learned to predict rewards in the new contingency and regained its influence on choices.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this paper, we proposed how an action can be identified through Bayesian inference, where the habit system provides a prior and the goal-directed system represents reward likelihood. Within the DopAct framework, the goal-directed and habit systems may not be viewed as fundamentally different systems, but rather as analogous segments of neural machinery performing inference in a hierarchical probabilistic model (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), which correspond to different levels of hierarchy.</p><p>In this section, we discuss the relationship of the framework to other theories and experimental data, mechanisms of habit formation, and suggest experimental predictions and directions for future work.</p><sec id="s3-1"><title>Relationship to other theories</title><p>The DopAct framework combines elements from four theories: reinforcement learning, active inference, habit formation, and planning as inference. For each of the theories we summarize key similarities, and highlight the ways in which the DopAct framework extends them.</p><p>As in classical reinforcement learning (<xref ref-type="bibr" rid="bib35">Houk et al., 1995</xref>; <xref ref-type="bibr" rid="bib56">Montague et al., 1996</xref>), in the DopAct framework the dopaminergic neurons in the valuation and goal-directed systems encode reward prediction errors, and these prediction errors drive learning to improve future choices. However, the key conceptual difference of the DopAct framework is that it assumes that animals aim to achieve a desired level of reserves (<xref ref-type="bibr" rid="bib9">Buckley et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Hull, 1952</xref>; <xref ref-type="bibr" rid="bib70">Stephan et al., 2016</xref>), rather than always maximize acquiring resources. It has been proposed that when a physiological state is considered, the reward an animal aims to maximize can be defined as a reduction of distance between the current and desired levels of reserves (<xref ref-type="bibr" rid="bib42">Juechems and Summerfield, 2019</xref>; <xref ref-type="bibr" rid="bib44">Keramati and Gutkin, 2014</xref>). Under this definition, a resource is equal to such subjective reward only if consuming it would not bring the animal beyond its optimal reserve level. When an animal is close to the desired level, acquiring a resource may even move the animal further from the desired level, resulting in a negative subjective reward. As the standard reinforcement learning algorithms do not consider physiological state, they do not always maximize the subjective reward defined in this way. By contrast, the DopAct framework offers flexibility to stop acquiring resources, when the reserves reach the desired level.</p><p>The DopAct framework relies on a key high-level principle from the active inference theory (<xref ref-type="bibr" rid="bib29">Friston, 2010</xref>) that the prediction errors can be minimized by both learning and action planning. Furthermore, the network implementations of the proposed models share a similarity with predictive coding networks that the neurons encoding prediction errors affect both the plasticity and the activity of its target neurons (<xref ref-type="bibr" rid="bib28">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib61">Rao and Ballard, 1999</xref>). A novel contribution of this paper is to show how these principles can be realized in anatomically identified networks in the brain.</p><p>The DopAct framework shares a feature of a recent model of habit formation (<xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref>) that learning in the habit system is driven by prediction errors that do not depend on reward, but rather encode the difference between the chosen and habitual actions. The key new contribution of this paper is to propose how such learning can be implemented in the basal ganglia circuit including multiple populations of dopaminergic neurons encoding different prediction errors.</p><p>Similarly as in the model describing goal-directed decision making as probabilistic inference (<xref ref-type="bibr" rid="bib69">Solway and Botvinick, 2012</xref>), the actions selected in the DopAct framework maximize a posterior probability of action given the reward. The new contribution of this paper is making explicit the rationale for why such probabilistic inference is the right thing for the brain to do: The resource that should be acquired in a given state depends on the level of reserves, so the inferred action should depend on the reward required to restore the reserves. We also proposed a detailed implementation of the probabilistic inference in the basal ganglia circuit.</p><p>It is useful to discuss the relationship of the DopAct framework to several other theories. The tonic level of dopamine has been proposed to determine the vigour of movements (<xref ref-type="bibr" rid="bib57">Niv et al., 2007</xref>). In our model selecting action intensity, the dopaminergic signals in the valuation and goal-directed systems indeed influence the resulting intensity of movement, but in the DopAct framework, it is the phasic rather than tonic dopamine that determines the vigour, in agreement with recent data (<xref ref-type="bibr" rid="bib13">da Silva et al., 2018</xref>). It has been also proposed that dopamine encodes incentive salience of the available rewards (<xref ref-type="bibr" rid="bib5">Berridge and Robinson, 1998</xref>; <xref ref-type="bibr" rid="bib51">McClure et al., 2003</xref>). Such encoding is present in the DopAct framework, where the prediction error in the goal-directed system depends on whether the available resource is desired by an animal.</p></sec><sec id="s3-2"><title>Relationship to experimental data</title><p>To relate the DopAct framework to experimental data, we need to assume a particular mapping of different systems on anatomically defined brain regions. Thus we assume that the striatal neurons in valuation, goal-directed, and habit systems can be approximately mapped on ventral, dorsomedial, and dorsolateral striatum. This mapping is consistent with the pattern of neural activity in the striatum, which shifts from encoding reward expectation to movement as one progresses from ventral to dorsolateral striatum (<xref ref-type="bibr" rid="bib10">Burton et al., 2015</xref>), and with increased activity in dorsolateral striatum during habitual movements (<xref ref-type="bibr" rid="bib76">Tricomi et al., 2009</xref>). This mapping is also consistent with the observation that deactivation of dorsomedial striatum impairs learning which action leads to larger rewards (<xref ref-type="bibr" rid="bib80">Yin et al., 2005</xref>), while lesion of dorsolateral striatum prevents habit formation (<xref ref-type="bibr" rid="bib79">Yin et al., 2004</xref>). Furthermore, we will assume that dopaminergic neurons in valuation, goal-directed, and habit systems can be mapped on a spectrum of dopaminergic neurons ranging from ventral tegmental area (VTA) to substantia nigra pars compacta (SNc). VTA is connected with striatal regions we mapped on the valuation system, while SNc with those mapped on the habit system (<xref ref-type="bibr" rid="bib33">Haber et al., 2000</xref>), so we assume that <inline-formula><mml:math id="inf236"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf237"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are represented in VTA and SNc respectively. Such mapping in consistent with lesions to SNc preventing habit formation (<xref ref-type="bibr" rid="bib25">Faure et al., 2005</xref>). The mapping of the dopaminergic neurons from the goal-directed system is less clear, so let us assume that these neurons may be present in both areas.</p><p>The key prediction of the DopAct framework is that the dopaminergic neurons in the valuation and goal-directed systems should encode reward prediction errors, while the dopaminergic neurons in the habit system should respond to non-habitual actions. This prediction can be most directly compared with the data in a study where rewards and movements have been dissociated. That study employed a task in which mice could make spontaneous movements and rewards were delivered at random times (<xref ref-type="bibr" rid="bib37">Howe and Dombeck, 2016</xref>). It has been observed that a fraction of dopaminergic neurons had increased responses to rewards, while a group of neurons responded to movements. Moreover, the reward responding neurons were located in VTA while most movement responding neurons in SNc (<xref ref-type="bibr" rid="bib37">Howe and Dombeck, 2016</xref>). In that study the rewards were delivered to animals irrespectively of movements, so the movements they generated were most likely not driven by processes aiming at achieving reward (simulated in this paper), but rather by other inputs (modelled by noise in our simulations). To relate this task to the DopAct framework, let us consider the prediction errors likely to occur at the times of reward and movement. At the time of reward the animal was not able to predict it, so <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, but it was not necessarily making any movements <inline-formula><mml:math id="inf239"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, while at the time of a movement the animal might have not expected reward <inline-formula><mml:math id="inf240"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, but might have made non-habitual movements <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Hence the framework predicts separate groups of dopaminergic neurons to produce responses at times of reward and movements, as experimentally observed (<xref ref-type="bibr" rid="bib37">Howe and Dombeck, 2016</xref>). Furthermore, the peak of the movement related response of SNc neurons was observed to occur after the movement onset (<xref ref-type="bibr" rid="bib37">Howe and Dombeck, 2016</xref>), which suggests that most of this dopaminergic activity was a response to a movement rather than activity initiating a movement. This timing is consistent with the role of dopaminergic neurons in the habit system, which compute a movement prediction error, rather than initiate movements.</p><p>While discussing dopaminergic neurons, one has to mention the influential studies showing that VTA neurons encode reward prediction error (<xref ref-type="bibr" rid="bib23">Eshel et al., 2016</xref>; <xref ref-type="bibr" rid="bib67">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="bib75">Tobler et al., 2005</xref>). So for completeness, let us reiterate that in the DopAct framework the valuation system is similar to the standard temporal difference learning model, hence it inherits the ability to account for the dopaminergic responses to unexpected rewards previously explained with that model (<xref ref-type="fig" rid="fig7">Figure 7B</xref>).</p><p>The DopAct framework also makes predictions on dopaminergic responses during movements performed to obtain rewards. In presented simulations, such responses were present in all systems (<xref ref-type="fig" rid="fig7">Figure 7B–C</xref>), and indeed responses to reward-directed movements were observed experimentally in both VTA and SNc (<xref ref-type="bibr" rid="bib22">Engelhard et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Schultz, 1986</xref>). The framework predicts that the responses to movements should be modulated by the magnitude of available reward in the valuation and goal-directed systems, but not in the habit system. This prediction can be compared with data from a task in which animals could press one of two levers that differed in magnitude of resulting rewards (<xref ref-type="bibr" rid="bib40">Jin and Costa, 2010</xref>). So for this task, the framework predicts that the dopaminergic neurons in the valuation and goal-directed systems should respond differently depending on which lever was pressed, while the dopaminergic response in the habit system should depend just on action intensity but not reward magnitude. Indeed, a diversity of dopaminergic neurons have been observed in SNc, and the neurons differed in whether their movement related response depended on reward available (Figure 4j in the paper by <xref ref-type="bibr" rid="bib40">Jin and Costa, 2010</xref>).</p><p>In the DopAct framework, the activity of dopaminergic neurons in the goal-directed system is normalized by the uncertainty of that system. Analogous scaling of dopaminergic activity by an estimate of reward variance is also present in a model by <xref ref-type="bibr" rid="bib31">Gershman, 2017</xref>. He demonstrated that such scaling is consistent with an experimental observation that dopaminergic responses adapt to the range of rewards available in a given context (<xref ref-type="bibr" rid="bib75">Tobler et al., 2005</xref>).</p><p>In the DopAct framework the role of dopamine during action planning is specific to preparing goal-directed but not habitual movements (<xref ref-type="fig" rid="fig7">Figure 7E</xref>). Thus the framework is consistent with an observation that blocking dopaminergic transmission slows responses to reward-predicting cues early in training, but not after extensive training, when the responses presumably became habitual (<xref ref-type="bibr" rid="bib11">Choi et al., 2005</xref>). Analogously, the DopAct framework is consistent with an impairment in Parkinson’s disease for goal-directed but not habitual choices (<xref ref-type="bibr" rid="bib16">de Wit et al., 2011</xref>) or voluntary but not cue driven movements (<xref ref-type="bibr" rid="bib41">Johnson et al., 2016</xref>). The difficulty in movement initiation in Parkinson’s disease seems to depend on whether the action is voluntary or in response to a stimulus, so even highly practiced movements like walking may be difficult if performed voluntarily, but easier in response to auditory or visual cues (<xref ref-type="bibr" rid="bib63">Rochester et al., 2005</xref>). Such movements performed to cues are likely to engage the habit system, because responding to stimuli is a hallmark of habitual behaviour (<xref ref-type="bibr" rid="bib20">Dickinson and Balleine, 2002</xref>).</p><p>Finally, let us discuss a feature of the DopAct framework related to the dynamics of competition between systems during action planning. Such competition is illustrated in the right display of <xref ref-type="fig" rid="fig11">Figure 11A</xref>, where after a reversal, the faster habit system initially prepared an incorrect action, but later the slower goal-directed system increased the intensity of the correct action. Analogous behaviour has been shown in a recent study, where human participants were extensively trained to make particular responses to given stimuli (<xref ref-type="bibr" rid="bib34">Hardwick et al., 2019</xref>). After a reversal, they tended to produce incorrect habitual actions when required to respond rapidly, but were able to produce the correct actions given sufficient time.</p></sec><sec id="s3-3"><title>Mechanisms of habitual behaviour</title><p>Since the mechanisms of habit formation in the DopAct framework fundamentally differ from a theory widely accepted by a computational neuroscience community (<xref ref-type="bibr" rid="bib14">Daw et al., 2005</xref>), this section is dedicated to comparing the two accounts, and discussing the properties of the habit system in the framework.</p><p>An influential theory suggests that two anatomically separate systems in the brain underlie goal-directed and habitual behaviour and a competition between them is resolved according to uncertainty of the systems (<xref ref-type="bibr" rid="bib14">Daw et al., 2005</xref>). The DopAct framework agrees with these general principles but differs from the theory of <xref ref-type="bibr" rid="bib14">Daw et al., 2005</xref> in the nature of computations in these systems, and their mapping on brain anatomy. <xref ref-type="bibr" rid="bib14">Daw et al., 2005</xref> proposed that goal-directed behaviour is controlled by a cortical model-based system that learns the transitions between states resulting from actions, while habitual behaviour arises from a striatal model-free system that learns policy according to standard reinforcement learning. By contrast, the DopAct framework suggests that goal-directed behaviour in simple lever-pressing experiments does not require learning state transitions, but such behaviour can be also supported by a striatal goal-directed system that learns expected rewards from actions in a way similar to standard reinforcement learning models. So in the DopAct framework it is the goal-directed rather than habit system that learns according to reward prediction error encoded by dopaminergic neurons. Furthermore, in the DopAct framework (following the model by <xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref>) habits arise simply from repeating actions, so their acquisition is not directly driven by reward prediction error, unlike in the model of <xref ref-type="bibr" rid="bib14">Daw et al., 2005</xref>.</p><p>The accounts of habit formation in the DopAct framework and the model of <xref ref-type="bibr" rid="bib14">Daw et al., 2005</xref> make different predictions. Since the theory of <xref ref-type="bibr" rid="bib14">Daw et al., 2005</xref> assumes that a system underlying habitual behaviour learns with standard reinforcement learning, it predicts that striatal neurons supporting habitual behaviour should receive reward prediction error. However, the dopaminergic neurons that have been famously shown to encode reward prediction error (<xref ref-type="bibr" rid="bib67">Schultz et al., 1997</xref>) are located in VTA, which does not send major projections to the dorsolateral striatum underlying habitual behaviour. These striatal neurons receive dopaminergic input from SNc (<xref ref-type="bibr" rid="bib33">Haber et al., 2000</xref>), and it is questionable to what extent dopaminergic neurons in SNc encode reward prediction error. Although such encoding has been reported (<xref ref-type="bibr" rid="bib81">Zaghloul et al., 2009</xref>), studies which directly compared the activity of VTA and SNc neurons demonstrated that neurons encoding reward prediction error are significantly more frequent in VTA than SNc (<xref ref-type="bibr" rid="bib37">Howe and Dombeck, 2016</xref>; <xref ref-type="bibr" rid="bib50">Matsumoto and Hikosaka, 2009</xref>). So the striatal neurons underlying habitual behaviour do not seem to receive much of the teaching signal that would be expected if habit formation arose from the processes of reinforcement learning proposed by <xref ref-type="bibr" rid="bib14">Daw et al., 2005</xref>. By contrast, the DopAct framework assumes that the habit system learns on the basis of a teaching signal encoding how the chosen action differs from the habitual one, so it predicts that SNc neurons should respond to non-habitual movements. It has indeed been observed that the dopaminergic neurons in SNc respond to movements (<xref ref-type="bibr" rid="bib37">Howe and Dombeck, 2016</xref>; <xref ref-type="bibr" rid="bib65">Schultz et al., 1983</xref>), but it has not been systematically analysed yet if these responses preferentially encode non-habitual movements (we will come back to this key prediction in the next section).</p><p>It is worth discussing how the habits may be suppressed if previously learnt habitual behaviour is no longer appropriate. In the DopAct framework, old habits die hard. When the habitual behaviour is no longer rewarded, the negative reward prediction errors do not directly suppress the behaviour in the habit system. So, as mentioned at the end of the Results section, in order to reverse behaviour, the control cannot be completely taken over by the habit system, but the goal-directed system needs to provide at least some contribution to action planning to initiate the reversal when needed. Nevertheless, simulations presented in this paper show that for certain parameters the control of habit system may be released when no longer required, and the model can reproduce the patterns of behaviour observed in extinction experiments (<xref ref-type="fig" rid="fig8">Figure 8</xref>). However, simulations by <xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref> show that their closely related model can sometimes persist in habitual behaviour even if it is not desired. Therefore, it is possible that there may exist other mechanisms that may help the goal-directed system to regain control if habitual behaviour ceases to be appropriate. For example, it has been proposed that a sudden increase in prediction errors occurring when environment changes may attract attention and result in the goal-directed system taking charge of animals’ choices (<xref ref-type="bibr" rid="bib26">FitzGerald et al., 2014</xref>).</p><p>Finally, let us discuss the relationship of the DopAct framework to an observation that habits are more difficult to produce in variable ratio schedules than variable interval schedules (<xref ref-type="bibr" rid="bib17">Dickinson et al., 1983</xref>). In the variable ratio schedules a lever press is followed by a reward with a fixed probability <inline-formula><mml:math id="inf242"><mml:mi>p</mml:mi></mml:math></inline-formula>. By contrast in the variable interval schedule a lever press is followed by a reward only if the reward is 'available'. Just after consuming a reward, lever pressing has no effect, and another reward may become “available” as time goes on with a fixed probability per unit of time. An elegant explanation for why habit formation depends on the schedule has been provided by <xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref>, and a partially similar explanation can be given within the DopAct framework, as we now summarize. <xref ref-type="bibr" rid="bib53">Miller et al., 2019</xref> noticed that reward rate as a function of action frequency follows qualitatively different relationships in different schedules. In particular, in the variable ratio schedule the expected number of rewards per unit time is directly proportional to number of lever presses, i.e. <inline-formula><mml:math id="inf243"><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi></mml:math></inline-formula>. By contrast, in the variable interval schedule, the reward rate initially increases with the number of level presses, but beyond some frequency there is little benefit of responding more often, so the reward rate is a nonlinear saturating function of action frequency. The model selecting action intensity in the DopAct framework assumes a linear dependence of mean reward on action intensity (orange Equation 3.2), so in the variable ratio schedule, it will learn <inline-formula><mml:math id="inf244"><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi></mml:math></inline-formula>, and then predict mean reward accurately no matter what action intensity is selected. By contrast, in the variable interval schedule the predictions will be less accurate, because the form of the actual dependence of reward on action frequency is different to that assumed by the model. Consequently, the reward uncertainty of the goal-directed system <inline-formula><mml:math id="inf245"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is likely to be lower in the variable ratio than variable interval schedule. This decreased uncertainty makes the goal-directed system less likely to give in to the habit system, resulting in less habitual behaviour in the variable ratio schedule.</p></sec><sec id="s3-4"><title>Experimental predictions</title><p>We start with describing two most critical predictions of the DopAct framework, testing of which may validate or falsify the two key assumptions of the framework, and next we discuss other predictions. The first key prediction of the DopAct framework is that the dopaminergic neurons in the habit system should respond to movements more, when they are not habitual, e.g. at an initial phase of task acquisition or after a reversal (<xref ref-type="fig" rid="fig11">Figure 11C</xref>). This prediction could be tested by monitoring the activity of dopaminergic neurons projecting to dorsolateral striatum in a task where animals are trained to perform a particular response for sufficiently long that it becomes habitual, and then the required response is reversed. The framework predicts that these dopaminergic neurons should have higher activity during initial training and in a period after the reversal, than during the period when the action is habitual.</p><p>The second key prediction follows from a central feature of the DopAct framework that the expectation of the reward in the goal-directed system arises from forming a motor plan to obtain it. Thus the framework predicts that the dopaminergic responses in the goal-directed system to stimuli predicting a reward should last longer if planning actions to obtain the reward takes more time, or if an animal is prevented from making a response. One way to test this prediction would be to optogenetically block striatal neurons expressing D1 receptors in the goal-directed system for a fixed period after the onset of a stimulus, so the action plan cannot be formed. The framework predicts that such manipulation should prolong the response of dopaminergic neurons in that system. Another way of testing this prediction would be to employ a task where goal-directed planning becomes more efficient and thus shorter with practice. The framework predicts that in such tasks the responses of dopaminergic neurons in the goal-directed system during action planning should get briefer with practice, and their duration should be correlated with reaction time across stages of task acquisition.</p><p>The DopAct framework also predicts distinct patterns of activity for different populations of dopaminergic neurons. As already mentioned above, dopaminergic neurons in the habit system should respond to movements more, when they are not habitual. When the movements become highly habitual, these neurons should tend to more often produce brief decreases in response (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, right). Furthermore, when the choices become mostly driven by the habit system, then dopaminergic neurons in the goal-directed system should no longer signal reward prediction error after stimulus (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, right). By contrast, the dopaminergic neurons in the valuation system should signal reward prediction error after stimulus even once the action becomes habitual (<xref ref-type="fig" rid="fig7">Figure 7B</xref>).</p><p>Patterns of prediction errors expected from the DopAct framework could also be investigated with fMRI. Models developed within the framework could be fitted to behaviour of human participants performing choice tasks. Such models could then generate patterns of different prediction errors (<inline-formula><mml:math id="inf246"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf247"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf248"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) expected on individual trials. Since prediction errors encoded by dopaminergic neurons are also correlated with striatal BOLD signal (<xref ref-type="bibr" rid="bib58">O'Doherty et al., 2004</xref>), one could investigate if different prediction errors in the DopAct framework are correlated with BOLD signal in different striatal regions.</p><p>In the DopAct framework dopaminergic neurons increase the gain of striatal neurons during action planning, only in the goal-directed but not in the habit system. Therefore, the framework predicts that the dopamine concentration should have a larger effect on the slope of firing-Input curves for the striatal neurons in the goal-directed than the habit system. This prediction may seem surprising, because striatal neurons express dopaminergic receptors throughout the striatum (<xref ref-type="bibr" rid="bib39">Huntley et al., 1992</xref>). Nevertheless, it is consistent with reduced effects of dopamine blockade on habitual movements (<xref ref-type="bibr" rid="bib11">Choi et al., 2005</xref>) that are known to rely on dorsolateral striatum (<xref ref-type="bibr" rid="bib79">Yin et al., 2004</xref>). Accordingly, the DopAct framework predicts that the dopaminergic modulation in dorsolateral striatum should primarily affect plasticity rather than excitability of neurons.</p></sec><sec id="s3-5"><title>Directions for future work</title><p>This paper described a general framework for understanding the function of dopaminergic neurons in the basal ganglia, and presented simple models capturing only a subset of experimental data. To describe responses observed in more realistically complex tasks, models could be developed following a similar procedure as in this paper. Namely, a probabilistic model could be formulated for a task, and a network minimizing the corresponding free-energy derived, simulated and compared with experimental data. This section highlights key experimental observations the models described in this paper are unable to capture, and suggests directions for developing models consistent with them.</p><p>The presented models do not mechanistically explain the dependence of dopamine release in ventral striatum on motivational state such as hunger or thirst (<xref ref-type="bibr" rid="bib59">Papageorgiou et al., 2016</xref>). To reproduce these activity patterns, it will be important to extend the framework to describe the computations in the valuation system. It will also be important to better understand the interactions between the valuation and goal-directed systems during the choice of action intensity. In the presented model, the selected action intensity depends on the value of the state estimated by the valuation system, and conversely, the produced action intensity influences reward and thus the value learned by the valuation system. In the presented simulations the parameters (e.g. learning rates) were chosen such that the model learned to select action intensity giving highest reward, but such behaviour was not present for all parameter values. Hence it needs to be understood how the interactions between the valuation and goal-directed systems need to be set up so the model robustly finds the action intensity giving the maximum reward.</p><p>The models do not describe how the striatal neurons distinguish whether dopaminergic prediction error should affect their plasticity or excitability, and for simplicity, in the presented simulations we allowed the weights to be modified only when reward was presented. However, the same dopaminergic signal after a stimulus predicting reward may need to trigger plasticity in one group of striatal neurons (selective for a past action that led to this valuable state), and changes in excitability in another group (selective for a future action). It will be important to further understand the mechanisms which can be employed by striatal neurons to appropriately react to dopamine signals (<xref ref-type="bibr" rid="bib4">Berke, 2018</xref>; <xref ref-type="bibr" rid="bib54">Mohebi et al., 2019</xref>).</p><p>The models presented in this paper described only a part of the basal ganglia circuit, and it will be important to include also other elements of the circuit. In particular, this paper focussed on a subset of striatal neurons expressing D1 receptors, which project directly to the output nuclei and facilitate movements, but another population expressing D2 receptors projects via an indirect pathway and inhibits movements (<xref ref-type="bibr" rid="bib45">Kravitz et al., 2010</xref>). Computational models suggest that these neurons predominantly learn from negative feedback (<xref ref-type="bibr" rid="bib12">Collins and Frank, 2014</xref>; <xref ref-type="bibr" rid="bib52">Mikhael and Bogacz, 2016</xref>; <xref ref-type="bibr" rid="bib55">Möller and Bogacz, 2019</xref>) and it would be interesting include their role in preventing unsuitable movements in the DopAct framework.</p><p>The basal ganglia circuit also includes a hyperdirect pathway, which contains the subthalamic nucleus. It has been proposed that a function of the subthalamic nucleus is to inhibit non-selected actions (<xref ref-type="bibr" rid="bib32">Gurney et al., 2001</xref>), and the hyperdirect pathway may support the competition between actions that is present in the framework. The subthalamic nucleus has also been proposed to be involved in determining when the planning process should finish and action should be initiated (<xref ref-type="bibr" rid="bib27">Frank et al., 2007</xref>). For simplicity, in this paper the process of action planning has been simulated for a fixed interval (until time <inline-formula><mml:math id="inf249"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> in <xref ref-type="fig" rid="fig7">Figures 7</xref> and <xref ref-type="fig" rid="fig11">11</xref>). It will be important to extend the framework to describe the mechanisms initiating an action. If actions were executed as soon as a motor plan is formed, the increase in the habit prediction error would be briefer than that depicted in <xref ref-type="fig" rid="fig7">Figure 7C</xref>. In such an extended model the valuation and goal-directed systems would also need to be modified to learn to expect reward at a particular time after the action.</p><p>The presented models cannot reproduce the ramping of dopaminergic activity, observed as animals approached rewards (<xref ref-type="bibr" rid="bib36">Howe et al., 2013</xref>). To capture these data, the valuation system could incorporate synaptic decay that has been shown to allow standard reinforcement learning models to reproduce the ramping of prediction error (<xref ref-type="bibr" rid="bib43">Kato and Morita, 2016</xref>).</p><p>It has been also observed that dopaminergic neurons respond not only to unexpected magnitude of reward, but also when the type of reward differs from that expected (<xref ref-type="bibr" rid="bib73">Takahashi et al., 2017</xref>). To capture such prediction errors, the framework could be extended to assume that each system tries to predict multiple dimensions of reward or movement (cf <xref ref-type="bibr" rid="bib30">Gardner et al., 2018</xref>).</p><p>Finally, dopaminergic neurons also project to regions beyond basal ganglia, such as amygdala, which plays a role in habit formation (<xref ref-type="bibr" rid="bib3">Balleine et al., 2003</xref>), and cortex, where they have been proposed to modulate synaptic plasticity (<xref ref-type="bibr" rid="bib64">Roelfsema and van Ooyen, 2005</xref>). It would be interesting to extend the DopAct framework to capture dopamine role in learning and action planning in these regions.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>This section describes details of simulations of models developed within the DopAct framework for two tasks: selecting action intensity and choice between two actions. The models were simulated in Matlab (RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001622">SCR_001622</ext-link>), and all codes are available at MRC Brain Network Dynamics Unit Data Sharing Platform (<ext-link ext-link-type="uri" xlink:href="https://data.mrc.ox.ac.uk/data-set/simulations-action-inference">https://data.mrc.ox.ac.uk/data-set/simulations-action-inference</ext-link>).</p><sec id="s4-1"><title>Selecting action intensity</title><p>We first describe the valuation system, and then provide details of the model in various simulated scenarios.</p><p>The valuation system was based on the standard temporal difference model (<xref ref-type="bibr" rid="bib56">Montague et al., 1996</xref>). Following that model we assume that the valuation system can access information on how long ago a stimulus was presented. In particular, we assume that time can be divided into brief intervals of length <inline-formula><mml:math id="inf250"><mml:mi>I</mml:mi></mml:math></inline-formula>. The state of the environment is represented by a column vector <inline-formula><mml:math id="inf251"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with entries corresponding to individual intervals, such that <inline-formula><mml:math id="inf252"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> if the stimulus has been present in the current interval, <inline-formula><mml:math id="inf253"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> if the stimulus was present in the previous interval, etc. Although more realistic generalizations of this representation have been proposed (<xref ref-type="bibr" rid="bib15">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib49">Ludvig et al., 2008</xref>), we use this standard representation for simplicity.</p><p><xref ref-type="fig" rid="fig12">Figure 12A</xref> lists equations describing the valuation system, which are based on temporal difference learning but adapted to continuous time. According to Equation 12.1, the estimate of the value of state <inline-formula><mml:math id="inf254"><mml:mi>s</mml:mi></mml:math></inline-formula> converges in equilibrium to <inline-formula><mml:math id="inf255"><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf256"><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> denotes a row vector of parameters describing how much reward can be expected after stimulus appearing in a particular interval. Equation 12.2 describes the dynamics of the prediction error in the valuation system, which converges to a difference between total reward (<inline-formula><mml:math id="inf257"><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>v</mml:mi></mml:math></inline-formula>) and the expectation of that reward made at a previous interval (<inline-formula><mml:math id="inf258"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), as in the standard temporal difference learning (<xref ref-type="bibr" rid="bib71">Sutton and Barto, 1998</xref>). The weight parameters are modified proportionally to the prediction error as described by Equation 12.3, where <inline-formula><mml:math id="inf259"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a learning rate, and <inline-formula><mml:math id="inf260"><mml:mover accent="true"><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> are eligibility traces associated with weights <inline-formula><mml:math id="inf261"><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula>, which describe when the weights can be modified. In basic reinforcement learning <inline-formula><mml:math id="inf262"><mml:mover accent="true"><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, i.e. a weight can only be modified if the corresponding state is present. Equation 12.4 describes the dynamics of the eligibility traces, and if one ignored the first term on the right, it would converge to <inline-formula><mml:math id="inf263"><mml:mover accent="true"><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. The first term on the right of Equation 12.4 ensures that the eligibility traces persist over time, and parameter <inline-formula><mml:math id="inf264"><mml:mi>λ</mml:mi></mml:math></inline-formula> describes what fraction of the eligibility traces survives from one interval to the next (<xref ref-type="bibr" rid="bib49">Ludvig et al., 2008</xref>). Such persistent eligibility traces are known to speed up learning (<xref ref-type="bibr" rid="bib71">Sutton and Barto, 1998</xref>). The first term on the right of Equation 12.4 includes an eligibility trace from time <inline-formula><mml:math id="inf265"><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mn>3</mml:mn><mml:mi>τ</mml:mi></mml:math></inline-formula>, that is from a time slightly further than one interval ago, to avoid the influence of transient dynamics occurring at the transition between intervals. It is also ensured in the simulations that parameters <inline-formula><mml:math id="inf266"><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> do not become negative, as the desired reward value <inline-formula><mml:math id="inf267"><mml:mi>v</mml:mi></mml:math></inline-formula> computed by the valuation system should not be negative. Thus if any element of <inline-formula><mml:math id="inf268"><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> becomes negative, it is set to 0. Finally, Equation 12.5 describes the dynamics of the reward signal <inline-formula><mml:math id="inf269"><mml:mi>r</mml:mi></mml:math></inline-formula>, which follows the actual value to reward <inline-formula><mml:math id="inf270"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. This dynamics has been introduced so that the reward signal rises with the same rate as the value estimate (the same time constant is used in Equations 12.1 and 12.5), and these quantities can be subtracted to result in no prediction error when the reward obtained is equal to that predicted by the valuation system.</p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Description of the valuation system.</title><p>(<bold>A</bold>) Temporal difference learning model used in simulations of action intensity selection. (<bold>B</bold>) A simplified model used in simulations of choice between two actions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53262-fig12-v2.tif"/></fig><p>In simulations involving selection of action intensity, the time represented by the valuation system was divided into intervals of <inline-formula><mml:math id="inf271"><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:math></inline-formula>. The stimulus was presented at time <inline-formula><mml:math id="inf272"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, while the reward was given at time <inline-formula><mml:math id="inf273"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>, thus the valuation system represented the value of 5 time intervals (i.e. vectors <inline-formula><mml:math id="inf274"><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula>, <inline-formula><mml:math id="inf275"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf276"><mml:mover accent="true"><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> had 5 elements each). The parameters controlling retention of eligibility trace was set to <inline-formula><mml:math id="inf277"><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math></inline-formula>. The state provided to the actor was equal to <inline-formula><mml:math id="inf278"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> from time <inline-formula><mml:math id="inf279"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> onwards. We assumed that the intensity of action executed by the agent was equal to the inferred action intensity plus motor noise with standard deviation <inline-formula><mml:math id="inf280"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> (this random number was added to action intensity at time <inline-formula><mml:math id="inf281"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>). During intervals in which rewards were provided (from <inline-formula><mml:math id="inf282"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> onwards) the parameters were continuously updated according to Equations 6.8-9. In simulations the learning rates were set to: <inline-formula><mml:math id="inf283"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf284"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf285"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf286"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf287"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula>. The time constants were set to: <inline-formula><mml:math id="inf288"><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf289"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:math></inline-formula>, and the differential equations were solved numerically using Euler method with integration step 0.001. The model parameters were initialized to: <inline-formula><mml:math id="inf290"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf291"><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf292"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf293"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math></inline-formula>.</p><p>To simulate devaluation, the expectation of reward was set to 0 by setting <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, as a recent modelling study suggests that such scaling of learned parameters by motivational state is required for reproducing experimentally observed effects of motivational state on dopaminergic responses encoding reward prediction error (<xref ref-type="bibr" rid="bib77">van Swieten and Bogacz, 2020</xref>).</p><p>In the simulations of Pavlovian-instrumental transfer, the valuation system was learning the values of two states corresponding to the presence of the lever and the conditioned stimulus. Thus the state vector <inline-formula><mml:math id="inf295"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> had 10 entries, where the first 5 entries were set to 1 at different intervals after 'lever appearance', while the other 5 entries were set to 1 at different intervals after conditioned stimulus. Consequently, the vector of parameters of the valuation system <inline-formula><mml:math id="inf296"><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> also had 10 entries. The simulations of the first stage (operant conditioning) consisted of 100 trials in which the model was trained analogously as in the simulations described in the above paragraph. At this stage only first 5 entries of vector <inline-formula><mml:math id="inf297"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> could take non-zero values, and hence only the first 5 entries of <inline-formula><mml:math id="inf298"><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> were modified. The state provided to the actor was equal to <inline-formula><mml:math id="inf299"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> when 'lever appeared' that is from time <inline-formula><mml:math id="inf300"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> onwards. The simulations of the second stage (classical conditioning) consisted of 100 trials in which only the valuation system was learning. At this stage, the conditioned stimulus was presented at time <inline-formula><mml:math id="inf301"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, and the reward <inline-formula><mml:math id="inf302"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn> <mml:mi/></mml:math></inline-formula> was given at time <inline-formula><mml:math id="inf303"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>, thus <inline-formula><mml:math id="inf304"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> for <inline-formula><mml:math id="inf305"><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>1.2</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> for <inline-formula><mml:math id="inf306"><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mn>1.2</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>1.4</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, etc. The simulations of the third stage (testing) consisted of 60 trials in which only negative reward accounting for effort <inline-formula><mml:math id="inf307"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:math></inline-formula> was given. On trials 21-30 and 41-50, both 'lever and conditioned stimulus were presented', that is <inline-formula><mml:math id="inf308"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> for <inline-formula><mml:math id="inf309"><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>1.2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, etc., while on the other trials only the 'lever was presented'. The model was simulated with the same parameters as described in the previous paragraph, except for modified values of two learning rates <inline-formula><mml:math id="inf310"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.015</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf311"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.005</mml:mn></mml:math></inline-formula>, to reproduce the dynamics of learning shown by experimental animals.</p><p>In all simulations in this paper, a constraint (or a 'hyperprior') on the minimum value of the variance parameters was introduced, such that if <inline-formula><mml:math id="inf312"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf313"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> decreased below 0.2, it was set to 0.2.</p></sec><sec id="s4-2"><title>Choice between two actions</title><p>Analogously, as in the previous section, we first describe the valuation system, and then provide the details of the simulations.</p><p>In the simulations of choice, we used a simplified version of the valuation system, which for each state <inline-formula><mml:math id="inf314"><mml:mi>j</mml:mi></mml:math></inline-formula> learns a single parameter <inline-formula><mml:math id="inf315"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mi/></mml:math></inline-formula> (rather than the vector of parameters encoding the reward predicted in different moments in time). The equations describing this simplified valuation system are shown in <xref ref-type="fig" rid="fig12">Figure 12B</xref>. According to Equation 12.6, the estimate of the value of state <inline-formula><mml:math id="inf316"><mml:mi>s</mml:mi></mml:math></inline-formula> converges in equilibrium to <inline-formula><mml:math id="inf317"><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula>. Following reward delivery, parameters <inline-formula><mml:math id="inf318"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are modified according to Equation 12.7, where <inline-formula><mml:math id="inf319"><mml:mi>v</mml:mi></mml:math></inline-formula> is taken as the estimated value at the end of simulation of the planning phase on this trial.</p><p>In order to simulate the actor, its description has been converted to differential equations in analogous way as in <xref ref-type="fig" rid="fig6">Figure 6C</xref>. At the end of the planning phase, Gaussian noise with standard deviation <inline-formula><mml:math id="inf320"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> was added to all entries of the action vector (to allow exploration), and the action with the highest intensity was 'chosen' by the model. Subsequently, for the chosen action <inline-formula><mml:math id="inf321"><mml:mi>i</mml:mi></mml:math></inline-formula> the intensity was set to <inline-formula><mml:math id="inf322"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, while for the other action it was set to <inline-formula><mml:math id="inf323"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. For simplicity we did not explicitly simulate the dynamics of the model after the delivery of reward <inline-formula><mml:math id="inf324"><mml:mi>r</mml:mi></mml:math></inline-formula>, but we computed the prediction errors in the goal-directed and habit system in an equilibrium (orange Equation 9.6 and Equation 10.1), and updated the parameters. In simulations the learning rate in the valuation system was set to <inline-formula><mml:math id="inf325"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula> on trials with <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and to <inline-formula><mml:math id="inf327"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula> when <inline-formula><mml:math id="inf328"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. Other learning rates were set to: <inline-formula><mml:math id="inf329"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf330"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf331"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:math></inline-formula>. The remaining parameters of the simulations had the same value as in the previous section.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work has been supported by MRC grants MC_UU_12024/5, MC_UU_00003/1 and BBSRC grant BB/S006338/1. The author thanks Moritz Moeller and Sashank Pisupati for comments on an earlier version of the manuscript, and Karl Friston, Yonatan Loewenstein, Mark Howe, Friedemann Zenke, Kevin Miller and Peter Dayan for discussion.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-53262-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Matlab codes for all simulations described in the paper are available at MRC Brain Network Dynamics Unit Data Sharing Platform (<ext-link ext-link-type="uri" xlink:href="https://data.mrc.ox.ac.uk/data-set/simulations-action-inference">https://data.mrc.ox.ac.uk/data-set/simulations-action-inference</ext-link>). Users must first create a free account (<ext-link ext-link-type="uri" xlink:href="https://data.mrc.ox.ac.uk/user/register">https://data.mrc.ox.ac.uk/user/register</ext-link>) before they can download the datasets from the site.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Simulations of action inference</data-title><source>MRC Brain Network Dynamics Unit Data Sharing Platform</source><pub-id assigning-authority="Oxford University" pub-id-type="accession" xlink:href="https://data.mrc.ox.ac.uk/data-set/simulations-action-inference">simulations-action-inference</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname> <given-names>GE</given-names></name><name><surname>DeLong</surname> <given-names>MR</given-names></name><name><surname>Strick</surname> <given-names>PL</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Parallel organization of functionally segregated circuits linking basal ganglia and cortex</article-title><source>Annual Review of Neuroscience</source><volume>9</volume><fpage>357</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.09.030186.002041</pub-id><pub-id pub-id-type="pmid">3085570</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Attias</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Planning by probabilistic inference</article-title><conf-name>AISTATS</conf-name></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balleine</surname> <given-names>BW</given-names></name><name><surname>Killcross</surname> <given-names>AS</given-names></name><name><surname>Dickinson</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The effect of lesions of the basolateral amygdala on instrumental conditioning</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>666</fpage><lpage>675</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-02-00666.2003</pub-id><pub-id pub-id-type="pmid">12533626</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berke</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What does dopamine mean?</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>787</fpage><lpage>793</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0152-y</pub-id><pub-id pub-id-type="pmid">29760524</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berridge</surname> <given-names>KC</given-names></name><name><surname>Robinson</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>What is the role of dopamine in reward: hedonic impact, reward learning, or incentive salience?</article-title><source>Brain Research Reviews</source><volume>28</volume><fpage>309</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1016/S0165-0173(98)00019-8</pub-id><pub-id pub-id-type="pmid">9858756</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Björklund</surname> <given-names>A</given-names></name><name><surname>Dunnett</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Dopamine neuron systems in the brain: an update</article-title><source>Trends in Neurosciences</source><volume>30</volume><fpage>194</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2007.03.006</pub-id><pub-id pub-id-type="pmid">17408759</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A tutorial on the free-energy framework for modelling perception and learning</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>198</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2015.11.003</pub-id><pub-id pub-id-type="pmid">28298703</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>Gurney</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The basal ganglia and cortex implement optimal decision making between alternative actions</article-title><source>Neural Computation</source><volume>19</volume><fpage>442</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.2.442</pub-id><pub-id pub-id-type="pmid">17206871</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckley</surname> <given-names>CL</given-names></name><name><surname>Kim</surname> <given-names>CS</given-names></name><name><surname>McGregor</surname> <given-names>S</given-names></name><name><surname>Seth</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The free energy principle for action and perception: a mathematical review</article-title><source>Journal of Mathematical Psychology</source><volume>81</volume><fpage>55</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2017.09.004</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname> <given-names>AC</given-names></name><name><surname>Nakamura</surname> <given-names>K</given-names></name><name><surname>Roesch</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>From ventral-medial to dorsal-lateral striatum: neural correlates of reward-guided decision-making</article-title><source>Neurobiology of Learning and Memory</source><volume>117</volume><fpage>51</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2014.05.003</pub-id><pub-id pub-id-type="pmid">24858182</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname> <given-names>WY</given-names></name><name><surname>Balsam</surname> <given-names>PD</given-names></name><name><surname>Horvitz</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Extended habit training reduces dopamine mediation of appetitive response expression</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>6729</fpage><lpage>6733</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1498-05.2005</pub-id><pub-id pub-id-type="pmid">16033882</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>AGE</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Opponent actor learning (OpAL): Modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive</article-title><source>Psychological Review</source><volume>121</volume><fpage>337</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1037/a0037015</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>da Silva</surname> <given-names>JA</given-names></name><name><surname>Tecuapetla</surname> <given-names>F</given-names></name><name><surname>Paixão</surname> <given-names>V</given-names></name><name><surname>Costa</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dopamine neuron activity before action initiation gates and invigorates future movements</article-title><source>Nature</source><volume>554</volume><fpage>244</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1038/nature25457</pub-id><pub-id pub-id-type="pmid">29420469</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1704</fpage><lpage>1711</lpage><pub-id pub-id-type="doi">10.1038/nn1560</pub-id><pub-id pub-id-type="pmid">16286932</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Courville</surname> <given-names>AC</given-names></name><name><surname>Tourtezky</surname> <given-names>DS</given-names></name><name><surname>Touretzky</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Representation and timing in theories of the dopamine system</article-title><source>Neural Computation</source><volume>18</volume><fpage>1637</fpage><lpage>1677</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.7.1637</pub-id><pub-id pub-id-type="pmid">16764517</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Wit</surname> <given-names>S</given-names></name><name><surname>Barker</surname> <given-names>RA</given-names></name><name><surname>Dickinson</surname> <given-names>AD</given-names></name><name><surname>Cools</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Habitual versus goal-directed action control in parkinson disease</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>1218</fpage><lpage>1229</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21514</pub-id><pub-id pub-id-type="pmid">20429859</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dickinson</surname> <given-names>A</given-names></name><name><surname>Nicholas</surname> <given-names>DJ</given-names></name><name><surname>Adams</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The effect of the instrumental training contingency on susceptibility to reinforcer devaluation</article-title><source>The Quarterly Journal of Experimental Psychology Section B</source><volume>35</volume><fpage>35</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1080/14640748308400912</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dickinson</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Actions and habits: the development of behavioural autonomy</article-title><source>Philosophical Transactions of the Royal Society of London B, Biological Sciences</source><volume>308</volume><fpage>67</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1098/rstb.1985.0010</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dickinson</surname> <given-names>A</given-names></name><name><surname>Balleine</surname> <given-names>B</given-names></name><name><surname>Watt</surname> <given-names>A</given-names></name><name><surname>Gonzalez</surname> <given-names>F</given-names></name><name><surname>Boakes</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Motivational control after extended instrumental training</article-title><source>Animal Learning &amp; Behavior</source><volume>23</volume><fpage>197</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.3758/BF03199935</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dickinson</surname> <given-names>A</given-names></name><name><surname>Balleine</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>The role of learning in the operation of motivational systems</chapter-title><person-group person-group-type="editor"><name><surname>Pashler</surname> <given-names>H</given-names></name><name><surname>Gallistel</surname> <given-names>R</given-names></name></person-group><source>Handbook of Experimental Psychology</source><publisher-name>John Wiley &amp; Sons</publisher-name><fpage>497</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1002/0471214426.pas0312</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dodson</surname> <given-names>PD</given-names></name><name><surname>Dreyer</surname> <given-names>JK</given-names></name><name><surname>Jennings</surname> <given-names>KA</given-names></name><name><surname>Syed</surname> <given-names>EC</given-names></name><name><surname>Wade-Martins</surname> <given-names>R</given-names></name><name><surname>Cragg</surname> <given-names>SJ</given-names></name><name><surname>Bolam</surname> <given-names>JP</given-names></name><name><surname>Magill</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Representation of spontaneous movement by dopaminergic neurons is cell-type selective and disrupted in parkinsonism</article-title><source>PNAS</source><volume>113</volume><fpage>E2180</fpage><lpage>E2188</lpage><pub-id pub-id-type="doi">10.1073/pnas.1515941113</pub-id><pub-id pub-id-type="pmid">27001837</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engelhard</surname> <given-names>B</given-names></name><name><surname>Finkelstein</surname> <given-names>J</given-names></name><name><surname>Cox</surname> <given-names>J</given-names></name><name><surname>Fleming</surname> <given-names>W</given-names></name><name><surname>Jang</surname> <given-names>HJ</given-names></name><name><surname>Ornelas</surname> <given-names>S</given-names></name><name><surname>Koay</surname> <given-names>SA</given-names></name><name><surname>Thiberge</surname> <given-names>SY</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name><name><surname>Witten</surname> <given-names>IB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Specialized coding of sensory, motor and cognitive variables in VTA dopamine neurons</article-title><source>Nature</source><volume>570</volume><fpage>509</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1261-9</pub-id><pub-id pub-id-type="pmid">31142844</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eshel</surname> <given-names>N</given-names></name><name><surname>Tian</surname> <given-names>J</given-names></name><name><surname>Bukwich</surname> <given-names>M</given-names></name><name><surname>Uchida</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dopamine neurons share common response function for reward prediction error</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>479</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1038/nn.4239</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Estes</surname> <given-names>WK</given-names></name></person-group><year iso-8601-date="1943">1943</year><article-title>Discriminative conditioning. I. A discriminative property of conditioned anticipation</article-title><source>Journal of Experimental Psychology</source><volume>32</volume><fpage>150</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1037/h0058316</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faure</surname> <given-names>A</given-names></name><name><surname>Haberland</surname> <given-names>U</given-names></name><name><surname>Condé</surname> <given-names>F</given-names></name><name><surname>El Massioui</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Lesion to the nigrostriatal dopamine system disrupts stimulus-response habit formation</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>2771</fpage><lpage>2780</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3894-04.2005</pub-id><pub-id pub-id-type="pmid">15772337</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>FitzGerald</surname> <given-names>TH</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Model averaging, optimal inference, and habit formation</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>457</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00457</pub-id><pub-id pub-id-type="pmid">25018724</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Samanta</surname> <given-names>J</given-names></name><name><surname>Moustafa</surname> <given-names>AA</given-names></name><name><surname>Sherman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hold your horses: impulsivity, deep brain stimulation, and medication in parkinsonism</article-title><source>Science</source><volume>318</volume><fpage>1309</fpage><lpage>1312</lpage><pub-id pub-id-type="doi">10.1126/science.1146157</pub-id><pub-id pub-id-type="pmid">17962524</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The free-energy principle: a unified brain theory?</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>127</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nrn2787</pub-id><pub-id pub-id-type="pmid">20068583</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname> <given-names>MPH</given-names></name><name><surname>Schoenbaum</surname> <given-names>G</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rethinking dopamine as generalized prediction error</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>285</volume><elocation-id>20181645</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2018.1645</pub-id><pub-id pub-id-type="pmid">30464063</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dopamine, inference, and uncertainty</article-title><source>Neural Computation</source><volume>29</volume><fpage>3311</fpage><lpage>3326</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01023</pub-id><pub-id pub-id-type="pmid">28957023</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gurney</surname> <given-names>K</given-names></name><name><surname>Prescott</surname> <given-names>TJ</given-names></name><name><surname>Redgrave</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A computational model of action selection in the basal ganglia. I. A new functional anatomy</article-title><source>Biological Cybernetics</source><volume>84</volume><fpage>401</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1007/PL00007984</pub-id><pub-id pub-id-type="pmid">11417052</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haber</surname> <given-names>SN</given-names></name><name><surname>Fudge</surname> <given-names>JL</given-names></name><name><surname>McFarland</surname> <given-names>NR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Striatonigrostriatal pathways in primates form an ascending spiral from the shell to the dorsolateral striatum</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>2369</fpage><lpage>2382</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-06-02369.2000</pub-id><pub-id pub-id-type="pmid">10704511</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardwick</surname> <given-names>RM</given-names></name><name><surname>Forrence</surname> <given-names>AD</given-names></name><name><surname>Krakauer</surname> <given-names>JW</given-names></name><name><surname>Haith</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Time-dependent competition between goal-directed and habitual response preparation</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>1252</fpage><lpage>1262</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0725-0</pub-id><pub-id pub-id-type="pmid">31570762</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Houk</surname> <given-names>J</given-names></name><name><surname>Adams</surname> <given-names>J</given-names></name><name><surname>Barto</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>A model of how the basal ganglia generate and use neural signals that predict reinforcement</chapter-title><person-group person-group-type="editor"><name><surname>Houk</surname> <given-names>J. C</given-names></name><name><surname>Davis</surname> <given-names>J. L</given-names></name><name><surname>Beiser</surname> <given-names>D. G</given-names></name></person-group><source>Models of Information Processing in the Basal Ganglia</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/4708.001.0001</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howe</surname> <given-names>MW</given-names></name><name><surname>Tierney</surname> <given-names>PL</given-names></name><name><surname>Sandberg</surname> <given-names>SG</given-names></name><name><surname>Phillips</surname> <given-names>PE</given-names></name><name><surname>Graybiel</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Prolonged dopamine signalling in striatum signals proximity and value of distant rewards</article-title><source>Nature</source><volume>500</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/nature12475</pub-id><pub-id pub-id-type="pmid">23913271</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howe</surname> <given-names>MW</given-names></name><name><surname>Dombeck</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rapid signalling in distinct dopaminergic axons during locomotion and reward</article-title><source>Nature</source><volume>535</volume><fpage>505</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1038/nature18942</pub-id><pub-id pub-id-type="pmid">27398617</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hull</surname> <given-names>CL</given-names></name></person-group><year iso-8601-date="1952">1952</year><source>A Behavior System; an Introduction to Behavior Theory Concerning the Individual Organism</source><publisher-name>Yale University Press</publisher-name></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huntley</surname> <given-names>GW</given-names></name><name><surname>Morrison</surname> <given-names>JH</given-names></name><name><surname>Prikhozhan</surname> <given-names>A</given-names></name><name><surname>Sealfon</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Localization of multiple dopamine receptor subtype mRNAs in human and monkey motor cortex and striatum</article-title><source>Molecular Brain Research</source><volume>15</volume><fpage>181</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/0169-328X(92)90107-M</pub-id><pub-id pub-id-type="pmid">1331674</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname> <given-names>X</given-names></name><name><surname>Costa</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Start/stop signals emerge in nigrostriatal circuits during sequence learning</article-title><source>Nature</source><volume>466</volume><fpage>457</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nature09263</pub-id><pub-id pub-id-type="pmid">20651684</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>LA</given-names></name><name><surname>Nebeck</surname> <given-names>SD</given-names></name><name><surname>Muralidharan</surname> <given-names>A</given-names></name><name><surname>Johnson</surname> <given-names>MD</given-names></name><name><surname>Baker</surname> <given-names>KB</given-names></name><name><surname>Vitek</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Closed-Loop deep brain stimulation effects on parkinsonian motor symptoms in a Non-Human primate - Is beta enough?</article-title><source>Brain Stimulation</source><volume>9</volume><fpage>892</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.1016/j.brs.2016.06.051</pub-id><pub-id pub-id-type="pmid">27401045</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juechems</surname> <given-names>K</given-names></name><name><surname>Summerfield</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Where does value come from?</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>836</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.07.012</pub-id><pub-id pub-id-type="pmid">31494042</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kato</surname> <given-names>A</given-names></name><name><surname>Morita</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Forgetting in reinforcement learning links sustained dopamine signals to motivation</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005145</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005145</pub-id><pub-id pub-id-type="pmid">27736881</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keramati</surname> <given-names>M</given-names></name><name><surname>Gutkin</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Homeostatic reinforcement learning for integrating reward collection and physiological stability</article-title><source>eLife</source><volume>3</volume><elocation-id>e04811</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04811</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname> <given-names>AV</given-names></name><name><surname>Freeze</surname> <given-names>BS</given-names></name><name><surname>Parker</surname> <given-names>PR</given-names></name><name><surname>Kay</surname> <given-names>K</given-names></name><name><surname>Thwin</surname> <given-names>MT</given-names></name><name><surname>Deisseroth</surname> <given-names>K</given-names></name><name><surname>Kreitzer</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Regulation of parkinsonian motor behaviours by optogenetic control of basal ganglia circuitry</article-title><source>Nature</source><volume>466</volume><fpage>622</fpage><lpage>626</lpage><pub-id pub-id-type="doi">10.1038/nature09159</pub-id><pub-id pub-id-type="pmid">20613723</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuśmierz</surname> <given-names>Ł</given-names></name><name><surname>Isomura</surname> <given-names>T</given-names></name><name><surname>Toyoizumi</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning with three factors: modulating hebbian plasticity with errors</article-title><source>Current Opinion in Neurobiology</source><volume>46</volume><fpage>170</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.08.020</pub-id><pub-id pub-id-type="pmid">28918313</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lahiri</surname> <given-names>AK</given-names></name><name><surname>Bevan</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dopaminergic transmission rapidly and persistently enhances excitability of D1 Receptor-Expressing striatal projection neurons</article-title><source>Neuron</source><volume>106</volume><fpage>277</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.01.028</pub-id><pub-id pub-id-type="pmid">32075716</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>RS</given-names></name><name><surname>Mattar</surname> <given-names>MG</given-names></name><name><surname>Parker</surname> <given-names>NF</given-names></name><name><surname>Witten</surname> <given-names>IB</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reward prediction error does not explain movement selectivity in DMS-projecting dopamine neurons</article-title><source>eLife</source><volume>8</volume><elocation-id>e42992</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.42992</pub-id><pub-id pub-id-type="pmid">30946008</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludvig</surname> <given-names>EA</given-names></name><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Kehoe</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Stimulus representation and the timing of reward-prediction errors in models of the dopamine system</article-title><source>Neural Computation</source><volume>20</volume><fpage>3034</fpage><lpage>3054</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.11-07-654</pub-id><pub-id pub-id-type="pmid">18624657</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsumoto</surname> <given-names>M</given-names></name><name><surname>Hikosaka</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Two types of dopamine neuron distinctly convey positive and negative motivational signals</article-title><source>Nature</source><volume>459</volume><fpage>837</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1038/nature08028</pub-id><pub-id pub-id-type="pmid">19448610</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClure</surname> <given-names>SM</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Montague</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A computational substrate for incentive salience</article-title><source>Trends in Neurosciences</source><volume>26</volume><fpage>423</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(03)00177-2</pub-id><pub-id pub-id-type="pmid">12900173</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikhael</surname> <given-names>JG</given-names></name><name><surname>Bogacz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Learning reward uncertainty in the basal ganglia</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005062</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005062</pub-id><pub-id pub-id-type="pmid">27589489</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>KJ</given-names></name><name><surname>Shenhav</surname> <given-names>A</given-names></name><name><surname>Ludvig</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Habits without values</article-title><source>Psychological Review</source><volume>126</volume><fpage>292</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1037/rev0000120</pub-id><pub-id pub-id-type="pmid">30676040</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohebi</surname> <given-names>A</given-names></name><name><surname>Pettibone</surname> <given-names>JR</given-names></name><name><surname>Hamid</surname> <given-names>AA</given-names></name><name><surname>Wong</surname> <given-names>JT</given-names></name><name><surname>Vinson</surname> <given-names>LT</given-names></name><name><surname>Patriarchi</surname> <given-names>T</given-names></name><name><surname>Tian</surname> <given-names>L</given-names></name><name><surname>Kennedy</surname> <given-names>RT</given-names></name><name><surname>Berke</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dissociable dopamine dynamics for learning and motivation</article-title><source>Nature</source><volume>570</volume><fpage>65</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1235-y</pub-id><pub-id pub-id-type="pmid">31118513</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Möller</surname> <given-names>M</given-names></name><name><surname>Bogacz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning the payoffs and costs of actions</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006285</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006285</pub-id><pub-id pub-id-type="pmid">30818357</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montague</surname> <given-names>PR</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A framework for mesencephalic dopamine systems based on predictive hebbian learning</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>1936</fpage><lpage>1947</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-05-01936.1996</pub-id><pub-id pub-id-type="pmid">8774460</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Joel</surname> <given-names>D</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Tonic dopamine: opportunity costs and the control of response vigor</article-title><source>Psychopharmacology</source><volume>191</volume><fpage>507</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1007/s00213-006-0502-4</pub-id><pub-id pub-id-type="pmid">17031711</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Doherty</surname> <given-names>J</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Schultz</surname> <given-names>J</given-names></name><name><surname>Deichmann</surname> <given-names>R</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title><source>Science</source><volume>304</volume><fpage>452</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1126/science.1094285</pub-id><pub-id pub-id-type="pmid">15087550</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papageorgiou</surname> <given-names>GK</given-names></name><name><surname>Baudonnat</surname> <given-names>M</given-names></name><name><surname>Cucca</surname> <given-names>F</given-names></name><name><surname>Walton</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mesolimbic dopamine encodes prediction errors in a State-Dependent manner</article-title><source>Cell Reports</source><volume>15</volume><fpage>221</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.03.031</pub-id><pub-id pub-id-type="pmid">27050518</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preston</surname> <given-names>RJ</given-names></name><name><surname>Bishop</surname> <given-names>GA</given-names></name><name><surname>Kitai</surname> <given-names>ST</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Medium spiny neuron projection from the rat striatum: an intracellular horseradish peroxidase study</article-title><source>Brain Research</source><volume>183</volume><fpage>253</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(80)90462-X</pub-id><pub-id pub-id-type="pmid">7353139</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>RP</given-names></name><name><surname>Ballard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname> <given-names>JN</given-names></name><name><surname>Hyland</surname> <given-names>BI</given-names></name><name><surname>Wickens</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A cellular mechanism of reward-related learning</article-title><source>Nature</source><volume>413</volume><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/35092560</pub-id><pub-id pub-id-type="pmid">11544526</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rochester</surname> <given-names>L</given-names></name><name><surname>Hetherington</surname> <given-names>V</given-names></name><name><surname>Jones</surname> <given-names>D</given-names></name><name><surname>Nieuwboer</surname> <given-names>A</given-names></name><name><surname>Willems</surname> <given-names>AM</given-names></name><name><surname>Kwakkel</surname> <given-names>G</given-names></name><name><surname>Van Wegen</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The effect of external rhythmic cues (auditory and visual) on walking during a functional task in homes of people with Parkinson's disease</article-title><source>Archives of Physical Medicine and Rehabilitation</source><volume>86</volume><fpage>999</fpage><lpage>1006</lpage><pub-id pub-id-type="doi">10.1016/j.apmr.2004.10.040</pub-id><pub-id pub-id-type="pmid">15895348</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelfsema</surname> <given-names>PR</given-names></name><name><surname>van Ooyen</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Attention-gated reinforcement learning of internal representations for classification</article-title><source>Neural Computation</source><volume>17</volume><fpage>2176</fpage><lpage>2214</lpage><pub-id pub-id-type="doi">10.1162/0899766054615699</pub-id><pub-id pub-id-type="pmid">16105222</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname> <given-names>W</given-names></name><name><surname>Ruffieux</surname> <given-names>A</given-names></name><name><surname>Aebischer</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The activity of pars Compacta neurons of the monkey substantia nigra in relation to motor activation</article-title><source>Experimental Brain Research</source><volume>51</volume><fpage>377</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1007/BF00237874</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Responses of midbrain dopamine neurons to behavioral trigger stimuli in the monkey</article-title><source>Journal of Neurophysiology</source><volume>56</volume><fpage>1439</fpage><lpage>1461</lpage><pub-id pub-id-type="doi">10.1152/jn.1986.56.5.1439</pub-id><pub-id pub-id-type="pmid">3794777</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname> <given-names>W</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Montague</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname> <given-names>W</given-names></name><name><surname>Flajolet</surname> <given-names>M</given-names></name><name><surname>Greengard</surname> <given-names>P</given-names></name><name><surname>Surmeier</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dichotomous dopaminergic control of striatal synaptic plasticity</article-title><source>Science</source><volume>321</volume><fpage>848</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1160575</pub-id><pub-id pub-id-type="pmid">18687967</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solway</surname> <given-names>A</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Goal-directed decision making as probabilistic inference: a computational framework and potential neural correlates</article-title><source>Psychological Review</source><volume>119</volume><fpage>120</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1037/a0026435</pub-id><pub-id pub-id-type="pmid">22229491</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephan</surname> <given-names>KE</given-names></name><name><surname>Manjaly</surname> <given-names>ZM</given-names></name><name><surname>Mathys</surname> <given-names>CD</given-names></name><name><surname>Weber</surname> <given-names>LA</given-names></name><name><surname>Paliwal</surname> <given-names>S</given-names></name><name><surname>Gard</surname> <given-names>T</given-names></name><name><surname>Tittgemeyer</surname> <given-names>M</given-names></name><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Haker</surname> <given-names>H</given-names></name><name><surname>Seth</surname> <given-names>AK</given-names></name><name><surname>Petzschner</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Allostatic Self-efficacy: a metacognitive theory of Dyshomeostasis-Induced fatigue and depression</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>550</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00550</pub-id><pub-id pub-id-type="pmid">27895566</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Introduction to Reinforcement Learning</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Syed</surname> <given-names>EC</given-names></name><name><surname>Grima</surname> <given-names>LL</given-names></name><name><surname>Magill</surname> <given-names>PJ</given-names></name><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>P</given-names></name><name><surname>Walton</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Action initiation shapes mesolimbic dopamine encoding of future rewards</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>34</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1038/nn.4187</pub-id><pub-id pub-id-type="pmid">26642087</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname> <given-names>YK</given-names></name><name><surname>Batchelor</surname> <given-names>HM</given-names></name><name><surname>Liu</surname> <given-names>B</given-names></name><name><surname>Khanna</surname> <given-names>A</given-names></name><name><surname>Morales</surname> <given-names>M</given-names></name><name><surname>Schoenbaum</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dopamine neurons respond to errors in the prediction of sensory features of expected rewards</article-title><source>Neuron</source><volume>95</volume><fpage>1395</fpage><lpage>1405</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.08.025</pub-id><pub-id pub-id-type="pmid">28910622</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thurley</surname> <given-names>K</given-names></name><name><surname>Senn</surname> <given-names>W</given-names></name><name><surname>Lüscher</surname> <given-names>HR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dopamine increases the gain of the input-output response of rat prefrontal pyramidal neurons</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>2985</fpage><lpage>2997</lpage><pub-id pub-id-type="doi">10.1152/jn.01098.2007</pub-id><pub-id pub-id-type="pmid">18400958</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tobler</surname> <given-names>PN</given-names></name><name><surname>Fiorillo</surname> <given-names>CD</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Adaptive coding of reward value by dopamine neurons</article-title><source>Science</source><volume>307</volume><fpage>1642</fpage><lpage>1645</lpage><pub-id pub-id-type="doi">10.1126/science.1105370</pub-id><pub-id pub-id-type="pmid">15761155</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tricomi</surname> <given-names>E</given-names></name><name><surname>Balleine</surname> <given-names>BW</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A specific role for posterior dorsolateral striatum in human habit learning</article-title><source>European Journal of Neuroscience</source><volume>29</volume><fpage>2225</fpage><lpage>2232</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2009.06796.x</pub-id><pub-id pub-id-type="pmid">19490086</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Swieten</surname> <given-names>MMH</given-names></name><name><surname>Bogacz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modeling the effects of motivation on choice and learning in the basal ganglia</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007465</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007465</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watabe-Uchida</surname> <given-names>M</given-names></name><name><surname>Zhu</surname> <given-names>L</given-names></name><name><surname>Ogawa</surname> <given-names>SK</given-names></name><name><surname>Vamanrao</surname> <given-names>A</given-names></name><name><surname>Uchida</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Whole-brain mapping of direct inputs to midbrain dopamine neurons</article-title><source>Neuron</source><volume>74</volume><fpage>858</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.017</pub-id><pub-id pub-id-type="pmid">22681690</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname> <given-names>HH</given-names></name><name><surname>Knowlton</surname> <given-names>BJ</given-names></name><name><surname>Balleine</surname> <given-names>BW</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning</article-title><source>European Journal of Neuroscience</source><volume>19</volume><fpage>181</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2004.03095.x</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname> <given-names>HH</given-names></name><name><surname>Ostlund</surname> <given-names>SB</given-names></name><name><surname>Knowlton</surname> <given-names>BJ</given-names></name><name><surname>Balleine</surname> <given-names>BW</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The role of the dorsomedial striatum in instrumental conditioning</article-title><source>European Journal of Neuroscience</source><volume>22</volume><fpage>513</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2005.04218.x</pub-id><pub-id pub-id-type="pmid">16045504</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaghloul</surname> <given-names>KA</given-names></name><name><surname>Blanco</surname> <given-names>JA</given-names></name><name><surname>Weidemann</surname> <given-names>CT</given-names></name><name><surname>McGill</surname> <given-names>K</given-names></name><name><surname>Jaggi</surname> <given-names>JL</given-names></name><name><surname>Baltuch</surname> <given-names>GH</given-names></name><name><surname>Kahana</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Human substantia nigra neurons encode unexpected financial rewards</article-title><source>Science</source><volume>323</volume><fpage>1496</fpage><lpage>1499</lpage><pub-id pub-id-type="doi">10.1126/science.1167342</pub-id><pub-id pub-id-type="pmid">19286561</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53262.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This manuscript presents a novel computational framework of dopamine function: DopAct. It integrates reinforcement learning with ideas from active inference, Bayesian inference, and the recent proposal that goal-directed and habitual systems use different prediction errors for learning. This model is exciting because it accounts for the role of dopamine in reward learning and action planning.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for sending your article entitled &quot;Dopamine role in learning and action inference&quot; for peer review at <italic>eLife</italic>. Your article is being evaluated by two peer reviewers, and the evaluation is being overseen by a Reviewing Editor and Kate Wassum as the Senior Editor.</p><p>Both reviewers and the editorial team felt that your manuscript describing a new model for dopamine in action planning and habit learning is very interesting.</p><p>However, both reviewers raised several major concerns. The points are detailed in the individual critiques (see below). We would appreciate your response to each point raised by both reviewers, paying special attention to the following most essential points.</p><p>1) Explain and demonstrate whether and how the model can account for the well-documented shift in dopamine bursts from reward to predictive stimulus with learning (reviewer #2, point 2).</p><p>2) Explain and demonstrate whether and how the model can account for extinction (reviewer #1, point 5-6; reviewer #2, point 2). This point is related to the general question of how the goal-directed and habit system interact, and how the goal-directed system can regain control over action selection.</p><p>3) The paper itself will require several major changes. It will be important to describe the model more clearly (including the valuation system) so that it can be fully understood (reviewer #1, points 1-3; reviewer #2, point 1 and 3), and to better connect the model to the neuroscience literature (reviewer #2, point 2).</p><p><italic>Reviewer #1:</italic></p><p>This paper presents a theoretical framework, referred to as DopAct, to account for the role of dopamine (DA) in learning and action planning. While this framework makes a set of novel assumptions, it draws heavily on classical reinforcement learning (e.g. Sutton and Barto, 1998), Friston's (2010) active inference theory, Miller et al.'s (2019) theory of habit formation and Solway and Botvinick's (2012) Bayesian inference theory of action selection. Following current thinking, DopAct proposes that different cortico-striatal loops targeted by distinct DAergic neuronal populations underpin outcome valuation, goal-directed action selection, and habit formation functions. Critically, however, DopAct extends the role of DA beyond that of driving learning and into action planning, although this is restricted to the goal-directed, but not the habit system. The major contribution of the paper is thus to provide a formal integration of (variants of) multiple preexisting theories and to propose a particular neural instantiation in the basal ganglia. My concerns are as follows.</p><p>1) I found the paper somewhat difficult to follow due to the fact that critical pieces of information necessary to understand how the various components of DopAct operate and interact with each other appear fragmented and scattered across the paper. In addition, the writing style in the paper is at times unpolished (by way of example, consider the sentence starting with &quot;The model has been simulated…”…, which undermines clarity.</p><p>2) In my opinion, the paper is made unnecessarily complex by the amalgamation of multiple theoretically independent ideas, not all of which are equally essential to the central message or formally implemented. For instance, at the beginning of the Results, the reader is given an overview of the DopAct framework, starting with the valuation system. Therein, it is stated that DopAct departs from the classical reinforcement learning principle of reward maximization, replacing it instead with the proposal that the valuation system computes the amount of resources (e.g., food) necessary to bring the animal to its optimal reserve levels. This notion is not, however, formally developed in the paper and is in practice relegated to a future direction. Indeed, all simulations assume low levels of reserves and thus are undistinguishable from standard reinforcement learning. For this reason, I think introducing this topic at the outset (or at all) only acts as a distractor of the main points of the DopAct framework and thus I would encourage the author to reserve this important yet here ancillary distinction for a future paper. When doing so, evidence should be provided to justify this assumption (e.g. evidence that a moderately hungry animal would choose a small over a large reward if the former is sufficient to restore desired reserve levels).</p><p>3) Insufficient details are given as to how prediction errors operate within the valuation system and how these errors relate to those being computed in the goal-directed system. Are expectations in the valuation system only generated by actions planned by the goal-directed system, or can states generate their own expectations without the mediation of actions? If the latter, how do the two types of expectations (cue-based vs. action-based) interact, if at all, during learning to determine the computation of prediction errors in each system? This is particularly important early on in training, while it is unclear to the agent whether the reward is brought about by an action or predicted by a cue and delivered independently of any action. In connection with these questions, the paper announces early on (Introduction section), that Friston's (2010) notions of active inference will be followed, whereby prediction errors can be minimized either by learning to update expectations (no intervening action) or by learning act so as to satisfy those expectations. This point would seem to be particularly relevant to Pavlovian-instrumental interactions during learning, yet such a theme is not developed thereafter in the paper as the focus is placed on action-generated reward expectations (which once again seemed somewhat of a red herring for the reader).</p><p>4) If I understood correctly, the goal-directed system is in charge of simultaneously computing reward expectation (generated from an action plan) as well as the required action intensity (at least before the habit system takes over). It would seem, however, that these two variables might not always correlate. For example, two equally large rewards might generate equally large expectations, but one of them might require an action of weaker intensity than the other. How would DopAct deal with such scenarios if action intensity and reward expectation are one and the same thing from the viewpoint of the goal-directed system?</p><p>5) Since Thorndike, traditional views of habit learning have proposed the formation of an S-R association stamped on by reinforcement, but excluding the reward itself from the associative representations involved. This view allows habitual animals to become insensitive to outcome devaluation while remaining sensitive to changes in the probability of reward given the action (e.g. extinction). Following Miller et al., 2019, this paper departs from this view by assuming that habit learning is driven by a different, outcome-independent, purely action-based prediction error, calculated as the difference between the habitual (and therefore expected) action and the action actually executed. Early on in training, the action executed is determined by the goal-directed system, but over the course of training control over action execution is yielded to the faster, more computationally efficient habit system. Given this departure from the conventional theoretical framework, a discussion of how these two distinct theoretical assumptions on habit compare to each other would be desirable.</p><p>6) Still on the topic of habit formation, I found the paper lacking in clarity as to how the relative contribution of the goal-directed and habit systems to action selection is negotiated when response-outcome contingencies change after habitual responding has been strongly established. First, what exactly happens to the goal-directed system once the habit system takes over? Does it become dormant in the model, or does it do its regular computations (which would seem inefficient)? And second, if the weight of this relative contribution shifts over training so as to favor habit-based responding, how can an animal ever get out of habitual responding unless control is yielded back to the goal-directed system? Based on the simulations show in Figure 7, it would seem that the DopAct has a hard time ever exiting habit mode even after 500 trials of reward omission. Note, however, that evidence (e.g. Balleine, Killcross and Dickinson, 2003) indicates that lesions that cause animals to become habitual (as measured by insensitivity to outcome devaluation) do not necessarily disrupt extinction learning, which would support traditional S-R views of habit. Later in the paper, however, while discussing reversal learning, it is explained that with sufficient reversal training the goal-directed system prevails and manages to retrain the habit system to choose the appropriate response. How does this happen if the action executed (which is what feeds into the prediction error of the habit system) is now determined by the habit system itself? Is it the case that the goal-directed system retains greater control over responding even after strong habits have been established? Such an assumption would indeed allow such retraining of the habit system by the goal-directed system, but it would seem to come at the price of the agent never entirely losing goal-directed behavior (i.e., never quite become habitual).</p><p><italic>Reviewer #2:</italic></p><p>Summary: This paper presents a new model of goal-directed and habitual learning based on active inference. The paper provides an interesting, integrative model that builds on several different strands of prior work. In addition, the model's approach to habitual and goal-directed learning provide some ideas of how the dopamine system might be parcellated computationally. Moreover, the model tackles a thorny (and largely unaddressed computationally) question of how dopamine can be simultaneously involved in reward prediction and model planning. As it stands, however, I don't think this paper does enough to both explain how the model works, nor justify its assumptions and empirical suitability. I unpack these points below.</p><p>1) The model has many assumptions and parameters, which seem not sufficiently motivated and constrained by the target behaviour and underlying neuroscience. For example, the valuation system is hardly explained and relies on resource levels that are not used in any of the simulations. Similarly, pointers are made to the neuroscience, but mostly in a speculative way, rather than a firmer linking between computational elements and underlying anatomy/physiology. What could possibly show that this model is wrong? That's not adequately clear. Relatedly, too much is left to future work-perhaps compounding the impression that the model is not sufficiently justified/supported as yet.</p><p>2) The range of applications used to justify the model is very narrow-only two basic behavioural cases are undertaken, while some of the fundamental results in this area (e.g., the shift in dopamine burst from Schultz, Dayan, Montague, 1997 in Science) are not obviously replicated. The claim is noted that the analogy to the TD model means that similar learning should occur, but that is not really shown (and very much not obvious from the exposition). In addition, the major point about how action intensity becomes habitized misses a key distinction between variable-interval and variable-ration schedules (VI schedules with lower response rates produce faster habits)-how would this model possibly account for this (it would seem to go counter the mechanism)? Even the simulated result (on omission) seems to overstate the case in that even after 500 trials, there is no extinction (Figure 7A).</p><p>3) The model seems to assume computationally intensive (and complex) operations for the organism to solve even simple problems. For example, the model requires sophisticated action planning to terminate the dopamine burst even on the very first trial, which would seem to require further justification. This does suggest a testable prediction, though: does the dopamine burst get shorter as the planning gets more efficient? When the only learning requires an intensity adjustment for a response (as modelled), this planning mechanism seems potentially plausible, but not clear how that would generalize more broadly. As a result, even the algorithmic level in the paper reads very much as a computational account (what the system should achieve, rather than how it does). There would seem to be some insights there in how food delivery is transformed into a perceived reward, but that aspect is not drawn out.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your article &quot;Dopamine role in learning and action inference&quot; for consideration by <italic>eLife</italic>. Your revised article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Kate Wassum as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Philipp Schwartenbeck (Reviewer #4).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>Summary:</p><p>All reviewers agreed that the revised manuscript is much improved. However, reviewers also felt that the overall structure of the manuscript could still be improved in terms of simplicity and clarity, and that additional clarifications were needed regarding a few select issues.</p><p>Revisions:</p><p>1) Reviewers agreed that the paper is still too dense and it would be important to clarify the writing to make it more concise. Reviewers suggested highlighting the general aspects of the model and simplifying specifics of the implementation. They suggested moving some of the details to an appendix, as well as providing concise summaries, and stating what are the key general points and what are more detailed implementational aspects for a specific simulation. Further, the discussion of the novel theoretical and empirical predictions is very rich at the moment but could be summarized and simplified to highlight some of the main points. This could include, for instance, the teaching signals in the valuation (RPE independent of action plan?), goal-directed (RPE dependent on action plan?) and habit (action prediction errors?) system, and the need for the goal-directed system to learn about reversals or the resulting effects on behavior (including blocking of the different systems).</p><p>2) A specific issue that needs to be clarified is how an animal can exit habit mode. Specifically, the author states: &quot;On later trials the action is jointly determined by the habit and goal-directed system (Figure 1B), and their relative contributions depend on their levels of confidence.&quot; It is still unclear how exactly a habitized animal would exit habit mode if the relative contributions of the goal-directed and habit systems are determined by their level of confidence. How is confidence formally estimated? For instance, if the expectancies of a habitized animal are violated, this assumption would seem to predict that the goal-directed system should lose some confidence. If so, it would seem to follow that the goal-directed system should yield even more control to the misguided but confident habit system. In DopAct an animal can get out of a habit only through exploration (Results final paragraph). It is unclear whether and how exploration is promoted by prediction errors, or whether it is treated as a constant in the model. This is critical because the rate of exploration in habit-dominated behaviors would be expected to be low once the learning agent has settled on a stable solution. Thus, given a change in contingencies, a confidence-based arbitration between the goal-directed and habit systems would seem to further privilege the habit system while the goal-directed system waits for an opportunity to rewire itself into a more adaptive set of associations. At what point will those new associations carry more confidence than those deeply entrenched in the habit system?</p><p>3) A key prediction relates to the dissociation between the goal-directed and habitual system learning from reward prediction errors and action prediction errors, respectively. Their roles and dynamics could be described more clearly and currently these descriptions are quite scattered around the text. Does this model predict an independent signature of reward and action prediction errors in a task where those two teaching signals can be fully dissociated (e.g., rewards without actions or the other way around)? Further, is the arbitration between those two modes based on their relative uncertainty? Is it just the decrease in uncertainty that drives actions to become habitual over time, or is there also some inherent complexity penalty for the goal-directed system, as for example described in the active inference framework (e.g. FitzGerald, Dolan and Friston, 2014)?</p><p>4) Another point that requires some clarification is &quot;Once an action plan has been formulated, the animal starts to expect the available reward, and the dopamine level encoding the prediction error decreases.&quot; Despite the substantial expansion, this section on active inference is still confusing. This may in part be due to semantics. For example, is by &quot;an action plan has been formulated&quot; actually meant &quot;an action plan has been implemented&quot;? It is hard to see how the mere formulation (i.e., elaboration) of an action plan could affect the kind of &quot;change in the world&quot; that is being proposed to contribute to reducing prediction errors (alongside learning). A related question: would habitual responses also reduce prediction errors, given that by definition they do not generate reward expectancies?</p><p>5) There is an interesting distinction between the valuation and the goal-directed system, since both display reward prediction errors but the goal-directed reward prediction errors are contingent on action plans. Would that predict an absence of reward prediction errors in the goal directed system if the agent cannot perform actions, or is forced to take actions that are likely to result in no rewards?</p><p>6) Related to this point the author states: &quot;in the DopAct framework the expectation of reward only arises from formulating a plan to achieve it.&quot; This statement is odd in light that the valuation system, which is part of the DopAct framework, is proposed to compute reward expectancies on the bases of antecedent stimuli without the mediation of action plans. Should this sentence read instead &quot;in the goal-directed system the expectation of reward only arises from formulating a plan to achieve it.&quot;? This would be consistent with the idea that, while action plans may be a fundamental component of generating reward expectations, animals can also acquire such expectancies through stimulus-stimulus (or state-state) learning. It would also allow DopAct to account for recent evidence in the sensory preconditioning paradigm indicating that associations between neutral stimuli are promoted by DA stimulation without the mediation of any action plan. These findings could not be accommodated by DopAct if the sentence “…but in the DopAct framework the expectation of reward only arises from formulating a plan to achieve it.” was true.</p><p>7) Given that the model is fully Bayesian, agents not only have access to their beliefs about actions and rewards but also their uncertainty in these beliefs. Does the model make any interesting predictions for signals that reflect the reduction of uncertainty about actions and rewards? Is there also an effect for the reduction of state uncertainty? The latter point would be particularly interesting with respect to recent reports about dopaminergic signals for state identity that are orthogonal to reward (e.g. Takahashi et al., 2017). If not, which changes to the model architecture would be necessary to account for such signals?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53262.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>However, both reviewers raised several major concerns. The points are detailed in the individual critiques (see below). We would appreciate your response to each point raised by both reviewers, paying special attention to the following most essential points.</p><p>1) Explain and demonstrate whether and how the model can account for the well-documented shift in dopamine bursts from reward to predictive stimulus with learning (reviewer #2, point 2).</p></disp-quote><p>A new Figure 7B has been added demonstrating that the model can reproduce the shift in dopamine burst from reward to predictive stimulus.</p><disp-quote content-type="editor-comment"><p>2) Explain and demonstrate whether and how the model can account for extinction (reviewer #1, point 5-6; reviewer #2, point 2). This point is related to the general question of how the goal-directed and habit system interact, and how the goal-directed system can regain control over action selection.</p></disp-quote><p>A new Figure 8 has been added, which demonstrates that the model can produce extinction, and qualitatively reproduce key features of extinction behaviour seen in two experimental studies. It has also been clarified how the goal-directed and habit systems interact: Figure 1B and text describing it (paragraph four “Overview of the framework”) have been modified to clarify that even when an action becomes habitual, both the goal-directed and habit systems contribute to the action planning, but their contribution are weighted by their confidence. A paragraph (final paragraph in the Results) has also been added discussing the mechanisms underlying reversal in the model.</p><disp-quote content-type="editor-comment"><p>3) The paper itself will require several major changes. It will be important to describe the model more clearly (including the valuation system) so that it can be fully understood (reviewer #1, points 1-3; reviewer #2, point 1 and 3), and to better connect the model to the neuroscience literature (reviewer #2, point 2).</p></disp-quote><p>I have been convinced by comment 2 of reviewer 1 that the conceptual description of validation system at the start of the Results section distracts from key points of the manuscript, and I followed his/her recommendation to shorten this description, so it is now contained within just a single paragraph. At the same time, details of the presented simulations of the valuation system have been added (Figure 12A and text in Materials and methods). Furthermore, additional simulations of classical paradigms suggested by the reviewers that have been added (classical conditioning – Figure 7B, devaluation – Figure 8A, and Pavlovian-instrumental transfer – Figure 8B) to better connect the manuscript with the neuroscience literature.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>This paper presents a theoretical framework, referred to as DopAct, to account for the role of dopamine (DA) in learning and action planning. While this framework makes a set of novel assumptions, it draws heavily on classical reinforcement learning (e.g. Sutton and Barto, 1998), Friston's (2010) active inference theory, Miller et al.'s (2019) theory of habit formation and Solway and Botvinick's (2012) Bayesian inference theory of action selection. Following current thinking, DopAct proposes that different cortico-striatal loops targeted by distinct DAergic neuronal populations underpin outcome valuation, goal-directed action selection, and habit formation functions. Critically, however, DopAct extends the role of DA beyond that of driving learning and into action planning, although this is restricted to the goal-directed, but not the habit system. The major contribution of the paper is thus to provide a formal integration of (variants of) multiple preexisting theories and to propose a particular neural instantiation in the basal ganglia. My concerns are as follows.</p><p>1) I found the paper somewhat difficult to follow due to the fact that critical pieces of information necessary to understand how the various components of DopAct operate and interact with each other appear fragmented and scattered across the paper.</p></disp-quote><p>The descriptions of the models (that in the previous version was spread between Results and Materials and method sections) has been integrated and is presented in the Results. The figures with details of the models have also been moved to the Results, and integrated with the figures in Results (new Figures 6, 9, 10).</p><disp-quote content-type="editor-comment"><p>In addition, the writing style in the paper is at times unpolished (by way of example, consider the sentence starting with &quot;The model has been simulated…”, which undermines clarity.</p></disp-quote><p>The sentence mentioned above is no longer included in the manuscript, because the simulations of the omission protocol it described have been replaced by the simulations aiming to replicate experimental data from extinction paradigms. The manuscript has been carefully re-read and re-checked.</p><disp-quote content-type="editor-comment"><p>2) In my opinion, the paper is made unnecessarily complex by the amalgamation of multiple theoretically independent ideas, not all of which are equally essential to the central message or formally implemented. For instance, at the beginning of the Results, the reader is given an overview of the DopAct framework, starting with the valuation system. Therein, it is stated that DopAct departs from the classical reinforcement learning principle of reward maximization, replacing it instead with the proposal that the valuation system computes the amount of resources (e.g., food) necessary to bring the animal to its optimal reserve levels. This notion is not, however, formally developed in the paper and is in practice relegated to a future direction. Indeed, all simulations assume low levels of reserves and thus are undistinguishable from standard reinforcement learning. For this reason, I think introducing this topic at the outset (or at all) only acts as a distractor of the main points of the DopAct framework and thus I would encourage the author to reserve this important yet here ancillary distinction for a future paper. When doing so, evidence should be provided to justify this assumption (e.g. evidence that a moderately hungry animal would choose a small over a large reward if the former is sufficient to restore desired reserve levels).</p></disp-quote><p>I would like to thank the reviewer for this great observation and suggestion. In the revised version of the manuscript, the description of the valuation system has been shortened (to avoid making unnecessary assumptions) and is now contained within a single paragraph (paragraph two “Overview of the framework”).</p><disp-quote content-type="editor-comment"><p>3) Insufficient details are given as to how prediction errors operate within the valuation system and how these errors relate to those being computed in the goal-directed system. Are expectations in the valuation system only generated by actions planned by the goal-directed system, or can states generate their own expectations without the mediation of actions? If the latter, how do the two types of expectations (cue-based vs. action-based) interact, if at all, during learning to determine the computation of prediction errors in each system? This is particularly important early on in training, while it is unclear to the agent whether the reward is brought about by an action or predicted by a cue and delivered independently of any action. In connection with these questions, the paper announces early on (Introduction section), that Friston's (2010) notions of active inference will be followed, whereby prediction errors can be minimized either by learning to update expectations (no intervening action) or by learning act so as to satisfy those expectations. This point would seem to be particularly relevant to Pavlovian-instrumental interactions during learning, yet such a theme is not developed thereafter in the paper as the focus is placed on action-generated reward expectations (which once again seemed somewhat of a red herring for the reader).</p></disp-quote><p>In the revised version of the manuscript, more detailed simulations of the valuation system were added, which demonstrate in detail how the prediction errors are generated. A new Figure 12A and text describing it have been added showing how the temporal difference learning algorithm has been implemented in the model.</p><p>Furthermore, a simulation of the model in Pavlovian-instrumental transfer paradigm has been added which demonstrates that the model can replicate experimentally observed pattern of behaviour, as shown in Figure 8B, and explained in Results and Materials and methods. Is has been clarified there that during operant conditioning reward expectation computed by the valuation system drives action planning, while during classical conditioning it may provide expectations without triggering actions. It has been also clarified how the interactions between the two types of expectations allow the model to reproduce Pavlovian instrumental transfer.</p><p>It has been clarified how prediction errors in the goal-directed system are reduced by both planning and learning: Figure 2 has been extended to include 2 panels that highlight these two ways of minimizing prediction errors, and the description has been added in the text (final paragraph “Overview of the framework”).</p><disp-quote content-type="editor-comment"><p>4) If I understood correctly, the goal-directed system is in charge of simultaneously computing reward expectation (generated from an action plan) as well as the required action intensity (at least before the habit system takes over). It would seem, however, that these two variables might not always correlate. For example, two equally large rewards might generate equally large expectations, but one of them might require an action of weaker intensity than the other. How would DopAct deal with such scenarios if action intensity and reward expectation are one and the same thing from the viewpoint of the goal-directed system?</p></disp-quote><p>A footnote has been added clarifying that in the above case the action with lower required intensity will have a higher value of parameter <inline-formula><mml:math id="inf332"><mml:mi>q</mml:mi></mml:math></inline-formula>. In this way, both actions will produce the same expectation, which is proportional to <inline-formula><mml:math id="inf333"><mml:mrow><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula> rather than <inline-formula><mml:math id="inf334"><mml:mi>q</mml:mi></mml:math></inline-formula> itself, but the two rewards will result in different action intensities, which in the model depend on both <inline-formula><mml:math id="inf335"><mml:mi>v</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf336"><mml:mi>q</mml:mi></mml:math></inline-formula>.</p><disp-quote content-type="editor-comment"><p>5) Since Thorndike, traditional views of habit learning have proposed the formation of an S-R association stamped on by reinforcement, but excluding the reward itself from the associative representations involved. This view allows habitual animals to become insensitive to outcome devaluation while remaining sensitive to changes in the probability of reward given the action (e.g. extinction). Following Miller et al., 2019, this paper departs from this view by assuming that habit learning is driven by a different, outcome-independent, purely action-based prediction error, calculated as the difference between the habitual (and therefore expected) action and the action actually executed. Early on in training, the action executed is determined by the goal-directed system, but over the course of training control over action execution is yielded to the faster, more computationally efficient habit system. Given this departure from the conventional theoretical framework, a discussion of how these two distinct theoretical assumptions on habit compare to each other would be desirable.</p></disp-quote><p>The paragraph discussing the relationship of DopAct framework to traditional computational models of habit formation has been substantially extended. Furthermore, a new paragraphs has been added that compares the accounts of habit formation in DopAct and traditional reinforcement learning models with experimental data (Discussion section).</p><disp-quote content-type="editor-comment"><p>6) Still on the topic of habit formation, I found the paper lacking in clarity as to how the relative contribution of the goal-directed and habit systems to action selection is negotiated when response-outcome contingencies change after habitual responding has been strongly established. First, what exactly happens to the goal-directed system once the habit system takes over? Does it become dormant in the model, or does it do its regular computations (which would seem inefficient)? And second, if the weight of this relative contribution shifts over training so as to favor habit-based responding, how can an animal ever get out of habitual responding unless control is yielded back to the goal-directed system? Based on the simulations show in Figure 7, it would seem that the DopAct has a hard time ever exiting habit mode even after 500 trials of reward omission. Note, however, that evidence (e.g. Balleine, Killcross and Dickinson, 2003) indicates that lesions that cause animals to become habitual (as measured by insensitivity to outcome devaluation) do not necessarily disrupt extinction learning, which would support traditional S-R views of habit. Later in the paper, however, while discussing reversal learning, it is explained that with sufficient reversal training the goal-directed system prevails and manages to retrain the habit system to choose the appropriate response. How does this happen if the action executed (which is what feeds into the prediction error of the habit system) is now determined by the habit system itself? Is it the case that the goal-directed system retains greater control over responding even after strong habits have been established? Such an assumption would indeed allow such retraining of the habit system by the goal-directed system, but it would seem to come at the price of the agent never entirely losing goal-directed behavior (i.e., never quite become habitual).</p></disp-quote><p>New simulations in Figure 8 have been added demonstrating that the model can produce extinction. The occurrence of extinction depends on the values of parameters of the simulated model, and in the previous version of the manuscript the parameters were chosen to replicate the simulations shown in Figure 4 in the paper by Miller et al., 2019, where their model also did not show extinction after extensive training (left panel in Figure 4 in their paper). However, thanks to reviewers’ comments I realized that such behaviour is not observed experimentally, hence in the revised version, different parameters were used for which the model reproduces profiles of extinction observed in behavioural experiments (re-plotted in top displays on Figure 8).</p><p>Furthermore, Figure 1B and text have been modified to clarify that even if an action becomes habitual, both goal-directed and habit systems contribute to action selection, and their contribution depends on their relative confidence. A paragraph (penultimate paragraph Results section) has also been added discussing the mechanism underlying reversal in the model, which clarifies that some contribution of the goal-directed system to action planning is necessary for reversal to take place, as pointed out by the reviewer above.</p><p>A sentence has been added in a section discussing future work mentioning the role of amygdala in habit formation and citing the paper by Balleine et al., 2003, highlighted by the reviewer.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…] 1) The model has many assumptions and parameters, which seem not sufficiently motivated and constrained by the target behaviour and underlying neuroscience. For example, the valuation system is hardly explained and relies on resource levels that are not used in any of the simulations.</p></disp-quote><p>Following a suggestion of reviewer 2, the description of the valuation system has been shortened (to avoid making unnecessary assumptions) and is now contained within a single paragraph of the Results. On the other hand, more details were provided on the operation of the valuation system in the simulations (new Figure 12A, and text).</p><disp-quote content-type="editor-comment"><p>Similarly, pointers are made to the neuroscience, but mostly in a speculative way, rather than a firmer linking between computational elements and underlying anatomy/physiology. What could possibly show that this model is wrong? That's not adequately clear. Relatedly, too much is left to future work-perhaps compounding the impression that the model is not sufficiently justified/supported as yet.</p></disp-quote><p>Additional simulations of classical paradigms (conditioning, extinction, Pavlovian-instrumental transfer) have been added to better connect the manuscript with neuroscience literature and are presented in new Figures 7B, 8A and 8B. Furthermore, the section discussing experimental predictions has been reorganized such that it starts with discussing the two most critical predictions that could allow verification or falsification of the model. Specific experiments have been described that could be performed to test these predictions.</p><disp-quote content-type="editor-comment"><p>2) The range of applications used to justify the model is very narrow-only two basic behavioural cases are undertaken, while some of the fundamental results in this area (e.g., the shift in dopamine burst from Schultz, Dayan, Montague, 1997 in Science) are not obviously replicated. The claim is noted that the analogy to the TD model means that similar learning should occur, but that is not really shown (and very much not obvious from the exposition).</p></disp-quote><p>The simulations of the shift of dopaminergic response from reward to conditioned stimulus have been added in the new Figure 7B and text.</p><disp-quote content-type="editor-comment"><p>In addition, the major point about how action intensity becomes habitized misses a key distinction between variable-interval and variable-ration schedules (VI schedules with lower response rates produce faster habits)-how would this model possibly account for this (it would seem to go counter the mechanism)?</p></disp-quote><p>This phenomenon has been discussed in detail in the paper by Miller et al., 2019, which shows how their model reproduces this effect in simulations (see Figure 5 in paper by Miller et al., 2019). Due to a conceptual similarity of the presented model to the model by Miller et al., these two models account for the phenomenon in related ways. A paragraph has been added in Discussion summarizing this account.</p><disp-quote content-type="editor-comment"><p>Even the simulated result (on omission) seems to overstate the case in that even after 500 trials, there is no extinction (Figure 7A).</p></disp-quote><p>New simulations in Figure 8 have been added demonstrating that the model can produce extinction. The occurrence of extinction depends on the values of parameters of the simulated model, and in the previous version of the manuscript the parameters were chosen to replicate the simulations shown in Figure 4 in the paper by Miller et al., 2019, where their model also did not show extinction after extensive training (left panel in Figure 4 in their paper). However, thanks to reviewers’ comments I realized that such behaviour is not observed experimentally, hence in the revised version, different parameters were used for which the model reproduces profiles of extinction observed in behavioural experiments (re-plotted in top displays on Figure 8).</p><disp-quote content-type="editor-comment"><p>3) The model seems to assume computationally intensive (and complex) operations for the organism to solve even simple problems. For example, the model requires sophisticated action planning to terminate the dopamine burst even on the very first trial, which would seem to require further justification. This does suggest a testable prediction, though: does the dopamine burst get shorter as the planning gets more efficient? When the only learning requires an intensity adjustment for a response (as modelled), this planning mechanism seems potentially plausible, but not clear how that would generalize more broadly. As a result, even the algorithmic level in the paper reads very much as a computational account (what the system should achieve, rather than how it does).</p></disp-quote><p>I would like to thank the reviewer for noticing this interesting prediction, it has been added to the section listing experimental predictions in the revised version of the manuscript.</p><disp-quote content-type="editor-comment"><p>There would seem to be some insights there in how food delivery is transformed into a perceived reward, but that aspect is not drawn out.</p></disp-quote><p>Such insight could be provided by a detailed model of the valuation system, but following the recommendation of reviewer 2, the description of the valuation system will be left for a future study.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Revisions:</p><p>1) Reviewers agreed that the paper is still too dense and it would be important to clarify the writing to make it more concise. Reviewers suggested highlighting the general aspects of the model and simplifying specifics of the implementation. They suggested moving some of the details to an appendix, as well as providing concise summaries, and stating what are the key general points and what are more detailed implementational aspects for a specific simulation.</p></disp-quote><p>I thank the reviewers for these great suggestions. The manuscript has been substantially reorganized to focus on general aspects of the model and to reduce or remove from Results the details of implementation:</p><list list-type="bullet"><list-item><p>The Abstract has been simplified to better emphasize the key points of the paper.</p></list-item><list-item><p>To make the paper easier to navigate, it has been divided into a larger number of shorter sections, with more informative titles.</p></list-item><list-item><p>Summary of key points has been added at the starts of multiple paragraphs.</p></list-item><list-item><p>A new paragraph has been added summarizing the key feature of the algorithm in DopAct (subsection “Algorithm for planning and learning” final paragraph).</p></list-item><list-item><p>The details of a possible way the prediction errors in the goal-directed system could be computed have been deleted are replaced by the key point that such computation would be local in the basal ganglia network. This also allowed simplification of network diagrams in Figures 5, 6 and 10.</p></list-item><list-item><p>Description of Figure 6 has been shortened, and some of the details have been moved from the main text to the figure caption.</p></list-item><list-item><p>Description of Figure 7C has been shortened and organized around the key messages rather than individual displays.</p></list-item><list-item><p>Description of details of devaluation simulations has been moved from Results to Materials and methods.</p></list-item><list-item><p>Description of hyperpriors of variance parameters have been moved from Results to Materials and methods, and shortened.</p></list-item><list-item><p>A paragraph discussing the details of units of various terms in Equation 9.9 has been deleted.</p></list-item><list-item><p>An uninteresting simulation of the valuation system has been removed from Figure 11C, which allowed shortening Materials and methods, and simplifying Figure 12B.</p></list-item></list><disp-quote content-type="editor-comment"><p>Further, the discussion of the novel theoretical and empirical predictions is very rich at the moment but could be summarized and simplified to highlight some of the main points. This could include, for instance, the teaching signals in the valuation (RPE independent of action plan?), goal-directed (RPE dependent on action plan?) and habit (action prediction errors?) system, and the need for the goal-directed system to learn about reversals or the resulting effects on behavior (including blocking of the different systems).</p></disp-quote><p>The Discussion section has been simplified and reorganized:</p><list list-type="bullet"><list-item><p>The description of the relationship of the DopAct to other theories has been shorten and focused on key points.</p></list-item><list-item><p>Section “Relationship to experimental data” has been substantially shortened, focussed on key experimental data, and reorganized around predictions rather than discussed studies.</p></list-item><list-item><p>Different paragraphs from the previous version of Discussion concerned with habit formation have been gathered in a new section “Mechanisms of habitual behaviour”.</p></list-item><list-item><p>Section “Direction of future work” has been shortened.</p></list-item></list><disp-quote content-type="editor-comment"><p>2) A specific issue that needs to be clarified is how an animal can exit habit mode. Specifically, the author states: &quot;On later trials the action is jointly determined by the habit and goal-directed system (Figure 1B), and their relative contributions depend on their levels of confidence.&quot; It is still unclear how exactly a habitized animal would exit habit mode if the relative contributions of the goal-directed and habit systems are determined by their level of confidence. How is confidence formally estimated?</p></disp-quote><p>Throughout the manuscript word “confidence” has been replaced by “certainty”, as the concept of uncertainty is formally defined in the paper.</p><disp-quote content-type="editor-comment"><p>For instance, if the expectancies of a habitized animal are violated, this assumption would seem to predict that the goal-directed system should lose some confidence. If so, it would seem to follow that the goal-directed system should yield even more control to the misguided but confident habit system.</p></disp-quote><p>A new paragraph has been added discussing this effect and pointing that it is only transient (Results final paragraph).</p><disp-quote content-type="editor-comment"><p>In DopAct an animal can get out of a habit only through exploration (Results final paragraph). It is unclear whether and how exploration is promoted by prediction errors, or whether it is treated as a constant in the model. This is critical because the rate of exploration in habit-dominated behaviors would be expected to be low once the learning agent has settled on a stable solution.</p></disp-quote><p>The mechanisms allowing a reversal in the model have been explained in more detail, and it has been emphasized that the amount of noise resulting in exploration was constant in simulations.</p><disp-quote content-type="editor-comment"><p>Thus, given a change in contingencies, a confidence-based arbitration between the goal-directed and habit systems would seem to further privilege the habit system while the goal-directed system waits for an opportunity to rewire itself into a more adaptive set of associations. At what point will those new associations carry more confidence than those deeply entrenched in the habit system?</p></disp-quote><p>A new paragraph has been added to section “Mechanisms of habitual behaviour” discussing how the goal-directed system can regain control from the habit system.</p><disp-quote content-type="editor-comment"><p>3) A key prediction relates to the dissociation between the goal-directed and habitual system learning from reward prediction errors and action prediction errors, respectively. Their roles and dynamics could be described more clearly and currently these descriptions are quite scattered around the text.</p></disp-quote><p>A paragraph has been added in section “Overview of the framework” which highlights that the two systems learn on the basis of different prediction errors. Also, Figure 1D has been modified to illustrate the process of learning in the habit system.</p><disp-quote content-type="editor-comment"><p>Does this model predict an independent signature of reward and action prediction errors in a task where those two teaching signals can be fully dissociated (e.g., rewards without actions or the other way around)?</p></disp-quote><p>This is indeed the key prediction of the model, and such pattern of activity was indeed observed by Howe and Dombeck. To increase prominence of the discussion of this key study, it has been moved to the second paragraph of section “Relationship to experimental data”.</p><disp-quote content-type="editor-comment"><p>Further, is the arbitration between those two modes based on their relative uncertainty? Is it just the decrease in uncertainty that drives actions to become habitual over time, or is there also some inherent complexity penalty for the goal-directed system, as for example described in the active inference framework (e.g. FitzGerald, Dolan and Friston, 2014)?</p></disp-quote><p>A sentence summarizing the key idea from the above paper has been added to section “Mechanisms of habitual behaviour”.</p><disp-quote content-type="editor-comment"><p>4) Another point that requires some clarification is &quot;Once an action plan has been formulated, the animal starts to expect the available reward, and the dopamine level encoding the prediction error decreases.&quot; Despite the substantial expansion, this section on active inference is still confusing. This may in part be due to semantics. For example, is by &quot;an action plan has been formulated&quot; actually meant &quot;an action plan has been implemented&quot;? It is hard to see how the mere formulation (i.e., elaboration) of an action plan could affect the kind of &quot;change in the world&quot; that is being proposed to contribute to reducing prediction errors (alongside learning).</p></disp-quote><p>Thank you for these great questions. A paragraph discussing them has been added in section “Overview of the framework”.</p><disp-quote content-type="editor-comment"><p>A related question: would habitual responses also reduce prediction errors, given that by definition they do not generate reward expectancies?</p></disp-quote><p>Yes, this effect is visible in the right display of Figure 7C, and a sentence has been added discussing it.</p><disp-quote content-type="editor-comment"><p>5) There is an interesting distinction between the valuation and the goal-directed system, since both display reward prediction errors but the goal-directed reward prediction errors are contingent on action plans. Would that predict an absence of reward prediction errors in the goal directed system if the agent cannot perform actions, or is forced to take actions that are likely to result in no rewards?</p></disp-quote><p>The DopAct framework predicts that such manipulation will result in prolong prediction errors, and this has been added in section “Experimental predictions”.</p><disp-quote content-type="editor-comment"><p>6) Related to this point the author states: &quot;in the DopAct framework the expectation of reward only arises from formulating a plan to achieve it.&quot; This statement is odd in light that the valuation system, which is part of the DopAct framework, is proposed to compute reward expectancies on the bases of antecedent stimuli without the mediation of action plans. Should this sentence read instead &quot;in the goal-directed system the expectation of reward only arises from formulating a plan to achieve it.&quot;? This would be consistent with the idea that, while action plans may be a fundamental component of generating reward expectations, animals can also acquire such expectancies through stimulus-stimulus (or state-state) learning. It would also allow DopAct to account for recent evidence in the sensory preconditioning paradigm indicating that associations between neutral stimuli are promoted by DA stimulation without the mediation of any action plan. These findings could not be accommodated by DopAct if the sentence “…but in the DopAct framework the expectation of reward only arises from formulating a plan to achieve it.” was true.</p></disp-quote><p>Phrase “in the goal-directed system” has been added to the sentence quoted above.</p><disp-quote content-type="editor-comment"><p>7) Given that the model is fully Bayesian, agents not only have access to their beliefs about actions and rewards but also their uncertainty in these beliefs. Does the model make any interesting predictions for signals that reflect the reduction of uncertainty about actions and rewards?</p></disp-quote><p>A paragraph describing such prediction has been added to section “Relationship to experimental data”.</p><disp-quote content-type="editor-comment"><p>Is there also an effect for the reduction of state uncertainty? The latter point would be particularly interesting with respect to recent reports about dopaminergic signals for state identity that are orthogonal to reward (e.g. Takahashi et., 2017). If not, which changes to the model architecture would be necessary to account for such signals?</p></disp-quote><p>A summary of this study and the description of an extension of the model that could capture these observations has been added to section “Directions for future work”.</p></body></sub-article></article>