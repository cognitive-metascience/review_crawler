<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">53798</article-id><article-id pub-id-type="doi">10.7554/eLife.53798</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The neurons that mistook a hat for a face</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-57481"><name><surname>Arcaro</surname><given-names>Michael J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4612-9921</contrib-id><email>marcaro@sas.upenn.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-166286"><name><surname>Ponce</surname><given-names>Carlos</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-82408"><name><surname>Livingstone</surname><given-names>Margaret</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, University of Pennsylvania</institution><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Neuroscience, Washington University in St. Louis</institution><addr-line><named-content content-type="city">St. Louis</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Department of Neurobiology, Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Pasternak</surname><given-names>Tatiana</given-names></name><role>Reviewing Editor</role><aff><institution>University of Rochester</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>10</day><month>06</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e53798</elocation-id><history><date date-type="received" iso-8601-date="2019-11-20"><day>20</day><month>11</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-05-21"><day>21</day><month>05</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Arcaro et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Arcaro et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-53798-v1.pdf"/><abstract><p>Despite evidence that context promotes the visual recognition of objects, decades of research have led to the pervasive notion that the object processing pathway in primate cortex consists of multiple areas that each process the intrinsic features of a few particular categories (e.g. faces, bodies, hands, objects, and scenes). Here we report that such category-selective neurons do not in fact code individual categories in isolation but are also sensitive to object relationships that reflect statistical regularities of the experienced environment. We show by direct neuronal recording that face-selective neurons respond not just to an image of a face, but also to parts of an image where contextual cues—for example a body—indicate a face ought to be, even if what is there is not a face.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>faces</kwd><kwd>bodies</kwd><kwd>object relationships</kwd><kwd>regularities</kwd><kwd>inferotemporal cortex</kwd><kwd>macaques</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>RO1 EY 16187</award-id><principal-award-recipient><name><surname>Livingstone</surname><given-names>Margaret</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P30 EY 12196</award-id><principal-award-recipient><name><surname>Livingstone</surname><given-names>Margaret</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neuronal recordings reveal complex, heterogeneous inputs to face-selective neurons, suggesting that inferotemporal neurons do not represent objects in isolation, but are also sensitive to object relationships that reflect environmental regularities.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Our experience with the visual world guides how we understand it. The regularity of our experience of objects within environments and with each other provides a rich context that influences our perception and recognition of objects and categories (<xref ref-type="bibr" rid="bib8">Bar, 2004</xref>; <xref ref-type="bibr" rid="bib21">Greene, 2013</xref>). We do not often see faces in isolation, rather our experience of faces is embedded within a contextually rich environment – a face is usually conjoined to a body – and this regular structure influences how we perceive and interact with faces (<xref ref-type="bibr" rid="bib9">Bar and Ullman, 1996</xref>; <xref ref-type="bibr" rid="bib35">Meeren et al., 2005</xref>; <xref ref-type="bibr" rid="bib44">Rice et al., 2013</xref>). However, it is generally found that neurons critical for the perception of faces (<xref ref-type="bibr" rid="bib2">Afraz et al., 2015</xref>; <xref ref-type="bibr" rid="bib1">Afraz et al., 2006</xref>; <xref ref-type="bibr" rid="bib37">Moeller et al., 2017</xref>) do not respond to bodies, and vice versa (<xref ref-type="bibr" rid="bib20">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib19">Freiwald et al., 2009</xref>; <xref ref-type="bibr" rid="bib30">Kiani et al., 2007</xref>; <xref ref-type="bibr" rid="bib48">Tsao et al., 2006</xref>), suggesting that such neurons code for the intrinsic features of either faces or bodies but not both (<xref ref-type="bibr" rid="bib13">Chang and Tsao, 2017</xref>; <xref ref-type="bibr" rid="bib26">Issa and DiCarlo, 2012</xref>; <xref ref-type="bibr" rid="bib34">Leopold et al., 2006</xref>). These studies typically probe neural function by presenting objects in isolation, independent of their typical context.</p><p>Here, we reconcile this apparent discrepancy between behavior and neural coding. We recorded from two face-selective domains within inferotemporal cortex (IT) in two macaque monkeys while they viewed complex natural images. By considering the spatial relationships of objects in a natural scene relative to a neuron’s receptive field, we show that face-selective IT neurons respond not just to an image of a face, but to parts of an image where contextual cues—such as a body—indicate a face ought to be, even if what is there is not a face. Thus, we find that face cells demonstrate a large contextual surround tuned to entities that normally co-occur with faces. These results reveal surprisingly complex and heterogeneous inputs to face-selective neurons, suggesting that IT neurons do not represent objects in isolation, but are also sensitive to object relationships that reflect statistical regularities of the experienced environment.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We recorded simultaneously from face-selective neurons in the fMRI-defined middle lateral (ML) and posterior lateral (PL) face patches in one rhesus macaque monkey, Monkey 1, and from ML in another monkey, Monkey 2, using chronically implanted 32-channel multi-electrode arrays (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). When mapped using ~2°x2° faces and round non-face objects, the activating regions were small (1-3°), located in the central visual field (within 2° of fixation, and largely contralateral; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). When tested with images of faces, hands, bodies (only for ML arrays), and objects presented centered on this activating region, all the visually responsive sites in both monkeys responded more to faces than to any other category (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). Category selectivity was assessed with an index (category1-response –category2-response)/ (category1-response + category2-response). In particular, in both monkeys’ ML face patches, responses to faces were much larger than to non-face images (mean channel face vs. non-face selectivity indices = 0.63 +/- 0.06 STD and 0.64 +/- 0. 14 for Monkey’s 1 and 2) or to bodies (mean channel face vs. body selectivity indices = 0.77 +/- 0.11 and 0.60 +/- 0.21). Responses to bodies were weak (body vs. object selectivity indices = −0.54 +/- 0.17 STD and 0.04 +/- 0.27 for ML in both Monkeys 1 and 2). Such a strong preference for faces over other categories is consistent with many previous studies (<xref ref-type="bibr" rid="bib11">Bell et al., 2011</xref>; <xref ref-type="bibr" rid="bib48">Tsao et al., 2006</xref>). The array in Monkey 1’s PL face patch was less face selective (mean channel face vs. non-face selectivity indices = 0.24 +/- 0.07 STD) and responses to hands were weak (mean channel hands vs. objects indices = 0.04 +/- 0.02 STD), also consistent with previous work (<xref ref-type="bibr" rid="bib26">Issa and DiCarlo, 2012</xref>).</p><p>To explore how response properties of face neurons represent naturalistic scenes, we mapped the spatial organization of face-selective neurons by presenting complex, familiar and unfamiliar scenes at various positions relative to each site’s activating region (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Conventionally IT neurons are studied by presenting single-object images at a single position while the animal holds fixation (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). Here instead, while the animal held fixation, we presented large 16° x 16° complex natural images at a series of different positions, such that different parts of the image were centered over the receptive field across presentations (<xref ref-type="fig" rid="fig1">Figure 1</xref>), in order to resolve the discrete spatial pattern of responses for each site to different parts of the image. The recording site illustrated in <xref ref-type="fig" rid="fig1">Figure 1C</xref> responded most strongly to the parts of that image that contained monkey faces. We had 28 to 32 visually responsive sites in each of the three implanted arrays. Within each array the responses were all face selective (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>) and showed similar receptive-field locations and spatial response patterns to complex scenes (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplements 2</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>). Given the uniformity of responses across channels in each array, we combined all visually responsive sites in each array for all subsequent analyses to improve signal to noise and present population-level results. Importantly, our findings are not contingent on individual unit/recording site effects. <xref ref-type="fig" rid="fig1">Figure 1</xref> shows the spatial pattern of responses to a series of images from the PL array in Monkey 1, and from the ML arrays in Monkeys 1 and 2. Consistent with the PSTHs of image categories presented in isolation (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>), all three arrays showed responsiveness that was spatially specific to the faces in each image. Consistent with prior physiological and anatomical work suggesting that face patches are organized into a hierarchy that varies along the posterior-anterior axis (<xref ref-type="bibr" rid="bib22">Grimaldi et al., 2016</xref>; <xref ref-type="bibr" rid="bib36">Moeller et al., 2008</xref>), ML recording sites were more face selective and had longer latencies compared to the PL sites during simultaneous recordings (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>). Activity specific to the eyes is apparent in both PL and ML (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Activity encompassing the rest of the face emerges at longer latencies. This is consistent with prior work showing a preference for the contralateral eye in area PL that emerges between 60–80 ms post stimulus onset and longer latencies for images without a contralateral eye (<xref ref-type="bibr" rid="bib26">Issa and DiCarlo, 2012</xref>). A similar shift in response latencies was evident when replacing the eyes with a visible occluder, the background of the image, and surrounding texture of the face (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). In our recordings, both PL and ML exhibited a shift in the latency for activity corresponding to faces without eyes, but PL activity still preceded ML (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), indicating that in the absence of eyes, face-specific activity still emerges across the ventral visual hierarchy in a bottom-up fashion.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental schematic and spatial selectivity of responsiveness.</title><p>(<bold>A</bold>) The monkey maintains fixation (red spot) while a large (16° x 16°) complex natural scene is presented at locations randomly sampled from a 17 × 17 (Monkey 1) or 9 × 9 (Monkey 2) grid of positions centered on the array’s receptive field (blue dotted circle). On a given trial, 3 different images are presented sequentially at 3 different positions. Across the experiment, each image is presented multiple times at each position on the grid. (<bold>B</bold>) Parts of a 16° x 16° image (black dots) that were centered within each array’s receptive fields across trials from the 9 × 9 stimulus presentation grid; position spacing, 2°. (<bold>C</bold>) Map of a face cell’s firing rate (100–250 ms after stimulus onset) in response to different parts of the image positioned over the receptive field (recording site 15 in Monkey 1 ML array). The most effective parts of this image were the monkey faces. (<bold>D</bold>) Complex natural images shown to the monkey. Population-level response maps to these images from (<bold>E</bold>) PL in Monkey 1; (<bold>F</bold>) ML in Monkey 1; and (<bold>G</bold>) ML in Monkey 2. Firing rates were averaged over 100–250 ms post stimulus onset for Monkey 1 and 150–300 ms for Monkey 2. For E, F, and G, the color scale represents the range between 0.5% and 99.5% percentiles of the response values to each image. All 3 arrays showed spatially specific activation to the faces in the images, with higher selectivity in the middle face patches. (<bold>H</bold>) PSTHs of responses to face (red) and non-face (blue) parts of these images for (top) Monkey 1 PL, (middle) Monkey 1 ML, and (bottom) M2 ML. Shading represents 95% confidence limits. Dashed red lines denote time window with a significant difference in response magnitude between face and non-face image parts (paired t-test across 8 images; t(7) &gt; 2.64; p&lt;0.05, FDR-corrected).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig1-v1.tif"/><permissions><copyright-statement>© 2014 Patrick Bolger. All rights reserved</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Patrick Bolger</copyright-holder><license><license-p>The image in Panel B is used with permission from Patrick Bolger@Dublin Zoo. It is not covered by the CC-BY 4.0 license and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions><permissions><copyright-statement>© 2009 Ralph Siegel. All rights reserved</copyright-statement><copyright-year>2009</copyright-year><copyright-holder>Ralph Siegel</copyright-holder><license><license-p>The second photograph in Panel D is courtesy of Ralph Siegel. It is not covered by the CC-BY 4.0 license and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions><permissions><copyright-statement>© 2012 A Stubbs. All rights reserved</copyright-statement><copyright-year>2012</copyright-year><copyright-holder>A Stubbs</copyright-holder><license><license-p>The fourth photograph in Panel D is courtesy of A Stubbs. It is not covered by the CC-BY 4.0 license and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions><permissions><copyright-statement>© 2008 Johnny Planet. All rights reserved</copyright-statement><copyright-year>2008</copyright-year><copyright-holder>Johnny Planet</copyright-holder><license><license-p>The fifth photograph in Panel D is courtesy of Johnny Planet. It is not covered by the CC-BY 4.0 license and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Localization of arrays.</title><p>Multi-electrode arrays were chronically implanted into two face patches within IT of two macaque monkeys. Arrays (outlined blue regions in the brain) were visualized with CT imaging. CT images were then rigidly aligned to a MPRAGE image of each monkey’s anatomy. (top) In Monkey 1, one Microprobes FMA array (blue outline) was implanted in the PL face patch. One NeuroNexus Matrix array was implanted in the ML face patch. This array was not visible on the CT image. (bottom) In Monkey 2, one Microprobes FMA array (outlined blue regions in the brain) was implanted in the ML face patch. Data threshold at t(2480) &gt; 3.91 (p&lt;0.0001 FDR-corrected) and t(1460) &gt; 3.91 (p&lt;0.0001 FDR-corrected) for monkeys 1 and 2, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Receptive-field maps for all visually responsive sites in the two arrays in Monkey 1 (both in the right hemisphere) and the single array in Monkey 2 (left hemisphere).</title><p>For each site, ~2°x2° images of faces and non-face objects were presented randomly interdigitated in the same recording session, and responses at each site were sorted by image type and normalized to the maximum response at that site. To estimate the response area of receptive fields, a 2D gaussian was fit to each channel’s response map. For RFs mapped with faces, mean (sigma) RF 1.91° +/- 0.27 STD for Monkey 1 PL; 2.30° +/- 0.32 STD for Monkey 1 ML and 2.61° +/- 0.56 STD for Monkey 2 ML. For RFs mapped with non-face objects, mean RF 1.96° +/- 0.23 STD for Monkey 1 PL; 2.23° +/- 0.43 STD for Monkey 1 ML and 2.36 ° +/- 0.49 STD for Monkey 2 ML. Examples of six faces and six non-face objects used for mapping are shown to the right.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Category selectivity of all visually responsive sites in PL in one monkey (top) and ML in two monkeys (middle and bottom).</title><p>The rows in each panel correspond to individual visually responsive recording sites in each electrode array, and the columns correspond to individual images, sorted by category. Images were 4° x 4°. Graphs on the right show PSTHs for each category averaged over all visually responsive sites in each of the electrode arrays in the two monkeys. Category selectivity was measured in Monkey 1 PL and ML in separate recording sessions, and the session for PL did not include bodies. The face selectivity index (face-response – nonface-response)/ (face-response + nonface-response) was greater than 0.3 in all sites in ML in both monkeys, corresponding to a face response at least twice as large as non-face responses (3). The average face selectivity index in Monkey 1 ML was 0.63 (std = 0.06) and in Monkey 2 ML was 0.64 (std = 0.14). The average face vs body selectivity index in Monkey 1 ML was 0.77 (std = 0.11), and in Monkey 2 ML was 0.60 (std = 0.21). The average body vs object selectivity index in Monkey 1 ML was −0.54 (std = 0.17) and in Monkey 2 ML 0.04 (std = 0.27). Face patch PL was less face selective, with an average face selectivity index of 0.24 (std = 0.07) and average hand selectivity index of 0.04 (std = 0.02) Face-selectivity indices were higher than hand or body indices for all channels in both PL and ML. There were no negative mean face or non-face object responses in ML of either monkey but one channel in each of the ML arrays had a below-baseline mean response to bodies, so those channels were not used in calculating the average body vs object indices.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Comparison of single-site response patterns with population average response patterns for all 3 arrays.</title><p>(Top row) Images used for correlation mapping. Top left image, still from clip, author ‘Videvo’ (2017) licensed under the ‘Videvo Attribution License’. Any reproduction requires crediting the author. Second image, copyright free photo (CC0) acquired from <ext-link ext-link-type="uri" xlink:href="https://pxhere.com/">pxhere.com</ext-link> (2017). Third image, still from The Adventurer, (1917) in the public domain as per <ext-link ext-link-type="uri" xlink:href="http://publicdomainmovie.net/">publicdomainmovie.net</ext-link>. Fourth image, still from clip, author ‘Videvo’ (2017) licensed under the ‘Videvo Attribution License.’ Any reproduction requires crediting the authors. Firing rates were averaged over 100–250 ms post stimulus onset for Monkey 1 and 150–300 ms for Monkey 2. Responses were scaled to 0.5% and 99.5% percentiles of the response values for each map. (right) Across images, the spatial patterns of responses for individual sites were correlated with average response maps for all other sites for Monkey 1 PL (mean r = 0.73; 0.10 std), Monkey 1 ML (mean r = 0.73; 0.26 std), and Monkey 2 ML (mean r = 0.73; 0.13 std).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig1-figsupp4-v1.tif"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>Comparison of response patterns between simultaneously recorded PL and ML arrays in Monkey 1 for different time windows.</title><p>Population-level response maps for 40 ms bins between 50 and 250 ms post-stimulus onset. For each image, face-specific activity emerged in PL earlier compared to ML Responses were scaled to 0.5% and 99.5% percentiles of the values for each map.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig1-figsupp5-v1.tif"/></fig></fig-group><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Cells in PL and ML responded to faces with and without eyes.</title><p>Population-level response maps from Monkey 1 PL and ML for 40 ms bins between 50 and 250 ms post stimulus onset. Face-specific activity arises in PL prior to ML for images both with and without eyes, even though the latency for each depends on the presence of eyes. (top) Activity specific to eyes emerges earlier than responses to the rest of the face in both PL and ML. (middle and bottom) For both PL and ML, face activity emerges at longer latencies when the eyes are blacked out. Photo courtesy of Ralph Seigel. Response maps scaled to 0.5% and 99.5% percentiles of the response values to each image.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig2-v1.tif"/><permissions><copyright-statement>© 2014 Santasombra. All rights reserved</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Santasombra</copyright-holder><license><license-p>The top-left image is reproduced with permission from Santasombra. It is not covered by the CC-BY 4.0 license and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Comparison of response patterns to images of eyeless faces between PL and ML in Monkey 1.</title><p>Population-level response maps for 40 ms bins between 50 and 250 ms post-stimulus onset. For each image, face-specific activity emerges in PL earlier than in ML. For both recording sites, face-specific activity emerges later for eyeless faces compared to the same faces with eyes (see also <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>). Responses were scaled to 0.5% and 99.5% percentiles of the values for each map.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig2-figsupp1-v1.tif"/></fig></fig-group><p>To explore contextual associations of natural face experience, we then presented complex scenes that lacked faces, but contained cues that indicated where a face ought to be. When presented with images in which the face was occluded by some object in the image, surprisingly, the face cells in ML in both monkeys responded to the part of the image where the face ought to be, more than to non-face parts of the images (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Though noisier than the population average, responses to occluded faces were apparent in individual channels (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The magnitude and timing of responses to occluded faces varied across stimuli. Across all stimuli, the population-level activity to occluded faces was reliably higher than the average response to the rest of each image (t-test across 12 and 10 images for Monkeys 1 and 2, respectively; p&lt;0.05, FDR-corrected). Interestingly, the responses to occluded faces on average were slower than the responses to intact faces by ~30–40 ms (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplements 2</xref> and <xref ref-type="fig" rid="fig3s3">3</xref>). The latency of responses to occluded faces was similar to that to eyeless faces (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). By 119 ms and 139 ms post stimulus onset for Monkey 1 and Monkey 2, respectively, the responses to occluded faces were significantly larger than the responses to the rest of the image. In contrast, responses to non-occluded versions of these faces were significantly larger than the responses to the rest of the image starting at 92 ms and 97 ms post stimulus onset for Monkey 1 and Monkey 2, respectively. It is possible that the face cells in ML responded to some tiny part of the head or neck that was still visible in each of the occluded-face photos, though similar visual features were present in other parts of each image that did not evoke strong firing. It is also unlikely that the occluding objects contained visual features that alone evoke strong firing as those objects were present in other parts of each image that did not evoke strong firing (e.g., the section of the rug in-between the two faces and boxes covering the face of the man walking down the stairs vs. boxes below the face). Furthermore, when presented with people wearing masks, baskets or paper bags covering their faces, cells in ML responded to the masked faces (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Similar to the occluded faces, responses to masked faces were also apparent in individual channels (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). The population-level responses to masked faces varied in magnitude and timing but were consistently stronger than average responses to the rest of each image (t-test across 10 and 11 images for Monkeys 1 and 2, respectively; p&lt;0.05, FDR-corrected). The latency of responses to masked faces (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplements 2</xref> and <xref ref-type="fig" rid="fig4s3">3</xref>) was also similar to that to eyeless faces (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Face cells in PL also responded more to occluded faces than to the non-face parts of the images (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>; p&lt;0.05, FDR corrected). Though cells in PL also responded on average more strongly to masked faces than to other parts of each image, this difference was not significant (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>; p&gt;0.05, FDR corrected). Thus, responses to occluded faces were more prominent at later stages of the ventral visual hierarchy.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Cells in ML in both monkeys responded to occluded faces.</title><p>(top) Images of occluded faces. Population-level response maps from (middle) Monkey 1 ML and (bottom) Monkey 2 ML. Firing rates were averaged over 100–250 ms post stimulus onset for Monkey 1 and 150–300 ms for Monkey 2. Response patterns scaled to 0.5% and 99.5% percentiles of the response values to each image. (right) PSTHs of responses in PL in Monkey 1 to regions of the image with occluded (orange) and non-occluded (red) faces and non-face parts (blue). Shading represents 95% confidence limits. (top) Responses of Monkey 1 ML. (bottom) Responses of Monkey 2 ML. Dashed red (and orange) lines denote time windows with significant differences in response magnitudes between face and occluded-face (and occluded vs. non-face) image parts (paired t-test across 12 images for Monkey 1; t(11) &gt; 2.80 for occluded-face vs. non-face; t(11) &gt; 2.70 for face vs. occluded-face; and 10 images for Monkey 2 t(9) &gt; 2.56 for occluded-face vs. non-face; t(9) &gt; 2.73; p&lt;0.05, FDR-corrected). Note, the small spike just after 300 ms in Monkey 1 is a juicer artifact from the reward delivered after each trial and at least 200 ms prior to the onset of the next stimulus presentation. Histograms show response differences to occluded-face regions minus non-face control regions, for each image, each channel (blue) and for the mean image across channels (orange).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Individual site responses to occluded faces in ML for Monkeys 1 and 2.</title><p>Response maps for images reported in <xref ref-type="fig" rid="fig3">Figure 3</xref> from 3 individual channels in (left) Monkey 1 ML and (right) Monkey 2 ML. Face- and body- selectivity indices are shown for each channel. Firing rates were averaged over 100–250 ms post stimulus onset for Monkey 1 and 150–300 ms for Monkey 2. Responses were scaled to 0.5% and 99.5% percentiles of the values for each map. The spatial patterns of responses for individual channels were correlated with average maps for all other sites for Monkey 1 ML intact faces (mean r = 0.87; 0.15 std), Monkey 1 ML occluded faces (mean r = 0.85; 0.14 std), Monkey 2 ML intact faces (mean r = 0.70; 0.14 std), and Monkey 2 ML occluded faces (mean r = 0.47; 0.13 std). Photographs shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Responses to occluded faces in Monkey 1 ML for different time windows.</title><p>Population-level response maps for 40 ms bins between 50 and 250 ms post-stimulus onset. For each image reported in <xref ref-type="fig" rid="fig3">Figure 3</xref>, activity where a face ought to be is apparent by 130 ms in ML for monkey 1. Similar to the latency shift observed for masked eyes (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), responses to the occluded faces were delayed relative to non-occluded faces. Responses were scaled to 0.5% and 99.5% percentiles of the values for each map. Photographs shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Responses to occluded faces in Monkey 2 ML for different time windows.</title><p>Population-level response maps for 50 ms bins between 50 and 300 ms post-stimulus onset. For each image reported in <xref ref-type="fig" rid="fig3">Figure 3</xref>, activity where a face ought to be is apparent by 150 ms in ML for monkey 2. Similar to the latency shift observed for masked eyes (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), responses to the occluded faces were delayed relative to non-occluded faces. Responses were scaled to 0.5% and 99.5% percentiles of the values for each map. Photographs shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig3-figsupp3-v1.tif"/></fig></fig-group><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Responses to masked faces.</title><p>Cells in ML in both monkeys responded to faces covered by masks, baskets, or paper bags. (top) Images of covered faces. Population-level response maps from (middle) Monkey 1 ML and (bottom) Monkey 2 ML. Firing rates were averaged 100–250 ms post stimulus onset for Monkey 1 and 150–300 ms for Monkey 2. Response patterns scaled to 0.5% and 99.5% percentiles of the response values to each image. (right) PSTHs of responses of sites in ML in Monkeys 1 and 2 to covered faces (orange). Shading represents 95% confidence limits. (top) Responses of sites in Monkey 1 patch ML. (bottom) Responses of sites in Monkey 2 ML. Dashed orange lines denote time windows with significant differences in response magnitudes between covered-face vs. non-face image parts (paired t-test across 10 images for Monkey 1 and 11 images for Monkey 2; t(9) &gt; 2.81 for Monkey 1; t(10) &gt; 2.42 for Monkey 2; p&lt;0.05, FDR-corrected). Histograms show response differences to masked face minus non-face control regions, for each image, each channel (blue) and for the mean image across channels (orange).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Individual channel responses to masked faces in Monkey 1 and 2 ML.</title><p>Response maps for images reported in <xref ref-type="fig" rid="fig4">Figure 4</xref> from 3 individual channels in (left) Monkey 1 ML and (right) Monkey 2 ML. Face- and body- selectivity indices are shown for each channel. Firing rates were averaged over 100–250 ms post stimulus onset for Monkey 1 and 150–300 ms for Monkey 2. Responses were scaled to 0.5% and 99.5% percentiles of the values for each map. The spatial patterns of responses for individual channels were correlated with the average maps for all other sites for Monkey 1 ML (mean r = 0.75; 0.16 std) and Monkey 2 ML (mean r = 0.55; 0.15 std). Photographs shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Responses to masked faces in Monkey 1 ML for different time windows.</title><p>Population-level response maps for 40 ms bins between 50 and 250 ms post-stimulus onset. For each image reported in <xref ref-type="fig" rid="fig4">Figure 4</xref>, activity where a face ought to be is apparent by 130 ms in Monkey 1 ML. Responses were scaled to 0.5% and 99.5% percentiles of the values for each map. Photographs shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Responses to masked faces in Monkey 2 ML at different time windows.</title><p>Population-level response maps for 50 ms bins between 50 and 300 ms post-stimulus onset. For each image reported in <xref ref-type="fig" rid="fig4">Figure 4</xref>, activity where the face ought to be is apparent by 150 ms in Monkey 1 ML. Responses were scaled to 0.5% and 99.5% percentiles of the values for each map. Photographs shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Responses to faces and masked faces in Monkey1 PL.</title><p>PSTHs of responses of sites in PL in Monkey 1 to (top) occluded and non-occluded faces, (middle) covered and non-covered faces, and (bottom) non-face images above a body or not above a body. Shading represents 95% confidence limits. Dashed red lines denote time windows with significant differences in response magnitudes between face and occluded face image parts (paired t-test across images; p&lt;0.05, FDR-corrected).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig4-figsupp4-v1.tif"/></fig></fig-group><p>To ask whether the face-cell responses to masked or occluded faces were due specifically to the context of a body just below the occluded face, we manipulated some images so that a non-face object was present in an image twice, once above a body, and once not (<xref ref-type="fig" rid="fig5">Figure 5</xref>, top). In both monkeys’ ML patches, responses to the non-face object positioned above a body were larger than responses to the same object when not above a body, both at the population level (<xref ref-type="fig" rid="fig5">Figure 5</xref>) and in individual channels (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Similar to the occluded and masked faces, population-level responses to these face-swapped images varied in magnitude and latency, but responses to non-face objects were consistently stronger when positioned above a body than when positioned elsewhere (t-test across 8 and 15 images for M1 and M2, respectively; p&lt;0.05, FDR-corrected). Similar to the occluded and masked faces, the latency of responses to the non-face object above a body (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplements 2</xref> and <xref ref-type="fig" rid="fig5s3">3</xref>) was comparable to that to eyeless faces (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Though cells in PL also responded on average more strongly to objects on top of bodies, this difference was not significant (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>; p&gt;0.05, FDR-corrected). Thus, responses in ML to a non-face image were facilitated by a body below it.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Face-like responses to non-face objects.</title><p>(top) Manipulated images with non-face objects in positions where a face ought and ought not to be. Population-level response maps from (middle) Monkey 1 ML and (bottom) Monkey 2 ML. Firing rates were averaged 100–250 ms post stimulus onset for Monkey 1 and 150–300 ms for Monkey 2. Response patterns scaled to 0.5% and 99.5% percentiles of the response values to each image. (right) PSTHs of Monkey 1 PL to non-face images above a body (orange) or not above a body (green). (top) PSTHs from Monkey 1 ML. (bottom) PSTHs from Monkey 2 ML. Shading indicates 95% confidence intervals. Dashed orange lines denote time windows with significant differences in response magnitudes between object-atop-body vs. object-apart-from-body (paired t-test across 8 and 15 images for Monkey 1 and 2, respectively; t(7) &gt; 3.04 for Monkey 1; t(14) &gt; 2.38 for Monkey 2; p&lt;0.05, FDR-corrected). Histograms show response differences to object atop body minus object control regions for each image, each channel (blue) and for the mean image across channels (orange).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Individual channel face-like responses to non-face objects in Monkey 1 and 2 ML.</title><p>Response maps for images reported in <xref ref-type="fig" rid="fig5">Figure 5</xref> from 3 individual channels in (left) Monkey 1 ML and (right) Monkey 2 ML. Face- and body- selectivity indices are shown for each channel. Firing rates were averaged over 100–250 ms post stimulus onset for Monkey 1 and 150–300 ms for Monkey 2. Responses were scaled to 0.5% and 99.5% percentiles of the values for each map. The spatial patterns of responses for individual channels were correlated with the average maps for all other sites for Monkey 1 ML (mean r = 0.76; 0.20 std) and Monkey 2 ML (mean r = 0.63; 0.17 std).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Face-like responses to non-face objects in Monkey 1 ML for different time windows.</title><p>Population-level response maps for 40 ms bins between 50 and 250 ms post-stimulus onset. For each image reported in <xref ref-type="fig" rid="fig5">Figure 5</xref>, activity where a face ought to be is apparent by 130 ms in Monkey 1 ML. Responses were scaled to 0.5% and 99.5% percentiles of the values for each map. Photographs shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Face-like responses to non-face objects in Monkey 1 ML for different time windows.</title><p>Population-level response maps for 50 ms bins between 50 and 300 ms post-stimulus onset. For each image reported in <xref ref-type="fig" rid="fig5">Figure 5</xref>, activity where the face ought to be is apparent by 150 ms in Monkey 2 ML. Responses were scaled to 0.5% and 99.5% percentiles of the values for each map. Photographs shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig5-figsupp3-v1.tif"/></fig></fig-group><p>Given our finding that bodies facilitated face-cell responses to masked, occluded, and non-face objects positioned above them, we might expect bodies to also facilitate responses to faces appropriately positioned above bodies, but this was not the case. Instead, responses to faces above bodies were indistinguishable from responses to disembodied faces (<xref ref-type="fig" rid="fig6">Figure 6</xref>, leftmost; <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref> and <xref ref-type="fig" rid="fig6s2">2</xref>, top two rows). This is consistent with the response maps to a subset of images that showed strong responses to faces regardless of the spatial relation to bodies (columns 3–5 in <xref ref-type="fig" rid="fig5">Figure 5</xref>, top). In contrast, responses to face-shaped noise patches (i.e. where face pixels were replaced by random 3-color channel pixel values), face outlines, and non-face objects were facilitated by the presence of a body positioned below (<xref ref-type="fig" rid="fig6">Figure 6</xref>; <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref> and <xref ref-type="fig" rid="fig6s2">2</xref>). Face-shaped noise patches and face-shaped outlines both elicited responses above baseline even in the absence of the body (middle two columns). This was likely due to these images containing typical shape features of a face, which is sufficient to drive these neurons (<xref ref-type="bibr" rid="bib26">Issa and DiCarlo, 2012</xref>; <xref ref-type="bibr" rid="bib48">Tsao et al., 2006</xref>). Importantly, the responses to these images when placed above a body were significantly larger (p&lt;0.05, FDR-corrected). Further, non-face objects activated these neurons only when placed above bodies (<xref ref-type="fig" rid="fig6">Figure 6</xref> rightmost column, p&lt;0.05, FDR-corrected; <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref> and <xref ref-type="fig" rid="fig6s2">2</xref>, bottom row). Together, these data suggest a ceiling effect--that bodies facilitate responses of face cells only when the presence of a face is ambiguous. Further, these neurons respond to the region above a body even if no foreground object is present (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>). In the absence of a face, the presence of a body below the receptive field appears to be sufficient for driving these face-selective cells.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Body-below facilitation of face-cell responses to non-face objects but not to faces.</title><p>Images of face, noise, face outline, and non-face object (top row) above a body and (second row) without a body. (bottom two rows) PSTHs from ML in Monkeys 1 and 2. Responses were calculated over the same face-shaped region for all images, for 7 different image sets of different individuals. Shading indicates 95% confidence intervals. Dashed colored lines denote time windows with significant differences in response magnitudes between face region above body vs. without body (paired t-test across 7 images; t(6) &gt; 2.79 for noise; t(6) &gt; 3.08 for outlines; t(6) &gt; 2.76 for non-face objects; p&lt;0.05, FDR-corrected). Inset histograms show response differences to the regions atop body minus the corresponding regions of no-body images, for each image, each channel (blue) and for the mean image across channels (orange).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Responses to face and non-face images with and without bodies.</title><p>Population-level response maps from Monkey 1 ML for images of intact faces, outlined face, face-shaped noise, and non-face objects above a body and without a body. To illustrate the effect of the body on response maps, each intact face, noise, and outlined image without a body was scaled to the maximum and minimum values of the corresponding image with a body. Object-above- body images were scaled to the min and max responses of the corresponding noise with body. Firing rates were averaged over 100–250 ms. Photographs shown in the section E of <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Responses to face and non-face images with and without bodies.</title><p>Population-level response maps from Monkey 2 ML for images of intact faces, outlined face, face-shaped noise, and non-face objects above a body and without a body. To illustrate the effect of the body on response maps, each intact face, noise, and outlined image without a body was scaled to the maximum and minimum values of the corresponding image with a body. Object-above-body images were scaled to the min and max responses of the corresponding noise with body. Firing rates were averaged over 150–300 ms post stimulus onset. Photographs shown in section E of <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig6-figsupp2-v1.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Responses to the region above bodies in the absence of faces or heads.</title><p>(top) Population-level response maps in Monkey 1 ML and Monkey 2 ML to images of faceless mannequins with and without heads. Population-level response maps in Monkey 1 ML and Monkey 2 ML to images of headless bodies with either uniform white or black backgrounds. Firing rates were averaged over 100–250 ms post stimulus onset for Monkey 1 and 150–300 ms for Monkey 2. Responses were scaled to 0.5% and 99.5% percentiles of the values for each map.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig6-figsupp3-v1.tif"/></fig></fig-group><p>If bodies positioned directly below the activating region facilitate face-cell responses, is it because face cells have a discrete excitatory region, below the activating region, that is simply responsive to bodies? Or is the effect more complex, reflecting contextual information that a body should be accompanied by a face? To address this question, we mapped responsiveness to bodies, with and without heads, at varying orientations and locations in a 9 × 9 grid pattern (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). For bodies-with-heads, responses were maximal when the face was centered in the cells’ activating region, for each orientation (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, top row). Headless bodies produced activation maps that were similarly maximal at positions in which a face ought to be centered on the cells’ activating region, not just when the body was positioned below the activating region (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, bottom two rows). The magnitude of firing rate varied across the tested visual field locations (e.g., firing rates tended to be higher in the upper half of the image). Composite maps show the preferred body orientation for each spatial position tested (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, right side). In experiments both with and without heads, preferred orientation systematically varied such that the preferred body orientation at that spatial location positioned the face, or where the face ought to be, within the RF center. Interestingly, these neurons were also responsive when the feet landed in the activating region, for the bodies without heads, and, to a lesser extent, the bodies with heads. This was especially apparent for inverted bodies, suggesting that both end-of-a-body and above-a-body are cues that a face might be expected. The surprising result that face cells are maximally responsive to where a face would be expected, largely irrespective of the body orientation, indicates that the responses to occluded faces documented in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref> are not responses merely to bodies located below the cells’ face-selective regions, but instead reflect information about where a face ought to be, for a given body configuration.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Body-orientation selectivity.</title><p>(<bold>A</bold>) Upper left panel shows an example head-only image overlaid on the population-level activation map averaged across all head only images (n = 8) from Monkey 1 ML. The upper row shows an example whole body image at eight different orientations each overlaid on the average activation maps for all whole bodies (n = 8); maximum responsiveness was focal to the face at each orientation. The lower two rows show a similar spatial selectivity for the region where a face ought to be at each orientation for headless bodies (n = 8) in Monkey 1 and 2 ML. Whole and headless body image sets were presented on a uniform white background (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). The composite maps on the far right show the best body orientation at each grid point for whole bodies and headless bodies for face patch ML in both monkeys, as indicated. The scale of the body orientations from top to bottom corresponds to the body orientations shown left to right in the response maps. Black sections denote areas with responses at or below baseline. For both whole and headless bodies, maximal activation occurred for bodies positioned such that the face (or where the face ought to be) would be centered on the cells’ activating region. (<bold>B</bold>) PSTHs when whole body and face-shaped noise atop bodies were presented within the cell’s activating region in both monkeys. For whole body images, there was only a brief difference in activity between upright and inverted configurations during the initial response. For face-shaped noise, the response was substantially larger for the upright (vs. inverted) configurations for the duration of the response. Dashed colored lines denote time windows with significant differences in response magnitudes between upright vs. inverted face regions of the image (t-test across 7 images; t(6) &gt; 4.74 for intact faces; t(6) &gt; 3.34 for noise; p&lt;0.05, FDR-corrected).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Body-orientation experiment.</title><p>Images of 8 different bodies, with or without heads, were presented at various positions in a 9 × 9 grid pattern while the animal fixated on a central fixation spot. Responses were calculated over a 100 ms to 250 ms window after each stimulus presentation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Responses to upright and inverted face and noise images.</title><p>Population-level response maps from(top) Monkey 1 ML and (bottom) Monkey 2 ML for upright and inverted images of intact faces with bodies and face-shaped noise with a body. To illustrate the effect of the inversion on response maps, inverted images were scaled to the maximum and minimum values of the corresponding upright versions. Firing rates were averaged over 100–250 ms post stimulus onset for Monkey 1 and 150–300 ms for Monkey 2. Upright and inverted intact face and noise photographs shown in the first and fourth columns in section E of <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig7-figsupp2-v1.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Stimuli used in each experiment and the corresponding binary ROIs of the image regions used for calculating responses or psths corresponding to faces or regions above the body.</title><p>(<bold>A</bold>) Stimuli and ROIs reported in <xref ref-type="fig" rid="fig1">Figure 1</xref>. (<bold>B</bold>) Stimuli and ROIs reported in <xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s3">3</xref>. (<bold>C</bold>) Stimuli and ROIs reported in <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>. (<bold>D</bold>) Stimuli and ROIs reported in <xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s3">3</xref>. (<bold>E</bold>) Stimuli and ROIs reported in <xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig7">7B</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref> and <xref ref-type="fig" rid="fig6s2">2</xref>, <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>. (<bold>F</bold>) Stimuli reported in <xref ref-type="fig" rid="fig7">Figure 7A</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig7-figsupp3-v1.tif"/></fig></fig-group><p>If bodies convey information to face cells about where a face ought to be, are there preferred face-body configurations? When face and face-shaped noise were presented attached to bodies in upright and inverted configurations, the strongest response was when the face (or face-shaped noise) was presented at the center of the activating region (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). For intact faces with bodies presented at the center of the activating region, there was only a small inversion effect (<xref ref-type="fig" rid="fig7">Figure 7B</xref>; whole bodies). While reaction time differences between upright and inverted faces are robust and reliable across studies (<xref ref-type="bibr" rid="bib32">Leehey et al., 1978</xref>; <xref ref-type="bibr" rid="bib50">Yin, 1969</xref>), differences in response magnitudes within IT face domains are modest or not consistently found in humans (<xref ref-type="bibr" rid="bib3">Aguirre et al., 1999</xref>; <xref ref-type="bibr" rid="bib24">Haxby et al., 1999</xref>; <xref ref-type="bibr" rid="bib28">Kanwisher et al., 1998</xref>) or monkeys (<xref ref-type="bibr" rid="bib12">Bruce, 1982</xref>; <xref ref-type="bibr" rid="bib41">Pinsk et al., 2009</xref>; <xref ref-type="bibr" rid="bib45">Rosenfeld and Van Hoesen, 1979</xref>; <xref ref-type="bibr" rid="bib47">Taubert et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Tsao et al., 2006</xref>). The lack of a strong inversion effect for intact-faces-over-bodies compared to a stronger inversion effect for non-face-objects-above-bodies could reflect a ceiling effect, or it could mean that, in the presence of a face, tuning is more flexible and more invariant to spatial transformations. In contrast to responses to intact faces, there was a large, sustained difference in response magnitude between upright and inverted face-shaped noise patches with bodies (<xref ref-type="fig" rid="fig7">Figure 7B</xref>; noise atop bodies), suggesting that the body facilitation is maximal when the presence of a face is ambiguous and above a body. The preference for upright vs. inverted configurations with the face-shaped noise is consistent with biases apparent in <xref ref-type="fig" rid="fig7">Figure 7A</xref>. Though face cells in ML were responsive to headless bodies at a variety of spatial positions, activity tended to be strongest when bodies were positioned below the activating region (<xref ref-type="fig" rid="fig7">Figure 7A</xref>; bottom two rows). This was particularly clear in Monkey 2 where the part of the image that gave the strongest response for the inverted (180-degree) body orientation was the feet, meaning that the inverted body still drove neurons most when it was positioned just below the activating region. Thus, for face-ambiguous stimulation, contextual responses are strongest to spatial configurations that reflect typical experience.</p><p>How can face-selective neurons respond to bodies positioned below their receptive fields? One possibility is that such contextual sensitivity is intrinsic to IT and pre-specified for the processing of biologically important information. Alternatively, given that the development of face domains depends on seeing faces during development (<xref ref-type="bibr" rid="bib4">Arcaro et al., 2017</xref>), these conjoint representations might also be learned over development as a consequence of frequently seeing faces and bodies together. To explore whether learning is sufficient to link face and body representations, we analyzed the representation of image categories in a computational model that has similar architectural features to the object processing pathway in primates. Specifically, we took a convolutional neural network, AlexNet (<xref ref-type="bibr" rid="bib31">Krizevsky et al., 2012</xref>), trained to classify natural images into 1000 object categories, and assessed its activation patterns evoked by images of isolated objects from 6 different categories; human faces, headless human bodies, animals, houses, inanimate objects, and handheld tools (<xref ref-type="fig" rid="fig8">Figure 8A</xref>, top). We tested a stimulus set widely used for localizing category-selective regions in IT cortex (<xref ref-type="bibr" rid="bib17">Downing et al., 2006</xref>). We assessed the similarity of activation patterns between all image pairs in every layer of AlexNet and in pixel space. As seen in the multidimensional scaling visualization, images are distinguished along categorical distinctions even based on just pixel intensity (<xref ref-type="fig" rid="fig8">Figure 8A</xref>, bottom). The Euclidean distances between image pairs within each category were smaller than between categories, indicating that images were more homogenous within than between categories and highlighting that low-level features such as shape, color, or texture are a major component of these commonly tested ‘semantic’ categories (also see <xref ref-type="bibr" rid="bib16">Dailey and Cottrell, 1999</xref>). Distinctions between categories were present in early layers of AlexNet (<xref ref-type="fig" rid="fig8">Figure 8A</xref>, bottom). The distinction between faces and non-faces persisted across all layers of AlexNet. The mean distance between face pairs (red) was smaller than the mean distance between faces and non-body, non-face categories (animals, houses, objects, and tools). Across all rectified linear unit (relu) layers of AlexNet, activation patterns were more similar for within category comparisons of both faces and bodies (<xref ref-type="fig" rid="fig8">Figure 8B</xref>, solid red and green circles, Spearman rank correlations) than between faces and other categories (dual colored circles). At the pixel level and in early layers of AlexNet, the mean similarity between faces and bodies (half red/half green circles) was greater than the similarity between faces and the other categories; this similarity was also stronger to that obtained under randomization of the stimulus labels for non-face categories (this randomization distribution is marked by the grey shaded region that denotes 2.5% and 97.5% bounds). In the deeper layers of AlexNet (relu6 and relu7), face and body image representations become more overlapping (<xref ref-type="fig" rid="fig8">Figure 8B</xref>, inset). In relu7, the correlation between face and body image pairs were only slightly weaker than the correlation between pairs of body images. Comparable effects were observed when assessing similarity with Pearson correlations and Euclidean distance on z-score normalized data. Further, these results were specific to the trained AlexNet, as untrained versions of the network did not yield strong face-body correlations and these correlations were no different than correlations between faces and other categories (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). These results demonstrate that representations of faces and bodies can merge over learning without any explicit training about faces and bodies. This is particularly impressive given that AlexNet was not trained to categorize faces, bodies, or even humans. Because faces and bodies co-occur in people and people were present in the training images, faces and bodies share variance in their informativeness with respect to class labels. There is thus pressure for the network to use overlapping representations in the face and body hidden layer representations in this supervised learning scenario. This phenomenon should hold more generally in unsupervised or self-supervised settings as the presence of image co-occurrence in the input would be expected to lead to similar overlap. Though convolutional networks can capture some aspects of the representational structure of biological networks (<xref ref-type="bibr" rid="bib25">Hong et al., 2016</xref>; <xref ref-type="bibr" rid="bib42">Ponce et al., 2019</xref>), these networks as currently implemented are not well suited for probing the retinotopic specificity of object representations, because they are explicitly constrained to process inputs from different portions of an image in the same way. Future work could probe the spatial relationships between commonly experienced object categories in computational models more optimized for maintaining spatial coding.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Learned face and body representations in hierarchical artificial neural network (AlexNet).</title><p>Representational similarity of a stimulus set comprising 240 images across 6 categories including faces and bodies. (<bold>A</bold>, top) Two example images from each category. (<bold>A</bold>, bottom) Visualization of similarity between faces (red), bodies (green), houses (blue), objects (green), and tools (pink) from multidimensional scaling of (left column) pixel intensity, (middle column) relu1 units, (right column) relu7 units) Euclidean distances. (<bold>B</bold>) Comparison of representational similarity within and between categories. Spearman rank correlations were calculated between image pairs based on pixel intensity and activations within AlexNet layers for all object categories. Mean similarity within (solid filled circles) and between (dual colored circles) categories are plotted for pixel intensity and the relu layers of AlexNet. The 2.5% and 97.5% percentiles from a permutation test in which the non-face stimulus labels were shuffled before calculating correlations (grey shaded region) are plotted for pixel and relu layers of AlexNet comparisons. Across all layers, both face and body images are more similar within category than between categories. (inset) The representations of faces and bodies are most similar to each other in deep layers (relu6 and relu7) of AlexNet.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig8-v1.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Comparison of correlations between trained and untrained versions of AlexNet.</title><p>The 2.5% and 97.5% percentiles from a permutation test in which the weights of the each AlexNet layer were shuffled prior to computing activations are shown for correlations between faces and bodies (black shaded region) and between faces and all other categories (grey shaded regions). Within and between category representational similarity for the pretrained AlexNet reported in <xref ref-type="fig" rid="fig8">Figure 8B</xref> are shown for comparison.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig8-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our finding that face-selective neurons respond to parts of an image where a face ought to be—above a body—goes well beyond previous observations on category selectivity in inferotemporal cortex. Previous neuronal recordings in macaques found that face-selective neurons respond only weakly to images of headless bodies, and body-selective neurons poorly to faces, suggesting that face and body circuits are largely independent (<xref ref-type="bibr" rid="bib20">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib19">Freiwald et al., 2009</xref>; <xref ref-type="bibr" rid="bib30">Kiani et al., 2007</xref>; <xref ref-type="bibr" rid="bib48">Tsao et al., 2006</xref>). Human fMRI studies have also provided evidence that face and body processing are independent (<xref ref-type="bibr" rid="bib27">Kaiser et al., 2014</xref>; <xref ref-type="bibr" rid="bib40">Peelen and Downing, 2005</xref>; <xref ref-type="bibr" rid="bib39">Peelen et al., 2006</xref>; <xref ref-type="bibr" rid="bib46">Schwarzlose et al., 2005</xref>) and that face-selective domains in the FFA do not respond to headless animals or human bodies presented at fixation (<xref ref-type="bibr" rid="bib29">Kanwisher et al., 1999</xref>; <xref ref-type="bibr" rid="bib40">Peelen and Downing, 2005</xref>; <xref ref-type="bibr" rid="bib46">Schwarzlose et al., 2005</xref>). Consistent with these prior findings, the face neurons we recorded from responded strongly and selectively to faces presented over a limited part of the visual field but did not respond to bodies presented to the same region. However, these neurons also responded to masked and occluded faces that lacked intrinsic facial features, but were above bodies. Further, when non-face objects that did not by themselves evoke a response from these neurons were placed above a body, the face-selective neurons fired robustly. Consistent across all six experiments, these neurons responded to bodies positioned such that the body implied the presence of a face within the neurons’ activating regions. In prior studies, both headless bodies and faces were presented at fixation, so our results suggest that these studies might have found larger responses to bodies if the bodies had been presented lower in the visual field, below the center of maximal activation to faces. Thus, face cells demonstrate a large contextual surround tuned to entities that normally co-occur with faces.</p><p>Though prior studies have mostly provided evidence for the independence of face- and body- processing circuits, a few fMRI studies found facilitatory influences of bodies on blood-flow responses in face-selective brain regions: The face fusiform area (FFA) in humans has been reported to be responsive to animal bodies with obscured faces (<xref ref-type="bibr" rid="bib14">Chao et al., 1998</xref>) or to blurred faces above bodies (<xref ref-type="bibr" rid="bib15">Cox et al., 2004</xref>). <xref ref-type="bibr" rid="bib15">Cox et al., 2004</xref> also found body-below facilitation of BOLD signal for blurred faces but not for clear faces, and they found that the BOLD signal facilitation depended on the face and body being in a correct spatial configuration. Both these findings are consistent with our neuronal recording results. In monkeys, the most anterior face patches show stronger fMRI activations to images of whole monkeys than to the sum of the responses to faces and to bodies; however, the middle and posterior face patches (which we recorded here) do not show facilitation, and have only sub-additive responses (<xref ref-type="bibr" rid="bib18">Fisher and Freiwald, 2015</xref>). Thus, although there is some fMRI evidence for interactions between face and body processing, fMRI studies are limited in that they can measure responses only of large populations of neurons of potentially mixed selectivity and therefore cannot resolve the relationship of face and body neural circuits. Here we show by direct neuronal recording that face and body circuits are not independent and that face-selective neurons can express large, complex, silent, contextual surround effects. From simultaneously recording neurons in PL and ML face patches, we show that these responses emerge fast and are first detected in the posterior patch, suggesting that contextual effects involve at least some feedforward inputs that cannot be attributed to cognitive processes such as mental imagery or expectation. The idea that the context of a visual scene leads to the expectation, enhanced probability, or prior of seeing some particular item is usually thought to involve top-down processes. However, if the feedforward inputs to, say, face-selective neurons, are sculpted by experience to be responsive not only to faces, but also to things frequently experienced in conjunction with faces, then such expectations would be manifest as the kind of complex response properties reported here. Furthermore, in contrast to the traditional view of a dichotomy of spatial and object vision, our data demonstrate that object processing in IT is spatially specific (also see <xref ref-type="bibr" rid="bib38">Op De Beeck and Vogels, 2000</xref>).</p><p>More broadly, these results highlight that neurons in inferotemporal cortex do not code objects and complex shapes in isolation. We propose that these neurons are sensitive to the statistical regularities of their cumulative experience and that the facilitation of face-selective neurons in the absence of a clear face reflects a lifetime of experience that bodies are usually accompanied by faces. Recently, we proposed that an intrinsic retinotopic organization (<xref ref-type="bibr" rid="bib6">Arcaro and Livingstone, 2017</xref>; also see <xref ref-type="bibr" rid="bib23">Hasson et al., 2002</xref>) guides the subsequent development of category-selectivity in inferotemporal cortex, such as face and body domains (<xref ref-type="bibr" rid="bib4">Arcaro et al., 2017</xref>). For example, monkeys normally fixate faces, and they develop face domains within regions of inferotemporal cortex that represent central visual space (<xref ref-type="fig" rid="fig9">Figure 9</xref>, top). Here, we propose that these same developmental mechanisms can account for contextual learning. Our results indicate that, just as the retinotopic regularity of experiencing faces guides where in IT face domains develop, the spatial regularity between faces and bodies is also learned in a retinotopic fashion and may account for why face and body patches stereotypically emerge in adjacent parts of cortex (<xref ref-type="fig" rid="fig9">Figure 9</xref>, bottom) in both humans (<xref ref-type="bibr" rid="bib46">Schwarzlose et al., 2005</xref>) and monkeys (<xref ref-type="bibr" rid="bib10">Bell et al., 2009</xref>; <xref ref-type="bibr" rid="bib41">Pinsk et al., 2009</xref>). Such a spatial regularity is consistent with prior observations of a lower visual field bias for the RF centers of body-selective neurons near ML (<xref ref-type="bibr" rid="bib43">Popivanov et al., 2015</xref>) and an upper visual field bias for eye-selective neurons in PL (<xref ref-type="bibr" rid="bib26">Issa and DiCarlo, 2012</xref>). Thus, something as complex and abstract as context may be firmly grounded in the ubiquitous mechanisms that govern the wiring-up of the brain across development (<xref ref-type="bibr" rid="bib5">Arcaro et al., 2019</xref>).</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Preferential looking at faces imposes stereotyped retinal experience of bodies.</title><p>(top left) Primates spend a disproportionate amount of time looking at faces in natural scenes. (top right) This retinal input is relayed across the visual system in a retinotopic fashion. (bottom left) Example retinal experience of a scene along the eccentricity dimension when fixating a face. Given that bodies are almost always experienced below faces, preferential face looking imposes a retinotopic regularity in the visual experience of bodies. (bottom right) This spatial regularity could explain why face and body domains are localized to adjacent regions of retinotopic IT.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-fig9-v1.tif"/><permissions><copyright-statement>© 2009 American Physiological Society. All rights reserved</copyright-statement><copyright-year>2009</copyright-year><copyright-holder>American Physiological Society</copyright-holder><license><license-p>Right images reproduced from <xref ref-type="bibr" rid="bib10">Bell et al., 2009</xref>. They are not covered by the CC-BY 4.0 license and further reproduction of this figure would need permission from the copyright holder.</license-p></license></permissions></fig></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All procedures were approved in protocol (1146) by the Harvard Medical School Institutional Animal Care and Use Committee (IACUC), following the <italic>Guide for the care and use of laboratory animals</italic> (Ed 8). This paper conforms to the ARRIVE Guidelines checklist.</p><sec id="s4-1"><title>Animals and behavior</title><p>Two adult male macaques (10–12 kg) implanted with chronic microelectrode arrays were used in this experiment. In monkey 1, two floating microelectrode arrays were implanted in the right hemisphere superior temporal sulcus – one within the posterior lateral (PL) face patch in posterior inferotemporal cortex (PIT) (32-channel Microprobes FMA; <ext-link ext-link-type="uri" xlink:href="https://microprobes.com/products/multichannel-arrays/fma">https://microprobes.com/products/multichannel-arrays/fma</ext-link>) and a second array within the middle lateral (ML) face patch in central inferotemporal cortex (CIT) (128-channel NeuroNexus Matrix array; <ext-link ext-link-type="uri" xlink:href="http://neuronexus.com/products/primate-large-brain/matrix-array-primate/">http://neuronexus.com/products/primate-large-brain/matrix-array-primate/</ext-link>). In monkey 2, one 32-channel floating microelectrode array was implanted in left hemisphere ML. Monkeys were trained to perform a fixation task. Neural recordings were performed on a 64-channel Plexon Omniplex Acquisition System. For monkey 1, recordings were performed simultaneously for the 32-ch PIT array and 32-ch from the CIT array. In both monkeys, the same channels were recorded from for each experimental session. The internal dimensions of the Microprobes array was 3.5 × 1.5 mm with a spacing of 370–400 microns between electrodes (across the width and length of the array respectively). The Neuronexus matrix array was a comb arrangement, with 200 micron spacing between the 4 shanks and 400 microns site spacing along the shanks. The arrays were oriented with the long axis AP. The task required the monkey to keep their gaze within ± 0.75° of a 0.25° fixation spot. We used an ISCAN eye monitoring system to track eye movements (<ext-link ext-link-type="uri" xlink:href="http://www.iscaninccom">www.iscaninccom</ext-link>).</p></sec><sec id="s4-2"><title>fMRI-guided array targeting</title><p>Functional MRI studies were carried out in both monkeys to localize face patches along the lower bank of the superior temporal sulcus. For scanning, all monkeys were alert, and their heads were immobilized using a foam-padded helmet with a chinstrap that delivered juice. The monkeys were scanned in a primate chair that allowed them to move their bodies and limbs freely, but their heads were restrained in a forward-looking position by the padded helmet. The monkeys were rewarded with juice during scanning. Monkeys were scanned in a 3 T TimTrio scanner with an AC88 gradient insert using 4-channel surface coils (custom made by Azma Mareyam at the Martinos Imaging Center). Each scan session consisted of 10–12 functional scans. We used a repetition time (TR) of 2 s, echo time (TE) of 13 ms, flip angle of 72°, iPAT = 2, 1 mm isotropic voxels, matrix size 96 × 96 mm, 67 contiguous sagittal slices. To enhance contrast (<xref ref-type="bibr" rid="bib33">Leite et al., 2002</xref>; <xref ref-type="bibr" rid="bib49">Vanduffel et al., 2001</xref>), we injected 12 mg/kg monocrystalline iron oxide nanoparticles (Feraheme, AMAG Pharmaceuticals, Cambridge, MA) in the saphenous vein just before scanning. Responses to image categories of faces and inanimate objects were probed. Brain regions that responses more strongly to faces than objects were identified along the lower bank of the STS (p&lt;0.0001, FDR-corrected). <xref ref-type="bibr" rid="bib6">Arcaro and Livingstone, 2017</xref> for additional details.</p><p>After array implantation, Computed Tomography (CT) scans (0.5 × 0.5×1.25 mm) were collected in both monkeys. The base of the FMA arrays and wires were visible on both CT images. The base of the NeuroNexus array was not clearly visible on the CT image. Each monkey’s CT image was spatially aligned to its MPRAGE anatomical image. Because brain/skull contrast is opposite between CT and MPRAGE MRI images, the two volumes were aligned by manually creating a binary brain mask for both CT and MPRAGE images and rigidly aligning the brain masks. The resulting spatial transformation matrix was applied to bring the CT and MPRAGE images into alignment. The location of the FMA arrays were then compared with the fMRI-defined PL and ML face patches.</p></sec><sec id="s4-3"><title>Electrophysiology display</title><p>We used MonkeyLogic to control experimental workflow (<ext-link ext-link-type="uri" xlink:href="https://monkeylogic.nimh.nih.gov">https://monkeylogic.nimh.nih.gov</ext-link>). Visual stimuli were displayed on an 19’ Dell LCD screen monitor with a 60 Hz refresh rate and a 4:3 aspect ratio positioned 54 cm in front of each monkey.</p></sec><sec id="s4-4"><title>Receptive field mapping</title><p>To map the location of each recording site’s receptive field, we flashed small (~2°x2°) images of faces or round non-face objects at different locations relative to the fixation point in a 16°x16° grid with 1–2° spacing centered at the fixation point. Each image was presented for 100 ms ON and 200 ms OFF. Responses were defined as the mean firing rate over 80–250 ms after picture onset minus the mean firing rate over the first 30 ms after image onset. Responses were averaged across image repetitions. A 2-dimensional gaussian was fit to each array channel’s 9 × 9 response grid to derive the centroids and response widths (1 sigma). We define the population RF as the mean array response. To characterize tuning of each recording site, images of isolated faces, hands, bodies, and objects on a white background were presented within the activating region of all the visually-responsive sites. Each image subtended 4° and was presented for 100 ms ON and 200 ms OFF. Responses were defined as the mean firing rate over 80–250 ms after image onset minus the mean firing rate over the first 30 ms after image onset. Responses were averaged across image repetitions. Category selectivity was assessed with an index (category1-response –category2-response)/ (category1-response + category2-response) (<xref ref-type="bibr" rid="bib7">Baker et al., 1981</xref>). For computing the index, channels with negative responses were excluded. There were no channels with negative responses in the PL array in Monkey 1. For the ML arrays in both monkeys, we excluded 1 channel each from the body vs objects index due below baseline responses to bodies.</p></sec><sec id="s4-5"><title>Main Experiment stimuli</title><p>Stimuli comprised 16° x 16° natural scene photographs embedded in a brown-noise image that filled the entire monitor screen. Each photo could contain humans, monkeys, other animals, or artificial objects (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>). Several of the photos were modified to remove the face or to replace the face with non-face objects or pixelated noise. Photos were grouped into several image sets. The first image set comprised unaltered natural scenes containing humans and monkeys. The second image set comprised images of people wearing masks. The third image set contained images where faces were occluded with objects. The fourth image set comprised images where objects in each scene were pasted over the faces. The fifth image set comprised 6 people and one monkey with 8 variations for a total of 56 images. For each person/monkey, the face was replaced with either pixelated noise (i.e. where face pixels were replaced by random 3-color channel pixel values), an outline, another object in the scene, or left intact. Each face manipulation was presented either above a body or without the body. The sixth image set comprised 7 people and 1 monkey each at one of 8 orientations at 45° intervals for a total of 64 images. The seventh image set comprised 6 people and 1 monkey with 4 variations for a total of 28 images. For each person/monkey, the face was presented either intact or replaced with pixelated noise and either in upright or inverted orientation. For each experiment, pictures were presented over a 17 x 17 point grid (spacing 1°) for Monkey 1 and a 9 × 9 point grid (spacing of 2°) for Monkey 2 with the image centered on the center of the population receptive field. Generally, Monkey 2 did not work for as long as Monkey 1 so a coarser grid was necessary in order to get enough repetitions of all stimuli at each position. The difference in grid sampling is unlikely to be a major factor as results for all experiments were consistent between monkeys. Further, the 2° spacing of this 9 × 9 grid remained smaller than the response width of the RF mapping and was sufficient for detecting spatially specific activity. Images were presented for 100 ms ON and 200 ms OFF. Three images were presented per trial (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). At the start of each trial, the fixation target appeared, and the animal had up to 5 s to direct its gaze to the fixation target. If the animal held fixation until the end of the trial, a juice reward was delivered at 100 ms after the last (third) image’s offset period. Every picture was presented at each grid position 5–10 times. In Monkey 1, PL and ML arrays were recorded from simultaneously.</p></sec><sec id="s4-6"><title>Experimental session workflow</title><p>Data were collected across 4 months in M1 and 1 month in M2. Each day, the animal would be head-fixed and its implant(s) connected to the experimental rig. First, the animal’s gaze was calibrated with a 9-point grid using a built-in MonkeyLogic routine. We used the Plexon Multichannel Acquisition Processor Data Acquisition System to collect electrophysiological information, including high-frequency (‘spike’) events, local field potentials, and other experimental variables, such as reward rate and photodiode outputs tracking monitor frame display timing. Each channel was auto-configured daily for the optimal gain and threshold; we collected all electrical events that crossed a threshold of 2.5 SDs from the mean peak height of the distribution of electrical signal amplitudes per channel. These signals included typical single- and multi-unit waveforms.</p></sec><sec id="s4-7"><title>Spike data preparation</title><p>The raw data comprised event (‘spike’) times per channel for the entire experimental session. In Monkey 1, 32 channels were recorded simultaneously from both the posterior and middle face patch arrays for the main experiment. The same channels were recorded from for the entire duration of this study.</p></sec><sec id="s4-8"><title>Analysis</title><p>For each image, trials were sorted based on grid position. Trial repetitions were averaged to derive the mean activity when each grid position was centered within the mean activating region of each array. The mean activity was visualized over each image with pixels between grid points linearly interpolated. Activity overlapping pixels within intact or manipulated faces were averaged and plotted as post stimulus time histograms (smoothed using a gaussian kernel over 20 ms sliding window). This activity was compared with the mean activity over pixels of other parts of the image. Significance was assessed across images with paired, two-way t-tests, corrected for a false discovery rate adjusted p value of 0.05.</p></sec><sec id="s4-9"><title>Convolution neural network image analysis</title><p>240 greyscale images evenly distributed across 6 categories (human faces, headless human bodies, animals, houses, tools, and inanimate objects) were analyzed. Each category image was presented on a uniform white background. Images were analyzed using the MATLAB implementation of a pre-trained convolutional neural network, AlexNet. Network activations across relu layers of AlexNet were computed for each image. To measure the representational similarity across images within a layer, Spearman rank correlations were computed between z-scored activations for each image pair. To visualize the representational structure of each layer, multidimensional scaling was performed on squared Euclidean distances between the z-scored activation patterns of image pairs. To probe distinctions purely based on image shape, correlations and multidimensional scaling were also performed on the vectorized 3 color-channel pixel intensities for each image. Permutation testing was performed by randomizing the labels corresponding of non-face images prior to computing correlations between face and non-face categories. Mean correlations and the 2.5% and 97.5% percentiles across 2500 randomizations were computed for each layer of AlexNet and for pixel intensity. To look at the effect of training, an additional permutation test was performed where the weights within each layer were shuffled prior to measuring the activations for each image and computing image pair correlations. This approach simulates an untrained network while preserving the distribution of weights found in the trained network. The mean correlations and the 2.5% and 97.5% percentiles across 500 permutations were computed for each layer of AlexNet.</p></sec><sec id="s4-10"><title>Quantification and statistical analyses</title><p>For activity maps in <xref ref-type="fig" rid="fig1">Figures 1</xref>–<xref ref-type="fig" rid="fig7">7</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplements 2</xref>, <xref ref-type="fig" rid="fig1s4">4</xref> and <xref ref-type="fig" rid="fig1s5">5</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s3">3</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s3">3</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref>–<xref ref-type="fig" rid="fig6s3">3</xref>, and <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref> spike data were averaged across trial repetitions. For PSTH graphs in <xref ref-type="fig" rid="fig1">Figures 1</xref>–<xref ref-type="fig" rid="fig7">7</xref> and <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>, the mean response and 95% confidence intervals across images were calculated. For each 1 ms bin, two-sample, paired t-tests were performed across images on the firing rates between the target and control regions. The resulting p values were adjusted for false discovery rates. Given, the relatively small number of observations going into each t-paired t-test, significant effects were verified further with nonparametric (Wilcoxon signed-rank) tests.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank A Schapiro for advice on analyses and helpful comments on the manuscript. This work was supported by NIH grants RO1 EY 16187 and P30 EY 12196.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Visualization, Methodology</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Data curation, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All procedures were approved in protocol (1146) by the Harvard Medical School Institutional Animal Care and Use Committee (IACUC), following the Guide for the care and use of laboratory animals (Ed 8). This paper conforms to the ARRIVE Guidelines checklist.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-53798-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Custom scripts and data to reproduce figures have been deposited to GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/mikearcaro/NoHeadResponseMaps">https://github.com/mikearcaro/NoHeadResponseMaps</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/NoHeadResponseMaps">https://github.com/elifesciences-publications/NoHeadResponseMaps</ext-link>). The complete dataset that was collected is available upon reasonable request to the corresponding author.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afraz</surname> <given-names>SR</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Microstimulation of inferotemporal cortex influences face categorization</article-title><source>Nature</source><volume>442</volume><fpage>692</fpage><lpage>695</lpage><pub-id pub-id-type="doi">10.1038/nature04982</pub-id><pub-id pub-id-type="pmid">16878143</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afraz</surname> <given-names>A</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Optogenetic and pharmacological suppression of spatial clusters of face neurons reveal their causal role in face gender discrimination</article-title><source>PNAS</source><volume>112</volume><fpage>6730</fpage><lpage>6735</lpage><pub-id pub-id-type="doi">10.1073/pnas.1423328112</pub-id><pub-id pub-id-type="pmid">25953336</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aguirre</surname> <given-names>GK</given-names></name><name><surname>Singh</surname> <given-names>R</given-names></name><name><surname>D'Esposito</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Stimulus inversion and the responses of face and object-sensitive cortical Areas</article-title><source>NeuroReport</source><volume>10</volume><fpage>189</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1097/00001756-199901180-00036</pub-id><pub-id pub-id-type="pmid">10094160</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Schade</surname> <given-names>PF</given-names></name><name><surname>Vincent</surname> <given-names>JL</given-names></name><name><surname>Ponce</surname> <given-names>CR</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Seeing faces is necessary for face-domain formation</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1404</fpage><lpage>1412</lpage><pub-id pub-id-type="doi">10.1038/nn.4635</pub-id><pub-id pub-id-type="pmid">28869581</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Schade</surname> <given-names>PF</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Universal mechanisms and the development of the face network: what you see is what you get</article-title><source>Annual Review of Vision Science</source><volume>5</volume><fpage>341</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091718-014917</pub-id><pub-id pub-id-type="pmid">31226011</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A hierarchical, retinotopic proto-organization of the primate visual system at birth</article-title><source>eLife</source><volume>6</volume><elocation-id>e26196</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26196</pub-id><pub-id pub-id-type="pmid">28671063</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname> <given-names>JF</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name><name><surname>Allman</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Visual response properties of neurons in four extrastriate visual Areas of the owl monkey (Aotus trivirgatus): a quantitative comparison of medial, dorsomedial, dorsolateral, and middle temporal Areas</article-title><source>Journal of Neurophysiology</source><volume>45</volume><fpage>397</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1152/jn.1981.45.3.397</pub-id><pub-id pub-id-type="pmid">7218008</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Visual objects in context</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>617</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1038/nrn1476</pub-id><pub-id pub-id-type="pmid">15263892</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name><name><surname>Ullman</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Spatial context in recognition</article-title><source>Perception</source><volume>25</volume><fpage>343</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1068/p250343</pub-id><pub-id pub-id-type="pmid">8804097</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname> <given-names>AH</given-names></name><name><surname>Hadj-Bouziane</surname> <given-names>F</given-names></name><name><surname>Frihauf</surname> <given-names>JB</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Object representations in the temporal cortex of monkeys and humans as revealed by functional magnetic resonance imaging</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>688</fpage><lpage>700</lpage><pub-id pub-id-type="doi">10.1152/jn.90657.2008</pub-id><pub-id pub-id-type="pmid">19052111</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname> <given-names>AH</given-names></name><name><surname>Malecek</surname> <given-names>NJ</given-names></name><name><surname>Morin</surname> <given-names>EL</given-names></name><name><surname>Hadj-Bouziane</surname> <given-names>F</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Relationship between functional magnetic resonance imaging-identified regions and neuronal category selectivity</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>12229</fpage><lpage>12240</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5865-10.2011</pub-id><pub-id pub-id-type="pmid">21865466</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Face recognition by monkeys: absence of an inversion effect</article-title><source>Neuropsychologia</source><volume>20</volume><fpage>515</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(82)90025-2</pub-id><pub-id pub-id-type="pmid">7145077</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>L</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The code for facial identity in the primate brain</article-title><source>Cell</source><volume>169</volume><fpage>1013</fpage><lpage>1028</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.05.011</pub-id><pub-id pub-id-type="pmid">28575666</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname> <given-names>LL</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name><name><surname>Lalonde</surname> <given-names>FM</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Faces, animals, and animals with obscured faces elicit similar fMRI activation in the ventral object vision pathway</article-title><source>NeuroImage</source><volume>7</volume><elocation-id>S350</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(18)31183-2</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname> <given-names>D</given-names></name><name><surname>Meyers</surname> <given-names>E</given-names></name><name><surname>Sinha</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Contextually evoked object-specific responses in human visual cortex</article-title><source>Science</source><volume>304</volume><fpage>115</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1126/science.1093110</pub-id><pub-id pub-id-type="pmid">15001712</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dailey</surname> <given-names>MN</given-names></name><name><surname>Cottrell</surname> <given-names>GW</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Organization of face and object recognition in modular neural network models</article-title><source>Neural Networks</source><volume>12</volume><fpage>1053</fpage><lpage>1074</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(99)00050-7</pub-id><pub-id pub-id-type="pmid">12662645</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname> <given-names>PE</given-names></name><name><surname>Chan</surname> <given-names>AW</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Dodds</surname> <given-names>CM</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Domain specificity in visual cortex</article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>1453</fpage><lpage>1461</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj086</pub-id><pub-id pub-id-type="pmid">16339084</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname> <given-names>C</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Whole-agent selectivity within the macaque face-processing system</article-title><source>PNAS</source><volume>112</volume><fpage>14717</fpage><lpage>14722</lpage><pub-id pub-id-type="doi">10.1073/pnas.1512378112</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A face feature space in the macaque temporal lobe</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1187</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1038/nn.2363</pub-id><pub-id pub-id-type="pmid">19668199</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title><source>Science</source><volume>330</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1194908</pub-id><pub-id pub-id-type="pmid">21051642</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greene</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Statistics of high-level scene context</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>777</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00777</pub-id><pub-id pub-id-type="pmid">24194723</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grimaldi</surname> <given-names>P</given-names></name><name><surname>Saleem</surname> <given-names>KS</given-names></name><name><surname>Tsao</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Anatomical connections of the functionally defined &quot;Face Patches&quot; in the Macaque Monkey</article-title><source>Neuron</source><volume>90</volume><fpage>1325</fpage><lpage>1342</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.009</pub-id><pub-id pub-id-type="pmid">27263973</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Levy</surname> <given-names>I</given-names></name><name><surname>Behrmann</surname> <given-names>M</given-names></name><name><surname>Hendler</surname> <given-names>T</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Eccentricity Bias as an organizing principle for human high-order object Areas</article-title><source>Neuron</source><volume>34</volume><fpage>479</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00662-1</pub-id><pub-id pub-id-type="pmid">11988177</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Clark</surname> <given-names>VP</given-names></name><name><surname>Schouten</surname> <given-names>JL</given-names></name><name><surname>Hoffman</surname> <given-names>EA</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The effect of face inversion on activity in human neural systems for face and object perception</article-title><source>Neuron</source><volume>22</volume><fpage>189</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80690-X</pub-id><pub-id pub-id-type="pmid">10027301</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>Majaj</surname> <given-names>NJ</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Explicit information for category-orthogonal object properties increases along the ventral stream</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>613</fpage><lpage>622</lpage><pub-id pub-id-type="doi">10.1038/nn.4247</pub-id><pub-id pub-id-type="pmid">26900926</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issa</surname> <given-names>EB</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Precedence of the eye region in neural processing of faces</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>16666</fpage><lpage>16682</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2391-12.2012</pub-id><pub-id pub-id-type="pmid">23175821</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Strnad</surname> <given-names>L</given-names></name><name><surname>Seidl</surname> <given-names>KN</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Whole person-evoked fMRI activity patterns in human fusiform gyrus are accurately modeled by a linear combination of face- and body-evoked activity patterns</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>82</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1152/jn.00371.2013</pub-id><pub-id pub-id-type="pmid">24108794</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname> <given-names>N</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name><name><surname>Nakayama</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The effect of face inversion on the human fusiform face area</article-title><source>Cognition</source><volume>68</volume><fpage>B1</fpage><lpage>B11</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(98)00035-3</pub-id><pub-id pub-id-type="pmid">9775518</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname> <given-names>N</given-names></name><name><surname>Stanley</surname> <given-names>D</given-names></name><name><surname>Harris</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The fusiform face area is selective for faces not animals</article-title><source>NeuroReport</source><volume>10</volume><fpage>183</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1097/00001756-199901180-00035</pub-id><pub-id pub-id-type="pmid">10094159</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name><name><surname>Mirpour</surname> <given-names>K</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Object category structure in response patterns of neuronal population in monkey inferior temporal cortex</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>4296</fpage><lpage>4309</lpage><pub-id pub-id-type="doi">10.1152/jn.00024.2007</pub-id><pub-id pub-id-type="pmid">17428910</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>ImageNet Classification with Deep Convolutional Neural Networks</article-title><conf-name>25th International Conference on Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leehey</surname> <given-names>S</given-names></name><name><surname>Carey</surname> <given-names>S</given-names></name><name><surname>Diamond</surname> <given-names>R</given-names></name><name><surname>Cahn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Upright and inverted faces: the right hemisphere knows the difference</article-title><source>Cortex</source><volume>14</volume><fpage>411</fpage><lpage>419</lpage><pub-id pub-id-type="doi">10.1016/S0010-9452(78)80067-7</pub-id><pub-id pub-id-type="pmid">710151</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leite</surname> <given-names>FP</given-names></name><name><surname>Tsao</surname> <given-names>D</given-names></name><name><surname>Vanduffel</surname> <given-names>W</given-names></name><name><surname>Fize</surname> <given-names>D</given-names></name><name><surname>Sasaki</surname> <given-names>Y</given-names></name><name><surname>Wald</surname> <given-names>LL</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Kwong</surname> <given-names>KK</given-names></name><name><surname>Orban</surname> <given-names>GA</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Mandeville</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Repeated fMRI using iron oxide contrast agent in awake, behaving macaques at 3 tesla</article-title><source>NeuroImage</source><volume>16</volume><fpage>283</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1110</pub-id><pub-id pub-id-type="pmid">12030817</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname> <given-names>DA</given-names></name><name><surname>Bondar</surname> <given-names>IV</given-names></name><name><surname>Giese</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Norm-based face encoding by single neurons in the monkey inferotemporal cortex</article-title><source>Nature</source><volume>442</volume><fpage>572</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1038/nature04951</pub-id><pub-id pub-id-type="pmid">16862123</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meeren</surname> <given-names>HK</given-names></name><name><surname>van Heijnsbergen</surname> <given-names>CC</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Rapid perceptual integration of facial expression and emotional body language</article-title><source>PNAS</source><volume>102</volume><fpage>16518</fpage><lpage>16523</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507650102</pub-id><pub-id pub-id-type="pmid">16260734</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Patches with links: a unified system for processing faces in the macaque temporal lobe</article-title><source>Science</source><volume>320</volume><fpage>1355</fpage><lpage>1359</lpage><pub-id pub-id-type="doi">10.1126/science.1157436</pub-id><pub-id pub-id-type="pmid">18535247</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Crapse</surname> <given-names>T</given-names></name><name><surname>Chang</surname> <given-names>L</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The effect of face patch microstimulation on perception of faces and objects</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>743</fpage><lpage>752</lpage><pub-id pub-id-type="doi">10.1038/nn.4527</pub-id><pub-id pub-id-type="pmid">28288127</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op De Beeck</surname> <given-names>H</given-names></name><name><surname>Vogels</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Spatial sensitivity of macaque inferior temporal neurons</article-title><source>The Journal of Comparative Neurology</source><volume>426</volume><fpage>505</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1002/1096-9861(20001030)426:4&lt;505::AID-CNE1&gt;3.0.CO;2-M</pub-id><pub-id pub-id-type="pmid">11027395</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Wiggett</surname> <given-names>AJ</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Patterns of fMRI activity dissociate overlapping functional brain Areas that respond to biological motion</article-title><source>Neuron</source><volume>49</volume><fpage>815</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.02.004</pub-id><pub-id pub-id-type="pmid">16543130</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Selectivity for the human body in the fusiform gyrus</article-title><source>Journal of Neurophysiology</source><volume>93</volume><fpage>603</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1152/jn.00513.2004</pub-id><pub-id pub-id-type="pmid">15295012</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinsk</surname> <given-names>MA</given-names></name><name><surname>Arcaro</surname> <given-names>M</given-names></name><name><surname>Weiner</surname> <given-names>KS</given-names></name><name><surname>Kalkus</surname> <given-names>JF</given-names></name><name><surname>Inati</surname> <given-names>SJ</given-names></name><name><surname>Gross</surname> <given-names>CG</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural representations of faces and body parts in macaque and human cortex: a comparative FMRI study</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>2581</fpage><lpage>2600</lpage><pub-id pub-id-type="doi">10.1152/jn.91198.2008</pub-id><pub-id pub-id-type="pmid">19225169</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponce</surname> <given-names>CR</given-names></name><name><surname>Xiao</surname> <given-names>W</given-names></name><name><surname>Schade</surname> <given-names>PF</given-names></name><name><surname>Hartmann</surname> <given-names>TS</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evolving images for visual neurons using a deep generative network reveals coding principles and neuronal preferences</article-title><source>Cell</source><volume>177</volume><fpage>999</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.04.005</pub-id><pub-id pub-id-type="pmid">31051108</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popivanov</surname> <given-names>ID</given-names></name><name><surname>Jastorff</surname> <given-names>J</given-names></name><name><surname>Vanduffel</surname> <given-names>W</given-names></name><name><surname>Vogels</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Tolerance of macaque middle STS body patch neurons to shape-preserving stimulus transformations</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>1001</fpage><lpage>1016</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00762</pub-id><pub-id pub-id-type="pmid">25390202</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rice</surname> <given-names>A</given-names></name><name><surname>Phillips</surname> <given-names>PJ</given-names></name><name><surname>Natu</surname> <given-names>V</given-names></name><name><surname>An</surname> <given-names>X</given-names></name><name><surname>O'Toole</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Unaware person recognition from the body when face identification fails</article-title><source>Psychological Science</source><volume>24</volume><fpage>2235</fpage><lpage>2243</lpage><pub-id pub-id-type="doi">10.1177/0956797613492986</pub-id><pub-id pub-id-type="pmid">24068115</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenfeld</surname> <given-names>SA</given-names></name><name><surname>Van Hoesen</surname> <given-names>GW</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Face recognition in the rhesus monkey</article-title><source>Neuropsychologia</source><volume>17</volume><fpage>503</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(79)90057-5</pub-id><pub-id pub-id-type="pmid">117394</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarzlose</surname> <given-names>RF</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Separate face and body selectivity on the fusiform gyrus</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>11055</fpage><lpage>11059</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2621-05.2005</pub-id><pub-id pub-id-type="pmid">16306418</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taubert</surname> <given-names>J</given-names></name><name><surname>Van Belle</surname> <given-names>G</given-names></name><name><surname>Vanduffel</surname> <given-names>W</given-names></name><name><surname>Rossion</surname> <given-names>B</given-names></name><name><surname>Vogels</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The effect of face inversion for neurons inside and outside fMRI-defined face-selective cortical regions</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>1644</fpage><lpage>1655</lpage><pub-id pub-id-type="doi">10.1152/jn.00700.2014</pub-id><pub-id pub-id-type="pmid">25520434</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A cortical region consisting entirely of face-selective cells</article-title><source>Science</source><volume>311</volume><fpage>670</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1126/science.1119983</pub-id><pub-id pub-id-type="pmid">16456083</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanduffel</surname> <given-names>W</given-names></name><name><surname>Fize</surname> <given-names>D</given-names></name><name><surname>Mandeville</surname> <given-names>JB</given-names></name><name><surname>Nelissen</surname> <given-names>K</given-names></name><name><surname>Van Hecke</surname> <given-names>P</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Orban</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Visual motion processing investigated using contrast agent-enhanced fMRI in awake behaving monkeys</article-title><source>Neuron</source><volume>32</volume><fpage>565</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(01)00502-5</pub-id><pub-id pub-id-type="pmid">11719199</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname> <given-names>RK</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Looking at upside-down faces</article-title><source>Journal of Experimental Psychology</source><volume>81</volume><fpage>141</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1037/h0027474</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53798.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Pasternak</surname><given-names>Tatiana</given-names></name><role>Reviewing Editor</role><aff><institution>University of Rochester</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The finding that the representation of faces in inferotemporal cortex depends on the location of the animal's fixation and is context specific was judged as novel and exciting. The elegant and unique design of the study allowed the authors to show that face-selective neurons respond not just to faces in isolation but also to parts of an image that do not contain a face but should do so, based on the context (proximity to the body). The work provided convincing evidence that the representation of complex objects in the brain reflects statistical regularities of the experienced environment.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;The neurons that mistook a hat for a face&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The authors report a series of recording experiments in macaque face patches (PL and ML) that examined the response of face responsive neuronal sites to natural images that contain bodies but with the faces occluded or obscured. Although the faces were absent or only partially visible, the face patch ML neurons tended to respond at the image location where the face was supposed to be. They conclude that face neurons of the face patch ML are sensitive to contextual influences and experienced environmental regularities. This paper reports novel data on the response properties of face neurons in face patch ML: responses to the location of a face when the latter is absent or occluded by another object but suggested by the context of a body. The experimental paradigm is also quite interesting and unique in this field: presenting a complex natural image at different locations and then analyzing the response as a function of the location or hotspot of the receptive field of the neuron as measured with a single face stimulus.</p><p>Essential revisions:</p><p>The manuscript was well received by both reviewers who found the work exciting and the results novel. However, they raised a number of issues, mainly concerned with the need for more detailed description of the data to give the readers a more rigorous impression of the observed effects. To achieve this goal, the paper needs better quantification of the prevalence of the effect and its presence in individual neurons in the recorded population. To help the authors, the reviewers provided a detailed list of questions, requests and suggestions, which must be addressed before the manuscript will be consider for publication in <italic>eLife</italic>. These are summarized below.</p><p><italic>Reviewer 1:</italic></p><p>1) Please clarify the data in Figures 2 and 3 by providing cell numbers for each plot and for each recorded region. In addition, provide information about stimulus selectivity of the recorded neurons and discuss the relationship between this selectivity and responses to faces shown in these figures.</p><p>2) Were the RFs showing responses to images in Figure 1—figure supplement 1, also mapped with small dots?</p><p>3) Please clarify the composite body orientation maps shown in Figure 6.</p><p><italic>Reviewer 2:</italic></p><p>1) Please discuss some of the discrepancies between the data and their description in the text outlined in detail by this reviewer. Also, please address the question whether scaling maps to the minimum and maximum response for each image prevents detecting responses at other locations. Also, in Figures 5 and 6B please show maps for the images, in addition to the PSTHs.</p><p>2) Differences in activity between different recording sites need to be discussed. A related question is the definition of non-face regions. This reviewer makes specific suggestions concerning Figure S6, which you may find helpful in dealing with the problem.</p><p>3) Please discuss responses to the legs for inverted bodies shown in Figure 6 and whether these multi-unit responses reflect mixed selectivity with the recorded patch. Showing responses of well-isolated units would help addressing this possibility.</p><p>4) Long latencies of responses to occluded head images cast doubt on the interpretation proposed in the Discussion that rules out top-down explanation of the effect. In addition, the proposed role of experience is hypothetical and should be presented as such.</p><p>5) This reviewer also made several points that should be addressed in the body of the paper or in the Discussion. These include, clarifications of mapping RFs with dots and the possibility that their size was underestimated, dealing with negative response values when computing the index, enlarging the size of Figures 4 and S6 for better visibility and correction of the legend for Figure 4.</p><p>6) Please comment on the validity of using parametric t-tests to evaluate the effects and whether the assumptions underlying these tests are fulfilled.</p><p><italic>Reviewer #1:</italic></p><p>This study demonstrates that neurons in two macaque face patches, ML and PL, respond in a retinopically specific way to face-related features of a scene. Surprisingly, these face-related responses are often elicited even when the basic structural features of a face are occluded, such that only the bodily cues indirectly indicate the presence of a face in the neuron's response field.</p><p>1) This is a very exciting study that has makes two important points. The first, which is not featured, perhaps because it is &quot;sort of&quot; known, is that for neurons in these face patches the animal's fixation position is extremely important. Responses to faces or &quot;unseen&quot; faces are wholly contingent on the face being present on a limited portion of the retina. I thought this aspect of the study is very important for a community that does not typically think about &quot;face cells&quot; in this way. The second aspect, which is featured more strongly, is that neurons respond in this retinotopically specific way to occluded faces. This quite a remarkable finding. The authors do some interesting extensions and controls, including showing a surprising lack of influence by adding the body to the visible face. This study provides much food for thought.</p><p>2) My main criticism of the paper is that I don't have a clear picture of how representative the featured effect is. Figures 2 and 3 show heat maps, and refer to &quot;maps of cells&quot;. Are these population maps (as in the bottom panels of Figure 1—figure supplement 3), or are they selected responses of individual cells? There is a statement early on that everything is population analysis, but the authors need to do a better job explaining how many cells go into each of the plots, since the reader needs to understand to what extent this behavior is typical of neurons in ML and PL. Having only one PL recording site also significantly weakens this aspect.</p><p>3) Relatedly, since neurons differ in their specific preferences, it's surprising that there is minimal discussion of this. The authors should do more to provide information about how stimulus selectivity (aside from positional selectivity) interacts with their main findings.</p><p><italic>Reviewer #2:</italic></p><p>The authors report a series of recording experiments in macaque face patches (PL and ML) that examined the response of face responsive neuronal sites to natural images that contain bodies but with the faces occluded or obscured. Although the faces were absent or only partially visible, the face patch ML neurons tended to respond at the image location where the face was supposed to be. They conclude that face neurons of the face patch ML are sensitive to contextual influences and experienced environmental regularities.</p><p>This paper reports novel data on the response properties of face neurons in face patch ML: responses to the location of a face when the latter is absent or occluded by another object but suggested by the context of a body. The experimental paradigm is also quite interesting and unique in this field: presenting a complex natural image at different locations and then analyzing the response as a function of the location or hotspot of the receptive field of the neuron as measured with a single face stimulus. The main result agrees with previous fMRI studies in human and monkeys that showed responses in face areas / patches to blurred faces on top of a body, but the present study examines this in some more detail at the spiking activity level in the posterior face patches.</p><p>In general, I found this a highly interesting and exciting paper but I have some concerns and comments that need to be addressed, in particular with respect to the presentation and interpretation of these remarkable data.</p><p>1) The results are not as clear-cut as the impression one gets from reading the text of the paper. Examples of such issues are the following. Figure 2: Monkey 1 ML: second column image (human carrying a box): in the occluded condition there is also strong activation in the top right corner where the box is. In Figure 3, third and fifth column: the location of the masked face and the hotspot of neural activity do not match well in the data of M2. Furthermore, for the 7th column image of Figure 3 (3 persons carrying boxes) there is a clear hotspot in both monkeys only for the rightmost (third) person: why not for the closer other two persons? In the 9th column image, there is an additional hotspot in the data of M2 on the body below the covered head. Also striking is the variability in effects between images and monkeys. This is apparent in the data of Figure 4: second column (little response for ball covering the head in M1), fourth column (no or little response to non-face object on face), fifth column (no response to covered face in M1). Also, in general the data of M2 appear to be more &quot;smeared out&quot; compared to those of M1: is that related to RF size differences, neuron-to-neuron variability or the mapping with larger steps in M2? The authors should discuss these discrepancies between the data and what they describe in the text in detail, acknowledging that these cells do not always respond at the location of the occluded head or sometimes even not close to the occluded head. One related problem concerning the data presentation is that the maps are scaled to the minimum and maximum response for each image. I wonder how the maps will look like when shown across the full possible range of net responses for a site (i.e. from 0 net response to maximum response): was there no response at other locations than (or close to) the (occluded) head/face? Also, related to this point, the authors should show the maps for the images of Figure 5 and 6B – not just the PSTHs (see next main point). In other words, these interesting data should be presented with greater detail.</p><p>2) The authors show only averages across electrodes per animal (except for the responses to unoccluded faces (Figure S3)) and one wonders how strong the differences are between the different neuron sites. To remedy this, the authors can show the maps for each image in Figure S6 (but please make the images larger) so that the reader can assess the (or lack of) consistency of the responses for the different images. The PSTHs in the current figures are averages across the images so the presented confidence intervals give some indication of the variability for the different images. However, what was unclear to me is how the authors defined the non-face region? They should indicate the latter in Figure S6 and describe in detail how they define the non-face region: centered on an object, equated for contrast/luminance with the face/occluded face, same area, etc? Choosing a valid non-face region does not seem trivial to me for these complex images, and such a choice may affect the responses shown in the PSTHs. Finally, the authors can compute for each site and image an index contrasting the net response to the face and control, non-face region and show the distribution of such indices for each experiment.</p><p>3) In the experiment of Figure 5, the authors compare the responses to a modified face alone and the modified face on top of a body and find stronger responses when the body is present. However, they should have as a control condition also the body without face, otherwise the increased response could have been due to a (weak) response to the body itself (summation of obscured face and body response). The absence of such summation effect for full faces could be due to a ceiling effect, as suggested by the authors.</p><p>4) The strong responses to the legs for the inverted body in Figure 6 is surprising to me. It clearly shows that the responses of these neurons to body stimuli without a head are more complex than just a response to an occluded face or absent head. The authors suggest that this is related to responses to short contours in the face cells. But if that is the case, can it not explain also the responses to the rectangular shapes (with also short contours) occluding the head in the images of Figure 3? Also, when it is related to short contours, why not then comparable strong responses to the images with a head (upper row of Figure 6)? It is clear to me that we do not understand the responses to these neurons. Also, how consistent was this leg response across the different sites? It also makes one wonder whether some of these &quot;weird&quot; responses are because the recordings are multi-unit, which will pool the responses of different neurons, some of which may not be face selective. In other words, it could be that the single neurons that respond to the legs are not face selective at all but mixed with face selective neurons in this patch. It would have been informative to show the responses of well-isolated single units in this case (and also in the other experiments).</p><p>5) The authors suggest in the Discussion that the responses to the occluded head are not due merely to expectation. However, the latencies of the responses to the occluded head appear to be longer than to a real face (and, in some cases, the responses were (as for the non-face) preceded by inhibition (Figure 2, 3, 4 and 5 in M1)). Thus I feel one cannot rule out an expectation or top-down explanation of the effects.</p><p>6) That the responses to the occluded heads result from experience makes sense but is speculation and not demonstrated in the present experiments. This should be made explicit.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your article &quot;The neurons that mistook a hat for a face&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: David Leopold (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>Summary:</p><p>The revised manuscript addressed many of the issues raised in the initial review. However, one of the reviewers raised additional concerns and made several suggestions that must be addressed for the manuscript to be accepted for publication. These are listed below.</p><p>Essential revisions:</p><p>1) Please provide the distribution of indices to allow the reader better assess the variability of the data.</p><p>2) Please create an additional figure that compares directly PSTHs recorded in the ML and PL face patches (see reviewer 2 comment #3)</p><p>3) The claim that the changes in face-body representations at successive layers of Alexnet simulations result from learning, requires additional analysis of category representations in an untrained Alexnet using random weights.</p><p>4) Please tone down the interpretation of the importance of the eyes when referring to Figure 2 in the second paragraph of the Results section.</p><p><italic>Reviewer #1:</italic></p><p>The authors have done an outstanding job addressing my comments, clarifying a few bits and then showing many single-cell examples of this striking phenomenon in the supplementary material. There are many important aspects of this paper beyond the headline, including the spatial specificity of face selective neurons, as well as the similar but delayed responses in ML compared to PL. I'm happy to recommend publication.</p><p><italic>Reviewer #2:</italic></p><p>The authors performed an extensive revision, with new interesting data and modelling. They responded to all my concerns and overall I am happy with their revision. Nonetheless, I still have a couple of concerns that should be addressed by the authors.</p><p>1) I understand that it is not possible to show the responses of all units to all stimuli, given the size of the data. I asked to show distribution of indices in order to get an idea about the across image and unit variability but the authors argue that this is not practical because of the variability in response latency amongst images and units. Such argument can always be made and then one should refrain from using indices (but the authors compute face and body indices). I think that when one uses net responses to compute indices, the variability in latency should not be a main cause of variance among indices of different neurons or images. One can employ an analysis window that captures the population response, computed across images and neurons, per experiment. The confidence intervals that are provided now depend on the number of observations; although they show the consistency of the mean population response, the CIs are not a straightforward metric of the variability among sites or images. Also, it is not clear whether the individual site examples shown in the figure supplements are representative of the observed diversity of response profiles. Hence, I still believe that distributions of indices will help to appreciate the variability among images and sites.</p><p>2) The authors provide novel data on the responses to images in which the eye is invisible. However, in the images shown in Figure 2, either more than the eyes are removed (top image) and it is unclear how these other changes affected the response to the disfigured face, or, the eyes are occluded by a visible occluder (bottom image). The latter is known to increase the response latency of IT neurons. To assess whether removing the eye from the image affects the response, it would have been better to present a face without eyes, the eye region being filled in with the neighboring texture, so that no additional changes (holes or bar occluder) are present. With the current images, I believe the authors should be more careful in interpreting the response and latency differences between the full face and the images in which the eyes were removed or occluded.</p><p>3) The authors suggest that the responses to the occluded faces do not result from top-down processing, using as argument that the response latency differences between ML and PL remain for the occluded faces, which is a reasonable argument. However, PL and ML responses are compared directly only for the intact faces and the &quot;invisible eye&quot; images, but not for the face occlusion conditions. One way to show this is to compare in one figure the population PSTHs between ML and PL for the different conditions.</p><p>4) They write, when describing the novel and interesting Alexnet simulations, that &quot;The Euclidean distances between image pairs within each category were smaller than between categories, indicating that images were more homogeneous within than between categories and highlighting that simple shape is a major component of these commonly tested &quot;semantic&quot; categories&quot;. However, the images also differ in texture and it is possible that not only shape but also texture is driving the category differences in Alexnet. In fact, some recent studies stressed the importance of texture cues for categorization in deep convolutional neural networks.</p><p>5) The authors employ the Alexnet simulation to demonstrate that representations of faces and bodies can merge over learning. However, to show that learning is critical they should perform a similar analysis of the category representations in an untrained Alexnet, e.g. with random weights.</p><p>6) The authors write &quot;that convolutional networks can capture the representational structure of biological networks.&quot; It all depends on what is meant by &quot;capture&quot; here, but there is still a discrepancy between IT and classical feedforward deep net (like Alexnet) representations, e.g. see recent work by DiCarlo's group. I suggest to tone down this statement on the correspondence between deep nets and ventral stream representations.</p><p>7) Can the authors explain why they permute the labels of the non-face images when computing the between face, non-face correlations (inset; Figure 8B); permutation of the non-face labels in this case should not make a difference, or perhaps I am missing something here.</p><p>8) The authors should provide the explained variance of the 2D MDS configuration shown in their novel Figure 8A, so that the reader can appreciate how the distances plotted in 2D correspond to the original distances in the high-dimensional feature space.</p><p>9) The authors write in their Discussion &quot;More broadly, these results highlight.… that the facilitation of face-selective neurons in the absence of a clear face reflects a lifetime of experience that bodies are usually accompanied by faces.&quot;. As I noted before, there is no evidence yet that the contextual effects reported by the authors do result from experience during the lifetime. Please rephrase.</p><p>10) Figure 1—figure supplement 1: the authors should indicate the t-values (with the corrected p = 0.05 threshold) of the face-object contrast in the fMRI maps.</p><p>11) Figure 1—figure supplement 3: can the authors explain the above-baseline responses in M1, ML before 50 ms after stimulus onset – one would expect that soon after stimulus onset responses equal to baseline because of the response latency of the neurons.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53798.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The manuscript was well received by both reviewers who found the work exciting and the results novel. However, they raised a number of issues, mainly concerned with the need for more detailed description of the data to give the readers a more rigorous impression of the observed effects. To achieve this goal, the paper needs better quantification of the prevalence of the effect and its presence in individual neurons in the recorded population. To help the authors, the reviewers provided a detailed list of questions, requests and suggestions, which must be addressed before the manuscript will be consider for publication in eLife. These are summarized below.</p></disp-quote><p>We thank the reviewers for their thorough reviews and positive assessment. We provide a point-by-point response to each of the reviewers’ comments. In summary, we made the following revisions to our manuscript:</p><p>1) We have made extensive edits to the text to address reviewer comments.</p><p>2) We provide additional quantifications of response consistencies and statistical analyses as requested.</p><p>3) We have added 12 additional supplementary figures (Figure 3—figure supplements 1-3, Figure 4—figure supplements 1-3, Figure 5—figure supplements 1-3, Figure 6—figure supplements 1-2, and Figure 7—figure supplement 2) that show more individual channel and population-level responses maps to images reported in the main text figures.</p><p>4) Using data we previously collected, but had not reported, we have added 1 new main text figure (Figure 2) and 3 additional supplementary figures (Figure 1—figure supplement 6, Figure 2—figure supplement 1, and Figure 6—figure supplement 3) that show response maps to additional images that illustrate the selectivity and latency of responses.</p><p>5) We have added a new analysis of category representations using an artificial neural network (Figure 8).</p><p>6) We have added a new supplementary figure illustrating the localization of the arrays to the face patches in IT.</p><disp-quote content-type="editor-comment"><p>Reviewer 1:</p><p>1) Please clarify the data in Figures 2 and 3 by providing cell numbers for each plot and for each recorded region. In addition, provide information about stimulus selectivity of the recorded neurons and discuss the relationship between this selectivity and responses to faces shown in these figures.</p></disp-quote><p>Data in Figures 3 and 4 (previously Figures 2 and 3) are population activity (averaged across all responsive channels within an array). We have clarified this in the figure legends as well as in Figures 5 and 7. Previously, we showed individual channel data from Monkey 1 and Monkey 2 only for Figure 1 in Figure 1—figure supplement 4. Now, we include supplementary figures that show 3 individual channels each for Monkey 1 and Monkey 2 for the maps in Figure 3 (Figure 3—figure supplement 3), Figure 4 (Figure 4—figure supplement 3), and Figure 5 (Figure 5—figure supplement 3). To better convey the consistency of responses across channels, we now also report the average Pearson correlation between each channel and the n-1 population average (i.e. leaving out that channel) for data in Figures 3 – 5.</p><disp-quote content-type="editor-comment"><p>2) Were the RFs showing responses to images in Figure 1—figure supplement 1, also mapped with small dots?</p></disp-quote><p>We apologize for this confusion. That was a mistake in the text. The RFs in Figure 1—figure supplement 2 (formerly Figure 1—figure supplement 1) were mapped with 2 degree faces and round non-face images. We now show examples of the faces and round non-face images in Figure 1—figure supplement 2 and report the mean and standard deviations of RF sizes across channels for each array in the legend. In a separate mapping session (not reported in manuscript) we did perform RF mapping in Monkey M1 using face features such as eyes and simplified versions of eyes (small dots and small concentric circles). Similar to the mapping with 2º faces and objects, the centers of those RFs were within the central visual field, but the RFs were even smaller (mean RF 1.41 +/- 0.24 STD for Monkey 1 ML). The RF centers were in similar locations using these stimuli. We did not perform this mapping with face features in monkey M2. For the purposes of our complex natural image recordings, only the center of the RF is critical for the analysis. If anything, larger RFs would reduce the resolution of focal activity and minimize the effects observed in our main experiments. The fact that activity was focal to faces in natural images demonstrates that the RFs were not so large as to be a major constraint on our results.</p><disp-quote content-type="editor-comment"><p>3) Please clarify the composite body orientation maps shown in Figure 6.</p></disp-quote><p>The composite orientation maps serve as summary data illustrating the best body orientation at each grid point. We feel this is important to include given the variability in response magnitudes across both body orientations and spatial location. This is especially true for upright vs. inverted headless bodies as shown more clearly in Figure 7B. Because of this, one cannot look at any individual orientation to know what the preferred tuning is for that point in space. As requested by the reviewers, we have made the color scale and corresponding body orientation images on the right side of Figure 7A (formerly Figure 6) larger to improve readability. We also changed the colormap for the composite orientation images to make them more distinct from the response maps so readers would be less likely to confuse those data for firing rates. We have included the following explanation of these composite maps in the Results:</p><p>“The magnitude of firing rate varied across the tested visual field location (e.g., firing rates tended to be higher above the upper half of the image). Composite maps show the preferred body orientation for each spatial position tested (Figure 7A, right side). In experiments both with and without heads, preferred orientation systematically varied such that the preferred body orientation at that spatial location positioned the face, or where the face ought to be, within the RF center.”</p><disp-quote content-type="editor-comment"><p>Reviewer 2:</p><p>1) Please discuss some of the discrepancies between the data and their description in the text outlined in detail by this reviewer.</p></disp-quote><p>Reviewer #2 made several comments noting the variability of the activity maps and appearance of responses to natural scenes that are not clearly driven by the body. We address this above in main comment #1 as well as below with reviewer #2’s specific comments.</p><disp-quote content-type="editor-comment"><p>Also, please address the question whether scaling maps to the minimum and maximum response for each image prevents detecting responses at other locations.</p></disp-quote><p>In our initial submission we chose a min to max response scaling to illustrate the full range of responses. However, large outliers can bias the appearance of response maps scaled between minimum and maximum values. To minimize the influence of outliers and to better center the color scale range to the values for each map, we now show every response map scaled between the 0.5 and 99.5% percentiles of each map. This has the effect of increasing the effective range for the bulk of each response map. We do not think this affects the interpretation of the response maps. We also looked at using 1% to 99% and 2.5 to 97.5% ranges, but found that 0.5% to 99.5% best illustrated the range of values within activity maps. To clarify, the minimum response for Monkey 1 is often slightly below baseline as can be seen in the PSTH responses to non-face regions. For Monkey 2, the minimum response was close to 0 with some images slightly below and others slightly above.</p><p>As the reviewer noted above, for any individual image, there are sometimes hot spots (yellow / red) outside the face area (or where the face ought to be). We are not claiming that these neurons respond only to the face stimuli; see for example Tsao et al., 2006 showing that face cells consistently respond to clocks and round fruits. Indeed, we also see consistent responses particularly to round, tan objects such as cookies. See <xref ref-type="fig" rid="respfig1">Author response image 1</xref>. It seems likely that at least some of these features drive these neurons because they share shape, color, or contrast features with faces. There may be other features that consistently drive these neurons across images. Relatively few studies have systematically tested tuning of face patch neurons to features embedded within naturalistic stimuli. This is certainly an interesting avenue of exploration, but beyond the scope of the current study. Here, we specifically study the effect of bodies positioned below face selective neuron RFs on firing rate</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Response maps from M1 to natural images without faces.</title><p>Hotspots are consistently found over circular shapes and/or above body-life structures.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-resp-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>Also, in Figures 5 and 6B please show maps for the images, in addition to the PSTHs.</p></disp-quote><p>We now show response maps for each stimulus in Figures 6 and 7B (previously Figures 5 and 6B). See Figure 6—figure supplements 1 and 2 and Figure 7—figure supplement 2.</p><disp-quote content-type="editor-comment"><p>2) Differences in activity between different recording sites need to be discussed. A related question is the definition of non-face regions. This reviewer makes specific suggestions concerning Figure S6, which you may find helpful in dealing with the problem.</p></disp-quote><p>The response selectivity across the arrays in both animals was consistent. In Figure 1—figure supplement 4, we provided a measure of the similarity (Pearson correlations) between each channel and the n-1 population average for responses to natural images with faces. We now provide examples of individual channels for the population-level response maps shown in Figure 3 (Figure 3—figure supplement 3), Figure 4 (Figure 4—figure supplement 3), and Figure 5 (Figure 5—figure supplement 3). To better convey the consistency of responses across channels, we now also report the average Pearson correlation between each channel and the n-1 population average for data in Figures 3– 5.</p><p>We did not see mixed selectivity in any of the arrays. We now provide indices for bodies vs. objects and faces vs. all other images for each responsive channel in all 3 recording sites in Figure 1—figure supplement 3. We find little to no body selectivity. For the experiments reported in Figures 3-5, we also now provide the mean correlation and variance between the spatial pattern of response maps for each channel and the n-1 group average within the ML arrays.</p><disp-quote content-type="editor-comment"><p>3) Please discuss responses to the legs for inverted bodies shown in Figure 6 and whether these multi-unit responses reflect mixed selectivity with the recorded patch. Showing responses of well-isolated units would help addressing this possibility.</p></disp-quote><p>We show in Figure 1—figure supplement 3 that these neurons responded to faces, but did not respond to bodies, when the bodies were presented within the receptive field. We find no evidence of mixed selectivity in our multi-unit RSVP data (e.g., strong body or hand selectivity). Reviewer #2 raised the concern that our multi-unit data comprises a predominant pool of face-selective neurons and a weaker pool of body-selective neurons. Such an interpretation is not consistent with our results. First, images of bodies did not significantly activate these neurons when presented within the RF (Figure 1—figure supplement 3). We now provide body-selectivity indices showing that there was little to no body selectivity in these arrays. Across our experiments, hot spots of activity are consistently above the body, not on it. A mixed-selectivity explanation would need the RFs of face-selective neurons to be systematically right above the RFs of body-selective neurons in both monkeys. However, such an account is not consistent with our results reported in Figure 6 (also see new Figure 6—figure supplements 1 and 2 for response maps for every image in this experiment). We show that the response to an intact face alone or above a body is near identical in magnitude. If the face and body responses were separate neuronal populations, these responses would have to summate and the face + body response would need to be larger than the face without body. Therefore, the only parsimonious explanation is that our multi-unit data comprised face-selective neurons that respond to a face presented within the RF or to a body presented below the RF. Upon consideration of the reviewers’ comments, we agree that the link to end-stopping was to speculative. In the results, we now limit our interpretation that the response to feet for inverted headless bodies may be a manifestation of this body-below responsiveness.</p><disp-quote content-type="editor-comment"><p>4) Long latencies of responses to occluded head images cast doubt on the interpretation proposed in the Discussion that rules out top-down explanation of the effect. In addition, the proposed role of experience is hypothetical and should be presented as such.</p></disp-quote><p>We understand the reviewer’s concern regarding longer latencies. We do not agree that long latencies cast doubt on our interpretation, nor that it necessarily suggests a top-down explanation. In fact, the latencies of these neurons can vary simply by changing the size of a face or adding/removing backgrounds. To better illustrate the dependency of latencies on the visual input, we now include two new figures (Figure 2 and Figure 2—figure supplement 1) showing the response latencies of PL and ML simultaneously recorded in Monkey 1 to natural face images with and without eyes. By simply removing the eyes from faces, we see a substantial shift in the latencies for both PL and ML. The ML response latencies for these images without eyes is comparable to the latencies of the masked and occluded faces. Importantly, PL still responds to the eyeless faces with a shorter latency than ML. We do not think this suggests top-down modulation, rather that these neurons are sensitive to the input of a variety of visual features that can manifest at different timescales. For example, we used a stimulus with parts of a face spatially separated (top of Figure 2). For both PL and ML, the response to eyes emerged in time windows earlier than the response to the surrounding face. This is consistent with prior findings in PL by Issa and DiCarlo (JN 2012; Figure 12).</p><p>In the discussion, we propose that contextual representations such as a body below a face can manifest locally within IT through experience. We do not claim that we have shown that experience is necessary but think this is quite plausible. To support this interpretation, we now provide an analysis of category representations using an artificial neural network trained on image classification (Figure 8). We show that representations of faces and bodies in an artificial neural network (AlexNet) trained to classify images into a set of 1000 categories (but not explicitly trained on face, body, or human categorization!) start off separate and merge closer to one another in deep layers of the architecture more than with other categories. We show this with multidimensional scaling visualizations of the distances between images for activations in early and deep layers of AlexNet (Figure 8A). We also show that the overlap in face and body representations is greatest for deep layers (relu6 and relu7) of AlexNet (Figure 8B). This provides some evidence that the regularity of experiencing faces and bodies is sufficient for overlapping representations to emerge in a hierarchical system.</p><disp-quote content-type="editor-comment"><p>5) This reviewer also made several minor points that should be addressed in the body of the paper or in the Discussion. These include, clarifications of mapping RFs with dots and the possibility that their size was underestimated, dealing with negative response values when computing the index, enlarging the size of Figures 4 and S6 for better visibility and correction of the legend for Figure 4.</p></disp-quote><p>We have clarified the issue of RF mapping above and addressed the concern about RF size.</p><p>There were no channels with negative (below baseline) responses to faces or non-face objects in the PL array in Monkey 1. There were no negative face or non-face object responses in ML of either monkey but one channel in each of the ML arrays had a below-baseline response to bodies, so those channels were not used in calculating the average body vs. object indices. We clarify this in the results and Figure 1—figure supplement 3’s legend.</p><disp-quote content-type="editor-comment"><p>6) Please comment on the validity of using parametric t-tests to evaluate the effects and whether the assumptions underlying these tests are fulfilled.</p></disp-quote><p>We acknowledge that a relatively small number of observations (images) went into each test, which can be problematic for parametric tests. To alleviate the reviewer’s concerns, we re-ran all the statistical analyses using non-parametric Wilcoxon tests. The results were nearly identical and did not change any interpretation of the results. We provide in <xref ref-type="fig" rid="respfig2">Author response image 2</xref> an illustration of the parametric vs. non-parametric results (both threshold at p &lt; 0.05 FDR-corrected) for all PSTH graphs with significant results.</p><fig id="respfig2"><label>Author response image 2.</label><caption><title>Comparison of t-test results with non-parametric Wilcoxon signed-rank test.</title><p>Both threshold at p&lt; 0.05 FDR-corrected.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-resp-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>This study demonstrates that neurons in two macaque face patches, ML and PL, respond in a retinopically specific way to face-related features of a scene. Surprisingly, these face-related responses are often elicited even when the basic structural features of a face are occluded, such that only the bodily cues indirectly indicate the presence of a face in the neuron's response field.</p><p>1) This is a very exciting study that has makes two important points. The first, which is not featured, perhaps because it is &quot;sort of&quot; known, is that for neurons in these face patches the animal's fixation position is extremely important. Responses to faces or &quot;unseen&quot; faces are wholly contingent on the face being present on a limited portion of the retina. I thought this aspect of the study is very important for a community that does not typically think about &quot;face cells&quot; in this way. The second aspect, which is featured more strongly, is that neurons respond in this retinotopically specific way to occluded faces. This quite a remarkable finding. The authors do some interesting extensions and controls, including showing a surprising lack of influence by adding the body to the visible face. This study provides much food for thought.</p></disp-quote><p>We thank the reviewer for this encouraging assessment.</p><disp-quote content-type="editor-comment"><p>2) My main criticism of the paper is that I don't have a clear picture of how representative the featured effect is. Figures 2 and 3 show heat maps, and refer to &quot;maps of cells&quot;. Are these population maps (as in the bottom panels of Figure 1—figure supplement 3), or are they selected responses of individual cells? There is a statement early on that everything is population analysis, but the authors need to do a better job explaining how many cells go into each of the plots, since the reader needs to understand to what extent this behavior is typical of neurons in ML and PL. Having only one PL recording site also significantly weakens this aspect.</p></disp-quote><p>We apologize for this confusion. All main text figures show population data – all visually responsive sites in each array. Figure 1—figure supplement 4 showed a few examples of response maps from a single channel for comparison with the population map for Monkeys 1 and 2. We focus on the population data because the selectivity from a RSVP paradigm (Figure 1—figure supplement 3) was similar across channels within each array and the response maps of individual channels (on average) to complex images containing faces were highly similar to the n-1 group average, so therefore the population response maps were representative of consistent activity across individual channels.</p><p>To give a better sense of the individual channel variability, we now include graphs of body- and face-selective indices for each array (Figure 1—figure supplement 3) and include new supplemental figures for Figures 3-5 (previously Figures 2-4) showing response maps for 3 individual channels each for both Monkeys 1 and 2. We also now report the similarity of response maps across channels within each array for the experiments in Figures 3-5.</p><p>We agree that only having a single PL recording array from one monkey limits what we can conclude about PL in general. Overall face-selectivity and the responses to occluded faces were weaker in PL so we focused on ML. We still feel that it is useful to report our PL results here, given that in this monkey we recorded simultaneously from PL and ML (which to our knowledge has not been reported previously). To make better use of the PL data, we now include a figure illustrating the latency differences between PL and ML from simultaneous recordings. For each natural image, the face response to PL preceded ML. Interestingly, when we occluded the eyes, the latencies for both PL and ML increased, but PL face responses still preceded ML. Though PL showed only weak responses to masked and occluded faces, we still saw a similar latency difference (Figure 2—figure supplement 2). We take this as additional evidence that the body-facilitation effect is not due to top-down feedback. Implanting a PL array in Monkey 2 is not an option at this time, so adding another PL monkey now would require training up a third monkey, targeting, and implanting PL, which would take months.</p><disp-quote content-type="editor-comment"><p>3) Relatedly, since neurons differ in their specific preferences, it's surprising that there is minimal discussion of this. The authors should do more to provide information about how stimulus selectivity (aside from positional selectivity) interacts with their main findings.</p></disp-quote><p>We did not see much heterogeneity in tuning across each of our arrays. We attribute this homogeneity in each array to targeting face patches using fMRI. In Figure 1—figure supplement 3, we show, using an independent stimulus set of isolated objects, that all visually responsive channels selectively responded to faces (as compared to bodies, hands, and objects). We now provide body- and face- selectivity indices for each channel. The degree of face selectivity varied across channels, but all channels were much more responsive to faces than to any other category. These neurons may vary in their preference for particular facial features. We did not test for this though all channels were particularly responsive to eyes. There was some variability in the specific tuning e.g., some face-selective channels in Monkey 2 had a small preference for human faces over monkey faces, and other channels in Monkey 2 preferred the opposite. We did not thoroughly test other dimensions that face cells are known to vary along such as viewpoint preference, and our complex stimuli did not substantially vary along this dimension. To better illustrate the similarity in response selectivity across channels, we now show response maps for 3 individual channels for Figures 3 – 5 (previously Figures 2 – 4).</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…]</p><p>1) The results are not as clear-cut as the impression one gets from reading the text of the paper. Examples of such issues are the following. Figure 2: Monkey 1 ML: second column image (human carrying a box): in the occluded condition there is also strong activation in the top right corner where the box is. In Figure 3, third and fifth column: the location of the masked face and the hotspot of neural activity do not match well in the data of M2. Furthermore, for the 7th column image of Figure 3 (3 persons carrying boxes) there is a clear hotspot in both monkeys only for the rightmost (third) person: why not for the closer other two persons? In the 9th column image, there is an additional hotspot in the data of M2 on the body below the covered head. Also striking is the variability in effects between images and monkeys. This is apparent in the data of Figure 4: second column (little response for ball covering the head in M1), fourth column (no or little response to non-face object on face), fifth column (no response to covered face in M1). Also, in general the data of M2 appear to be more &quot;smeared out&quot; compared to those of M1: is that related to RF size differences, neuron-to-neuron variability or the mapping with larger steps in M2? The authors should discuss these discrepancies between the data and what they describe in the text in detail, acknowledging that these cells do not always respond at the location of the occluded head or sometimes even not close to the occluded head.</p></disp-quote><p>The reviewer is correct that responses were variable across individual stimuli. That is to be expected from recordings in IT cortex. Critically, as we show for each and every experiment, across stimuli responses to faces, or where the face ought to be, are on average stronger than to non-face parts of these images. For each experiment, we show averaged PSTHs along with confidence intervals showing the variability across images and perform statistical testing to convey the reliability of these responses.</p><p>As the reviewer noted, it is interesting that for images with multiple faces, some hot spots were more prominent than others. We have been testing the effect of clutter on face responses more directly in a separate set of experiments, but this is beyond the scope of the current study.</p><p>Regarding Monkey 2’s activity maps. We note in the Materials and methods and in describing the data reported in Figure 1 that the step size used for mapping Monkey 2’s response was double that of Monkey 1. This was a limitation due to the length of time Monkey 2 would work on a given day. Importantly, the spatial spread of the response maps due to this larger step size did not preclude finding statistically significant responses to bodies below the receptive field, as reported for all seven experiments. As shown in Figure 1—figure supplement 4, the response maps were similar across channels in Monkey 2’s array, therefore it is unlikely that channel variability had a major effect of smearing the response maps. The receptive field sizes across Monkey 2’s ML array were similar to Monkey 1’s (Figure 1—figure supplement 2). For large RFs, we would certainly expect more spatial spread. If anything, large RFs would work against/prevent the spatially specific effects we find.</p><p>We agree with the reviewer that this variability across individual stimulus response maps should be acknowledged. In each section we now include the following clarification:</p><p>For Figure 3:</p><p>“The magnitude and timing of responses to occluded faces varied across stimuli. Across all stimuli, the population level activity to occluded faces was reliably higher than the average response to the rest of each image (t-test across 8 images; p &lt; 0.05, FDR-corrected).”</p><p>For Figure 4:</p><p>“The population-level responses to masked faces varied in magnitude and timing but were consistently stronger than average responses to the rest of each image (t-test across 12 images; p &lt; 0.05, FDR-corrected).”</p><p>For Figure 5:</p><p>“Similar to the occluded and masked faces, population-level responses to these face-swapped images varied in magnitude and latency but responses to non-face objects were consistently stronger when positioned above a body than when positioned elsewhere (t-test across 8 and 15 images for M1 and M2, respectively; p &lt; 0.05, FDR corrected).”</p><disp-quote content-type="editor-comment"><p>One related problem concerning the data presentation is that the maps are scaled to the minimum and maximum response for each image. I wonder how the maps will look like when shown across the full possible range of net responses for a site (i.e. from 0 net response to maximum response): was there no response at other locations than (or close to) the (occluded) head/face? Also, related to this point, the authors should show the maps for the images of Figure 5 and 6B – not just the PSTHs (see next main point). In other words, these interesting data should be presented with greater detail.</p></disp-quote><p>We address this comment in the Essential Revisions section above.</p><disp-quote content-type="editor-comment"><p>2) The authors show only averages across electrodes per animal (except for the responses to unoccluded faces (Figure S3)) and one wonders how strong the differences are between the different neuron sites. To remedy this, the authors can show the maps for each image in Figure S6 (but please make the images larger) so that the reader can assess the (or lack of) consistency of the responses for the different images.</p></disp-quote><p>We performed statistical analyses for each experiment to quantify the reliability of effects across test images.</p><p>Across all experiments we showed 222 images. Each of the 3 arrays recorded 32 channels. We do not think it is reasonable to provide a supplementary figure with 21,312 images (made large). For each experiment we show the mean responses and confidence intervals across all images and provide appropriate statistics to demonstrate reliable responses.</p><p>We now provide several additional supplemental figures to illustrate:</p><p>1) the variability of the response maps across time (Figure 1—figure supplement 5, Figure 2—figure supplement 1, Figure 3—figure supplement 1, Figure 3—figure supplement 2, Figure 4—figure supplement 1, Figure 4—figure supplement 2, Figure 5—figure supplement 1, Figure 5—figure supplement 2)</p><p>2) the variability across channels (Figure 3—figure supplement 3, Figure 4—figure supplement 3, Figure 5—figure supplement 3).</p><p>3) population-level response maps for every image reported in Figure 6 (Figure 6—figure supplement 1 and 2) and Figure 7B (Figure 7—figure supplement 2).</p><disp-quote content-type="editor-comment"><p>The PSTHs in the current figures are averages across the images so the presented confidence intervals give some indication of the variability for the different images. However, what was unclear to me is how the authors defined the non-face region? They should indicate the latter in Figure S6 and describe in detail how they define the non-face region: centered on an object, equated for contrast/luminance with the face/occluded face, same area, etc? Choosing a valid non-face region does not seem trivial to me for these complex images, and such a choice may affect the responses shown in the PSTHs.</p></disp-quote><p>For Figures 1, 3 and 4, non-face responses were defined as the mean response across all non-face parts of the image. These regions are indicated as grey in Figure 7—figure supplement 3. We acknowledge that the nonface region averages responses over a wide variety of visual features and choosing a non-face region for comparison is not trivial. Because of this, we performed several additional experiments (reported in Figures 5, 6 and 7B) that provide analyses that have well controlled comparisons. For Figure 5, the response of nonface objects atop a body is compared to responses to the identical objects occurring at another part of the scene. For Figure 6, the PSTHs within each of the 4 face groups (intact, noise, outline, and object) were taken from the same region of the image and comprised identical pixels. The only aspect of the image that varied was the presence or absence of a body below this region of interest. For Figure 7B, the pixels for each image analyzed are identical. The only thing that varies is whether the image was upright or inverted.</p><disp-quote content-type="editor-comment"><p>Finally, the authors can compute for each site and image an index contrasting the net response to the face and control, non-face region and show the distribution of such indices for each experiment.</p></disp-quote><p>The consistency and timecourse of responses to face vs. non-face regions across images in Figures 1-4 is shown in the PSTHs and evaluated statistically. For the mapping experiments, we avoided making such indices because that would require averaging across a set time window, and the latencies were variable across stimuli and experiments. We now provide several additional figures that highlight the variable timecourses of responses (Figure 1—figure supplement 5, Figure 2, Figure 2—figure supplement 1, Figure 3—figure supplement 1, Figure 3—figure supplement 2, Figure 4—figure supplement 1, Figure 4—figure supplement 2, Figure 5—figure supplement 1, Figure 5—figure supplement 2). We feel that a more objective approach is to show the actual responses and provide statistics on the consistency of response across images at each millisecond bin.</p><disp-quote content-type="editor-comment"><p>3) In the experiment of Figure 5, the authors compare the responses to a modified face alone and the modified face on top of a body and find stronger responses when the body is present. However, they should have as a control condition also the body without face, otherwise the increased response could have been due to a (weak) response to the body itself (summation of obscured face and body response). The absence of such summation effect for full faces could be due to a ceiling effect, as suggested by the authors.</p></disp-quote><p>The point of this study is that face-selective cells do give a weak response to bodies, but only when a body is presented below the receptive-field center. We do show responses to headless bodies in Figures 6 and 7. The experiment reported in Figure 6 (previously Figure 5) demonstrates that a body positioned below the RF can influence activity in the absence of an intact face regardless of what is within the RF – whether a face outline, face-like shape (noise), or even non-face object. The PSTHs comprise activity only within the face region above the body, not when the body is in the RF. The difference between each colored line in Figure 6 and the black line is sufficient to demonstrate the effect of a body being present below the RF. We now show responses to intact heads, face-shaped noise, face outlines, and objects with and without bodies (Figure 6—figure supplement 1). We also now show the response maps for each image in this experiment for both monkeys (Figure 6—figure supplements 1 and 2).</p><p>The question of whether these neurons would respond when a body is presented below a receptive field by itself is an interesting question. We already show those effects in Figure 7A. Additionally, we now show response maps for several images we tested during a pilot experiment that include headless mannequins and headless bodies (Figure 6—figure supplement 3). In each image, increased activity is apparent above each body.</p><disp-quote content-type="editor-comment"><p>4) The strong responses to the legs for the inverted body in Figure 6 is surprising to me. It clearly shows that the responses of these neurons to body stimuli without a head are more complex than just a response to an occluded face or absent head. The authors suggest that this is related to responses to short contours in the face cells. But if that is the case, can it not explain also the responses to the rectangular shapes (with also short contours) occluding the head in the images of Figure 3? Also, when it is related to short contours, why not then comparable strong responses to the images with a head (upper row of Figure 6)? It is clear to me that we do not understand the responses to these neurons. Also, how consistent was this leg response across the different sites? It also makes one wonder whether some of these &quot;weird&quot; responses are because the recordings are multi-unit, which will pool the responses of different neurons, some of which may not be face selective. In other words, it could be that the single neurons that respond to the legs are not face selective at all but mixed with face selective neurons in this patch. It would have been informative to show the responses of well-isolated single units in this case (and also in the other experiments).</p></disp-quote><p>For the experiment reported in Figure 7A, the responses are focused on and in-between the feet, but only when the feet are above a body. While this response was consistent across stimuli in this experiment, the ML arrays were generally unresponsive to feet in the other experiments. e.g., the responses around the feet are weaker than above the body in the bowling image in Figure 5, the monkey’s foot (except when photoshopped over the body) in Figure 6—figure supplements 1 and 2, the people carrying wood in Figure 4, the monkeys in Figure 4, the man carrying boxes in Figure 3, and the guy with the orangutan in Figure 1. As discussed in the results, we think the responses to the feet in Figure 7A are likely due to the peculiar situation that in the inverted body situation the feet are above a body. We showed in Figure 6 that pretty much any object above a body will elicit a response. The monkey object with body image in Figure 6—figure supplement 2 is a particularly clear example that whatever the base firing rate to a leg is, positioning it over the body increases its firing rate.</p><p>More broadly, the reviewer is concerned whether our effects could be the result of mixed selectivity from multi-unit data. We address the concern of mixed selectivity in the Essential Revisions section above. That said, we agree with the reviewer that the responses of these neurons are more complex than simply responding to an intact (or even occluded) face. We see robust activity to non-face objects – for example, we see hot spots on objects that have features similar to faces e.g., round, concentric circles or the tops of body-like structures (see <xref ref-type="fig" rid="respfig1">Author response image 1</xref>). This does not diminish our findings. Our experiments were specifically aimed at probing the influence of a body below these neurons’ RFs. In particular the experiments reported in Figures 5, 6, and 7B target the effect of placing a body below the RF regardless of tuning properties to other visual features.</p><disp-quote content-type="editor-comment"><p>5) The authors suggest in the Discussion that the responses to the occluded head are not due merely to expectation. However, the latencies of the responses to the occluded head appear to be longer than to a real face (and, in some cases, the responses were (as for the non-face) preceded by inhibition (Figure 2, 3, 4 and 5 in M1)). Thus I feel one cannot rule out an expectation or top-down explanation of the effects.</p></disp-quote><p>An expectation or top-down account is hard to reconcile with our data. Stimuli were shown for only 100ms. Though latencies to occluded faces were longer than to an intact face, preferential responses to occluded faces were statistically significant by ~120-140ms, which seems too short for ‘top-down expectation’. Instead, we have now added to the discussion the idea that such ‘expectation’ could be broad enough to include the idea that the inputs to face cells become sculpted by experience to respond to things that are commonly seen with faces. Furthermore, the latency differences between PL and ML are also inconsistent with a strong topdown account. We now provide a new Figure 2 that demonstrates how the latencies vary in both PL and ML depending on the features present in the image, but PL always responds to intact of face features prior to ML. The fact that the eyeless faces have longer latencies (Figure 2) indicates these neurons likely have complex spatiotemporal RFs that are not driven by top-down expectation. We also now provide an analysis of face and body images using a hierarchical artificial network trained on image classification. We find that the representations of human faces and human bodies in deeper layers are more overlapping than the representations of faces and other categories (Figure 8). This network was not explicitly trained on faces, bodies or humans and thus demonstrates that representations of faces and bodies can merge over learning without any explicit training about faces and bodies.</p><p>To further explain our reasoning, we now provide the following Discussion:</p><p>“The idea that the context of a visual scene leads to the expectation, enhanced probability, or prior, of seeing some particular item is usually thought to involve top-down processes. However, if the feedforward inputs to, say, face-selective neurons, are sculpted by experience to be responsive not only to faces, but also to things frequently experienced in conjunction with faces, then such expectations would be manifest as the kind of complex response properties reported here.”</p><disp-quote content-type="editor-comment"><p>6) That the responses to the occluded heads result from experience makes sense but is speculation and not demonstrated in the present experiments. This should be made explicit.</p></disp-quote><p>We have softened our Discussion to make it clearer that our results are consistent with an interpretation that this contextual coding is dependent on experience without claiming that we have tested this. To bolster this hypothesis, we provide modeling results demonstrating how face and body representations can merge in a hierarchical system from statistical regularities of experience (even when not explicitly trained on those features).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The authors performed an extensive revision, with new interesting data and modelling. They responded to all my concerns and overall I am happy with their revision. Nonetheless, I still have a couple of concerns that should be addressed by the authors.</p><p>1) I understand that it is not possible to show the responses of all units to all stimuli, given the size of the data. I asked to show distribution of indices in order to get an idea about the across image and unit variability but the authors argue that this is not practical because of the variability in response latency amongst images and units. Such argument can always be made and then one should refrain from using indices (but the authors compute face and body indices). I think that when one uses net responses to compute indices, the variability in latency should not be a main cause of variance among indices of different neurons or images. One can employ an analysis window that captures the population response, computed across images and neurons, per experiment. The confidence intervals that are provided now depend on the number of observations; although they show the consistency of the mean population response, the CIs are not a straightforward metric of the variability among sites or images. Also, it is not clear whether the individual site examples shown in the figure supplements are representative of the observed diversity of response profiles. Hence, I still believe that distributions of indices will help to appreciate the variability among images and sites.</p></disp-quote><p>We now provide response histograms for both monkeys in Figures 3-6. We use the same time window as used for the response maps for each monkey. We show the distribution of responses across channels for individual images as well as the distribution of channel responses for the mean across images. Each graph shows that most channels show greater responses to the region above the body vs. comparison regions. The only condition we did not see this effect was for the face with body vs. face without body comparison in Figure 6 (leftmost graph), which, as previously discussed, did not show a significant difference in the population responses across images.</p><disp-quote content-type="editor-comment"><p>2) The authors provide novel data on the responses to images in which the eye is invisible. However, in the images shown in Figure 2, either more than the eyes are removed (top image) and it is unclear how these other changes affected the response to the disfigured face, or, the eyes are occluded by a visible occluder (bottom image). The latter is known to increase the response latency of IT neurons. To assess whether removing the eye from the image affects the response, it would have been better to present a face without eyes, the eye region being filled in with the neighboring texture, so that no additional changes (holes or bar occluder) are present. With the current images, I believe the authors should be more careful in interpreting the response and latency differences between the full face and the images in which the eyes were removed or occluded.</p></disp-quote><p>The reviewer raises a good point that in Figure 2, the images either had more than the eye removed or are presented with a visible occluder. We agree that the particular manner in which the eyes are removed from an image could affect neural responses. In Figure 2—figure supplement 1, we showed as the reviewer requested, an image in which the eye region was filled in with the neighboring texture (third image from the top). We took the face texture from sides of the monkey face and filled in the eye region. A similar shift in the latencies is evident in this image. We modified the main text to directly address this:</p><p>“A similar shift in response latencies was evident when replacing the eyes with a visible occluder, the background of the image, and surrounding texture of the face (Figure 2—figure supplement 1).”</p><disp-quote content-type="editor-comment"><p>3) The authors suggest that the responses to the occluded faces do not result from top-down processing, using as argument that the response latency differences between ML and PL remain for the occluded faces, which is a reasonable argument. However, PL and ML responses are compared directly only for the intact faces and the &quot;invisible eye&quot; images, but not for the face occlusion conditions. One way to show this is to compare in one figure the population PSTHs between ML and PL for the different conditions.</p></disp-quote><p>While we do not plot the responses to PL and ML on the same graph for face occlusion conditions, the difference in responses is apparent when comparing the graphs in Figures 3-5 to Figure 4—figure supplement 4 where responses occur prior to 100ms in PL and after 100ms in ML. We include as <xref ref-type="fig" rid="respfig3">Author response image 3</xref> a direct comparison of these PL and ML responses in monkey 1. Darker responses are ML and fainter responses are PL.</p><fig id="respfig3"><label>Author response image 3.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-resp-fig3-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>4) They write, when describing the novel and interesting Alexnet simulations, that &quot;The Euclidean distances between image pairs within each category were smaller than between categories, indicating that images were more homogeneous within than between categories and highlighting that simple shape is a major component of these commonly tested &quot;semantic&quot; categories&quot;. However, the images also differ in texture and it is possible that not only shape but also texture is driving the category differences in Alexnet. In fact, some recent studies stressed the importance of texture cues for categorization in deep convolutional neural networks.</p></disp-quote><p>We agree with the reviewer that texture may also play an important role. We have modified the text to include reference to texture differences:</p><p>“The Euclidean distances between image pairs within each category were smaller than between categories, indicating that images were more homogenous within than between categories and highlighting that low-level features like shape, color, or texture are a major component of these commonly tested “semantic” categories (also see Dailey and Cottrell, 1999).”</p><disp-quote content-type="editor-comment"><p>5) The authors employ the Alexnet simulation to demonstrate that representations of faces and bodies can merge over learning. However, to show that learning is critical they should perform a similar analysis of the category representations in an untrained Alexnet, e.g. with random weights.</p></disp-quote><p>As requested, we now show that the similarity of faces and bodies in AlexNet is indeed specific to the trained network and not present in untrained networks. We simulated untrained networks by shuffling the weights of the trained AlexNet in each layer. This preserves the distribution of weights and allows for more direct comparisons with the trained network. We calculated image pair correlations on 500 permutations of the shuffled weights. In a new supplementary figure (Figure 8—figure supplement 1, we show that the mean and 97.5% percentiles of face-body correlations across permutations are below the face-body correlations in the trained network. Also, these correlations were no different than the correlations between faces and other categories (black vs. grey shaded regions in Figure 8—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>6) The authors write &quot;that convolutional networks can capture the representational structure of biological networks&quot;. It all depends on what is meant by &quot;capture&quot; here, but there is still a discrepancy between IT and classical feedforward deep net (like Alexnet) representations, e.g. see recent work by DiCarlo's group. I suggest to tone down this statement on the correspondence between deep nets and ventral stream representations.</p></disp-quote><p>We agree with the reviewer that convolutional nets such as AlexNet differ from the ventral stream in fundamental ways and it is erroneous to equate the two. We did not intend to draw such strong equivalencies. The second half of that quoted sentence highlights some of the ways in which these two differ. We agree with the reviewer that differences in representational structure also have been shown between deep nets and IT. We simply intended to convey that deep nets such as AlexNet can capture some of the representational structure such as these differences between image categories in order to demonstrate the effect of the statistics of experience on representations. This is consistent with DiCarlo’s (and others) work. As the reviewer suggested we have toned down this statement to avoid confusion:</p><p>“Though convolutional networks can capture some aspects of the representational structure of biological networks (Hong et al., 2016; Ponce et al., 2019),”</p><disp-quote content-type="editor-comment"><p>7) Can the authors explain why they permute the labels of the non-face images when computing the between face, non-face correlations (inset; Figure 8B); permutation of the non-face labels in this case should not make a difference, or perhaps I am missing something here.</p></disp-quote><p>Permuting all labels of non-face images including bodies and non-bodies tests whether correlations between faces and bodies are part of the same distribution of correlations for faces and other non-face, non-body images. By showing that the face-body correlations are stronger than 97.5% of permuted correlations, this indicates that the representations of faces and bodies are more overlapping than faces and other non-face, non-body images.</p><disp-quote content-type="editor-comment"><p>8) The authors should provide the explained variance of the 2D MDS configuration shown in their novel Figure 8A, so that the reader can appreciate how the distances plotted in 2D correspond to the original distances in the high-dimensional feature space.</p></disp-quote><p>We have added the variance explained to each of the 3 MDS plots. While the first 2 dimensions capture the majority of the variance for pixel intensity and relu7 (72% and 74%). The variance explained by the first two dimensions was substantially lower for relu1 (39%) and increased moving to deeper layers. We are using MDS as a visualization. Importantly, the correlation analyses reported in 8B are performed on the full dimensional feature spaces. We include as <xref ref-type="fig" rid="respfig4">Author response image 4</xref> the variance explained across all relu layers for 2D MDS.</p><fig id="respfig4"><label>Author response image 4.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53798-resp-fig4-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>9) The authors write in their Discussion &quot;More broadly, these results highlight.… that the facilitation of face-selective neurons in the absence of a clear face reflects a lifetime of experience that bodies are usually accompanied by faces.&quot; As I noted before, there is no evidence yet that the contextual effects reported by the authors do result from experience during the lifetime. Please rephrase.</p></disp-quote><p>We have modified the text to more clearly indicate that we infer this:</p><p>“More broadly, these results highlight that neurons in inferotemporal cortex do not code objects and complex shapes in isolation. We propose that these neurons are sensitive to the statistical regularities of their cumulative experience and that the facilitation of face-selective neurons in the absence of a clear face reflects a lifetime of experience that bodies are usually accompanied by faces.”</p><disp-quote content-type="editor-comment"><p>10) Figure 1—figure supplement 1: the authors should indicate the t-values (with the corrected p = 0.05 threshold) of the face-object contrast in the fMRI maps.</p></disp-quote><p>We now include the t and p values for the fMRI maps:</p><p>“Data threshold at t(2480) &gt; 3.91 (p &lt; 0.0001 FDR-corrected) and t(1460) &gt; 3.91 (p &lt; 0.0001 FDR-corrected) for monkeys 1 and 2, respectively.”</p><disp-quote content-type="editor-comment"><p>11) Figure 1—figure supplement 3: can the authors explain the above-baseline responses in M1, ML before 50 ms after stimulus onset – one would expect that soon after stimulus onset responses equal to baseline because of the response latency of the neurons.</p></disp-quote><p>We thank the reviewer for catching this. The responses were baseline-subtracted, but the axis labels were incorrect. We’ve fixed the graph to correct this. We confirm that the other response graphs were not incorrectly labeled.</p></body></sub-article></article>