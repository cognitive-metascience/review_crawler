<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">32605</article-id><article-id pub-id-type="doi">10.7554/eLife.32605</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Comprehensive machine learning analysis of <italic>Hydra</italic> behavior reveals a stable basal behavioral repertoire</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-99535"><name><surname>Han</surname><given-names>Shuting</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9315-3089</contrib-id><email>shuting.han@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-99536"><name><surname>Taralova</surname><given-names>Ekaterina</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-99537"><name><surname>Dupre</surname><given-names>Christophe</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5929-8492</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-38842"><name><surname>Yuste</surname><given-names>Rafael</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4206-497X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">NeuroTechnology Center, Department of Biological Sciences</institution><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-1056"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Reviewing Editor</role><aff id="aff2"><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>28</day><month>03</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e32605</elocation-id><history><date date-type="received" iso-8601-date="2017-10-09"><day>09</day><month>10</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-03-23"><day>23</day><month>03</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Han et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Han et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-32605-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.32605.001</object-id><p>Animal behavior has been studied for centuries, but few efficient methods are available to automatically identify and classify it. Quantitative behavioral studies have been hindered by the subjective and imprecise nature of human observation, and the slow speed of annotating behavioral data. Here, we developed an automatic behavior analysis pipeline for the cnidarian <italic>Hydra vulgaris</italic> using machine learning. We imaged freely behaving <italic>Hydra</italic>, extracted motion and shape features from the videos, and constructed a dictionary of visual features to classify pre-defined behaviors. We also identified unannotated behaviors with unsupervised methods. Using this analysis pipeline, we quantified 6 basic behaviors and found surprisingly similar behavior statistics across animals within the same species, regardless of experimental conditions. Our analysis indicates that the fundamental behavioral repertoire of <italic>Hydra</italic> is stable. This robustness could reflect a homeostatic neural control of &quot;housekeeping&quot; behaviors which could have been already present in the earliest nervous systems.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.32605.002</object-id><title>eLife digest</title><p>How do animals control their behavior? Scientists have been trying to answer this question for over 2,000 years, and many studies have analysed specific behaviors in different animals. However, most of these studies have traditionally relied on human observers to recognise and classify different behaviors such as movement, rest, grooming or feeding. This approach is subject to human error and bias, and is also very time consuming. Because of this, reseachers normally only study one particular behavior, in a piecemeal fashion. But to capture all the different actions an animal generates, faster, more objective methods of systematically classifying and quantifying behavior would be ideal.</p><p>One promising opportunity comes from studying a small freshwater organism called <italic>Hydra</italic>, one of the most primitive animals with a nervous system. Thanks to <italic>Hydra</italic>’s transparent body, modern imaging techniques can be used to observe the activity of their whole nervous system all at once, while the animal is engaged in different actions. However, to realise this potential, scientists need a quick way of automatically recognising different <italic>Hydra</italic> behaviors, such as contracting, bending, tentacle swaying, feeding or somersaulting. This is particularly difficult because <italic>Hydra</italic>’s bodies can change shape in different situations.</p><p>To address this, Han et al. borrowed cutting-edge techniques from the field of computer vision to create a computer program that could automatically analyse hours of videos of freely-moving <italic>Hydra</italic> and classify their behavior automatically. The computer algorithms can learn how to recognise different behaviors in two ways: by learning from examples already classified by humans (known as ‘supervised learning’) or by letting it pick out different patterns by itself (known as ‘unsupervised learning’). The program was able to identify all the behaviors previously classified by humans, as well as new types that had been missed by human observation.</p><p>Using this new computer program, Han et al. discovered that <italic>Hydra</italic>’s collection of six basic behaviors stays essentially the same under different environmental conditions, such as light or darkness. One possible explanation for this is that its nervous system adapts to the environment to maintain a basic set of actions it needs for survival, although another possibility is that <italic>Hydra</italic> just does not care and goes along with its basic behaviors, regardless of the environment. Han et al.’s new method is useful not only for classifying all behavioral responses in <italic>Hydra</italic>, but could potentially be adapted to study all the behaviors in other animal species. This would allow scientists to systematically perform experiments to understand how the nervous system controls all animal behavior, a goal that it is the holy grail of neuroscience.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Hydra vulgaris</kwd><kwd>behavior</kwd><kwd>machine learning</kwd><kwd>ethology</kwd><kwd>computer vision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000185</institution-id><institution>Defense Advanced Research Projects Agency</institution></institution-wrap></funding-source><award-id>HR0011-17-C-0026</award-id><principal-award-recipient><name><surname>Yuste</surname><given-names>Rafael</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><award-id>Howard Hughes Medical Institute International Student Research Fellowship</award-id><principal-award-recipient><name><surname>Han</surname><given-names>Shuting</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001654</institution-id><institution>Grass Foundation</institution></institution-wrap></funding-source><award-id>Grass Fellowship</award-id><principal-award-recipient><name><surname>Dupre</surname><given-names>Christophe</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel automated behavior analysis method for <italic>Hydra</italic> identifies pre-defined and new behavior types, and reveals a stable behavior repertoire.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Animal behavior is generally characterized by an enormous variability in posture and the motion of different body parts, even if many complex behaviors can be reduced to sequences of simple stereotypical movements (<xref ref-type="bibr" rid="bib3">Berman et al., 2014</xref>; <xref ref-type="bibr" rid="bib5">Branson et al., 2009</xref>; <xref ref-type="bibr" rid="bib17">Gallagher et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Srivastava et al., 2009</xref>; <xref ref-type="bibr" rid="bib63">Wiltschko et al., 2015</xref>; <xref ref-type="bibr" rid="bib64">Yamamoto and Koganezawa, 2013</xref>). As a way to systematic capture this variability and compositionality, quantitative behavior recognition and measurement methods could provide an important tool for investigating behavioral differences under various conditions using large datasets, allowing for the discovery of behavior features that are beyond the capability of human inspection, and defining a uniform standard for describing behaviors across conditions (<xref ref-type="bibr" rid="bib15">Egnor and Branson, 2016</xref>). In addition, much remains unknown about how the specific spatiotemporal pattern of activity of the nervous systems integrate external sensory inputs and internal neural network states in order to selectively generate different behavior. Thus, automatic methods to measure and classify behavior quantitatively could allow researchers to indetify potential neural mechanisms by providing a standard measurement of the behavioral output of the nervous system.</p><p>Indeed, advances in calcium imaging techniques have enabled the recording of the activity of large neural populations (<xref ref-type="bibr" rid="bib9">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Jin et al., 2012</xref>; <xref ref-type="bibr" rid="bib28">Kralj et al., 2011</xref>; <xref ref-type="bibr" rid="bib51">St-Pierre et al., 2014</xref>; <xref ref-type="bibr" rid="bib56">Tian et al., 2009</xref>; <xref ref-type="bibr" rid="bib65">Yuste and Katz, 1991</xref>), including whole brain activity from small organisms such as <italic>C. elegans</italic> and larval zebrafish (<xref ref-type="bibr" rid="bib1">Ahrens et al., 2013</xref>; <xref ref-type="bibr" rid="bib38">Nguyen et al., 2016</xref>; <xref ref-type="bibr" rid="bib46">Prevedel et al., 2014</xref>). A recent study has demonstrated the cnidarian <italic>Hydra</italic> can be used as an alternative model to image the complete neural activity during behavior (<xref ref-type="bibr" rid="bib14">Dupre and Yuste, 2017</xref>). As a cnidarian, <italic>Hydra</italic> is close to the earliest animals in evolution that had nervous systems. As the output of the nervous system, animal behavior allows individuals to adapt to the environment at a time scale that is much faster than natural selection, and drives the rapid evolution of the nervous system, providing a rich context to study nervous system functions and evolution (<xref ref-type="bibr" rid="bib2">Anderson and Perona, 2014</xref>). As <italic>Hydra</italic>'s nervous system evolved from that present in the last common ancestor of cnidarians and bilaterians, the behaviors of <italic>Hydra</italic> could also represent some of the most primitive examples of coordination between a nervous system and non-neuronal cells. This could make <italic>Hydra</italic> particularly relevant to our understanding of the nervous systems of model organisms such as <italic>Caenorhabditis elegans</italic>, <italic>Drosophila</italic>, zebrafish, and mice, as it provides an evolutionary perspective to discern whether neural mechanisms found in those species represent a specialization or are generally conserved. In fact, although <italic>Hydra</italic> behavior has been study for centuries, it is still unknown whether <italic>Hydra</italic> possesses complex behaviors such as social interactions and learning, how its behavior changes under environmental, physiological, nutritional or pharmacological manipulations, or what are the underlying neural mechanisms of these potential changes. Having an unbiased and automated behavior recognition and quantification method would therefore enable such studies with large datasets. This could allow high-throughput systematic pharmacological assays, lesion studies, environmental and physiological condition changes in behavior, or alternations under activation of subsets of neurons, testing quantitative models, and linking behavior outputs with the underlying neural activity patterns.</p><p><italic>Hydra</italic> behavior was first described by <xref ref-type="bibr" rid="bib57">Trembley (1744</xref>), and it consists of both spontaneous and stimulus-evoked movements. Spontaneous behaviors include contraction (<xref ref-type="bibr" rid="bib42">Passano and McCullough, 1964</xref>) and locomotion such as somersaulting and inchworming (<xref ref-type="bibr" rid="bib34">Mackie, 1974</xref>), and can sometimes be induced by mechanical stimuli or light. Food-associated stimuli induce a stereotypical feeding response that consists of three distinct stages: tentacle writhing, tentacle ball formation and mouth opening (<xref ref-type="bibr" rid="bib27">Koizumi et al., 1983</xref>; <xref ref-type="bibr" rid="bib32">Lenhoff, 1968</xref>). This elaborate reflex-like behavior is fundamental to the survival of <italic>Hydra</italic> and sensitive to its needs: well-fed animals do not appear to show feeding behavior when exposed to a food stimulus (<xref ref-type="bibr" rid="bib31">Lenhoff and Loomis, 1961</xref>). In addition, feeding behavior can be robustly induced by small molecules such as glutathione and S-methyl-glutathione (GSM) (<xref ref-type="bibr" rid="bib33">Lenhoff and Lenhoff, 1986</xref>). Besides these relatively complex behaviors, <italic>Hydra</italic> also exhibits simpler behaviors with different amplitudes and in different body regions, such as bending, individual tentacle movement, and radial and longitudinal contractions. These simpler behaviors can be oscillatory and occur in an overlapping fashion and are often hard to describe in a quantitative manner. This, in turn, makes complex behaviors such as social or learning behaviors, which can be considered as sequences of simple behaviors, hard to quantitatively define. Indeed, to manually annotate behaviors in videos that are hours or days long is not only extremely time-consuming, but also partly subjective and imprecise (<xref ref-type="bibr" rid="bib2">Anderson and Perona, 2014</xref>). However, analyzing large datasets of behaviors is necessary to systematically study behaviors across individuals in a long-term fashion. Recently, computational methods have been developed to define and recognize some behaviors of <italic>C. elegans</italic> (<xref ref-type="bibr" rid="bib6">Brown et al., 2013</xref>; <xref ref-type="bibr" rid="bib52">Stephens et al., 2008</xref>) and <italic>Drosophila</italic> (<xref ref-type="bibr" rid="bib3">Berman et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Johnson et al., 2016</xref>). These pioneer studies identify the movements of animals by generating a series of posture templates and decomposing the animal posture at each time points with these standard templates. This general framework works well for animals with relatively fixed shapes. However, <italic>Hydra</italic> has a highly deformable body shape that contracts, bends and elongates in a continuous and non-isometric manner, and the same behavior can occur at various body postures. Moreover, Hydra has different numbers of tentacles and buds across individuals, which presents further challenges for applying template-based methods. Therefore, a method that encodes behavior information in a statistical rather than an explicit manner is desirable.</p><p>As a potential solution to this challenge, the field of computer vision has recently developed algorithms for deformable human body recognition and action classification. Human actions have large variations based on the individual’s appearance, speed, the strength of the action, background, illumination, etc. (<xref ref-type="bibr" rid="bib60">Wang et al., 2011</xref>). To recognize the same action across conditions, features from different videos need to be represented in a unified way. In particular, the Bag-of-Words model (BoW model) (<xref ref-type="bibr" rid="bib37">Matikainen et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Sun et al., 2009</xref>; <xref ref-type="bibr" rid="bib59">Venegas-Barrera and Manjarrez, 2011</xref>; <xref ref-type="bibr" rid="bib60">Wang et al., 2011</xref>) has become a standard method for computer vision, as it is a video representation approach that captures the general statistics of image features in videos by treating videos as ‘bags’ of those features. This enables to generalize behavior features in a dataset that is rich with widely varied individual-specific characteristics. The BoW model originated from document classification and spam-detection algorithms, where a text is represented by an empirical distribution of its words. To analyze videos of moving scenes, the BoW model has two steps: feature representation and codebook representation. In the first step, features (i.e. ‘words’ such as movements and shapes) are extracted and unified into descriptor representations. In the second step, these higher order descriptors from multiple samples are clustered (i.e. movement motifs), usually by k-means algorithms, and then averaged descriptors from each cluster are defined as ‘codewords’ that form a large codebook. This codebook in principle contains representative descriptors of all the different movements of the animal. Therefore, each clip of the video can be represented as a histogram over all codewords in the codebook. These histogram representations can be then used to train classifiers such as SVMs, or as inputs to various clustering algorithms, supervised or unsupervised, to identify and quantify behavior types. BoW produces an abstract representation compared to manually specified features, and effectively leverages the salient statistics of the data, enabling modeling of large populations. Doing so on a large scale with manually selected features is not practical. The power of such a generalization makes the BoW framework particularly well suited for addressing the challenge of quantifying <italic>Hydra</italic> behavior.</p><p>Inspired by previous work on <italic>C. elegans</italic> (<xref ref-type="bibr" rid="bib6">Brown et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Kato et al., 2015</xref>; <xref ref-type="bibr" rid="bib52">Stephens et al., 2008</xref>) and <italic>Drosophila</italic> (<xref ref-type="bibr" rid="bib3">Berman et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Johnson et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Robie et al., 2017</xref>) as well as by progress in computer vision (<xref ref-type="bibr" rid="bib60">Wang et al., 2011</xref>), we explored the BoW approach, combining computer vision and machine learning techniques, to identify both known and unannotated behavior types in <italic>Hydra</italic>. To do so, we imaged behaviors from freely moving <italic>Hydra</italic>, extracted motion and shape features from the videos, and constructed a dictionary of these features. We then trained classifiers to recognize <italic>Hydra</italic> behavior types with manual annotations, and identified both annotated and unannotated behavior types in the embedding space. We confirmed the performance of the algorithms with manually annotated data and then used the method for a comprehensive survey of <italic>Hydra</italic> behavior, finding a surprising stability in the expression of six basic behaviors, regardless of the different experimental and environmental conditions. These findings are consistent with the robust behavioral and neural circuit homeostasis found in other invertebrate nervous systems for &quot;housekeeping&quot; functions (<xref ref-type="bibr" rid="bib18">Haddad and Marder, 2017</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Capturing the movement and shape statistics of freely moving <italic>Hydra</italic></title><p>Our goal was to develop a method to characterize the complete behavioral repertoire of <italic>Hydra</italic> under different laboratory conditions. We collected a <italic>Hydra</italic> behavior video dataset (<xref ref-type="bibr" rid="bib19">Han, 2018a</xref>) using a widefield dissecting microscope, allowing <italic>Hydra</italic> to move freely in a culture dish (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). We imaged 53 <italic>Hydra</italic> specimens at a rate of 5 Hz for 30 min, and we either allowed each of them to behave freely, or induced feeding behavior with glutathione, since feeding could not be observed without the presence of prey (which would have obscured the imaging). From viewing these data, we visually identified eight different behaviors, and manually annotated every frame of the entire dataset with the following labels for these eight behavioral states: silent (no apparent motion), elongation, tentacle swaying, body swaying, bending, contraction, somersaulting, and feeding (<xref ref-type="fig" rid="fig1">Figure 1b</xref>; <xref ref-type="fig" rid="fig1">Figure 1e-l</xref>; <xref ref-type="video" rid="video1">Videos 1</xref>–<xref ref-type="video" rid="video7">7</xref>). Overall, we acquired an annotated <italic>Hydra</italic> behavior dataset with 360,000 fames in total. We noticed that most behaviors in our manual annotation lasted less than 10 s (<xref ref-type="fig" rid="fig1">Figure 1c</xref>), and that, within a time window of 5 s, most windows contained only one type of behavior (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). A post-hoc comparison of different window sizes (1–20 s) with the complete analysis framework also demonstrated that 5 s windows result in the best performance (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a</xref>). Therefore, we chose 5 s as the analysis length of a behavior element in <italic>Hydra</italic>.</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.32605.003</object-id><label>Figure 1.</label><caption><title>Acquiring an annotated <italic>Hydra</italic> behavior dataset.</title><p>(<bold>a</bold>) Imaging <italic>Hydra</italic> behavior with a widefield dissecting microscope. A <italic>Hydra</italic> polyp was allowed to move freely in a Petri dish, which was placed on a dark surface under the microscope objective. The light source was placed laterally, creating an bright image of the <italic>Hydra</italic> polyp on a dark background. (<bold>b</bold>) Histogram of the eight annotated behavior types in all data sets. (<bold>c</bold>) Histogram of the duration of annotated behaviors. (<bold>d</bold>) Histogram of total number of different behavior types in 1 s, 5 s and 10 s time windows. (<bold>e–l</bold>) Representative images of silent (<bold>e</bold>), elongation (<bold>f</bold>), tentacle swaying (<bold>g</bold>), body swaying (<bold>h</bold>), bending (<bold>i</bold>), contraction (<bold>j</bold>), feeding (<bold>k</bold>), and somersaulting (<bold>l</bold>) behaviors.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-32605-fig1-v2"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.32605.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Variability of human annotators.</title><p>(<bold>a</bold>) Two example segments of annotations from two different human annotators. (<bold>b</bold>) Confusion matrix of the two annotations from four representative behavior videos. The overall match is 52%.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-32605-fig1-figsupp1-v2"/></fig></fig-group><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.005</object-id><label>Video 1.</label><caption><title>Example of elongation behavior.</title><p>The animal was allowed to move freely in a petri dish. The video was taken at 5 Hz, and was accelerated 20 fold.</p></caption></media><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video2.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.006</object-id><label>Video 2.</label><caption><title>Example of tentacle swaying behavior.</title><p>The animal was allowed to move freely in a petri dish. The video was taken at 5 Hz, and was accelerated 20 fold.</p></caption></media><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video3.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.007</object-id><label>Video 3.</label><caption><title>Example of body swaying behavior.</title><p>The animal was allowed to move freely in a petri dish. The video was taken at 5 Hz, and was accelerated 20 fold.</p></caption></media><media id="video4" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video4.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.008</object-id><label>Video 4.</label><caption><title>Example of bending behavior.</title><p>The animal was allowed to move freely in a petri dish. The video was taken at 5 Hz, and was accelerated 20 fold.</p></caption></media><media id="video5" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video5.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.009</object-id><label>Video 5.</label><caption><title>Example of a contraction burst.</title><p>The animal was allowed to move freely in a petri dish. The video was taken at 5 Hz, and was accelerated 20 fold.</p></caption></media><media id="video6" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video6.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.010</object-id><label>Video 6.</label><caption><title>Example of induced feeding behavior.</title><p>The animal was treated with reduced L-glutathione at 45 s. The video was taken at 5 Hz, and was accelerated 20 fold.</p></caption></media><media id="video7" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video7.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.011</object-id><label>Video 7.</label><caption><title>Example of somersaulting behavior.</title><p>The video was taken at 5 Hz, and was accelerated by 20 fold.</p></caption></media><p>Due to the large shape variability of the highly deformable <italic>Hydra</italic> body during behavior, methods that construct postural eigenmodes from animal postures are not suitable. Therefore, we designed a novel pipeline consisting of four steps: pre-processing, feature extraction, codebook generation, and feature encoding (<xref ref-type="bibr" rid="bib20">Han, 2018b</xref>) (<xref ref-type="fig" rid="fig2">Figure 2</xref>), in line with the BoW framework. Pre-processing was done to exclude the variability in size and rotation angle during imaging, which introduces large variance. To do so, we first defined a behavior element as a 5 s time window, splitting each behavior video into windows accordingly. Then we fitted the body column of <italic>Hydra</italic> into an ellipse, and centered, rotated, and scaled the ellipse to a uniform template ellipse in each element window. We then encoded spatial information into the BoW framework by segmenting the <italic>Hydra</italic> area through the videos with an automated program, dividing it into a tentacle region, an upper body region, and a lower body region (Materials and methods; <xref ref-type="video" rid="video8">Video 8</xref>).</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.32605.012</object-id><label>Figure 2.</label><caption><title>Analysis pipeline.</title><p>Videos of freely moving <italic>Hydra</italic> polyps were collected (<bold>1</bold>), then, <italic>Hydra</italic> images were segmented from background, and the body column was fit to an ellipse. Each time window was then centered and registered, and the <italic>Hydra</italic> region was separated into three separate body parts: tentacles, upper body column, and lower body column (<bold>2</bold>). Interest points were then detected and tracked through each time window, and HOF, HOG and MBH features were extracted from local video patches of interest points. Gaussian mixture codebooks were then generated for each features subtype (<bold>4</bold>), and Fisher vectors were calculated using the codebooks (<bold>5</bold>). Supervised learning using SVM (<bold>6</bold>), or unsupervised learning using t-SNE embedding (<bold>7</bold>) was performed using Fisher vector representations.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-32605-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.32605.013</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Model and parameter selection.</title><p>(<bold>a</bold>) Classification performance using time windows of 1, 3, 5, 8, 10 and 20 s, on training, validation and two test data sets. (<bold>b</bold>) Classification performance with normalized histogram representation, Fisher Vector (FV) representation, Fisher Vector with three spatial body part segmentation (3SP), Fisher Vector with six spatial body part segmentation (6SP), on training, validation and two test data sets. (<bold>c</bold>) Classification performance with K = 64, 128 and 256 Gaussian Mixtures for FV encoding, on training, validation and two test data sets.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-32605-fig2-figsupp1-v2"/></fig></fig-group><media id="video8" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video8.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.014</object-id><label>Video 8.</label><caption><title>Example of the output of body part segmentation.</title><p>White represents tentacle region, yellow represents upper body column region, and red represents lower body column region.</p></caption></media><p>After this encoding, in a feature extraction step we applied a dense trajectory method in each 5 s window element (<xref ref-type="bibr" rid="bib60">Wang et al., 2011</xref>). This dense trajectory method represents video patches by several shape and motion descriptors, including a Histogram of Oriented Gradient (HOG) (<xref ref-type="bibr" rid="bib12">Dalal and Triggs, 2005</xref>), which is based on edge properties in the image patch; and a Histogram of Optical Flow (HOF) as well as a Motion Boundary Histogram (MBH) (<xref ref-type="bibr" rid="bib11">Dalal et al., 2006</xref>), based on motion properties. With the dense trajectory method, we first detected and tracked points with prominent features throughout the videos. Then, for each feature point, we analyzed a small surrounding local patch and computed the motion and shape information therein represented by HOF, HOG and MBH descriptors (<xref ref-type="video" rid="video9">Video 9</xref>). Thus, each video window element was captured as motion and shape descriptors associated with a set of local video patches with distinguished visual features.</p><media id="video9" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video9.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.015</object-id><label>Video 9.</label><caption><title>Examples of detected interest points (red) and dense trajectories (green) in tentacle swaying (left), elongation (middle left), body swaying (middle right), and contraction (right) behaviors in 2 s video clips.</title><p>Upper panels show the original videos; lower panels show the detected features.</p></caption></media><p>To quantize the ‘bags’ of features from each element time window, we collected a uniform feature codebook using all the dense trajectory features. Intuitively, the elements in the codebook are the representative features for each type of motion or shape in a local patch, therefore they can be regarded as standard entries in a dictionary. Here, we generate the codebook in a ‘soft’ manner, where the codebook contains information of the centroid of clusters and their shape. We fitted the features with k Gaussian mixtures. Because each Gaussian is characterized not only by its mean, but also by its variance, we preserved more information than with other ‘hard’ methods like k-means. The next step was to encode the features with the codebook. For this, ‘hard’ methods where one encodes the features by assigning each feature vector to its nearest Gaussian mixture, lose information concerning the shapes of the Gaussians. To avoid this, we encoded the features using Fisher vectors, which describe the distance between features and the Gaussian mixture codebook entries in a probabilistic way, encoding both the number of occurrence and the distribution of the descriptors (<xref ref-type="bibr" rid="bib43">Perronnin et al., 2010</xref>) (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>). Since each element window was split into tentacle, upper body and lower body region, we were able to integrate spatial information by encoding the features in each of the three body regions separately (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>). Finally, we represented the behavior in each element window by the concatenated Fisher vector from the three regions.</p></sec><sec id="s2-2"><title><italic>Hydra</italic> behavior classified from video statistics</title><p>Like all animals, <italic>Hydra</italic> exhibits behaviors at various time scales. Basic behaviors such as elongation and bending are usually long and temporally uniform, while tentacle swaying, body swaying and contraction are usually short and executed in a burst-like manner. Feeding and somersaulting are more complex behaviors that can be broken down into short behavior motifs (<xref ref-type="video" rid="video6">Videos 6</xref>–<xref ref-type="video" rid="video7">7</xref>) (<xref ref-type="bibr" rid="bib31">Lenhoff and Loomis, 1961</xref>). Feeding is apparently a stepwise, fixed action pattern-like uniform behavior, with smooth transitions between tentacle writhing, ball formation, and mouth opening (<xref ref-type="video" rid="video6">Video 6</xref>). Somersaulting represents another fixed action pattern-like behavior and typically consists of a sequence of basic behaviors with elongation accompanied by tentacle movements, contraction, bending, contraction, elongation, and contraction; completing the entire sequence takes a few minutes (<xref ref-type="video" rid="video7">Video 7</xref>). The time spent during each step and the exact way each step is executed vary between animals. Thus, to study <italic>Hydra</italic> behavior, it is essential to accurately recognize the basic behavior types that comprise these complex activities.</p><p>We aimed to capture basic behaviors including silent, elongation, tentacle swaying, body swaying, bending, contraction, and feeding, using the Fisher vector features that encode the video statistics. These features were extracted from 5 s element windows and exhibited stronger similarity within the same behavior type, but were distinguished from features of different behavior types (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). We then trained support vector machine (SVM) classifiers with manual labels on data from 50 <italic>Hydra</italic>, and tested them on a random 10% withheld validation dataset. We evaluated classification performance via the standard receiver operating characteristic (ROC) curve and area under curve (AUC). In addition, we calculated three standard measurements from the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN): accuracy, defined as (TP + TN)/(TP + TN + FP + FN); precision, defined as TP/(TP + FP); and recall, defined as TP/(TP + FN). We achieved perfect training performance (AUC = 1, accuracy 100%), while on the validation data the overall accuracy was 86.8%, and mean AUC was 0.97 (<xref ref-type="fig" rid="fig3">Figure 3b and c</xref>; <xref ref-type="table" rid="table1">Table 1</xref>). This classification framework was easily generalized to new data. With data from three <italic>Hydra</italic> that were not involved in either codebook generation or classifier training, we extracted and encoded features using the generated codebook, and achieved classification accuracy of 90.3% for silent (AUC = 0.95), 87.9% for elongation (AUC = 0.91), 71.9% for tentacle swaying (AUC = 0.76), 83.4% for body swaying (AUC = 0.75), 93.9% for bending (AUC = 0.81) and 92.8% for contraction (AUC = 0.92). All the classifiers achieved significantly better performance than chance levels (<xref ref-type="fig" rid="fig3">Figure 3b, c and d</xref>; <xref ref-type="table" rid="table1">Table 1</xref>; <xref ref-type="video" rid="video10">Video 10</xref>). Interestingly, the variability in classifier performance with new data matched human annotator variability (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). This demonstrates that the codebook generated from training data efficiently captured <italic>Hydra</italic> behaviors and that trained classifiers can robustly identify the basic behaviors of <italic>Hydra</italic> and predict their occurrence automatically from the data.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.32605.016</object-id><label>Figure 3.</label><caption><title>SVM classifiers recognize pre-defined <italic>Hydra</italic> behavior types.</title><p>(<bold>a</bold>) Pairwise Euclidean similarity matrix of extracted Fisher vectors. Similarity values are indicated by color code. (<bold>b</bold>) Confusion matrices of trained classifiers predicting training, validation, and test data. Each column of the matrix represents the number in a predicted class; each row represents the number in a true class. Numbers are color coded as color bar indicates. (Training: n = 50, randomly selected 90% samples; validation: n = 50, randomly selected 10% samples; test: n = 3) (<bold>c</bold>) ROC curves of trained classifiers predicting training, validation and test data. TPR, true positive rate; FPR, false positive rate. Dashed lines represent chance level. (<bold>d</bold>) An example of predicted ethogram using the trained classifiers. (<bold>e</bold>) Three examples of SVM classification of somersaulting behaviors. Dashed boxes indicate the core bending and flipping events.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-32605-fig3-v2"/></fig><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.32605.017</object-id><label>Table 1.</label><caption><title>SVM statistics. AUC: area under curve; Acc: accuracy; Prc: precision; Rec: recall.</title></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Behavior</th><th colspan="6">Train</th><th colspan="6">Withheld</th><th colspan="6">Test</th></tr><tr><th>AUC</th><th>AUC chance</th><th>Acc</th><th>Acc chance</th><th>Prc</th><th>Rec</th><th>AUC</th><th>AUC chance</th><th>Acc</th><th>Acc chance</th><th>Prc</th><th>Rec</th><th>AUC</th><th>AUC chance</th><th>Acc</th><th>Acc chance</th><th>Prc</th><th>Rec</th></tr></thead><tbody><tr><td>Silent</td><td>1</td><td>0.5</td><td>100%</td><td>9.6%</td><td>100%</td><td>100%</td><td>0.98</td><td>0.5</td><td>95.6%</td><td>9.6%</td><td>75.6%</td><td>97.4%</td><td>0.95</td><td>0.5</td><td>90.3%</td><td>1.9%</td><td>18.4%</td><td>90.3%</td></tr><tr><td>Elongation</td><td>1</td><td>0.5</td><td>100%</td><td>14.2%</td><td>100%</td><td>100%</td><td>0.96</td><td>0.5</td><td>93.4%</td><td>13.6%</td><td>76.4%</td><td>95.9%</td><td>0.91</td><td>0.5</td><td>87.9%</td><td>22.2%</td><td>71.4%</td><td>92.6%</td></tr><tr><td>Tentacle sway</td><td>1</td><td>0.5</td><td>100%</td><td>25.1%</td><td>100%</td><td>100%</td><td>0.95</td><td>0.5</td><td>89.6%</td><td>25.0%</td><td>77.5%</td><td>92.4%</td><td>0.76</td><td>0.5</td><td>71.9%</td><td>30.2%</td><td>47.9%</td><td>76.7%</td></tr><tr><td>Body sway</td><td>1</td><td>0.5</td><td>100%</td><td>10.0%</td><td>100%</td><td>100%</td><td>0.92</td><td>0.5</td><td>92.9%</td><td>9.3%</td><td>65.7%</td><td>97.0%</td><td>0.75</td><td>0.5</td><td>83.4%</td><td>17.7%</td><td>52.8%</td><td>95.4%</td></tr><tr><td>Bending</td><td>1</td><td>0.5</td><td>100%</td><td>5.2%</td><td>100%</td><td>100%</td><td>0.98</td><td>0.5</td><td>97.3%</td><td>6.1%</td><td>74.4%</td><td>98.4%</td><td>0.81</td><td>0.5</td><td>93.9%</td><td>6.1%</td><td>38.9%</td><td>96.5%</td></tr><tr><td>Contraction</td><td>1</td><td>0.5</td><td>100%</td><td>6.6%</td><td>100%</td><td>100%</td><td>0.97</td><td>0.5</td><td>95.7%</td><td>6.9%</td><td>70.4%</td><td>97.7%</td><td>0.92</td><td>0.5</td><td>92.8%</td><td>11.7%</td><td>63.2%</td><td>95.5%</td></tr><tr><td>Feeding</td><td>1</td><td>0.5</td><td>100%</td><td>29.2%</td><td>100%</td><td>100%</td><td>1</td><td>0.5</td><td>98.8%</td><td>29.6%</td><td>98.5%</td><td>99.4%</td><td>0.83</td><td>0.5</td><td>81.0%</td><td>10.2%</td><td>39.6%</td><td>94.1%</td></tr></tbody></table></table-wrap><media id="video10" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video10.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.018</object-id><label>Video 10.</label><caption><title>Example of the trained SVM classifiers predicting new data.</title></caption></media><p><italic>Hydra</italic> can exhibit overlapping behaviors at the same time. For example, a <italic>Hydra</italic> specimen could be moving its tentacles while bending, or swaying its body while elongating. In such cases, it would be imprecise to allow only a single behavior label per time window. To capture this situation, we allowed a ‘soft’ classification strategy, taking up to three highest classification types that have a classifier probability within a twofold difference between them. With joint classifiers, we achieved 86.8% overall accuracy on the validation data (81.6% with hard classification), and 59.0% with new test data (50.1% with hard classification). Soft classification improved classification performance by allowing a realistic situation when <italic>Hydra</italic> transitions between two behaviors, or executes multiple behaviors simultaneously.</p><p>In addition to optimally classifying the seven basic behaviors described above, classifying somersaulting video clips with basic behavior classifiers showed a conserved structure during the progression of this behavior (<xref ref-type="fig" rid="fig3">Figure 3e</xref>; <xref ref-type="video" rid="video11">Video 11</xref>). Somersaulting is a complex behavioral sequence that was not included in the seven visually identified behavior types. This long behavior can typically be decomposed into a sequence of simple behaviors of tentacle swaying, elongation, body swaying, contraction, and elongation. Indeed, in our classification of somersaulting with the seven basic behavior types, we noticed a strong corresponding structure: the classified sequences start with tentacle swaying, elongation, and body swaying, then a sequence of contraction and elongation before a core bending event (<xref ref-type="fig" rid="fig3">Figure 3e</xref>); finally, elongation and contraction complete the entire somersaulting behavior. This segmented classification based on breaking down a complex behavior into a sequence of multiple elementary behaviors agrees with human observations, indicating that our method is able to describe combined behaviors using the language of basic behavior types.</p><media id="video11" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video11.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.019</object-id><label>Video 11.</label><caption><title>Example of the trained SVM classifiers predicting somersaulting behavior from a new video.</title><p>Soft prediction was allowed here.</p></caption></media></sec><sec id="s2-3"><title>Unsupervised discovery of behavior states in embedding space</title><p>Manual annotation identifies behavior types on the basis of distinct visual features. However, it is subjective by nature, especially when the <italic>Hydra</italic> exhibits multiple behaviors simultaneously and can be affected by the individual biases of the annotator. Therefore, to complement the supervised method described above, where classifiers were trained with annotated categories, we sought to perform unsupervised learning to discover the structural features of <italic>Hydra</italic> behaviors. Since the Fisher vector representation of video statistics is high-dimensional, we applied a nonlinear embedding technique, t-Distributed Stochastic Neighbor Embedding (t-SNE), to reduce the feature vector dimensionality (<xref ref-type="bibr" rid="bib3">Berman et al., 2014</xref>; <xref ref-type="bibr" rid="bib58">Van Der Maaten, 2009</xref>). This also allowed us to directly visualize the data structure in two dimensions while preserving the local structures in the data, serving as a method for revealing potential structures of the behavior dataset.</p><p>Embedding the feature vectors of training data resulted in a t-SNE map that corresponded well to our manual annotation (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). Generating a density map over the embedded data points revealed cluster-like structures in the embedding space (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). We segmented the density map into regions with a watershed method, which defined each region as a behavior motif region (<xref ref-type="fig" rid="fig4">Figure 4c and e</xref>). We evaluated the embedding results by quantifying the manual labels of data points in each behavior motif region. We then assigned a label to each region based on the majority of the manually labeled behavior types in it. Using this approach, we identified 10 distinct behavior regions in the map (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). These regions represented not only the seven types we defined for supervised learning, but also a somersaulting region, and three separate regions representing the three stages of feeding behavior (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). Embedding with continuous 5 s time windows, which exclude the effect of the hard boundaries of separating the behavior elements, revealed the same types of behaviors (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.32605.020</object-id><label>Figure 4.</label><caption><title>t-SNE embedding map of behavior types.</title><p>(<bold>a</bold>) Scatter plot with embedded Fisher vectors. Each dot represents projection from a high-dimensional Fisher vector to its equivalent in the embedding space. Color represents the manual label of each dot. (<bold>b</bold>) Segmented density map generated from the embedding scatter plot. (<bold>c</bold>) Behavior motif regions defined using the segmented density map. (<bold>d</bold>) Labeled behavior regions. Color represents the corresponding behavior type of each region. (<bold>e</bold>) Percentage of the number of samples in each segmented region. (<bold>f</bold>) Two examples of embedded behavior density maps from test <italic>Hydra</italic> polyps that were not involved in generating the codebooks or generating the embedding space. (<bold>g</bold>) Quantification of manual label distribution in training, validation and test datasets. Dashed boxes highlight the behavior types that were robustly recognized in all the three datasets. Feeding 1, the tentacle writhing or the first stage of feeding behavior; feeding 2, the ball formation or the second stage of feeding behavior; feeding 3, the mouth opening or the last stage of feeding behavior.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-32605-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.32605.021</object-id><label>Figure 4—figure supplement 1.</label><caption><title>t-SNE embedding of continuous time windows.</title><p>(<bold>a</bold>) Scatter plot with embedded Fisher vectors. Each dot represents projection from a high-dimensional Fisher vector to its equivalent in the embedding space. The Fisher vectors were encoded from continuous 5 s windows with an overlap of 24 frames. Color represents the manual label of each dot. (<bold>b</bold>) Segmented density map generated from the embedding scatter plot. (<bold>c</bold>) Behavior motif regions defined using the segmented density map. (<bold>d</bold>) Labeled behavior regions with manual labels. Color represents the corresponding behavior type of each region.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-32605-fig4-figsupp1-v2"/></fig></fig-group><p>The generated embedding space could be used to embed new data points (<xref ref-type="bibr" rid="bib3">Berman et al., 2014</xref>). We embedded feature vectors from a withheld validation dataset, as well as from three <italic>Hydra</italic> that were involved neither in generating the feature codebook, nor in the embedding space generation (<xref ref-type="fig" rid="fig4">Figure 4f</xref>). Quantitative evaluation of embedding performance with manual labels showed that all behavior types were accurately identified by embedding in the validation data. In test samples, embedding identification of elongation, tentacle sway, body sway, contraction, and the ball formation stage of feeding, all agreed with manual labels (<xref ref-type="fig" rid="fig4">Figure 4g</xref>). Therefore, embedding of feature vectors can identify the same behavior types that are identified by human annotation.</p></sec><sec id="s2-4"><title>Embedding reveals unannotated behaviors in long datasets</title><p>We wondered if <italic>Hydra</italic> has any spontaneous behaviors under natural day/night cycles that were not included in our manually labeled sets. We mimicked natural conditions by imaging a <italic>Hydra</italic> polyp for 3 days and nights with a 12 hr dark/light cycle (<xref ref-type="fig" rid="fig5">Figure 5a</xref>), keeping the <italic>Hydra</italic> in a 100 µm thick coverslip covered chamber to constrain it within the field of view of the microscope (<xref ref-type="fig" rid="fig5">Figure 5b</xref>) (<xref ref-type="bibr" rid="bib14">Dupre and Yuste, 2017</xref>). This imaging approach, although constraining the movement of <italic>Hydra</italic>, efficiently reduced the complexity of the resulting motion from a three-dimensional to a two-dimensional projection, while still allowing the <italic>Hydra</italic> to exhibit a basic repertoire of normal behaviors.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.32605.022</object-id><label>Figure 5.</label><caption><title>t-SNE embedding reveals unannotated egestion behavior.</title><p>(<bold>a</bold>) Experimental design. A <italic>Hydra</italic> polyp was imaged for 3 days and nights, with a 12 hr light/12 hr dark cycle. (<bold>b</bold>) A <italic>Hydra</italic> polyp was imaged between two glass coverslips separated by a 100 µm spacer. (<bold>c</bold>) Left: density map of embedded behavior during the 3-day imaging. Right: segmented behavior regions with the density map. Magenta arrow indicates the behavior region with discovered egestion behavior. (<bold>d</bold>) Identification of egestion behavior using width profile. Width of the <italic>Hydra</italic> polyp (gray trace) was detected by fitting the body column of the animal to an ellipse, and measuring the minor axis length of the ellipse. The width trace was then filtered by subtracting a 15-minute mean width after each time point from a 15-minute mean width before each time point (black trace). Peaks (red stars) were then detected as estimated time points of egestion events (Materials and methods). (<bold>e</bold>) Density of detected egestion behaviors in the embedding space. Magenta arrow indicates the high density region that correspond to the egestion region discovered in <bold>c</bold>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-32605-fig5-v2"/></fig><p>Using this new dataset, we generated a t-SNE embedding density map from the feature vectors as previously described, and segmented it into behavior motif regions (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). Among the resulting 260 motif regions, we not only discovered previously defined behavior types including silent, elongation, bending, tentacle swaying, and contraction, but also found subtypes within certain classes (<xref ref-type="video" rid="video12">Videos 12</xref>–<xref ref-type="video" rid="video19">19</xref>). In elongation, for example, we found three different subtypes based on the state of the animal: slow elongation during the resting state of the animal, fast elongation after a contraction burst, and inter-contraction elongation during a contraction burst (<xref ref-type="video" rid="video13">Videos 13</xref>–<xref ref-type="video" rid="video15">15</xref>). In contraction, we found two different subtypes: the initial contraction of a contraction burst, and the subsequent individual contraction events when the animal is in a contracted state (<xref ref-type="video" rid="video18">Videos 18</xref>–<xref ref-type="video" rid="video19">19</xref>). Interestingly, we also discovered one region in the embedding map that showed a previously unannotated egestion behavior (<xref ref-type="fig" rid="fig5">Figure 5c</xref>; <xref ref-type="video" rid="video20">Video 20</xref>). Egestion behavior (also known as radial contraction) has been observed before (<xref ref-type="bibr" rid="bib14">Dupre and Yuste, 2017</xref>), and is typically a fast, radial contraction of the body column that happens within 1 s and empties the body cavity of fluid. Although this behavior happens with animals in their natural free movement, its fast time scale and the unconstrained movement make it hard to identify visually during human annotation. In addition, another t-SNE region showed a novel hypostome movement associated with egestion, characterized by a regional pumping-like movement in hypostome and lower tentacle regions (<xref ref-type="video" rid="video21">Video 21</xref>).</p><media id="video12" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video12.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.023</object-id><label>Video 12.</label><caption><title>Examples from the identified silent region in the embedding space.</title></caption></media><media id="video13" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video13.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.024</object-id><label>Video 13.</label><caption><title>Examples from the identified slow elongation region in the embedding space.</title></caption></media><media id="video14" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video14.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.025</object-id><label>Video 14.</label><caption><title>Examples from the identified fast elongation region in the embedding space.</title></caption></media><media id="video15" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video15.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.026</object-id><label>Video 15.</label><caption><title>Examples from the identified inter-contraction elongation region in the embedding space.</title></caption></media><media id="video16" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video16.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.027</object-id><label>Video 16.</label><caption><title>Examples from the identified bending region in the embedding space.</title></caption></media><media id="video17" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video17.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.028</object-id><label>Video 17.</label><caption><title>Examples from the identified tentacle swaying region in the embedding space.</title></caption></media><media id="video18" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video18.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.029</object-id><label>Video 18.</label><caption><title>Examples from the identified initial contraction region in the embedding space.</title></caption></media><media id="video19" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video19.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.030</object-id><label>Video 19.</label><caption><title>Examples from the identified contracted contraction region in the embedding space.</title></caption></media><media id="video20" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video20.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.031</object-id><label>Video 20.</label><caption><title>Examples from the identified egestion region in the embedding space.</title></caption></media><media id="video21" mime-subtype="mp4" mimetype="video" xlink:href="elife-32605-video21.mp4"><object-id pub-id-type="doi">10.7554/eLife.32605.032</object-id><label>Video 21.</label><caption><title>Examples from the identified hypostome movement region in the embedding space.</title></caption></media><p>We evaluated the reliability of the identification of this newly discovered egestion behavior from the embedding method by detecting egestion with an additional ad-hoc method. We measured the width of the <italic>Hydra</italic> body column by fitting it to an ellipse, and low-pass filtered the width trace. Peaks in the trace then represent estimated time points of egestion behavior, which is essentially a rapid decrease in the body column width (<xref ref-type="fig" rid="fig5">Figure 5d</xref>). Detected egestion time points were densely distributed in the newly discovered egestion region in the embedding map (<xref ref-type="fig" rid="fig5">Figure 5e</xref>), confirming that our method is as an efficient way to find novel behavior types.</p></sec><sec id="s2-5"><title>Basic behavior of <italic>Hydra</italic> under different experimental conditions</title><p>Although basic <italic>Hydra</italic> behaviors such as contraction, feeding and somersaulting have been described for over two centuries, the quantitative understanding of <italic>Hydra</italic> behaviors has been limited by the subjective nature of human annotation and by the amount of data that can be processed by manual examination. To build quantitative descriptions that link behaviors to neural processes and to explore behavior characteristics of <italic>Hydra</italic>, we used our newly developed method to compare the statistics of behavior under various physiological and environmental conditions.</p><p>In its natural habitat, <italic>Hydra</italic> experiences day/night cycles, food fluctuations, temperature variations, and changes in water chemistry. Therefore, we wondered whether <italic>Hydra</italic> exhibit different behavioral frequencies or behavioral variability under dark and light conditions, as well as in starved and well-fed conditions. Since we did not expect <italic>Hydra</italic> to exhibit spontaneous feeding behavior in the absence of prey, we only analyzed six basic behavior types using the trained classifiers: silent, elongation, tentacle swaying, body swaying, bending, and contraction. Lighting conditions (light vs. dark) did not result in any significant changes in either the average time spent in each of the six behavior types (<xref ref-type="fig" rid="fig6">Figure 6a</xref>) or the individual behavior variability defined by the variation of the percentage of time spent in each behavior in 30 min time windows (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). Also, compared with starved <italic>Hydra</italic>, well-fed <italic>Hydra</italic> did not show significant changes in the percentage of time spent in elongation behavior (<xref ref-type="fig" rid="fig6">Figure 6c</xref>), but showed less variability in it (<xref ref-type="fig" rid="fig6">Figure 6d</xref>; starved: 8.95 ± 0.69%, fed: 5.46 ± 0.53%, p=0.0047).</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.32605.033</object-id><label>Figure 6.</label><caption><title>Similar behavior statistics under different conditions but differences across species.</title><p>(<bold>a</bold>) Percentage of time <italic>Hydra</italic> spent in each behavior, in dark (red to infra-red) and light conditions. Each circle represents data from one individual. The horizontal line represents the average of all samples. Red represents dark condition, blue represents light condition. (n<sub>dark</sub> = 6, n<sub>light</sub> = 7) (<bold>b</bold>) Standard deviations of behaviors within each individual animal, calculated with separate 30 min time windows in the recording. Each circle represents the behavior variability of one individual. (<bold>c</bold>) Percentage of time <italic>Hydra</italic> spent in each behavior, in starved and well-fed condition. (n<sub>starved</sub> = 6, n<sub>fed</sub> = 7) (<bold>d</bold>) Standard deviations of individual behaviors under starved and well-fed conditions. (<bold>e</bold>) Percentage of time small and large <italic>Hydra</italic> spent in each behavior. (n<sub>small</sub> = 10, n<sub>large</sub> = 7). (<bold>f</bold>) Standard deviations of behaviors of small and large individuals. (<bold>g</bold>) Percentage of time <italic>Hydra vulgaris</italic> and <italic>Hydra viridissima</italic> spent in each behavior type. (n<sub>vulgaris</sub> = 7, n<sub>viridissima</sub> = 5). (<bold>h</bold>) Standard deviations of individual brown and green <italic>Hydra</italic>. *p&lt;0.05, **p&lt;0.01, Wilcoxon rank-sum test.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-32605-fig6-v2"/></fig><p>As <italic>Hydra</italic> polyps vary significantly in size depending on the developmental stage (e.g. freshly detached buds vs. fully grown animals,) and nutrition status (e.g. <italic>Hydra</italic> that has been starved for a week vs. well-fed <italic>Hydra</italic>), we also explored whether <italic>Hydra</italic> of different sizes exhibit different behavioral characteristics. For this, we imaged behaviors of <italic>Hydra</italic> with up to a threefold difference in sizes. Large <italic>Hydra</italic> polyps had similar silent, body swaying, and contraction patterns, but spent slightly less time in elongation, and more in tentacle swaying (<xref ref-type="fig" rid="fig6">Figure 6e</xref>; elongation small: 22.42 ± 1.35%, large: 17.00 ± 0.74%, p=0.0068; tentacle swaying small: 34.24 ± 1.24%, large: 41.06 ± 2.70%, p=0.03). The individual behavior variability remained unchanged (<xref ref-type="fig" rid="fig6">Figure 6f</xref>).</p><p>Finally, we further inquired if different <italic>Hydra</italic> species have different behavioral repertoires. To answer this, we compared the behaviors of <italic>Hydra vulgaris</italic>, and <italic>Hydra viridissima,</italic> (i.e. green <italic>Hydra</italic>), which contains symbiotic algae in its endodermal epithelial cells(<xref ref-type="bibr" rid="bib36">Martínez et al., 2010</xref>). The last common ancestor of these two species was at the base of <italic>Hydra</italic> radiation. Indeed, we found that <italic>Hydra viridissima</italic> exhibited statistically less silent and bending behaviors, but more elongations (<xref ref-type="fig" rid="fig6">Figure 6g</xref>; elongation <italic>vulgaris</italic>: 15.74 ± 0.50%, <italic>viridissima</italic>: 18.63 ± 0.87%, p=0.0303; bending <italic>vulgaris</italic>: 2.31 ± 0.27%, <italic>viridissima</italic>: 1.35 ± 0.17%, p=0.0177), while individual <italic>viridissima</italic> specimens also exhibit slightly different variability in bending (<xref ref-type="fig" rid="fig6">Figure 6h</xref>; <italic>vulgaris</italic>: 2.17% ± 0.26%, <italic>viridissima</italic>: 1.33 ± 0.20%, p=0.0480). We concluded that different <italic>Hydra</italic> species can have different basic behavioral repertoires.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>A machine learning method for quantifying behavior of deformable animals</title><p>Interdisciplinary efforts in the emerging field of computational ethology are seeking novel ways to automatically measure and model natural behaviors of animals (<xref ref-type="bibr" rid="bib2">Anderson and Perona, 2014</xref>) (<xref ref-type="bibr" rid="bib3">Berman et al., 2014</xref>; <xref ref-type="bibr" rid="bib5">Branson et al., 2009</xref>; <xref ref-type="bibr" rid="bib6">Brown et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Creton, 2009</xref>; <xref ref-type="bibr" rid="bib13">Dankert et al., 2009</xref>; <xref ref-type="bibr" rid="bib23">Johnson et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Kabra et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Pérez-Escudero et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Robie et al., 2017</xref>; <xref ref-type="bibr" rid="bib52">Stephens et al., 2008</xref>; <xref ref-type="bibr" rid="bib54">Swierczek et al., 2011</xref>; <xref ref-type="bibr" rid="bib63">Wiltschko et al., 2015</xref>). Most of these approaches rely on recognizing variation of the shapes of animals based on fitting video data to a standard template of the body of the animal. However, unlike model organisms like worms, flies, fishes and mice, <italic>Hydra</italic> differs dramatically from these bilaterian organisms in having an extremely deformable and elastic body. Indeed, during contraction, <italic>Hydra</italic> appears as a ball with all tentacles shortened, while during elongation, <italic>Hydra</italic> appears as a long and thin column with tentacles relaxed. Moreover, these deformations are non-isometric, that is, different axes, and different parts of the body, change differently. The number of tentacles each <italic>Hydra</italic> has also varies. These present difficult challenges for recognizing <italic>Hydra</italic> behaviors using preset templates.</p><p>To tackle the problem of measuring behavior in a deformable animal, we developed a novel analysis pipeline using approaches from computer vision that have achieved success in human action classification tasks (<xref ref-type="bibr" rid="bib26">Ke et al., 2007</xref>; <xref ref-type="bibr" rid="bib29">Laptev et al., 2008</xref>; <xref ref-type="bibr" rid="bib45">Poppe, 2010</xref>; <xref ref-type="bibr" rid="bib61">Wang et al., 2009</xref>; <xref ref-type="bibr" rid="bib60">Wang et al., 2011</xref>). Such tasks usually involve various actions and observation angles, as well as occlusion and cluttered background. Therefore, they require more robust approaches to capture stationary and motion statistics, compared to using pre-defined template-based features. In particular, the bag-of-words (BoW) framework is an effective approach for extracting visual information from videos of humans or animals with arbitrary motion and deformation. The BoW framework originated from document classification tasks with machine learning. In this framework, documents are considered ‘bags’ of words, and are then represented by a histogram of word counts using a common dictionary. These histogram representations are widely used for classifying document types because of their efficiency. In computer vision, the BoW framework considers pictures or videos as ‘bags’ of visual words, such as small patches in the images, or shape and motion features extracted from such patches. Compared with another popular technique in machine vision, template matching, BoW is more robust against challenges such as occlusion, position, orientation, and viewing angle changes. It also proves to be successful in capturing object features in various scenes, and thus has become one of the most important developments and cutting edge methods in this field. For these reasons, BoW appears ideally suited for the problem behavior recognition tasks of deformable animals, such as <italic>Hydra</italic>.</p><p>We modified the BoW framework by integrating other computational methods, including body part segmentation (which introduces spatial information), dense trajectory features (which encode shape and motion statistics in video patches) and Fisher vectors (which represent visual words in a statistical manner). Our choice of framework and parameters proved to be quite adequate, considering both its training and validation accuracy, as well as its generalizability on test datasets (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Indeed, the robust correspondence between supervised, unsupervised and manual classification that we report provides internal cross-validation to the validity and applicability of our BoW machine learning approach. Our developed framework, which uses both supervised and unsupervised techniques, is in principle applicable to all organisms, since it does not rely on specific information of <italic>Hydra</italic>. Compared with previously developed methods, our method would be particularly suitable for behaviors in natural conditions that involve deformable body shapes, as a first step to developing more sophisticated behavioral methods in complex environment for other species.</p><p>Our goal was to describe all possible <italic>Hydra</italic> behavior quantitatively. Because of this, we used the BoW framework to capture the overall statistics with a given time frame. We defined the length of basic behavior elements to be 5 s, which maximizes the number of behaviors that were kept intact while uncontaminated by other behavior types (<xref ref-type="fig" rid="fig1">Figure 1c–d</xref>). However, it should be noted that our approach could not capture fine-level behavior differences, for example, single tentacle behavior. This would require modeling the animal with an explicit template, or with anatomical landmarks, as demonstrated by deformable human body modeling with wearable sensors. Our approach also does not recover transition probabilities between behavior types, or behavioral interactions between individual specimens. In fact, since our method treats each time window as an independent ‘bag’ of visual words, there was no constraint on the temporal smoothness of classified behaviors. Classifications were allowed to be temporally noisy, therefore they could not be applied for temporal structure analysis. A few studies have integrated state-space models for modeling both animal and human behavior (<xref ref-type="bibr" rid="bib17">Gallagher et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Ogale et al., 2007</xref>; <xref ref-type="bibr" rid="bib63">Wiltschko et al., 2015</xref>), while others have used discriminative models such as Conditional Random Field models for activity recognition (<xref ref-type="bibr" rid="bib49">Sminchisescu et al., 2006</xref>; <xref ref-type="bibr" rid="bib62">Wang and Suter, 2007</xref>). These methods may provide promising candidates for modeling behavior with temporal structure in combination with our approach (<xref ref-type="bibr" rid="bib45">Poppe, 2010</xref>).</p><p>In our analysis pipeline, we applied both supervised and unsupervised approaches to characterize <italic>Hydra</italic> behavior. In supervised classifications (with SVM), we manually defined seven types of behaviors, and trained classifiers to infer the label of unknown samples. In unsupervised analysis (t-SNE), we did not pre-define behavior types, but rather let the algorithm discover the structures that were embedded in the behavior data. In addition, we found that unsupervised learning could discover previously unannotated behavior types such as egestion. However, the types of behaviors discovered by unsupervised analysis are limited by the nature of the encoded feature vectors. Since the BoW model provides only a statistical description of videos, those features do not encode fine differences in behaviors. Due to this difference, we did not apply unsupervised learning to analyze behavior statistics under different environmental and physiological conditions, as supervised learning appeared more suitable for applications where one needs to assign a particular label to a new behavior video.</p></sec><sec id="s3-2"><title>Stability of the basic behavioral repertoire of <italic>Hydra</italic></title><p>Once we established the reliability or our method, we quantified the differences between six basic behaviors in <italic>Hydra</italic> under different experimental conditions with two different species of <italic>Hydra</italic> and found that <italic>Hydra vulgaris</italic> exhibits essentially the same behavior statistics under dark/light, large/small and starved/fed conditions. Although some small differences were observed among experimental variables, the overall dwell time and variance of the behavioral repertoire of <italic>Hydra</italic> were unexpectedly very similar in all these different conditions. Although we could not exclude the possibility that there were differences in the transition probabilities between behaviors, our results still show that , from the six basic behaviors analyzed, <italic>Hydra</italic> possess a surprisingly robust behavioral frequencies and similarities across environmental and physiological conditions, while interspecies differences introduce stronger behavior differences.</p><p><xref ref-type="bibr" rid="bib42">Passano and McCullough (1964</xref>) reported that <italic>Hydra littoralis</italic>, a close relative with our <italic>Hydra vulgaris</italic> AEP strain (<xref ref-type="bibr" rid="bib36">Martínez et al., 2010</xref>), showed fewer contraction bursts in the evenings and nights than in the day, and feeding every third or fourth day resulted in fewer contraction bursts than was seen with daily feeding. However, they detected contraction bursts by electrical recording of epithelial cell activity, and defined coordinated activity as a contraction event. In our method, we did not measure the number of such events, but instead measured the number of time windows that contain such contractile behavior. This is essentially a measurement of the time spent in contractions instead of frequency of individual events. Using natural light instead of lamp light could also lead to a difference in the observation results. Interestingly, we observed that <italic>Hydra vulgaris</italic> exhibits different behavior statistics compared with <italic>Hydra viridissima</italic>. The split leading to <italic>Hydra vulgaris</italic> and <italic>Hydra viridissima</italic> is the earliest one in the <italic>Hydra</italic> phylogenetic tree (<xref ref-type="bibr" rid="bib36">Martínez et al., 2010</xref>), thus these two species are quite divergent. <italic>Hydra viridissima</italic> also possesses symbiotic algae, and requires light for normal growth (<xref ref-type="bibr" rid="bib30">Lenhoff and Brown, 1970</xref>). These differences in genetics and growth conditions could help explaining the observed behavioral differences.</p><p>Given the similarity in statistics of basic behaviors in different conditions across different animals within the same species, we naturally wondered if our approach might not be effective or sensitive enough to detect significant behavioral differences among animals. However, the high accuracy of the classification of annotated behavior subtypes (<xref ref-type="fig" rid="fig3">Figure 3</xref>) and also the method reproducibility, with small variances when measuring different datasets, rules out the possibility that this machine learning method is insensitive, in which case the results of our behavioral analysis would have been noisy and irreproducible. This conclusion was corroborated by the statistical differences in behavior found across two different <italic>Hydra</italic> species.</p><p>We had originally expected to observe larger variability of behaviors under different experimental conditions and we report essentially the opposite result. We interpret the lack of behavioral differences across individuals as evidence for robust neural control of a basic behavioral pattern, which appears unperturbed by different experimental conditions. While this rigidity may not seem ideal if one assumes that behavior should flexibly adapt to the environment, it is possible that the six behaviors we studied represent a basic ‘house keeping’ repertoire that needs to be conserved for the normal physiology and survival of the animal. Our results are reminiscent of work on the stomatogastric ganglion of crustaceans that has revealed homeostatic mechanisms that enable central pattern generators to function robustly in different environmental conditions, such as changes in temperature (<xref ref-type="bibr" rid="bib18">Haddad and Marder, 2017</xref>). In fact, in this system, neuropeptides and neuromodulators appear to be flexibly used to enable circuit and behavioral homeostasis (<xref ref-type="bibr" rid="bib35">Marder, 2012</xref>). Although we do not yet understand the neural mechanisms responsible for the behavioral stability in <italic>Hydra</italic>, it is interesting to note that the <italic>Hydra</italic> genome has more than one hundred neuropeptides that could play neuromodulator roles (<xref ref-type="bibr" rid="bib8">Chapman et al., 2010</xref>; <xref ref-type="bibr" rid="bib16">Fujisawa and Hayakawa, 2012</xref>). This vast chemical toolbox could be used to supplement a relatively sparse wiring pattern with mechanisms to ensure that the basic behavior necessary for the survival of the animal remains constant under many different environmental conditions. One can imagine that different neuromodulators could alter the biophysical properties of connections in the <italic>Hydra</italic> nerve net and thus keep a stable operating regime of its neurons in the physiological states.</p><p>In addition, a possible reason for the behavioral similarity among different specimens of <italic>Hydra</italic> could be their genetic similarities. We used animals derived from the same colony (<italic>Hydra</italic> AEP strain), which was propagated by clonal budding. Thus, it is likely that many of the animals were isogenic, or genetically very similar. The lack of genetic variability, although it does not explain the behavioral robustness, could partly be a reason behind our differences across species, and it would explain a relatively small quantitative variability across animals of our <italic>H. vulgaris</italic> colony, as opposed to a larger variability in specimens from the wild.</p><p>Finally, it is also possible that the behavioral repertoire of cnidarians, which represents some of the simplest nervous systems in evolution in structure and probably also in function, could be particularly simple and hardwired as compared with other metazoans or with bilaterians. From this point of view, the robustness we observed could reflect a ‘passive stability’ where the neural mechanisms are simply unresponsive to the environment, as opposed to a homeostatic ‘active stability’, generated perhaps by neuromodulators. This distinction mirrors the difference between open-loop and closed-loop control systems in engineering (<xref ref-type="bibr" rid="bib48">Schiff, 2012</xref>). Thus, it would be fascinating to reverse engineer the <italic>Hydra</italic> nerve net and discern to what extent its control mechanisms are regulated externally. Regardless of the reason for this behavioral stability, our analysis provides a strong baseline for future behavioral analysis of <italic>Hydra</italic> and for the quantitative analysis of the relation between behavior, neural and non-neuronal cell activity.</p></sec><sec id="s3-3"><title>Hydra as a model system for investigating neural circuits underlying behavior</title><p>Revisiting <italic>Hydra</italic> as a model system with modern imaging and computational tools to systematically analyze its behavior provides a unique opportunity to image the entire neural network in an organism and decode the relation between neural activity and behaviors (<xref ref-type="bibr" rid="bib4">Bosch et al., 2017</xref>). With recently established GCaMP6s transgenic <italic>Hydra</italic> lines (<xref ref-type="bibr" rid="bib14">Dupre and Yuste, 2017</xref>) and the automated behavior recognition method introduced in this study, it should now be possible to identify the neural networks responsible for each behavior in <italic>Hydra</italic> under laboratory conditions.</p><p>With this method, we demonstrate that we are able to recognize and quantify <italic>Hydra</italic> behaviors automatically, and to identify novel behavior types. This allows us to investigate the behavioral repertoire stability under different environmental, physiological and genetic conditions, providing insight into how a primitive nervous system adapt to its environment. Although our framework does not currently model temporal information directly, it serves as a stepping-stone toward building more comprehensive models of <italic>Hydra</italic> behaviors. Future work that incorporates temporal models would allow us to quantify behavior sequences, and to potentially investigate more complicated behaviors in <italic>Hydra</italic> such as social and learning behaviors.</p><p>As a member of the phylum Cnidaria, <italic>Hydra</italic> is a sister to bilaterians, and its nervous system and bilaterians nervous systems share a common ancestry. As demonstrated by the analysis of its genome (<xref ref-type="bibr" rid="bib8">Chapman et al., 2010</xref>), <italic>Hydra</italic> is closer in gene content to the last common ancestor of the bilaterian lineage than some other models systems used in neuroscience research, such as <italic>Drosophila</italic> and <italic>C. elegans</italic>. In addition, comparative studies are essential to discern whether the phenomena and mechanisms found when studying one particular species are specialized or general and can thus help illuminate essential principles that apply widely. Moreover, as was found in developmental biology, where the body plan of animals is built using the same logic and molecular toolbox (<xref ref-type="bibr" rid="bib39">Nüsslein-Volhard and Wieschaus, 1980</xref>), it is possible that the function and structure of neural circuits could also be evolutionarily conserved among animals. Therefore, early diverging metazoans could provide an exciting opportunity to understand the fundamental mechanisms by which nervous systems generate and regulate behaviors.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title><italic>Hydra</italic> behavior dataset</title><p>The <italic>Hydra</italic> behavior dataset consisted of 53 videos from 53 <italic>Hydra</italic> with an average length of 30 min. The AEP strain of <italic>Hydra</italic> was used for all experiments. <italic>Hydra</italic> polyps were maintained at 18°C in darkness and were fed with <italic>Artemia</italic> nauplii once or more times a week by standard methods (<xref ref-type="bibr" rid="bib30">Lenhoff and Brown, 1970</xref>). During imaging, <italic>Hydra</italic> polyps were placed in a 3.5 cm plastic petri dish under a dissecting microscope (Leica M165) equipped with a sCMOS camera (Hamamatsu ORCA-Flash 4.0). Videos were recorded at 5 Hz. <italic>Hydra</italic> polyps were allowed to behave either undisturbed, or in the presence with reduced L-glutathione (Sigma-Aldrich, G4251-5G) to induce feeding behavior, since <italic>Hydra</italic> does not exhibit feeding behavior in the absence of prey.</p></sec><sec id="s4-2"><title>Manual annotation</title><p>Each video in the <italic>Hydra</italic> behavior dataset was examined manually at a high playback speed, and each frame in the video was assigned a label in the following eleven classes based on the behavior that <italic>Hydra</italic> was performing: silent, elongation, tentacle swaying, body swaying, bending, contraction, somersaulting, tentacle writhing of feeding, ball formation of feeding, mouth opening of feeding, and a none class. These behaviors were labeled as 1 through 11, where larger numbers correspond to more prominent behaviors, and the none class is labeled as 0. To generate manual labels for a given time window, the top two most frequent labels, L<sub>1</sub> and L<sub>2</sub>, within this time window were identified. The window was assigned as L<sub>2</sub> if its count exceed L<sub>1</sub> by three-fold and if L<sub>1</sub> is more prominent than L<sub>2</sub>; otherwise, the window was assigned as L<sub>1</sub>. This annotation method labels time windows as more prominent behaviors if behaviors with large motion, e.g. contraction, happens in only a few frames, while the majority of frames are slow behaviors.</p></sec><sec id="s4-3"><title>Video pre-processing</title><p>Prior work has shown that the bag of words methods for video action classification perform better when encoding spatial structure (<xref ref-type="bibr" rid="bib55">Taralova et al., 2011</xref>; <xref ref-type="bibr" rid="bib61">Wang et al., 2009</xref>). Encoding spatial information is especially important in our case because allowing the animal to move freely produces large variations in orientation, which is not related to behavior classification. Therefore, we performed a basic image registration procedure that keeps the motion information invariant, but aligns the <italic>Hydra</italic> region to a canonical scale and orientation. This involves three steps: background segmentation, registration, and body part segmentation. In brief, the image background was calculated by a morphological opening operation, and the background was removed from the raw image. Then, image contrast was adjusted to enhance tentacle identification. Images were then segmented by clustering the pixel intensity profiles to three clusters corresponding to <italic>Hydra</italic> body, weak-intensity tentacle regions and background by k-means, and the largest cluster from the result was treated as background, and the other two clusters as foreground, that is <italic>Hydra</italic> region. Connected components that occupied less than 0.25% of total image area in this binary image were removed as noise, and the resulting <italic>Hydra</italic> mask was then dilated by three pixels. To detect the body column, the background-removed image was convolved with a small 3-by-3 Gaussian filter with sigma equals one pixel, and the filtered image was thresholded with Otsu’s segmentation algorithm. The binarization was repeated with a new threshold defined with Otsu’s method within the previous above-threshold region, and the resulting binary mask was considered as the body column. The body column region was then fitted with an ellipse; the major axis, centroid, and orientation of the ellipse were noted. To determine the orientation, two small square masks were placed on both ends of the ellipse along the major axis, and the area of the <italic>Hydra</italic> region excluding the body column under the patch was calculated; the end with the larger area was defined as the tentacle/mouth region, and the end with the smaller area was defined as the foot region. To separate the <italic>Hydra</italic> region into three body parts, the part under the upper body square mask excluding the body column was defined as the tentacle region, and the rest of the mask was split at the minor axis of the ellipse; the part close to the tentacle region was defined as the upper body region, and the other as the lower body region. This step has shown to improve representation efficiency (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>).</p><p>Each 5-s video clip was then centered by calculating the average ellipse centroid position and centering it. The average major axis length and the average orientation were also calculated. Each image in the video clip was rotated according to the average orientation to make the <italic>Hydra</italic> vertical, and was scaled to make the length of the <italic>Hydra</italic> body 100 pixels, with an output size of 300 by 300 pixels, while only keeping the region under the <italic>Hydra</italic> binary mask.</p></sec><sec id="s4-4"><title>Feature extraction</title><p>Video features including HOF, HOG and MBH were extracted using a codebase that was previously released (<xref ref-type="bibr" rid="bib60">Wang et al., 2011</xref>). Briefly, interest points were densely sampled with five pixels spacing at each time point in each 5 s video clip and were then tracked throughout the video clip with optical flow for 15 frames. The tracking quality threshold was set to 0.01; the minimum variation of trajectory displacement was set to 0.1, the maximum variation was set to 50, and the maximum displacement was set to 50. The neighboring 32 pixels of each interest point were then extracted, and HOF (8 dimensions for eight orientations plus one extra zero bin), HOG (eight dimensions) and MBH (eight dimensions) features were calculated with standard procedures. Note that MBH was calculated for horizontal and vertical optical flow separately, therefore two sets of MBH features, MBHx and MBHy were generated. All features were placed into three groups based on the part of body they fall in, that is tentacles, upper body column, and lower body column. All parameters above were cross-validated with the training and test datasets.</p></sec><sec id="s4-5"><title>Gaussian mixture codebook and Fisher vector</title><p>A Gaussian mixture codebook and Fisher vectors were generated using the code developed by Jegou et al. for each feature type (<xref ref-type="bibr" rid="bib21">Jégou et al., 2012</xref>), using 50 <italic>Hydra</italic> in the behavior dataset that includes all behavior types. Features from each body part were centered at zero, then PCA was performed on centered features from all three body parts, keeping half of the original dimension (five for HOF, four for HOG, MBHx and MBHy). Whitening was performed on the PCA data as following, which de-correlates the data and removes redundant information:<disp-formula id="equ1"><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>,</mml:mo> <mml:mi mathvariant="normal"/><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>√</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula>where <inline-formula><mml:math id="inf1"><mml:mi>x</mml:mi></mml:math></inline-formula> denotes principal components, and <inline-formula><mml:math id="inf2"><mml:mi>λ</mml:mi></mml:math></inline-formula> denotes eigenvalues. <inline-formula><mml:math id="inf3"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:math></inline-formula> Gaussian mixtures were then fitted with the whitened data using a subset of 256,000 data points. We then calculated the Fisher vectors as following:<disp-formula id="equ2"><mml:math id="m2"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow> <mml:mi/><mml:mo>∇</mml:mo></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub> <mml:mi/><mml:mi mathvariant="normal">L</mml:mi><mml:mfenced close="|" separators="|"><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mfenced><mml:mi>λ</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="inf4"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn> <mml:mi/><mml:mo>…</mml:mo> <mml:mi/><mml:mi>T</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> is a set of <inline-formula><mml:math id="inf5"><mml:mi>T</mml:mi></mml:math></inline-formula> data points that were assumed to be generated with Gaussian distributions <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf7"><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> denotes the Gaussian parameters, and <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the decomposed Fisher Information Matrix:<disp-formula id="equ3"><mml:math id="m3"><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow> <mml:mi/><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>≡</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>~</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mo>∇</mml:mo></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mo>∇</mml:mo></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Fisher vectors then represent the normalized gradient vector obtained from Fisher kernel <inline-formula><mml:math id="inf9"><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi>'</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>:<disp-formula id="equ4"><mml:math id="m4"><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi>'</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow> <mml:mi/><mml:mo>∇</mml:mo></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub> <mml:mi/><mml:mi mathvariant="normal">L</mml:mi><mml:mfenced close="|" separators="|"><mml:mrow> <mml:mi mathvariant="normal"/><mml:mi mathvariant="normal">X</mml:mi> <mml:mi mathvariant="normal"/></mml:mrow></mml:mfenced> <mml:mi/><mml:mi>λ</mml:mi> <mml:mi/><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup> <mml:mi/><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow> <mml:mi/><mml:mo>∇</mml:mo></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub> <mml:mi/><mml:mi mathvariant="normal">L</mml:mi><mml:mfenced close="|" separators="|"><mml:mrow> <mml:mi mathvariant="normal"/><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">'</mml:mi> <mml:mi mathvariant="normal"/></mml:mrow></mml:mfenced> <mml:mi/><mml:mi>λ</mml:mi> <mml:mi/><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Comparing with hard-assigning each feature to a code word, the Gaussian mixtures can be regarded as probabilistic vocabulary, and Fisher vectors encode information of both the position and the shape of each word with respect to the Gaussian mixtures. Power normalization was then performed on the Fisher vectors to improve the quality of representation:<disp-formula id="equ5"><mml:math id="m5"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo> <mml:mi/><mml:mo>=</mml:mo> <mml:mi/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced close="|" open="|" separators="|"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula>with <inline-formula><mml:math id="inf10"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, followed by <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> normalization, which removes scale dependence (<xref ref-type="bibr" rid="bib43">Perronnin et al., 2010</xref>). The final representation of each video clip is a concatenation of Fisher vectors of HOF, HOG, MBHx and MBHy. In this paper, the GMM size was set to 128 with cross-validation (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1c</xref>).</p></sec><sec id="s4-6"><title>SVM classification</title><p>PCA was first performed on the concatenated Fisher vectors to reduce the dimensions while keeping 90% of the original variance. A random 90% of samples from the 50 training <italic>Hydra</italic> were selected as training data, and the remaining 10% were withheld as validation data. Another three <italic>Hydra</italic> that exhibit all behavior types were kept as test data. Because each behavior type has different numbers of data points, we trained SVM classifiers using the libSVM implementation (<xref ref-type="bibr" rid="bib7">Chang and Lin, 2011</xref>) by assigning each type a weight of <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf13"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>7</mml:mn></mml:math></inline-formula> denotes the behavior type, and <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the number of data points that belong to type <inline-formula><mml:math id="inf15"><mml:mi>i</mml:mi></mml:math></inline-formula>. We trained SVM classifiers with a radial basis kernel, allowing probability estimate, and a fivefold cross-validation testing the cost parameter <inline-formula><mml:math id="inf16"><mml:mi>c</mml:mi></mml:math></inline-formula> with a range of <inline-formula><mml:math id="inf17"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn><mml:mo>:</mml:mo><mml:mn>2</mml:mn><mml:mo>:</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, and the <inline-formula><mml:math id="inf18"><mml:mi>g</mml:mi></mml:math></inline-formula> in the kernel function with a range of <inline-formula><mml:math id="inf19"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn><mml:mo>:</mml:mo><mml:mn>2</mml:mn><mml:mo>:</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, where <inline-formula><mml:math id="inf20"><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn><mml:mo>:</mml:mo><mml:mn>2</mml:mn><mml:mo>:</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> denotes integers ranging from −5 to 15 with a step of 2. The best parameter combination from cross-validation was chosen to train the SVM classifiers.</p><p>To classify test data, features were extracted as above and were encoded with Fisher vectors with the codebook generated from the training data. PCA was performed using the projection matrix from training data. A probability estimate for each behavior type was given by the classifiers, and the final assigned label is the classifier with the highest probability. For soft classifications, we allowed up to three labels for each sample if the second highest label probability is &gt;50% of the highest label, and the third is &gt;50% of the second highest label. To evaluate classification performance, true positives (<inline-formula><mml:math id="inf21"><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:math></inline-formula>), false positives (<inline-formula><mml:math id="inf22"><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:math></inline-formula>), true negatives (<inline-formula><mml:math id="inf23"><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:math></inline-formula>) and false negatives (<inline-formula><mml:math id="inf24"><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:math></inline-formula>) were calculated. Accuracy was defined as <inline-formula><mml:math id="inf25"><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>; precision was defined as <inline-formula><mml:math id="inf26"><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>; recall was defined as <inline-formula><mml:math id="inf27"><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Two other measurements were calculated: true positive rate <inline-formula><mml:math id="inf28"><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, and false-positive rate <inline-formula><mml:math id="inf29"><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Plotting TPR against FPR gives the standard ROC curve, and the area under curve (AUC) reflects the performance of classification. In this plot, a straight line TPR = FPR with AUC = 0.5 represents random guess; the upper left quadrant with AUC &gt;0.5 represents better performance than random.</p></sec><sec id="s4-7"><title>t-SNE embedding</title><p>Embedding was performed with the dimension-reduced data. A random 80% of the dataset from the 50 training <italic>Hydra</italic> were chosen to generate the embedding map, and the remaining 20% were withheld as validation dataset. Three other <italic>Hydra</italic> were used as test dataset. We followed the procedures of <xref ref-type="bibr" rid="bib3">Berman et al. (2014</xref>), with a slight modification that uses Euclidean distance as the distance measurement. Embedding perplexity was chosen as 16. To generate a density map, a probability density function was calculated in the embedding space by convolving the embedded points with a Gaussian kernel; <inline-formula><mml:math id="inf30"><mml:mi>σ</mml:mi></mml:math></inline-formula> of the Gaussian was chosen to be 1/40 of the maximum value in the embedding space by cross-validation with human examination to minimize over-segmentation. In the 3-day dataset, <inline-formula><mml:math id="inf31"><mml:mi>σ</mml:mi></mml:math></inline-formula> was chosen to be 1/60 of the maximum value in order to reveal finer structures. To segment the density map, peaks were found in the density map, a binary map containing peak positions was generated, and peak points were dilated by three pixels. A distance map of the binary image was generated and inverted, and the peak positions were set to be minimum. Watershed was performed on the inverted distance map, and the boundaries were defined with the resulting watershed segmentation.</p></sec><sec id="s4-8"><title>Egestion detection</title><p>Estimated egestion time points were calculated by first extracting the width profile of <italic>Hydra</italic> from the pre-processing step, then filtering the width profile by taking the mean width during 15 min after each time point <inline-formula><mml:math id="inf32"><mml:mi>t</mml:mi></mml:math></inline-formula>, and the mean width during 15 min before time <inline-formula><mml:math id="inf33"><mml:mi>t</mml:mi></mml:math></inline-formula>, and subtracting the former from the latter. Peaks were detected on the resulting trace and were regarded as egestion behaviors, since they represent a sharp decrease in the thickness of the animals.</p></sec><sec id="s4-9"><title>Behavior experiments</title><p>All <italic>Hydra</italic> used for experiments were fed three times a week and were cultured at 18°C. On non-feeding days, the culture medium was changed. <italic>Hydra viridissima</italic> was cultured at room temperature under sunlight coming through the laboratory windows. For imaging, animals were placed in a petri dish under the microscope without disturbance to habituate for at least 30 min. Imaging typically started between 7 pm and 9 pm, and ended between 9 am and 11 am except for the large/small experiments. All imagings were done excluding environmental light by putting a black curtain around the microscope. For dark condition, a longpass filter with a cutoff frequency of 650 nm (Thorlabs, FEL0650) was placed at the source light path to create ‘<italic>Hydra</italic> darkness’ (<xref ref-type="bibr" rid="bib41">Passano and McCullough, 1962</xref>). For starved condition, <italic>Hydra</italic> were fed once a week. For the large/small experiment, <italic>Hydra</italic> buds that were detached from their parents within 3 days were chosen as small <italic>Hydra</italic>, and mature post-budding mature <italic>Hydra</italic> polyps were chosen as large <italic>Hydra</italic>. There was a two- to threefold size difference between small and large <italic>Hydra</italic> when they were relaxed. However, since the <italic>Hydra</italic> body was constantly contracting and elongating, it was difficult to measure the exact size. Imaging for this experiment was done during the day time for 1 hr per <italic>Hydra</italic>.</p></sec><sec id="s4-10"><title>Statistical analysis</title><p>All statistical analyses were done using Wilcoxon rank-sum test unless otherwise indicated. Data is represented by mean ± S.E.M unless otherwise indicated.</p></sec><sec id="s4-11"><title>Resource availability</title><p>The code for the method developed in this paper is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/hanshuting/Hydra_behavior">https://github.com/hanshuting/Hydra_behavior</ext-link>. A copy is archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/hydra_behavior">https://github.com/elifesciences-publications/hydra_behavior</ext-link> (<xref ref-type="bibr" rid="bib20">Han, 2018b</xref>). The annotated behavior dataset is available on Academic Commons (<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7916/D8WH41ZR">dx.doi.org/10.7916/D8WH41ZR</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Drs. Robert Steele, Charles David, and Adrienne Fairhall for discussions. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-17-C-0026. SH is a Howard Hughes Medical Institute International Student Research fellow. This work was partly supported by the Grass Fellowship (CD) during the summer of 2016, and CD would like to thank the Director, Associate Director, members of the Grass laboratory and Grass Foundation for their generous feedback and support. RY was a Whitman fellow at the Marine Biological Laboratory and this Hydra research was also supported in part by competitive fellowship funds from the H Keffer Hartline, Edward F MacNichol, Jr. Fellowship Fund, and the E E Just Endowed Research Fellowship Fund, Lucy B. Lemann Fellowship Fund, and Frank R. Lillie Fellowship Fund of the Marine Biological Laboratory in Woods Hole, MA. The authors declare no competing financial interests.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation, Investigation, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Writing—review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><sec id="s7" sec-type="datasets"><title>Major datasets</title><p>The following dataset was generated:</p><p><related-object content-type="generated-dataset" id="dataset1" source-id="http://dx.doi.org/10.5061/dryad.f6v067r" source-id-type="uri"><collab collab-type="author">Han S</collab><year>2018</year><source>Hydra behavior dataset</source><ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.f6v067r">http://dx.doi.org/10.5061/dryad.f6v067r</ext-link><comment>Available at Dryad Digital Repository under a CC0 Public Domain Dedication.</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname> <given-names>MB</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Robson</surname> <given-names>DN</given-names></name><name><surname>Li</surname> <given-names>JM</given-names></name><name><surname>Keller</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whole-brain functional imaging at cellular resolution using light-sheet microscopy</article-title><source>Nature Methods</source><volume>10</volume><fpage>413</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2434</pub-id><pub-id pub-id-type="pmid">23524393</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>DJ</given-names></name><name><surname>Perona</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Toward a science of computational ethology</article-title><source>Neuron</source><volume>84</volume><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.005</pub-id><pub-id pub-id-type="pmid">25277452</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname> <given-names>GJ</given-names></name><name><surname>Choi</surname> <given-names>DM</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title><source>Journal of the Royal Society Interface</source><volume>11</volume><elocation-id>20140672</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2014.0672</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosch</surname> <given-names>TCG</given-names></name><name><surname>Klimovich</surname> <given-names>A</given-names></name><name><surname>Domazet-Lošo</surname> <given-names>T</given-names></name><name><surname>Gründer</surname> <given-names>S</given-names></name><name><surname>Holstein</surname> <given-names>TW</given-names></name><name><surname>Jékely</surname> <given-names>G</given-names></name><name><surname>Miller</surname> <given-names>DJ</given-names></name><name><surname>Murillo-Rincon</surname> <given-names>AP</given-names></name><name><surname>Rentzsch</surname> <given-names>F</given-names></name><name><surname>Richards</surname> <given-names>GS</given-names></name><name><surname>Schröder</surname> <given-names>K</given-names></name><name><surname>Technau</surname> <given-names>U</given-names></name><name><surname>Yuste</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Back to the basics: cnidarians start to fire</article-title><source>Trends in Neurosciences</source><volume>40</volume><fpage>92</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2016.11.005</pub-id><pub-id pub-id-type="pmid">28041633</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Branson</surname> <given-names>K</given-names></name><name><surname>Robie</surname> <given-names>AA</given-names></name><name><surname>Bender</surname> <given-names>J</given-names></name><name><surname>Perona</surname> <given-names>P</given-names></name><name><surname>Dickinson</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>High-throughput ethomics in large groups of Drosophila</article-title><source>Nature Methods</source><volume>6</volume><fpage>451</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1328</pub-id><pub-id pub-id-type="pmid">19412169</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname> <given-names>AE</given-names></name><name><surname>Yemini</surname> <given-names>EI</given-names></name><name><surname>Grundy</surname> <given-names>LJ</given-names></name><name><surname>Jucikas</surname> <given-names>T</given-names></name><name><surname>Schafer</surname> <given-names>WR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A dictionary of behavioral motifs reveals clusters of genes affecting Caenorhabditis elegans locomotion</article-title><source>PNAS</source><volume>110</volume><fpage>791</fpage><lpage>796</lpage><pub-id pub-id-type="doi">10.1073/pnas.1211447110</pub-id><pub-id pub-id-type="pmid">23267063</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>C-C</given-names></name><name><surname>Lin</surname> <given-names>C-J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIBSVM: A library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chapman</surname> <given-names>JA</given-names></name><name><surname>Kirkness</surname> <given-names>EF</given-names></name><name><surname>Simakov</surname> <given-names>O</given-names></name><name><surname>Hampson</surname> <given-names>SE</given-names></name><name><surname>Mitros</surname> <given-names>T</given-names></name><name><surname>Weinmaier</surname> <given-names>T</given-names></name><name><surname>Rattei</surname> <given-names>T</given-names></name><name><surname>Balasubramanian</surname> <given-names>PG</given-names></name><name><surname>Borman</surname> <given-names>J</given-names></name><name><surname>Busam</surname> <given-names>D</given-names></name><name><surname>Disbennett</surname> <given-names>K</given-names></name><name><surname>Pfannkoch</surname> <given-names>C</given-names></name><name><surname>Sumin</surname> <given-names>N</given-names></name><name><surname>Sutton</surname> <given-names>GG</given-names></name><name><surname>Viswanathan</surname> <given-names>LD</given-names></name><name><surname>Walenz</surname> <given-names>B</given-names></name><name><surname>Goodstein</surname> <given-names>DM</given-names></name><name><surname>Hellsten</surname> <given-names>U</given-names></name><name><surname>Kawashima</surname> <given-names>T</given-names></name><name><surname>Prochnik</surname> <given-names>SE</given-names></name><name><surname>Putnam</surname> <given-names>NH</given-names></name><name><surname>Shu</surname> <given-names>S</given-names></name><name><surname>Blumberg</surname> <given-names>B</given-names></name><name><surname>Dana</surname> <given-names>CE</given-names></name><name><surname>Gee</surname> <given-names>L</given-names></name><name><surname>Kibler</surname> <given-names>DF</given-names></name><name><surname>Law</surname> <given-names>L</given-names></name><name><surname>Lindgens</surname> <given-names>D</given-names></name><name><surname>Martinez</surname> <given-names>DE</given-names></name><name><surname>Peng</surname> <given-names>J</given-names></name><name><surname>Wigge</surname> <given-names>PA</given-names></name><name><surname>Bertulat</surname> <given-names>B</given-names></name><name><surname>Guder</surname> <given-names>C</given-names></name><name><surname>Nakamura</surname> <given-names>Y</given-names></name><name><surname>Ozbek</surname> <given-names>S</given-names></name><name><surname>Watanabe</surname> <given-names>H</given-names></name><name><surname>Khalturin</surname> <given-names>K</given-names></name><name><surname>Hemmrich</surname> <given-names>G</given-names></name><name><surname>Franke</surname> <given-names>A</given-names></name><name><surname>Augustin</surname> <given-names>R</given-names></name><name><surname>Fraune</surname> <given-names>S</given-names></name><name><surname>Hayakawa</surname> <given-names>E</given-names></name><name><surname>Hayakawa</surname> <given-names>S</given-names></name><name><surname>Hirose</surname> <given-names>M</given-names></name><name><surname>Hwang</surname> <given-names>JS</given-names></name><name><surname>Ikeo</surname> <given-names>K</given-names></name><name><surname>Nishimiya-Fujisawa</surname> <given-names>C</given-names></name><name><surname>Ogura</surname> <given-names>A</given-names></name><name><surname>Takahashi</surname> <given-names>T</given-names></name><name><surname>Steinmetz</surname> <given-names>PR</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Aufschnaiter</surname> <given-names>R</given-names></name><name><surname>Eder</surname> <given-names>MK</given-names></name><name><surname>Gorny</surname> <given-names>AK</given-names></name><name><surname>Salvenmoser</surname> <given-names>W</given-names></name><name><surname>Heimberg</surname> <given-names>AM</given-names></name><name><surname>Wheeler</surname> <given-names>BM</given-names></name><name><surname>Peterson</surname> <given-names>KJ</given-names></name><name><surname>Böttger</surname> <given-names>A</given-names></name><name><surname>Tischler</surname> <given-names>P</given-names></name><name><surname>Wolf</surname> <given-names>A</given-names></name><name><surname>Gojobori</surname> <given-names>T</given-names></name><name><surname>Remington</surname> <given-names>KA</given-names></name><name><surname>Strausberg</surname> <given-names>RL</given-names></name><name><surname>Venter</surname> <given-names>JC</given-names></name><name><surname>Technau</surname> <given-names>U</given-names></name><name><surname>Hobmayer</surname> <given-names>B</given-names></name><name><surname>Bosch</surname> <given-names>TC</given-names></name><name><surname>Holstein</surname> <given-names>TW</given-names></name><name><surname>Fujisawa</surname> <given-names>T</given-names></name><name><surname>Bode</surname> <given-names>HR</given-names></name><name><surname>David</surname> <given-names>CN</given-names></name><name><surname>Rokhsar</surname> <given-names>DS</given-names></name><name><surname>Steele</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The dynamic genome of Hydra</article-title><source>Nature</source><volume>464</volume><fpage>592</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1038/nature08830</pub-id><pub-id pub-id-type="pmid">20228792</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>TW</given-names></name><name><surname>Wardill</surname> <given-names>TJ</given-names></name><name><surname>Sun</surname> <given-names>Y</given-names></name><name><surname>Pulver</surname> <given-names>SR</given-names></name><name><surname>Renninger</surname> <given-names>SL</given-names></name><name><surname>Baohan</surname> <given-names>A</given-names></name><name><surname>Schreiter</surname> <given-names>ER</given-names></name><name><surname>Kerr</surname> <given-names>RA</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name><name><surname>Looger</surname> <given-names>LL</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Kim</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title><source>Nature</source><volume>499</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1038/nature12354</pub-id><pub-id pub-id-type="pmid">23868258</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Creton</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Automated analysis of behavior in zebrafish larvae</article-title><source>Behavioural Brain Research</source><volume>203</volume><fpage>127</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2009.04.030</pub-id><pub-id pub-id-type="pmid">19409932</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dalal</surname> <given-names>N</given-names></name><name><surname>Triggs</surname> <given-names>B</given-names></name><name><surname>Schmid</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2006">2006</year><chapter-title>Human detection using oriented histograms of flow and appearance</chapter-title><source>Lecture Notes in Computer Science</source><publisher-name>Springer-Verlag</publisher-name><fpage>428</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1007/11744047_33</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dalal</surname> <given-names>N</given-names></name><name><surname>Triggs</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Histograms of oriented gradients for human detection</article-title><conf-name>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), (IEEE)</conf-name><fpage>886</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2005.177</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dankert</surname> <given-names>H</given-names></name><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Hoopfer</surname> <given-names>ED</given-names></name><name><surname>Anderson</surname> <given-names>DJ</given-names></name><name><surname>Perona</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Automated monitoring and analysis of social behavior in Drosophila</article-title><source>Nature Methods</source><volume>6</volume><fpage>297</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1310</pub-id><pub-id pub-id-type="pmid">19270697</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dupre</surname> <given-names>C</given-names></name><name><surname>Yuste</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Non-overlapping neural networks in hydra vulgaris</article-title><source>Current Biology</source><volume>27</volume><fpage>1085</fpage><lpage>1097</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.02.049</pub-id><pub-id pub-id-type="pmid">28366745</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egnor</surname> <given-names>SE</given-names></name><name><surname>Branson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational analysis of behavior</article-title><source>Annual Review of Neuroscience</source><volume>39</volume><fpage>217</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013845</pub-id><pub-id pub-id-type="pmid">27090952</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujisawa</surname> <given-names>T</given-names></name><name><surname>Hayakawa</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Peptide signaling in Hydra</article-title><source>The International Journal of Developmental Biology</source><volume>56</volume><fpage>543</fpage><lpage>550</lpage><pub-id pub-id-type="doi">10.1387/ijdb.113477tf</pub-id><pub-id pub-id-type="pmid">22689368</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallagher</surname> <given-names>T</given-names></name><name><surname>Bjorness</surname> <given-names>T</given-names></name><name><surname>Greene</surname> <given-names>R</given-names></name><name><surname>You</surname> <given-names>YJ</given-names></name><name><surname>Avery</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The geometry of locomotive behavioral states in C. elegans</article-title><source>PLoS One</source><volume>8</volume><elocation-id>e59865</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0059865</pub-id><pub-id pub-id-type="pmid">23555813</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Haddad</surname> <given-names>SA</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Circuit robustness to temperature perturbation is altered by neuromodulators</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/178764</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Han</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018a</year><data-title>Hydra behavior dataset</data-title><source>Columbia Academic Commons</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7916/D8WH41ZR">https://doi.org/10.7916/D8WH41ZR</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Han</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>hydra_behavior</article-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/hanshuting/hydra_behavior">https://github.com/hanshuting/hydra_behavior</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jégou</surname> <given-names>H</given-names></name><name><surname>Perronnin</surname> <given-names>F</given-names></name><name><surname>Douze</surname> <given-names>M</given-names></name><name><surname>Sánchez</surname> <given-names>J</given-names></name><name><surname>Pérez</surname> <given-names>P</given-names></name><name><surname>Schmid</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Aggregating local image descriptors into compact codes</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>34</volume><fpage>1704</fpage><lpage>1716</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2011.235</pub-id><pub-id pub-id-type="pmid">22156101</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname> <given-names>L</given-names></name><name><surname>Han</surname> <given-names>Z</given-names></name><name><surname>Platisa</surname> <given-names>J</given-names></name><name><surname>Wooltorton</surname> <given-names>JR</given-names></name><name><surname>Cohen</surname> <given-names>LB</given-names></name><name><surname>Pieribone</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Single action potentials and subthreshold electrical events imaged in neurons with a fluorescent protein voltage probe</article-title><source>Neuron</source><volume>75</volume><fpage>779</fpage><lpage>785</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.06.040</pub-id><pub-id pub-id-type="pmid">22958819</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>MJ</given-names></name><name><surname>Duvenaud</surname> <given-names>D</given-names></name><name><surname>Wiltschko</surname> <given-names>AB</given-names></name><name><surname>Datta</surname> <given-names>SR</given-names></name><name><surname>Adams</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Composing graphical models with neural networks for structured representations and fast inference</article-title><source>Advances in Neural Information Processing Systems</source><volume>29</volume><fpage>514</fpage><lpage>521</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabra</surname> <given-names>M</given-names></name><name><surname>Robie</surname> <given-names>AA</given-names></name><name><surname>Rivera-Alba</surname> <given-names>M</given-names></name><name><surname>Branson</surname> <given-names>S</given-names></name><name><surname>Branson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>JAABA: interactive machine learning for automatic annotation of animal behavior</article-title><source>Nature Methods</source><volume>10</volume><fpage>64</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2281</pub-id><pub-id pub-id-type="pmid">23202433</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kato</surname> <given-names>S</given-names></name><name><surname>Kaplan</surname> <given-names>HS</given-names></name><name><surname>Schrödel</surname> <given-names>T</given-names></name><name><surname>Skora</surname> <given-names>S</given-names></name><name><surname>Lindsay</surname> <given-names>TH</given-names></name><name><surname>Yemini</surname> <given-names>E</given-names></name><name><surname>Lockery</surname> <given-names>S</given-names></name><name><surname>Zimmer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Global brain dynamics embed the motor command sequence of Caenorhabditis elegans</article-title><source>Cell</source><volume>163</volume><fpage>656</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.09.034</pub-id><pub-id pub-id-type="pmid">26478179</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ke</surname> <given-names>Y</given-names></name><name><surname>Sukthankar</surname> <given-names>R</given-names></name><name><surname>Hebert</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatio-temporal shape and flow correlation for action recognition</article-title><conf-name><italic>In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</italic></conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2007.383512</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koizumi</surname> <given-names>O</given-names></name><name><surname>Haraguchi</surname> <given-names>Y</given-names></name><name><surname>Ohuchida</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Reaction chain in feeding behavior of hydra : different speeificities of three feeding responses</article-title><source>Journal of Comparative Physiology. A, Sensory, Neural, and Behavioral Physiology</source><pub-id pub-id-type="doi">10.1007/BF00605293</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kralj</surname> <given-names>JM</given-names></name><name><surname>Douglass</surname> <given-names>AD</given-names></name><name><surname>Hochbaum</surname> <given-names>DR</given-names></name><name><surname>Maclaurin</surname> <given-names>D</given-names></name><name><surname>Cohen</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Optical recording of action potentials in mammalian neurons using a microbial rhodopsin</article-title><source>Nature Methods</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1782</pub-id><pub-id pub-id-type="pmid">22120467</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Laptev</surname> <given-names>I</given-names></name><name><surname>Marszalek</surname> <given-names>M</given-names></name><name><surname>Schmid</surname> <given-names>C</given-names></name><name><surname>Rozenfeld</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Learning realistic human actions from movies</article-title><conf-name><italic>2008 IEEE Conference on Computer Vision and Pattern Recognition, (IEEE)</italic></conf-name><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2008.4587756</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lenhoff</surname> <given-names>HM</given-names></name><name><surname>Brown</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Mass culture of hydra: an improved method and its application to other aquatic invertebrates</article-title><source>Laboratory Animals</source><volume>4</volume><fpage>139</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1258/002367770781036463</pub-id><pub-id pub-id-type="pmid">5527814</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lenhoff</surname> <given-names>HM</given-names></name><name><surname>Loomis</surname> <given-names>WF</given-names></name></person-group><year iso-8601-date="1961">1961</year><source>The Biology of Hydra: And of Some Other Coelenterates</source></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lenhoff</surname> <given-names>HM</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Behavior, hormones, and hydra</article-title><source>Science</source><volume>161</volume><fpage>434</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1126/science.161.3840.434</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lenhoff</surname> <given-names>SG</given-names></name><name><surname>Lenhoff</surname> <given-names>HM</given-names></name></person-group><year iso-8601-date="1986">1986</year><source>Hydra and the Birth of Experimental Biology - 1744</source><publisher-name>Boxwood Pr</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mackie</surname> <given-names>GO</given-names></name></person-group><year iso-8601-date="1974">1974</year><source>Coelenterate Biology: Reviews and New Perspectives</source><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuromodulation of neuronal circuits: back to the future</article-title><source>Neuron</source><volume>76</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.010</pub-id><pub-id pub-id-type="pmid">23040802</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martínez</surname> <given-names>DE</given-names></name><name><surname>Iñiguez</surname> <given-names>AR</given-names></name><name><surname>Percell</surname> <given-names>KM</given-names></name><name><surname>Willner</surname> <given-names>JB</given-names></name><name><surname>Signorovitch</surname> <given-names>J</given-names></name><name><surname>Campbell</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Phylogeny and biogeography of Hydra (Cnidaria: Hydridae) using mitochondrial and nuclear DNA sequences</article-title><source>Molecular Phylogenetics and Evolution</source><volume>57</volume><fpage>403</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.ympev.2010.06.016</pub-id><pub-id pub-id-type="pmid">20601008</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Matikainen</surname> <given-names>P</given-names></name><name><surname>Hebert</surname> <given-names>M</given-names></name><name><surname>Sukthankar</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Trajectons: Action recognition through the motion analysis of tracked features</article-title><conf-name><italic>2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops 2009, (IEEE)</italic></conf-name><fpage>514</fpage><lpage>521</lpage></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname> <given-names>JP</given-names></name><name><surname>Shipley</surname> <given-names>FB</given-names></name><name><surname>Linder</surname> <given-names>AN</given-names></name><name><surname>Plummer</surname> <given-names>GS</given-names></name><name><surname>Liu</surname> <given-names>M</given-names></name><name><surname>Setru</surname> <given-names>SU</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Leifer</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Whole-brain calcium imaging with cellular resolution in freely behaving Caenorhabditis elegans</article-title><source>PNAS</source><volume>113</volume><fpage>E1074</fpage><lpage>E1081</lpage><pub-id pub-id-type="doi">10.1073/pnas.1507110112</pub-id><pub-id pub-id-type="pmid">26712014</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nüsslein-Volhard</surname> <given-names>C</given-names></name><name><surname>Wieschaus</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Mutations affecting segment number and polarity in Drosophila</article-title><source>Nature</source><volume>287</volume><fpage>795</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1038/287795a0</pub-id><pub-id pub-id-type="pmid">6776413</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ogale</surname> <given-names>AS</given-names></name><name><surname>Karapurkar</surname> <given-names>A</given-names></name><name><surname>Aloimonos</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2007">2007</year><chapter-title>View-Invariant Modeling and Recognition of Human Actions Using Grammars</chapter-title><source>Dynamical Vision</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer Berlin Heidelberg</publisher-name><fpage>115</fpage><lpage>126</lpage></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Passano</surname> <given-names>LM</given-names></name><name><surname>McCullough</surname> <given-names>CB</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>The light response and the rhythmic potentials of hydra</article-title><source>PNAS</source><volume>48</volume><fpage>1376</fpage><lpage>1382</lpage><pub-id pub-id-type="doi">10.1073/pnas.48.8.1376</pub-id><pub-id pub-id-type="pmid">16590985</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Passano</surname> <given-names>LM</given-names></name><name><surname>McCullough</surname> <given-names>CB</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Co-ordinating systems and behaviour in hydra: i. pacemaker system of the periodic contractions</article-title><source>The Journal of Experimental Biology</source><volume>41</volume><fpage>643</fpage><lpage>664</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Perronnin</surname> <given-names>F</given-names></name><name><surname>Sánchez</surname> <given-names>J</given-names></name><name><surname>Mensink</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><chapter-title>Improving the Fisher Kernel for Large-Scale Image Classification</chapter-title><source>Lecture Notes in Computer Science</source><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-15561-1_11</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pérez-Escudero</surname> <given-names>A</given-names></name><name><surname>Vicente-Page</surname> <given-names>J</given-names></name><name><surname>Hinz</surname> <given-names>RC</given-names></name><name><surname>Arganda</surname> <given-names>S</given-names></name><name><surname>de Polavieja</surname> <given-names>GG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>idTracker: tracking individuals in a group by automatic identification of unmarked animals</article-title><source>Nature Methods</source><volume>11</volume><fpage>743</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2994</pub-id><pub-id pub-id-type="pmid">24880877</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poppe</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A survey on vision-based human action recognition</article-title><source>Image and Vision Computing</source><volume>28</volume><fpage>976</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2009.11.014</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prevedel</surname> <given-names>R</given-names></name><name><surname>Yoon</surname> <given-names>YG</given-names></name><name><surname>Hoffmann</surname> <given-names>M</given-names></name><name><surname>Pak</surname> <given-names>N</given-names></name><name><surname>Wetzstein</surname> <given-names>G</given-names></name><name><surname>Kato</surname> <given-names>S</given-names></name><name><surname>Schrödel</surname> <given-names>T</given-names></name><name><surname>Raskar</surname> <given-names>R</given-names></name><name><surname>Zimmer</surname> <given-names>M</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name><name><surname>Vaziri</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Simultaneous whole-animal 3D imaging of neuronal activity using light-field microscopy</article-title><source>Nature Methods</source><volume>11</volume><fpage>727</fpage><lpage>730</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2964</pub-id><pub-id pub-id-type="pmid">24836920</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robie</surname> <given-names>AA</given-names></name><name><surname>Hirokawa</surname> <given-names>J</given-names></name><name><surname>Edwards</surname> <given-names>AW</given-names></name><name><surname>Umayam</surname> <given-names>LA</given-names></name><name><surname>Lee</surname> <given-names>A</given-names></name><name><surname>Phillips</surname> <given-names>ML</given-names></name><name><surname>Card</surname> <given-names>GM</given-names></name><name><surname>Korff</surname> <given-names>W</given-names></name><name><surname>Rubin</surname> <given-names>GM</given-names></name><name><surname>Simpson</surname> <given-names>JH</given-names></name><name><surname>Reiser</surname> <given-names>MB</given-names></name><name><surname>Branson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mapping the Neural Substrates of Behavior</article-title><source>Cell</source><volume>170</volume><fpage>393</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.06.032</pub-id><pub-id pub-id-type="pmid">28709004</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schiff</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Neural Control Engineering: The Emerging Intersection Between Control Theory and Neuroscience</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sminchisescu</surname> <given-names>C</given-names></name><name><surname>Kanaujia</surname> <given-names>A</given-names></name><name><surname>Metaxas</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Conditional models for contextual human motion recognition</article-title><source>Computer Vision and Image Understanding</source><volume>104</volume><fpage>210</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2006.07.014</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname> <given-names>N</given-names></name><name><surname>Clark</surname> <given-names>DA</given-names></name><name><surname>Samuel</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Temporal analysis of stochastic turning behavior of swimming C. elegans</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>1172</fpage><lpage>1179</lpage><pub-id pub-id-type="doi">10.1152/jn.90952.2008</pub-id><pub-id pub-id-type="pmid">19535479</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>St-Pierre</surname> <given-names>F</given-names></name><name><surname>Marshall</surname> <given-names>JD</given-names></name><name><surname>Yang</surname> <given-names>Y</given-names></name><name><surname>Gong</surname> <given-names>Y</given-names></name><name><surname>Schnitzer</surname> <given-names>MJ</given-names></name><name><surname>Lin</surname> <given-names>MZ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>High-fidelity optical reporting of neuronal electrical activity with an ultrafast fluorescent voltage sensor</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>884</fpage><lpage>889</lpage><pub-id pub-id-type="doi">10.1038/nn.3709</pub-id><pub-id pub-id-type="pmid">24755780</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephens</surname> <given-names>GJ</given-names></name><name><surname>Johnson-Kerner</surname> <given-names>B</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Ryu</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dimensionality and dynamics in the behavior of C. elegans</article-title><source>PLoS Computational Biology</source><volume>4</volume><elocation-id>e1000028</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000028</pub-id><pub-id pub-id-type="pmid">18389066</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname> <given-names>J</given-names></name><name><surname>Wu</surname> <given-names>X</given-names></name><name><surname>Yan</surname> <given-names>S</given-names></name><name><surname>Cheong</surname> <given-names>L-F</given-names></name><name><surname>Chua</surname> <given-names>T-S</given-names></name><name><surname>Li</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Hierarchical spatio-temporal context modeling for action recognition</article-title><conf-name><italic>2009 IEEE Conference on Computer Vision and Pattern Recognition, (IEEE)</italic></conf-name><fpage>2004</fpage><lpage>2011</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206721</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swierczek</surname> <given-names>NA</given-names></name><name><surname>Giles</surname> <given-names>AC</given-names></name><name><surname>Rankin</surname> <given-names>CH</given-names></name><name><surname>Kerr</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>High-throughput behavioral analysis in C. elegans</article-title><source>Nature Methods</source><volume>8</volume><fpage>592</fpage><lpage>598</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1625</pub-id><pub-id pub-id-type="pmid">21642964</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Taralova</surname> <given-names>E</given-names></name><name><surname>De la Torre</surname> <given-names>F</given-names></name><name><surname>Hebert</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Source constrained clustering</article-title><conf-name><italic>2011 International Conference on Computer Vision</italic></conf-name><fpage>1927</fpage><lpage>1934</lpage></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>L</given-names></name><name><surname>Hires</surname> <given-names>SA</given-names></name><name><surname>Mao</surname> <given-names>T</given-names></name><name><surname>Huber</surname> <given-names>D</given-names></name><name><surname>Chiappe</surname> <given-names>ME</given-names></name><name><surname>Chalasani</surname> <given-names>SH</given-names></name><name><surname>Petreanu</surname> <given-names>L</given-names></name><name><surname>Akerboom</surname> <given-names>J</given-names></name><name><surname>McKinney</surname> <given-names>SA</given-names></name><name><surname>Schreiter</surname> <given-names>ER</given-names></name><name><surname>Bargmann</surname> <given-names>CI</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Looger</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Imaging neural activity in worms, flies and mice with improved GCaMP calcium indicators</article-title><source>Nature Methods</source><volume>6</volume><fpage>875</fpage><lpage>881</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1398</pub-id><pub-id pub-id-type="pmid">19898485</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Trembley</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1744">1744</year><source>Mémoires Pour Servir À l’Histoire D’un Genre De Polypes D’eau Douce, Bras en Forme De Cornes</source><publisher-loc>A Leide</publisher-loc><publisher-name>Chez Jean &amp; Herman Verbeek</publisher-name></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Der Maaten</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Learning a parametric embedding by preserving local structure</article-title><source>JMLR Proc</source><volume>5</volume><fpage>384</fpage><lpage>391</lpage></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venegas-Barrera</surname> <given-names>CS</given-names></name><name><surname>Manjarrez</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual Categorization with Bags of Keypoints</article-title><source>Revista Mexicana De Biodiversidad</source><volume>82</volume><fpage>179</fpage><lpage>191</lpage></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>H</given-names></name><name><surname>Kl</surname> <given-names>A</given-names></name><name><surname>Schmid</surname> <given-names>C</given-names></name><name><surname>Liu</surname> <given-names>C-L</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Action Recognition by Dense Trajectories</source><fpage>3169</fpage><lpage>3176</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2011.5995407</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>H</given-names></name><name><surname>Ullah</surname> <given-names>MM</given-names></name><name><surname>Klaser</surname> <given-names>A</given-names></name><name><surname>Laptev</surname> <given-names>I</given-names></name><name><surname>Schmid</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Evaluation of local spatio-temporal features for action recognition</article-title><conf-name>BMVC 2009 - Br. Mach. Vis. Conf</conf-name><fpage>124.1</fpage><lpage>12124</lpage></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Suter</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Recognizing human activities from silhouettes: motion subspace and factorial discriminative graphical model</article-title><conf-name><italic>2007 IEEE Conference on Computer Vision and Pattern Recognition, (IEEE)</italic></conf-name><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2007.383298</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname> <given-names>AB</given-names></name><name><surname>Johnson</surname> <given-names>MJ</given-names></name><name><surname>Iurilli</surname> <given-names>G</given-names></name><name><surname>Peterson</surname> <given-names>RE</given-names></name><name><surname>Katon</surname> <given-names>JM</given-names></name><name><surname>Pashkovski</surname> <given-names>SL</given-names></name><name><surname>Abraira</surname> <given-names>VE</given-names></name><name><surname>Adams</surname> <given-names>RP</given-names></name><name><surname>Datta</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping sub-second structure in mouse behavior</article-title><source>Neuron</source><volume>88</volume><fpage>1121</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id><pub-id pub-id-type="pmid">26687221</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamamoto</surname> <given-names>D</given-names></name><name><surname>Koganezawa</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Genes and circuits of courtship behaviour in Drosophila males</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>681</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1038/nrn3567</pub-id><pub-id pub-id-type="pmid">24052176</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuste</surname> <given-names>R</given-names></name><name><surname>Katz</surname> <given-names>LC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Control of postsynaptic Ca2+ influx in developing neocortex by excitatory and inhibitory neurotransmitters</article-title><source>Neuron</source><volume>6</volume><fpage>333</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1016/0896-6273(91)90243-S</pub-id><pub-id pub-id-type="pmid">1672071</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.32605.038</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Reviewing Editor</role><aff id="aff3"><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Comprehensive machine learning analysis of <italic>Hydra</italic> behavior reveals a stable behavioral repertoire&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers,, one of whom, Ronald Calabrese is a member of our Board of Reviewing Editors and the evaluation has been overseen by Eve Marder as the Senior Editor. The following individual involved in review of your submission has also agreed to reveal his identity: Gordon J Berman.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission</p><p>Summary:</p><p>This is an interesting manuscript that reports the development a novel behavioral analysis pipeline using approaches from computer vision to assess various natural behaviors compatible with variable observation angles. It extracts visual information from videos of animals with arbitrary motion and deformation. It then integrates a few modern computational methods, including dense trajectory features, Fisher vectors and t-SNE embedding for robust recognition and classification of Hydra behaviors with the bag-of-words framework. The pipeline, which uses both supervised and unsupervised techniques, is suitable for use not only with Hydra, as demonstrated here, but compared with previously developed methods this method is particularly suitable for behaviors in natural conditions that involve animals with deformable body shapes. The pipeline is used to describe behaviors in Hydra and is successful in identifying previously-identified behaviors and novel behaviors not previously identified. The paper then goes on to specify the frequency and variance of these behaviors under a variety of conditions (e.g. fed vs. unfed) and surprisingly found similar behavioral statistics. They conclude that the behavioral repertoire of Hydra is robust which may reflect homeostatic neural principles or a particularly stable ground state of the nervous system. Comparisons with another distantly related Hydra species interestingly reveal some strong differences.</p><p>Essential revisions:</p><p>The reviewers found that there was a substantial contribution to methodology and behavioral analyses of Hydra in this paper but had concerns that these contributions were obscured by the explanation of the methodology and the presentation and interpretation of the behavioral results. There concerns are well summarized in the thorough review discussion. Reviewer #2 commented &quot;I agree with the importance of quantifying Hydra behavior but have two reservations:</p><p>1) The choices for their particular pipeline need to be clarified and discussed. This includes Reviewer #3's concern on the 5-second window but extends to other choices in the stream. As machine learning techniques advance it is getting much easier to represent video information with numbers and I expect that as a result we will see many future advances in behavioral representation. However, these representations are often idiosyncratic and so it is important to understand what aspects are universal, or at the very least include a discussion about various choices. I also think it is important to discuss what kind of behavior might be missing in this approach.</p><p>2) They need to do a better job of motivating and discussing the important questions that their quantitative behavioral repertoire can answer. This is the science of behavior not simply representation of videos as numbers. And it's here that we can learn how Hydra compares to worms and flies what we might expect to find in the neural recordings.&quot;</p><p>Reviewer #3 then commented &quot;I think that Reviewer #2 put it well. I would like to see them talk a bit more about:</p><p>1) The motivations for their representational choices and, importantly, the consequences and implications that these choices have.</p><p>2) How they anticipate using these numbers to answer biological questions. Any measurement representation should have a rational relationship to the questions being asked, so addressing what their method will be useful for (and what it won't be useful for) will be valuable for the literature moving forward. Will this simply be a better detection technique, or does their unsupervised approach allow the field fundamentally new types of behavioral measurements?&quot;</p><p>These concerns and the minor issues from the original review that are appended can form the basis of a revision that clarifies the methodology and brings out the behavioral significance revealed by the new methodology.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Comprehensive machine learning analysis of Hydra behavior reveals a stable behavioral repertoire&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Eve Marder (Senior Editor), a Reviewing editor, and three reviewers.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>The authors have endeavored to address many of the major concerns of the previous review with further explanations of the method and its underlying assumptions and method choices, and the inherent limitations of the method/analysis. They have also tried to clarify how their method can be used to answer biological questions. There is still one major concern.</p><p>1a) The authors should try windows greater than 5 seconds. It's hardly surprising that less than five seconds is less effective, but why not 8, 10, 20? Just saying &quot;we noticed 5 seconds is a reasonable length to define a single behavior&quot; is hardly convincing (neither is Figure 1C).</p><p>1b) It still remains possible that the highly-fragmented t-SNE representation results form the fact that behaviors are unnecessarily chopped-up by imposing a 5 second window. Problems might occur because the behavior spans a window boundary. The analysis should be performed using a sliding 5-second window rather than separated windows. This may remove some of the observed over-segmentation of the space. There are several methods (including one in Berman, 2014, but others as well) for handling tens to hundreds of millions of data points. Since the space is one of the cruxes of the paper's arguments, and the authors might get better results with the sliding window, it seems somewhat remiss to not attempt this (it would be ~ 24 hours of running time on a machine that can handle a 30,000-point t-SNE). The Barnes-Hut implementation from: https://lvdmaaten.github.io/tsne/ may prove helpful.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.32605.039</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>1) The choices for their particular pipeline need to be clarified and discussed. This includes Reviewer #3's concern on the 5-second window but extends to other choices in the stream. As machine learning techniques advance it is getting much easier to represent video information with numbers and I expect that as a result we will see many future advances in behavioral representation. However, these representations are often idiosyncratic and so it is important to understand what aspects are universal, or at the very least include a discussion about various choices. I also think it is important to discuss what kind of behavior might be missing in this approach.</p></disp-quote><p>Our goal was to describe all possible <italic>Hydra</italic> behavior quantitatively. For this purpose, we chose the bag-of-words (BoW) framework, which captures the overall statistics of a dataset with a given time frame and has demonstrated success in deformable human action classification tasks. The BoW framework originated from document classification tasks in the machine learning field. In this framework, documents are considered “bags” of words, and are then represented by a histogram of word counts using a common dictionary. These histogram representations are demonstrated to be efficient for classifying document types. In computer vision, the BoW framework considers instead pictures or videos as “bags” of visual words such as small patches in the images, or shape and motion features extracted from such patches. Compared with another popular technique, template matching, BoW is more robust against challenges such as occlusion, position, orientation, and viewing angle changes. It also proves to be successful in capturing object features in various scenes and is thus one of the most important concepts and cutting-edge techniques in this field. For behavior recognition tasks of deformable animals, BoW is therefore ideally suited for the problem.</p><p>In the BoW framework, we made the choices of segmenting the <italic>Hydra</italic> from background, scale and register <italic>Hydra</italic> to eliminate variance introduced by size and orientation, using dense trajectories features that captures both shape and motion statistics, using Gaussian Mixture codebooks and Fisher vectors to encode the features in a probabilistic way, classifying <italic>Hydra</italic> behaviors with standard SVM classifiers, and identifying behavior types with the t-SNE embedding which has demonstrated success in fly behavior analysis in an unsupervised way. Although these choices may seem arbitrary, they are anchored on the structure of the data and task at hand, as we explain below. Our developed framework is a modified version of the original BoW framework, which is simply a normalized histogram representation of selected visual features. This modification includes the key steps of 3 body part segmentation, dense trajectory features, and Fisher vector encoding. We also compared the supervised classification performance of histogram representation vs. Fisher vector representation, the effect of introducing body part segmentation of 3 and 6 segments (Figure 2—figure supplement 1B), different time window sizes (Figure 2—figure supplement 1A), and different Gaussian Mixture codebook sizes (Figure 2—figure supplement 1C). Our choice of framework and parameters proves to be quite ideal considering both training and validation accuracy, as well as generalizability on test datasets. Although it is early to say if BoW will be adopted by computational ethologists and neuroscientists, our developed framework is also in principle universal to all organisms since it does not rely on specific information of <italic>Hydra</italic>, presenting the stepping stone to developing more sophisticated behavioral methods.</p><p>To come clean, as a limitation we should mention that our framework is constrained by the lack of temporal information, which is lost in the bag-of-words approach. Nevertheless, we show that we can still encode <italic>Hydra</italic> behavior even when we do not model the temporal information explicitly. BoW also does not model fine behaviors on the level of single tentacle twitching, or local muscle twitching in body column. This would require an explicit model of the <italic>Hydra</italic> body, instead of the statistical bag-of-words model. Depending on the specific biological question, more specialized method could be developed in the future to investigate these behavior differences.</p><p>Revision:</p><p>We expanded a paragraph in subsection “A machine learning method for quantifying behavior of deformable animals”, to discuss the general choice of BoW for our behavior recognition task:</p><p>“To tackle the problem of measuring behavior in a deformable animal, we developed a novel analysis pipeline using approaches from computer vision that have achieved success in human action classification tasks (Ke et al., 2007; Laptev et al., 2008; Poppe, 2010; Wang et al., 2009, 2011). Such tasks usually involve various actions and observation angles, as well as occlusion and cluttered background. Therefore, they require more robust approaches to capture stationary and motion statistics, compared to using pre-defined template-based features. In particular, the bag-of-words (BoW) framework is an effective approach for extracting visual information from videos of animals with arbitrary motion and deformation. The BoW framework originated from document classification tasks with machine learning. In this framework, documents are considered “bags” of words, and are then represented by a histogram of word counts using a common dictionary. These histogram representations are demonstrated to be efficient for classifying document types. In computer vision, the BoW framework considers pictures or videos as “bags” of visual words such as small patches in the images, or shape and motion features extracted from such patches. Compared with another popular technique, template matching, it is robust against challenges such as occlusion, position, orientation, and viewing angle changes. It also proves to be successful in capturing object features in various scenes and is thus one of the most important concepts and cutting edge techniques in this field. For behavior recognition tasks of deformable animals, it is therefore ideally suited for the problem.”</p><p>We modified the following paragraph to discuss the specific modifications we made to the original BoW framework, subsection “A machine learning method for quantifying behavior of deformable animals”:</p><p>“We modified the BoW framework by integrating a few state-of-the-art computational methods, including body part segmentation which introduces spatial information, dense trajectory features which encode shape and motion statistics in video patches, Fisher vectors which represent visual words in a statistical manner. Our choice of framework and parameters proves to be quite adequate, considering both its training and validation accuracy, as well as the generalizability on test datasets (Figure 2—figure supplement 1). Indeed, the robust correspondence between supervised, unsupervised and manual classification that we report provides internal cross-validation to the validity and applicability of our machine learning approach. Our developed framework, which uses both supervised and unsupervised techniques, is in principle applicable to all organisms, since it does not rely on specific information of Hydra. Compared with previously developed methods, our method is particularly suitable for behaviors in natural conditions that involve deformable body shapes, as a first step to developing more sophisticated behavioral methods in complex environment for other species.”</p><p>We also introduced a paragraph with discussions concerning the potential drawbacks of our method, subsection “A machine learning method for quantifying behavior of deformable animals”:</p><p>“Our goal was to describe all possible Hydra behavior quantitatively. Because of this, we chose the bag-of-words framework which captures the overall statistics with a given time frame. We defined the length of basic behavior elements to be 5 seconds, which maximizes the number of behaviors that were kept intact while uncontaminated by other behavior types (Figure 1C–D). The bag-of-words framework has shown success in human action classification tasks; here we improved the basic bag-of-words framework by densely sample feature points in the videos and allowing soft feature quantization with Gaussian Mixture codebook and Fisher vector encoding. However, it should be noted that our approach could not capture fine-level behavior differences, e.g. single tentacle behavior. This would require modeling the animal with an explicit template, or with anatomical landmarks as demonstrated by deformable human body modeling with wearable sensors. Our approach also does not recover transition probabilities between behavior types, or behavioral interactions between individual specimens. In fact, since our method treats each time window as an independent “bag” of visual words, there was no constraint on the temporal smoothness of classified behaviors. Classifications were allowed to be temporally noisy, therefore they could not be applied for temporal structure analysis. A few studies have integrated state-space models for modeling both animal and human behavior (Gallagher et al., 2013; Ogale et al., 2007; Wiltschko et al., 2015), while others have used discriminative models such as Conditional Random Field models for activity recognition (Sminchisescu et al., 2006; Wang and Suter, 2007). These methods may provide promising candidates for modeling behavior with temporal structure in combination with our approach (Poppe, 2010).”</p><p>In the Materials and methods section, we added discussions to justify our choice of framework and parameters, as following: subsection “Video pre-processing”:</p><p>“… To separate the <italic>Hydra</italic> region into three body parts, the part under the upper body square mask excluding the body column was defined as the tentacle region, and the rest of the mask was split at the minor axis of the ellipse; the part close to the tentacle region was defined as the upper body region, and the other as the lower body region. This step has shown to improve representation efficiency (Figure 2—figure supplement 1B).”</p><p>In subsection “Feature extraction”:</p><p>“…All parameters above were cross-validated with the training and test datasets.”</p><p>And in subsection “Gaussian mixture codebook and Fisher vector”:</p><p>“In this paper, the GMM size was set to 128 with cross-validation (Figure 2—figure supplement 1C).”</p><p>Along with the revised text, we provided a supplementary figure (Figure 2—figure supplement 1) to justify our specific choices of framework and parameters.</p><p>We believe that these modifications together will make our choice of framework and specific steps stronger and will provide a more comprehensive view of our choices.</p><disp-quote content-type="editor-comment"><p>2) They need to do a better job of motivating and discussing the important questions that their quantitative behavioral repertoire can answer. This is the science of behavior not simply representation of videos as numbers. And it's here that we can learn how Hydra compares to worms and flies what we might expect to find in the neural recordings.&quot;</p></disp-quote><p>Thank you for the comments. Quantitative behavior recognition and measurement methods provide an important tool for investigating behavioral differences under various conditions from large datasets, allows the discovery of behavior features that are beyond the capability of human visual system, and defines a uniform standard for describing behaviors across conditions. But beyond the purely ethology questions that such methods could answer, they also allow researchers to address potential neural mechanisms by providing a standard and quantitative measurement of the behavioral output of the nervous system. Both ethological and neuroscience application seem important and our approach is quite well poised for these tasks.</p><p>Our method also enables the recognition and quantification of <italic>Hydra</italic> behaviors in an automated fashion. Because of this, it provides a quantitative and objective tool to characterize the behavior differences of <italic>Hydra</italic> under pharmacological assays, lesion studies, optogenetic activation of subsets of neurons, or testing the existence of more advanced behaviors such as learning and social behavior. As a proof of concept, it also allows testing quantitative models of behaviors in <italic>Hydra</italic>, investigating the underlying neural activity patterns of each behavior, and predicting the behavioral output from neural activity. As the first pass of its kind, our method opens the possibility to discovery interesting behavioral mechanisms in <italic>Hydra</italic>. And why is Hydra interesting? We would argue that, as a cnidarian, <italic>Hydra’s</italic> nervous system represents one of the earliest nervous systems in evolution. Thus, studying <italic>Hydra</italic> behavior as the output of this primitive nervous system would provide insight into how the nervous system adapts to the changing environment and further evolves.</p><p>Revision:</p><p>We modified the first and second paragraphs of the Introduction section to integrate the above discussion:</p><p>“Animal behaviors are generally characterized by an enormous variability in posture and the motion of different body parts, even if many complex behaviors can be reduced to sequences of simple stereotypical movements (Berman et al., 2014; Branson et al., 2009; Gallagher et al., 2013; Srivastava et al., 2009; Wiltschko et al., 2015; Yamamoto and Koganezawa, 2013). As a way to systematic capture this variability and compositionality, quantitative behavior recognition and measurement methods could provide an important tool for investigating behavioral differences under various conditions from large datasets, allowing for the discovery of behavior features that are beyond the capability of human visual system, and defining a uniform standard for describing behaviors across conditions (Egnor and Branson, 2016). In addition, much remains unknown about how the specific spatiotemporal pattern of activity of the nervous systems integrate external sensory inputs and internal neural network states in order to selectively generate different behavior. Thus automatic methods to measure and classify behavior quantitatively allow researchers to address potential neural mechanisms by providing a standard measurement of the behavioral output of the nervous system.”</p><p>“Indeed, advances in calcium imaging techniques have enabled the recording of large neural populations (Chen et al., 2013; Jin et al., 2012; Kralj et al., 2012; St-Pierre et al., 2014; Tian et al., 2009; Yuste and Katz, 1991) and whole brain activity from small organisms such as <italic>C. elegans</italic> and larval zebrafish (Ahrens et al., 2013; Nguyen et al., 2016; Prevedel et al., 2014). A recent study has demonstrated the cnidarian Hydra can be used as an alternative model to image the complete neural activity during behavior (Dupre and Yuste, 2017). As a cnidarian, Hydra is closer to the earliest animals in evolution that possess a nervous system. As an important function of the nervous system, animal behaviors allow individuals to adapt to the environment at a time scale that is much faster than natural selection, and drives rapid evolution of the nervous system, providing a rich context to study nervous system functions and evolution (Anderson and Perona, 2014). As Hydra nervous system evolved from the nervous system present in the last common ancestor of cnidarians and bilaterians, the behaviors of Hydra could also represent some of the most primitive examples of coordination between a nervous system and non-neuronal cells, making it relevant to our understanding of the nervous systems of model organisms such as <italic>C. elegans, Drosophila</italic>, zebrafish, and mice, as it provides an evolutionary perspective to discern whether neural mechanisms found in a particular species represent a specialization or are generally conserved. In fact, although Hydra behavior has been studied for centuries, it is largely unknown whether Hydra possesses complex behaviors such as social behavior and learning behavior, how its behavior changes under environmental, physiological, nutritional or pharmacological manipulations, and the underlying neural mechanisms of the potential changes. Having an unbiased and automated behavior recognition and quantification method would therefore enable such studies with large datasets, allowing us to address the behavioral differences of Hydra with pharmacological assays, lesion studies, environmental and physiological condition changes, under activation of subsets of neurons, testing quantitative models of Hydra behaviors, and linking behavior outputs with the underlying neural activity patterns.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 then commented &quot;I think that Reviewer #2 put it well. I would like to see them talk a bit more about:</p><p>1) The motivations for their representational choices and, importantly, the consequences and implications that these choices have.</p></disp-quote><p>If we have to point to a single reason, we chose the current framework specifically because of its advantage of dealing with deformable shapes. But, as discussed in the response to Reviewer #2’s first question, many specific choices in the framework was made due to additional advantages: segmentation and registration eliminate variance caused by background noise and orientation, body part segmentation introduces spatial information to the BoW framework, dense trajectories features maximizes the information captured by the features, Gaussian Mixture codebook and Fisher vectors avoid the inaccuracy of hard encoding from simple k-means codebook and histogram representations. These modifications to the original BoW framework greatly improved the representation efficiency as shown by an overall increase in classification accuracy and generalization ability (Figure 2—figure supplement 1).</p><p>As discussed in the response to Reviewer #2’s first question, one important consequence of our framework is the lack of temporal information, which could be done in further work.</p><p>Revision:</p><p>We made the changes described in answer to Reviewer #2’s first question.</p><disp-quote content-type="editor-comment"><p>2) How they anticipate using these numbers to answer biological questions. Any measurement representation should have a rational relationship to the questions being asked, so addressing what their method will be useful for (and what it won't be useful for) will be valuable for the literature moving forward. Will this simply be a better detection technique, or does their unsupervised approach allow the field fundamentally new types of behavioral measurements?</p><p>These concerns and the minor issues from the original review that are appended can form the basis of a revision that clarifies the methodology and brings out the behavioral significance revealed by the new methodology.</p></disp-quote><p>Thank you for the comments. Our method could be used to recognize and quantify <italic>Hydra</italic> behaviors from large datasets in an automated and consistent way and would allow us to address questions at the level of behavior repertoire statistics. As discussed in the answer to reviewer #2’s second question, our method provides the possibility to study the behavioral changes of <italic>Hydra</italic> under various conditions such as pharmacological regulations, lesions, environmental and physiological changes. It also provides the tool to investigate the existence of complex behaviors such as social and learning, as well as building and testing quantitative models of <italic>Hydra</italic> behaviors. In this paper, we demonstrated that we can use this method to investigate the behavioral difference under different conditions (e.g. fed/starved). Importantly, our developed framework is not limited to <italic>Hydra</italic>; it is potentially applicable to all animal models since it does not rely on assumptions about the specific features of <italic>Hydra</italic>. However, as pointed out above, our method does lacks temporal information, therefore could not be used to model any particular behavioral sequences and the differences therein.</p><p>In addition, the unsupervised approach depends heavily on the quality of encoded features. Since the BoW model provides only a statistical description of videos, the features do not encode fine differences in behaviors. Therefore, the types of behavior that can be identified and quantified by the unsupervised approach have the same constraints as described in the response to Reviewer #2’s first question.</p><p>Revision:</p><p>Besides the changes we made in response to Reviewer #2’s second question, we added a paragraph in subsection “<italic>Hydra</italic> as a model system for investigating neural circuits underlying behavior”, to address the second part of this comment, concerning the usefulness of our method:</p><p>“With our method, we demonstrate that we are able to recognize and quantify Hydra behaviors automatically and identify novel behavior types. This allows us to investigate the behavioral repertoire stability under different environmental, physiological and genetic conditions, providing insight into how a primitive nervous system adapt to its environment. Although our framework does not currently model temporal information directly, it serves as a stepping-stone towards building more comprehensive models of Hydra behaviors. Future work that incorporates temporal models would allow us to quantify behavior sequences, and to potentially investigate more complicated behaviors in Hydra such as social and learning behaviors.”</p><p>We also revised the last paragraph of subsection “A machine learning method for quantifying behavior of deformable animals”, in response to the question “will this simply be a better detection technique, or does their unsupervised approach allow the field fundamentally new types of behavioral measurements”:</p><p>“In our pipeline, we applied both supervised and unsupervised approaches to characterize Hydra behavior. In supervised classifications (with SVM), we manually defined seven types of behaviors, and trained classifiers to infer the label of unknown samples. In unsupervised analysis (t-SNE), we did not pre-define behavior types, but rather let the algorithm discover the structures that were embedded in the behavior data. In addition, we found that unsupervised learning could discover previously unannotated behavior types such as egestion. However, the types of behaviors discovered by unsupervised analysis are limited by the nature of the encoded feature vectors. Since the bag-of-words model provides only a statistical description of videos, those features do not encode fine differences in behaviors. Due to this difference, we did not apply unsupervised learning to analyze the behavior statistics under different environmental and physiological conditions, as supervised learning appears more suitable for applications where one needs to assign a particular label to a new behavior video.”</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The authors have endeavored to address many of the major concerns of the previous review with further explanations of the method and its underlying assumptions and method choices, and the inherent limitations of the method/analysis. They have also tried to clarify how their method can be used to answer biological questions. There is still one major concern.</p><p>1a) The authors should try windows greater than 5 seconds. It's hardly surprising that less than five seconds is less effective, but why not 8, 10, 20? Just saying &quot;we noticed 5 seconds is a reasonable length to define a single behavior&quot; is hardly convincing (neither is Figure 1C).</p></disp-quote><p>We now tested window size of 8 seconds, 10 seconds and 20 seconds with our developed analysis framework, and compared the training, validation and test classification accuracy. The result shows that 5-second time window still performs best with all the accuracy measurements (Figure2—figure supplement 1A). Therefore, we believe 5-second is a reasonable length to define a single behavior with our method.</p><p>Revision:</p><p>We modified Figure2—figure supplement 1A to include the classification accuracy of window sizes greater than 5 seconds, and modified the corresponding text (subsection “Capturing the movement and shape statistics of freely-moving <italic>Hydra”</italic>):</p><p>“Our goal was to […] A post hoc comparison of different window sizes (1-20 seconds) with the complete analysis framework also demonstrated that 5-second windows result in the best performance (Figure 2—figure supplement 1A). Therefore, we chose 5-second as the length of a behavior element in Hydra.”</p><p>We also modified the corresponding Figure legend.</p><disp-quote content-type="editor-comment"><p>1b) It still remains possible that the highly-fragmented t-SNE representation results form the fact that behaviors are unnecessarily chopped-up by imposing a 5 second window. Problems might occur because the behavior spans a window boundary. The analysis should be performed using a sliding 5-second window rather than separated windows. This may remove some of the observed over-segmentation of the space. There are several methods (including one in Berman, 2014, but others as well) for handling tens to hundreds of millions of data points. Since the space is one of the cruxes of the paper's arguments, and the authors might get better results with the sliding window, it seems somewhat remiss to not attempt this (it would be ~ 24 hours of running time on a machine that can handle a 30,000-point t-SNE). The Barnes-Hut implementation from: https://lvdmaaten.github.io/tsne/ may prove helpful.</p></disp-quote><p>We performed t-SNE with the fast Barnes-Hut implementation on the complete training dataset of 50 <italic>Hydra</italic>, with a sliding 5-second window. The resulting space did not show improved segmentation (see <xref ref-type="fig" rid="respfig1">Author response image 1</xref>). It is possible that, by performing sliding windows, the highly-overlapping windows introduce more local structures that represent the similarities within each individual, rather than within each behavior category. As t-SNE is designed for discovering local similarity structures, this results in the highly segmented embedding map as shown below. We believe that the original embedding analysis presented in the manuscript represents a proof-of-concept demonstration of categorizing behavior types with unsupervised methods, without too much bias created by individual similarities.</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.32605.035</object-id><label>Author response image 1.</label><caption><title>t-SNE embedding of continuous time windows.</title><p>a, Scatter plot with embedded Fisher vectors from 50 <italic>Hydra</italic>. Each dot represents projection from a high-dimensional Fisher vector to its equivalent in the embedding space. The Fisher vectors were encoded from continuous 5-second windows with an overlap of 24 frames. Color represents the manual label of each dot. b, Segmented density map generated from the embedding scatter plot. c, Behavior motif regions defined using the segmented density map. d, Labeled behavior regions with manual labels. Color represents the corresponding behavior type of each region.</p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-32605-resp-fig1-v2"/></fig></body></sub-article></article>