<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">06213</article-id><article-id pub-id-type="doi">10.7554/eLife.06213</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Speech encoding by coupled cortical theta and gamma oscillations</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-21200"><name><surname>Hyafil</surname><given-names>Alexandre</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23743"><name><surname>Fontolan</surname><given-names>Lorenzo</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23744"><name><surname>Kabdebon</surname><given-names>Claire</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-6744"><name><surname>Gutkin</surname><given-names>Boris</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23745"><name><surname>Giraud</surname><given-names>Anne-Lise</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">INSERM U960, Group for Neural Theory, Département d'Etudes Cognitives</institution>, <institution>Ecole Normale Supérieure</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Neuroscience</institution>, <institution>University of Geneva</institution>, <addr-line><named-content content-type="city">Geneva</named-content></addr-line>, <country>Switzerland</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Centre for Cognition and Decision Making</institution>, <institution>National Research University Higher School</institution>, <addr-line><named-content content-type="city">Moscow</named-content></addr-line>, <country>Russia</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-10835"><name><surname>Brownell</surname><given-names>Hiram</given-names></name><role>Reviewing editor</role><aff><institution>Boston College</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>alexandre.hyafil@gmail.com</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>29</day><month>05</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>4</volume><elocation-id>e06213</elocation-id><history><date date-type="received"><day>23</day><month>12</month><year>2014</year></date><date date-type="accepted"><day>28</day><month>05</month><year>2015</year></date></history><permissions><copyright-statement>© 2015, Hyafil et al</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Hyafil et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-06213-v3.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.06213.001</object-id><p>Many environmental stimuli present a quasi-rhythmic structure at different timescales that the brain needs to decompose and integrate. Cortical oscillations have been proposed as instruments of sensory <italic>de-multiplexing</italic>, i.e., the parallel processing of different frequency streams in sensory signals. Yet their causal role in such a process has never been demonstrated. Here, we used a neural microcircuit model to address whether coupled theta–gamma oscillations, as observed in human auditory cortex, could underpin the multiscale sensory analysis of speech. We show that, in continuous speech, theta oscillations can flexibly track the syllabic rhythm and temporally organize the phoneme-level response of gamma neurons into a code that enables syllable identification. The tracking of slow speech fluctuations by theta oscillations, and its coupling to gamma-spiking activity both appeared as critical features for accurate speech encoding. These results demonstrate that cortical oscillations can be a key instrument of speech de-multiplexing, parsing, and encoding.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.001">http://dx.doi.org/10.7554/eLife.06213.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.06213.002</object-id><title>eLife digest</title><p>Some people speak twice as fast as others, while people with different accents pronounce the same words in different ways. However, despite these differences between speakers, humans can usually follow spoken language with remarkable ease.</p><p>The different elements of speech have different frequencies: the typical frequency for syllables, for example, is about four syllables per second in speech. Phonemes, which are the smallest elements of speech, appear at a higher frequency. However, these elements are all transmitted at the same time, so the brain needs to be able to process them simultaneously.</p><p>The auditory cortex, the part of the brain that processes sound, produces various ‘waves’ of electrical activity, and these waves also have a characteristic frequency (which is the number of bursts of neural activity per second). One type of brain wave, called the theta rhythm, has a frequency of three to eight bursts per second, which is similar to the typical frequency of syllables in speech, and the frequency of another brain wave, the gamma rhythm, is similar to the frequency of phonemes. It has been suggested that these two brain waves may have a central role in our ability to follow speech, but to date there has been no direct evidence to support this theory.</p><p>Hyafil et al. have now used computer models of neural oscillations to explore this theory. Their simulations show that, as predicted, the theta rhythm tracks the syllables in spoken language, while the gamma rhythm encodes the specific features of each phoneme. Moreover, the two rhythms work together to establish the sequence of phonemes that makes up each syllable. These findings will support the development of improved speech recognition technologies.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.002">http://dx.doi.org/10.7554/eLife.06213.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>speech perception</kwd><kwd>cross-frequency coupling</kwd><kwd>theta oscillation</kwd><kwd>spiking network</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council (ERC)</institution></institution-wrap></funding-source><award-id>CompusLang 260347</award-id><principal-award-recipient><name><surname>Fontolan</surname><given-names>Lorenzo</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Schweizerische Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>320030-149319</award-id><principal-award-recipient><name><surname>Giraud</surname><given-names>Anne-Lise</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gutkin</surname><given-names>Boris</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004794</institution-id><institution>Centre National de la Recherche Scientifique</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Giraud</surname><given-names>Anne-Lise</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.3</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Computational modelling shows that coupled theta and gamma oscillations in the auditory cortex can decompose speech into its syllabic constituents, and organize the neural spiking at faster timescale into a decodable format.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The physical complexity of biological and environmental signals poses a fundamental problem to the sensory systems. Sensory signals are often made of different rhythmic streams organized at multiple timescales, which require to be processed in parallel and recombined to achieve unified perception. Speech constitutes an example of such a physical complexity, in which different rhythms index linguistic representations of different granularities, from phoneme to syllables and words (<xref ref-type="bibr" rid="bib66">Rosen, 1992</xref>; <xref ref-type="bibr" rid="bib82">Zion Golumbic et al., 2012</xref>). Before meaning can be extracted from continuous speech, two critical pre-processing steps need to be carried out: a de-multiplexing step, i.e., the parallel analysis of each constitutive rhythm, and a parsing step, i.e., the discretization of the acoustic signal into linguistically relevant chunks that can be individually processed (<xref ref-type="bibr" rid="bib70">Stevens, 2002</xref>; <xref ref-type="bibr" rid="bib64">Poeppel, 2003</xref>; <xref ref-type="bibr" rid="bib23">Ghitza, 2011</xref>). While parsing is presumably modulated in a top-down way, by knowing a priori through developmental learning (<xref ref-type="bibr" rid="bib55">Ngon et al., 2013</xref>) where linguistic boundaries should lie, it is likely largely guided by speech acoustic dynamics. It has recently been proposed that speech de-multiplexing and parsing could both be handled in a bottom-up way by the combined action of auditory cortical oscillations in distinct frequency ranges, enabling parallel computations at syllabic and phonemic timescales (<xref ref-type="bibr" rid="bib23">Ghitza, 2011</xref>; <xref ref-type="bibr" rid="bib25">Giraud and Poeppel, 2012</xref>). Intrinsic coupling across cortical oscillations of distinct frequencies, as observed in electrophysiological recordings of auditory cortex (<xref ref-type="bibr" rid="bib39">Lakatos et al., 2005</xref>; <xref ref-type="bibr" rid="bib21">Fontolan et al., 2014</xref>), could enable the hierarchical combination of syllabic- and phonemic-scale computations, subsequently restoring the natural arrangement of phonemes within syllables (<xref ref-type="bibr" rid="bib25">Giraud and Poeppel, 2012</xref>).</p><p>The most pronounced energy fluctuations in speech occur at about 4 Hz (<xref ref-type="bibr" rid="bib82">Zion Golumbic et al., 2012</xref>) and can serve as an acoustic guide for signalling the syllabic rhythm (<xref ref-type="bibr" rid="bib49">Mermelstein, 1975</xref>). Since the syllabic rate coincides with the auditory cortex theta rhythm (3–8 Hz), syllable boundaries could be viably signalled by a given <italic>phase</italic> in the theta cycle. The relevance of speech tracking by the theta neural rhythm (<xref ref-type="bibr" rid="bib29">Henry et al., 2014</xref>) is highlighted by experimental data showing that speech intelligibility depends on the degree of phase-locking of the theta-range neural activity in auditory cortex (<xref ref-type="bibr" rid="bib2">Ahissar et al., 2001</xref>; <xref ref-type="bibr" rid="bib47">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib61">Peelle et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Gross et al., 2013</xref>). By analogy with the spatial and mnemonic oscillatory processes that take place in the hippocampus (<xref ref-type="bibr" rid="bib33">Jensen and Lisman, 1996</xref>; <xref ref-type="bibr" rid="bib45">Lisman and Jensen, 2013</xref>; <xref ref-type="bibr" rid="bib41">Lever et al., 2014</xref>), the theta oscillation may orchestrate gamma neural activity to facilitate its subsequent decoding (<xref ref-type="bibr" rid="bib10">Canolty et al., 2007</xref>): the phase of theta-paced neural activity could regulate faster neural activity in the low-gamma range (&gt;30 Hz) involved in linguistic coding of phonemic details (<xref ref-type="bibr" rid="bib23">Ghitza, 2011</xref>; <xref ref-type="bibr" rid="bib25">Giraud and Poeppel, 2012</xref>). The control of gamma by theta oscillations could hence both modulate the excitability of gamma neurons to devote more processing power to the informative parts of syllabic sound patterns, and constitute a reference time frame aligned on syllabic contours for interpreting gamma-based phonemic processing (<xref ref-type="bibr" rid="bib68">Shamir et al., 2009</xref>; <xref ref-type="bibr" rid="bib23">Ghitza, 2011</xref>; <xref ref-type="bibr" rid="bib35">Kayser et al., 2012</xref>; <xref ref-type="bibr" rid="bib58">Panzeri et al., 2014</xref>).</p><p>Compelling as this hypothesis may sound, direct evidence for neural mechanisms linking speech constituents and oscillatory components is still lacking. One way to address a causal role of oscillations in speech processing is computational modelling, as it permits to directly test the efficiency of cross-coupled theta and gamma oscillations as an instrument of speech de-multiplexing, parsing, and encoding. Previous models of speech processing involved only gamma oscillations in the context of isolated speech segments (<xref ref-type="bibr" rid="bib68">Shamir et al., 2009</xref>) or did not involve neural oscillations at all (<xref ref-type="bibr" rid="bib28">Gütig and Sompolinsky, 2009</xref>; <xref ref-type="bibr" rid="bib80">Yildiz et al., 2013</xref>). On the other hand, previous models of cross-frequency coupled oscillations did not address sensory functions as parsing and de-multiplexing (<xref ref-type="bibr" rid="bib33">Jensen and Lisman, 1996</xref>; <xref ref-type="bibr" rid="bib73">Tort et al., 2007</xref>). Here, we examined how a biophysically inspired model of coupled theta and gamma neural oscillations can process continuous speech (spoken sentences). Specifically, we determined: (i) whether theta oscillations are able to accurately parse speech into syllables, (ii) whether syllable-related theta signal may serve as a reference time frame to improve gamma-based decoding of continuous speech; (iii) whether this decoding requires theta to modulate the activity of the gamma network. To address the last two points, we compared speech decoding performance of the model with two control versions of the network, in which we removed the neural connection entraining the theta neurons by speech fluctuations or the link that couples them to the gamma neurons.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Model architecture and spontaneous behaviour</title><p>The model proposed here (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) is inspired from cortical architecture (<xref ref-type="bibr" rid="bib17">Douglas and Martin, 2004</xref>; <xref ref-type="bibr" rid="bib14">da Costa and Martin, 2010</xref>) and function (<xref ref-type="bibr" rid="bib38">Lakatos et al., 2007</xref>) as well as from previous biophysical models of cross-frequency coupled oscillation generation (<xref ref-type="bibr" rid="bib73">Tort et al., 2007</xref>; <xref ref-type="bibr" rid="bib37">Kopell et al., 2010</xref>; <xref ref-type="bibr" rid="bib75">Vierling-Claassen et al., 2010</xref>). We used the well documented Pyramidal Interneuron Gamma (PING) model for implementing a gamma network: bursts of inhibitory neurons immediately follow bursts of excitatory neurons (<xref ref-type="bibr" rid="bib31">Jadi and Sejnowski, 2014</xref>), creating the overall spiking rhythm. Given that gamma and theta oscillations are both locally present in superficial cortical layers (<xref ref-type="bibr" rid="bib39">Lakatos et al., 2005</xref>), we assume similar local generation mechanisms for theta and gamma with a direct connection between them. Direct evidence for a local generation of theta oscillations in auditory cortex is still scarce (<xref ref-type="bibr" rid="bib3">Ainsworth et al., 2011</xref>) and we cannot completely rule out that they might spread from remote generators (e.g., in the hippocampus; <xref ref-type="bibr" rid="bib73">Tort et al., 2007</xref>; <xref ref-type="bibr" rid="bib37">Kopell et al., 2010</xref>). Yet, we built the case for local generation from the following facts: (1) neocortical (somatosensory) theta oscillations are observed in vitro (<xref ref-type="bibr" rid="bib18">Fanselow et al., 2008</xref>), (2) MEG, EEG, and combined EEG/FMRI recordings in humans show that theta activity phase-locks to speech amplitude envelope in A1 and immediate association cortex—but not beyond—(<xref ref-type="bibr" rid="bib2">Ahissar et al., 2001</xref>; <xref ref-type="bibr" rid="bib47">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib13">Cogan and Poeppel, 2011</xref>; <xref ref-type="bibr" rid="bib53">Morillon et al., 2012</xref>), and (3) theta phase-locking to speech is not accompanied by power increase, arguing for a phase restructuring of a local oscillation (<xref ref-type="bibr" rid="bib47">Luo and Poeppel, 2007</xref>). We assumed a similar generation mechanism for theta and gamma oscillations, with slower excitatory and inhibitory synaptic time constants for theta (<xref ref-type="bibr" rid="bib37">Kopell et al., 2010</xref>; <xref ref-type="bibr" rid="bib75">Vierling-Claassen et al., 2010</xref>). The distinct dynamics for the two modules reflect the diversity of inhibitory synaptic timescales observed experimentally, with Martinotti cells displaying slow synaptic inhibition (<italic>Ti</italic> neurons), and basket cells showing faster inhibition decay (<italic>Gi</italic> neurons) (<xref ref-type="bibr" rid="bib69">Silberberg and Markram, 2007</xref>). We refer to the theta network as Pyramidal Interneuron Theta (PINTH), by analogy with PING. The full model is hence composed of a theta-generating module with interconnected spiking excitatory (<italic>Te</italic>) and inhibitory (<italic>Ti</italic>) neurons that spontaneously synchronize at theta frequency (6–8 Hz) through slow decaying inhibition; and of a gamma-generating module with excitatory (<italic>Ge</italic>) and inhibitory (<italic>Gi</italic>) neurons that burst at a faster rate (25–45 Hz) synchronized by fast decaying inhibition (PING; <xref ref-type="fig" rid="fig1">Figure 1B</xref>) (<xref ref-type="bibr" rid="bib7">Börgers and Kopell, 2005</xref>). The firing pattern of our simulated neurons is sparse and weakly synchronous at rest, consistent with the low spiking rate of cortical neurons (<xref ref-type="bibr" rid="bib9">Brunel and Wang, 2003</xref>) (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D</xref>). Unlike the classical 50–80 Hz PING seen in in vitro preparations of rat auditory cortex (<xref ref-type="bibr" rid="bib3">Ainsworth et al., 2011</xref>), our network produced a lower gamma frequency around 30 Hz, as observed in human auditory cortex in response to speech (<xref ref-type="bibr" rid="bib56">Nourski et al., 2009</xref>; <xref ref-type="bibr" rid="bib60">Pasley et al., 2012</xref>).<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.06213.003</object-id><label>Figure 1.</label><caption><title>Network architecture and dynamics.</title><p>(<bold>A</bold>) Architecture of the full model. <italic>Te</italic> excitatory neurons (n = 10) and <italic>Ti</italic> inhibitory neurons (n = 10) form the PINTH loop generating theta oscillations. <italic>Ge</italic> excitatory neurons (n = 32) and <italic>Gi</italic> inhibitory neurons (n = 32) form the PING loop generating gamma oscillations. <italic>Te</italic> neurons receive non-specific projections from all auditory channels, while <italic>Ge</italic> units receive specific projection from a single auditory channel, preserving tonotopy in the <italic>Ge</italic> population. PING and PINTH loops are coupled through all-to-all projections from <italic>Te</italic> to <italic>Ge</italic> units. (<bold>B</bold>) Network activity at rest and during speech perception. Raster plot of spikes from representative <italic>Ti</italic> (dark green), <italic>Te</italic> (light green), <italic>Gi</italic> (dark blue), and <italic>Ge</italic> (light blue). Simulated LFP is shown on top and the auditory spectrogram of the input sentence &quot;<italic>Ralph prepared red snapper with fresh lemon sauce for dinner</italic>&quot; is shown below. <italic>Ge</italic> spikes relative to theta burst (red boxes) form the output of the network. Gamma synchrony is visible in <italic>Gi</italic> spikes. (<bold>C</bold>) Evoked potential (ERP) and Post-stimulus time histograms (PSTH) of <italic>Te</italic> and <italic>Ge</italic> population from 50 simulations of the same sentence: ERP (i.e., simulated LFP averaged over simulations, black line), acoustic envelope of the sentence (red line, filtered at 20 Hz), PSTH for theta (green line) and gamma (blue line) neurons. Vertical bars show scale of 10 spikes for both PSTH. The theta network phase-locks to speech slow fluctuations and entrains the gamma network through the theta–gamma connection. (<bold>D</bold>) Theta/gamma phase-amplitude coupling in Ge spiking activity. Top panel: LFP gamma envelope follows LFP theta phase in single trials. Bottom<bold>-</bold>Left panel: LFP phase-amplitude coupling (measured by Modulation Index) for pairs of frequencies during rest, showing peak in theta–gamma pairs. Bottom-right panel: MI phase-amplitude coupling at the spiking level for the intact model and a control model with no theta–gamma connection (red arrow on A panel), during rest (blue bars) and speech presentation (brown bars).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.003">http://dx.doi.org/10.7554/eLife.06213.003</ext-link></p></caption><graphic xlink:href="elife-06213-fig1-v3.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06213.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Spectral analysis.</title><p>(<bold>A</bold>) Theta phase pattern (left panels) and theta power pattern (right panels) for 50 presentations of the same sentence in the uncoupled theta–gamma control model (top panels) and intact panels (bottom panels). Phase/power is binned into 4 different bins and colour coded. Theta phase is much more reliably imprinted by speech stimulus than power. (<bold>B</bold>) (Left panel) Spike phase-amplitude coupling: mean value for PING amplitude (defined as the number of <italic>Gi</italic> neurons spiking within a gamma burst) as a function of PINTH phase (defined from interpolation between successive theta bursts). Intact model is shown in black while the uncoupled theta–gamma model is shown in blue. Data for rest (thick dashed lines) and during processing of speech (full thick lines) almost perfectly match. Thin dashed lines represent s.e.m. Spike PAC was very strong in the full model but quasi-absent when the theta–gamma connection was removed. (Right panel) Spontaneous spike phase-frequency coupling: mean value for PING frequency (defined from the duration between successive gamma bursts) as a function of PINTH phase. Same legend as left panel. Spike PFC is strong when and only when the theta–gamma connection is present (significant coupling p &lt; 10<sup>−9</sup> for both speech and rest). (<bold>C</bold>) Phase-locking of the theta and gamma oscillations to speech. Phase concentration of the filtered LFP theta (top panel) and gamma (bottom panel) signals through time for 200 presentations of the same sentence (same as <xref ref-type="fig" rid="fig1">Figure 1B,C</xref>). The horizontal orange bar indicates the presentation of the sentence. There is a rapid transition from uniform theta distribution before sentence onset to perfectly phase-locked theta. Phase-locking vanishes at the end of sentence presentation. (<bold>D</bold>) Spike pattern Coefficient of Variation (left) and spike count Fano factors (right) during speech presentation. Both measures were computed from the response of the network to 100 presentations of the same one-second speech segment. Bars and error bars represent mean and standard deviation over distinct neural populations. (<bold>E</bold>) LFP average (ERP) and standard deviation computed from the 100 repeats of presentation of the same sentence to the network. Note that the LFP variability is greatly reduced at speech onset, mainly due to phase-locking of theta and gamma oscillations.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.004">http://dx.doi.org/10.7554/eLife.06213.004</ext-link></p></caption><graphic xlink:href="elife-06213-fig1-figsupp1-v3.tif"/></fig></fig-group></p><p>At rest the PINTH population activity synchronizes at the theta timescale, and the PING population at the gamma time scale. Both the <italic>Te</italic> and <italic>Ge</italic> populations receive projections from a ‘subcortical’ module that mimics the nonlinear filtering of acoustic input by subcortical structures, which primarily includes a signal decomposition into 32 auditory channels (<xref ref-type="bibr" rid="bib11">Chi et al., 2005</xref>). Individual excitatory neurons in the theta module received channel-averaged input while those in the gamma module received frequency selective input. Such a differential selectivity was motivated by experimental observations from intracranial recordings (<xref ref-type="bibr" rid="bib53">Morillon et al., 2012</xref>; <xref ref-type="bibr" rid="bib21">Fontolan et al., 2014</xref>) suggesting that unlike the gamma one, the theta response does not depend on the input spectrum. It also mirrors the dissociation in primate auditory cortex between a population of 'stereotyped' neurons responding very rapidly and non-selectively to any acoustic stimulus (putatively <italic>Te</italic> neurons) and a population of 'modulated' neurons responding selectively to specific spectro-temporal features (putatively <italic>Ge</italic> neurons) (<xref ref-type="bibr" rid="bib8">Brasselet et al., 2012</xref>). Each <italic>Ge</italic> neuron receives input from one specific channel, preserving the auditory tonotopy, so that the whole <italic>Ge</italic> population represents the rich spectral structure of the stimulus. Each <italic>Te</italic> neuron receives input from all the channels, i.e., the <italic>Te</italic> population conveys a widely tuned temporal signal capturing slow stimulus fluctuations. Importantly, the two oscillating modules are connected through all-to-all connections from <italic>Te</italic> neurons to <italic>Ge</italic> neurons allowing the theta oscillations to control the activity of the faster gamma oscillations. This structure enables syllable boundary detection (through the theta module) to constrain the decoding of faster phonemic information. The output of the network is taken from the <italic>Ge</italic> neurons as we assume that the <italic>Ge</italic> neurons provide the input to higher-level cortical structures performing operations like phoneme categorization and providing access to lexicon. Accordingly, in the model the <italic>Ge</italic> neurons receive more spectral details about speech than the <italic>Te</italic> neurons (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). <italic>Ge</italic> spiking is then referenced with respect to timing of theta spikes, and submitted to decoding algorithms.</p></sec><sec id="s2-2"><title>Model dynamics in response to natural sentences</title><p>We first explored the dynamic behaviour of the model. As expected from its architecture and biophysical parameters (see ‘Materials and methods’), the neural network produced activity in theta (6–8 Hz) and low gamma (25–45 Hz) ranges, both at rest and during speech presentation. Consistent with experimental observations (<xref ref-type="bibr" rid="bib47">Luo and Poeppel, 2007</xref>) there was no notable increase in theta spiking during speech presentation, but sentence onsets induced a phase-locking of theta oscillations as shown by the Post-stimulus time histograms of theta neurons, which was further enhanced by all edges in speech envelope. Consequently, the resulting global evoked activity followed the acoustic envelope of the speech signal (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) (<xref ref-type="bibr" rid="bib1">Abrams et al., 2008</xref>). Local Field Potential (LFP) indexes the global synaptic activity over the network (excitatory neurons of both networks) and its dynamics closely followed spiking dynamics. Unlike the LFP theta power pattern, the LFP theta phase pattern was robust across repetitions of the same sentence (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A,C,E</xref>), replicating LFP behaviour from the primate auditory cortex (<xref ref-type="bibr" rid="bib36">Kayser et al., 2009</xref>), and human MEG data (<xref ref-type="bibr" rid="bib47">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib46">Luo et al., 2010</xref>). In line with other empirical data from human auditory cortex (<xref ref-type="bibr" rid="bib56">Nourski et al., 2009</xref>) gamma oscillations followed the onset of sentences (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Owing to the feed-forward connection from the theta to the gamma sub-circuits, the gamma amplitude was coupled to the theta phase both at rest and during speech (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). The coupling was visible both in the spiking (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>) and LFP signal (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Critically, this coupling disappeared when the theta/gamma connection was removed, showing that a common input to <italic>Te</italic> and <italic>Ge</italic> cells is not sufficient to couple the two oscillations.</p></sec><sec id="s2-3"><title>Syllable boundary detection by theta oscillations</title><p>Before testing the speech decoding properties of the model, we explored whether syllable boundaries could reliably be detected at the cortical level by a theta network (see Methods). This first study was based on a corpus consisting of 4620 phonetically labelled English sentences (TIMIT <xref ref-type="bibr" rid="bib44">Linguistic Data Consortium, 1993</xref>). The acoustic analysis of these sentences confirmed a correspondence between the dominant peak of the speech modulation spectrum and the mean syllabic rate (3–6 Hz) (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>), whereby syllabic boundaries correspond to trough in speech slow fluctuations (<xref ref-type="bibr" rid="bib61">Peelle et al., 2013</xref>). The theta network in the model (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>) was explicitly designed to exploit such regularities and infer syllable boundaries. When presenting sentences to the theta module, we observed a consistent theta burst within 50 ms following syllable onset followed by a locking of theta oscillations to theta acoustic fluctuations in the speech signal (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C,D</xref>). More importantly, neuronal theta bursts closely aligned to the timing of syllable boundaries in the presented sentences (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). We compared the performance of the theta network to that of two alternative models also susceptible to predict syllable boundaries: a simple linear-nonlinear acoustic boundary detector (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1E</xref>) and Mermelstein algorithm, a state-of-the-art model which, unlike the model developed here, only permits ‘off-line’ syllable boundary detection (<xref ref-type="bibr" rid="bib49">Mermelstein, 1975</xref>). The theta network performed better than both the linear model and the Mermelstein algorithm (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, all p-values &lt;10<sup>−12</sup>). Similar to results from behavioural studies of human perception (<xref ref-type="bibr" rid="bib50">Miller et al., 1984</xref>; <xref ref-type="bibr" rid="bib56">Nourski et al., 2009</xref>; <xref ref-type="bibr" rid="bib54">Mukamel et al., 2011</xref>) the theta network could adapt to different speech rates. The model performed better than other algorithms, with a syllabic alignment accuracy remaining well above chance levels (p &lt; 10<sup>−12</sup>) in the twofold and threefold time compression conditions. (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.06213.005</object-id><label>Figure 2.</label><caption><title>Theta entrainment by syllabic structure.</title><p>(<bold>A</bold>) Theta spikes align to syllable boundaries. Top graph shows the activity of the theta network at rest and in response to a sentence, including the LFP traces displaying strong theta oscillations, and raster plots for spikes in the <italic>Ti</italic> (light green) and <italic>Te</italic> (dark green) populations. Theta bursts align well to the syllable boundaries obtained from labelled data (vertical black lines shown on top of auditory spectrogram in graph below). (<bold>B</bold>) Performance of different algorithms in predicting syllable onsets: Syllable alignment score indexes how well theta bursts aligned onto syllable boundaries for each sentence in the corpus, and the score was averaged over the 3620 sentences in the test data set (error bars: standard error). Results compare Mermelstein algorithm (grey bar), linear-nonlinear predictor (LN, pink) and theta network (green), both for normal speed speech (compression factor 1) and compressed speech (compression factors 2 and 3). Performance was assessed on a different subsample of sentences than those used for parameter fitting.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.005">http://dx.doi.org/10.7554/eLife.06213.005</ext-link></p></caption><graphic xlink:href="elife-06213-fig2-v3.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06213.006</object-id><label>Figure 2—figure supplement 1.</label><caption><title>TIMIT corpus and models used for syllable boundary detection.</title><p>(<bold>A</bold>) Acoustic analysis of TIMIT corpus. Left panel: speech modulation frequency increases with syllabic rate. All 4620 sentences of the TIMIT corpus (Test data set) were sorted into quartiles according to syllabic rate (i.e., number of syllables per second). Speech envelope spectrum (with 1/f correction) was averaged over all sentences within each quartile, and the four averages are plotted. Colour bars on top of the graphs represent the syllabic rate range for all four quartiles, showing a correspondence between the modal frequency and the syllabic rate over the corpus. Middle panel<italic>:</italic> average channel spectrum. Spectrum was taken for each 128 auditory channels of the Chi and colleagues pre-cortical auditory model (<xref ref-type="bibr" rid="bib11">Chi et al., 2005</xref>), averaged over all sentences in the corpus. All channels show a clear peak in the same 4–8 Hz range, showing that the theta modulation is very present in the input to auditory cortex. Right panel: syllable onset corresponds to a dip in spectrogram. Average of auditory spectrogram channels of sentences phase-locked to syllable onsets. t = 0 (green line) corresponds to syllable onset. Red colours correspond to high value, blue colours to low values. Dip at syllable onset is particularly pronounced over medium frequencies corresponding to formants. Auditory channels were averaged over all syllable onsets over the entire corpus (4620 sentences). This plot shows the connection between syllable boundaries and fluctuations of auditory channels that the auditory cortex may take advantage of in order to predict syllable boundaries. (<bold>B</bold>) Theta network model. Left panel: the architecture of the theta model is the same as the full model network without the PING component. Speech data are decomposed into auditory channels as in the LN model and projected non-specifically onto 10 <italic>Te</italic> excitatory neurons. The <italic>Te</italic> population interacts reciprocally with 10 <italic>Ti</italic> inhibitory neurons, generating theta oscillations. Theta bursts provide the model prediction for syllable boundary timing. (<bold>C</bold>) <italic>Te</italic> neurons burst at speech onset: <italic>Te</italic> neurons provide onset-signalling neurons that respond non-specifically to the onset of all sentences. The spikes from one <italic>Te</italic> neuron were collected over presentation of 500 distinct sentences, and then referenced in time with respect to sentence onset. Here, sentence onset was defined as the time when speech envelope first reached a given threshold (1000 a.u.). Spikes counts are then averaged in 20 ms bins, showing that this neuron displays a strong activity peak 0–60 ms after sentence onset. A secondary burst occurs around 200 ms after onset, as present in the example neuron shown in <xref ref-type="bibr" rid="bib8">Brasselet et al., 2012</xref>. (<bold>D</bold>) Model of linear-nonlinear (LN) predictor of syllable boundaries. Auditory channels are filtered, summed, and passed through a nonlinear function: the output determines the expected probability of syllable onset. A negative feedback loop prevents repeated onset at close timings. Values for filters, nonlinear function, and feedback loops are optimized through fitting to a sub-sample of sentences. (<bold>E</bold>) Stimulus-network coherence. Theta phase (4–8 Hz) was extracted from both the simulated LFP and speech input. Coherence at each data point was computed as the Phase-Locking Value of the phase difference computed from 100 simulations with a distinct sentence. Coherence established in the 0–200 ms following sentence onset to a stable high coherence value of about 0.4.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.006">http://dx.doi.org/10.7554/eLife.06213.006</ext-link></p></caption><graphic xlink:href="elife-06213-fig2-figsupp1-v3.tif"/></fig></fig-group></p><p>This first study demonstrates that theta activity provides a reliable, syllable-based, internal time reference that the neural system could use when reading out the activity of gamma neurons.</p></sec><sec id="s2-4"><title>Decoding of simple temporal stimuli from output spike patterns</title><p>Our next step was to test whether the theta-based syllable chunks of output spike trains (<italic>Ge</italic> neurons) for the different input types could be properly classified. We first quantified the model's ability to encode stimuli designed as simple temporal patterns. We used 50 ms sawtooth stimuli whose shape was parametrically varied by changing the peak position (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), with interstimulus interval between 50 and 250 ms. This toy set of stimuli was previously used in a gamma-based speech encoding model and argued to represent idealized formant transitions (<xref ref-type="bibr" rid="bib68">Shamir et al., 2009</xref>). We extracted spike patterns from all the <italic>Ge</italic> (output) neurons from −20 ms before each sawtooth onset to 20 ms after its offset. This procedure is referred to as ‘stimulus timing’ since it uses the stimulus onset as time reference. Using a clustering method (see ‘Materials and methods’), we observed that the identity of the presented sawtooth could be decoded from the output spike patterns (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) with over 60% accuracy (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, light grey bar). We also computed the decoding performance when we used an internal time reference provided by the theta timing rather than by the stimulus timing. When spike patterns were analysed within a window defined by two successive theta bursts (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, dark grey bar), sawtooth decoding was still possible and even relatively well preserved (mean decoding rate of 41.7%). Noise in the theta module allows the alignment of theta bursts to stimulus onset and thus improves detection performance by enabling consistent theta chunking of spike patterns.<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.06213.007</object-id><label>Figure 3.</label><caption><title>Sawtooth classification.</title><p>(<bold>A</bold>) Gamma spiking patterns in response to simple stimuli. The model was presented with 50 ms sawtooth stimuli, where peak timing was parameterized between 0 (peak at onset) and 1 (peak at offset). Spiking is shown for different <italic>Ge</italic> neurons (y axis) in windows phase-locked to theta bursts (−20 to +70 ms around the burst, x-axis). Neural patterns are plotted below the corresponding sawtooths. (<bold>B</bold>) Simulated networks. The analysis was performed on simulated data from three distinct networks: ‘Undriven-theta model’ (no speech input to <italic>Te</italic> units, top), ‘Uncoupled theta/gamma model’ (no projection from <italic>Te</italic> to <italic>Ge</italic> units, middle), full intact model (bottom). (<bold>C</bold>) Classification performance using stimulus vs. theta timing for the three simulated networks. The stimulus timing (light bars) is obtained by extracting <italic>Ge</italic> spikes in a fixed-size window locked to the onset of the external stimulus; the theta timing (dark bars) is obtained by extracting <italic>Ge</italic> spikes in a window defined by consecutive theta bursts (<italic>theta chunk</italic>, see <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Classification was repeated 10 times for each network and neural code, and mean values and standard deviation were extracted. Average expected chance level is 10%. (<bold>D</bold>) Stimulus detection performance, for the intact and control models. Rest neural patterns were discriminated against any of the 10 neural patterns defined by the 10 distinct temporal shapes. (<bold>E</bold>) Confusion matrices for stimulus- and theta-timing and the two control models (using theta-timing code). The colour of each cell represents the number of trials where a stimulus parameter was associated with a decoded parameter (blue: low numbers; red: high numbers). Values on the diagonal represent correct decoding.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.007">http://dx.doi.org/10.7554/eLife.06213.007</ext-link></p></caption><graphic xlink:href="elife-06213-fig3-v3.tif"/></fig></p><p>We then compared the decoding performance from the full model with that of two control models: one in which the theta module was not driven by the stimulus (<italic>undriven theta</italic> model) and one in which the theta module was not connected with the gamma module (<italic>uncoupled theta/gamma</italic> model) (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, green and blue). Decoding performance of both control models, as revealed by the mean performance (<xref ref-type="fig" rid="fig3">Figure 3C</xref>) and confusion matrices (<xref ref-type="fig" rid="fig3">Figure 3E</xref>), was degraded for either neural code (theta onset and stimulus timing, all p-values &lt;10<sup>−9</sup>). The details of the raw confusion matrices show that the temporal patterns are decoded correctly or as a neighbouring temporal shape only in the intact version of the model (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Furthermore, the intact model achieved better signal vs rest discrimination than the two control models, notably avoiding false alarms (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). In summary, these analyses show that gamma-spiking neurons within theta bursts provide a reliable internal code for characterizing simple temporal patterns, and that this ability is granted by the time-locking of theta neurons (<italic>Te</italic> units) to stimulus and the modulation they exert on the fast-scale output (<italic>Ge</italic>) units.</p></sec><sec id="s2-5"><title>Continuous speech encoding by model output spike patterns</title><p>The overarching goal of this theoretical work was to assess whether coupled cortical oscillations can achieve on-line speech decoding from <italic>continuous</italic> signal. We therefore set out to classify syllables from natural sentences. To decode <italic>Ge</italic> spiking, we used similar procedures as for the encoding/decoding of simple temporal patterns. Output <italic>Ge</italic> spikes were parsed into spike patterns based on the theta chunks, and the decoding analysis was used to recover syllable identity (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). To evaluate the importance of the precise spike timing of gamma neurons, we compared decoding (see ‘Materials and methods’) using spike <italic>patterns</italic> (i.e., spikes labelled with their precise timing w.r.t. chunk onset) vs those obtained from plain spike <italic>counts</italic> (i.e., unlabelled spikes). When using spike patterns syllable decoding reached a high level of accuracy in the intact model: 58% of syllables were correctly classified within a set of 10 possible (randomly chosen) syllables (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Syllable decoding dropped when using spike counts instead of spike patterns (p &lt; 10<sup>−12</sup>). Critically, decoding was poor in both control models (undriven theta and uncoupled theta/gamma) using either spike counts or spike patterns (significantly lower than decoding using spike patterns in the full model, all p-values &lt; 10<sup>−12</sup>, and non-significantly higher than decoding using spike counts in the full model, all p-values &gt; 0.08 uncorrected).<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.06213.008</object-id><label>Figure 4.</label><caption><title>Continuous speech parsing and syllable classification.</title><p>(<bold>A</bold>) Decoding scheme. Output spike patterns were built by extracting <italic>Ge</italic> spikes occurring within time windows defined by consecutive theta bursts (red boxes) during speech processing simulations. Each output pattern was then labelled with the corresponding syllable (grey bars). (<bold>B</bold>) Syllable decoding average performance for uncompressed speech. Performance for the three simulated models (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) using two possible neural codes: <italic>spike count</italic> and <italic>spike pattern</italic>. (<bold>C</bold>) Syllable decoding average performance across speakers, using the spike pattern code. Syllable decoding was optimal when syllable duration was within the 100–300 ms range, i.e., corresponded to the duration of one theta cycle. The intact model performed better than the two controls irrespective of syllable duration range. Chance level is 10%. Colour code same as <bold>B</bold>. (<bold>D</bold>) Syllable decoding performance for compressed speech for the intact model using the spike pattern code (same speaker, as in <bold>B</bold>). Compression ranges from 1 (uncompressed) to 3. Average chance level is 10% (horizontal line in the right plot).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.008">http://dx.doi.org/10.7554/eLife.06213.008</ext-link></p></caption><graphic xlink:href="elife-06213-fig4-v3.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06213.009</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Syllable classification across speakers.</title><p>(<bold>A</bold>) Distribution of syllable duration across sentences and 462 speakers. The shaded area (100–300 ms) indicates region of maximal density. Extreme values probably correspond to ill-defined syllables.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.009">http://dx.doi.org/10.7554/eLife.06213.009</ext-link></p></caption><graphic xlink:href="elife-06213-fig4-figsupp1-v3.tif"/></fig></fig-group></p><p>We also explored the model performance for encoding syllables spoken by different speakers. We used a similar decoding procedure as above, but here the classifier was trained on different speakers pronouncing the same two sentences. Theta chunks were classified into syllables based on the network response to the two sentences uttered by 99 other speakers. The material included sentences spoken by 462 speakers of various ethnic and geographical origins, showing a marked heterogeneity in phonemic realization and syllable durations (as labelled by phoneticians). The syllable duration distribution was skewed with the median at 200 ms and tail values ranging from a few ms to over 800 ms (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). Given that theta activity is meant to operate in a 3–9 Hz range, i.e., integrate speech chunks of about 100–300 ms (<xref ref-type="bibr" rid="bib23">Ghitza, 2011</xref>, <xref ref-type="bibr" rid="bib24">2014</xref>), we did not expect the model to perform equally well along the whole syllable duration range. Accordingly, decoding accuracy was not uniform across the whole syllable duration range. When decoding from spike pattern, the intact model allowed 24% accuracy (chance level at 10%). It showed a peak in performance in the range in which it is expected to operate, i.e., for syllables durations between 100 and 300 ms. Given the cross-speaker phonemic variability such a performance is fairly good. Critically, the intact model outperformed control models both within the 100 to 300 ms range (p &lt; 0.001), and throughout the whole syllable duration span (p &lt; 0.001). These analyses overall show that the model can flexibly track syllables within a physiological operating window, and that syllable decoding relies on the integrity of the model architecture.</p><p>Lastly, we tested more directly the resilience of the spike pattern code to speech temporal compression and found that while degrading the decoding performance remained above chance for compression rates of 2 and 3 (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), mimicking humans decoding performance (<xref ref-type="bibr" rid="bib2">Ahissar et al., 2001</xref>). Altogether, the decoding of syllables from continuous speech showed that coupled theta and gamma oscillations provide a viable instrument for syllable parsing and decoding, and that its performance relies on the coupling between the two oscillation networks.</p></sec><sec id="s2-6"><title>Encoding properties of model neurons</title><p>We finally assessed the physiological plausibility of the model by comparing the encoding properties of the simulated neurons, without further parameter fitting, with those of neurons recorded from primate auditory cortex (<xref ref-type="bibr" rid="bib36">Kayser et al., 2009</xref>; <xref ref-type="bibr" rid="bib35">2012</xref>). The first analysis of neural encoding properties consisted of comparing the ability to classify neural codes from the model into arbitrary speech segments of fixed duration (as opposed to classification into syllables as in previous section). We simulated data using natural speech and studied the spiking activity of <italic>Ge</italic> neurons by implementing the same methods of analysis as in the original experiment. We extracted fixed-size windows of spike patterns activity for individual <italic>Ge</italic> neurons, and assessed neural encoding characteristics using different neural codes. Speech encoding was first evaluated using a nearest-mean classifier and then using mutual information techniques (<xref ref-type="bibr" rid="bib36">Kayser et al., 2009</xref>).</p><sec id="s2-6-1"><title>Classifier analysis</title><p>In this analysis, neural patterns were classified not into syllables as above or into any linguistic constituent but into arbitrary segments of speech, allowing for a-theoretical insight into the encoding properties of neurons. We extracted a subset of 25 sentences from the TIMIT corpus and exposed the network to 50 presentations of each sentence from the subset. We defined 10 stimuli as 10 distinct windows of a given size (from 80 to 480 ms) randomly extracted from the 25 sentences, and then assessed the capacity to decode the identity of a stimulus from the activity of individual <italic>Ge</italic> neurons within that window (<xref ref-type="bibr" rid="bib35">Kayser et al., 2012</xref>). Three different codes were used (<xref ref-type="fig" rid="fig5">Figure 5A</xref>): a simple <italic>spike count</italic> was used as reference code; a <italic>time-partitioned code</italic> where spikes were assigned to one of 8 bins of equal duration within the temporal window; a <italic>phase-partitioned code</italic> where spikes were labelled with the phase of LFP theta at the timing of spike (the spikes were then assigned into one of 8 bins according to their phase).<fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.06213.010</object-id><label>Figure 5.</label><caption><title>Comparison with encoding properties of auditory cortical neurons.</title><p>(<bold>A</bold>) Neural codes. Stimulus decoding was performed on patterns of <italic>Ge</italic> spikes chunked in fixed-size windows (the figure illustrates the pattern for one neuron extracted from one window). <italic>Spike count</italic> consisted of counting all spikes for each neuron within the window. <italic>Time-partitioned code</italic> was obtained in dividing the window in <italic>N</italic> equal size bins (vertical grey bars) and counting spikes within each bin. <italic>Phase-partitioned code</italic> was obtained by binning LFP phase into <italic>N</italic> bins (depicted by the four colours in the top graph) and assigning each spike with the corresponding phase bin. (<bold>B</bold>) Spike pattern decoding. (Left) Decoding performance across <italic>Ge n</italic>eurons for the intact model using <italic>N =</italic> 8 bins for each code: spike count (black curve), time-partitioned (blue curve), and phase-partitioned codes (green curve). (Right) Data from the original experiment. Adapted from <xref ref-type="bibr" rid="bib35">Kayser et al., 2012</xref>. (<bold>C</bold>) Mutual information (MI). (Left) Mean MI between stimulus and individual output neuron activity during sentence processing in the intact model for spike count (black curve), time-partitioned (blue line), combined count and phase-partitioned (green line) and combined time- and phase-partitioned codes (red line). (Right) Comparison with experimental data from auditory cortex neurons (adapted from <xref ref-type="bibr" rid="bib36">Kayser et al., 2009</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.010">http://dx.doi.org/10.7554/eLife.06213.010</ext-link></p></caption><graphic xlink:href="elife-06213-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06213.011</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Speech decoding performance and MI (control models).</title><p>(<bold>A</bold>) Stimulus decoding performance for each neural code across <italic>Ge</italic> neurons for the control models (left:undriven theta; right: no theta–gamma connection): spike count (black line); time-partitioned neural code (blue line); phase-partitioned neural code (green line). (<bold>B</bold>) Stimulus decoding performance as a function of bin number, for all three variants of the model and experimental data. The number of bins used to partition the spikes was varied from 2 to 16, while the duration of the window was kept at 160 ms. Each dot corresponds to the average over 1000 different sets of stimuli and neuron (bars represent s.e.m.). Data from the original experiment, recording auditory cortex neurons from monkeys listening to naturalistic sounds. Experimental data are reproduced qualitatively by the intact model but not by the control model. Adapted from <xref ref-type="fig" rid="fig3">Figure 3E</xref> of <xref ref-type="bibr" rid="bib35">Kayser et al., 2012</xref>. (<bold>C</bold>) Mutual Information (MI) between acoustic stimulus and individual <italic>Ge</italic> neurons for the control models (left: undriven theta; right: no theta–gamma connection): spike count (black line); time-partitioned neural code (blue line); phase-partitioned and spike count neural code (green line); phase- and time-partitioned neural code (red lines). Both control models display low MI values and fail to display the pattern of experimental data shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.011">http://dx.doi.org/10.7554/eLife.06213.011</ext-link></p></caption><graphic xlink:href="elife-06213-fig5-figsupp1-v3.tif"/></fig></fig-group></p><p>We observed that for 80 to 240 ms windows (within one theta cycle), decoding was almost as good for the phase-partitioned code as for the time-partitioned code (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, left). In other words, stimulus decoding using theta timing was nearly as good as when using stimulus timing. Performance using the spike count was considerably lower (p &lt; 10<sup>−12</sup> for all 6 window sizes). Overall, there was a qualitative and even quantitative match between the results from simulated data and the original experimental results (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, right). When we removed either the input-to-theta (undriven theta model) or the theta-to-gamma connection (uncoupled theta/gamma model) in the network, the performance of the phase-partitioned code dropped to just above that of the spike count code (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>; significantly lower increase in decoding performance using phase-partitioned instead of spike count code compared to full model, p &lt; 10<sup>−12</sup> for all 6 window sizes and both control models), and the simulations no longer predicted the experimental results. Finally, experimental data and simulations from the intact model also matched when we investigated the dependence of decoding accuracy on the number of bins, which was not the case for any of the control models (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>).</p></sec><sec id="s2-6-2"><title>Mutual information (MI) analysis</title><p>MI between the input (acoustic stimulus) and the output (neural pattern) provides an alternative measure for how well stimuli are encoded in the output pattern (see ‘Materials and methods’). We used the same simulation data as for the classification procedure, but the sentences were subdivided into shorter chunks using a non-overlapping time window (length T: 8–48 ms) (<xref ref-type="bibr" rid="bib36">Kayser et al., 2009</xref>). We compared the MI between the stimulus and neural activity in individual <italic>Ge</italic> neurons as a function of the length of stimulus window, using four neural codes: spike count, time-partitioned code, phase-partitioned code combined with spike count and finally combined phase- and time-partitioned codes. These codes are qualitatively equivalent to the decoding strategies used in the previous classifier analysis. <xref ref-type="fig" rid="fig5">Figure 5C</xref> shows that taking into account the spike phase boosts the MI carried by the <italic>Spike count code</italic> or the <italic>Time-partitioned code</italic> alone (p &lt; 10<sup>−12</sup> for all 6 window sizes). In other words, spike phase provided additional rather than redundant information to more traditional codes. The gain provided by spike phase increased when enlarging the window and when combined with either spike count or spike pattern (<italic>Spike Count</italic> vs <italic>Time-partitioned</italic>; <italic>Spike count and Phase-partitioned code</italic> vs <italic>Time- and Phase-partitioned code</italic>). These results replicate the original experimental data from monkey auditory cortex (<xref ref-type="bibr" rid="bib36">Kayser et al., 2009</xref>). Such a pattern was not reproduced using any of the control models (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>). These results hence show that in addition to enhancing the reliability of the spike phase code, the theta–gamma connection enhanced the temporal precision of <italic>Ge</italic> neurons spiking in response to speech stimuli.</p><p>Critically, results from both classifier and mutual information analyses demonstrate that the full network architecture of the model provides an efficient way of boosting the encoding capacity of neurons in a way that bears remarkable similarities to actual neurons from primate auditory cortex.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Like most complex natural patterns, speech contains rhythmic activity at different scales that conveys different and sometimes non-independent categories of information. Using a biophysically inspired model of auditory cortex function, we show that cortical theta–gamma cross-frequency coupling provides a means of using the timing of syllables to orchestrate the readout of speech-induced gamma activity. The current modelling data demonstrate that theta bursts generated by a theta (PINTH) network can predict ‘on-line’ syllable boundaries at least as accurately as state-of-the-art offline syllable detection algorithms. Syllable boundary detection by a theta network hence provides an endogenous time reference for speech decoding. Our simulated data further show that a gamma biophysical network, receiving a spectral decomposition of speech as input, can take advantage of the theta time reference to encode fast phonemic information. The central result of our work is that the gamma network could efficiently encode temporal patterns (from simple sawtooths to natural speech), as long as it was entrained by the theta rhythm driven by syllable boundaries. The proposed theta/gamma network displayed sophisticated spectral and encoding properties that compared both qualitatively and quantitatively to existing neurophysiological evidence including cross-frequency coupling properties (<xref ref-type="bibr" rid="bib67">Schroeder and Lakatos, 2009</xref>) and theta-referenced stimulus encoding (<xref ref-type="bibr" rid="bib36">Kayser et al., 2009</xref>; <xref ref-type="bibr" rid="bib35">2012</xref>). The projections from the <italic>Te</italic> to <italic>Ge</italic> neurons endowed the network with phase-amplitude and phase-frequency coupling between gamma and theta oscillations, at both the spike and the LFP levels (<xref ref-type="bibr" rid="bib32">Jensen and Colgin, 2007</xref>). This closely reproduces the theta/gamma phase-amplitude coupling observed from intracortical recordings (<xref ref-type="bibr" rid="bib25">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib39">Lakatos et al., 2005</xref>). Importantly, due to the dissociation of excitatory populations we obtained denser gamma spiking immediately after the theta burst evoked by the syllable onset. This validates a critical point of theta/gamma parsing system, namely that a more in-depth encoding is carried-out by the auditory cortex during the early phase of syllables, when more information needs to be extracted (<xref ref-type="bibr" rid="bib67">Schroeder and Lakatos, 2009</xref>; <xref ref-type="bibr" rid="bib25">Giraud and Poeppel, 2012</xref>).</p><p>The human auditory system, like other sensory systems, is able to produce invariant responses to different physical presentations of the same input. Importantly, it is relatively insensitive to the speed at which speech is being produced. Speech can double in speed from one speaker to another and yet remain intelligible up to an artificial compression factor of 3 (<xref ref-type="bibr" rid="bib2">Ahissar et al., 2001</xref>). In the current model, theta bursts could still signal syllable boundaries when speech was compressed by a factor 2 and this alignment deteriorated for higher compression factors. Syllable decoding was significantly degraded for compressed speech, yet remained twice as accurate as chance. Our network is purely bottom-up and does not include high level linguistic processes and representations, which in all likelihood plays an important role in speech perception (<xref ref-type="bibr" rid="bib15">Davis et al., 2011</xref>; <xref ref-type="bibr" rid="bib61">Peelle et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Gagnepain et al., 2012</xref>): its relative resilience to speech compression is thus a fairly good performance. A previous model (<xref ref-type="bibr" rid="bib28">Gütig and Sompolinsky, 2009</xref>) proposed a neural code that was robust to speech warping, based on the notion that individual neurons correct for speech rate by their overall level of activity. While this model achieved very good speech categorization performance, it relied on extremely precise spiking behaviour (neurons spiked only once, when their associated channel reached a certain threshold), for which neurophysiological evidence is scarce. Another model developed by Hopfield proposes that a low gamma external current provides encoding neurons with reliable timing and dynamical memory spanning up to 200 ms, a long enough window to integrate information over a full syllable (<xref ref-type="bibr" rid="bib30">Hopfield, 2004</xref>). The utility of gamma oscillations for precise spiking is arguably similar in both Hopfield's model and ours, whereas the syllable integration process is irregularly ensured by intermittent traces of recent (∼200 ms) neural activity in Hopfield's, and in ours by regularly spaced theta bursts that are locked to the speech signal. The advantage of our model is that integration over long speech segments is <italic>permanently</italic> enabled by the phase of output spikes with respect to the ongoing theta oscillation. Our approach shows that accurate encoding can be achieved using a system that does not require explicit memory processes, and in which the temporal integration buffer is only emulated by a slow neural oscillator aligned to speech dynamics.</p><p>In the current combined theta/gamma model, theta oscillations do not only act as a syllable-scale integration buffer, but also as a precise neural timer. Because syllabic contours are reflected in the slow modulations of speech, the theta oscillator can flexibly entrain to them (3–7 Hz, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>) and signal syllable boundaries. The spiking behaviour of theta neurons parallels experimental observations that a subset of neurons in A1 respond to the onset of naturalistic sounds (<xref ref-type="bibr" rid="bib19">Fishbach et al., 2001</xref>; <xref ref-type="bibr" rid="bib62">Phillips et al., 2002</xref>; <xref ref-type="bibr" rid="bib78">Wang et al., 2008</xref>), providing an endogenous time reference that serves as a landmark to decode from other neurons (<xref ref-type="bibr" rid="bib35">Kayser et al., 2012</xref>; <xref ref-type="bibr" rid="bib8">Brasselet et al., 2012</xref>; <xref ref-type="bibr" rid="bib57">Panzeri and Diamond, 2010</xref>; <xref ref-type="bibr" rid="bib58">Panzeri et al., 2014</xref>). This parallels the dissociation between <italic>Ge</italic> and <italic>Te</italic> units in our model: while <italic>Ge</italic> units are channel specific, <italic>Te</italic> units cover the whole acoustic spectrum, which allow them to respond quickly and reliably to the onset of all auditory stimuli (<xref ref-type="bibr" rid="bib8">Brasselet et al., 2012</xref>). In the model, however, theta neurons did not only discharge at stimulus onset but at regular landmarks along the speech signal, the syllable boundaries (<xref ref-type="bibr" rid="bib81">Zhou and Wang, 2010</xref>). These neurons, hence, tie together the fast neural activity of gamma excitatory neurons into strings of linguistically relevant chunks (syllables), acting like punctuation in written language (<xref ref-type="bibr" rid="bib44">Lisman and Buzsáki, 2008</xref>). This mechanism for segmentation is conceptually similar to the segmentation of neural codes by theta oscillations in the hippocampus during spatial navigation (<xref ref-type="bibr" rid="bib27">Gupta et al., 2012</xref>).</p><p>From an evolutionary viewpoint, because the theta rhythm is neither auditory- nor human-specific, it might have been incorporated as a speech-parsing tool in the course of language evolution. Likewise, human language presumably optimized the length of its main constituents, syllables, to the parsing capacity of the auditory cortex. As a result, syllables have the ideal temporal format to interface with, e.g., hippocampal memory processes, or with motor routines reflecting other types of rhythmic mechanical constrains, e.g., the natural motion rate of the jaw (4Hz) (<xref ref-type="bibr" rid="bib42">Lieberman, 1985</xref>).</p><p>Although conceptually promising, syllable tracking and speech encoding by a theta/gamma network, as proposed here, also show some limitations. While our current model is purely bottom-up, top-down predictions play a significant role in guiding speech perception (<xref ref-type="bibr" rid="bib4">Arnal and Giraud, 2012</xref>; <xref ref-type="bibr" rid="bib22">Gagnepain et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Poeppel et al., 2008</xref>) presumably across different frequency channels and processing timescales (<xref ref-type="bibr" rid="bib77">Wang, 2010</xref>; <xref ref-type="bibr" rid="bib6">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="bib21">Fontolan et al., 2014</xref>). How these predictions interplay with theta- and gamma-parsing activity remain unclear (<xref ref-type="bibr" rid="bib40">Lee et al., 2013</xref>). Experimental findings suggest that theta activity might be at the interface of bottom-up and top-down processes (<xref ref-type="bibr" rid="bib61">Peelle et al., 2013</xref>). Theta auditory activity is better synchronized to speech modulations when speech is intelligible, irrespective of its temporal or spectral structure (<xref ref-type="bibr" rid="bib47">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib61">Peelle et al., 2013</xref>). In the present model, theta activity bears an intrinsic temporal predictive function: it is driven by speech modulations, but is also resilient enough to syllable length variations to stay tuned to the global statistics of speech (average syllable duration). The model performed well above chance level when decoding syllables from a new speaker, showing flexibility in syllable tracking within a 3 to 9 Hz range. A natural follow-up of this work will hence be to explore how the intrinsic dynamics of theta and gamma activity interact not only with sensory input but also with linguistic top-down signals, e.g., word, sentence level predictions (<xref ref-type="bibr" rid="bib22">Gagnepain et al., 2012</xref>), and even cross-modal predictions (<xref ref-type="bibr" rid="bib5">Arnal et al., 2009</xref>). The trade-off between the autonomous functioning of theta and gamma oscillatory activity on one hand and their entrainment to sensory input on the other hand are at the core of future experimental and theoretical challenges.</p><p>In conclusion, our model provides a direct evidence that theta/gamma coupled oscillations can be a viable instrument to de-multiplex speech, and by extension to analyse complex sensory scenes at different timescales in parallel. By tying the gamma-organized spiking to the syllable boundaries, theta activity allows for decoding individual syllables in continuous speech streams. The model demonstrates the computational value of neural oscillations for parsing sensory stimuli based on their temporal properties and offers new perspectives for syllable-based automatic speech recognition (<xref ref-type="bibr" rid="bib79">Wu et al., 1997</xref>) and brain-machine interfaces using oscillation-based neuromorphic algorithms.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Architecture of the full model</title><p>The model is composed of 4 types of cells: theta inhibitory neurons (<italic>Ti,</italic> 10 neurons), theta excitatory cells (<italic>Te</italic>, 10 neurons), gamma inhibitory neurons (<italic>Gi</italic>, 32 neurons), and gamma excitatory neurons (<italic>Ge</italic>, 32 neurons) also called <italic>output</italic> neurons. All neurons were modeled as leaky integrate-and-fire neurons, where the dynamics of the membrane potential <italic>V</italic><sub><italic>i</italic></sub> of the neurons followed:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>C</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>Y</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>C</italic> is the capacitance of the membrane potential; <italic>g</italic><sub><italic>L</italic></sub> and <italic>V</italic><sub><italic>L</italic></sub> are the conductance and equilibrium potential of the leak current; <italic>I</italic><sup><italic>SYN</italic></sup><italic>, I</italic><sup><italic>INP</italic></sup> and <italic>I</italic><sup><italic>DC</italic></sup> are the synaptic and constant currents, respectively; <italic>η</italic>(t) is a Gaussian noise term of <italic>σ</italic><sub><italic>i</italic></sub> variance.</p><p>Whenever <italic>V</italic><sub><italic>i</italic></sub> reached the threshold potential <italic>V</italic><sub><italic>THR</italic></sub>, the neuron emitted a spike and <italic>V</italic><sub><italic>i</italic></sub> was turned back to <italic>V</italic><sub><italic>RESET</italic></sub>.</p><p><italic>I</italic><sup><italic>SYN</italic></sup> is the sum of all synaptic currents from all projecting neurons in the network:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>Y</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>Y</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>g</italic><sub><italic>ij</italic></sub> is the synaptic conductance of the <italic>j-to-i</italic> synapse, <italic>s</italic><sub><italic>ij</italic></sub>(t) is the corresponding activation variable, and <italic>V</italic><sup><italic>SYN</italic></sup> is the equilibrium potential of synaptic current (0 mV for excitatory neurons, −80 mV for inhibitory neurons). The activation variable <italic>s</italic><sub><italic>ij</italic></sub>(<italic>t</italic>) varies as follow:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>j</mml:mi><mml:mi>D</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf1"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf2"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>j</mml:mi><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are the time constants for synaptic rise and synaptic decay, respectively.</p><p>The connectivity among the cells is the following:<list list-type="order"><list-item><p><italic>Te</italic> and <italic>Ti</italic> are reciprocally connected with all-to-all connections, generating the PINTH rhythm. There were also all-to-all connections within <italic>Ti</italic> cells.</p></list-item><list-item><p><italic>Ge</italic> and <italic>Gi</italic> are also reciprocally connected with all-to-all connections, generating the PING rhythm.</p></list-item><list-item><p><italic>Te</italic> projected with all-to-all connections to <italic>Ge</italic> cells, enabling cross-frequency coupling.</p></list-item></list></p><p>Input current <italic>I</italic><sub><italic>i</italic></sub><sup><italic>INP</italic></sup>(t) is non-null only for <italic>Te</italic> and <italic>Ge</italic> cells and follows the equation:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>x</italic><sub><italic>c</italic></sub>(<italic>t</italic>) is the signal from channel <italic>c</italic> and <italic>ω</italic><sub><italic>ci</italic></sub> is the weight of the projection from channel <italic>c</italic> to unit <italic>i</italic><sub>.</sub></p><p>Input to <italic>Te</italic> units is computed by filtering the auditory spectrogram by an optimized 2D spectro-temporal kernel (see section <italic>LN model</italic> below). LFP signal was simulated by summing the absolute values of all synaptic currents to all excitatory cells (both <italic>Ge</italic> and <italic>Te</italic>), as in <xref ref-type="bibr" rid="bib48">Mazzoni et al. (2008)</xref>. All simulations were run on Matlab. Differential equations were solved using Euler method with a time step of 0.005 ms. Values for all parameters are provided in <xref ref-type="table" rid="tbl1">Tables 1</xref> and <xref ref-type="table" rid="tbl2">2</xref>.<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.06213.012</object-id><label>Table 1.</label><caption><p>Full network parameter set</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.012">http://dx.doi.org/10.7554/eLife.06213.012</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th>C</th><th><italic>V</italic><sub><italic>THR</italic></sub></th><th><italic>V</italic><sub><italic>RESET</italic></sub></th><th><italic>V</italic><sub><italic>K</italic></sub></th><th><italic>V</italic><sub><italic>L</italic></sub></th><th><italic>g</italic><sub><italic>L</italic></sub></th><th><inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf4"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf5"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></th></tr></thead><tbody><tr><td>Value</td><td>1 F/cm2</td><td>−40 mV</td><td>−87 mV</td><td>−100 mV</td><td>−67 mV</td><td>0.1</td><td>5/N<sub>Ge</sub></td><td>5/N<sub>Gi</sub></td><td>0.3/N<sub>Te</sub></td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th><inline-formula><mml:math id="inf8"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf9"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf10"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf11"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf12"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf13"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf14"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf15"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula></th></tr></thead><tbody><tr><td>Value</td><td>0.2 ms</td><td>4 ms</td><td>0.5 ms</td><td>5 ms</td><td>2 ms</td><td>20 ms</td><td>3</td><td>1</td></tr></tbody></table></table-wrap><table-wrap id="tbl2" position="float"><object-id pub-id-type="doi">10.7554/eLife.06213.013</object-id><label>Table 2.</label><caption><p>Optimal parameters for the LN model</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.013">http://dx.doi.org/10.7554/eLife.06213.013</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th><inline-formula><mml:math id="inf16"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf17"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></th><th><italic>DC</italic></th></tr></thead><tbody><tr><td>Value</td><td>0.0748</td><td>1.433</td><td>0.4672</td></tr></tbody></table></table-wrap></p></sec><sec id="s4-2"><title>Stimuli</title><p>We used oral recordings of English sentences produced by male and female speakers from the TIMIT database (<xref ref-type="bibr" rid="bib43">Linguistic Data Consortium, 1993</xref>). The sentences were first processed through a model of subcortical auditory processing (<xref ref-type="bibr" rid="bib11">Chi et al., 2005</xref>) to the sentences. The model decomposes the auditory input into 128 channels of different frequency bands, reproducing the cochlear filterbank (<ext-link ext-link-type="uri" xlink:href="http://www.isr.umd.edu/Labs/NSL/Software.htm">http://www.isr.umd.edu/Labs/NSL/Software.htm</ext-link>). The frequency-decomposed signals undergo a series of nonlinear filters reflecting the computations taking place in the auditory nerve and other subcortical nuclei. We then reduced the number of channels from 128 to 32 by averaging the signal of each group of four consecutive channels, and used these 32 channels as input to the network. Each channel projected onto a distinct <italic>Ge</italic> cell (i.e., specific connections, <inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). As for Te input, each channel was convolved by the temporal filter and projected to all <italic>Te</italic> cells (all-to-all connections). Such a convolution can be implemented by a population of relay neurons that transmit their input with a certain delay, here between 0 and 50 ms.</p><p>Phoneme identity and boundaries have been labelled by phoneticians in every sentence of the corpus. We used the Tsylb2 program (<xref ref-type="bibr" rid="bib20">Fisher, 1996</xref>) that automatically syllabifies phonetic transcriptions (<xref ref-type="bibr" rid="bib34">Kahn, 1976</xref>) to merge these sequences of phonemes into sequences of syllables according to English grammar rules and thus get a timing for syllable boundaries.</p><p>To address the resilience of the model to speech compression, we produced compressed sentences by applying a pitch-synchronous, overlap and add (PSOLA) procedure implemented by PRAAT, a speech analysis and modification software (<ext-link ext-link-type="uri" xlink:href="http://www.fon.hum.uva.nl/praat/">http://www.fon.hum.uva.nl/praat/</ext-link>). The procedure retains all spectral properties from the original speech data in the compressed process. The same precortical filters were then applied as for uncompressed data before feeding into the network.</p></sec><sec id="s4-3"><title>Syllable boundary prediction algorithms</title><p>Syllable boundaries triggered average (STAs) were computed as follow: for each syllable boundary (syllable onsets excluding the first of each sentence), we extracted a 700 ms window of the corresponding locked to the syllable boundary and averaged over all syllable boundaries. STAs were computed for speech envelope and for each channel of the <xref ref-type="bibr" rid="bib11">Chi et al. (2005)</xref> model.</p><sec id="s4-3-1"><title>Predictive models</title><p>We compared the performance of four distinct families of models to predict the timing of syllable boundaries based on speech envelope or speech audiogram: the Mermelstein algorithm, a Linear–Nonlinear (LN) model (a simplified integration-to-threshold algorithm), the entrained theta neural oscillator and a purely rhythmic control model. The four algorithms are presented in the sections below.</p><sec id="s4-3-1-1"><title>Mermelstein algorithm</title><p>The Mermelstein algorithm is a standard algorithm that predicts syllable boundaries by identifying troughs in the power of the speech signal (<xref ref-type="bibr" rid="bib49">Mermelstein, 1975</xref>; <xref ref-type="bibr" rid="bib76">Villing et al., 2004</xref>). The predicted boundaries are computed according to the following steps. First, extract the power of speech signal in the 500–4000 Hz range (grossly corresponding to formants) and low-pass filter at 40 Hz to remove fast fluctuations, defining a so-called <italic>loudness function</italic>. Second, for each sentence, compute the convex hull of the loudness signal and extract the maximum of the difference between the loudness signal and its convex hull. If that difference exceeds a certain threshold <italic>T</italic><sub><italic>min</italic></sub> and if the peak intensity of the interval of no more than <italic>P</italic><sub><italic>max</italic></sub> smaller than the peak intensity of the whole sentence, then that time of maximal difference is defined as a predicted boundary and the same procedure is applied recursively to the intervals to the left and right of that boundary. Parameters <italic>T</italic><sub><italic>min</italic></sub> and <italic>P</italic><sub><italic>max</italic></sub> were optimized to yield minimum prediction distance (see below), yielding <italic>T</italic><sub><italic>min</italic></sub> <italic>= 0.152 dB</italic> and <italic>P</italic><sub><italic>max</italic></sub> <italic>= 15.85 dB</italic>.</p><p>Note that this algorithm cannot be run <italic>online</italic> since the convex hull at a given time depends on the future value of speech power. Thus syllable boundaries can only be predicted after a certain delay, which makes it impractical for online speech comprehension as occurring in the human brain.</p></sec><sec id="s4-3-1-2"><title>LN model and variations</title><p>To evaluate the capacity of a simplified neural system to predict syllable boundaries, we trained a generalized linear point process model on the syllable data set. The model (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D</xref>) does not incorporate full neural dynamics but simply comprises a linear stimulus kernel followed by nonlinear function. The process issues a ‘spike’ or ‘syllable boundary signal’ whenever the output reaches a certain threshold (<xref ref-type="bibr" rid="bib63">Pillow et al., 2008</xref>). This signal is fed back into the nonlinear function (another kernel <italic>Ih</italic> is used here): such negative feedback loop implements a relative refractory period. This model is a generalization of the Linear–Nonlinear Poisson model, hence we refer to it simply as <italic>LN</italic> model. We used the 32 auditory channels as input to the model and trained it to maximize its syllable boundary prediction performance.</p><p>We looked for a linear filter that is separable in its temporal and spectral component. We first computed the Spike Triggered Average (or rather ‘Syllable Boundary Triggered Average’) for all 32 channels from 600 ms to 0 ms prior to the actual boundary in 10 ms time steps. Yet <italic>STA</italic> provides the optimal estimate for the linear kernel in a LN model only when stimulus consists of uncorrelated white noise (<xref ref-type="bibr" rid="bib12">Chichilnisky, 2001</xref>). To get the optimal values out of the white noise condition, we looked at the separable filter H that yields best prediction of the output, i.e., <inline-formula><mml:math id="inf19"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext>Ŷ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>t</mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mtext>H</mml:mtext></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where:<list list-type="bullet"><list-item><p><italic>Y</italic>(<italic>t</italic>) is a binary output equal to 1 if there is a syllabic boundary in the 10 ms interval, 0 otherwise,</p></list-item><list-item><p><italic>H</italic> is a separable spectro-temporal filter (i.e., H(ω, u) = S(ω)T(u) for all orders u and all frequencies ω. S and T are, respectively, the spectral and temporal component of filter H.</p></list-item><list-item><p><inline-formula><mml:math id="inf20"><mml:mrow><mml:mtext>Ŷ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>t</mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mtext>H</mml:mtext></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mtext>u,w</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mtext>H</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>w,u</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mtext>X</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>ω,t</mml:mtext><mml:mo>−</mml:mo><mml:mtext>u</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the value of auditory channel <italic>ω</italic> at time step t.</p></list-item></list></p><p>Optimal solutions of the system verify:<disp-formula id="equ6"><mml:math id="m6"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>u</mml:mi></mml:munder><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ξ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>∀</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>ω</mml:mi></mml:munder><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ξ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>∀</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext>t</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (i.e., <italic>R</italic> is the Spike Triggered Average)and <italic>M</italic> is the covariance tensor for <italic>X</italic>, i.e., <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Solutions to <italic>T</italic> and <italic>S</italic> for that system of equations can be approximated numerically using the following iterative procedure:<disp-formula id="equ7"><mml:math id="m7"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mo>∀</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mo>∀</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ξ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>and then stopping when the resulting square error <inline-formula><mml:math id="inf24"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ξ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> goes below a minimum value (we used a threshold of <inline-formula><mml:math id="inf25"><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). The first 6 components (i.e., time bins) of the temporal kernel (i.e., 0–50 ms) were also used for input convolution in the theta model. We did not integrate further components (60–400 ms) since their weight was much lower and its implementation by relay neurons seemed less realistic.</p><p>To retrieve the optimal value for all parameters of the model, we used the GLM matlab toolbox developed in the Pillow lab (<ext-link ext-link-type="uri" xlink:href="http://pillowlab.cps.utexas.edu/code_GLM.html">http://pillowlab.cps.utexas.edu/code_GLM.html</ext-link>), using as input the one-dimensional signal <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>ω</mml:mi></mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>. Other parameters of the <italic>LN</italic> model including the self-inhibition temporal kernel <italic>Ih</italic> were optimized using the gradient descent implemented in the toolbox. This method provides estimation for a stochastic generalized <italic>LN</italic> model. We were interested in assessing the performance of a deterministic <italic>LN</italic> model. We then run a deterministic model with the same parameters as the stochastic model plus one new free parameter describing the normalized time to next spike (in the stochastic model, that time is drawn from an exponential distribution). The value of <inline-formula><mml:math id="inf27"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> was optimized using the same minimization procedure used for others models (see Optimisation section below). Two other parameters were also optimized again, since this procedure minimized a different score than the GLM toolbox score: time scale of self-inhibition <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and constant input to the model <italic>DC</italic> (<xref ref-type="table" rid="tbl2">Table 2</xref>).</p><p>We made one last modification to this <italic>LN</italic> model. We optimized the model such that it would maximally fire not at the time of syllable boundaries but 10 ms posterior to that time (<italic>de facto</italic>, we simply slid the STA window by 10 ms). This provides a delayed signal but likely more reliable since it can use more information (notably the rebound in the auditory spectrogram that is present right after a syllable boundary).</p></sec><sec id="s4-3-1-3"><title>Theta model</title><p>The theta model is composed of the <italic>Te</italic> and <italic>Ti</italic> cells from the full network model described above, with the exact same parameter set. 11 parameters were optimized in the full model, 10 in the control model (see values in <xref ref-type="table" rid="tbl3">Table 3</xref>).<table-wrap id="tbl3" position="float"><object-id pub-id-type="doi">10.7554/eLife.06213.014</object-id><label>Table 3.</label><caption><p>Optimal parameters for the theta model</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.014">http://dx.doi.org/10.7554/eLife.06213.014</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Pars</th><th><inline-formula><mml:math id="inf29"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf30"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf31"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf32"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf33"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf34"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf35"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf36"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf38"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></th></tr></thead><tbody><tr><td>Value</td><td>0.282 A <inline-formula><mml:math id="inf39"><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mrow><mml:mtext>ms</mml:mtext></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mtext>cm</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula></td><td>2.028 A <inline-formula><mml:math id="inf40"><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mrow><mml:mtext>ms</mml:mtext></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mtext>cm</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula></td><td>24.3</td><td>30.36</td><td>15</td><td>1.25</td><td>0.0851</td><td>0.432</td><td>0.207</td><td>0.264</td></tr></tbody></table></table-wrap></p></sec><sec id="s4-3-1-4"><title>Control model</title><p>The control model was used to provide a baseline for assessing the performance of other models. Under these control conditions, predicted syllable boundaries were generated rhythmically at a fixed time interval, irrespective of the stimulus. The rate of the rhythmic process was varied from 1 Hz to 15 Hz in 0.5 Hz intervals. Such control model yielded better performance than another control model consisting of a homogeneous Poisson process. It thus provides a more stringent control for estimating the efficiency of other algorithms.</p></sec></sec><sec id="s4-3-2"><title>Model performance evaluation</title><p>We evaluated how well syllable boundaries predicted by any model matched with the boundaries derived from labelled speech data. As an evaluation metrics, we used a point process distance that is used to compare distance between spike trains (<xref ref-type="bibr" rid="bib74">Victor and Purpura, 1997</xref>). Shift cost was set to 20 s<sup>−1</sup> (in other words, a predicted and an actual boundary could be matched if they were no more than 50 msec apart).</p><p>To draw comparison between different models, for each level of compression, we computed the (non-normalized) distance measure for the theta model summed over all sentences in the test data set, as well as the average number of predicted boundaries per sentence. We then matched the theta model to a control rhythmic model with the same predicted syllabic rate, and computed the difference between the non-normalized distance for the theta model and for that matched rhythmic model.</p></sec><sec id="s4-3-3"><title>Optimisation</title><p>We optimized the parameters from all models to get the minimal normalized point process distance between predicted and actual boundaries in each sentence. Optimization was made using global gradient descent (function <italic>fminsearch</italic> in Matlab) and repeated with many initial points to avoid retaining a local minimum. Although both the theta model and the control model are intrinsically stochastic, the sample size was large enough for the objective function over the entire sample to be nearly deterministic, allowing for convergence of the gradient descent algorithm. The list of optimized parameters for each type of model is provided in the related model sections above. We split the entire TIMIT TRAIN data set (4620 sentences) into two data sets: a first data set of 1000 sentences was used to compute optimal parameters; final assessment of an algorithm performance with its optimal parameters was done on a separate set of 3620 sentences.</p></sec></sec><sec id="s4-4"><title>Analysis of model behaviour</title><sec id="s4-4-1"><title>LFP spectral analysis</title><p>Simulated LFP was downsampled to 1000 Hz before applying a time-frequency decomposition using complex Morlet wavelet transform, with all frequencies between 2 and 100 Hz with a 0.5 Hz precision. Coherence between stimulus and LFP signal was then computed for each time point <italic>t</italic> and each frequency <italic>f</italic> over 100 simulations using 100 distinct sentences <italic>sen</italic>, using the formula from <xref ref-type="bibr" rid="bib51">Mitra and Pesaran (1999)</xref>. Synchronized bursts of the PING or PINTH were detected using spike timings in <italic>Gi</italic> and <italic>Ti</italic> populations since spikes of inhibitory neurons were more synchronized than those of excitatory neurons. Synchronous bursts of spikes were detected within a given population whenever more than 10% of neurons in the population spikes within a 6 ms interval (15 ms for <italic>Ti</italic> cells).</p></sec><sec id="s4-4-2"><title>Cross-frequency coupling</title><p>We computed cross-frequency coupling from 50 simulations of the model, each with a different TIMIT sentence preceded by 1000–1500 ms rest.</p><p>For the LFP phase-amplitude coupling, we extracted phase and amplitude from all frequencies from 2 Hz to 70 Hz in 1 Hz interval, and computed the Modulation Index for all pairs of frequencies (<xref ref-type="bibr" rid="bib72">Tort et al., 2010</xref>). Data from all trials were concatenated (separately for spontaneous and speech-related activity) across all trials beforehand. To compute Modulation Index, in each condition, signal amplitude values <italic>x(f</italic><sub><italic>amp</italic></sub><italic>,t,sen)</italic> were binned in <italic>N =</italic> 18 different bins according to the simultaneous phase of <italic>x(f</italic><sub><italic>phase</italic></sub><italic>,t,sen)</italic>. For spike phase-amplitude coupling, we defined spike gamma amplitude as the number of <italic>Gi</italic> neurons spiking at a given gamma burst, and the spike theta phase was defined by linear interpolation from −π for a theta spike burst to +π for the subsequent theta burst.</p></sec></sec><sec id="s4-5"><title>Simple temporal patterns decoding</title><p>We first explored the model's performance using simple sawtooth signals (<xref ref-type="bibr" rid="bib68">Shamir et al., 2009</xref>), representing prototypical realizations of formant transitions in a given frequency band. Each stimulus consisted of a rising component between 0 and 1, followed by a decay component from 1 back to 0. The overall length of the sawtooth was 50 ms, and the relative position of the maximal point <italic>t</italic><sub><italic>MAX</italic></sub> between the starting point <italic>t</italic><sub><italic>START</italic></sub> and end point <italic>t</italic><sub><italic>END</italic></sub> was defined by a variable <italic>a =</italic> (<italic>t</italic><sub><italic>MAX</italic></sub> <italic>− t</italic><sub><italic>START</italic></sub>)/(<italic>t</italic><sub><italic>END</italic></sub> <italic>− t</italic><sub><italic>START</italic></sub>).</p><p>The input connectivity had to be slightly modified since sawtooths are one-dimensional signals in contrast to the multi-dimensional channel signals that we have to use for speech stimuli: for <italic>Te</italic> units, we used <inline-formula><mml:math id="inf41"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> = <italic>20</italic>; and for the connections to <italic>Ge</italic> units in line with the original model (<xref ref-type="bibr" rid="bib68">Shamir et al., 2009</xref>), we used different input levels across the population, ranging from 0.125 to 4 in 0.125 intervals. The rest of the model remained unchanged.</p><p>We simulated the response of the network to a series of 500 sawtooths with parameter <italic>a</italic> taking one of 10 equally spaced values within the [0 1] interval. Interstimulus interval varied randomly between 50 and 250 ms.</p><p>We compared the model's performance for different neural codes. For the ‘stimulus timing’ code (see ‘Results’ section), we extracted the spike pattern of output (<italic>Ge</italic>) neurons between 20 ms before and 70 ms after of each sawtooth onset. We computed the distance between all output spike patterns using a spike train distance measure (<xref ref-type="bibr" rid="bib74">Victor and Purpura, 1997</xref>), implemented in the Spike Train Analysis Toolkit (<ext-link ext-link-type="uri" xlink:href="http://neuroanalysis.org/toolkit/">http://neuroanalysis.org/toolkit/</ext-link>). We used a shift cost of 200 s<sup>−1</sup> corresponding to a timing resolution of 5 ms. We decoded the peak parameter using the simple leave-one-out clustering procedure of the STA toolkit, using a clustering exponent of −10. By comparing the ‘decoded parameter’, i.e., the parameter corresponding to the closest cluster, to the input sawtooth parameter, we built confusion matrices and computed decoding performance.</p><p>In the ‘theta-timing’ code, we extracted the spike pattern of output neuron in windows starting 20 before a theta burst and finishing 20 ms after the next theta burst (‘<italic>theta chunks</italic>’, <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Spike times within each chunk were referenced with respect to the onset of the window. Each spike pattern was labelled with the corresponding value of the stimulus if the theta burst occurred during the presentation of the stimulus, or with the label ‘rest’ if the theta burst occurred during an interstimulus interval. The same decoding analysis was applied on such internally referenced neural patterns, yielding a 11 × 11 confusion matrix (10 stimulus shapes and rest). Detection theory measures (hits, misses, correct rejections, and false alarms) were computed by summing values in blocks of the confusion matrix (of size 10 × 10, 10 × 1, 1 × 10, and 1 × 1, respectively). A classification confusion matrix was obtained by removing the last row and last column of that confusion matrix.</p><p>We run the same decoding analysis on variants of the network: the full network; a control model where <italic>Te</italic> units do not receive the sawtooth input (<italic>undriven theta network</italic>) and another control where theta–gamma connections were removed (<italic>uncoupled theta–gamma network</italic>).</p></sec><sec id="s4-6"><title>Syllable decoding from sentences</title><p>The classification procedure was similar for syllable decoding, where we tried to decode the identity of syllables within continuous stream of speech (full sentences) from the activity of output neurons. We stimulated the network by presenting 25 sentences from the TIMIT corpus repeated 100 times each. We extracted theta chunks of <italic>Ge</italic> spike patterns as explained previously. Each chunk was labelled with the identity of the syllable being presented at the time of the first theta burst of the chunk. We randomly selected 10 syllables from the whole set of syllables within the 25 sentences. As in some cases there were several consecutive theta chunks corresponding to the same syllable, we equated the total number of theta chunks per syllable by randomly selecting 100 theta chunks labelled with each of the 10 syllables. Syllable classification of theta-chunked <italic>Ge</italic> spike patterns was performed using two different neural codes. For the <italic>spike pattern code</italic>, we applied the same procedure as for sawtooth classification, using a smaller value of spike shift cost corresponding to a timing resolution of 60 ms. For the <italic>spike count code</italic>, we measured the number of spikes emitted by each <italic>Ge</italic> neuron within a theta chunk. We then ran a simple nearest-mean classification procedure to decode syllable identity corresponding to each theta chunk from the spike counts of all <italic>Ge</italic> neurons (see ‘Classification analysis’ below). Both methods relied on the leave-one-out procedure that consists in identifying a chunk after the decoder was trained on all chunks but the to-be-decoded one. Decoding was repeated 200 times using each time a different set of 10 random syllables, and the analysis was performed over all three variants of the network.</p><p>For syllable classification across speakers, we used the two sentences from the TIMIT corpus that have been recorded for each of the 462 speakers ('<italic>She had your dark suit in greasy wash water all year</italic>' and '<italic>Don't ask me to carry an oily rag like that</italic>') and trained the network to classify syllables based on the neural output from other speakers, thus testing generalization across speakers. There is a wide variability of pronunciations over speakers as attested by the variability of chain of phonemes labelled of phoneticians, but the two sentences could nonetheless be parsed into 25 syllables overall for each speaker. We simulated the network presenting these 924 sentences and used the theta-chunked output to decode syllable identity. The method used was very similar to the syllable decoding analysis, where we classified theta-chunked neural patterns into one of 10 possible syllables (drawn randomly from the set of 25 syllables), with the only difference that here the classifier was based on theta chunks coming from different speakers. The classification was repeated 100 times for different subsets of syllables.</p></sec><sec id="s4-7"><title>Neural encoding properties: classification analysis</title><p>The first analysis of neural encoding properties consisted in comparing the ability to classify neural codes from the model into arbitrary speech segments (as opposed to syllables as in previous section). The methods, as detailed below, were inspired by the decoding of neural auditory cortical activity recorded in monkeys in response to naturalistic sounds (<xref ref-type="bibr" rid="bib35">Kayser et al., 2012</xref>). We simulated the network by presenting 25 different sentences from the TIMIT corpus repeated 50 times each. For a given window size (ranging from 80 to 480 ms in 80 ms intervals), we randomly extracted 10 windows (defined as <italic>stimuli</italic>) from the overall set of 25 sentences. We then retrieved stimulus identity based on the activity of a neuron that was randomly drawn from the <italic>Ge</italic> population using three different neural codes. In the <italic>neural count code</italic>, we counted the number of spikes emitted by that neuron within each window. In the <italic>time-partitioned code</italic>, we divided each window into <italic>N</italic> equally size bins, and computed the number of spikes for each of the 8 bins separately. In the <italic>phase-partitioned code</italic>, we divided the window based on theta-phase- rather time-intervals: each spike was labelled with the phase of the theta oscillation at the corresponding spike time, and we computed the number of spikes falling into each of the <italic>N</italic> subdivisions of the [−π;π] interval.</p><p>We then used a nearest-mean template matching procedure to decode the stimuli. To classify each stimulus exemplar using each neural code, we averaged the vectors over all presentations of each stimulus using a leave-one-out procedure; we then computed the Euclidian distance from the current vector to each of the 10 stimulus-averaged template. Finally, we ‘decoded’ the neural code by assigning it to the stimulus class with minimal distance to template. A more detailed explanation of the procedure is provided in the original experiment article (<xref ref-type="bibr" rid="bib35">Kayser et al., 2012</xref>). The procedure was repeated 1000 times, each time with a different set of 10 random stimuli, and performed the 3 variants of network.</p></sec><sec id="s4-8"><title>Neural encoding properties: mutual information analysis</title><p>We complemented the stimulus classification with a similar analysis using mutual information between the acoustic ‘stimulus’ and response of individual <italic>Ge</italic> neurons to further characterize the encoding properties of the network. Mutual Information (<italic>MI</italic>) estimates the reduction of uncertainty about the acoustic ‘<italic>stimulus’</italic> that is obtained from the knowledge of a single trial of neural response. The data set was identical to the one previously used for stimulus classification analysis, where each stimulus was again segmented into non-overlapping windows of length T (here 8 to 48 ms) (<xref ref-type="bibr" rid="bib36">Kayser et al., 2009</xref>; <xref ref-type="bibr" rid="bib16">de Ruyter van Steveninck et al., Strong, 1997</xref>).</p><p>Mutual Information was computed for the same neural codes as in <xref ref-type="bibr" rid="bib36">Kayser et al. (2009)</xref>. We used <italic>Spike count code</italic> and <italic>Time-partitioned code</italic> as described above (for the Time-partitioned code the size of the bins was kept constant to 8 bins; the number of bins in a window hence increased with window size. As slow LFP phase was more reliable over sentence repetitions than power, we combined spike count and LFP theta phase to get a <italic>Spike count &amp; Phase-partitioned code</italic> (<xref ref-type="bibr" rid="bib52">Montemurro et al., 2008</xref>). For this code, the phase of slow LFP was divided into <italic>N =</italic> 4 bins, and the firing rate in each window was labelled according to the phase at which the first spike occurred. Finally, we explored the influence of slow LFP phase on MI when combined with temporal spiking patterns. Thus, in the <italic>Time- &amp; Phase-partitioned code</italic> spikes carry two distinct tags, the first one referring to the position of the spike inside one of the four subdivisions of the stimulus window, the second indicating the phase of the underlying LFP at the moment of the spike occurrence.</p><p>We corrected for sampling bias (<xref ref-type="bibr" rid="bib36">Kayser et al., 2009</xref>) first by using a <italic>shuffling</italic> method (<xref ref-type="bibr" rid="bib59">Panzeri et al., 2007</xref>), then the quadratic extrapolation method (<xref ref-type="bibr" rid="bib71">Strong et al., 1998</xref>). We further reduced the residual bias using a bootstrapping technique (200 resampled data) (<xref ref-type="bibr" rid="bib52">Montemurro et al., 2008</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was funded by the European Research Council (Compuslang project; Grant agreement 260347), the Swiss National Fund (grant 320030-149319), the Agence National de la Recherche, the CNRS. We warmly thank Oded Ghitza for stimulating discussions, Maoz Shamir and Andy Brughera for sharing elements of code with us, Adrien Wohrer for help with the mathematical analysis and Jean-Paul Haton for his input from the perspective of automatic speech recognition. BSG gratefully acknowledges partial support from the National Research University Higher School of Economics.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>AH, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>LF, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>CK, Acquisition of data, Analysis and interpretation of data</p></fn><fn fn-type="con" id="con4"><p>BG, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con5"><p>A-LG, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><sec id="s6-1" sec-type="datasets"><title>Major dataset</title><p>The following previously published dataset was used:</p><p><related-object content-type="existing-dataset" id="dataro1" source-id="https://catalog.ldc.upenn.edu/LDC93S1" source-id-type="uri"><collab>Linguistic Data Consortium (LDC)</collab>, <year>1993</year><x>, </x><source>TIMIT Acoustic-Phonetic Continuous Speech Corpus</source><x>, </x><ext-link ext-link-type="uri" xlink:href="https://catalog.ldc.upenn.edu/LDC93S1">https://catalog.ldc.upenn.edu/LDC93S1</ext-link><x>, </x><comment>Available from the Linguistic Data Consortium (registration required)</comment>.</related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abrams</surname><given-names>DA</given-names></name><name><surname>Nicol</surname><given-names>T</given-names></name><name><surname>Zecker</surname><given-names>S</given-names></name><name><surname>Kraus</surname><given-names>N</given-names></name></person-group><year>2008</year><article-title>Right-hemisphere auditory cortex is dominant for coding syllable patterns in speech</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>3958</fpage><lpage>3965</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0187-08.2008</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahissar</surname><given-names>E</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name><name><surname>Ahissar</surname><given-names>M</given-names></name><name><surname>Protopapas</surname><given-names>A</given-names></name><name><surname>Mahncke</surname><given-names>H</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name></person-group><year>2001</year><article-title>Speech comprehension is correlated with temporal response patterns recorded from auditory cortex</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>98</volume><fpage>13367</fpage><lpage>13372</lpage><pub-id pub-id-type="doi">10.1073/pnas.201400998</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ainsworth</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>S</given-names></name><name><surname>Cunningham</surname><given-names>MO</given-names></name><name><surname>Roopun</surname><given-names>AK</given-names></name><name><surname>Traub</surname><given-names>RD</given-names></name><name><surname>Kopell</surname><given-names>NJ</given-names></name><name><surname>Whittington</surname><given-names>MA</given-names></name></person-group><year>2011</year><article-title>Dual gamma rhythm generators control interlaminar synchrony in auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>17040</fpage><lpage>17051</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2209-11.2011</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year>2012</year><article-title>Cortical oscillations and sensory predictions</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>390</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.05.003</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Kell</surname><given-names>CA</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year>2009</year><article-title>Dual neural routing of visual facilitation in speech processing</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>13445</fpage><lpage>13453</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3194-09.2009</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Usrey</surname><given-names>WM</given-names></name><name><surname>Adams</surname><given-names>RA</given-names></name><name><surname>Mangun</surname><given-names>GR</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year>2012</year><article-title>Canonical microcircuits for predictive coding</article-title><source>Neuron</source><volume>76</volume><fpage>695</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Börgers</surname><given-names>C</given-names></name><name><surname>Kopell</surname><given-names>NJ</given-names></name></person-group><year>2005</year><article-title>Effects of noisy drive on rhythms in networks of excitatory and inhibitory neurons</article-title><source>Neural Computation</source><volume>17</volume><fpage>557</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1162/0899766053019908</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brasselet</surname><given-names>R</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year>2012</year><article-title>Neurons with stereotyped and rapid responses provide a reference frame for relative temporal coding in primate auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>2998</fpage><lpage>3008</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5435-11.2012</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year>2003</year><article-title>What determines the frequency of fast network oscillations with irregular neural discharges? I. Synaptic dynamics and excitation-inhibition balance</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>415</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1152/jn.01095.2002</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canolty</surname><given-names>RT</given-names></name><name><surname>Soltani</surname><given-names>M</given-names></name><name><surname>Dalal</surname><given-names>SS</given-names></name><name><surname>Edwards</surname><given-names>E</given-names></name><name><surname>Dronkers</surname><given-names>NF</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name><name><surname>Kirsch</surname><given-names>HE</given-names></name><name><surname>Barbaro</surname><given-names>NM</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name></person-group><year>2007</year><article-title>Spatiotemporal dynamics of word processing in the human brain</article-title><source>Frontiers in Neuroscience</source><volume>1</volume><fpage>185</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.1.1.014.2007</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname><given-names>T</given-names></name><name><surname>Ru</surname><given-names>P</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year>2005</year><article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title><source>The Journal of the Acoustical Society of America</source><volume>118</volume><fpage>887</fpage><lpage>906</lpage><pub-id pub-id-type="doi">10.1121/1.1945807</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year>2001</year><article-title>A simple white noise analysis of neuronal light responses</article-title><source>Network</source><volume>12</volume><fpage>199</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1080/713663221</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cogan</surname><given-names>GB</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year>2011</year><article-title>A mutual information analysis of neural coding of speech by low-frequency MEG phase information</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>554</fpage><lpage>563</lpage><pub-id pub-id-type="doi">10.1152/jn.00075.2011</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>da Costa</surname><given-names>NM</given-names></name><name><surname>Martin</surname><given-names>KA</given-names></name></person-group><year>2010</year><article-title>Whose cortical column would that be?</article-title><source>Frontiers in Neuroanatomy</source><volume>4</volume><fpage>16</fpage><pub-id pub-id-type="doi">10.3389/fnana.2010.00016</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Ford</surname><given-names>MA</given-names></name><name><surname>Kherif</surname><given-names>F</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name></person-group><year>2011</year><article-title>Does semantic context benefit speech understanding through “top-down” processes? Evidence from time-resolved sparse fMRI</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>3914</fpage><lpage>3932</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00084</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name><name><surname>Lewen</surname><given-names>GD</given-names></name><name><surname>Strong</surname><given-names>SP</given-names></name><name><surname>Koberle</surname><given-names>R</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year>1997</year><article-title>Reproducibility and variability in neural spike trains</article-title><source>Science</source><volume>275</volume><fpage>1805</fpage><lpage>1808</lpage><pub-id pub-id-type="doi">10.1126/science.275.5307.1805</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Douglas</surname><given-names>RJ</given-names></name><name><surname>Martin</surname><given-names>KA</given-names></name></person-group><year>2004</year><article-title>Neuronal circuits of the neocortex</article-title><source>Annual Review of Neuroscience</source><volume>27</volume><fpage>419</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144152</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanselow</surname><given-names>EE</given-names></name><name><surname>Richardson</surname><given-names>KA</given-names></name><name><surname>Connors</surname><given-names>BW</given-names></name></person-group><year>2008</year><article-title>Selective, state-dependent activation of somatostatin-expressing inhibitory interneurons in mouse neocortex</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>2640</fpage><lpage>2652</lpage><pub-id pub-id-type="doi">10.1152/jn.90691.2008</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fishbach</surname><given-names>A</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>Yeshurun</surname><given-names>Y</given-names></name></person-group><year>2001</year><article-title>Auditory edge detection: a neural model for physiological and psychoacoustical responses to amplitude transients</article-title><source>Journal of Neurophysiology</source><volume>85</volume><fpage>2303</fpage><lpage>2323</lpage></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>WM</given-names></name></person-group><year>1996</year><article-title>tsylb2</article-title><publisher-name>National Institute of Standards and Technology</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="http://www.nist.gov/speech/tools">http://www.nist.gov/speech/tools</ext-link></comment></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Liégeois-Chauvel</surname><given-names>C</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year>2014</year><article-title>The contribution of frequency-specific activity to hierarchical information processing in the human auditory cortex</article-title><source>Nature Communications</source><volume>5</volume><fpage>4694</fpage><pub-id pub-id-type="doi">10.1038/ncomms5694</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gagnepain</surname><given-names>P</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year>2012</year><article-title>Temporal predictive codes for spoken words in auditory cortex</article-title><source>Current Biology</source><volume>22</volume><fpage>615</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.02.015</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname><given-names>O</given-names></name></person-group><year>2011</year><article-title>Linking speech perception and neurophysiology: speech decoding guided by cascaded oscillators locked to the input rhythm</article-title><source>Frontiers in Psychology</source><volume>2</volume><fpage>130</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00130</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname><given-names>O</given-names></name></person-group><year>2014</year><article-title>Behavioral evidence for the role of cortical θ oscillations in determining auditory channel capacity for speech</article-title><source>Frontiers in Psychology</source><volume>5</volume><fpage>652</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00652</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year>2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Hoogenboom</surname><given-names>N</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Schyns</surname><given-names>P</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name></person-group><year>2013</year><article-title>Speech rhythms and multiplexed oscillatory sensory coding in the human brain</article-title><source>PLOS Biology</source><volume>11</volume><fpage>e1001752</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.1001752</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>AS</given-names></name><name><surname>van der Meer</surname><given-names>MA</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year>2012</year><article-title>Segmentation of spatial experience by hippocampal theta sequences</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1032</fpage><lpage>1039</lpage><pub-id pub-id-type="doi">10.1038/nn.3138</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gütig</surname><given-names>R</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year>2009</year><article-title>Time-warp-invariant neuronal processing</article-title><source>PLOS Biology</source><volume>7</volume><fpage>e1000141</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.1000141</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henry</surname><given-names>MJ</given-names></name><name><surname>Herrmann</surname><given-names>B</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><year>2014</year><article-title>Entrained neural oscillations in multiple frequency bands comodulate behavior</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>111</volume><fpage>14935</fpage><lpage>14940</lpage><pub-id pub-id-type="doi">10.1073/pnas.1408741111</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><year>2004</year><article-title>Encoding for computation: recognizing brief dynamical patterns by exploiting effects of weak rhythms on action-potential timing</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>101</volume><fpage>6255</fpage><lpage>6260</lpage><pub-id pub-id-type="doi">10.1073/pnas.0401125101</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jadi</surname><given-names>MP</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year>2014</year><article-title>Cortical oscillations arise from contextual interactions that regulate sparse coding</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>111</volume><fpage>6780</fpage><lpage>6785</lpage><pub-id pub-id-type="doi">10.1073/pnas.1405300111</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Colgin</surname><given-names>LL</given-names></name></person-group><year>2007</year><article-title>Cross-frequency coupling between neuronal oscillations</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>267</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.05.003</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Lisman</surname><given-names>JE</given-names></name></person-group><year>1996</year><article-title>Theta/gamma networks with slow NMDA channels learn sequences and encode episodic memory: role of NMDA channels in recall</article-title><source>Learning &amp; Memory</source><volume>3</volume><fpage>264</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1101/lm.3.2-3.264</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kahn</surname><given-names>D</given-names></name></person-group><year>1976</year><source>Syllable-based generalizations in English phonology</source><comment><ext-link ext-link-type="uri" xlink:href="http://seas3.elte.hu/szigetva/courses/syllable/kahn76-pres-szpsyllableracz.pdf">http://seas3.elte.hu/szigetva/courses/syllable/kahn76-pres-szpsyllableracz.pdf</ext-link></comment></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year>2012</year><article-title>Analysis of slow (theta) oscillations as a potential temporal reference frame for information coding in sensory cortices</article-title><source>PLOS Computational Biology</source><volume>8</volume><fpage>e1002717</fpage><comment>Edited by Tim Behrens</comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002717</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Montemurro</surname><given-names>MA</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year>2009</year><article-title>Spike-phase coding boosts and stabilizes information carried by spatial and temporal spike patterns</article-title><source>Neuron</source><volume>61</volume><fpage>597</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.008</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kopell</surname><given-names>NJ</given-names></name><name><surname>Börgers</surname><given-names>C</given-names></name><name><surname>Pervouchine</surname><given-names>DD</given-names></name><name><surname>Malerba</surname><given-names>P</given-names></name></person-group><year>2010</year><article-title>Gamma and theta rhythms in biophysical models of hippocampal circuits</article-title><person-group person-group-type="editor"><name><surname>Cutsuridis</surname><given-names>V</given-names></name></person-group><source>Hippocampal microcircuits</source><publisher-name>Springer</publisher-name><fpage>423</fpage><lpage>457</lpage></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>CM</given-names></name><name><surname>O'Connell</surname><given-names>MN</given-names></name><name><surname>Mills</surname><given-names>A</given-names></name></person-group><year>2007</year><article-title>Neuronal oscillations and multisensory interaction in primary auditory cortex</article-title><source>Neuron</source><volume>53</volume><fpage>279</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.12.011</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>PP</given-names></name><name><surname>Shah</surname><given-names>AS</given-names></name><name><surname>Knuth</surname><given-names>KH</given-names></name><name><surname>Ulbert</surname><given-names>I</given-names></name><name><surname>Karmos</surname><given-names>G</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year>2005</year><article-title>An oscillatory hierarchy controlling neuronal excitability and stimulus processing in the auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>1904</fpage><lpage>1911</lpage><pub-id pub-id-type="doi">10.1152/jn.00263.2005</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Whittington</surname><given-names>MA</given-names></name><name><surname>Kopell</surname><given-names>NJ</given-names></name></person-group><year>2013</year><article-title>Top-down beta rhythms support selective attention via interlaminar interaction: a model</article-title><source>PLOS Computational Biology</source><volume>9</volume><fpage>e1003164</fpage><comment>Edited by Stephen Coombes</comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003164</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lever</surname><given-names>C</given-names></name><name><surname>Kaplan</surname><given-names>R</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><year>2014</year><person-group person-group-type="editor"><name><surname>Derdikman</surname><given-names>D</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name></person-group><source>Space,Time and Memory in the Hippocampal Formation</source><publisher-loc>Vienna</publisher-loc><publisher-name>Springer Vienna</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-7091-1292-2</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieberman</surname><given-names>P</given-names></name></person-group><year>1985</year><article-title>On the evolution of human syntactic ability. Its pre-adaptive Bases—Motor control and speech</article-title><source>Journal of Human Evolution</source><volume>14</volume><fpage>657</fpage><lpage>668</lpage><pub-id pub-id-type="doi">10.1016/S0047-2484(85)80074-9</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><collab>Linguistic Data Consortium</collab></person-group><year>1993</year><article-title>TIMIT acoustic-phonetic continuous speech corpus</article-title><comment><ext-link ext-link-type="uri" xlink:href="https://catalog.ldc.upenn.edu/LDC93S1">https://catalog.ldc.upenn.edu/LDC93S1</ext-link></comment></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname><given-names>JE</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year>2008</year><article-title>A neural coding scheme formed by the combined function of gamma and theta oscillations</article-title><source>Schizophrenia Bulletin</source><volume>34</volume><fpage>974</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1093/schbul/sbn060</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname><given-names>JE</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year>2013</year><article-title>The theta-gamma neural code</article-title><source>Neuron</source><volume>77</volume><fpage>1002</fpage><lpage>1016</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.03.007</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>H</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year>2010</year><article-title>Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulation</article-title><source>PLOS Biology</source><volume>8</volume><fpage>e1000445</fpage><comment>Edited by Robert Zatorre</comment><pub-id pub-id-type="doi">10.1371/journal.pbio.1000445</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>H</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year>2007</year><article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title><source>Neuron</source><volume>54</volume><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.004</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazzoni</surname><given-names>A</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year>2008</year><article-title>Encoding of naturalistic stimuli by local field potential spectra in networks of excitatory and inhibitory neurons</article-title><source>PLOS Computational Biology</source><volume>4</volume><fpage>e1000239</fpage><comment>Edited by KarlJ Friston</comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000239</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mermelstein</surname><given-names>P</given-names></name></person-group><year>1975</year><article-title>Automatic segmentation of speech into syllabic units</article-title><source>The Journal of the Acoustical Society of America</source><volume>58</volume><fpage>880</fpage><lpage>883</lpage><pub-id pub-id-type="doi">10.1121/1.380738</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>JL</given-names></name><name><surname>Grosjean</surname><given-names>F</given-names></name><name><surname>Lomanto</surname><given-names>C</given-names></name></person-group><year>1984</year><article-title>Articulation rate and its variability in spontaneous speech: a reanalysis and some implications</article-title><source>Phonetica</source><volume>41</volume><fpage>215</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1159/000261728</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitra</surname><given-names>PP</given-names></name><name><surname>Pesaran</surname><given-names>B</given-names></name></person-group><year>1999</year><article-title>Analysis of dynamic brain imaging data</article-title><source>Biophysical Journal</source><volume>76</volume><fpage>691</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(99)77236-X</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montemurro</surname><given-names>MA</given-names></name><name><surname>Rasch</surname><given-names>MJ</given-names></name><name><surname>Murayama</surname><given-names>Y</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year>2008</year><article-title>Phase-of-firing coding of natural visual stimuli in primary visual cortex</article-title><source>Current Biology</source><volume>18</volume><fpage>375</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.02.023</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Liégeois-Chauvel</surname><given-names>C</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Bénar</surname><given-names>CG</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year>2012</year><article-title>Asymmetric function of theta and gamma activity in syllable processing: an Intra-cortical study</article-title><source>Frontiers in Psychology</source><volume>3</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00248</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukamel</surname><given-names>EA</given-names></name><name><surname>Wong</surname><given-names>KF</given-names></name><name><surname>Prerau</surname><given-names>MJ</given-names></name><name><surname>Brown</surname><given-names>EN</given-names></name><name><surname>Purdon</surname><given-names>PL</given-names></name></person-group><year>2011</year><article-title>Phase-based measures of cross-frequency coupling in brain electrical dynamics under general anesthesia</article-title><source>Conference Proceedings: Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source><volume>2011</volume><fpage>1981</fpage><lpage>1984</lpage><pub-id pub-id-type="doi">10.1109/IEMBS.2011.6090558</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ngon</surname><given-names>C</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name><name><surname>Dupoux</surname><given-names>E</given-names></name><name><surname>Cabrol</surname><given-names>D</given-names></name><name><surname>Dutat</surname><given-names>M</given-names></name><name><surname>Peperkamp</surname><given-names>S</given-names></name></person-group><year>2013</year><article-title>(Non)words, (non)words, (non)words: evidence for a protolexicon during the first year of life</article-title><source>Developmental Science</source><volume>16</volume><fpage>24</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1111/j.1467-7687.2012.01189.x</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nourski</surname><given-names>KV</given-names></name><name><surname>Reale</surname><given-names>RA</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Kawasaki</surname><given-names>H</given-names></name><name><surname>Kovach</surname><given-names>CK</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Howard</surname><given-names>MA</given-names></name><name><surname>Brugge</surname><given-names>JF</given-names></name></person-group><year>2009</year><article-title>Temporal envelope of time-compressed speech represented in the human auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>15564</fpage><lpage>15574</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3065-09.2009</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name></person-group><year>2010</year><article-title>Information carried by population spike times in the whisker sensory cortex can be decoded without knowledge of stimulus time</article-title><source>Frontiers in Synaptic Neuroscience</source><volume>2</volume><fpage>17</fpage><pub-id pub-id-type="doi">10.3389/fnsyn.2010.00017</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year>2014</year><article-title>Reading spike timing without a clock: intrinsic decoding of spike trains</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>369</volume><fpage>20120467</fpage><pub-id pub-id-type="doi">10.1098/rstb.2012.0467</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Senatore</surname><given-names>R</given-names></name><name><surname>Montemurro</surname><given-names>MA</given-names></name><name><surname>Petersen</surname><given-names>RS</given-names></name></person-group><year>2007</year><article-title>Correcting for the sampling bias problem in spike train information measures</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>1064</fpage><lpage>1072</lpage><pub-id pub-id-type="doi">10.1152/jn.00559.2007</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasley</surname><given-names>BN</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year>2012</year><article-title>Reconstructing speech from human auditory cortex</article-title><source>PLOS Biology</source><volume>10</volume><fpage>e1001251</fpage><comment>Edited by Robert Zatorre</comment><pub-id pub-id-type="doi">10.1371/journal.pbio.1001251</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year>2013</year><article-title>Phase-locked responses to speech in human auditory cortex are enhanced during comprehension</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>1378</fpage><lpage>1387</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs118</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>DP</given-names></name><name><surname>Hall</surname><given-names>SE</given-names></name><name><surname>Boehnke</surname><given-names>SE</given-names></name></person-group><year>2002</year><article-title>Central auditory onset responses, and temporal asymmetries in auditory perception</article-title><source>Hearing Research</source><volume>167</volume><fpage>192</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(02)00393-3</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year>2008</year><article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title><source>Nature</source><volume>454</volume><fpage>995</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/nature07140</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year>2003</year><article-title>The analysis of speech in different temporal integration windows: cerebral lateralization as ‘asymmetric sampling in time’</article-title><source>Speech Communication</source><volume>41</volume><fpage>245</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/S0167-6393(02)00107-3</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Idsardi</surname><given-names>WJ</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name></person-group><year>2008</year><article-title>Speech perception at the interface of neurobiology and linguistics</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>363</volume><fpage>1071</fpage><lpage>1086</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2160</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname><given-names>S</given-names></name></person-group><year>1992</year><article-title>Temporal information in speech: acoustic, auditory and linguistic aspects</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>336</volume><fpage>367</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1098/rstb.1992.0070</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name></person-group><year>2009</year><article-title>The gamma oscillation: master or slave?</article-title><source>Brain Topography</source><volume>22</volume><fpage>24</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1007/s10548-009-0080-y</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamir</surname><given-names>M</given-names></name><name><surname>Ghitza</surname><given-names>O</given-names></name><name><surname>Epstein</surname><given-names>S</given-names></name><name><surname>Kopell</surname><given-names>NJ</given-names></name></person-group><year>2009</year><article-title>Representation of time-varying stimuli by a network exhibiting oscillations on a faster time scale</article-title><source>PLOS Computational Biology</source><volume>5</volume><fpage>e1000370</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000370</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silberberg</surname><given-names>G</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year>2007</year><article-title>Disynaptic inhibition between neocortical pyramidal cells mediated by Martinotti cells</article-title><source>Neuron</source><volume>53</volume><fpage>735</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.02.012</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>KN</given-names></name></person-group><year>2002</year><article-title>Toward a model for lexical access based on acoustic landmarks and distinctive features</article-title><source>The Journal of the Acoustical Society of America</source><volume>111</volume><fpage>1872</fpage><pub-id pub-id-type="doi">10.1121/1.1458026</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strong</surname><given-names>SP</given-names></name><name><surname>Koberle</surname><given-names>R</given-names></name><name><surname>van Steveninck</surname><given-names>RRR</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year>1998</year><article-title>Entropy and information in neural spike trains</article-title><source>Physical Review Letters</source><volume>80</volume><fpage>197</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.80.197</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tort</surname><given-names>AB</given-names></name><name><surname>Komorowski</surname><given-names>R</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name><name><surname>Kopell</surname><given-names>N</given-names></name></person-group><year>2010</year><article-title>Measuring phase-amplitude coupling between neuronal oscillations of different frequencies</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>1195</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1152/jn.00106.2010</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tort</surname><given-names>AB</given-names></name><name><surname>Rotstein</surname><given-names>HG</given-names></name><name><surname>Dugladze</surname><given-names>T</given-names></name><name><surname>Gloveli</surname><given-names>T</given-names></name><name><surname>Kopell</surname><given-names>NJ</given-names></name></person-group><year>2007</year><article-title>On the formation of gamma-coherent cell assemblies by oriens lacunosum-moleculare interneurons in the hippocampus</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>104</volume><fpage>13490</fpage><lpage>13495</lpage><pub-id pub-id-type="doi">10.1073/pnas.0705708104</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Victor</surname><given-names>J</given-names></name><name><surname>Purpura</surname><given-names>K</given-names></name></person-group><year>1997</year><article-title>Metric-space analysis of spike trains: theory, algorithms and application</article-title><source>Network</source><volume>8</volume><fpage>127</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1088/0954-898X/8/2/003</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vierling-Claassen</surname><given-names>D</given-names></name><name><surname>Cardin</surname><given-names>JA</given-names></name><name><surname>Moore</surname><given-names>CI</given-names></name><name><surname>Jones</surname><given-names>SR</given-names></name></person-group><year>2010</year><article-title>Computational modeling of distinct neocortical oscillations driven by cell-type selective optogenetic drive: separable resonant circuits controlled by low-threshold spiking and fast-spiking interneurons</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><fpage>198</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2010.00198</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villing</surname><given-names>R</given-names></name><name><surname>Timoney</surname><given-names>J</given-names></name><name><surname>Ward</surname><given-names>T</given-names></name><name><surname>Costello</surname><given-names>J</given-names></name></person-group><year>2004</year><article-title>Automatic blind syllable segmentation for continuous speech</article-title><source>Electronic Engineering</source></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year>2010</year><article-title>Neurophysiological and computational principles of cortical rhythms in cognition</article-title><source>Physiological Reviews</source><volume>90</volume><fpage>1195</fpage><lpage>1268</lpage><pub-id pub-id-type="doi">10.1152/physrev.00035.2008</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Lu</surname><given-names>T</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name><name><surname>Bartlett</surname><given-names>E</given-names></name></person-group><year>2008</year><article-title>Neural coding of temporal information in auditory thalamus and cortex</article-title><source>Neuroscience</source><volume>157</volume><fpage>484</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2008.07.050</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>SL</given-names></name><name><surname>Shire</surname><given-names>ML</given-names></name><name><surname>Greenberg</surname><given-names>S</given-names></name><name><surname>Morgan</surname><given-names>N</given-names></name></person-group><year>1997</year><article-title>Integrating syllable boundary information into speech recognition</article-title><source>Acoustics, Speech, and Signal Processing, 1997. ICASSP-97., 1997 IEEE International Conference on</source><volume>2</volume><publisher-name>IEEE</publisher-name><fpage>987</fpage><lpage>990</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=596105">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=596105</ext-link></comment></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yildiz</surname><given-names>IB</given-names></name><name><surname>von Kriegstein</surname><given-names>K</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name></person-group><year>2013</year><article-title>From birdsong to human speech recognition: bayesian inference on a hierarchy of nonlinear dynamical systems</article-title><source>PLOS Computational Biology</source><volume>9</volume><fpage>e1003219</fpage><comment>Edited by Viktor K Jirsa</comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003219</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><year>2010</year><article-title>Cortical processing of dynamic sound envelope transitions</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>16741</fpage><lpage>16754</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2016-10.2010</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname><given-names>EM</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year>2012</year><article-title>Temporal context in speech processing and attentional stream selection: a behavioral and neural perspective</article-title><source>Brain and Language</source><volume>122</volume><fpage>151</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2011.12.010</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.06213.015</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Brownell</surname><given-names>Hiram</given-names></name><role>Reviewing editor</role><aff><institution>Boston College</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for sending your work entitled &quot;Speech encoding by coupled cortical theta and gamma oscillations&quot; for consideration at <italic>eLife</italic>. Your article has been favorably evaluated by Eve Marder (Senior editor), three reviewers, and a member of our Board of Reviewing Editors (Hiram Brownell).</p><p>The Reviewing editor and the reviewers discussed their comments before we reached this decision, and the Reviewing editor has assembled the following comments to help you prepare a revised submission.</p><p>The study breaks new ground in that it shows that a simple, biologically plausible model of coupled theta and gamma oscillators performs better at speech decoding than other methods, including the authors' own circuit with uncoupled oscillators. While the reviewers value the potential contribution of this work, they were clear that they felt unable to fairly evaluate the suitability of the paper for publication without substantially more information. The major points to be addressed in a revision are presented below. Please note that presenting the additional information will not necessarily make the paper suitable for publication. With the requested additional information (e.g., results of statistical tests) presented as part of a revised submission, the reviewers will carry out a new review and arrive at a decision.</p><p>1) A general area of concern is the presentation of the model and the motivation for its specific architecture. Much of the <italic>eLife</italic> readership will not be familiar with the modeling literature.</p><p>1A) One specific issue is the motivation for the differential connectivity of theta- and gamma-associated neurons to the input signal).</p><p>1B) In the subsection headed “Model architecture and spontaneous behaviour” some of the aspects of the model appear somewhat ad-hoc: how realistic is it to assume that gamma and theta are generated by the same mechanism? How realistic is the connectivity structure between neurons? To preempt any concerns in this direction, one suggestion is to state explicitly and together the ways in which the model is based on neural data (connectivity structure, time constants, etc.) and the ways it is not. Some of this content is already stated in various places.</p><p>2) The connection of this work to prior literature could be improved.</p><p>2A) Given previous work on coupled oscillators and entrainment of oscillators, it should be made more clear what was known before the current work carried out and what is different and surprising about the results of the current work.</p><p>2B) The two main aims of the study could be better developed to set the stage for the rest of the manuscript. The first one (that speech constituents can be tracked with oscillations) has been shown with real data many times. A suggested rephrasing could include the modeling aspect. The second aim could be stated more clearly. (It is unclear what is meant by 'shape efficient neural code'.)</p><p>3) The description of the stimulus items used for training and testing the model is a major aspect of the paper needing additional detail. How do stimuli differ from each other? Since the test of decoding is a comparison of neural activity in response to test stimuli compared to that activity in response to trained stimuli, it is essential to know how different were the test and trained stimuli. In the subsection headed “Syllable decoding”, &quot;25 sentences… repeated 100 times each&quot; suggest identical stimuli were used. Was each sentence spoken by a different speaker? If not, especially if some test stimuli were identical to training stimuli, then trial-to-trial differences could be due to only internal noise in the circuit, which could always be lowered to improve performance in any circuit. Thus, relevant questions include: were different exemplars of syllables spoken by different speakers? Or did they appear in different phrases or words but spoken by the same speaker?</p><p>4) More information is needed regarding support for the model.</p><p>4A) It was not clear what data supported the network architecture, specifically, that the <italic>Ge</italic> neurons receive input from only one specific auditory channel, whereas the <italic>Te</italic> neurons receive input from all channels. Are the theta- and gamma-associated neurons anatomically dissociable?</p><p>4B) A major issue raised consistently in the reviews is that throughout the paper, the specific statistical tests applied were opaque or missing. Examples include p values (or Bayes factors, etc.) to back up many of the comparisons across conditions, syllable boundary detection (is the presented model's performance significantly better than the others?), and decoding of simple stimuli (again, is performance significantly better for the intact model?). What is the chance level of decoding for the conditions shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref> and <xref ref-type="fig" rid="fig4">Figure 4B</xref>? These are not necessarily what might be expected, depending on the number of samples available. Although the approaches seem generally reasonable, statistical support must be reported (incorporating, if applicable, correction for multiple comparisons).</p><p>5) Some additional comment on phase-reset is warranted. For example, <xref ref-type="fig" rid="fig2">Figure 2</xref> nicely shows PINTH activity (LFP and theta-associated activity) occurring in a (quasi-)regular fashion before the presentation of the speech stimulus. The degree to which the incoming speech signal systematically alters the phase of this activity is unclear: from <xref ref-type="fig" rid="fig2">Figure 2</xref>, it appears to happen instantaneously, with no missed/extra spikes. Such performance seems intriguing but potentially unrealistic. How does the phase following speech presentation compare with that prior to speech presentation?</p><p>6) Please check figures and figure captions. For example, there are captions for <xref ref-type="fig" rid="fig2">Figure 2</xref> C, D and E, but these panels are missing in <xref ref-type="fig" rid="fig2">Figure 2</xref>. (Note also a similar mismatch for <xref ref-type="fig" rid="fig2s1">Figure 2–figure supplement 1</xref>.)</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for submitting your work entitled &quot;Speech encoding by coupled cortical theta and gamma oscillations&quot; for peer review at <italic>eLife</italic>. Your submission has been favorably evaluated by Eve Marder (Senior editor), a Reviewing editor, and three reviewers.</p><p>The reviewers have discussed the reviews with one another and the Reviewing editor has drafted this decision to help you prepare a revised submission.</p><p>Summary: The paper is greatly improved and needs only relatively small revisions prior to publication.</p><p>Essential revisions:</p><p>1) Please address the following in the paper itself. The comment is in response to a point made in the authors' cover letter.</p><p>A system without noise would always perform at 100% on any classification test if given repeats of certain items in training, and then tested on those same items. Your point that noise is needed to optimize detection of stimulus onset time is important and should be included in the paper (that is, for tests in which stimulus onset time is not provided to the model).</p><p>2) Given the authors' intent to take into account the constraint of neural noise, additional information as to how well this is done should be provided. At a minimum, in the spiking models, please provide the CV of the spike trains and the Fano factors for responses to identical stimuli. For the LFP signals, also provide an indicator of the trial-to-trial variability in response to identical stimuli: e.g., when stimuli are identical, CV of heights of particular LFP peaks, or of time intervals between specific peaks and troughs.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.06213.016</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>1) A general area of concern is the presentation of the model and the motivation for its specific architecture. Much of the eLife readership will not be familiar with the modeling literature</italic>.</p><p><italic>1A) One specific issue is the motivation for the differential connectivity of theta- and gamma-associated neurons to the input signal)</italic>.</p><p>We thank the reviewers for bringing up this important point. Our rationale was as follows. Human intracortical data obtained in primary auditory cortex (<xref ref-type="bibr" rid="bib53">Morillon et al., 2012</xref>) indicate that the cerebro/acoustic coherence in the theta range does not depend on the input frequency. In the gamma range, however, coherence is stronger at specific stimulus frequencies (see <xref ref-type="fig" rid="fig6">Author response figure 1</xref>). This suggests that theta oscillations inside auditory cortex are broadly responsive to the whole hearing spectrum, while local gamma generators are finely tuned to specific frequencies. This claim also makes sense from the point of view of multiplexing to track a broadband and finely tuned signals simultaneously at two distinct frequencies (Panzeri et al. 2010; <xref ref-type="bibr" rid="bib26">Gross et al. 2013</xref>). This was our main motivation for feeding a broadband signal to individual theta neurons and a more fine-grained, spectrally complex input to individual gamma neurons. This is now specified in the Results section of the manuscript together with the information requested in Point 3:</p><p>&quot;Such a differential selectivity was motivated experimental observations from intracranial recordings (<xref ref-type="bibr" rid="bib21">Fontolan et al. 2014</xref>; <xref ref-type="bibr" rid="bib53">Morillon et al. 2012</xref>) suggesting that unlike the gamma one, the theta response does not depend on the input spectrum&quot;.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.06213.017</object-id><label>Author response image 1.</label><caption><title>Human stereotactic EEG recording in auditory cortex.</title><p>A: the location of the electrode shaft, B: acoustic stimulus (one sentence); C: time-frequency representation of the cortical response in primary auditory cortex; D: spectrum of the stimulus in relation with E: the cross-correlation between stimulus (Hilbert transform of B) and cortical response for 33 different frequency bands (corresponding to 33 cochlear filters) spanning the speech audio spectrum up to 8 kHz. Note that the cross-correlation is broad-band in the theta range, but frequency specific in the gamma range, with peaks of correlation corresponding roughly to the energy peaks (speech formants). The data partly published in <xref ref-type="bibr" rid="bib53">Morillon et al., 2012</xref> and <xref ref-type="bibr" rid="bib25">Giraud and Poeppel, 2012</xref>, and partly original.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06213.017">http://dx.doi.org/10.7554/eLife.06213.017</ext-link></p></caption><graphic xlink:href="elife-06213-resp-fig1-v3.tif"/></fig></p><p><italic>1B) In the subsection headed “Model architecture and spontaneous behaviour” some of the aspects of the model appear somewhat ad-hoc: how realistic is it to assume that gamma and theta are generated by the same mechanism? How realistic is the connectivity structure between neurons? To preempt any concerns in this direction, one suggestion is to state explicitly and together the ways in which the model is based on neural data (connectivity structure, time constants, etc.) and the ways it is not. Some of this content is already stated in various places</italic>.</p><p>Thanks you for this suggestion. We have now updated the manuscript and put all the information regarding experimental support for the model architecture in the &quot;Model architecture and spontaneous behaviour&quot; subsection of the Results section. We believe that this change, motivated by reviewers comments, permit to better disentangle those assumptions that were directly constrained by experimental evidence, and those that were put forward for future experimental verification.</p><p>Regarding the generation of theta oscillations, we state (now in the section “Model architecture and spontaneous behaviour”) that in the absence of conclusive evidence about the underlying mechanisms, we chose the most parsimonious option: a mechanism similar to gamma generation (PING) based on the attested presence of inhibitory neurons with slower time constants (our <italic>Ti</italic> neurons), which were involved in theta generation in previous models of neocortical oscillations (<xref ref-type="bibr" rid="bib75">Vierling-Claassen et al., 2010</xref>; Compte et al., J Neuroscience 2008).</p><p>When developing our model, we did consider alternative architectures but concluded that they were not as compelling. Most obvious example is a three-population model with <italic>Gi</italic> units<italic>, Ti</italic> units and a single population of pyramidal neurons, (as in Tort et al. PNAS 2005 model of hippocampus) was not appropriate as it did not permit a differential frequency channel tuning for the inputs to the gamma excitatory and the theta excitatory neurons (since they represent the same population). We decided to avoid a lengthy discussion about alternative models in the manuscript, as it would probably sound unappealing to most readers. Overall we believe these changes should add to the clarity of the manuscript.</p><p><italic>2) The connection of this work to prior literature could be improved</italic>.</p><p><italic>2A) Given previous work on coupled oscillators and entrainment of oscillators, it should be made more clear what was known before the current work carried out and what is different and surprising about the results of the current work</italic>.</p><p>We agree with this remark and have now thoroughly rewritten the Introduction to make a more explicit connection between the previous literature and the present study. To the best of our knowledge, until now coupled cross-frequency oscillations were only used to model the functions of the hippocampus and prefrontal cortex (<xref ref-type="bibr" rid="bib33">Jensen and Lisman 1996</xref>; Tort et al. 2005; etc.). These models were not envisaged to account for the sampling function of sensory areas. In particular, the functional interaction of an intrinsic oscillation with a sensory signal bearing pseudo-rhythmic modulations such as speech has, to the best of our knowledge, never been explored before. While supported by some experimental evidence, the role of coupled oscillations in speech processing has only been formulated from a neurophysiological perspective (<xref ref-type="bibr" rid="bib25">Giraud and Poeppel, 2012</xref>) or in phenomenological terms (Ghitza, Front Psychology 2010).</p><p><italic>2B) The two main aims of the study could be better developed to set the stage for the rest of the manuscript. The first one (that speech constituents can be tracked with oscillations) has been shown with real data many times. A suggested rephrasing could include the modeling aspect. The second aim could be stated more clearly. (It is unclear what is meant by 'shape efficient neural code'</italic>.<italic>)</italic></p><p>We respectfully disagree with the statement about our first aim. A number of experiments shows that neural oscillations in auditory cortex track the slow fluctuations of speech, but none of them has explicitly tied oscillations to specific linguistic constituents, such as we do here with respect to theta oscillations and syllables. Thus, the efficiency of biologically plausible coupled theta and gamma oscillators in speech encoding has never been evaluated before. Also, given the large variability of the syllabic rhythm in natural speech, it was not clear that neural oscillations could accurately track the syllabic rate. Finally, no experimental data has ever proven causality with respect to the hypothesized functions. It is precisely because causality cannot easily be established in human recordings (it would require a specific interference with the theta rhythm with e.g. optogenetics), that we initiated this modelling work.</p><p>The new Introduction paragraph now clarifies the current state of knowledge, and specifies the three specific aims of the study. Note that for the readability of the manuscript, the changes are not tracked in the Introduction.</p><p><italic>3) The description of the stimulus items used for training and testing the model is a major aspect of the paper needing additional detail. How do stimuli differ from each other? Since the test of decoding is a comparison of neural activity in response to test stimuli compared to that activity in response to trained stimuli, it is essential to know how different were the test and trained stimuli. In the subsection headed “Syllable decoding”, &quot;25 sentences…repeated 100 times each&quot; suggest identical stimuli were used. Was each sentence spoken by a different speaker? If not, especially if some test stimuli were identical to training stimuli, then trial-to-trial differences could be due to only internal noise in the circuit, which could always be lowered to improve performance in any circuit. Thus, relevant questions include: were different exemplars of syllables spoken by different speakers? Or did they appear in different phrases or words but spoken by the same speaker</italic>?</p><p>As correctly pointed out by the reviewers, we used identical stimuli for syllable classification in the analysis where we aimed at comparing both different model versions and different neural codes. To thoroughly assess the different versions of the model and explore the added value of reading out gamma spiking as a function of theta phase, some consistency in syllable length was required. We believe that the differential results are both valid and important. To address the reviewers’ concern, we now additionally present decoding results when the classifier was trained on 2 sentences uttered by 462 different speakers, where the training material was hence different than the tested material. Despite the fact that the new dataset involved a wide range of phonemic realizations (as labelled by phoneticians) due to accents and pronunciation variants, classification reached 20-24% (depending on model version), which remained well above chance level (10%). Decoding accuracy using the intact model was significantly better than either control model, and importantly the full model performed optimally when the syllable duration was within the 100-300 ms range, i.e. roughly one theta cycle.</p><p>Details about this control analysis have been included in the Results section and we added a corresponding figure (<xref ref-type="fig" rid="fig4">Figure 4C</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4–figure supplement 1</xref>).</p><p>The point about reducing noise in the system makes sense from an algorithmic point of view but less so from a neuroscience perspective. Neuronal noise is a neural constraint requiring robust computations. Moreover, if reducing noise in the gamma network may arguably enhance chance performance, it should be noted that the level of noise in the theta module was optimized for syllabic boundary detection, thus noisy theta is expected to allow better classification performance than noiseless theta.</p><p>We thank the reviewers for this remark and we believe the across-speaker decoding results contribute to strengthen the main claims of the manuscript.</p><p><italic>4) More information is needed regarding support for the model</italic>.</p><p><italic>4A) It was not clear what data supported the network architecture – specifically, that the</italic> Ge <italic>neurons receive input from only one specific auditory channel, whereas the</italic> Te <italic>neurons receive input from all channels. Are the theta- and gamma-associated neurons anatomically dissociable</italic>?</p><p>Regarding the different patterns of input to the <italic>Te</italic> and <italic>Ge</italic> populations, please refer to our reply to point 1A. Regarding the issue of anatomic dissociability, there is unfortunately very little experimental evidence to build from. We draw a putative link between our two subpopulations of theta and gamma neurons and the two subclasses of stereotyped and modulated neurons found in primate auditory cortex by Brasselet and colleagues (<xref ref-type="bibr" rid="bib8">Brasselet et al., 2012</xref>). The authors report the existence of two subclasses of neurons in primate auditory cortex corresponding to our two populations of theta and gamma neurons (see Discussion):</p><p>A subclass of 'stereotyped' neurons responding very rapidly and non-selectively to any acoustic stimulus, presumably receiving input from the whole acoustic spectrum, which we model as theta neurons, signalling boundaries in the speech signal.</p><p>A subclass of 'modulated' neurons responding more slowly and selective to some specific spectro-temporal features, indicating a narrower receptive field, which we model as gamma excitatory neurons.</p><p>The authors did not report a spatial dissociation between the two subclasses, leaving the possibility open that theta and gamma circuits do indeed coexist in one place. This point has been added to the Result section of the manuscript:</p><p>&quot;It also mirrored the dissociation in primate auditory cortex between a population of 'stereotyped' neurons responding very rapidly and non-selectively to any acoustic stimulus (putatively <italic>Te</italic> neurons) and a population of 'modulated' neurons responding selectively to specific spectro-temporal features (putatively <italic>Ge</italic> neurons) (<xref ref-type="bibr" rid="bib8">Brasselet et al. 2012</xref>).&quot;</p><p><italic>4B) A major issue raised consistently in the reviews is that throughout the paper, the specific statistical tests applied were opaque or missing. Examples include p values (or Bayes factors, etc.) to back up many of the comparisons across conditions, syllable boundary detection (is the presented model's performance significantly better than the others?), and decoding of simple stimuli (again, is performance significantly better for the intact model?). What is the chance level of decoding for the conditions shown in</italic> <xref ref-type="fig" rid="fig3"><italic>Figure 3C</italic></xref> <italic>and</italic> <xref ref-type="fig" rid="fig4"><italic>Figure 4B</italic></xref><italic>? These are not necessarily what might be expected, depending on the number of samples available. Although the approaches seem generally reasonable, statistical support must be reported (incorporating, if applicable, correction for multiple comparisons)</italic>.</p><p>We thank the reviewers for this comment, which urged us to provide more statistical details. We have now included results from statistical testing at all steps of the analyses. Statistical effects were very strong in all cases and reported both below and in the main manuscript. For the case of chance levels, given that we use 10 classes for all classification analysis, the average expected chance level would be 10%. Based on Combrisson and Jerbi, the threshold for 5% testing would be resp. 12.2% and 11.6% for each sawtooth (500 samples) and syllable (1000 samples) classification analysis. Importantly here, the classification procedure was repeated many times (10 times for sawtooth, 200 times for syllables) with a different set of samples. Hence the distribution of classification score could be compared through a simple t-test against its expected mean value for the chance level (10%).</p><p>Details of the statistical results are as follows:</p><p>Spike phase-amplitude coupling: coupling was significant for the full model both for rest and speech (p&lt; 10<sup>-9</sup> for both);</p><p>Syllable boundary detection performance (in the subsection headed “Syllable boundary detection by theta oscillations”): the theta network performed significantly better than the Mermelstein and LN algorithms and than chance level for uncompressed and compressed speech (all p-values &lt; 10<sup>-12</sup>, tested over 3620 sentences);</p><p>Sawtooth decoding (in the subsection headed “Decoding of simple temporal stimuli from output spike patterns”): decoding using the full model was significantly larger than using any of the two control networks either using stimulus timing or theta timing (all p-values &lt;10<sup>-9</sup>, testing over 10 repetitions of the classification procedure);</p><p>Syllable decoding (in the subsection headed “Continuous speech encoding by model output spike patterns”): in the full model, decoding was lower using spike count than using spike patterns (p&lt;10<sup>-12</sup> over 200 repetitions); decoding using any of the control models and any of the neural code (spike count/spike patterns) was significantly lower than using spike patterns for the full model (all p-values &lt;10<sup>-12</sup>), and none was significantly better than using spike counts for the full model (all p-values&gt;.08 uncorrected);</p><p>Classifier analysis (in the subsection headed “Classifier analysis”): decoding using spike counts was significantly smaller than using spike patterns (p&lt;10<sup>-12</sup> over 200 repetitions for all 6 window sizes); the increase in decoding performance using phase-partitioned code rather than spike count was significantly reduced in both control models and all 6 window sizes compared to full model (all p-values&lt;10<sup>-12</sup>);</p><p>Mutual Information (in the subsection headed “Mutual information”): in the full model, MI was significantly larger when adding phase-partitioned code on top of both spike count and time-partitioned code (p&lt;10<sup>-12</sup> with vs without phase information (p-values over 32 <italic>Ge</italic> neurons: p&lt;10<sup>-12</sup> for both).</p><p><italic>5) Some additional comment on phase-reset is warranted. For example,</italic> <xref ref-type="fig" rid="fig2"><italic>Figure 2</italic></xref> <italic>nicely shows PINTH activity (LFP and theta-associated activity) occurring in a (quasi-)regular fashion before the presentation of the speech stimulus. The degree to which the incoming speech signal systematically alters the phase of this activity is unclear: from</italic> <xref ref-type="fig" rid="fig2"><italic>Figure 2</italic></xref><italic>, it appears to happen instantaneously, with no missed/extra spikes. Such performance seems intriguing but potentially unrealistic. How does the phase following speech presentation compare with that prior to speech presentation</italic>?</p><p>The strong speech input to <italic>Te</italic> neurons provides a somewhat hard reset of theta phase in correspondence of a sentence onset. Unlike the weak coupling situation, the theta phase after the onset is virtually independent of the phase prior to speech onset. This mechanism enables very rapid and strong phaselocking of the theta to speech throughout the sentence. Gross et al. (Plos Biology, 2013 showed that phaselocking to speech edges occurs in auditory cortex within a few hundred milliseconds. To better illustrate this important property, we added a figure (<xref ref-type="fig" rid="fig1s1">Figure 1–figure supplement 1C</xref>) that shows theta phase concentration for multiple presentation of the same sentence: there is a rapid transition within a few hundred ms (e.g. a theta cycle or less) from uniform phase distribution before sentence onset to very strong phase-locking. The figure is referenced in the Results section “Model dynamics in response to natural sentences”.</p><p><italic>6) Please check figures and figure captions. For example, there are captions for</italic> <xref ref-type="fig" rid="fig2"><italic>Figure 2</italic></xref> <italic>C, D and E, but these panels are missing in</italic> <xref ref-type="fig" rid="fig2"><italic>Figure 2</italic></xref><italic>. (Note also a similar mismatch for</italic> <xref ref-type="fig" rid="fig2s1"><italic>Figure 2–figure supplement 1</italic></xref>.<italic>)</italic></p><p>This was an error. This has been corrected for <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2–figure supplement 1</xref>. The legends for two panels of <xref ref-type="fig" rid="fig5s1">Figure 5–figure supplement 1</xref> have also been correctly reordered.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p><italic>1) Please address the following in the paper itself. The comment is in response to a point made in the authors' cover letter</italic>.</p><p><italic>A system without noise would always perform at 100% on any classification test if given repeats of certain items in training, and then tested on those same items. Your point that noise is needed to optimize detection of stimulus onset time is important and should be included in the paper (that is, for tests in which stimulus onset time is not provided to the model)</italic>.</p><p>We agree with reviewers that this point should be made explicit in the manuscript. We have now added in the Results section:</p><p>&quot;Noise in the theta module allows the alignment of theta bursts to stimulus onset and thus improves detection performance by enabling consistent theta chunking of spike patterns.&quot;</p><p><italic>2) Given the authors' intent to take into account the constraint of neural noise, additional information as to how well this is done should be provided. At a minimum, in the spiking models, please provide the CV of the spike trains and the Fano factors for responses to identical stimuli. For the LFP signals, also provide an indicator of the trial-to-trial variability in response to identical stimuli: e.g., when stimuli are identical, CV of heights of particular LFP peaks, or of time intervals between specific peaks and troughs</italic>.</p><p>We have added (in <xref ref-type="fig" rid="fig1s1">Figure 1–figure supplement 1</xref>) spike train CV and spike count Fano factors for the different types of neurons in response to speech as well as the standard deviation of LFP in response to speech. The latter shows a great decrease in LFP variability at sentence onset that is mostly due to the phase-locking of theta and gamma oscillations. We added reference to both new panels in the Results section.</p></body></sub-article></article>