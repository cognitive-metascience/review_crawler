<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">37841</article-id><article-id pub-id-type="doi">10.7554/eLife.37841</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Augmented reality powers a cognitive assistant for the blind</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-112195"><name><surname>Liu</surname><given-names>Yang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8155-9134</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-112194"><name><surname>Stiles</surname><given-names>Noelle RB</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-3549"><name><surname>Meister</surname><given-names>Markus</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2136-6506</contrib-id><email>meister@caltech.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf3"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Division of Biology and Biological Engineering</institution><institution>California Institute of Technology</institution><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Computation and Neural Systems Program</institution><institution>California Institute of Technology</institution><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Institute for Biomedical Therapeutics, Keck School of Medicine</institution><institution>University of Southern California</institution><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution>University of Washington</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Marder</surname><given-names>Eve</given-names></name><role>Senior Editor</role><aff><institution>Brandeis University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>27</day><month>11</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e37841</elocation-id><history><date date-type="received" iso-8601-date="2018-04-24"><day>24</day><month>04</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2018-10-27"><day>27</day><month>10</month><year>2018</year></date></history><permissions><copyright-statement>Â© 2018, Liu et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Liu et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-37841-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.37841.001</object-id><p>To restore vision for the blind, several prosthetic approaches have been explored that convey raw images to the brain. So far, these schemes all suffer from a lack of bandwidth. An alternate approach would restore vision at the cognitive level, bypassing the need to convey sensory data. A wearable computer captures video and other data, extracts important scene knowledge, and conveys that to the user in compact form. Here, we implement an intuitive user interface for such a device using augmented reality: each object in the environment has a voice and communicates with the user on command. With minimal training, this system supports many aspects of visual cognition: obstacle avoidance, scene understanding, formation and recall of spatial memories, navigation. Blind subjects can traverse an unfamiliar multi-story building on their first attempt. To spur further development in this domain, we developed an open-source environment for standardized benchmarking of visual assistive devices.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>hearing</kwd><kwd>vision</kwd><kwd>assistive technology</kwd><kwd>sensory substitution</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010319</institution-id><institution>Shurl and Kay Curci Foundation</institution></institution-wrap></funding-source><award-id>103212</award-id><principal-award-recipient><name><surname>Meister</surname><given-names>Markus</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A non-invasive cognitive assistant for blind people endows objects in the environment with voices, allowing users to explore the scene, localize objects, and navigate through a building with minimal training.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>About 36 million people are blind worldwide (<xref ref-type="bibr" rid="bib4">Bourne et al., 2017</xref>). In industrialized nations, the dominant causes of blindness are age-related diseases of the eye, all of which disrupt the normal flow of visual data from the eye to the brain. In some of these cases, biological repair is a potential option, and various treatments are being explored involving gene therapy, stem cells, or transplantation (<xref ref-type="bibr" rid="bib30">Scholl et al., 2016</xref>). However, the dominant strategy for restoring vision has been to bring the image into the brain through alternate means. The most direct route is electrical stimulation of surviving cells in the retina (<xref ref-type="bibr" rid="bib34">Stingl and Zrenner, 2013</xref>; <xref ref-type="bibr" rid="bib39">Weiland and Humayun, 2014</xref>) or of neurons in the visual cortex (<xref ref-type="bibr" rid="bib9">Dobelle et al., 1974</xref>). Another option involves translating the raw visual image into a different sensory modality (<xref ref-type="bibr" rid="bib18">Loomis et al., 2012</xref>; <xref ref-type="bibr" rid="bib20">Maidenbaum et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">Proulx et al., 2016</xref>), such as touch (<xref ref-type="bibr" rid="bib36">Stronks et al., 2016</xref>) or hearing (<xref ref-type="bibr" rid="bib2">Auvray et al., 2007</xref>; <xref ref-type="bibr" rid="bib6">Capelle et al., 1998</xref>; <xref ref-type="bibr" rid="bib23">Meijer, 1992</xref>). So far, none of these approaches has enabled any practical recovery of the functions formerly supported by vision. Despite decades of efforts all users of such devices remain legally blind (<xref ref-type="bibr" rid="bib19">Luo and da Cruz, 2016</xref>; <xref ref-type="bibr" rid="bib33">Stingl et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Striem-Amit et al., 2012</xref>; <xref ref-type="bibr" rid="bib36">Stronks et al., 2016</xref>). While one can certainly hope for progress in these domains, it is worth asking what are the fundamental obstacles to restoration of visual function.</p><p>The human eye takes in about 1 gigabit of raw image information every second, whereas our visual system extracts from this just tens of bits to guide our thoughts and actions (<xref ref-type="bibr" rid="bib24">Pitkow and Meister, 2014</xref>). All the above approaches seek to transmit the raw image into the brain. This requires inordinately high data rates. Further, the signal must arrive in the brain in a format that can be interpreted usefully by the visual system or some substitute brain area to perform the key steps of knowledge acquisition, like scene recognition and object identification. None of the technologies available today deliver the high data rate required to retain the relevant details of a scene, nor do they produce a neural code for the image that matches the expectations of the human brain, even given the prodigious degree of adaptive plasticity in the nervous system.</p><p>Three decades ago, one of the pioneers of sensory substitution articulated his vision of a future visual prosthesis (<xref ref-type="bibr" rid="bib7">Collins, 1985</xref>): &quot;I strongly believe that we should take a more sophisticated approach, utilizing the power of artificial intelligence for processing large amounts of detailed visual information in order to substitute for the missing functions of the eye and much of the visual pre-processing performed by the brain. We should off-load the blind travelers' brain of these otherwise slow and arduous tasks which are normally performed effortlessly by the sighted visual system&quot;. Whereas at that time the goal was hopelessly out of reach, todayâs capabilities in computer vision, artificial intelligence, and miniaturized computing power are converging to make it realistic.</p><p>Here, we present such an approach that bypasses the need to convey the sensory data entirely, and focuses instead on the important high-level knowledge, presented at a comfortable data rate and in an intuitive format. We call the system CARA: a cognitive <italic>a</italic>ugmented <italic>r</italic>eality <italic>a</italic>ssistant for the blind.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Design principles</title><p>CARA uses a wearable augmented reality device to give voices to all the relevant objects in the environment (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Unlike most efforts at scene sonification (<xref ref-type="bibr" rid="bib5">Bujacz and StrumiÅÅo, 2016</xref>; <xref ref-type="bibr" rid="bib8">CsapÃ³ and WersÃ©nyi, 2013</xref>), our system communicates through natural language. Each object in the scene can talk to the user with a voice that comes from the objectâs location. The voiceâs pitch increases as the object gets closer. The user actively selects which objects speak through several modes of control (<xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref>): In Scan mode, the objects call out their names in sequence from left to right, offering a quick overview of the scene. In Spotlight mode, the object directly in front speaks, and the user can explore the scene by moving the head. In Target mode, the user selects one object that calls repeatedly at the press of a clicker. In addition, any surface in the space emits a hissing sound as a collision warning when the user gets too close (<xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref>).</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.37841.002</object-id><label>Figure 1.</label><caption><title>Hardware platform and object localization task.</title><p>(<bold>A</bold>) The Microsoft HoloLens wearable augmented reality device. Arrow points to one of its stereo speakers. (<bold>B</bold>) In each trial of the object localization task, the target (green box) is randomly placed on a circle (red). The subject localizes and turns to aim at the target. (<bold>C</bold>) Object localization relative to the true azimuth angle (dashed line). Box denotes s.e.m., whiskers s.d. (<bold>D</bold>) Characteristics of the seven blind subjects.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig1-v1"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37841.003</object-id><label>Figure 1âfigure supplement 1.</label><caption><title>Obstacle avoidance utility and active scene exploration modes.</title><p>(<bold>A</bold>) to (<bold>C</bold>) An object avoidance system is active in the background at all times. Whenever a real scanned surface or a virtual object enters a danger volume around the user (red in <bold>A</bold>), a spatialized warning sound is emitted from the point of contact (<bold>B</bold>). The danger volume expands automatically as the user moves (<bold>C</bold>), so as to deliver warnings in time. (<bold>D</bold>) to (<bold>E</bold>) Active exploration modes. In Scan mode (<bold>D</bold>) objects whose azimuthal angles fall in a certain range (e.g. between â60 andÂ +60 deg) call themselves out from left to right. In Spotlight mode (<bold>E</bold>) only objects within a narrow cone are activated, and the object closest to the forward-facing vector calls out.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig1-figsupp1-v1"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37841.004</object-id><label>Figure 1âfigure supplement 2.</label><caption><title>Process of scene sonification.</title><p>The acquisition system should parse the sceneÂ (<bold>A</bold>) into objects and assign each object a name and a voice (<bold>B</bold>). In our study, this was accomplished by a combination of the HoloLens and the experimenter. The HoloLens scans the physical space (<bold>C</bold>) and generates a 3D mesh of all surfaces (<bold>D</bold>). In this digitized space (<bold>E</bold>) the experimenter can perform manipulations such as placing and labeling virtual objects, computing paths for navigation, and animating virtual guides (<bold>F</bold>). Because of the correspondence established in D, these virtual labels are tied to the physical objects in real space.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig1-figsupp2-v1"/></fig></fig-group><p>The system is implemented on the Microsoft HoloLens (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), a powerful head-mounted computer designed for augmented reality (<xref ref-type="bibr" rid="bib11">Hoffman, 2016</xref>). The HoloLens scans all surfaces in the environment using video and infrared sensors, creates a 3D map of the surrounding space, and localizes itself within that volume to a precision of a few centimeters (<xref ref-type="fig" rid="fig1s2">Figure 1âfigure supplement 2</xref>). It includes a see-through display for digital imagery superposed on the real visual scene; open ear speakers that augment auditory reality while maintaining regular hearing; and an operating system that implements all the localization functions and provides access to the various sensor streams.</p><p>Any cognitive assistant must both acquire knowledge about the environment and then communicate that knowledge to the user. Tracking and identifying objects and people in a dynamic scene still presents a challenge, but those capabilities are improving at a remarkable rate (<xref ref-type="bibr" rid="bib12">Jafri et al., 2014</xref>; <xref ref-type="bibr" rid="bib38">Verschae and Ruiz-del-Solar, 2015</xref>), propelled primarily by interests in autonomous vehicles (see also <italic>Technical extensions</italic> below). Anticipating that the acquisition problems will be solved shortly, we focus here on the second task, the interface to the user. Thus, we populated the real-space volume scanned by the HoloLens with virtual objects that interact with the user. The applications were designed using the Unity game development platform which allows tracking of the userâs head in the experimental space; the simulation of virtual objects; the generation of speech and sounds that appear to emanate from specific locations; and interaction with the user via voice commands and a clicker.</p></sec><sec id="s2-2"><title>Human subject tests</title><p>After a preliminary exploration of these methods, we settled on a fixed experimental protocol and recruited seven blind subjects (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Subjects heard a short explanation of what to expect, then donned the HoloLens and launched into a series of four fully automated tasks without experimenter involvement. No training sessions were provided, and all the data were gathered within a 2 hr visit.</p></sec><sec id="s2-3"><title>Object localization</title><p>Here, we tested the userâs ability to localize an augmented reality sound source (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). A virtual object placed randomly at a 2 m distance from the subject called out âboxâ whenever the subject pressed a clicker. The subject was asked to orient the head towards the object and then confirm the final choice of direction with a voice command. All subjects found this a reasonable request and oriented surprisingly well, with an accuracy of 3â12 degrees (standard deviation across trials, <xref ref-type="fig" rid="fig1">Figure 1C</xref>). Several subjects had a systematic pointing bias to one or the other side of the target (â9 toÂ +13 deg, <xref ref-type="fig" rid="fig1">Figure 1C</xref>), but no attempt was made to correct for this bias. These results show that users can accurately localize the virtual voices generated by HoloLens, even though the software used a generic head-related transfer function without customization.</p></sec><sec id="s2-4"><title>Spatial memory</title><p>Do object voices help in forming a mental image of the scene (<xref ref-type="bibr" rid="bib14">Lacey, 2013</xref>) that can be recalled for subsequent decisions? A panel of five virtual objects was placed in the horizontal plane 2 m from the subject, spaced 30 degrees apart in azimuth (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The subject scanned this scene actively using the Spotlight mode for 60 s. Then the object voices were turned off and we asked the subject to orient towards the remembered location of each object, queried in random order. All subjects performed remarkably well, correctly recalling the arrangement of all objects (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>) with just one error (1/28 trials). Even the overall scale of the scene and the absolute positions of the objects were reproduced well from memory, to an average accuracy ofÂ ~15 deg (rms deviation from true position, <xref ref-type="fig" rid="fig2">Figure 2CâD</xref>). In a second round, we shuffled the object positions and repeated the task. Here three of the subjects made a mistake, presumably owing to interference with the memory formed on the previous round. Sighted subjects who inspected the scene visually performed similarly on the recall task (<xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). These experiments suggest that active exploration of object voices builds an effective mental representation of the scene that supports subsequent recall and orientation in the environment.</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.37841.005</object-id><label>Figure 2.</label><caption><title>Spatial memory task.</title><p>(<bold>A</bold>) Five objects are arranged on a half-circle; the subject explores the scene, then reports the recalled object identities and locations. (<bold>B</bold>) Recall performance during blocks 1 (left) and 2 (right). Recalled target angle potted against true angle. Shaded bar along the diagonal shows the 30 deg width of each object; data points within the bar indicate perfect recall. Dotted lines are linear regressions. (<bold>C</bold>) Slope and (<bold>D</bold>) correlation coefficient for the regressions in panel (B).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig2-v1"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37841.006</object-id><label>Figure 2âfigure supplement 1.</label><caption><title>Mental imagery task supplementary data.</title><p>Spatial memory data (<xref ref-type="fig" rid="fig2">Figure 2</xref>) from blocks 1 (left) and 2 (right) by subject. Shaded areas indicate the true azimuthal extent of each object. Markers indicate recalled location. Most recalled locations overlap with the true extent of the object. Subjects 8â10 were normally sighted and performed the exploration phase using vision.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig2-figsupp1-v1"/></fig></fig-group></sec><sec id="s2-5"><title>Direct navigation</title><p>Here, the subject was instructed to walk to a virtual chair, located 2 m away at a random location (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). In Target mode, the chair called out its name on every clicker press. All subjects found the chair after walking essentially straight-line trajectories (<xref ref-type="fig" rid="fig3">Figure 3BâC</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>). Most users followed a two-phase strategy: first localize the voice by turning in place, then walk swiftly toward it (<xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1DâE</xref>). On rare occasions (~5 of 139 trials), a subject started walking in the opposite direction, then reversed course (<xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1C</xref>), presumably owing to ambiguities in azimuthal sound cues (<xref ref-type="bibr" rid="bib22">McAnally and Martin, 2014</xref>). Subject seven aimed consistently to the left of the target (just as in the task of <xref ref-type="fig" rid="fig1">Figure 1</xref>) and thus approached the chair in a spiral trajectory (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Regardless, for all subjects the average trajectory was only 11â25% longer than the straight-line distance (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1A</xref>).</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.37841.007</object-id><label>Figure 3.</label><caption><title>Direct navigation task.</title><p>(<bold>A</bold>) For each trial, a target chair is randomly placed at one of four locations. The subject begins in the starting zone (red shaded circle), follows the voice of the chair, and navigates to the target zone (green shaded circle). (<bold>B</bold>) All raw trajectories from one subject (#6) including 1 s time markers. Oscillations from head movement are filtered out in subsequent analysis. (<bold>C</bold>) Filtered and aligned trajectories from all trials of 3 subjects (#3, 4, 7). Arrow highlights a trial where the subject started in the wrong direction. (<bold>D</bold>) Trajectories of subjects performing the task with only a cane and no HoloLens. (<bold>E</bold>) Deviation index, namely the excess length of the walking trajectory relative to the shortest distance between start and target. Note logarithmic axis and dramatic difference between HoloLens and Cane conditions. (<bold>F</bold>) Speed of each subject normalized to the free-walking speed.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig3-v1"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37841.008</object-id><label>Figure 3âfigure supplement 1.</label><caption><title>Direct navigation task extended data.</title><p>Trial distance (<bold>A</bold>) and trial duration (<bold>B</bold>) for the first 20 trials of all subjects. A modest effect of practice on task duration can be observed across all subjects (<bold>B</bold>). (<bold>C</bold>) Low-pass filtered, aligned trajectories of all subjects. In most trials, subjects reach the target with little deviation. (<bold>D</bold>) Dynamics of navigation, showing the distance to target as a function of trial time for one subject. (<bold>E</bold>) Head orientation vs distance to target for two subjects. Note subject six begins by orienting without walking, then walks to the target. Subject two orients and walks at the same time, especially during early trials.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig3-figsupp1-v1"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37841.009</object-id><label>Figure 3âfigure supplement 2.</label><caption><title>Additional experimental functions.</title><p>(<bold>A</bold>) to <bold>B</bold>) Automated sign recognition using computer vision. Using Vuforia software (<ext-link ext-link-type="uri" xlink:href="https://www.vuforia.com/">https://www.vuforia.com/</ext-link>) the HoloLens recognizes a menâs room sign (<bold>A</bold>), image viewed through HoloLens) and installs a virtual object (cube, arrow) next to the sign. (<bold>B</bold>) This object persists in the space even when the sign is no longer visible. (<bold>C</bold>) Automated wayfinding. The HoloLens generates a path to the target (door) that avoids the obstacle (white box). Then a virtual guide (orange balloon) can lead the user along the path. See <xref ref-type="video" rid="video2">Videos 2</xref>â<xref ref-type="video" rid="video3">3</xref>. (<bold>D</bold>) Navigation in the presence of obstacles. The subject navigates from the starting zone (red circle) to an object in the target zone (green circle) using calls emitted by the object. Three vertical columns block the path (black circles), and the subject must weave between them using the obstacle warning system. Raw trajectories (no filtering) of a blind subject (#5) are shown during outbound (left) and return trips (right), illustrating effective avoidance of the columns. This experiment was performed with a version of the apparatus built around the HTC Vive headset. (<bold>E</bold>) Orienting functions of the virtual guide. In addition to spatialized voice calls, the virtual guide may also offer turning commands toward the next waypoint. In the illustrated example, the instruction is âin x meters, turn right.â (<bold>F</bold>) Real-time object detection using YOLO (<xref ref-type="bibr" rid="bib26">Redmon and Farhadi, 2018</xref>). Left: A real scene. Note even small objects on a textured background are identified efficiently based on a single video frame. Right: A virtual scene from the benchmarking environment, rendered by Unity software.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig3-figsupp2-v1"/></fig></fig-group><p>For comparison, we asked subjects to find a real chair in the same space using only their usual walking aid (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). These searches took on average eight times longer and covered 13 times the distance needed with CARA. In a related series of experiments we encumbered the path to the target with several virtual obstacles. Using the alarm sounds, our subjects weaved through the obstacles without collision (<xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2D</xref>). Informal reports from the subjects confirmed that steering towards a voice is a natural function that can be performed automatically, leaving attentional bandwidth for other activities. For example, some subjects carried on a conversation while following CARA.</p></sec><sec id="s2-6"><title>Long-range guided navigation</title><p>If the target object begins to move as the subject follows its voice, it becomes a âvirtual guideâ. We designed a guide that follows a precomputed path and repeatedly calls out âfollow meâ. The guide monitors the subjectâs progress, and stays at most 1 m ahead of the subject. If the subject strays off the path, the guide stops and waits for the subject to catch up. The guide also offers warnings about impending turns or a flight of stairs. To test this design, we asked subjects to navigate a campus building that had been pre-scanned by the HoloLens (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>). The path led from the ground-floor entrance across a lobby, up two flights of stairs, around several corners and along a straight corridor, then into a second floor office (<xref ref-type="fig" rid="fig4">Figure 4BâC</xref>). The subjects had no prior experience with this part of the building. They were told to follow the voice of the virtual guide, but given no assistance or coaching during the task.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.37841.010</object-id><label>Figure 4.</label><caption><title>Long-range guided navigation task.</title><p>(<bold>A</bold>) 3D reconstruction of the experimental space with trajectories from all subjects overlaid. (<bold>B and C</bold>) 2D floor plans with all first trial trajectories overlaid. Trajectories are divided into three segments: lobby (Start â Start 2), stairwell (Start 2 â Start 3), and hallway (Start 3 â Destination). Red arrows indicate significant deviations from the planned path. (<bold>D</bold>) Deviation index (as in <xref ref-type="fig" rid="fig3">Figure 3E</xref>) for all segments by subject. Outlier corresponds to initial error by subject 7. Negative values indicate that the subject cut corners relative to the virtual guide. (<bold>E</bold>) Duration and (<bold>F</bold>) normalized speed of all the segments by subject.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig4-v1"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37841.011</object-id><label>Figure 4âfigure supplement 1.</label><caption><title>Guided navigation trajectories.</title><p>(<bold>A</bold>) 3D model of the experimental space as scanned by the HoloLens. (<bold>B</bold>) Subject and guide trajectories from the long-range guided navigation task. Note small differences between guide trajectories across experimental days, owing to variations in detailed waypoint placement.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig4-figsupp1-v1"/></fig></fig-group><p>All seven subjects completed the trajectory on the first attempt (<xref ref-type="fig" rid="fig4">Figure 4BâC</xref>, <xref ref-type="video" rid="video1">Video 1</xref>). Subject seven transiently walked off course (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), due to her left-ward bias (<xref ref-type="fig" rid="fig1">Figures 1C</xref> and <xref ref-type="fig" rid="fig3">3C</xref>), then regained contact with the virtual guide. On a second attempt, this subject completed the task without straying. On average, this task required 119 s (range 73â159 s), a tolerable investment for finding an office in an unfamiliar building (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). The median distance walked by the subjects was 36 m (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), slightly shorter (~1%) than the path programmed for the virtual guide, because the subjects can cut corners (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). The subjectsâ speed varied with difficulty along the route, but even on the stairs they proceeded atÂ ~60% of their free-walking speed (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). On arriving at the office, one subject remarked &quot;That was fun! When can I get one?â. Other comments from subjects regarding user experience with CARA are provided in âSupplementary Observationsâ.</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-37841-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.37841.012</object-id><label>Video 1.</label><caption><title>Long-range navigation.</title><p>A video recording of a subject navigating in the long-range navigation task. The top right panel shows the first person view of the subject recorded by the HoloLens.</p></caption></media></sec><sec id="s2-7"><title>Technical extensions</title><p>As discussed above, the capabilities for identification of objects and people in a dynamic scene are rapidly developing. We have already implemented real-time object naming for items that are easily identified by the HoloLens, such as standardized signs and bar codes (<xref ref-type="bibr" rid="bib37">Sudol et al., 2010</xref>) (<xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2AâB</xref>). Furthermore, we have combined these object labels with a scan of the environment to compute in real time a navigable path around obstacles toward any desired target (<xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2C</xref>, <xref ref-type="video" rid="video2">Video 2</xref>, <xref ref-type="video" rid="video3">Video 3</xref>). In the few months since our experimental series with blind subjects, algorithms have appeared that come close to a full solution. For example, YOLO (<xref ref-type="bibr" rid="bib26">Redmon and Farhadi, 2018</xref>) will readily identify objects in a real time video feed that match one of 9000 categories. The algorithm already runs on the HoloLens and we are adopting it for use within CARA (<xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2F</xref>).</p><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-37841-video2.mp4"><object-id pub-id-type="doi">10.7554/eLife.37841.013</object-id><label>Video 2.</label><caption><title>Automatic wayfinding explained.</title><p>A video demonstration of how automatic wayfinding works in a virtual environment.</p></caption></media><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-37841-video3.mp4"><object-id pub-id-type="doi">10.7554/eLife.37841.014</object-id><label>Video 3.</label><caption><title>Automatic wayfinding in an office.</title><p>A point of view video demonstration of the automatic wayfinding function in an office space with obstacles. The path is calculated at the userâs command based on the geometry of the office.</p></caption></media></sec><sec id="s2-8"><title>An open-source benchmarking environment for assistive devices</title><p>The dramatic advances in mobile computing and machine vision are enabling a flurry of new devices and apps that offer one or another assistive function for the vision impaired. To coordinate these developments one needs a reliable common standard by which to benchmark and compare different solutions. In several domains of engineering, the introduction of a standardized task with a quantitative performance metric has stimulated competition and rapid improvement of designs (<xref ref-type="bibr" rid="bib3">Berens et al., 2018</xref>; <xref ref-type="bibr" rid="bib29">Russakovsky et al., 2015</xref>).</p><p>On this background, we propose a method for the standardized evaluation of different assistive devices for the blind. The user is placed into a virtual environment implemented on the HTC Vive platform (<xref ref-type="bibr" rid="bib41">Wikipedia, 2018</xref>).This virtual reality kit is widely used for gaming and relatively affordable. Using this platform, researchers anywhere in the world can replicate an identical environment and use it to benchmark their assistive methods. This avoids having to replicate and construct real physical spaces.</p><p>At test time the subject dons a wireless headset and moves freely within a physical space of 4 m x 4 m. The Vive system localizes position and orientation of the headset in that volume. Based on these data, the virtual reality software computes the subjectâs perspective of the virtual scene, and presents that view through the headsetâs stereo goggles. An assistive device of the experimenterâs choice can use that same real-time view of the environment to guide a blind or blind-folded subject through the space. This approach is sufficiently general to accommodate designs ranging from raw sensory substitution â like vOICe (<xref ref-type="bibr" rid="bib23">Meijer, 1992</xref>) and BrainPort (<xref ref-type="bibr" rid="bib36">Stronks et al., 2016</xref>) â to cognitive assistants like CARA. The tracking data from the Vive system then serve to record the userâs actions and evaluate the performance on any given task.</p><p>To illustrate this method, we constructed a virtual living room with furniture (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Within that space we defined three tasks that involve (1) scene understanding, (2) short-range navigation, and (3) finding a small object dropped on the floor. To enable blind subjects in these tasks we provided two assistive technologies: (a) the high-level assistant CARA, using the same principle of talking objects as described above on the HoloLens platform; (b) the low-level method vOICe that converts photographs to soundscapes at the raw image level (<xref ref-type="bibr" rid="bib23">Meijer, 1992</xref>). The vOICe system was implemented using software provided by its inventor (<xref ref-type="bibr" rid="bib31">Seeing With Sound, 2018</xref>).</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.37841.015</object-id><label>Figure 5.</label><caption><title>Benchmark testing environment.</title><p>(<bold>A</bold>) A virtual living room including 16 pieces of furniture and other objects. (<bold>B</bold>) Localization of a randomly chosen object relative to the true object location (0 deg, dashed line) for four subjects using CARA (<bold>C</bold>) or vOICe (<bold>V</bold>). Box denotes s.e.m., whiskers s.d. For all subjects the locations obtained with vOICe are consistent with a uniform circular distribution (Rayleigh z test, p&gt;0.05). (<bold>C</bold>) Navigation toward a randomly placed chair. Trajectories from one subject using CARA (left) and vOICe (middle), displayed as in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. Right: Number of trials completed and time per trial (meanÂ Â±s.d.). (<bold>D</bold>) Navigation toward a randomly placed key on the floor (small green circle). Trajectories and trial statistics displayed as in panel C.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig5-v1"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37841.016</object-id><label>Figure 5âfigure supplement 1.</label><caption><title>Benchmark tests in a virtual environment.</title><p>Trajectories of three additional subjects. (<bold>A</bold>) Navigation to a randomly placed chair, using either CARA or vOICe, displayed as in <xref ref-type="fig" rid="fig5">Figure 5C</xref>. Subject #4 exhibited some directed navigation using vOICe. (<bold>B</bold>) Finding a dropped key, as in <xref ref-type="fig" rid="fig5">Figure 5D</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37841-fig5-figsupp1-v1"/></fig></fig-group><p>Here, we report performance of four subjects, all normally sighted. Each subject was given a short explanation of both CARA and vOICe. The subject was allowed to practice (~10 min) with both methods by viewing the virtual scene while either CARA or vOICe provided translation to sound delivered by headphones. Then, the subjects were blindfolded and performed the three tasks with sound alone. Each task consisted of 20 trials with randomly chosen goals, and a time limit of 60 s was applied to each trial.</p><p>On the first task, the subject stood in the middle of the virtual living room and was asked to locate one of the objects and point at it with the head. With CARA, subjects mostly used the Target mode to efficiently find the desired object, and located it based on the 3D sound cues, with a typical aiming error of 10 degrees (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, bias of â0.2â13 deg, accuracy 4.5â27 deg). With vOICe, subjects reported great difficulty at identifying objects, despite the earlier opportunity to practice with visual feedback. Their aiming choices were statistically consistent with a uniform random distribution (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><p>On the second task, the subject was asked to walk from the middle of the arena toward a chair placed randomly in one of four locations, as in the directed navigation task of <xref ref-type="fig" rid="fig3">Figure 3</xref>. Using CARA, subjects found the chair efficiently, requiring only 10 s on average (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Using vOICe, most subjects meandered through the arena, on occasion encountering the chair by accident. Only one subject was able to steer toward the chair (<xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>). None of the subjects were able to complete 20 trials within the 60 s time limit.</p><p>On the third task, the subject was asked to find a key that had fallen on the floor of the arena. To complete the task, the subjectâs head had to point toward the key atÂ &lt;1 m distance. Under those conditions, one can readily reach out and grasp the object. Using CARA, subjects found the key efficiently (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Using vOICe none of the subjects were able to locate the key (<xref ref-type="fig" rid="fig5">Figure 5D</xref>), although two of them encountered it once by accident (<xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>).</p><p>These experiments illustrate the use of a standardized testing environment. Naive subjects performed well on these real-world tasks using CARA, but not using vOICe. It should be said that interpreting the vOICe sounds is very non-intuitive. Our subjects received the basic instructions offered on the vOICe web site and the âExerciseâ mode of vOICe (<xref ref-type="bibr" rid="bib31">Seeing With Sound, 2018</xref>), followed by a short period of practice. Extensive training with vOICe confers blind subjects with some ability to distinguish high contrast shapes on a clean background (<xref ref-type="bibr" rid="bib2">Auvray et al., 2007</xref>; <xref ref-type="bibr" rid="bib35">Striem-Amit et al., 2012</xref>). Conceivably an experienced vOICe user might perform better on the tests described here. Other investigators can attempt to demonstrate this using our published code and specificationsÂ (<xref ref-type="bibr" rid="bib16">Liu and Meister, 2018</xref>;Â copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/CARA_Public">https://github.com/elifesciences-publications/CARA_Public</ext-link>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>âSeeing is knowing what is where by lookingâ (<xref ref-type="bibr" rid="bib21">Marr, 1982</xref>). The cognitive assistant CARA enables exactly this process while replacing the eye and early visual system. CARA conveys âwhatâ by the names of objects and âwhereâ by the location from where each object calls. âLookingâ occurs when the user actively requests these calls. The principal reason sighted people rely on vision much more than audition is that almost all objects in the world emit useful light signals almost all the time, whereas useful sound signals from our surroundings are few and sporadic. CARA can change this calculus fundamentally, such that all the relevant objects emit easily interpretable sounds. It is conceivable that the extended use of such an intuitive modality eventually produces subjective qualia that resemble the feeling of seeing.</p><p>The use of spatialized sound in assistive devices has been considered for some time (<xref ref-type="bibr" rid="bib32">Spagnol et al., 2018</xref>), although to date there are few practical implementations with test results. For the purpose of outdoor navigation, <xref ref-type="bibr" rid="bib17">Loomis et al. (2005)</xref> tested a system that guides the user along a path by virtual sounds emitted from way points. Their subjects preferred speech sounds over simple tones, and performed better under those conditions, which supports the use of voices in CARAâs virtual guide. To enable recognition of scenes and objects, various schemes have been considered that convert the scene into spatialized sound, but they largely encode low-level geometric features. At the simplest level, the vOICe system takes a photograph, interprets it as a spectrogram, and presents the resulting sound with a left-to-right scan (<xref ref-type="bibr" rid="bib10">Haigh et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Meijer, 1992</xref>). Another system parses the scene into inclined planes and encodes each plane with a sequence of musical sounds; a chair might consist of two planes (<xref ref-type="bibr" rid="bib27">Ribeiro et al., 2012</xref>). These designs still doÂ not do enough to âoff-load the blind traveler's brain of â¦ arduous tasksâ (<xref ref-type="bibr" rid="bib7">Collins, 1985</xref>). Instead of posing acoustic puzzles of this kind, CARA uses plain language to tell the user what is out there and where it is. As a rule, assistive devices have been designed to perform one well-circumscribed function, such as obstacle avoidance, or route finding, or object recognition, or reading of signage (<xref ref-type="bibr" rid="bib18">Loomis et al., 2012</xref>; <xref ref-type="bibr" rid="bib28">Roentgen et al., 2008</xref>). Our main contribution here is to show that augmented reality with object voices offers a natural and effortless human interface for all these functionalities, implemented in a single device.</p><p>So far, we have focused on indoor applications. Blind people report that outdoor navigation is supported by many services (access vans, GPS, mobile phones with navigation apps) but these all fall away when one enters a building (<xref ref-type="bibr" rid="bib13">Karimi, 2015</xref>). In its present form CARA can already function in this underserved domain, for example as a guide in a large public building, hotel, or mall. No physical modifications of the space are required. The virtual guide can be programmed to offer navigation options according to the known building geometry. Thanks to the intuitive interface, naive visitors could pick up a device at the building entrance and begin using it in minutes. In this context, recall that our subjects were chosen without prescreening, including cases of early and late blindness and various hearing deficits (<xref ref-type="fig" rid="fig1">Figure 1D</xref>): They represent a small but realistic sample of the expected blind user population.</p><p>The functionality of this system can be enhanced far beyond replacing vision, by including information that is not visible. As a full service computer with online access, the HoloLens can be programmed to annotate the scene and offer ready access to other forms of knowledge. Down the line one can envision an intelligent cognitive assistant that is attractive to both blind and sighted users, with somewhat different feature sets. Indeed this may help integrate the blind further into the community. By this point, we expect that the reader already has proposals in mind for enhancing the cognitive assistant to communicate things that we cannot see. A hardware/software platform is now available to rapidly implement those ideas and test them with human subjects.</p><p>Finally, we demonstrated a separate platform for standardized tests to evaluate and benchmark CARA and other assistive technologies on the same tasks. The use of virtual reality on an affordable consumer device can recreate the identical test environment for blind subjects in laboratories around the world, allowing quantitative performance comparisons even with implanted prosthetic devices. We hope that other developers of assistive technology will engage and collaborate to produce an agreed-upon standard test suite, starting with our published example. This in turn should stimulate competition to exploit the ongoing revolution in wearable computers and machine vision toward creative solutions in the domain of assistive technology.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>General implementation of CARA</title><p>The hardware platform for the cognitive assistant is the Microsoft HoloLens Development Edition, without any modifications. This is a self-contained wearable augmented reality (AR) device that can map and store the 3D mesh of an indoor space, localize itself in real time, and provide spatialized audio and visual display (<xref ref-type="bibr" rid="bib11">Hoffman, 2016</xref>). We built custom software in Unity 2017.1.0f3 (64-bit) with HoloToolkit-Unity-v1.5.5.0. The scripts are written in C# with MonoDevelop provided by Unity. The experiments are programmed on a desktop computer running Windows 10 Education and then deployed to Microsoft HoloLens. The software is versatile enough to be easily deployed to other hardware platforms, such as AR enabled smart phones.</p></sec><sec id="s4-2"><title>User interface</title><p>Before an experiment, the relevant building areas are scanned by the experimenter wearing theÂ HoloLens, so the system has a 3D model of the space ahead of time. For each object in the scene, the system creates a voice that appears to emanate form the objectâs location, with a pitch that increases inversely with object distance. Natural spatialized sound is computed based on a generic head-related transfer function (<xref ref-type="bibr" rid="bib40">Wenzel et al., 1993</xref>); nothing about the software was customized to individual users. Object names and guide commands are translated into English using the text-to-speech engine from HoloToolkit. The user provides input by moving the head to point at objects, pressing a wireless Clicker, using hand gesture commands or English voice commands.</p><p>In addition to instructions shown in the main body of the article, non-spatialized instructions are available at the userâs request by voice commands. The user can use two voice commands (e.g. âdirectionâ, âdistanceâ) to get the direction of the current object of interest or its distance. Depending on the mode, the target object can be the object label of userâs choice (Target Mode) or the virtual guide. âTurn-by-turnâ instructions can be activated by voice commands (e.g. âinstructionâ). The instruction generally consists of two parts, the distance the user has to travel until reaching the current target waypoint, and the turn needed to orient to the next waypoint (<xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2E</xref>).</p></sec><sec id="s4-3"><title>Experimental design</title><p>All results in <xref ref-type="fig" rid="fig1">Figures 1</xref>â<xref ref-type="fig" rid="fig4">4</xref> were gathered using a frozen experimental protocol, finalized before recruitment of the subjects. The tasks were fully automated, with dynamic instructions from the HoloLens, so that no experimenter involvement was needed during the task. Furthermore, we report performance of all subjects on all trials gathered this way. Some incidental observations and anecdotes from subject interviews are provided in Supplementary Observations. All procedures involving human subjects were reviewed and approved by the Institutional Review Board at Caltech. All subjects gave their informed consent to the experiments, and where applicable to publication of videos that accompany this article.</p></sec><sec id="s4-4"><title>Measurement</title><p>Timestamps are generated by the internal clock of the HoloLens. The six parameters of the subjectâs head location and orientation are recorded at 5 Hz from the onset to the completion of each trial in each task. All performance measures are derived from these time series. Localization errors of the HoloLens amount toÂ &lt;4 cm (<xref ref-type="bibr" rid="bib15">Liu et al., 2018</xref>), which is insignificant compared to the distance measures reported in our study, and smaller than the line width in the graphs of trajectories in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>.</p></sec><sec id="s4-5"><title>Task design</title><p><italic>Task 1, object localization (<xref ref-type="fig" rid="fig1">Figure 1</xref>):</italic> In each trial, a single target is placed 1 m from the subject at a random azimuth angle drawn from a uniform distribution between 0 and 360 degrees. To localize the target, the subject presses the Clicker to hear a spatialized call from the target. After aiming the face at the object the subject confirms via a voice command (âTarget confirmedâ). When the location is successfully registered, the device plays a feedback message confirming the voice command and providing the aiming error. The subject was given 10â15 practice trials to learn the interaction with CARA, followed by 21 experimental trials. To estimate the upper limit on performance in this task, two sighted subjects performed the task with eyes open: this produced a standard deviation across trials of 0.31 and 0.36 degrees, and a bias of 0.02 and 0.06 degrees. That includes instrumentation errors as well as uncertainties in the subjectâs head movement. Note that these error sources are insignificant compared to the accuracy and bias reported in <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>.</p><p><italic>Task 2, spatial memory (<xref ref-type="fig" rid="fig2">Figure 2</xref>):</italic> This task consists of an exploration phase in which the subject scans the scene, followed by a recall phase with queries about the scene. Five objects are placed two meters from the subject at azimuth angles of â60Â°, â30Â°, 0Â°, 30Â°, 60Â° from the subjectâs initial orientation. Throughout the experiment, a range between â7.5Â° and 7.5Â° in azimuth angle is marked by âsonar beepsâ to provide the subject a reference orientation. During the 60 s exploration phase, the subject uses âSpotlight Modeâ: This projects a virtual spotlight cone of 30Â° aperture around the direction the subject is facing and activates object voices inside this spotlight. Typically subjects scan the virtual scene repeatedly, while listening to the voices. In the recall phase, âSpotlight Modeâ is turned off and the subject performs four recall trials. For each recall trial, the subject presses the Clicker, then a voice instruction specifies which object to turn to, the subject faces in the recalled direction, and confirms with a voice command (âTarget confirmedâ). The entire task was repeated in two blocks that differed in the arrangement of the objects. The object sequence from left to right was âpianoâ, âtableâ, âchairâ, âlampâ, âtrash binâ (block 1), and âtrash binâ, âpianoâ, âtableâ, âchairâ,â lampâ (block 2). The center object is never selected as a recall target because 0Â° is marked by sonar beeps and thus can be aimed at trivially.</p><p><italic>Task 3, direct navigation (<xref ref-type="fig" rid="fig3">Figure 3</xref>):</italic> In each trial, a single chair is placed at 2 m from the center of the arena at an azimuth angle randomly drawn from four possible choices: 0Â°, 90Â°, 180Â°, 270Â°. To start a trial, the subject must be in a starting zone of 1 m diameter in the center. During navigation, the subject can repeatedly press the Clicker to receive a spatialized call from the target. The trial completes when the subject arrives within 0.5 m of the center of the target. Then the system guides the subject back to the starting zone using spatialized calls emanating from the center of the arena, and the next trial begins. Subjects performed 19â21 trials. All blind subjects moved freely without cane or guide dog during this task.</p><p>To measure performance on a comparable search without CARA, each subject performed a single trial with audio feedback turned off. A real chair is placed at one of the locations previously used for virtual chairs. The subject wears the HoloLens for tracking and uses a cane or other walking aid as desired. The trial completes when the subject touches the target chair with a hand. All blind subjects used a cane during this silent trial.</p><p><italic>Task 4, long range guided navigation (<xref ref-type="fig" rid="fig4">Figure 4</xref>):</italic> The experimenter defined a guide path ofÂ ~36 m length from the first-floor lobby to the second-floor office by placing nine waypoints in the pre-scanned environment. In each trial, the subject begins in a starting zone within 1.2 m of the first waypoint, and presses the Clicker to start. A virtual guide then follows the trajectory and guides the subject from the start to the destination. The guide calls out âfollow meâ with spatialized sound every 2 s, and it only proceeds along the path when the subject is less than 1 m away. Just before waypoints 2â8, a voice instruction is played to inform the subject about the direction of turn as well as approaching stairs. The trial completes when the subject arrives within 1.2 meters of the target. Voice feedback (âYou have arrivedâ) is played to inform the subject about arrival. In this task all blind subjects used a cane.</p><p><italic>Free walking:</italic> To measure the free walking speed, we asked subjects to walk for 20 m in a straight line in an unobstructed hallway using their preferred walking aid. Subjects 1 and 2 used a guide dog, the others a cane.</p><sec id="s4-5-1"><title>Data analysis and visualization</title><p>MatLab 2017b (Mathworks) and Excel (Microsoft) were used for data analysis and visualization. Unity 5.6.1f1 was used to generate 3D cartoons of experiments and to visualize 3D trajectories. Photoshop CC 2017 was used for overlaying trajectories on floor plans.</p><p><italic>Aiming:</italic> In tasks 1 and 2, aiming errors are defined as the difference between the target azimuth angle and the subjectâs front-facing azimuth angle. In task 2, to correct for the delay of voice command registration, errors are measured at 1 s before the end of each trial.</p><p><italic>Trajectory smoothing:</italic> The HoloLens tracks its wearerâs head movement, which includes lateral movements perpendicular to the direction of walking. To estimate the center of mass trajectory of the subject we applied a moving average with 2 s sliding window to the original trajectory.</p><p><italic>Length of trajectory and deviation index:</italic> In the directed navigation task and the long-range guided navigation task we computed the excess distance traveled by the subject relative to an optimal trajectory or the guide path. The deviation index, <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula>, is defined as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Â where <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>exp</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the length of the trajectory measured by experiment and <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the length of the reference trajectory. A value near 0 indicates that the subject followed the reference trajectory well.</p><p>In the direct navigation task, we divided each trial into an orientation phase where the subject turns the body to face the target, and a navigation phase where the subject approaches the target. We calculated head orientation and 2D distance to target in each frame, and marked the onset of the navigation phase when the subjectâs distance to target changed by 0.3 m. Note that with this criterion the navigation phase includes the occasional trajectory where the subject starts to walk in the wrong direction. In this task, <inline-formula><mml:math id="inf4"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is defined as the length of the straight line from the subjectâs position at the onset of the navigation phase to the nearest point of the target trigger zone.</p><p>In the long-range guided navigation task, <inline-formula><mml:math id="inf5"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the length of the guide trajectory. Due to variability in placing waypoints and tracking, the length of guide trajectories varied slightly across subjects (<inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>=36.4Â Â±Â 0.7 m, meanÂ Â±s.d.). Negative <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> values are possible in this task if the subject cuts corners of the guide trajectory.</p><p><italic>Speed:</italic> Speed is calculated frame-by-frame using the displacements in the filtered trajectories. For the long-range guided navigation task, which includes vertical movements through space, the speed of translation is computed in three dimensions, whereas for the other tasks that occur on a horizontal plane we did not include the vertical dimension. For all tasks, we estimated walking speed by the 90th percentile of the speed distribution, which robustly rejects the phases where the subject chooses an orientation. The normalized speed is obtained by dividing this value by the free walking speed.</p></sec></sec><sec id="s4-6"><title>Supplementary observations</title><p>Here, we report incidental observations during experiments with CARA that were not planned in the frozen protocol, and comments gathered from blind subjects in the course of the experiments.</p><p><italic>Subject 1:</italic> During navigation with the virtual guide says âseems to me the âfollow meâ sound means keep going straightâ. Thinks addition of GPS services could make the system useful outdoors as well. Suggests experimenting with bone conduction headphones. Offers us 1 hr on his radio show.</p><p><italic>Subject 2:</italic> During direct navigation says âpitch change [with distance] was informativeâ. During navigation with the virtual guide says ââFollow meâ was too much informationâ. Prefers to follow the explicit turn instructions. She could then transmit those instructions to her guide dog.</p><p><italic>Subject 3:</italic> In addition to object voices, he likes instructions of the type âkeep going forward for xx metersâ. During a previous visit using a similar system he commented on possible adoption by the blind community: âI could see people spending in 4 figures for [something] light and reliable, and use it all the timeâ. Also supports the concept of borrowing a device when visiting a public building or mall. Devices in the form of glasses would be better, preferably light and thin. âUse the computing power of my phone, then I donât have to carry anything else.â Likes the external speakers because they donât interfere with outside sound. Finds it easy to localize the virtual sound sources.</p><p><italic>Subject 4:</italic> After navigation with the virtual guide says âThat was fun. When can I get one?â Primarily used the âfollow meâ voice, and the cane to correct for small errors. Reports that the turn instructions could be timed earlier (this is evident also in Video 1). On a previous visit using a similar system: âIâm very excited about all of this, and I would definitely like to be kept in the loopâ. Also suggests the system could be used in gaming for the blind.</p><p><italic>Subject 5:</italic> During navigation with the virtual guide realized she made a wrong turn (see Fig. 4C) but the voice made her aware and allowed her to correct. Reports that the timing of turn instructions is a little off.</p><p><italic>Subject 6:</italic> After all tasks says âThat was pretty coolâ and âThe technology is there.â</p><p><italic>Subject 7:</italic> On the second trial with the virtual guide reports that she paid more attention to the âfollow meâ sound (she strayed temporarily on the first trial, Fig. 4B). Wonders whether the object voices will be strong enough in a loud environment.</p></sec><sec id="s4-7"><title>Benchmarking platform using virtual reality</title><p>The benchmarking platform runs on the HTC Vive VR headset and a Windows 10 desktop computer. A TPCast wireless adapter (CE-01H, <ext-link ext-link-type="uri" xlink:href="https://www.tpcastvr.com/">https://www.tpcastvr.com/</ext-link>) replaces the standard headset cable so the subject can move freely within the 4 m x 4 m square arena. An Xbox One S controller is connected wirelessly to the host computer for the subject to start trials, confirm aiming, and control the modes of CARA. Audio is delivered by a pair of wireless headphones (SONY WH-1000XM2). All code and data to replicate this environment and the reported tests is publically availableÂ at <ext-link ext-link-type="uri" xlink:href="https://github.com/meisterlabcaltech/CARA_Public">https://github.com/meisterlabcaltech/CARA_Public</ext-link> (<xref ref-type="bibr" rid="bib16">Liu and Meister, 2018</xref>;Â copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/CARA_Public">https://github.com/elifesciences-publications/CARA_Public</ext-link>).</p><p>All three benchmarking tasks are set in a 10 m x 10 m virtual environment that simulates a living room with 16 objects labeled with sound tags. The rendered scenes are close to photo-realistic, and computer vision algorithms trained on real scenes will correctly identify objects in the virtual scene as well (<xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2F</xref>). The first benchmark task (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) resembles Task 1 (see section on Task Design above) except that the aiming target is chosen at random from the 16 objects in the studio. CARA users have access to Spotlight and Target Mode. The second benchmark task (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) resembles Task 3 with a 60 s time limit on each trial. CARA users only have Target Mode available for this task. To facilitate detection with vOICe the chair was made white, and left unoccluded by any other objects. The third benchmark task (<xref ref-type="fig" rid="fig5">Figure 5D</xref>) replaces the chair in the second benchmark task with a key on the floor. Within CARA, Target Mode and Spotlight Mode can be used in this task. Two conditions have to be met to finish a trial: (1) the head of the subject is within 1 m of the key and (2) the subject faces within 30 degrees of the key. To accomplish this the subject must bend down or kneel facing the key. At this point, a simple reach with the hand would allow grasping a real object. A trial fails if not finished within 60 s. To avoid excess frustration among the subjects a task is terminated after five failed trials.</p><sec id="s4-7-2"><title><italic>Voice control on CARA</italic>Â </title><p>In addition to the Clicker, subjects can also use natural language (e.g. English) as input to the system. Two subsystems of voice input are implemented: 1) keyword recognition (PhraseRecognitionSystem) monitors in the background what the user says, detects phrases that match the registered keywords, and activates corresponding functions on detection of keyword matches. 2) dictation (DictationRecognizer) records what the user says and converts it into text. The first component enables subjects to confirm their aiming in the object localization task and mental imagery task with the voice command âtarget confirmedâ. It also enables the experimenter to control the experiment at runtime.</p><p>Keywords and their functions are defined through adding keywords to the keyword manager script provided by HoloToolkit and editing their responses. The KeywordRecognizer component starts at the beginning of each instance of the application and runs in the background throughout the instance of the application except for the time period in which dictation is in use.</p><p>To allow users to create object labels, the DictationRecognizer provided by HoloToolkit is used to convert natural language spoken by the user to English text. Due to the mutual exclusivity, KeywordRecognizer is shut down before DictationRecognizer is activated, and restarted after the dictation is finished.</p></sec><sec id="s4-7-3"><title><italic>Automated wayfinding</italic>Â </title><p>In addition to hand-crafting paths, we implemented automated wayfinding by taking advantage of Unityâs runtime NavMesh âbakingâ which calculates navigable areas given a 3D model of the space. At runtime, we import and update the 3D mesh of the scanned physical space and use it to bake the 3D mesh. When the user requests guided navigation, a path from the userâs current location to the destination of choice is calculated. If the calculated path is valid, the virtual guide guides the user to the destination using the computer-generated path.</p></sec><sec id="s4-7-4"><title><italic>Cost of the CARA system</italic>Â </title><p>The hardware platform used in the research â Microsoft HoloLens Development Edition â currently costs $3000. Several comparable AR goggles are in development, and one expects their price to drop in the near future. In addition, smart phones are increasingly designed with AR capabilities, although they do not yet match the HoloLens in the ability to scan the surrounding space and localize within it.</p></sec><sec id="s4-7-5"><title>Battery and weight</title><p>The current HoloLens weighs 579 g. Like all electronic devices, this will be further miniaturized in the future. The current battery supports our system functions for 2â5 hr, sufficient for the indoor excursions we envision in public buildings, led by the âvirtual guideâ. A portable battery pack can extend use to longer uninterrupted sessions.</p></sec><sec id="s4-7-6"><title>Tracking robustness</title><p>While in most indoor scenarios that we have tested the tracking of HoloLens was reliable and precise, we have encountered occasional loss of tracking or localization errors. This occurs particularly when the environment lacks visual features such as a narrow space with white walls.</p></sec><sec id="s4-7-7"><title><italic>Extensions</italic>Â </title><p>Because this cognitive assistant is largely defined by software its functionalities are very flexible. For example, the diverse recommendations from subjects noted above (Supplementary Observations) can be implemented in short order. In addition, one can envision hardware extensions by adding peripherals to the computer. For example, a haptic belt or vest could be used to convey collision alarms (<xref ref-type="bibr" rid="bib1">Adebiyi et al., 2017</xref>), thus leaving the auditory channel open for the highly informative messages.</p></sec></sec><sec id="s4-8"><title>Data and materials availability</title><p>Data and code that produced the figures are available on the Dryad Digital Repository.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>Supported by grant 103212 from the Shurl and Kay Curci Foundation. We thank Ralph Adolphs, David Anderson, and Shin Shimojo for comments on the manuscript, and Kristina Dylla for posing for <xref ref-type="fig" rid="fig1">Figure 1A</xref> and field testing all of the tasks.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>is an author on a patent application (Patent Application No. 15/852,034) 'Systems and Methods for Generating Spatial Sound Information Relevant to Real-World Environments'</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf3"><p>Markus Meister: is an author on a patent application (Patent Application No. 15/852,034) 'Systems and Methods for Generating Spatial Sound Information Relevant to Real-World Environments'</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writingâreview and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Validation, Investigation, Methodology</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Writingâoriginal draft, Project administration, Writingâreview and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All procedures involving human subjects were reviewed and approved by the Institutional Review Board at Caltech, Human Subjects Protocol 16-0663. All subjects gave their informed consent to the experiments, and where applicable to publication of videos that accompany this article.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.37841.017</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-37841-transrepform-v1.pdf"/></supplementary-material><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files. Source data files for all figures have been deposited in Dryad. Additional code is published on a Github repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/meisterlabcaltech/CARA_Public">https://github.com/meisterlabcaltech/CARA_Public</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/CARA_Public">https://github.com/elifesciences-publications/CARA_Public</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Stiles</surname><given-names>NRB</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Data from: Augmented Reality Powers a Cognitive Prosthesis for the Blind</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.8mb5r88</pub-id></element-citation></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adebiyi</surname> <given-names>A</given-names></name><name><surname>Sorrentino</surname> <given-names>P</given-names></name><name><surname>Bohlool</surname> <given-names>S</given-names></name><name><surname>Zhang</surname> <given-names>C</given-names></name><name><surname>Arditti</surname> <given-names>M</given-names></name><name><surname>Goodrich</surname> <given-names>G</given-names></name><name><surname>Weiland</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Assessment of feedback modalities for wearable visual aids in blind mobility</article-title><source>Plos One</source><volume>12</volume><elocation-id>e0170531</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0170531</pub-id><pub-id pub-id-type="pmid">28182731</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auvray</surname> <given-names>M</given-names></name><name><surname>Hanneton</surname> <given-names>S</given-names></name><name><surname>O'Regan</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning to perceive with a visuo-auditory substitution system: localisation and object recognition with 'the vOICe'</article-title><source>Perception</source><volume>36</volume><fpage>416</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1068/p5631</pub-id><pub-id pub-id-type="pmid">17455756</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berens</surname> <given-names>P</given-names></name><name><surname>Freeman</surname> <given-names>J</given-names></name><name><surname>Deneux</surname> <given-names>T</given-names></name><name><surname>Chenkov</surname> <given-names>N</given-names></name><name><surname>McColgan</surname> <given-names>T</given-names></name><name><surname>Speiser</surname> <given-names>A</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name><name><surname>Turaga</surname> <given-names>SC</given-names></name><name><surname>Mineault</surname> <given-names>P</given-names></name><name><surname>Rupprecht</surname> <given-names>P</given-names></name><name><surname>Gerhard</surname> <given-names>S</given-names></name><name><surname>Friedrich</surname> <given-names>RW</given-names></name><name><surname>Friedrich</surname> <given-names>J</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Bolte</surname> <given-names>B</given-names></name><name><surname>Machado</surname> <given-names>TA</given-names></name><name><surname>Ringach</surname> <given-names>D</given-names></name><name><surname>Stone</surname> <given-names>J</given-names></name><name><surname>Rogerson</surname> <given-names>LE</given-names></name><name><surname>Sofroniew</surname> <given-names>NJ</given-names></name><name><surname>Reimer</surname> <given-names>J</given-names></name><name><surname>Froudarakis</surname> <given-names>E</given-names></name><name><surname>Euler</surname> <given-names>T</given-names></name><name><surname>RomÃ¡n RosÃ³n</surname> <given-names>M</given-names></name><name><surname>Theis</surname> <given-names>L</given-names></name><name><surname>Tolias</surname> <given-names>AS</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Community-based benchmarking improves spike rate inference from two-photon calcium imaging data</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006157</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006157</pub-id><pub-id pub-id-type="pmid">29782491</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourne</surname> <given-names>RRA</given-names></name><name><surname>Flaxman</surname> <given-names>SR</given-names></name><name><surname>Braithwaite</surname> <given-names>T</given-names></name><name><surname>Cicinelli</surname> <given-names>MV</given-names></name><name><surname>Das</surname> <given-names>A</given-names></name><name><surname>Jonas</surname> <given-names>JB</given-names></name><name><surname>Keeffe</surname> <given-names>J</given-names></name><name><surname>Kempen</surname> <given-names>JH</given-names></name><name><surname>Leasher</surname> <given-names>J</given-names></name><name><surname>Limburg</surname> <given-names>H</given-names></name><name><surname>Naidoo</surname> <given-names>K</given-names></name><name><surname>Pesudovs</surname> <given-names>K</given-names></name><name><surname>Resnikoff</surname> <given-names>S</given-names></name><name><surname>Silvester</surname> <given-names>A</given-names></name><name><surname>Stevens</surname> <given-names>GA</given-names></name><name><surname>Tahhan</surname> <given-names>N</given-names></name><name><surname>Wong</surname> <given-names>TY</given-names></name><name><surname>Taylor</surname> <given-names>HR</given-names></name><collab>Vision Loss Expert Group</collab></person-group><year iso-8601-date="2017">2017</year><article-title>Magnitude, temporal trends, and projections of the global prevalence of blindness and distance and near vision impairment: a systematic review and meta-analysis</article-title><source>The Lancet Global Health</source><volume>5</volume><fpage>e888</fpage><lpage>e897</lpage><pub-id pub-id-type="doi">10.1016/S2214-109X(17)30293-0</pub-id><pub-id pub-id-type="pmid">28779882</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bujacz</surname> <given-names>M</given-names></name><name><surname>StrumiÅÅo</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Sonification: review of auditory display solutions in electronic travel aids for the blind</article-title><source>Archives of Acoustics</source><volume>41</volume><fpage>401</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1515/aoa-2016-0040</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Capelle</surname> <given-names>C</given-names></name><name><surname>Trullemans</surname> <given-names>C</given-names></name><name><surname>Arno</surname> <given-names>P</given-names></name><name><surname>Veraart</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A real-time experimental prototype for enhancement of vision rehabilitation using auditory substitution</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>45</volume><fpage>1279</fpage><lpage>1293</lpage><pub-id pub-id-type="doi">10.1109/10.720206</pub-id><pub-id pub-id-type="pmid">9775542</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="1985">1985</year><chapter-title>On Mobility Aids for the Blind</chapter-title><source>Electronic Spatial Sensing for the Blind</source><publisher-loc>Dordrecht</publisher-loc><publisher-name>Springer</publisher-name><fpage>35</fpage><lpage>64</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>CsapÃ³</surname> <given-names>ÃdÃ¡m</given-names></name><name><surname>WersÃ©nyi</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Overview of auditory representations in human-machine interfaces</article-title><source>ACM Computing Surveys</source><volume>46</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1145/2543581.2543586</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobelle</surname> <given-names>WH</given-names></name><name><surname>Mladejovsky</surname> <given-names>MG</given-names></name><name><surname>Girvin</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Artifical vision for the blind: electrical stimulation of visual cortex offers hope for a functional prosthesis</article-title><source>Science</source><volume>183</volume><fpage>440</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1126/science.183.4123.440</pub-id><pub-id pub-id-type="pmid">4808973</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haigh</surname> <given-names>A</given-names></name><name><surname>Brown</surname> <given-names>DJ</given-names></name><name><surname>Meijer</surname> <given-names>P</given-names></name><name><surname>Proulx</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>How well do you see what you hear? The acuity of visual-to-auditory sensory substitution</article-title><source>Frontiers in Psychology</source><volume>4</volume><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00330</pub-id><pub-id pub-id-type="pmid">23785345</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The future of three-dimensional thinking</article-title><source>Science</source><volume>353</volume><elocation-id>876</elocation-id><pub-id pub-id-type="doi">10.1126/science.aah5394</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jafri</surname> <given-names>R</given-names></name><name><surname>Ali</surname> <given-names>SA</given-names></name><name><surname>Arabnia</surname> <given-names>HR</given-names></name><name><surname>Fatima</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Computer vision-based object recognition for the visually impaired in an indoors environment: a survey</article-title><source>The Visual Computer</source><volume>30</volume><fpage>1197</fpage><lpage>1222</lpage><pub-id pub-id-type="doi">10.1007/s00371-013-0886-1</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Karimi</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Indoor Wayfinding and Navigation</source><publisher-loc>United States</publisher-loc><publisher-name>CRC Press</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lacey</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Multisensory Imagery</source><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Dong</surname> <given-names>H</given-names></name><name><surname>Zhang</surname> <given-names>L</given-names></name><name><surname>El Saddik</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Technical evaluation of HoloLens for multimedia: a first look</article-title><source>IEEE MultiMedia</source><pub-id pub-id-type="doi">10.1109/MMUL.2018.2873473</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Cognitive Augmented Reality Assistant (CARA) for the Blind</data-title><source>GitHub</source><version designator="e5514ff">e5514ff</version><ext-link ext-link-type="uri" xlink:href="https://github.com/meisterlabcaltech/CARA_Public">https://github.com/meisterlabcaltech/CARA_Public</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loomis</surname> <given-names>JM</given-names></name><name><surname>Marston</surname> <given-names>JR</given-names></name><name><surname>Golledge</surname> <given-names>RG</given-names></name><name><surname>Klatzky</surname> <given-names>RL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Personal guidance system for people with visual impairment: a comparison of spatial displays for route guidance</article-title><source>Journal of Visual Impairment &amp; Blindness</source><volume>99</volume><fpage>219</fpage><lpage>232</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Loomis</surname> <given-names>JM</given-names></name><name><surname>Klatzky</surname> <given-names>RL</given-names></name><name><surname>Giudice</surname> <given-names>NA</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Sensory Substitution of Vision: Importance of Perceptual and Cognitive Processing</chapter-title><person-group person-group-type="editor"><name><surname>Manduchi</surname> <given-names>R</given-names></name><name><surname>Kurniawan</surname> <given-names>S</given-names></name></person-group><source>Assistive Technology for Blindness and Low Vision</source><publisher-loc>Boca Raton, FL</publisher-loc><publisher-name>CRC</publisher-name><fpage>161</fpage><lpage>191</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>YH</given-names></name><name><surname>da Cruz</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The argus II retinal prosthesis system</article-title><source>Progress in Retinal and Eye Research</source><volume>50</volume><fpage>89</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.preteyeres.2015.09.003</pub-id><pub-id pub-id-type="pmid">26404104</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maidenbaum</surname> <given-names>S</given-names></name><name><surname>Abboud</surname> <given-names>S</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sensory substitution: closing the gap between basic research and widespread practical visual rehabilitation</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>41</volume><fpage>3</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2013.11.007</pub-id><pub-id pub-id-type="pmid">24275274</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision: A Computational Investigation Into the Human Representation and Processing of Visual Information</source><publisher-loc>New York</publisher-loc><publisher-name>Henry Holt and Co</publisher-name></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAnally</surname> <given-names>KI</given-names></name><name><surname>Martin</surname> <given-names>RL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sound localization with head movement: implications for 3-d audio displays</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>210</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00210</pub-id><pub-id pub-id-type="pmid">25161605</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijer</surname> <given-names>PB</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>An experimental system for auditory image representations</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>39</volume><fpage>112</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1109/10.121642</pub-id><pub-id pub-id-type="pmid">1612614</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pitkow</surname> <given-names>X</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Neural computation in sensory systems</chapter-title><person-group person-group-type="editor"><name><surname>Gazzaniga</surname> <given-names>M. S</given-names></name><name><surname>Mangun</surname> <given-names>G. R</given-names></name></person-group><source>The Cognitive Neurosciences</source><edition>fifth edition</edition><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>305</fpage><lpage>318</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proulx</surname> <given-names>MJ</given-names></name><name><surname>Gwinnutt</surname> <given-names>J</given-names></name><name><surname>Dell'Erba</surname> <given-names>S</given-names></name><name><surname>Levy-Tzedek</surname> <given-names>S</given-names></name><name><surname>de Sousa</surname> <given-names>AA</given-names></name><name><surname>Brown</surname> <given-names>DJ</given-names></name><name><surname>Sousa</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Other ways of seeing: From behavior to neural mechanisms in the online &quot;visual&quot; control of action with sensory substitution</article-title><source>Restorative Neurology and Neuroscience</source><volume>34</volume><fpage>29</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.3233/RNN-150541</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Redmon</surname> <given-names>J</given-names></name><name><surname>Farhadi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>YOLOv3: An Incremental Improvement</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1804.02767">http://arxiv.org/abs/1804.02767</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ribeiro</surname> <given-names>F</given-names></name><name><surname>FlorÃªncio</surname> <given-names>D</given-names></name><name><surname>Chou</surname> <given-names>PA</given-names></name><name><surname>Zhang</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Auditory augmented reality: Object sonification for the visually impaired</article-title><conf-name>In 2012 IEEE 14th International Workshop on Multimedia Signal Processing (MMSP)</conf-name><fpage>319</fpage><lpage>324</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roentgen</surname> <given-names>UR</given-names></name><name><surname>Gelderblom</surname> <given-names>GJ</given-names></name><name><surname>Soede</surname> <given-names>M</given-names></name><name><surname>de Witte</surname> <given-names>LP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Inventory of electronic mobility aids for persons with visual impairments: a literature review</article-title><source>Journal of Visual Impairment &amp; Blindness</source><volume>102</volume><fpage>702</fpage><lpage>724</lpage></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname> <given-names>O</given-names></name><name><surname>Deng</surname> <given-names>J</given-names></name><name><surname>Su</surname> <given-names>H</given-names></name><name><surname>Krause</surname> <given-names>J</given-names></name><name><surname>Satheesh</surname> <given-names>S</given-names></name><name><surname>Ma</surname> <given-names>S</given-names></name><name><surname>Huang</surname> <given-names>Z</given-names></name><name><surname>Karpathy</surname> <given-names>A</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Bernstein</surname> <given-names>M</given-names></name><name><surname>Berg</surname> <given-names>AC</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ImageNet large scale visual recognition challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholl</surname> <given-names>HP</given-names></name><name><surname>Strauss</surname> <given-names>RW</given-names></name><name><surname>Singh</surname> <given-names>MS</given-names></name><name><surname>Dalkara</surname> <given-names>D</given-names></name><name><surname>Roska</surname> <given-names>B</given-names></name><name><surname>Picaud</surname> <given-names>S</given-names></name><name><surname>Sahel</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Emerging therapies for inherited retinal degeneration</article-title><source>Science Translational Medicine</source><volume>8</volume><elocation-id>368rv6</elocation-id><pub-id pub-id-type="doi">10.1126/scitranslmed.aaf2838</pub-id><pub-id pub-id-type="pmid">27928030</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Seeing With Sound</collab></person-group><year iso-8601-date="2018">2018</year><article-title>Seeing With Sound</article-title><ext-link ext-link-type="uri" xlink:href="https://www.seeingwithsound.com">https://www.seeingwithsound.com</ext-link><date-in-citation iso-8601-date="2018-08-21">August 21, 2018</date-in-citation></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spagnol</surname> <given-names>S</given-names></name><name><surname>WersÃ©nyi</surname> <given-names>G</given-names></name><name><surname>Bujacz</surname> <given-names>M</given-names></name><name><surname>BÄlan</surname> <given-names>O</given-names></name><name><surname>Herrera MartÃ­nez</surname> <given-names>M</given-names></name><name><surname>Moldoveanu</surname> <given-names>A</given-names></name><name><surname>Unnthorsson</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Current use and future perspectives of spatial audio technologies in electronic travel aids</article-title><source>Wireless Communications and Mobile Computing</source><volume>2018</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1155/2018/3918284</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stingl</surname> <given-names>K</given-names></name><name><surname>Schippert</surname> <given-names>R</given-names></name><name><surname>Bartz-Schmidt</surname> <given-names>KU</given-names></name><name><surname>Besch</surname> <given-names>D</given-names></name><name><surname>Cottriall</surname> <given-names>CL</given-names></name><name><surname>Edwards</surname> <given-names>TL</given-names></name><name><surname>Gekeler</surname> <given-names>F</given-names></name><name><surname>Greppmaier</surname> <given-names>U</given-names></name><name><surname>Kiel</surname> <given-names>K</given-names></name><name><surname>Koitschev</surname> <given-names>A</given-names></name><name><surname>KÃ¼hlewein</surname> <given-names>L</given-names></name><name><surname>MacLaren</surname> <given-names>RE</given-names></name><name><surname>Ramsden</surname> <given-names>JD</given-names></name><name><surname>Roider</surname> <given-names>J</given-names></name><name><surname>Rothermel</surname> <given-names>A</given-names></name><name><surname>Sachs</surname> <given-names>H</given-names></name><name><surname>SchrÃ¶der</surname> <given-names>GS</given-names></name><name><surname>Tode</surname> <given-names>J</given-names></name><name><surname>Troelenberg</surname> <given-names>N</given-names></name><name><surname>Zrenner</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Interim results of a multicenter trial with the new electronic subretinal implant alpha AMS in 15 patients blind from inherited retinal degenerations</article-title><source>Frontiers in Neuroscience</source><volume>11</volume><pub-id pub-id-type="doi">10.3389/fnins.2017.00445</pub-id><pub-id pub-id-type="pmid">28878616</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stingl</surname> <given-names>K</given-names></name><name><surname>Zrenner</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Electronic approaches to restitute vision in patients with neurodegenerative diseases of the retina</article-title><source>Ophthalmic Research</source><volume>50</volume><fpage>215</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1159/000354424</pub-id><pub-id pub-id-type="pmid">24081198</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Striem-Amit</surname> <given-names>E</given-names></name><name><surname>Guendelman</surname> <given-names>M</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>'Visual' acuity of the congenitally blind using visual-to-auditory sensory substitution</article-title><source>PLoS ONE</source><volume>7</volume><elocation-id>e33136</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0033136</pub-id><pub-id pub-id-type="pmid">22438894</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stronks</surname> <given-names>HC</given-names></name><name><surname>Mitchell</surname> <given-names>EB</given-names></name><name><surname>Nau</surname> <given-names>AC</given-names></name><name><surname>Barnes</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual task performance in the blind with the BrainPort V100 vision aid</article-title><source>Expert Review of Medical Devices</source><volume>13</volume><fpage>919</fpage><lpage>931</lpage><pub-id pub-id-type="doi">10.1080/17434440.2016.1237287</pub-id><pub-id pub-id-type="pmid">27633972</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sudol</surname> <given-names>J</given-names></name><name><surname>Dialameh</surname> <given-names>O</given-names></name><name><surname>Blanchard</surname> <given-names>C</given-names></name><name><surname>Dorcey</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Looktel - A comprehensive platform for computer-aided visual assistance</article-title><conf-name>2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops</conf-name><fpage>73</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1109/CVPRW.2010.5543725</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verschae</surname> <given-names>R</given-names></name><name><surname>Ruiz-del-Solar</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Object detection: current and future directions</article-title><source>Frontiers in Robotics and AI</source><volume>2</volume><pub-id pub-id-type="doi">10.3389/frobt.2015.00029</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiland</surname> <given-names>JD</given-names></name><name><surname>Humayun</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Retinal prosthesis</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>61</volume><fpage>1412</fpage><lpage>1424</lpage><pub-id pub-id-type="doi">10.1109/TBME.2014.2314733</pub-id><pub-id pub-id-type="pmid">24710817</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wenzel</surname> <given-names>EM</given-names></name><name><surname>Arruda</surname> <given-names>M</given-names></name><name><surname>Kistler</surname> <given-names>DJ</given-names></name><name><surname>Wightman</surname> <given-names>FL</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Localization using nonindividualized head-related transfer functions</article-title><source>The Journal of the Acoustical Society of America</source><volume>94</volume><fpage>111</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1121/1.407089</pub-id><pub-id pub-id-type="pmid">8354753</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Wikipedia</collab></person-group><year iso-8601-date="2018">2018</year><article-title>HTC Vive</article-title><source>Wikipedia</source><ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/HTC_Vive">https://en.wikipedia.org/wiki/HTC_Vive</ext-link><date-in-citation iso-8601-date="2018-08-21">August 21, 2018</date-in-citation></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.37841.021</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution>University of Washington</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>[Editorsâ note: a previous version of this study was rejected after peer review, but the authors submitted for reconsideration. The first decision letter after peer review is shown below.]</p><p>Thank you for submitting your work entitled &quot;Augmented Reality Powers a Cognitive Prosthesis for the Blind&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by a Senior Editor.</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in <italic>eLife</italic>.</p><p>Two main concerns emerged in the reviews and consultation among reviewers. The first regards overlap of the proposed system with other similar approaches. This overlap is not discussed sufficiently in the paper to evaluate how much of an advance the current system offers. The second is that the paper does not introduce a set of benchmarks that could be used to compare different systems and describing the limitations of the current system in more detail.</p><p><italic>Reviewer #1:</italic> </p><p>This paper presents an interesting approach to help restore independence to people with significant visual impairments. The approach centers around proving auditory guidance about the location of objects in an environment, or about a desired path through an environment. The work appears well done and the paper is clearly written. My main concerns have to do with novelty.</p><p>The Discussion states that the main contribution of this paper is to show that devices such as the one proposed can provide a substrate for many visual tasks, while previous work has tended to focus on specific tasks. I am not an expert in this area, but a cursory look through the literature brings up several papers using quite similar technology (e.g. using the Kinect system and auditory feedback for avoiding obstacles â Saputra et al., 2014). I think the impact of the present work would be enhanced considerably if more visual tasks were probed and the abilities and limitations of this approach were laid out in more detail. This would not necessarily require visually impaired subjects. Examples might include identifying objects in cluttered environments, navigating a cluttered environment, reaching or grasping an object. In many ways, the tasks probed in the paper were too easy for the subjects and do not define the capabilities of the approach. Are there a few tasks that could be standardized and then used to compare different approaches? Several of the tasks in the paper get close to this, but both a larger complement of tasks and clearer standardization of task conditions might serve to set a clearer performance standard.</p><p><italic>Reviewer #2:</italic> </p><p>The study describes a system for sensory substitution: auditory guidance for the blind. It is erroneously called prosthesis, even though instead of restoring sight it just replaces it with the auditory guidance. The term &quot;prosthesis&quot; should be replaced with &quot;auditory substitute&quot; or something along these lines, throughout the manuscript.</p><p>&quot;Here we present such an approach that bypasses the need to convey the sensory data entirely, and focuses instead on the important high-level knowledge, presented at a comfortable data rate and in an intuitive format.&quot;</p><p>Sensory substitution and augmentation certainly has its utility, but is unlikely to replace the richness of the visual input (if it could be restored).</p><p>The manuscript does not mention a related system already on the market, called Orcam. It has a subset of features mentioned in this manuscript, and others could be added.</p><p>The manuscript also does not cite a previous study about auditory guidance for the blind â &quot;Assessment of feedback modalities for wearable visual aids in blind mobility&quot;, by A. Adebiyi et al., 2017. Nor does it mention the whole set of studies about auditory encoding of the space from A. Amedi's lab: http://brain.huji.ac.il/publications.asp</p><p><italic>Reviewer #3:</italic> </p><p>The study by Meister and colleagues proposes a novel approach to a visual prosthesis, by substituting objects in the environment with spatially localized voices. They demonstrate the utility of this in a set of simplified scenarios, showing that the user can identify, localize, and navigate based on these localized voice cues. The study does not address the (probably more challenging) issue of converting a visual scene into voice cues.</p><p>This is a clever idea and is likely to have a significant impact, particularly because this is a technology that can be implemented imminently, safely, and relatively cheaply, as opposed to approaches such as retinal prostheses or gene therapies. My comments are centered around clarifying exactly what has been achieved in the current study. These are relatively minor comments for what is overall an intriguing study.</p><p>Primarily this centers around the claim in the Abstract which states &quot;A wearable computer captures video and other data, extracts the important scene knowledge, and conveys that.â¦&quot; Yet the experiments presented do not use the video to generate the cues, they are either virtual objects or a precomputed path through a building. Only in the Discussion (Technical extensions) do they note that the first half of the problem has not yet been solved. This should be corrected in the abstract and noted explicitly early on, as well as the fact that tests were performed with virtual objects. It is also not clear at many points in the text that &quot;targets are placed&quot; means virtual, rather than physical targets.</p><p>Likewise, the prosthesis is described as conveying information at the cognitive level, however most of what they present is better described as a hybrid of sensory substitution with cognitive labels, as it is largely dependent on auditory spatial localization. Indeed, their first experiment is simply confirming that a subject can point towards a synthesized auditory stimulus. The only aspects where the language component really comes into play is the spatial memory of 5 objects, or warning signals for navigation. It would be helpful to explicitly point out the key role auditory spatial localization plays.</p><p>Finally, a major challenge that should be at least discussed (or even better, tested) is that real scenes do not have just 1-5 isolated objects, but actually many more. I can imagine ways that this could be addressed, but it would be helpful to be explicit about this and other challenges in moving from simple virtual environments to the real world.</p><p>[Editorsâ note: what now follows is the decision letter after the authors submitted for further consideration.]</p><p>Thank you for submitting your article &quot;Augmented Reality Powers a Cognitive Prosthesis for the Blind&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Fred Rieke as the Reviewing Editor, and the evaluation has been overseen by Eve Marder as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Cristopher M Niell.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>This is a revision of a paper investigating the capabilities of a 'cognitive prosthetic' to help the blind explore their environment. All of the reviewers agreed that the paper had improved considerably. Specifically, the authors have done a good job of answering all of the comments from the previous reviews, and in particular being clear about what their system achieves and its relationship with previous work. The new benchmarking system, and inclusion of a cluttered scene, provides a more stringent test of the device's capabilities and substantially enhances the paper. The reviewers noted a few areas in which the paper could be further improved:</p><p>Description of the device: The authors addressed concerns with review of the prior articles, but their description of the system as a &quot;prosthesis&quot; is still a concern.</p><p>1) In their response letter, the authors state: &quot;Our device can restore the main function of sight (&quot;knowing what is where by looking&quot;) although it does not emulate what sight used to feel like. Similarly, a prosthetic leg restores the function of walking without emulating what the leg used to feel like.&quot;</p><p>Functionally, prosthetic replacement of a leg is much more similar to the original than vision being replaced with auditory explanation of the scene. Replacement of the missing legs with a wheelchair, which provides mobility but through a different mechanism, is a better analogy. And again, nobody calls a wheelchair a prosthesis.</p><p>2) Regarding the title: &quot;cognitive prosthesis&quot; sounds very odd for such a device. For example, many websites and e-book readers provide an audio option, which certainly enables blind people to read, but nobody calls such a reader a &quot;cognitive prosthesis&quot;. We suggest replacing &quot;prosthesis&quot; with &quot;assistant&quot;, as defined in the name CARA: &quot;cognitive augmented reality assistant for the blind&quot;.</p><p>3) &quot;Visual prosthesis&quot; is mentioned again at the end of the Abstract, again please replace with a term that is more in keeping with what you have actually done.</p><p>Benchmarking: Performance on the benchmarking tasks in Figure 5 with CARA is very good. Did you in any cases increase the task difficulty to see where performance degraded? One specific task that might be interesting is gradually increasing the number of objects in the environment to see at what point and how performance degrades. More broadly, some discussion of what is challenging with the device from any of the users would be welcome.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.37841.022</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editorsâ note: the author responses to the first round of peer review follow.]</p><disp-quote content-type="editor-comment"><p>Two main concerns emerged in the reviews and consultation among reviewers. The first regards overlap of the proposed system with other similar approaches. This overlap is not discussed sufficiently in the paper to evaluate how much of an advance the current system offers.</p></disp-quote><p>The revised manuscript offers more on this. Related approaches are discussed in the Introduction. Incorporation of other technical developments in the subsection âTechnical extensionsâ. Sensory substitution by vOICe in the subsection âAn open-source benchmarking environment for assistive devicesâ. Other uses of spatialized sound in the Discussion.</p><disp-quote content-type="editor-comment"><p>The second is that the paper does not introduce a set of benchmarks that could be used to compare different systems and describing the limitations of the current system in more detail.</p></disp-quote><p>We have added a new section that introduces a system for benchmarking, allowing researchers anywhere to compare different devices on the same tasks (subsections âAn open-source benchmarking environment for assistive devicesâ and âBenchmarking platform using virtual realityâ). We illustrate its use with a side-by-side comparison of two very different approaches to a visual prosthesis, including new experiments on human subjects (see new Figure 5 and Figure 5âfigure supplement 1). We also published on GitHub all the specs and code needed to implement the benchmarking system.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>This paper presents an interesting approach to help restore independence to people with significant visual impairments. The approach centers around proving auditory guidance about the location of objects in an environment, or about a desired path through an environment. The work appears well done and the paper is clearly written. My main concerns have to do with novelty.</p><p>The Discussion states that the main contribution of this paper is to show that devices such as the one proposed can provide a substrate for many visual tasks, while previous work has tended to focus on specific tasks. I am not an expert in this area, but a cursory look through the literature brings up several papers using quite similar technology (e.g. using the Kinect system and auditory feedback for avoiding obstacles â Saputra et al., 2014).</p></disp-quote><p>This example reinforces our claim regarding the integration of many visual functions. That article (with zero citations in Web of Science) only addresses obstacle avoidance. No localization. No scene representation. No guidance. No long-range navigation.</p><disp-quote content-type="editor-comment"><p>I think the impact of the present work would be enhanced considerably if more visual tasks were probed and the abilities and limitations of this approach were laid out in more detail. This would not necessarily require visually impaired subjects. Examples might include identifying objects in cluttered environments, navigating a cluttered environment, reaching or grasping an object.</p></disp-quote><p>Following this suggestion, we built a benchmarking environment and illustrated it with performance tests that cover exactly these three cases (subsections âAn open-source benchmarking environment for assistive devicesâ and âBenchmarking platform using virtual realityâ, Figure 5 and Figure 5âfigure supplement 1). We performed these tests with two different assistive devices using sighted but blindfolded subjects.</p><disp-quote content-type="editor-comment"><p>In many ways, the tasks probed in the paper were too easy for the subjects and do not define the capabilities of the approach.</p></disp-quote><p>Our blind subjects would not agree; they were surprised how well they did given the challenges, such as crossing a large open space and climbing stairs.</p><disp-quote content-type="editor-comment"><p>Are there a few tasks that could be standardized and then used to compare different approaches? Several of the tasks in the paper get close to this, but both a larger complement of tasks and clearer standardization of task conditions might serve to set a clearer performance standard.</p></disp-quote><p>This suggestion has been implemented in the new section on benchmarking (subsections âAn open-source benchmarking environment for assistive devicesâ and âBenchmarking platform using virtual realityâ, Figure 5 and Figure 5âfigure supplement 1).</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The study describes a system for sensory substitution: auditory guidance for the blind. It is erroneously called prosthesis, even though instead of restoring sight it just replaces it with the auditory guidance.</p></disp-quote><p>Our device can restore the main function of sight (âknowing what is where by lookingâ) although it does not emulate what sight used to feel like. Similarly a prosthetic leg restores the function of walking without emulating what the leg used to feel like.</p><disp-quote content-type="editor-comment"><p>The term &quot;prosthesis&quot; should be replaced with &quot;auditory substitute&quot; or something along these lines, throughout the manuscript.</p></disp-quote><p>We have revised the text to use the term âcognitive prosthesisâ. This appears in the Oxford English Dictionary as âan electronic computational device that extends the capability of human cognition or sense perceptionâ, exactly what our system aims for. The term âvisual prosthesisâ appears once when citing Collins (1985) who used it liberally for systems of related design.</p><disp-quote content-type="editor-comment"><p>&quot;Here we present such an approach that bypasses the need to convey the sensory data entirely, and focuses instead on the important high-level knowledge, presented at a comfortable data rate and in an intuitive format.&quot;</p><p>Sensory substitution and augmentation certainly has its utility, but is unlikely to replace the richness of the visual input (if it could be restored).</p></disp-quote><p>It is conceivable that the extended use of such a cognitive prosthesis for a broad range of functions eventually produces a new type of subjective qualia that resemble the feeling of seeing. The Discussion mentions this briefly.</p><disp-quote content-type="editor-comment"><p>The manuscript does not mention a related system already on the market, called Orcam. It has a subset of features mentioned in this manuscript, and others could be added.</p></disp-quote><p>Orcam is another single-feature gadget: a camera attached to an eye frame that can read text and money out loud if you point it in the right direction. No localization. No scene analysis. No obstacle detection. No navigation. There are dozens more of these apps and gadgets, and we cite reviews that cover them (Bujacz and Strumillo, 2016; Jafri et al., 2014; Karimi, 2015; Roentgen et al., 2008).</p><disp-quote content-type="editor-comment"><p>The manuscript also does not cite a previous study about auditory guidance for the blind â &quot;Assessment of feedback modalities for wearable visual aids in blind mobility&quot;, by A. Adebiyi et al., 2017.</p></disp-quote><p>Actually this was the first article on our list of references.</p><disp-quote content-type="editor-comment"><p>Nor does it mention the whole set of studies about auditory encoding of the space from A. Amedi's lab: http://brain.huji.ac.il/publications.asp</p></disp-quote><p>In fact we did cite Striem-Amit et al., 2012, which in turn reviews a good number of other papers from Amediâs group. We have now added another citation to Maidenbaum, Abboud and Amedi, 2014.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>[â¦]</p><p>Primarily this centers around the claim in the Abstract which states &quot;A wearable computer captures video and other data, extracts the important scene knowledge, and conveys that.â¦&quot; Yet the experiments presented do not use the video to generate the cues, they are either virtual objects or a precomputed path through a building. Only in the Discussion (Technical extensions) do they note that the first half of the problem has not yet been solved. This should be corrected in the abstract and noted explicitly early on, as well as the fact that tests were performed with virtual objects. It is also not clear at many points in the text that &quot;targets are placed&quot; means virtual, rather than physical targets.</p></disp-quote><p>The revised manuscript clarifies this and also points to tantalizing recent developments towards real-time object identification in video feed (see Figure 3âfigure supplement 2F). We expect that this will lead to a complete front end solution for object acquisition within a year. Meanwhile we want to publish our solution for the back end (communication with the user) because that can be combined with any number of front end solutions. By publishing the human interface now, and drawing attention to the powerful platform of the HoloLens, we enable others whose talents and expertise are on the front end.</p><disp-quote content-type="editor-comment"><p>Likewise, the prosthesis is described as conveying information at the cognitive level, however most of what they present is better described as a hybrid of sensory substitution with cognitive labels, as it is largely dependent on auditory spatial localization. Indeed, their first experiment is simply confirming that a subject can point towards a synthesized auditory stimulus. The only aspects where the language component really comes into play is the spatial memory of 5 objects, or warning signals for navigation. It would be helpful to explicitly point out the key role auditory spatial localization plays.</p></disp-quote><p>Yes, the reason subjects adopt this system so easily is peopleâs effortless ability to localize sounds combined with the high cognitive content of language labels.</p><disp-quote content-type="editor-comment"><p>Finally, a major challenge that should be at least discussed (or, even better, tested) is that real scenes do not have just 1-5 isolated objects, but actually many more. I can imagine ways that this could be addressed, but it would be helpful to be explicit about this and other challenges in moving from simple virtual environments to the real world.</p></disp-quote><p>The new manuscript section on standardized benchmarking includes tests in a room with many more objects (see Figure 5A). These tests also make more obvious use of the language labels. Within the benchmarking system that we published one can design environments of arbitrary visual complexity.</p><p>[Editorsâ note: the author responses to the re-review follow.]</p><disp-quote content-type="editor-comment"><p>The reviewers noted a few areas in which the paper could be further improved:</p><p>Description of the device: The authors addressed concerns with review of the prior articles, but their description of the system as a &quot;prosthesis&quot; is still a concern.</p><p>1) In their response letter, the authors state: &quot;Our device can restore the main function of sight (&quot;knowing what is where by looking&quot;) although it does not emulate what sight used to feel like. Similarly, a prosthetic leg restores the function of walking without emulating what the leg used to feel like.&quot;</p><p>Functionally, prosthetic replacement of a leg is much more similar to the original than vision being replaced with auditory explanation of the scene. Replacement of the missing legs with a wheelchair, which provides mobility but through a different mechanism, is a better analogy. And again, nobody calls a wheelchair a prosthesis.</p><p>2) Regarding the title: &quot;cognitive prosthesis&quot; sounds very odd for such a device. For example, many websites and e-book readers provide an audio option, which certainly enables blind people to read, but nobody calls such a reader a &quot;cognitive prosthesis&quot;. We suggest replacing &quot;prosthesis&quot; with &quot;assistant&quot;, as defined in the name CARA: &quot;cognitive augmented reality assistant for the blind&quot;.</p><p>3) &quot;Visual prosthesis&quot; is mentioned again at the end of the Abstract, again please replace with a term that is more in keeping with what you have actually done.</p></disp-quote><p>We have replaced âprosthesisâ with âassistantâ throughout, except where we refer to an invasive device or where we quote another author.</p><disp-quote content-type="editor-comment"><p>Benchmarking: Performance on the benchmarking tasks in Figure 5 with CARA is very good. Did you in any cases increase the task difficulty to see where performance degraded? One specific task that might be interesting is gradually increasing the number of objects in the environment to see at what point and how performance degrades.</p></disp-quote><p>We have not tried to push the limits of clutter in the environment. This would become more informative once there is a competing technology with performance similar to CARA. In the current version, âspotlight modeâ activates objects inside a 30 deg cone. If the user wanted to search the space more finely that parameter could be adjusted; of course this could also be done dynamically during the search.</p><disp-quote content-type="editor-comment"><p>More broadly, some discussion of what is challenging with the device from any of the users would be welcome.</p></disp-quote><p>A supplementary file contains comments from our users regarding perceived challenges and desired features. The revised manuscript has an explicit pointer to this supplement.</p></body></sub-article></article>