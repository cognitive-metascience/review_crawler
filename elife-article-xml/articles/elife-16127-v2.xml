<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">16127</article-id><article-id pub-id-type="doi">10.7554/eLife.16127</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>A Bayesian model of context-sensitive value attribution</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-53659"><name><surname>Rigoli</surname><given-names>Francesco</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2233-934X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-11070"><name><surname>Friston</surname><given-names>Karl J</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7984-8909</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-55033"><name><surname>Martinelli</surname><given-names>Cristina</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-59250"><name><surname>Selaković</surname><given-names>Mirjana</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-55035"><name><surname>Shergill</surname><given-names>Sukhwinder S</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-19714"><name><surname>Dolan</surname><given-names>Raymond J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">The Wellcome Trust Centre for Neuroimaging</institution>, <institution>University College London</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychosis Studies, Institute of Psychiatry, Psychology and Neuroscience</institution>, <institution>King's College London</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Psychiatry</institution>, <institution>Sismanoglio General Hospital</institution>, <addr-line><named-content content-type="city">Athens</named-content></addr-line>, <country>Greece</country></aff><aff id="aff4"><label>4</label><institution>Max Planck UCL Centre for Computational Psychiatry and Ageing Research</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gershman</surname><given-names>Sam</given-names></name><aff id="aff5"><institution>Harvard University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>f.rigoli@ucl.ac.uk</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>22</day><month>06</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e16127</elocation-id><history><date date-type="received"><day>16</day><month>03</month><year>2016</year></date><date date-type="accepted"><day>16</day><month>06</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Rigoli et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Rigoli et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-16127-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.16127.001</object-id><p>Substantial evidence indicates that incentive value depends on an anticipation of rewards within a given context. However, the computations underlying this context sensitivity remain unknown. To address this question, we introduce a normative (Bayesian) account of how rewards map to incentive values. This assumes that the brain inverts a model of how rewards are generated. Key features of our account include (i) an influence of prior beliefs about the context in which rewards are delivered (weighted by their reliability in a Bayes-optimal fashion), (ii) the notion that incentive values correspond to precision-weighted prediction errors, (iii) and contextual information unfolding at different hierarchical levels. This formulation implies that incentive value is intrinsically context-dependent. We provide empirical support for this model by showing that incentive value is influenced by context variability and by hierarchically nested contexts. The perspective we introduce generates new empirical predictions that might help explaining psychopathologies, such as addiction.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.001">http://dx.doi.org/10.7554/eLife.16127.001</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>incentive value</kwd><kwd>context influence</kwd><kwd>choice</kwd><kwd>Bayesian</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>098362/Z/12/Z</award-id><principal-award-recipient><name><surname>Rigoli</surname><given-names>Francesco</given-names></name><name><surname>Friston</surname><given-names>Karl J</given-names></name><name><surname>Dolan</surname><given-names>Raymond J</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Shergill</surname><given-names>Sukhwinder S</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>NIHR-BRC</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Martinelli</surname><given-names>Cristina</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The dependence of incentive value attribution on the anticipation of rewards within a given context is explained via a normative Bayesian account of how rewards map to incentive values.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Choice preferences vary as a function of context, and recent studies have shed light on the processes underlying these contextual influences (<xref ref-type="bibr" rid="bib27">Huber et al., 1982</xref>; <xref ref-type="bibr" rid="bib29">Johnson and Busemeyer, 2005</xref>; <xref ref-type="bibr" rid="bib39">Ludvig et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">Louie et al., 2013</xref>, <xref ref-type="bibr" rid="bib38">2014</xref>, <xref ref-type="bibr" rid="bib35">2015</xref>; <xref ref-type="bibr" rid="bib56">Roe et al., 2011</xref>; <xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>; <xref ref-type="bibr" rid="bib57">Simonson and Tversky, 1992</xref>; <xref ref-type="bibr" rid="bib58">Soltani et al., 2012</xref>; <xref ref-type="bibr" rid="bib62">Stewart, 2009</xref>; <xref ref-type="bibr" rid="bib61">Stewart et al., 2003</xref>; <xref ref-type="bibr" rid="bib63">Summerfield and Tsetsos, 2012</xref>, <xref ref-type="bibr" rid="bib64">2015</xref>; <xref ref-type="bibr" rid="bib69">Tsetsos et al., 2010</xref>, <xref ref-type="bibr" rid="bib67">2012</xref>, <xref ref-type="bibr" rid="bib68">2016</xref>; <xref ref-type="bibr" rid="bib70">Tversky, 1972</xref>; <xref ref-type="bibr" rid="bib71">Usher and McClelland, 2004</xref>; <xref ref-type="bibr" rid="bib72">Vlaev et al., 2012</xref>). For example, in (<xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>) participants performed a task where blocks of trials were associated with either a low or a high-value context (with overlapping distributions). Choice behaviour was consistent with the hypothesis that the incentive values of identical rewards were <italic>larger</italic> in the <italic>low</italic> compared to the high-value context. This and similar evidence suggests that, at least in some cases, contextual effects on choice behaviour are explained by an incentive value that reflects the <italic>relative</italic> value of rewards anticipated within a given context (<xref ref-type="bibr" rid="bib37">Louie et al., 2013</xref>; <xref ref-type="bibr" rid="bib39">Ludvig et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>; <xref ref-type="bibr" rid="bib62">Stewart, 2009</xref>; <xref ref-type="bibr" rid="bib61">Stewart et al., 2003</xref>).</p><p>However, the computational mechanisms underlying the context sensitive nature of incentive value remain unclear. A promising explanatory framework builds on the notion that the brain’s computations correspond to Bayesian inference and learning. Several empirical and theoretical arguments support a Bayesian inference as a general account of brain function (<xref ref-type="bibr" rid="bib10">Chater et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib17">Dayan et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Ernst, 2006</xref>; <xref ref-type="bibr" rid="bib24">Friston, 2010</xref>). The application of similar principles to value learning and planning has inspired the notions of planning as inference and active inference (<xref ref-type="bibr" rid="bib6">Botvinick and Toussaint, 2012</xref>; <xref ref-type="bibr" rid="bib22">Friston et al., 2013</xref>, <xref ref-type="bibr" rid="bib21">2015</xref>; <xref ref-type="bibr" rid="bib47">Pezzulo and Rigoli, 2011</xref>; <xref ref-type="bibr" rid="bib46">Pezzulo et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Solway and Botvinick, 2012</xref>). Here, we consider the possibility that the context-sensitive value is a product of Bayesian inference. This implies that incentive value will depend on expectation and uncertainty about rewards, conditioned upon contextual factors. If this is the case, we would expect to see choice behaviour change with any contextual variable that is an <italic>ancestor</italic> of rewards in the subject’s generative model of these rewards.</p><p>We refer to our account as a Bayesian model of context-sensitive value (BCV). Below, we introduce the model and compare it with previous accounts of contextual influence on incentive value and choice (Bushong et al., unpublished; <xref ref-type="bibr" rid="bib33">Kőszegi and Rabin, 2006</xref>; <xref ref-type="bibr" rid="bib34">Kőszegi and Szeidl, 2013</xref>; <xref ref-type="bibr" rid="bib38">Louie et al., 2014</xref>, <xref ref-type="bibr" rid="bib35">2015</xref>). We then report data from two behavioural experiments where we analysed two key predictions of BCV.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Bayesian account of context-sensitive value</title><p>The general framework of BCV is Bayesian, building on a proposal that the brain performs some form of Bayesian inference (<xref ref-type="bibr" rid="bib10">Chater et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib17">Dayan et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Ernst, 2006</xref>; <xref ref-type="bibr" rid="bib24">Friston, 2010</xref>). This approach considers the brain to possess a generative model of the sensorium, comprising a set of random variables (i.e., hidden states or causes of sensory outcomes) and their causal links (i.e., probabilistic contingencies). The variables can be separated into hidden and observable variables; the former representing the latent causes of observations, and the latter representing sensory evidence or cues. Sensory evidence is conveyed by observable variables, and this evidence is combined with prior beliefs to produce a posterior belief about the (hidden) causes of observations. The application of this logic to perception is straightforward and has proved effective in explaining several empirical phenomena in perception (<xref ref-type="bibr" rid="bib10">Chater et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib17">Dayan et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Ernst, 2006</xref>; <xref ref-type="bibr" rid="bib24">Friston, 2010</xref>). For instance, there is evidence for integrating different perceptual modalities (e.g., visual and tactile) in a manner consistent with Bayesian principles (<xref ref-type="bibr" rid="bib19">Ernst, 2006</xref>).</p><p>We propose a Bayesian scheme for BCV that accommodates the influence of context on incentive value. BCV focuses on scenarios (i) where incentive value depends on contextual information (either represented by cues or by previous rewards) provided before options or rewards are presented, and (ii) where reward is defined by a single attribute (e.g., reward amount). To describe the basic principles of BCV, we adopt the formalism of Bayesian graphs (<xref ref-type="bibr" rid="bib4">Bishop, 2006</xref>) where a generative model is described by nodes or circles, representing random variables (shaded and white circles refer to observed and non-observed variables respectively), and arrows, representing causal relationships among variables. A simple generative model hypothesized by BCV is shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>, where C represents prior beliefs about the average reward expected in a given context. Formally, this corresponds to a (Gaussian) prior belief (with mean <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> over the mean of a (Gaussian) distribution of reward options R (with variance <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>). When R is observed, a posterior expectation about the context is obtained by application of Bayes rule (<xref ref-type="bibr" rid="bib4">Bishop, 2006</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.16127.002</object-id><label>Figure 1.</label><caption><title>Generative models of reward: these generative models are depicted as directed acyclic graphs or Bayesian networks.</title><p>Circles represent random variables (shaded and white circles refer to observed and non-observed variables respectively). An arrow denotes a conditional dependence – in which one random variable supplies a sufficient statistic of the probability distribution of its children. In BCV, contextual variables generate the sufficient statistics (expectation and variance) of a Gaussian observable variable corresponding to reward. In these examples, the contextual variables generate first-order sufficient statistics of their descendants (e.g., the mean of Gaussian distributions) as in parametric or empirical Bayesian models. Alternatively, the contextual variables could determine the variance of Gaussian random variables; in which case this would be a hierarchical Gaussian filter. Inverting this model, given observations, furnishes posterior beliefs over the context variables. This inference determines incentive value which is conceived as precision-weighted prediction error. (<bold>A</bold>) Generative model where a contextual variable C reflects a prior belief over the reward mean. (<bold>B</bold>) Generative model where a contextual variable C generates a prior expectancy of zero over the reward mean, and a noisy observation O of the context is provided. (<bold>C</bold>) Generative model where context is organized hierarchically and comprises a high level (HC; e.g., a neighbourhood) and a low level (LC; e.g., a restaurant), both associated with noisy observations (HO and LO respectively).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.002">http://dx.doi.org/10.7554/eLife.16127.002</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16127-fig1-v2.tif"/></fig></p><p>The crucial proposal we advance here is that the incentive value V(R) attributed to a certain reward option is embedded within this belief update process and corresponds to a precision-weighted prediction error (<xref ref-type="bibr" rid="bib23">Friston, 2005</xref>); namely, to the difference between R and the expected reward μ<sub>C,</sub> multiplied by a gain term, which depends on the variances of both reward and context (i.e., relative confidence or precision):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The notion that incentive value corresponds to precision-weighted prediction error links with ideas in other cognitive domains proposing that prior expectations are explained away and perception corresponds to (precision-weighted) residuals, or prediction errors (<xref ref-type="bibr" rid="bib5">Blakemore et al., 1999</xref>; <xref ref-type="bibr" rid="bib7">Brown et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib49">Rao and Ballard, 1999</xref>). For example, it has been proposed that our sense of agency emerges from explaining away action-dependent somatosensory predictions, and hence what is perceived as externally generated sensation corresponds to (precision-weighted) residuals of the sensory input (<xref ref-type="bibr" rid="bib7">Brown et al., 2013</xref>).</p><p>Here, we propose that a similar mechanism is involved in attribution of incentive value. This implies two fundamental forms of contextual normalization. First, a subtractive normalization is exerted when <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is different from zero. For example, if we assign positive and negative numbers to rewards and punishments respectively, their corresponding incentive values may change in sign, depending on whether punishment (i.e., <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> ) or reward (i.e., <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> ) is expected on average within the context. This implies that small rewards can appear as losses in contexts where large rewards are expected. Second, a divisive normalization depends on the gain <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. This implies that the positive and negative value of profits (i.e.,<inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and losses (i.e.,<inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) will be augmented or attenuated, depending upon the relative precision of prior beliefs about (prior confidence in) the context <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and of sensory evidence about the reward option <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Equation two applies every time novel information about reward is provided, which is when a prediction error occurs. This happens (i) when a (primary or secondary) reward is delivered (or is not delivered when expected), which can be post choice as well as in other conditions (e.g., in classical conditioning paradigms, when a reward is delivered independent of action), and (ii) when one (or more) option is presented. The latter follows because an agent has an expectation about an option, which leads to a prediction error when the actual option is presented.</p><p>A key aspect of our proposal addresses how contextual variables are implemented within a generative model. One possibility, illustrated in <xref ref-type="fig" rid="fig1">Figure 1B,</xref> is a generative model that includes an observation O reporting information about context. This model assumes that a value C is drawn from a Gaussian distribution with mean <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. A zero mean captures context-independent information, as it implies that overall rewards (i.e.,<inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and punishments (i.e.,<inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) will be attributed positive and negative incentive values respectively. The variance <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> reflects prior uncertainty about the hidden or latent context. A context observation O is sampled from a Gaussian distribution with mean <italic>f</italic>(C) and variance <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> (reflecting the reliability of the context-related cue). For simplicity, we assume that <italic>f</italic>(C) = C, though in general this can be any function (similar simplifications are assumed below). A reward R is observed after being sampled from a Gaussian distribution with mean C and variance <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> (reflecting uncertainty about the reward distribution).</p><p>We propose that agents form posterior beliefs about the context P(C|O,R) using Bayesian belief updating – first accumulating contextual information by estimating P(C|O), and then reward information to give P(C|O,R). This sequential inference (c.f., evidence accumulation) is motivated by the fact that information about context is usually provided at an earlier time point than reward options. The mean of the posterior distribution P(C|O) is:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mi>O</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>And the posterior variance:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The mean of the posterior distribution P(C|O,R) corresponds to:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Implying the following incentive value for reward:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This shows that, other things being equal, information about the context (reflected in the value of O) induces a subtractive value normalization.</p><p>A possible extension of this generative model is illustrated in <xref ref-type="fig" rid="fig1">Figure 1C</xref> where contexts are organized hierarchically (<xref ref-type="bibr" rid="bib46">Pezzulo et al., 2015</xref>). Imagine evaluating the same dish in different restaurants (a low-level context) and in different neighbourhoods (a high-level context). This example highlights the fact that some (high-level) contexts are more generic, while other (low-level) contexts are more specific. Crucially, if a context exerted no impact on incentive value, one would expect that the dish would be equally attractive, irrespective of where it was experienced. If one context exerted an influence, we would expect, for example, that the incentive value of the dish depends on the restaurant and not on the neighbourhood. Finally, if both contextual levels are in play, one would predict that different incentive values would be attributed to the same dish as a function of both the restaurant and neighbourhood. Here, we examined the possibility that incentive value depends on generative models where contexts are nested hierarchically. A higher-level contextual variable (e.g., the neighbourhood) is represented by a Gaussian distribution with mean <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> equal to zero and variance <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, from which a value HC is sampled. Sensory evidence about HC is provided by HO, which is sampled from a Gaussian distribution with mean HC and variance <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. A lower-level contextual variable (e.g., the restaurant) is represented by a (Gaussian) distribution with mean HC and variance <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, from which a value LC is sampled. Sensory evidence about LC is provided and represented by LO which is sampled from a Gaussian distribution with mean LC and variance <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. A reward option is obtained and sampled from a Gaussian distribution with mean LC and variance <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. We propose that agents infer the posterior distribution P(LC|HO,LO,R) sequentially by estimating, in the order, P(HC|HO), P(LC|HO), P(LC|HO,LO), and P(LC|HO,LO,R). This produces an equation for incentive value with the following form (see Appendix for derivation):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>O</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Three normalization effects are implicit here. The first (<inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) is a subtractive normalization proportional to the value LO observed at the low contextual level. A second one (<inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) is a subtractive normalization proportional to the value HO observed at the high contextual level. The terms τ represent precision-dependent weights and describe the relative precision of the low-level (<inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and high-level (<inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) cues. Finally, a third factor (K) implements divisive normalization and depends on a gain term which includes reward variance (see Appendix).</p><p>In summary, this Bayesian formulation outlines a principled theoretical explanation for how we contextualise rewards based on prior expectation and uncertainty with potential deep hierarchical structure. The key role of uncertainty is reflected in the precision-weighting of the prediction errors (e.g., outcome or reward prediction errors).</p><p>The proposal advanced here has some similarities with classical theories of value, such as Expected Utility theory (<xref ref-type="bibr" rid="bib73">von Neumann and Morgenstern, 1944</xref>) and Prospect theory (<xref ref-type="bibr" rid="bib30">Kahneman and Tversky, 1979</xref>). For example, there are convergences between the influence of the average reward in BCV and the impact of wealth on marginal utility as postulated in Expected Utility theory. Similarities exist also between the role of the average reward in BCV and the status quo notion in Prospect theory, which distinguishes between loss and profit. Indeed, in BCV profits can be conceived in terms of values larger than the expected reward and losses as values smaller than the expected reward.</p><p>We see a more direct link between BCV and recent economic models which postulate that incentive value is adapted to the statistics of the expected reward distribution (Bushong et al., unpublished; <xref ref-type="bibr" rid="bib33">Kőszegi and Rabin, 2006</xref>; <xref ref-type="bibr" rid="bib34">Kőszegi and Szeidl, 2013</xref>), which in turn depends on prior experience within an environment (<xref ref-type="bibr" rid="bib60">Stewart et al., 2006</xref>; <xref ref-type="bibr" rid="bib62">Stewart, 2009</xref>). These theories can be broadly classified into those based on subtractive normalization, which assume that incentive value corresponds to the reward minus a reference value (<xref ref-type="bibr" rid="bib33">Kőszegi and Rabin, 2006</xref>), and those based on divisive normalization, prescribing that incentive value corresponds to the reward divided (or multiplied; <xref ref-type="bibr" rid="bib34">Kőszegi and Szeidl, 2013</xref>) by either the expected reward (<xref ref-type="bibr" rid="bib38">Louie et al., 2014</xref>, <xref ref-type="bibr" rid="bib35">2015</xref>) or the range of an expected distribution of rewards (Bushong et al., unpublished; <xref ref-type="bibr" rid="bib34">Kőszegi and Szeidl, 2013</xref>).</p><p>BCV differs in important ways from previous theories in its attempt to derive contextual normalization from normative assumptions of Bayesian statistics. This approach conceives incentive value as precision-weighted prediction error and implies two forms of contextual adaptation (<xref ref-type="fig" rid="fig2">Figure 2</xref>). First, as in some previous theories (<xref ref-type="bibr" rid="bib33">Kőszegi and Rabin, 2006</xref>), subtractive normalization emerges as the expected reward is subtracted from the actual reward. Second, the gain term implements divisive normalization, an aspect similar to a recent model in which the range of the reward distribution (which is analogous – though not identical - to the gain term) divides the reward (Bushong et al., unpublished). These predictions are specific and distinguish BCV from other models. For instance, BCV predicts that divisive normalization derives from the gain term (i.e., reward variance) and not from the expected reward (<xref ref-type="bibr" rid="bib38">Louie et al., 2014</xref>, <xref ref-type="bibr" rid="bib35">2015</xref>), and that the reward variance divides – and not multiplies (<xref ref-type="bibr" rid="bib34">Kőszegi and Szeidl, 2013</xref>) – the prediction error. Importantly, these predictions are not <italic>ad hoc</italic> but derive necessarily from Bayesian assumptions.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.16127.003</object-id><label>Figure 2.</label><caption><title>Effects predicted by BCV on the incentive value (V(R)) as a function of reward (R) and contexts associated with specific distributions of rewards presented sequentially over trials arranged in blocks.</title><p>(<bold>A</bold>) Example with a single hierarchical level where two contexts have different average reward. In blocks associated with a low-average context (LA; in lighter grey), the possible rewards are x, x+1 and x+2; in blocks associated with a high-average context (HA; in darker grey), the possible rewards are x+1, x+2 and x+3. (<bold>B</bold>) BCV prediction of the incentive value attributed to rewards depending on these contexts. Larger values are predicted in the LA compared to the HA for amounts common to both contexts. (<bold>C</bold>) Effects predicted by BCV dependent on contexts with different reward variance. In blocks associated with a high-variance context (HV; in lighter grey), the possible rewards are x, x+1 x+2 and x+3; in blocks associated with a low-variance context (LV; in grey), the possible rewards are x+1 and x+2. (<bold>D</bold>) BCV prediction of the incentive value attributed to rewards depending on these contexts. Considering rewards common to both contexts, BCV predicts a higher incentive value for x+1 in the high-variance context and for x+2 in the low-variance context. (<bold>E</bold>) Example with two hierarchical levels (low-level (LL) contexts, represented by patterns of bars, and high-level (HL) contexts, represented by frames). Blocks associated with HL contexts comprise several sub-blocks associated with LL contexts having specific average reward. In the HL context with low-value (HL-LA; light frame), a LL context with low value (LL-LA, where rewards are x, x+1 and x+2) and a LL context with a medium value (LL-MA, where rewards are x+1, x+2 and x+3) alternate. In the HL context with high-value (HL-HA; dark frame), a LL-MA context and a LL context with high value (LL-HA, where rewards are x+2, x+3 and x+4) alternate. (<bold>F</bold>) BCV prediction of the incentive value attributed to rewards depending on these hierarchical contexts. The pattern of bars represents the LL context condition, the outline colour represents the HL context condition. BCV predicts that incentive values derive from integrating both hierarchical levels, with larger values emerging when the average reward is lower at both context levels.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.003">http://dx.doi.org/10.7554/eLife.16127.003</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16127-fig2-v2.tif"/></fig></p></sec><sec id="s2-2"><title>Experiment one</title><p>Data from conditions where BCV is applicable, namely those involving a single attribute and where context depends on past options (and not simultaneously presented options), are relatively scarce. Here, empirical evidence has shown a <italic>subtractive</italic> normalization, whereby incentive values are rescaled to the expected reward (<xref ref-type="bibr" rid="bib33">Kőszegi and Rabin, 2006</xref>; <xref ref-type="bibr" rid="bib39">Ludvig et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>). In addition, there is an absence of evidence for a <italic>divisive</italic> normalization exerted by the expected reward (i.e., where values are <italic>divided</italic> by the expected reward; <xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>). Both findings are consistent with BCV. However, another key prediction of BCV relies on a divisive normalization dependent on reward variance (<xref ref-type="fig" rid="fig2">Figure 2C–D</xref>), though it remains unknown whether such variance-dependent normalization actually occurs. Here, we present data from a behavioural experiment where we investigate this very question.</p><p>Participants performed a computer-based decision-making task (<xref ref-type="fig" rid="fig3">Figure 3</xref>) in which a monetary amount, changing trial by trial, was presented in the centre of the screen and participants had to choose whether to accept half of it for sure or select a 50–50 gamble between the full monetary amount and zero, a scenario where the sure option and gamble always carry the same expected value (EV). The task was organized in blocks, each associated with one of two contexts which determined the possible EVs associated with the block. These EVs were £3 and £4 for the low-variance context, and £2, £3, £4 and £5 for the high-variance context. Note that average choice EV was equal across contexts (i.e., £3.5). Contexts were cued by the associated EVs displayed on the top of the screen in brackets. To ensure incentive compatibility, at the end of the experiment one single outcome was randomly selected and paid out to participants.<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.16127.004</object-id><label>Figure 3.</label><caption><title>Behavioural paradigm.</title><p>On each trial, participants were presented a monetary amount (£8 in this example) in the middle of the screen and had to choose between half of the amount for sure (pressing a left button) and a gamble (pressing a right button) associated with either the full amount or a zero outcome, each with an equal chance. The outcome was shown right after response for one second. During the intertrial interval (ITI), the possible amounts of the next trial were shown on the top of the screen in brackets. The same amounts were presented throughout blocks associated either with a low-variance (LV) context, including £6 and £8 amounts, or a high-variance (HV) context, including £4, £6, £8 and £10 amounts.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.004">http://dx.doi.org/10.7554/eLife.16127.004</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16127-fig3-v2.tif"/></fig></p><p>In the analyses we focused on choices common to both contexts, namely involving £3 and £4 EV. p&lt;0.05 was used as significance criterion and trials with reaction times slower than 3 s (and faster than 400ms) excluded. For these choices, the average gambling probability did not differ from 50% (mean = 49; SD = 23; t(35) = −0.31, p&lt;0.76). In addition, gambling probability was equivalent when comparing £4 and £3 choices (t(35) = −0.81, p =0.43). Across individuals, there was no correlation between the average gambling probability for £4 and £3 and the difference in gambling probability between these two EVs (<xref ref-type="fig" rid="fig4">Figure 4C–D;</xref> r(36) = −0.06, p =0.75). The latter result replicates previous findings (<xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>) and supports the idea of a differentiation between an average gambling propensity and a preference to gamble with large or small EV as determinant of risk choice.<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.16127.005</object-id><label>Figure 4.</label><caption><title>Results of experiment one (data used for the analyses reported here are provided in <xref ref-type="supplementary-material" rid="SD1-data">Source code 1</xref>).</title><p>(<bold>A</bold>) Gambling proportion for different EVs and contexts for participants who gambled more for £4 compared to £3 choices. (<bold>B</bold>) Gambling proportion for different EVs and context for participants who gambled less for £4 compared to £3 choices. (<bold>C</bold>) Relationship between average gambling proportion for £3 and £4 EV choices and gambling proportion for £4 minus £3 choices. Data showed no correlation (r(36) = -0.06, p = 0.75). (<bold>D</bold>) The same relationship is reported for data simulated with the computational model of choice behaviour and the parameters estimated from choice behaviour (r(36) = −0.03, p = 0.87). (<bold>E</bold>) Relationship between (i) the gambling proportion for £4 minus £3 choices and (ii) the interaction term reflecting the gambling proportion for £4 minus £3 choices in the low-variance context compared to the gambling proportion for £4 minus £3 choices in the high-variance context. Data showed a positive correlation (r(36) = 0.45, p = 0.005). Note that this result remains significant also using Spearman correlation, which is less affected by outliers (rho = 0.51, p = 0.002). (<bold>F</bold>) The same relationship is reported for data simulated with the computational model of choice behaviour and the parameters estimated from choice behaviour (r(36) = 0.48, p = 0.003; the plot reports an example taken from the 100 simulations).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.005">http://dx.doi.org/10.7554/eLife.16127.005</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16127-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.16127.006</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Experiment one: distribution of parameters estimated from choice data with the full model.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.006">http://dx.doi.org/10.7554/eLife.16127.006</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16127-fig4-figsupp1-v2.tif"/></fig></fig-group></p><p>We investigated key predictions of BCV regarding a divisive normalization (or precision weighting) effect exerted by context, which is captured by the gain term <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">equation 2</xref>. We can formalize our context manipulation by varying the reward variance <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is larger in the high-variance context, implying a smaller gain term. Given that £4 and £3 are larger and smaller than the context average (i.e., £3.5) respectively, we predicted that (at option presentation) they induce a positive and negative prediction error respectively. Because of the gain term, BCV predicts (<xref ref-type="fig" rid="fig2">Figure 2C–D</xref>) that £4 is attributed a larger (i.e., more positive) incentive value in the low-variance context while £3 is attributed larger (i.e., less negative) incentive value in the high-variance context. Note that in our task there are two types of variance. The first refers to the variance of possible outcomes of the gamble (which is perfectly correlated with the EV of options, as in <xref ref-type="bibr" rid="bib52">Rigoli et al., 2016</xref>), and is not the focus of our study. The second refers to variance <italic>across options</italic> (i.e., the variance characterizing the distribution of successive options), which is what we experimentally manipulate and investigate. In the model, this is reflected in and affects the gain term in <xref ref-type="disp-formula" rid="equ2">equation 2</xref>.</p><p>First, we tested our central predictions by analysing raw choice data. Though we observed no overall difference in gambling across participants for £4 and £3 EV (see above), participants could be differentiated based on those who gambled more with £4 or £3 EV. In line with previous observations (<xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>), we predicted that the impact of context on gambling depended on a subject-specific propensity to gamble more with large or small choice EVs; in other words on whether a participant prefers to gamble for £3 compared to £4 EV or vice versa. Combining this prediction with BCV predictions (<xref ref-type="fig" rid="fig2">Figure 2C–D</xref>), participants who risked more with increasing EVs would be expected to gamble more for £4 and less for £3 in the low-variance context (when £4 and £3 would be attributed larger and smaller incentive value respectively) compared to the high-variance context. On the contrary, participants who risked more with decreasing EVs would be expected to gamble less for £4 and more for £3 in the low-variance context compared to the high-variance context. To examine these predictions we tested for an interaction, corresponding to the differential gambling percentage across contexts (low-variance minus high-variance context) for £4 choices minus the differential gambling percentage across contexts (low-variance minus high-variance context) for £3 choices. Across participants, the interaction term did not differ from zero (t(35) = -0.43, p =0.67) but, consistent with our hypothesis, it showed a significant correlation with the gambling probability for £4 minus £3 choices (<xref ref-type="fig" rid="fig4">Figure 4A,B,E,F;</xref> r(36) = 0.45, p = 0.005). Note that this result remains significant when using a Spearman correlation, which is less affected by outliers (rho = 0.51, p = 0.002).</p><p>Next, we adopted a model-based approach to assess whether BCV explains choice data. Following <xref ref-type="disp-formula" rid="equ2">equation 2</xref>, if the option EV is R, then its associated incentive value will be:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>ε</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is an indicator of the high (<inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ε</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) or low-variance context (<inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ε</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates the contextual mean (and is equal to £3.5 for both contexts), and <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a free parameter (bounded within the 0.1–10 range) which implements a gain term and captures divisive normalization of reward. To connect value adaptation to choice, we used a logistic regression model of gambling where the probability of choosing the gamble does not depend on the objective option EV, but on the associated incentive value (i.e., transformed by contextual normalization):<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext></mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where α is a value-related parameter which determines whether gambling increases (α &gt; 0) or decreases (α &lt; 0) with larger incentive value and ρ represents a gambling bias parameter.</p><p>We used likelihood ratio tests (see Materials and methods) to compare this model with simpler (i.e., reduced) models, where one or more parameters were set to zero. Model comparison favoured the full model (comparison with: random model: χ<sup>2</sup>(108) = 3268, p&lt;0.001; model with α: = 2502, p&lt;0.001; model with ρ: χ<sup>2</sup>(72) = 1526, p&lt;0.001; model with α and ρ: (36) = 704, p&lt;0.001). In addition, the model predicts that the free parameter τ is smaller than one or log(τ) is less than zero. Consistent with BCV and variance-dependent normalization, the mean of log(τ) was significantly smaller than zero (t(35) = −2.81, p = 0.008).</p><p>We next compared the full model with an alternative model without a subtractive normalization component (as postulated by BCV, where the expected reward is subtracted to the actual reward); namely, where the incentive value was equal to <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>ε</mml:mi></mml:mrow></mml:msup><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The latter model derives from previous accounts of context-sensitive value (Bushong et al., unpublished; <xref ref-type="bibr" rid="bib34">Kőszegi and Szeidl, 2013</xref>). We compared the negative log-likelihood of the two models (given that they had an equal number of free parameters) and found a smaller score for the model with the subtractive component (difference in log-likelihood: 254). We tested this difference by performing a Chi-square test with one degree of freedom (treating the model predicted by BCV as having an additional parameter). This test was significant (χ<sup>2</sup>(1) = 508, p&lt;0.001), meaning that the model implementing both subtractive and divisive normalization (derived from BCV and described by <xref ref-type="disp-formula" rid="equ8">equation 8</xref>) fits the data better.</p><p>Finally, we used the (selected) model and subject-specific parameter estimates of the last analysis to generate simulated choice behaviour and performed behavioural analyses on the ensuing data (data were simulated 100 times and the average statistics are reported). Consistent with real data, the full model replicated the lack of correlation between average gambling (for £4 and £3 choices) and the difference in gambling for £4 and £3 choices (average r(36) = −0.06, p = 0.73), while a correlation emerged when data were simulated using a model without the gambling bias parameter ρ (average r(36) = 0.51, p = 0.001). Moreover, and again consistent with empirical data, the full model replicated the correlation between gambling for £4 minus £3 choices and the context-EV interaction effect term (average r(36) = 0.48, p = 0.003), a result not obtained when data were simulated using a model without the value-function parameter α (average r(36) = 0.01, p = 0.95) or without the context parameter τ (average r(36) = 0.09, p = 0.60).</p><p>Collectively, these analyses validate the proposal of a divisive normalization component dependent on a gain term (in turn dependent on reward variance) consistent with BCV.</p></sec><sec id="s2-3"><title>Experiment two</title><p>Another key prediction of BCV is that the generative model can reflect contexts organized hierarchically and that incentive value and choice are adapted to contextual information available at different hierarchical levels (<xref ref-type="fig" rid="fig2">Figure 2E–F</xref>). So far, there has been a focus on non-hierarchical settings (<xref ref-type="bibr" rid="bib33">Kőszegi and Rabin, 2006</xref>; <xref ref-type="bibr" rid="bib39">Ludvig et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>), and therefore whether adaptation combines influence from context at multiple hierarchical levels remains unknown. Here, we present data from a behavioural experiment where we investigated this question.</p><p>Participants played a computer-based task (<xref ref-type="fig" rid="fig5">Figure 5</xref>) where, on each trial, two rectangles representing two decks of cards appeared. Each card was associated with a monetary reward, and the average card reward for each deck was displayed in brackets on the deck. The decks were coloured; one in grey and the other in blue. A card was pseudo-randomly drawn from the blue deck and the corresponding monetary reward was presented in the middle of the screen. Participants had to choose between half of the monetary reward for sure and a gamble between the full reward and a zero outcome, each with 50% chance. Note that, as in experiment one, the two options carry the same EV. After making a choice, the outcome was then shown. To ensure incentive compatibility, at the end of the experiment, one outcome was randomly selected and paid out to participants.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.16127.007</object-id><label>Figure 5.</label><caption><title>Experimental paradigm: participants played a computer-based task where, on each trial, two rectangles representing two decks of cards appeared.</title><p>(<bold>A</bold>) Each card was associated with a monetary gain, and the average monetary gain of each deck was displayed in brackets on the deck. The decks were coloured one in blue and the other in grey, indicating the selected and unselected deck respectively. Among these two decks shown on the screen, the selected deck (coloured in blue) alternated pseudo-randomly over blocks (each including 5 trials). In addition, two sets of decks alternated over longer blocks (20 trials) in a pseudo-random way. After decks were shown during an inter-trial-interval of 1.5 s, a card was pseudo-randomly drawn from the blue deck and the corresponding monetary amount was presented in the middle of the screen. Participants had to choose between half of the monetary amount for sure (pressing a left button) and a gamble between the full amount and a zero outcome (pressing a right button), each with a 50% chance. After choosing, the choice outcome appeared for one second. At the end of the experiment, one outcome was randomly selected and paid to participants. (<bold>B</bold>) Schematic of how contexts are organized in this paradigm. The selected deck alternated pseudo-randomly over blocks. In addition, two sets of decks alternated over longer blocks in a pseudo-random way. The low-value deck-set (LV Deck-set; light grey frame) comprised decks associated with £5 and £7 on average; the high-value deck-set (HV Deck-set; dark grey frame) comprised decks associated with £7 and £9 on average. The cards of the £5 deck could be associated with £3, £5 and £7; the cards of the £7 deck could be associated with £5, £7 and £9; the cards of the £9 deck could be associated with £7, £9 and £11.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.007">http://dx.doi.org/10.7554/eLife.16127.007</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16127-fig5-v2.tif"/></fig></p><p>The deck selected by the computer alternated pseudo-randomly over blocks. In addition, two sets of decks alternated over longer blocks in a pseudo-random way. The first deck-set (low-value deck-set) comprised decks associated with an average of £5 and £7, the second (high-value deck-set) comprised decks associated with an average of £7 and £9. The cards of the £5 deck were associated with £3, £5 and £7; the cards of the £7 deck were associated with £5, £7 and £9; the cards of the £9 deck were associated with £7, £9 and £11. The aim of this experimental paradigm was to manipulate context at two hierarchical levels, a low-level associated with decks, and a high-level associated with deck-sets. Note that the rewards overlapped between contexts at both levels; namely, across decks and deck-sets. In relation to decks, £5 and £7 cards were common to both decks in the low-value deck-set, and £7 and £9 cards were common to both decks in the high-value deck-set. If this level of context exerted an influence, it should elicit changes in choice consistent with BCV when comparing choices based upon the same reward across decks. In relation to deck-sets, the deck associated with £7 average was present in both sets. If the deck-set level exerted an influence, this would elicit changes in choice consistent with context sensitive values during the presentation of the £7 deck.</p><p>The average gambling percentage did not differ from fifty percent across subjects (mean = 54; SD = 16; t(31) = 1.48, p = 0.12). P&lt;0.05 was used as significance criterion and trials with reaction times slower than 3 s (and faster than 400 ms) excluded. We assessed the impact of option EV on choice using a logistic regression model, where EV was included as regressor. The associated regression coefficient was not significantly different from zero across participants (t(31) = -1.67, p = 0.11). We then investigated the relationship between the average propensity to gamble and the effect of EV on gambling but did not find any correlation (Figure 7A; r(32) = 0.23, p = 0.21). This result again replicates previous studies using a similar paradigm (<xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>), and highlights two different determinants of risk attitude; one linked with a baseline gambling propensity and the other linked with a preference to gamble with large or small reward amounts.</p><p>Though across participants, we observed no overall effect of option EV on gambling, participants could be differentiated based on those who showed a positive or negative effect of option EV on gambling. Based on previous findings (<xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>), we hypothesized that context sensitive value would predispose participants who preferred to gamble for large EVs to gamble more when EVs were larger relative to contextual expectations, namely in lower value contexts (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Similarly, we expected participants who preferred to gamble for small EVs to gamble more when EVs were smaller relative to contextual expectations, namely, in higher value contexts. We investigated this hypothesis both at the level of decks and deck-sets (<xref ref-type="fig" rid="fig6">Figure 6</xref>).<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.16127.008</object-id><label>Figure 6.</label><caption><title>Gambling proportion for different card amounts and different context conditions (data used for this figure are provided in <xref ref-type="supplementary-material" rid="SD2-data">Source code 2</xref>), separately for (<bold>A</bold>) participants showing a negative effect of card monetary amount (i.e., the slope of the logistic regression model with card amount as a predictor) on gambling (n = 19) and (<bold>B</bold>) participants showing a positive effect of the card amount on gambling (n = 13).</title><p>When considering amounts that are common to multiple contexts, these data show that (i) for the first group of participants, gambling decreases as the context condition is characterized by lower expectations (after integrating both contexts) and (ii) for the second group of participants, gambling increases when the context is characterized by lower expectations, after integrating both contexts (except for the £9 amount). These data are consistent with our hypotheses; namely (i) with predictions arising from BCV (<xref ref-type="fig" rid="fig2">Figure 2D</xref>) which implies subtractive normalization of incentive value at both hierarchical levels and (ii) with the prediction (derived from previous observations; <xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>) that the influence of incentive value on gambling proportion depends on the individual preference to gamble with large or small card amounts.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.008">http://dx.doi.org/10.7554/eLife.16127.008</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16127-fig6-v2.tif"/></fig></p><p>At the level of decks, we computed – for each deck-set – the difference in gambling between lower and higher value decks for rewards common to both decks (corresponding to £5 and £7 in the low-value deck-set, and to £7 and £9 in the high-value deck-set). The mean of these two differences correlated across subjects with the effect of EV on gambling (i.e., the associated regression coefficient of the logistic regression model; <xref ref-type="fig" rid="fig7">Figure 7C</xref>; r(32) = 0.55, p = 0.001), consistent with a contextualisation of reward by decks at the lower contextual level. At the level of deck-sets, we computed the difference in gambling between the low and high value deck-set for the £7 deck (common to both deck-sets). This difference correlated across subjects with the effect of EV on gambling (i.e., the associated regression coefficient of the logistic regression model; <xref ref-type="fig" rid="fig7">Figure 7E</xref>; r(32) = 0.42, p = 0.018), consistent with a context sensitive value effect of the higher contextual level.<fig-group><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.16127.009</object-id><label>Figure 7.</label><caption><title>Results of experiment two (data used for the analyses reported here are provided in <xref ref-type="supplementary-material" rid="SD2-data">Source code 2</xref>).</title><p>(<bold>A</bold>) Relationship between the individual gambling percentage and the effect of monetary amount on gambling proportion (i.e., the associated regression coefficient of the logistic regression model; r(32) = 0.23, p = 0.21, non-significant). (<bold>B</bold>) The same relationship is reported for data simulated with the computational model of choice behaviour and the parameters estimated from choice behaviour (r(32) = 0.06, p = 0.76, non-significant; the plot reports an example taken from the 100 simulations). (<bold>C</bold>) We computed for each deck-set the difference in gambling between lower and higher value decks for amounts common to both decks; corresponding to £5 and £7 in the low-value deck-set, and to £7 and £9 in the high-value deck-set. The relationship between the mean of these two differences and the effect of card amount on gambling is reported (r(32) = 0.55, p = 0.001). (<bold>D</bold>) The same relationship is reported for data simulated with the computational model of choice behaviour and the parameters estimated from choice behaviour (r(32) = 0.58, p&lt;0.001; the plot reports an example taken from the 100 simulations). (<bold>E</bold>) Relationship between the difference in gambling between the low and high value deck-set for the £7 deck (common to both deck-sets) and the effect of card amount on gambling (r(32) = 0.42, p = 0.018). (<bold>F</bold>) The same relationship is reported for data simulated with the computational model of choice behaviour and the parameters estimated from choice behaviour (r(32) = 0.57, p = 0.03; the plot reports an example taken from the 100 simulations).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.009">http://dx.doi.org/10.7554/eLife.16127.009</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16127-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.16127.010</object-id><label>Figure 7—figure supplement 1.</label><caption><title>Experiment two: distribution of parameters estimated from choice data with the full model.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.010">http://dx.doi.org/10.7554/eLife.16127.010</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16127-fig7-figsupp1-v2.tif"/></fig></fig-group></p><p>We next investigated whether BCV explains the context effects implicit in these results. Since, in our task, contexts are organized hierarchically (i.e., decks and deck-sets are associated with high and low levels – in the sense that the set determines the possible decks), we refer to the generative model shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, where incentive value is described by <xref ref-type="disp-formula" rid="equ7">equation 7</xref>. Recall that there are three normalization terms under this model (see <xref ref-type="disp-formula" rid="equ7">equation 7</xref>): first a subtractive term (<inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) proportional to the value LO observed at the low contextual level, second a subtractive term (<inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) proportional to the value HO observed at the high contextual level, and third a divisive factor (K) dependent on precision. In our task, the average contextual reward was manipulated, enabling us to examine the subtractive terms outlined in <xref ref-type="disp-formula" rid="equ7">equation 7</xref>. We did this by treating the contextual averages as observations of the true underlying contexts of the task (in <xref ref-type="disp-formula" rid="equ7">equation 7</xref>, HO and LO would be associated with deck-set and deck respectively). However, since reward variance (which enters the divisive normalization factor K) was not manipulated, this task is not suitable to quantify an effect of precision-weighting – and therefore we omitted this factor from the model of our empirical data. Specifically, based on <xref ref-type="disp-formula" rid="equ7">equation 7</xref> and omitting K, if the option EV is R, then its associated incentive value will be:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <italic>χ<sub>LO</sub></italic> indicates the average option EV for the deck (for £9 deck: <italic>χ<sub>LO</sub></italic> = 4.5; for £7 deck: <italic>χ<sub>L</sub></italic><sub><italic>O</italic> </sub>= 3.5; for £5 deck: <italic>χ</italic><sub><italic>LO</italic></sub> = 2.5), χ<sub>HO</sub>indicates the average option EV for the deck-set (high-value deck-set: <italic>χ</italic><sub><italic>HO</italic></sub> = 4; low-value deck-set: <italic>χ</italic><sub><italic>HO</italic></sub> = 3) τ<italic><sub>LO</sub></italic> is a free parameter that mediates contextual effects at the deck level, and τ<italic><sub>HO</sub></italic> is a free parameter that mediates contextual effects at the deck-set level (see Materials and methods).</p><p>To connect value adaptation to choice, we used a logistic regression model as in <xref ref-type="disp-formula" rid="equ9">equation 9</xref> where α is a value-related parameter which determines whether gambling increases (α &gt; 0) or decreases (α &lt; 0) with larger incentive value and ρ represents a gambling bias parameter. We used a likelihood ratio test to compare this model with simpler (reduced) models where one or more parameters were set to zero. The full model was favoured (comparison with: random model: χ<sup>2</sup>(128) = 2959, p&lt;0.001; model with α: χ<sup>2</sup>(96) = 1530, p&lt;0.001; model with ρ: χ<sup>2</sup>(96) = 1622, p&lt;0.001; model with α and ρ: χ<sup>2</sup>(64) = 262, p&lt;0.001; model with α, ρ and τ<sub><italic>LO</italic></sub>(32) = 134, p&lt;0.001; model with α, ρ and τ<italic><sub>HO</sub></italic>: χ<sup>2</sup>(32) = 58, p&lt;0.001). In addition, consistent with context sensitivity at both hierarchical levels, the context-related free parameters of the full model were significantly larger than zero (<xref ref-type="fig" rid="fig6">Figure 6</xref>; τ<sub><italic>LO</italic></sub>: t(31) = 4.55, p&lt;0.001; τ<italic><sub>HO</sub></italic> t(31) =2.67, p = 0.012).</p><p>We next compared the full model with an alternative model where the context parameters (capturing the influence of the reward expected within a context at multiple hierarchical levels) divided the reward rather than being subtracted from the reward; in other words where the incentive value corresponds to:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The latter model derives from previous accounts of context sensitivity (<xref ref-type="bibr" rid="bib38">Louie et al., 2014</xref>, <xref ref-type="bibr" rid="bib35">2015</xref>). We compared the negative log-likelihood of the two models (given they had an equal number of free parameters) and found a smaller score for the model with subtractive normalization (difference in log-likelihood for divisive minus subtractive model; full models: 39; for models with α, ρ and τ<italic><sub>LO</sub></italic>: 13; for models with α, ρ and τ<sub><italic>LH</italic></sub>: 25). This difference was tested statistically by performing a Chi-square test with one degree of freedom (treating the model predicted by BCV as having an additional parameter). This test was significant (full models: χ<sup>2</sup>(1) = 78, p&lt;0.001; for models with α, ρ and τ<sub><italic>LO</italic></sub>: χ<sup>2</sup>(1) = 26, p&lt;0.001; for models with α, ρ and τ<sub><italic>LH</italic></sub>: χ<sup>2</sup>(1) = 50, p&lt;0.001), meaning that the model implementing subtractive normalization (consistent with BCV) is a better explanation for the data.</p><p>Finally, we used the (selected) model and subject-specific parameter estimates of the last analysis to generate simulated choice behaviour and performed behavioural analyses on the ensuing data (data were simulated 100 times and the average statistics are reported). As with real data, our modelling replicated the absence of correlation between average gambling and the effect of reward on gambling (i.e., the slope of the logistic regression; see above) (<xref ref-type="fig" rid="fig7">Figure 7B</xref>; average r(32) = 0.06, p = 0.76), while a correlation emerged when the data were simulated using a model without the gambling bias parameter ρ (average r(32) = 0.85, p&lt;0.001). Moreover, consistent with empirical data, the full model replicated the correlation between (i) the effect of EV on gambling and the difference in gambling across decks for choices common to both decks of a deck-set (combining both deck-sets; <xref ref-type="fig" rid="fig7">Figure 7D</xref>; average r(32) = 0.58, p&lt;0.001), (ii) the effect of EV on gambling and the difference across deck-sets in gambling for the £7 deck (common to both deck-sets; <xref ref-type="fig" rid="fig7">Figure 7F</xref>; average r(32) = 0.57, p=0.001). These correlations were not replicated when data were simulated using a model without the value-related parameter α (first correlation: average r(32) = 0.09, p = 0.62; second correlation: average r(32) = =0.02, p = 0.89). Furthermore, the first correlation was not replicated when using a model without the parameter τ<sub><italic>LO</italic></sub> (average r(32) = −0.15, p = 0.41) and the second correlation was not obtained when using a model without the parameter τ<italic><sub>HO</sub></italic>(average r(32) = −0.03, p = 0.87).</p><p>Collectively, these analyses show subtractive normalization exerted by contextual effects at multiple hierarchical levels consistent with predictions from BCV.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We propose a Bayesian scheme (BCV) as a model of contextual influences on incentive value attribution. BCV is based on Bayesian inference principles and on generative models of reward. Adopting two novel experimental designs, we provide behavioural evidence that supports two key predictions of BCV, namely that value attribution is affected by reward variance (which exerts divisive normalization) and by hierarchically organized contexts.</p><p>Our account is motivated by normative principles of Bayesian statistics – and fits within a Bayesian brain hypothesis framework (<xref ref-type="bibr" rid="bib10">Chater et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib17">Dayan et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Ernst, 2006</xref>; <xref ref-type="bibr" rid="bib24">Friston, 2010</xref>). As such, it provides a principled account of decision making under uncertainty. In particular, it accommodates expectation and uncertainty that may have a deep hierarchical structure, as in real world situations. Bayesian schemes are based on a formal and a clear definition of the imperatives that motivate cognitive processes, which are conceived in terms of inference. This allows BCV to establish a link with Bayesian perspectives in other domains of cognitive neuroscience, helping unifying perspectives on brain functioning.</p><p>Our proposal is closely linked to the framework of planning as inference and active inference (<xref ref-type="bibr" rid="bib6">Botvinick and Toussaint, 2012</xref>; <xref ref-type="bibr" rid="bib22">Friston et al., 2013</xref>, <xref ref-type="bibr" rid="bib22">2015</xref>; <xref ref-type="bibr" rid="bib47">Pezzulo and Rigoli, 2011</xref>; <xref ref-type="bibr" rid="bib46">Pezzulo et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Solway and Botvinick, 2012</xref>). This recasts decision-making and planning – usually understood in terms of value or utility maximization – as a form Bayesian inference, and hence can provide a unifying inferential account of perception and action. Hierarchical implementations of active inference schemes have been proposed previously, and the notion of hierarchically-organized contexts fits comfortably within these schemes (<xref ref-type="bibr" rid="bib46">Pezzulo et al., 2015</xref>). BCV extends this framework by focusing on the determinants of incentive value, conceived as precision-weighted prediction error based on (potentially hierarchical) contextual expectations.</p><p>BCV postulates that the two fundamental determinants of incentive value are prediction error and precision. A prediction error is determined by the difference between the observed and expected reward which, in BCV, derives from integrating different expectations under contextual uncertainty. Gain depends on (relative) precision or confidence– and ensures that the prediction error is normalised and (Bayes) optimally weighted in relation to uncertainty about both context and reward. In brief, only precise prediction errors have an effect on expectations higher in the hierarchy during Bayesian belief updating. BCV predicts that precision exerts an influence in two ways. First, at the highest hierarchical levels, precision determines the optimal integration of multiple contextual representations–as it mandates that contexts characterized by a high precision (greater reliability) exert more influence on reward expectancy. For instance, if we assume that subjects have very precise beliefs about the low-level context (e.g., the deck), then the effect of the high-level (e.g., the deck set) will disappear. Formally, this is because in the hierarchical model the low-level context constitutes a Markov blanket for the posterior expectation about the reward option (<xref ref-type="bibr" rid="bib4">Bishop, 2006</xref>). In other words, the effect of the high-level context tells us that if subjects are using a hierarchical model, there must be posterior uncertainty about the low-level context. Heuristically, even though they can see which deck they are currently playing with, they still nuance their expectations about this deck based upon the deck-set from which it came. Second, at the lowest hierarchical level, the precision determines the gain assigned to the prediction error and hence is a direct determinant of incentive value. In our first experiment, we show evidence consistent with the latter expression of precision.</p><p>The central role attributed to precision-weighted prediction error is consistent with Bayesian models in other domains, and speaks to the idea of common computational principles in the brain. In fact, one central idea of many influential Bayesian proposals is that, when sensory inputs are presented, predictions are explained away and the resulting perception corresponds to (precision-weighted) prediction errors. For instance, predictive coding models are based on evidence that activity in certain brain regions responsible for perception reflects prediction errors and not raw sensory inputs (<xref ref-type="bibr" rid="bib23">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib49">Rao and Ballard, 1999</xref>). Moreover, it has been proposed that our sense of agency depends on explaining away somatosensory predictions associated with motor commands, with unexplained sensations alone (i.e., residuals) attributed to external forces (<xref ref-type="bibr" rid="bib7">Brown et al., 2013</xref>). A similar view characterizes active inference schemes, which assume that action is not steered by stimuli per se but by the (precision-weighted) prediction error elicited by those stimuli, expressing the extent to which they depart from expectation in a meaningful way (<xref ref-type="bibr" rid="bib22">Friston et al., 2013</xref>, <xref ref-type="bibr" rid="bib21">2015</xref>).</p><p>Within BCV contextual representations can be hierarchically organized, with high levels characterized by more general conditions. Because reward options are descendants of all hierarchical levels, any context can exert an influence on incentive value, insofar as these levels determine the reward expected in a certain condition. Specifically, BCV can integrate–in a Bayes optimal way–context-independent beliefs about the reward distribution with context-sensitive beliefs unfolding at multiple hierarchical levels. The possibility that subjects use hierarchical generative models is in fact supported by our empirical findings. Our results are consistent with the idea that rewards have larger incentive values when both high and low-level contexts are characterized by reward distributions with a smaller average. This indicates that information about more specific (e.g., the deck) and more general (e.g., the deck-set) contexts are integrated to determine incentive values and choice behaviour.</p><p>A hierarchical nesting might explain why contextual effects observed in psychological experiments are usually substantial but not extreme. In other words, it is unlikely that 10 p will be attributed the same value as £100, even when the contextual manipulation may appear to induce an equivalence between the two quantities. This can be explained by contextual effects from the highest hierarchical level (e.g., that reward options have, in general, a prior expectation of zero). This supraordinate level can be conceived as representing a context-independent distribution of rewards that may derive from the overall statistics of our environment (<xref ref-type="bibr" rid="bib60">Stewart et al., 2006</xref>, <xref ref-type="bibr" rid="bib63">2009</xref>) and/or from innate prior beliefs about the distribution of incentives (<xref ref-type="bibr" rid="bib53">Rigoli et al., 2016c</xref>, <xref ref-type="bibr" rid="bib54">2016d</xref>).</p><p>The proposal that incentive value corresponds to (precision-weighted) reward prediction error should not be confounded with the idea that it corresponds to the posterior reward expectation. Though both possibilities derive from Bayesian principles, they make opposite predictions about the role of prior reward expectancy. While the posterior reward expectation hypothesis predicts the larger value with larger prior reward expectancy, our data show larger value with smaller prior expectancy, consistent with the prediction error hypothesis presented here (see also <xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>, <xref ref-type="bibr" rid="bib52">2016b</xref>).</p><p>With reference to the three levels of analysis (i.e., computational, algorithmic and implementation) proposed by <xref ref-type="bibr" rid="bib40">Marr (1982)</xref>, BCV speaks to the computational level as it focuses on normative principles (implicit in optimal Bayesian inference) proposed to explain value and choice adaptation. In addition, BCV also has implications for the other levels and there are now several biologically-plausible accounts of how Bayesian inference might be implemented in the brain (e.g., <xref ref-type="bibr" rid="bib18">Doya et al., 2007</xref>; <xref ref-type="bibr" rid="bib23">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib26">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib31">Knill and Pouget, 2004</xref>), where some accounts consider neuronal circuits (and generative models) characterized by a hierarchical organization (e.g., <xref ref-type="bibr" rid="bib23">Friston, 2005</xref>). BCV fits comfortably within these biologically-plausible accounts. Consistent with BCV are findings that several brain regions show a response to reward that adapts to both expected reward (e.g., in signalling reward prediction error) and reward range (<xref ref-type="bibr" rid="bib3">Bermudez and Schultz, 2010</xref>; <xref ref-type="bibr" rid="bib15">Cox and Kable, 2014</xref>; <xref ref-type="bibr" rid="bib36">Louie et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Padoa-Schioppa, 2009</xref>; <xref ref-type="bibr" rid="bib42">Padoa-Schioppa and Assad, 2008</xref>; <xref ref-type="bibr" rid="bib45">Park et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>; <xref ref-type="bibr" rid="bib65">Tobler et al., 2005</xref> <xref ref-type="bibr" rid="bib32">Kobayashi et al., 2010</xref>; <xref ref-type="bibr" rid="bib66">Tremblay and Schultz, 1999</xref>). Recent evidence for an association between neural and choice adaptation is also in line with BCV (<xref ref-type="bibr" rid="bib51">Rigoli et al., 2016a</xref>). However, key neurobiological predictions of BCV, including the specific neural mechanisms that realize choice adaptation as well as the implementation of hierarchical generative models of reward, await further investigation.</p><p>Theoretical work has indicated that adaptation of neuronal responses is consistent with efficient coding, whereby the signalling of a finite pool of neurons – with a finite dynamic range of responses – can be optimally tuned to the statistics of stimuli in the environment, so as to maximize discriminability among the stimuli (<xref ref-type="bibr" rid="bib8">Carandini and Heeger, 2012</xref>; <xref ref-type="bibr" rid="bib35">Louie et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Rangel and Clithero, 2012</xref>; <xref ref-type="bibr" rid="bib64">Summerfield and Tsetsos, 2015</xref>.) This idea has now been extended to reward processing. Here proposals diverge as to the prediction of whether adaptive neuronal coding determines either stability or adaptation in choice behaviour (<xref ref-type="bibr" rid="bib35">Louie et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Padoa-Schioppa and Rustichini, 2014</xref>; <xref ref-type="bibr" rid="bib48">Rangel and Clithero, 2012</xref>; <xref ref-type="bibr" rid="bib64">Summerfield and Tsetsos, 2015</xref>). In addressing this, BCV (similar to other Bayesian inference schemes for continuous variables) implements adaptive coding, because precision-weighted prediction error postulated to be signalled by value-processing neurons is a normalized quantity (<xref ref-type="bibr" rid="bib18">Doya et al., 2007</xref>). Also in line with previous accounts (<xref ref-type="bibr" rid="bib35">Louie et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Rangel and Clithero, 2012</xref>; <xref ref-type="bibr" rid="bib64">Summerfield and Tsetsos, 2015</xref>), BCV proposes that this normalized signal corresponds to incentive value, and that adaptive coding in the brain should be reflected in a behavioural choice adaptation. In other words, BCV implies that, as well as neural signalling, behaviour itself is tuned to the statistics of the incentives, so as to maximize discriminability among these incentives.</p><p>We highlight shortcomings of the model, though the framework itself may be fruitful in addressing some of these shortcomings. First, our focus is on scenarios where the incentive value depends on contextual information (either represented by cues or by previous rewards) provided before reward delivery. Another form of context effect on incentive value is induced by options that are simultaneously available (<xref ref-type="bibr" rid="bib37">Louie et al., 2013</xref>, <xref ref-type="bibr" rid="bib35">2015</xref>). Further theoretical work is needed to link BCV and this form of influence, though a sequential inferential process (e.g. Bayesian belief updating), similar to the process described here (possibly linked to attention) might be involved in simultaneous contextual effects. Second, our focus has been on conditions where reward is defined by a single attribute (e.g., reward amount). Contextual influences (e.g., the decoy effect) can emerge when multiple dimensions need to be evaluated and integrated, as investigated by multi-attribute theories (<xref ref-type="bibr" rid="bib27">Huber et al., 1982</xref>; <xref ref-type="bibr" rid="bib29">Johnson and Busemeyer, 2005</xref>; <xref ref-type="bibr" rid="bib56">Roe et al., 2011</xref>; <xref ref-type="bibr" rid="bib57">Simonson and Tversky, 1992</xref>; <xref ref-type="bibr" rid="bib58">Soltani et al., 2012</xref>; <xref ref-type="bibr" rid="bib69">Tsetsos et al., 2010</xref>, <xref ref-type="bibr" rid="bib67">2012</xref>, <xref ref-type="bibr" rid="bib68">2016</xref>; <xref ref-type="bibr" rid="bib70">Tversky, 1972</xref>; <xref ref-type="bibr" rid="bib71">Usher and McClelland, 2004</xref>). BCV can in principle be extended to these scenarios, for instance connecting to a body of work on multisensory integration using Bayesian principles (<xref ref-type="bibr" rid="bib19">Ernst, 2006</xref>). This would provide an opportunity to model attentional processes determining an optimal weighting of different attributes based on their importance and reliability. Third, our current formulation assumes that the model parameters are given, while these parameters need to be learned in the first place. Questions about the mechanisms that might underpin learning of generative models adopted for Bayesian inference are still largely open, though substantial contributions exist particularly in the context of structure learning (<xref ref-type="bibr" rid="bib1">Acuna and Schrater, 2009</xref>; <xref ref-type="bibr" rid="bib2">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib13">Collins and Frank, 2013</xref>; <xref ref-type="bibr" rid="bib14">Courville et al., 2006</xref>; <xref ref-type="bibr" rid="bib20">FitzGerald et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">Gershman and Niv, 2010</xref>; <xref ref-type="bibr" rid="bib41">Mathys et al., 2011</xref>).</p><p>Here, we have assumed that variables of the generative model are Gaussian. This allows us to present the model in a simple and clear way, as posterior beliefs can be inferred analytically with relatively simple equations as adopted in standard decision-making schemes (<xref ref-type="bibr" rid="bib50">Rescorla and Wagner, 1972</xref>). Though Gaussian assumptions are probably an over-simplification, with appropriate adjustments BCV can be extended to generative models with non-Gaussian variables (<xref ref-type="bibr" rid="bib28">Jazayeri and Shadlen, 2010</xref>). Indeed, the arguments behind BCV can be applied to any variables with an exponential distribution. However, the key idea (tested in our experiments – and here derived from Gaussian assumptions) that reward average and variance elicit subtractive and divisive normalization, respectively, is quite general and can also be applied, for instance, to uniform (and, in general, non-skewed) distributions.</p><p>Finally, there are questions related to psychopathology that can be fruitfully formulated in terms of BCV, for example addiction. Consider the consequences of drug misuse, including the development of tolerance (i.e., the need of increased dosages to obtain the same effects as those obtained previously) and the lack of satisfaction when engaging in activities that were pleasurable before the development of addiction. BCV interprets these effects in terms of increases in expected reward (following drug misuse) that decreases the incentive value of rewards, including the drug itself and other motivational stimuli. A similar explanation has been proposed by classical homeostatic theories, where ingestion of the drug is conceived in terms of a means to re-establish a biological set point (e.g., expressed in baseline activity of dopamine neurons), coupled with the fact that the repeated drug misuse raises this set point. BCV formalise and extend the set-point model. First, it relaxes the homeostatic assumption because incentive value depends on a reference point (i.e., the reward average), but does not correspond to distance from a set point as in homeostatic schemes. Indeed, set-point models predict that drug consumption always reduces a negative affect state by re-establishing a set point. Conversely, BCV suggests that drug consumption can decrease a negative state (when the drug-associated outcome is worse than expected) but also induce a positive affect (when the drug-associated outcome is better than expected), a prediction more consistent with empirical evidence (<xref ref-type="bibr" rid="bib55">Robinson and Berridge, 2000</xref>). Second, leveraging a Bayesian framework, BCV assigns a crucial role to reward uncertainty, above and beyond a role assigned to expected reward. For instance, increased uncertainty over prior reward beliefs may boost the magnitude of the (positive) prediction error elicited by drug consumption, hence enhancing individual predisposition to drug addiction.</p><p>In summary, we introduce a normative Bayesian model to explain the influence of contexts on incentive values. Key features of this account include an explicit generative model of reward and the assumption that incentive value corresponds to precision-weighted prediction error. This formulation implies that incentive value is intrinsically context-dependent. We tested key predictions of the model in two human experiments and show choice behaviour consistent with an adapting incentive value based on reward variance and on an average reward expected after integrating contexts at two hierarchical levels, one more general and the other more specific. An important consideration is that expression of context effect, though apparently irrational, can derive from Bayes (optimal) inference. Indeed, if incentive values are (precision-weighted) prediction errors, they are necessarily context-dependent, and this dependency can be described under a Bayes optimal scheme. We argue that this approach could be useful in generating new empirical predictions and in explaining phenomena in psychopathologies characterized by dysfunctional value attribution, such as addiction.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>36 healthy right-handed adults (19 females; 20–40 age range; mean age 26) participated in experiment one, and 32 healthy right-handed adults (18 females, aged 20–40, mean age 27) participated in experiment two. All participants had normal or corrected-to-normal vision. None had a history of head injury, a diagnosis of any neurological or psychiatric condition, or was currently on medication affecting the central nervous system. The first experiment was conducted at the Wellcome Trust Centre for Neuroimaging at the University College London and was approved by the University College of London Research Ethics Committee. The second experiment was conducted at the Institute of Psychiatry, Psychology &amp; Neuroscience at the King’s College of London and was approved by the King’s College of London Research Ethics Committee. All participants provided written informed consent and were paid for participating.</p></sec><sec id="s4-2"><title>Experimental paradigm and procedure</title><sec id="s4-2-1"><title>Experiment one</title><p>Participants performed a computer-based decision-making task lasting approximately 30 min. A monetary amount (referred as trial amount), changing trial by trial, was presented in the centre of the screen and participants had to choose whether to accept half of it for sure (pressing a left button) or select a gamble (pressing a right button). The outcomes of this choice were either zero or the full monetary amount, each with equal probability, ensuring the sure option and gamble always had the same EV. The task was organized in blocks, each associated with one of two contexts which determined the possible EVs associated with the block. These EVs were £3 and £4 for the low-variance context, and £2, £3, £4 and £5 for the high-variance context. Note that average choice EV was equal across contexts (i.e., £3.5). Contexts were cued by the associated EVs displayed on the top of the screen in brackets. Before a new block started, the statement 'New set' appeared for two seconds, followed by the contextual cue for two seconds. Next, the trial amount of the first trial was displayed followed, after the subject made a choice, by the outcome shown for one second. The possible contextual cue remained on the screen during an inter trial interval that lasted one and a half seconds. Participants had three seconds to make their choices; otherwise the statement 'too late' appeared and they received a zero outcome amount.</p><p>Eight blocks were presented, alternating between a low and high-variance context with the order counterbalanced across subjects. The former blocks comprised 40 trials each and the latter blocks comprised 80 trials, in such a way that the EVs common to both contexts (i.e., £3 and £4) were shown an equal amount of time in the two contexts. The order of trial amounts and outcomes were pseudo-randomized. At the end of the experiment, one outcome was randomly selected among those received and added to an initial participation payment of £5. Before the task, participants were fully instructed both on task contingencies and payment rules.</p></sec><sec id="s4-2-2"><title>Experiment two</title><p>Participants played a computer-based task lasting approximately 40 min. On each of the 480 trials, two rectangles representing two decks of cards appeared, one on the top and the other on the bottom of the screen. Each card was associated with a monetary amount, and the average amount of each deck was displayed in brackets upon the deck. The decks were coloured: one in gray and the other in blue and were shown for 1.5 s. On each trial a card was pseudo-randomly drawn from the blue deck and the corresponding monetary amount was presented in the middle of the screen. Participants had to choose between half of the monetary amount for sure (pressing a right button) and a gamble between the full amount and a zero outcome (pressing a left button), each with 50% chance. After choosing, the outcome appeared for one second and a new trial started immediately. If no response occurred before three seconds, a statement 'too late' was presented for one second, resulting in a zero outcome.</p><p>Among the two decks shown on the screen, the selected deck (coloured in blue) alternated pseudo-randomly over blocks (each including 5 trials). In addition, at some points during the task, the decks were replaced by new decks. Two sets of decks alternated over blocks of 20 trials in a pseudo-random way. The first deck-set (low-value deck-set) comprised decks returning £5 and £7 on average, the second deck-set (high-value deck-set) comprised decks returning £7 and £9 on average. The cards of the £5 deck could be associated with £3, £5 and £7; the cards of the £7 deck could be associated with £5, £7 and £9; the cards of the £9 deck could be associated with £7, £9 and £11. When a new deck was selected (i.e., it was coloured in blue), decks were shown for 2.5 s before the card amount appeared; when a new deck-set appeared, decks were shown for 4.5 s before the card amount appeared. At the end of the experiment, one of the outcomes was randomly selected by the computer, added to an initial payment of £5 and the total amount was paid to participants. The participants were fully instructed about task rules and about the way payment was carried out prior to task performance.</p></sec></sec><sec id="s4-3"><title>Behavioural modelling</title><p>The free parameters of the models were estimated separately for each subject using <italic>fminsearchbnd</italic> function of the Optimization toolbox in Matlab. Parameters were constrained within the following ranges: −5 and 5 for α (in both experiments), −10 and 10 for ρ (in both experiments), 0.1 and 10 for τ (in experiment one), −1 and 1 for τ<sub><italic>LC</italic></sub> and τ<sub><italic>HC</italic></sub> (in experiment two). In addition, to minimize the effect of biased outlier estimates, Gaussian priors with mean zero (<xref ref-type="bibr" rid="bib16">Daw, 2011</xref>) were used for estimation of log(τ) (in experiment one) and estimation of τ<sub><italic>LC</italic></sub> and τ<sub><italic>HC</italic></sub> (in experiment two). Starting values for parameter estimation was zero for all parameters, except for the context parameter τ in experiment one for which it was one. Distributions of estimated parameters are reported in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref> (experiment one) and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplements 1</xref> (experiment two), and show no evidence of outliers (i.e., scores larger or smaller than 3 SD compared to the mean) for any of the parameters.</p><p>For each model, the log-likelihood of the choice data given the best fitting parameters (estimated by the method described above) was computed subject by subject and summed across subjects. We compared the full model with nested models, namely where one or more parameters were fixed to zero. To do this, we used the standard approach of the likelihood-ratio test (<xref ref-type="bibr" rid="bib9">Casella and Berger, 2002</xref>; <xref ref-type="bibr" rid="bib16">Daw, 2011</xref>), which allows for a comparison of nested models. This is based on the fact that the difference in negative log-likelihood times two (2<italic>d</italic>) between a nested and a more complex model follows a chi-square distribution, where the number of degrees of freedom is equal to the number of additional parameters of the more complex model. A chi-square test can be performed to estimate the probability that the observed <italic>2d</italic> is due to chance under the null hypothesis that data are generated by the nested model, allowing acceptance or rejection of the null hypothesis.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Wellcome Trust (Ray Dolan Senior Investigator Award 098362/Z/12/Z) and the Max Planck Society. The Wellcome Trust Centre for Neuroimaging is supported by core funding from the Wellcome Trust 091593/Z/10/Z. SSS is supported by a consolidator award from the European Research Council. C.M is funded by the NIHR-BRC at South London and Maudsley NHS Foundation Trust and Institute of Psychiatry, Psychology and Neuroscience King’s College London via a research studentship.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>FR, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>KJF, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>CM, Acquisition of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con4"><p>MS, Acquisition of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con5"><p>SSS, Acquisition of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con6"><p>RJD, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Experiment one was approved by the University College London Research Ethics Committee. Experiment two was approved by the King's College of London Research Ethics Committee. All participants provided written informed consent and were paid for participating.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.16127.011</object-id><label>Source code 1.</label><caption><title>Gambling proportion for the different conditions of experiment one.</title><p>Rows indicate participants and columns indicate conditions (from left to right: £2, £3, £4, £5 EV in the high-variance context, and £3 and £4 EV in the low variance context).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.011">http://dx.doi.org/10.7554/eLife.16127.011</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-16127-code1-v2.zip"/></supplementary-material><supplementary-material id="SD2-data"><object-id pub-id-type="doi">10.7554/eLife.16127.012</object-id><label>Source code 2.</label><caption><title>Data from experiment two. Rows indicate participants and columns indicate data.</title><p>The first twelve columns report the gambling proportion for the different conditions (from left to right: HV-deck-set, £9 deck, £11 card; HV-deck-set, £9 deck, £9 card; HV-deck-set, £9 deck, £7 card; HV-deck-set, £7 deck, £9 card; HV-deck-set, £7 deck, £7 card; HV-deck-set, £7 deck, £5 card; LV-deck-set, £7 deck, £9 card; LV-deck-set, £7 deck, £7 card; LV-deck-set, £7 deck, £5 card; LV-deck-set, £5 deck, £7 card; LV-deck-set, £5 deck, £5 card; LV-deck-set, £5 deck, £3 card). Columns 13 and 14 report the average gambling proportion and the effect of card amount on gambling probability (i.e., i.e., the associated regression coefficient of the logistic regression model), respectively. For each deck-set, the difference in gambling between lower and higher value decks for amounts common to both decks (corresponding to £5 and £7 in the low-value deck-set, and to £7 and £9 in the high-value deck-set) was computed and the mean of these two differences is reported in column 15. Column 16 reports the difference in gambling between the low and high value deck-set for the £7 deck (common to both deck-sets).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16127.012">http://dx.doi.org/10.7554/eLife.16127.012</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-16127-code2-v2.zip"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acuña</surname><given-names>DE</given-names></name><name><surname>Schrater</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Structure learning in human sequential decision-making</article-title><source>PLOS Computational Biology</source><volume>6</volume><fpage>1001003</fpage><lpage>1001008</lpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1001003</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1038/nn1954</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bermudez</surname><given-names>MA</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Reward magnitude coding in primate amygdala neurons</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>3424</fpage><lpage>3432</lpage><pub-id pub-id-type="doi">10.1152/jn.00540.2010</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Pattern Recognition and Machine Learning</source><publisher-loc>Germany</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blakemore</surname><given-names>SJ</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Spatio-temporal prediction modulates the perception of self-produced stimuli</article-title><source>Journal of Cognitive Neuroscience</source><volume>11</volume><fpage>551</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1162/089892999563607</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Toussaint</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Planning as inference</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>485</fpage><lpage>488</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.08.006</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>H</given-names></name><name><surname>Adams</surname><given-names>RA</given-names></name><name><surname>Parees</surname><given-names>I</given-names></name><name><surname>Edwards</surname><given-names>M</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Active inference, sensory attenuation and illusions</article-title><source>Cognitive Processing</source><volume>14</volume><fpage>411</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1007/s10339-013-0571-3</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Casella</surname><given-names>G</given-names></name><name><surname>Berger</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Statistical Inference</source><publisher-loc>Pacific Grove, CA</publisher-loc><publisher-name>Duxbury</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chater</surname><given-names>N</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Yuille</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Probabilistic models of cognition: conceptual foundations</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>287</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.05.007</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheadle</surname><given-names>S</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name><name><surname>Myers</surname><given-names>N</given-names></name><name><surname>de Gardelle</surname><given-names>V</given-names></name><name><surname>Herce Castañón</surname><given-names>S</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adaptive gain control during human perceptual choice</article-title><source>Neuron</source><volume>81</volume><fpage>1429</fpage><lpage>1441</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.020</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title><source>The Behavioral and Brain Sciences</source><volume>36</volume><fpage>181</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AG</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cognitive control over learning: creating, clustering, and generalizing task-set structure</article-title><source>Psychological Review</source><volume>120</volume><fpage>190</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1037/a0030852</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Courville</surname><given-names>AC</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian theories of conditioning in a changing world</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>294</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.05.004</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>KM</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>BOLD subjective value signals exhibit robust range adaptation</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>16533</fpage><lpage>16543</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3927-14.2014</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Trial-by-trial data analysis using computational models</article-title><source>Decision Making, Affect, and Learning: Attention and Performance XXIII</source><volume>23</volume><fpage>3</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1093/acprof:oso/9780199600434.003.0001</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Neal</surname><given-names>RM</given-names></name><name><surname>Zemel</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The Helmholtz machine</article-title><source>Neural Computation</source><volume>7</volume><fpage>889</fpage><lpage>904</lpage><pub-id pub-id-type="doi">10.1162/neco.1995.7.5.889</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Doya</surname><given-names>K</given-names></name><name><surname>Ishii</surname><given-names>S</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Rao</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Bayesian Brain: Probabilistic Approach to Neural Coding and Learning</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2006">2006</year><chapter-title>A Bayesian view on multimodal cue integration</chapter-title><source>Human Body Perception From the Inside Out</source><fpage>p 105</fpage><lpage>p 131</lpage></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>FitzGerald</surname><given-names>THB</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Model averaging, optimal inference, and habit formation</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>457</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00457</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Ognibene</surname><given-names>D</given-names></name><name><surname>Mathys</surname><given-names>C</given-names></name><name><surname>Fitzgerald</surname><given-names>T</given-names></name><name><surname>Pezzulo</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Active inference and epistemic value</article-title><source>Cognitive Neuroscience</source><volume>6</volume><fpage>187</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1080/17588928.2015.1020053</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Schwartenbeck</surname><given-names>P</given-names></name><name><surname>Fitzgerald</surname><given-names>T</given-names></name><name><surname>Moutoussis</surname><given-names>M</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The anatomy of choice: active inference and agency</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>598</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00598</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The free-energy principle: a unified brain theory?</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>127</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nrn2787</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Learning latent structure: carving nature at its joints</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>251</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.02.008</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Fast sampling-based inference in balanced neuronal networks</chapter-title><source>Advances in Neural Information Processing Systems</source><fpage>2240</fpage><lpage>2248.</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>J</given-names></name><name><surname>Payne</surname><given-names>JW</given-names></name><name><surname>Puto</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Adding asymmetrically dominated alternatives: violations of regularity and the similarity hypothesis</article-title><source>Journal of Consumer Research</source><volume>9</volume><fpage>90</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1086/208899</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Temporal context calibrates interval timing</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1020</fpage><lpage>1026</lpage><pub-id pub-id-type="doi">10.1038/nn.2590</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>JG</given-names></name><name><surname>Busemeyer</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A dynamic, stochastic, computational model of preference reversal phenomena</article-title><source>Psychological Review</source><volume>112</volume><fpage>841</fpage><lpage>861</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.112.4.841</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahneman</surname><given-names>D</given-names></name><name><surname>Tversky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Prospect theory: An analysis of decision under risk</article-title><source>Econometrica</source><volume>47</volume><fpage>263</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.2307/1914185</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title><source>Trends in Neurosciences</source><volume>27</volume><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname><given-names>S</given-names></name><name><surname>Pinto de Carvalho</surname><given-names>O</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Adaptation of reward sensitivity in orbitofrontal neurons</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>534</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4009-09.2010</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kőszegi</surname><given-names>B</given-names></name><name><surname>Rabin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A model of reference-dependent preferences</article-title><source>The Quarterly Journal of Economics</source><volume>121</volume><fpage>1133</fpage><lpage>1165</lpage></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kőszegi</surname><given-names>B</given-names></name><name><surname>Szeidl</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A model of focusing in economic choice</article-title><source>The Quarterly Journal of Economics</source><volume>128</volume><fpage>53</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1093/qje/qjs049</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Webb</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adaptive neural coding: from biological to behavioral decision-making</article-title><source>Current Opinion in Behavioral Sciences</source><volume>5</volume><fpage>91</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2015.08.008</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Grattan</surname><given-names>LE</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reward value-based gain control: divisive normalization in parietal cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>10627</fpage><lpage>10639</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1237-11.2011</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Khaw</surname><given-names>MW</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Normalization is a general neural mechanism for context-dependent decision making</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>110</volume><fpage>6139</fpage><lpage>6144</lpage><pub-id pub-id-type="doi">10.1073/pnas.1217854110</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>LoFaro</surname><given-names>T</given-names></name><name><surname>Webb</surname><given-names>R</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic divisive normalization predicts time-varying value coding in decision-related circuits</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>16046</fpage><lpage>16057</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2851-14.2014</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludvig</surname><given-names>EA</given-names></name><name><surname>Madan</surname><given-names>CR</given-names></name><name><surname>Spetch</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Extreme Outcomes Sway Risky Decisions from Experience</article-title><source>Journal of Behavioral Decision Making</source><volume>27</volume><fpage>146</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1002/bdm.1792</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision: A Computational Investigation Into the Human Representation and Processing of Visual Information</source><publisher-loc>New York</publisher-loc><publisher-name>Freeman</publisher-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathys</surname><given-names>C</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A bayesian foundation for individual learning under uncertainty</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><elocation-id>39</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2011.00039</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The representation of economic value in the orbitofrontal cortex is invariant for changes of menu</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>95</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1038/nn2020</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Rustichini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rational attention and adaptive coding: A puzzle and a solution</article-title><source>American Economic Review</source><volume>104</volume><lpage>507</lpage><pub-id pub-id-type="doi">10.1257/aer.104.5.507</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Range-adapting representation of economic value in the orbitofrontal cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>14004</fpage><lpage>14014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3751-09.2009</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>SQ</given-names></name><name><surname>Kahnt</surname><given-names>T</given-names></name><name><surname>Talmi</surname><given-names>D</given-names></name><name><surname>Rieskamp</surname><given-names>J</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Heekeren</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Adaptive coding of reward prediction errors is gated by striatal coupling</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>109</volume><fpage>4285</fpage><lpage>4289</lpage><pub-id pub-id-type="doi">10.1073/pnas.1119969109</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezzulo</surname><given-names>G</given-names></name><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Active Inference, homeostatic regulation and adaptive behavioural control</article-title><source>Progress in Neurobiology</source><volume>134</volume><fpage>17</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2015.09.001</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezzulo</surname><given-names>G</given-names></name><name><surname>Rigoli</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The value of foresight: how prospection affects decision-making</article-title><source>Frontiers in Neuroscience</source><volume>5</volume><elocation-id>79</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2011.00079</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rangel</surname><given-names>A</given-names></name><name><surname>Clithero</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Value normalization in decision making: theory and evidence</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>970</fpage><lpage>981</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.07.011</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rescorla</surname><given-names>RA</given-names></name><name><surname>Wagner</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</chapter-title><source>Classical Conditioning II: Current Research and Theory</source><fpage>64</fpage><lpage>99</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Rutledge</surname><given-names>RB</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>The influence of contextual reward statistics on risk preference</article-title><source>NeuroImage</source><volume>128</volume><fpage>74</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.016</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Rutledge</surname><given-names>RB</given-names></name><name><surname>Chew</surname><given-names>B</given-names></name><name><surname>Ousdal</surname><given-names>OT</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Dopamine Increases a Value-Independent Gambling Propensity</article-title><source>Neuropsychopharmacology</source><pub-id pub-id-type="doi">10.1038/npp.2016.68</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Pezzulo</surname><given-names>G</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2016">2016c</year><article-title>Prospective and Pavlovian mechanisms in aversive behaviour</article-title><source>Cognition</source><volume>146</volume><fpage>415</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2015.10.017</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Chew</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2016">2016d</year><article-title>The dopaminergic midbrain mediates an effect of average reward on pavlovian vigor</article-title><source>Journal of Cognitive Neuroscience</source><volume>13</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00972</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>TE</given-names></name><name><surname>Berridge</surname><given-names>KC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The psychology and neurobiology of addiction: an incentive-sensitization view</article-title><source>Addiction</source><volume>95 Suppl 2</volume><fpage>S91</fpage><lpage>S117</lpage></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roe</surname><given-names>RM</given-names></name><name><surname>Busemeyer</surname><given-names>JR</given-names></name><name><surname>Townsend</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Multialternative decision field theory: a dynamic connectionist model of decision making</article-title><source>Psychological Review</source><volume>108</volume><fpage>370</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.108.2.370</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonson</surname><given-names>I</given-names></name><name><surname>Tversky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Choice in Context: Tradeoff Contrast and Extremeness Aversion</article-title><source>Journal of Marketing Research</source><volume>29</volume><fpage>281</fpage><pub-id pub-id-type="doi">10.2307/3172740</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soltani</surname><given-names>A</given-names></name><name><surname>De Martino</surname><given-names>B</given-names></name><name><surname>Camerer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A range-normalization model of context-dependent choice: a new model and evidence</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002607</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002607</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solway</surname><given-names>A</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Goal-directed decision making as probabilistic inference: a computational framework and potential neural correlates</article-title><source>Psychological Review</source><volume>119</volume><lpage>120</lpage><pub-id pub-id-type="doi">10.1037/a0026435</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>N</given-names></name><name><surname>Chater</surname><given-names>N</given-names></name><name><surname>Brown</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Decision by sampling</article-title><source>Cognitive Psychology</source><volume>53</volume><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2005.10.003</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>N</given-names></name><name><surname>Chater</surname><given-names>N</given-names></name><name><surname>Stott</surname><given-names>HP</given-names></name><name><surname>Reimers</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Prospect relativity: How choice options influence decision under risk</article-title><source>Journal of Experimental Psychology: General</source><volume>132</volume><fpage>23</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.132.1.23</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decision by sampling: The role of the decision environment in risky choice</article-title><source>The Quarterly Journal of Experimental Psychology</source><volume>62</volume><fpage>1041</fpage><lpage>1062</lpage><pub-id pub-id-type="doi">10.1080/17470210902747112</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Building Bridges between Perceptual and Economic Decision-Making: Neural and Computational Mechanisms</article-title><source>Frontiers in Neuroscience</source><volume>6</volume><elocation-id>70</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2012.00070</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Do humans make good decisions?</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>27</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.11.005</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Fiorillo</surname><given-names>CD</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Adaptive coding of reward value by dopamine neurons</article-title><source>Science</source><volume>307</volume><fpage>1642</fpage><lpage>1645</lpage><pub-id pub-id-type="doi">10.1126/science.1105370</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tremblay</surname><given-names>L</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Relative reward preference in primate orbitofrontal cortex</article-title><source>Nature</source><volume>398</volume><fpage>704</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1038/19525</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsetsos</surname><given-names>K</given-names></name><name><surname>Chater</surname><given-names>N</given-names></name><name><surname>Usher</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Salience driven value integration explains decision biases and preference reversal</article-title><source>Proceedings of the National Academy of Sciences of United States of America</source><volume>109</volume><fpage>9659</fpage><lpage>9664</lpage><pub-id pub-id-type="doi">10.1073/pnas.1119569109</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsetsos</surname><given-names>K</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Moreland</surname><given-names>J</given-names></name><name><surname>Chater</surname><given-names>N</given-names></name><name><surname>Usher</surname><given-names>M</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Economic irrationality is optimal during noisy decision making</article-title><source>Proceedings of the National Academy of Sciences of United States of America.</source><volume>113</volume><fpage>3102</fpage><lpage>3107</lpage><pub-id pub-id-type="doi">10.1073/pnas.1519157113</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsetsos</surname><given-names>K</given-names></name><name><surname>Usher</surname><given-names>M</given-names></name><name><surname>Chater</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Preference reversal in multiattribute choice</article-title><source>Psychological Review</source><volume>117</volume><lpage>1275</lpage><pub-id pub-id-type="doi">10.1037/a0020580</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tversky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Elimination by aspects: A theory of choice</article-title><source>Psychological Review</source><volume>79</volume><fpage>281</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1037/h0032955</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Usher</surname><given-names>M</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Loss aversion and inhibition in dynamical models of multialternative choice</article-title><source>Psychological Review</source><volume>111</volume><lpage>757</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.111.3.757</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vlaev</surname><given-names>I</given-names></name><name><surname>Chater</surname><given-names>N</given-names></name><name><surname>Stewart</surname><given-names>N</given-names></name><name><surname>Brown</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Does the brain calculate value?</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>546</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.09.008</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>von Neumann</surname><given-names>J</given-names></name><name><surname>Morgenstern</surname><given-names>O</given-names></name></person-group><year iso-8601-date="1944">1944</year><source>Theory of Games and Economic Behavior</source><publisher-loc>Princeton</publisher-loc><publisher-name>Princeton University Press</publisher-name></element-citation></ref></ref-list><app-group><app id="app1"><title>Appendix</title><boxed-text><sec id="s15" sec-type="appendix"><p>We derive <xref ref-type="disp-formula" rid="equ7">equation 7</xref> from the generative model shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>. A higher-level contextual variable (e.g., the neighbourhood) is represented by a Gaussian distribution with mean <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> equal to zero and variance <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, from which a value HC is sampled. Sensory evidence about HC is provided and represented by HO which is sampled from a Gaussian distribution with mean HC and variance <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. A lower-level contextual variable (e.g., the restaurant) is represented by a (Gaussian) distribution with mean HC and variance <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, from which a value LC is sampled. Sensory evidence about LC is provided and represented by LO which is sampled from a Gaussian distribution with mean LC and variance <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> A reward is obtained and sampled from a Gaussian distribution with mean LC and variance <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. We propose that agents infer the posterior distribution P(LC|HO,LO,R) sequentially by estimating, in the order, P(HC|HO), P(LC|HO), P(LC|HO,LO), and P(LC|HO,LO,R). The posterior mean of P(HC|HO) is:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>And the posterior variance:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The posterior mean of P(LC|HO) is equal to <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> , while the posterior variance is:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The posterior mean of P(LC|HO,PO) is:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>And the posterior variance:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Finally, the posterior mean of P(LC|HO,LO,R) is:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Implying (with few algebraic transformations) the following incentive value for the reward:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mi>L</mml:mi><mml:mi>O</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This equation implements three normalization factors: (i) a subtractive normalization factor <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mi>L</mml:mi><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> proportional to the value LO observed at the low contextual level, (ii) a subtractive normalization factor <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> proportional to the value HO observed at the high contextual level, (iii) a divisive normalization factor <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi><mml:mspace linebreak="newline"/><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that captures the weighting dependent on the (relative) reward variance. If we define the three factors as and and K respectively, we obtain <xref ref-type="disp-formula" rid="equ7">equation 7</xref>.</p></sec></boxed-text></app></app-group></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.16127.014</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gershman</surname><given-names>Sam</given-names></name><aff id="aff6"><institution>Harvard University</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;A Bayesian model of context-sensitive value attribution&quot; for consideration by <italic>eLife</italic>. Your article has been favorably evaluated by Sabine Kastner (Senior Editor) and three reviewers, one of whom, Sam Gershman (Reviewer #1), also served as Guest Reviewing Editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This work develops a Bayesian theory of context effects in decision making, and tests it in a behavioral experiment. The proposed model goes beyond previous efforts by introducing multiple levels of hierarchy. The reviewers agree that this is an interesting and important topic, but have concerns about the analysis methodology, presentation of the model/results, adequacy of the experimental data, and relation to previous work.</p><p>Essential revisions:</p><p>1) Analysis methodology: please address the points raised by Reviewers 1 and 3 concerning statistical analyses and model comparison.</p><p>2) Presentation of the model/results: all the reviewers suggested ways in which the presentation could be improved, including clarification of modeling details and task description.</p><p>3) Adequacy of experimental data: Reviewer 2 raised important concerns about whether the reported experiment provides a strong test of the theory (particularly the precision-weighting component), and Reviewer 3 pointed out a number of confounds in the design. The reviewers are in agreement that the paper should include new experimental data that addresses the confounds and ideally also provides a stronger test of the theory.</p><p>4) Relation to previous work: Reviewer 1 points out several other theoretical frameworks for certain context effects. Please address these in the revision.</p><p><italic>Reviewer #1:</italic></p><p>1) I didn't see what would seem to be the most obvious analysis of the choice data, namely comparing the LV vs. HV bars (as well as 5 vs. 9 bars) in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. The authors report correlations but shouldn't the model predict a difference in means as in <xref ref-type="fig" rid="fig2">Figure 2</xref>?</p><p>2) The simulation details were unclear to me. Did the authors simulate a synthetic dataset many times and then average the correlations, or did they just run a single simulation? I think the former procedure is better justified, since we don't know whether the results found with the simulations are idiosyncratic or reliable.</p><p>3) Some researchers, such as Rangel &amp; Clithero (2012) and Louie, Glimcher &amp; Webb (2015), have drawn connections between context effects and efficient coding, appealing to the idea that divisive normalization is a mechanism for removing statistical redundancies. A Bayesian theory like BCV enables but does not require efficient coding; a given distribution could be coded with varying degrees of efficiency. However, BCV also appears to make some claims about mechanistic implementation which might be relevant to the question of efficiency. How do the authors see the relationship between these theories and BCV? More generally, the authors could deepen their contribution by considering the realization of BCV across Marr's levels.</p><p>4) Showing that BCV can account for earlier value normalization results would also bolster the theory. In addition, there is a rich literature on context/decoy effects that would be relevant to at least mention here. Relatedly, work in economics has studied the idea that reference points depend on expectations (e.g., Koszegi &amp; Rabin, 2006), and there have been a number of important recent papers on range effects and relativistic choice processes (e.g., Bordalo et al., 2012; Cunningham, 2013,; Bushong, Rabin &amp; Schwartzstein, 2015). It would be illuminating to better understand how these frameworks relate to BCV.</p><p><italic>Reviewer #2:</italic></p><p>My major reservation about the paper is that the human data offers only partial confirmation for the Bayesian context model. In its full specification (in the Appendix), the BCV model predicts that incentive value will depend on a divisive scaling term (implementing precision weighting dependent on relative reward variance) and subtractive prediction error terms (low and high context reward predictions, weighted by their relative contextual cue variances). However, the effects of precision – which are a key element of Bayesian approaches – are untested in the analysis of the experimental data. As the authors state in the paper, reward variance is not manipulated and the divisive term (K) cannot be examined. In addition, the subtractive terms should also be precision-weighted by terms relating posterior and prior variances (denoted by tau_LO and tau_HO). However, the results only speak to overall context effects, essentially asking whether there is an overall effect of low and high context on gambling (population regression effects in <xref ref-type="fig" rid="fig5">Figure 5</xref>, and the chi-square tests showing positive tau_LO and tau_HO parameters).</p><p>The issue is that adaptive effects to average rewards are well known (for example, successive contrast effects in the animal literature and reference point models like prospect theory); without validating the precision-dependent predictions of their model, I'm not sure that the authors can convincingly argue that BCV is a more appropriate model – particularly as other models are not tested. Given the experimental setup, testing the divisive weighting term is not possible in this dataset; however, can the authors make any predictions about not just the significance but the relative magnitude of the weighting factors tau_LO and tau_HO (predicted and fit to data)?</p><p><italic>Reviewer #3:</italic></p><p>1) Behavioural effects of reward context on incentive value are already well-established and even the specific paradigm used here has already been published by the same authors. Is there any new, surprising behavioural effect that follows from the new model?</p><p>2) The Bayesian model presented in the manuscript is not formally compared to other well-established models that may similarly account for the behavioural effects. The authors should show with formal model comparisons that their model outperforms other classic (non-Bayesian) models commonly employed to model context effects on value-based choice.</p><p>3) The model is not biologically realistic. This is not always a problem; in fact, there are many elegant demonstrations that Bayesian frameworks can account for optimal performance in various domains better than other accounts. However, in the specific context of reward-guided decision-making, it is unclear why and how a Bayesian framework should apply, and to what degree it is more consistent with behaviour and the underlying neural computations. Please provide a lot more information on how this model may be implemented by neural computations. In particular, it would help if there was any empirical evidence for the hierarchical representation of reward context.</p><p>4) The Abstract claims that the model &quot;generates new empirical predictions and may help explain important phenomena in psychopathologies such as addiction.&quot; I found the corresponding text in the Discussion rather vague. Please provide explicit predictions for specific experimental effects that follow from this model and please explain much more concretely which important phenomena in psychopathologies are explained by it.</p><p>5) The manuscript claims in several places that a reward's incentive value corresponds to the (precision-weighted) prediction error. This is misleading. By definition, the incentive value is the property of a stimulus/expected reward that triggers approach behaviour and choice of the corresponding option. This representation must therefore be computed before the choice is taken and the reward is obtained. The prediction error, by contrast, is the deviation of the reward obtained as a consequence of the choice from the reward expected prior to the choice. This post-choice representation can therefore not be the incentive value guiding choice. The authors need to clarify their terminology and ensure that they remain consistent with established definitions in the literature.</p><p>In addition to the conceptual points listed above, the manuscript also has shortcomings with respect to methodology and results presentation that will need to be addressed:</p><p>6) The behavioural task was not designed to allow proper tests of the full model. Some of these problems are listed by the authors themselves and have led to adaptations of the model so that it could be fit to the data. For instance, reward variance in the different decks is not varied, is heavily constrained (there are only 3 different reward values per deck), and is perfectly correlated with average reward magnitude. To properly test whether the precision of reward prediction errors established by the different contexts really plays an important role, the authors should fit the full model to datasets with contexts that differ substantially in their reward variance and that disentangle reward magnitude from reward variance. Moreover, the contexts should be associated with a lot more than just three possible reward values so that the form of the expected reward distributions can be properly approximated (see below).</p><p>7) The model specification does not match the environment established by the behavioural task. The decks were associated with 3 equiprobable values per deck. Therefore, &quot;smart&quot; subjects would employ a flat discrete expectation of the three possible values within the given context. In contrast, the modelling solution is based on continuous (Gaussian) distributions that are not restricted by the bounds imposed by each context. There are two problems with this: (1) If we assume that the subjects indeed optimally integrate all information, then a flat prior belief bounded by the context's minimum and maximum reward would be accurate. This cannot be modelled by the presented specification. (2) Even if subjects employed continuous Gaussian priors to model reward expectations, such distributions would probably not be narrow enough with respect to the context bounds (i.e., the priors would wrongly lead to expectations of rewards that are outside of the bounded scale). These problems will probably be evident if the authors report the values of the latent variables after fitting the model to the empirical data. Please include such a table to allow the reader to inspect this issue.</p><p>In my view, if the authors really wanted to maintain a Bayesian optimal observer model, then they should examine how the prior (potentially flat, but can also be modelled if the authors like) is combined with the likelihood of the actual numeric representation to obtain a posterior estimate that should naturally occur within the actual numeric bounds of the context (for an example on how to formally deal with Bayesian problems of this kind, i.e. bounded contexts, see Jazayeri and Shadlen 2010, Nature Neuroscience). This formal specification can then be expanded to the interesting contextual hierarchical framework that the authors propose in their study.</p><p>8) The authors rescaled the contextual averages and reward values to perform their model fits. Why? A correctly specified model should be able to take as inputs the actual values of the contexts and rewards of their behavioural paradigm (which are all single-digit numbers after all). This would help to assess the model's explanatory power.</p><p>9) The authors perform model comparison by summing log-likelihoods across participants. I find the selection of this approach for model comparison surprising, given that several of the co-authors have pushed the use of precise Bayesian model selection methods that properly account for the complexity and variability of the model fits across trials and participants. The authors should employ such methods and should provide values quantifying the quality of the model fits after penalizing for model complexity.</p><p>10) The behavioural task is described in a fashion that makes it hard to replicate. For instance, is it true that on every trial, a card was drawn from the blue deck? If so, how were the different deck contexts varied across the different colours? Please make sure the task described in sufficient detail so that another person could program it.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.16127.015</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>1) Analysis methodology: please address the points raised by Reviewers 1 and 3 concerning statistical analyses and model comparison.</italic></p><p>We believe that all the points concerning statistical analyses and model comparison have been addressed satisfactorily. Please see specific replies to reviewers below.</p><p><italic>2) Presentation of the model/results: all the reviewers suggested ways in which the presentation could be improved, including clarification of modeling details and task description.</italic></p><p>We are grateful for the feedback from reviewers on this aspect, and we have clarified the presentation of the model and task description (see replies below).</p><p><italic>3) Adequacy of experimental data: Reviewer 2 raised important concerns about whether the reported experiment provides a strong test of the theory (particularly the precision-weighting component), and Reviewer 3 pointed out a number of confounds in the design. The reviewers are in agreement that the paper should include new experimental data that addresses the confounds and ideally also provides a stronger test of the theory.</italic></p><p>Thanks for the feedback on this point. We have clarified why we believe that the experiment is adequately designed to investigate a key prediction of BCV in relation to contextual influences at multiple hierarchical levels. However, we also agree that the previous experiment is not suitable to test other key predictions of the theory; most importantly the effect of precision as suggested by reviewers.</p><p>Investigating the role of precision is something we already had in mind when we had completed this study. Indeed, we already had run an experiment investigating the role of context precision in value adaptation. As the reviewers have highlighted this point we are delighted to be able to include these new data in our revision. These data provide supportive evidence for a role of context precision, consistent with BCV. As pointed out by the reviewers, such role is a key feature of Bayesian schemes and is something which specifically supports BCV.</p><p>In sum, we now report a new experiment (along the lines suggested by the reviewers) investigating the role of reward variance (experiment one) together with the previous experiment investigating the role of hierarchical contexts (experiment two).</p><p><italic>4) Relation to previous work: Reviewer 1 points out several other theoretical frameworks for certain context effects. Please address these in the revision.</italic></p><p>Thanks for feedback on this point. We now link our work more extensively with other theoretical frameworks (and we compare explicitly predictions from different frameworks; see replies below).</p><p><italic>Reviewer #1:</italic> </p><p><italic>1) I didn't see what would seem to be the most obvious analysis of the choice data, namely comparing the LV vs. HV bars (as well as 5 vs. 9 bars) in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. The authors report correlations but shouldn't the model predict a difference in means as in <xref ref-type="fig" rid="fig2">Figure 2</xref>?</italic></p><p>Thanks for the feedback on this point, and we agree this needs to be explained better. <xref ref-type="fig" rid="fig2">Figure 2</xref> shows predictions about incentive value V(R) postulated by BCV in the different conditions of our experiments. Crucially, the predicted V(R) does not correspond to the predicted gambling proportion, as (based on previous findings; Rigoli et al., 2016a; 2016b) we expected the gambling proportion to depend on V(R) in a different way for different participants. Specifically, we predicted that the incentive value determines choice behaviour according to a logistic regression P(gambling) = sigmoid(α V(R)+ρ), where the slope α is positive or negative for different participants. Given this prediction and the adaptation processes postulated by BCV: if we consider two contexts with common trial EVs, we expected that participants with positive α would gamble more for common EVs when these are associated with larger V(R), while we expected that participants with negative α would gamble more for common EVs when these are associated with smaller V(R). To test these predictions we correlated α (i.e., the slope of the logistic regression) with the difference in gambling for common EVs. Note that these predictions do not imply an overall difference in mean gambling across contexts, as the gambling difference across contexts is expected to differ when comparing participants with positive and negative α. This issue is now clarified in analysis of experiment one:</p><p>“Though we observed no overall difference in gambling across participants for £4 and £3 EV (see above), participants could be differentiated based on those who gambled more with £4 or £3 EV. […] To examine these predictions we tested for an interaction, corresponding to the differential gambling percentage across contexts (low-variance minus high-variance context) for £4 choices minus the differential gambling percentage across contexts (low-variance minus high-variance context) for £3 choices.”</p><p>In analysis of experiment two:</p><p>“Though across participants we observed no overall effect of option EV on gambling, participants could be differentiated based on those who showed a positive or negative effect of option EV on gambling. […] We investigated this hypothesis both at the level of decks and deck-sets (<xref ref-type="fig" rid="fig6">Figure 6</xref>).”</p><p>And in the caption of <xref ref-type="fig" rid="fig6">Figure 6</xref>:</p><p>“Gambling proportion for different card amounts and different context conditions, separately for (A) participants showing a negative effect of card monetary amount (i.e., the slope of the logistic regression model with card amount as predictor) on gambling (n = 19) and (B) participants showing a positive effect of card amount on gambling (n = 13). […] These data are consistent with our hypotheses; namely (i) with predictions arising from BCV (<xref ref-type="fig" rid="fig2">Figure 2D</xref>) which implies subtractive normalization of incentive value at both hierarchical levels and (ii) with the prediction (derived from previous observations; Rigoli et al., 2016a; 2016b) that the influence of incentive value on gambling proportion depends on the individual preference to gamble with large or small card amounts.”</p><p><italic>2) The simulation details were unclear to me. Did the authors simulate a synthetic dataset many times and then average the correlations, or did they just run a single simulation? I think the former procedure is better justified, since we don't know whether the results found with the simulations are idiosyncratic or reliable.</italic></p><p>The referee is correct and in the initial submission of the manuscript a single simulation was employed, but we agree that a better method would entail repeating several simulations and considering the average correlation from these simulations. We have followed this suggestion and this method is now adopted in the new manuscript, where 100 simulations where considered and the average correlation reported. This is now clarified:</p><p>“Finally, we used the (selected) model and subject-specific parameter estimates of the last analysis to generate simulated choice behaviour and performed behavioural analyses on the ensuing data (data were simulated 100 times and the average statistics are reported).”</p><p><italic>3) Some researchers, such as Rangel &amp; Clithero (2012) and Louie, Glimcher &amp; Webb (2015), have drawn connections between context effects and efficient coding, appealing to the idea that divisive normalization is a mechanism for removing statistical redundancies. A Bayesian theory like BCV enables but does not require efficient coding; a given distribution could be coded with varying degrees of efficiency. However, BCV also appears to make some claims about mechanistic implementation which might be relevant to the question of efficiency. How do the authors see the relationship between these theories and BCV? More generally, the authors could deepen their contribution by considering the realization of BCV across Marr's levels.</italic></p><p>Thanks for this suggestion for greater elaboration on this point. We agree a crucial aspect of any theory of value adaptation needs to address implications for efficient coding. This is now discussed here, together with a more general framing of the theory within the Marr’s model, as proposed by the reviewer:</p><p>“With reference to the three levels of analysis (i.e., computational, algorithmic and implementation) proposed by Marr (1982), BCV speaks to the computational level as it focuses on normative principles (implicit in optimal Bayesian inference) proposed to explain value and choice adaptation. […] In other words, BCV implies that, as well as neural signalling, behaviour itself is tuned to the statistics of the incentives, so as to maximize discriminability among these incentives.”</p><p><italic>4) Showing that BCV can account for earlier value normalization results would also bolster the theory. In addition, there is a rich literature on context/decoy effects that would be relevant to at least mention here. Relatedly, work in economics has studied the idea that reference points depend on expectations (e.g., Koszegi &amp; Rabin, 2006), and there have been a number of important recent papers on range effects and relativistic choice processes (e.g., Bordalo et al., 2012</italic>; <italic>Cunningham, 2013; Bushong, Rabin &amp; Schwartzstein, 2015). It would be illuminating to better understand how these frameworks relate to BCV.</italic></p><p>We agree it is relevant to discuss the theory as it relates to previous models mentioned by the reviewer (and other classical theories). We now do this:</p><p>“The proposal advanced here has some similarities with classical theories of value, such as Expected Utility theory (von Neumann &amp; Morgenstern, 1944) and Prospect theory (Kahneman &amp; Tversky, 1979). […] For instance, BCV predicts that divisive normalization derives from the gain term (i.e., reward variance) and not from the expected reward (Louie et al., 2014; 2015), and that the reward variance divides – and not multiplies (Kőszegi &amp; Szeidl, 2014) – the prediction error. Importantly, these predictions are not ad hoc but derive necessarily from Bayesian assumptions.”</p><p>In addition, we agree it is important to discuss the theory in light of earlier empirical results. As also indicated by the reviewer, most empirical studies have focused on effects elicited by options simultaneously available during choice and where multiple attributes are presented (e.g. the decoy effect). On the contrary, BCV focuses on context effects elicited by expectations before options are presented and where a single attribute is salient. Data on this domain are relatively scarce, and are discussed in the introduction to the new experiment we now report (experiment one in the new manuscript):</p><p>“Data from conditions where BCV is applicable, namely those involving a single attribute and where context depends on past options (and not simultaneously presented options), are relatively scarce. […] Here, we present data from a behavioural experiment where we investigate this very question.”</p><p>We also mention explicitly other forms of context effect emerging in conditions where BCV is not applicable at the moment, like the decoy effect. We acknowledge this as a shortcoming of BCV, though we argue that similar principles might be valid in these conditions too:</p><p>“We highlight shortcomings of the model, though the framework itself may be fruitful in addressing some of these shortcomings. […] This would provide an opportunity to model attentional processes determining an optimal weighting of different attributes based on their importance and reliability.”</p><p><italic>Reviewer #2:</italic> </p><p><italic>My major reservation about the paper is that the human data offers only partial confirmation for the Bayesian context model. In its full specification (in the Appendix), the BCV model predicts that incentive value will depend on a divisive scaling term (implementing precision weighting dependent on relative reward variance) and subtractive prediction error terms (low and high context reward predictions, weighted by their relative contextual cue variances). However, the effects of precision – which are a key element of Bayesian approaches – are untested in the analysis of the experimental data. As the authors state in the paper, reward variance is not manipulated and the divisive term (K) cannot be examined. In addition, the subtractive terms should also be precision-weighted by terms relating posterior and prior variances (denoted by tau_LO and tau_HO). However, the results only speak to overall context effects, essentially asking whether there is an overall effect of low and high context on gambling (population regression effects in <xref ref-type="fig" rid="fig5">Figure 5</xref>, and the chi-square tests showing positive tau_LO and tau_HO parameters).</italic> </p><p><italic>The issue is that adaptive effects to average rewards are well known (for example, successive contrast effects in the animal literature and reference point models like prospect theory); without validating the precision-dependent predictions of their model, I'm not sure that the authors can convincingly argue that BCV is a more appropriate model – particularly as other models are not tested. Given the experimental setup, testing the divisive weighting term is not possible in this dataset; however, can the authors make any predictions about not just the significance but the relative magnitude of the weighting factors tau_LO and tau_HO (predicted and fit to data)?</italic></p><p>We are grateful for this suggestion, and agree on the importance of demonstrating an actual effect of precision. Such a test is something we already had in mind when preparing this manuscript, and hence, before obtaining these reviews, we had run (and analysed data from) a novel experiment investigating the role of context precision in value adaptation. We are grateful that the reviewer has highlighted the importance of this point – and now we present data from this new experiment which addresses this very point (experiment one in the new manuscript). In brief, this provides evidence for context precision consistent with BCV. As pointed out by the reviewer, such a role is a key feature of Bayesian schemes and is an observation which supports BCV over alternative theories (e.g., those based on reference point adaptation alone such as Prospect theory).</p><p>The question about the comparison between factors tau_LO and tau_HO is interesting, though we believe that our task is not suitable to address this question. An experiment aimed at investigating this question will require an explicit manipulation of the salience of information regarding different hierarchical levels (e.g., manipulating the relative “perceptual noise” of this information), while in our task it is unclear whether (and which) level is more salient.</p><p><italic>Reviewer #3:</italic></p><p><italic>1) Behavioural effects of reward context on incentive value are already well-established and even the specific paradigm used here has already been published by the same authors. Is there any new, surprising behavioural effect that follows from the new model?</italic></p><p>We agree this should be made clearer. Two novel and important empirical predictions generated by BCV are: (i) the fact that hierarchically organized contexts can exert adaptation on incentive value and choice, (ii) the variance of the reward context can exert adaptation on incentive value and choice. We investigate both predictions in the current version of the manuscript. The first prediction is tested in the behavioural study reported also in initial submission version of the manuscript. In the present version, we now report a new experiment where we also test the second prediction. These issues are now clarified in the Discussion:</p><p>“We propose a Bayesian scheme (BCV) as a model of contextual influences on incentive value attribution. BCV is based on Bayesian inference principles and on generative models of reward. Adopting two novel experimental designs, we provide behavioural evidence that supports two key predictions of BCV, namely that value attribution is affected by reward variance (which exerts divisive normalization) and by hierarchically organized contexts.”</p><p><italic>2) The Bayesian model presented in the manuscript is not formally compared to other well-established models that may similarly account for the behavioural effects. The authors should show with formal model comparisons that their model outperforms other classic (non-Bayesian) models commonly employed to model context effects on value-based choice.</italic></p><p>Thanks for highlighting this point. We agree that the comparison with other theories should be explained better and formally tested. We now do this in the new version of the manuscript. The connections with other theories are explained:</p><p>“The proposal advanced here has some similarities with classical theories of value, such as Expected Utility theory (von Neumann &amp; Morgenstern, 1944) and Prospect theory (Kahneman &amp; Tversky, 1979). […] Importantly, these predictions are not ad hoc but derive necessarily from Bayesian assumptions.”</p><p>In addition, we perform formal model comparison to compare BCV with predictions derived from alternative theories. This is done for experiment one:</p><p>“We next compared the full model with an alternative model without a subtractive normalization component (as postulated by BCV, where the expected reward is subtracted to the actual reward); namely, where the incentive value was equal to <inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mi>ε</mml:mi></mml:msup><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>. […] This test was significant (χ<sup>2</sup>(1) = 508, p &lt; 0.001), meaning that the model implementing both subtractive and divisive normalization (derived from BCV and described by <xref ref-type="disp-formula" rid="equ8">equation 8</xref>) fits the data better.”</p><p>And for experiment two:</p><p>“We next compared the full model with an alternative model where the context parameters (capturing the influence of the reward expected within a context at multiple hierarchical levels) divided the reward rather than being subtracted from the reward; in other words where the incentive value corresponds to:<disp-formula id="equ19"><label>(11)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>[…] This test was significant (full models: χ<sup>2</sup>(1) = 78, p &lt; 0.001; for models with α, ρ and <inline-formula><mml:math id="inf50"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: (1) = 26, p &lt; 0.001; for models with α, ρ and τ<sub><italic>LH</italic></sub>: (1) = 50, p &lt; 0.001), meaning that the model implementing subtractive normalization (consistent with BCV) is a better explanation for the data.”</p><p><italic>3) The model is not biologically realistic. This is not always a problem; in fact, there are many elegant demonstrations that Bayesian frameworks can account for optimal performance in various domains better than other accounts. However, in the specific context of reward-guided decision-making, it is unclear why and how a Bayesian framework should apply, and to what degree it is more consistent with behaviour and the underlying neural computations. Please provide a lot more information on how this model may be implemented by neural computations. In particular, it would help if there was any empirical evidence for the hierarchical representation of reward context.</italic></p><p>Thanks for highlighting this point. We agree it is important to clarify the link between BCV and brain function. We stress that there are several existing proposals as to how Bayesian inference schemes (and hence BCV) might be implemented in the brain (e.g., Doya et al., 2007; Friston, 2005; Hennequin et al., 2014; Knill &amp; Pouget, 2004). Therefore we consider that BCV is relevant for ideas regarding neural implementation level, and is equally biologically plausible as other Bayesian inference schemes.</p><p>Several neural observations are consistent with BCV, such as the observation that a reward signal in several brain regions is adapted to a reference point and to reward range (Bermudez &amp; Schultz, 2010; Cox &amp; Kable, 2014; Louie et al., 2011; Padoa-Schioppa, 2009; Padoa-Schioppa &amp; Assad, 2008; Park et al., 2012; Rigoli et al., 2016a; Tobler et al., 2005 Kobayashi et al., 2010; Tremblay &amp; Schultz, 1999), and like a recent finding of a link between neural and choice adaptation (Rigoli et al., 2016). Yet there are important aspects of BCV which will need to be probed at the neural level. For instance, as pointed out by the reviewer, a test of an hypothesis of a hierarchical organization of reward context in the brain is lacking. However, we believe that the fact that aspects of the model have not yet been tested represents a strength (rather than a weakness) of the theory, as it makes novel and precise predictions. We now refer to these issues in the following text:</p><p>“With reference to the three levels of analysis (i.e., computational, algorithmic and implementation) proposed by Marr (1982), BCV speaks to the computational level as it focuses on normative principles (implicit in optimal Bayesian inference) proposed to explain value and choice adaptation. […] In other words, BCV implies that, as well as neural signalling, behaviour itself is tuned to the statistics of the incentives, so as to maximize discriminability among these incentives.”</p><p><italic>4) The Abstract claims that the model &quot;generates new empirical predictions and may help explain important phenomena in psychopathologies such as addiction.&quot; I found the corresponding text in the Discussion rather vague. Please provide explicit predictions for specific experimental effects that follow from this model and please explain much more concretely which important phenomena in psychopathologies are explained by it.</italic></p><p>We agree this was not clear. Two empirical predictions arising from the theory are: (i) hierarchically organized contexts exert adaptation of incentive value and choice, (ii) variance of the reward context can exert adaptation of incentive value and choice. We investigate both predictions here, the second with new experimental data. The first prediction was tested in the previous version of the manuscript. In this revised manuscript, we report an experiment where the second prediction is tested. The specific predictions of the model are now presented in detail in <xref ref-type="fig" rid="fig2">Figure 2</xref> and are discussed here in relation with other models:</p><p>“The proposal advanced here has some similarities with classical theories of value, such as Expected Utility theory (von Neumann &amp; Morgenstern, 1944) and Prospect theory (Kahneman &amp; Tversky, 1979). […] Importantly, these predictions are not ad hoc but derive necessarily from Bayesian assumptions.”</p><p>In relation with the link with psychopathology, we focus on addiction. BCV may be useful to understand the computational mechanisms underlying the development of addiction and tolerance. In addition, BCV may be useful also to clarify the computational mechanisms representing an individual predisposition to drug abuse, for instance dependent on increased uncertainty of prior belief on reward. This is clarified:</p><p>“Finally, there are questions related to psychopathology that can be fruitfully formulated in terms of BCV, for example addiction. […] For instance, increased uncertainty over prior reward beliefs may boost the magnitude of the (positive) prediction error elicited by drug consumption, hence enhancing individual predisposition to drug addiction.”</p><p><italic>5) The manuscript claims in several places that a reward's incentive value corresponds to the (precision-weighted) prediction error. This is misleading. By definition, the incentive value is the property of a stimulus/expected reward that triggers approach behaviour and choice of the corresponding option. This representation must therefore be computed before the choice is taken and the reward is obtained. The prediction error, by contrast, is the deviation of the reward obtained as a consequence of the choice from the reward expected prior to the choice. This post-choice representation can therefore not be the incentive value guiding choice. The authors need to clarify their terminology and ensure that they remain consistent with established definitions in the literature.</italic></p><p>We apologise for the confusion and agree on a need for clarity. BCV is applicable every time novel information about reward is provided, which is when a prediction error occurs. This happens: (i) when a (primary or secondary) reward is delivered (or is not delivered when expected), which can be post choice but also in other conditions (e.g., in classical conditioning paradigms), when a reward is delivered independent of action) (ii) when one (or more) option is presented, which is the condition exploited in our experiments. This is because an agent has expectancies about the value of an option, which can be confirmed or not when options are presented, leading to prediction errors. We agree that, to avoid confusion, this needs to be clarified. We stress that the idea that a prediction error occurs when options are presented is not uncommon in the reward literature, as in standard computational temporal difference algorithms (Sutton &amp; Barto, 1998). This is now clarified:</p><p>“Equation two applies every time novel information about reward is provided, which is when a prediction error occurs. […] The latter follows because an agent has an expectation about an option, which leads to a prediction error when the actual option is presented.”</p><p><italic>In addition to the conceptual points listed above, the manuscript also has shortcomings with respect to methodology and results presentation that will need to be addressed:</italic> </p><p><italic>6) The behavioural task was not designed to allow proper tests of the full model. Some of these problems are listed by the authors themselves and have led to adaptations of the model so that it could be fit to the data. For instance, reward variance in the different decks is not varied, is heavily constrained (there are only 3 different reward values per deck), and is perfectly correlated with average reward magnitude. To properly test whether the precision of reward prediction errors established by the different contexts really plays an important role, the authors should fit the full model to datasets with contexts that differ substantially in their reward variance and that disentangle reward magnitude from reward variance. Moreover, the contexts should be associated with a lot more than just three possible reward values so that the form of the expected reward distributions can be properly approximated (see below).</italic></p><p>We appreciate this suggestion, and agree that investigating the effect of precision is a fundamental test for BCV. Such test is something we already had in mind, and had run a novel experiment prior to submission of the original manuscript. This experiment is relevant as in it we investigate the role of context precision in value adaptation. We now include these data (experiment one in the new manuscript) which provide evidence for a role of context precision consistent with BCV. As pointed out by the reviewer, such role is a key feature of Bayesian schemes and supports BCV over alternative accounts.</p><p>To avoid confusion, we now clarify in the novel paradigm that there are two types of variance. The first refers to the variance of possible outcomes of the gamble (which is perfectly correlated with the EV of options as in a previous experiment and as in (Rigoli et al., 2016a; 2016b), and is not the focus of the experiment. The second refers to the variance across options (i.e., the variance characterizing the distribution of serially-provided options), which is what we manipulate and is reflected in and affects the gain term of equation one and two. This is clarified:</p><p>“Note that in our task there are two types of variance. The first refers to the variance of possible outcomes of the gamble (which is perfectly correlated with the EV of options, as in Rigoli et al., 2016), and is not the focus of our study. The second refers to variance across options (i.e., the variance characterizing the distribution of successive options), which is what we experimentally manipulate and investigate. In the model, this is reflected in and affects the gain term in <xref ref-type="disp-formula" rid="equ2">equation 2</xref>.”</p><p><italic>7) The model specification does not match the environment established by the behavioural task. The decks were associated with 3 equiprobable values per deck. Therefore, &quot;smart&quot; subjects would employ a flat discrete expectation of the three possible values within the given context. In contrast, the modelling solution is based on continuous (Gaussian) distributions that are not restricted by the bounds imposed by each context. There are two problems with this: (1) If we assume that the subjects indeed optimally integrate all information, then a flat prior belief bounded by the context's minimum and maximum reward would be accurate. This cannot be modelled by the presented specification. (2) Even if subjects employed continuous Gaussian priors to model reward expectations, such distributions would probably not be narrow enough with respect to the context bounds (i.e., the priors would wrongly lead to expectations of rewards that are outside of the bounded scale). These problems will probably be evident if the authors report the values of the latent variables after fitting the model to the empirical data. Please include such a table to allow the reader to inspect this issue.</italic></p><p><italic>In my view, if the authors really wanted to maintain a Bayesian optimal observer model, then they should examine how the prior (potentially flat, but can also be modelled if the authors like) is combined with the likelihood of the actual numeric representation to obtain a posterior estimate that should naturally occur within the actual numeric bounds of the context (for an example on how to formally deal with Bayesian problems of this kind, i.e. bounded contexts, see Jazayeri and Shadlen 2010, Nature Neuroscience). This formal specification can then be expanded to the interesting contextual hierarchical framework that the authors propose in their study.</italic></p><p>We understand the reviewer’s concerns and agree. It is true that real agents may build non-Gaussian representations, especially in conditions where the true distributions are non-Gaussian. However, the advantage of using Gaussian assumptions is that posterior probability can be estimated analytically with equations which are simple and clear, and resemble common formulations widely used in decision-making and motivation research (e.g., Rescorla-Wagner update rule). Therefore, we would like to stick with Gaussian variables when presenting the model, which we believe is a better approach in making the model clear and transparent. Also on the question of fitting the model to choice data, we believe that Gaussian assumptions should not be problematic, as the equations used are more general. In other words, these equations can be derived from Gaussian assumptions but are valid also for other distributions such as uniform distributions (in general, non-skewed distributions).</p><p>Please consider <xref ref-type="disp-formula" rid="equ8">equation 8</xref>:<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mi>ε</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This simply prescribes a subtractive normalization to the average reward <inline-formula><mml:math id="inf51"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and a divisive normalization elicited by reward variance, captured by a quantity τ (a free parameter bounded between 0.1 and 10) in the low-variance context (where ε = 1 indicates a low variance and ε = 0 indicates high-variance context). This equation is valid also for other distributions, such as uniform distributions (in general, non-skewed distributions). In addition, we did not impose a bound a priori on the parameter τ between 0 and 1, as would be predicted by BCV. This because we tested whether the τ estimated from choice data was between 0 and 1.</p><p>Similarly, please consider <xref ref-type="disp-formula" rid="equ10">equation 10</xref>:<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>This prescribes subtractive normalization exerted by the average reward expected at two hierarchical levels. Again this equation is general and valid also for other distributions, such as uniform distributions (in general, non-skewed distributions). As above, we did not impose a bound a priori on the parameter τ to be positive, as would be predicted by BCV. This is because we <italic>tested</italic> whether the τ estimated from choice data was positive.</p><p>Overall, our goal was to (i) derive equations using Gaussian assumptions, but in a general form potentially consistent also with other non-skewed distributions (e.g., uniform) (ii) fit these equations to choice with unconstrained context parameters, (iii) test whether the estimated context parameters where consistent with predictions.</p><p>The problem of context boundaries is also relevant. In general, in Bayesian approaches, it is often useful to model participants’ beliefs in continuous space, even in contexts which are “objectively” bounded. For instance, in models of normalization, incentive values go beyond the “objective” context boundaries almost by definition. Therefore we would expect that if one assumes bounded variables within the generative model, we would not obtain normalization. This we believe represents a fundamental argument for using continuous and not bounded variables in the context of value normalization. This is clarified in the Discussion:</p><p>“Here, we have assumed that variables of the generative model are Gaussian. This allows us to present the model in a simple and clear way, as posterior beliefs can be inferred analytically with relatively simple equations as adopted in standard decision-making schemes (Rescorla &amp; Wagner, 1972). […] However, the key idea (tested in our experiments – and here derived from Gaussian assumptions) that reward average and variance elicit subtractive and divisive normalization, respectively, is quite general and can also be applied, for instance, to uniform (and, in general, non-skewed) distributions.”</p><p><italic>8) The authors rescaled the contextual averages and reward values to perform their model fits. Why? A correctly specified model should be able to take as inputs the actual values of the contexts and rewards of their behavioural paradigm (which are all single-digit numbers after all). This would help to assess the model's explanatory power.</italic></p><p>Thanks for pointing this out. We now follow the reviewer’s advice and use non rescaled contextual averages:</p><p>“Where <inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> indicates the average option EV for the deck (for £9 deck: <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>4.5</mml:mn><mml:mo> </mml:mo><mml:mo>;</mml:mo><mml:mo> </mml:mo></mml:mrow></mml:math></inline-formula>for £7 deck: <inline-formula><mml:math id="inf54"><mml:mrow><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3.5</mml:mn></mml:mrow></mml:math></inline-formula>; for £5 deck: <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn></mml:mrow></mml:math></inline-formula>), <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> indicates the average option EV for the deck-set (high-value deck-set: <inline-formula><mml:math id="inf57"><mml:mrow><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>; low-value deck-set: <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), τ<sub><italic>LO</italic> </sub>is a free parameter that mediates contextual effects at the deck level, and <inline-formula><mml:math id="inf59"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a free parameter that mediates contextual effects at the deck-set level (see methods).”</p><p><italic>9) The authors perform model comparison by summing log-likelihoods across participants. I find the selection of this approach for model comparison surprising, given that several of the co-authors have pushed the use of precise Bayesian model selection methods that properly account for the complexity and variability of the model fits across trials and participants. The authors should employ such methods and should provide values quantifying the quality of the model fits after penalizing for model complexity.</italic></p><p>Thanks for highlighting this point. It is true that in the previous version we estimated the parameters without strong constraints. This is potentially problematic because this method may lead to outlier estimates which are biased, and is a potential problem for our findings in relation with model-comparison and in relation with within-subjects t-tests over the context parameters. To address this possibility, we now adopt a Gaussian prior with mean zero (Daw, 2011) during estimation of the context parameters. This allows extreme parameter estimates only if they improve substantially the negative log-likelihood, hence avoiding possible biased extreme outliers. Note in addition that a prior mean of zero does not bias the t-tests since we tested the parameters against zero. For a sanity-check, in supplemental materials we now report frequencies of the parameter estimates (for experiment one: <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>; for experiment two: <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). We can see that outlier estimates (i.e., scores larger or smaller than 3 SD compared to the mean) are absent.</p><p>For model comparison, we considered the probability of the data given the model and given the parameters estimated in this way. We used the likelihood ratio test because our model-comparison involves nested models. This approach is a standard and well-established method for model-comparison involving nested models (Casella &amp; Berger, 2002; Daw, 2011). Penalization for model complexity is implicit in this test since it is based on the null hypothesis that data are generated with the simpler model, and hence the more complex model is accepted only if evidence for the data given the simpler model is below significance threshold. In addition, different levels of complexity are distinguished and penalized in a different way, thanks to the fact that the degrees of freedom of the likelihood ratio test (capturing how “strong” the evidence against the simper model needs to be to refuse the null hypothesis) increase with the number of extra parameters. The details regarding the method adopted for model fitting and model comparison are explained in detail:</p><p>“The free parameters of the models were estimated separately for each subject using <italic>fminsearchbnd</italic> function of the Optimization toolbox in Matlab. […] A chi-square test can be performed to estimate the probability that the observed <italic>2d</italic> is due to chance under the null hypothesis that data are generated by the nested model, allowing acceptance or rejection of the null hypothesis.”</p><p><italic>10) The behavioural task is described in a fashion that makes it hard to replicate. For instance, is it true that on every trial, a card was drawn from the blue deck? If so, how were the different deck contexts varied across the different colours? Please make sure the task described in sufficient detail so that another person could program it.</italic> </p><p>Thank you – this is now clarified in the caption of <xref ref-type="fig" rid="fig5">Figure 5</xref>:</p><p>“The decks were coloured one in blue and the other in grey, indicating the selected and unselected deck respectively. Among these two decks shown on the screen, the selected deck (coloured in blue) alternated pseudo-randomly over blocks (each including 5 trials). In addition, two sets of decks alternated over longer blocks (20 trials) in a pseudo-random way”.</p><p>And in the main text:</p><p>“Among the two decks shown on the screen, the selected deck (coloured in blue) alternated pseudo-randomly over blocks (each including 5 trials). In addition, at some points during the task, decks were replaced by new decks. Two sets of decks alternated over blocks of 20 trials in a pseudo-random way.”</p></body></sub-article></article>