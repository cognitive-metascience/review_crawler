<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">19113</article-id><article-id pub-id-type="doi">10.7554/eLife.19113</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Detecting and representing predictable structure during auditory scene analysis</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-64377"><name><surname>Sohoglu</surname><given-names>Ediz</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0755-6445</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-4307"><name><surname>Chait</surname><given-names>Maria</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7808-3593</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">UCL Ear Institute</institution>, <institution>University College London</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Reviewing editor</role><aff id="aff2"><institution>University of Oxford</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>e.sohoglu@gmail.com</email> (ES);</corresp><corresp id="cor2"><email>m.chait@ucl.ac.uk</email> (MC)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>07</day><month>09</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e19113</elocation-id><history><date date-type="received"><day>24</day><month>06</month><year>2016</year></date><date date-type="accepted"><day>14</day><month>08</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Sohoglu et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Sohoglu et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-19113-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.19113.001</object-id><p>We use psychophysics and MEG to test how sensitivity to input statistics facilitates auditory-scene-analysis (ASA). Human subjects listened to ‘scenes’ comprised of concurrent tone-pip streams (sources). On occasional trials a new source appeared partway. Listeners were more accurate and quicker to detect source appearance in scenes comprised of temporally-regular (REG), rather than random (RAND), sources. MEG in passive listeners and those actively detecting appearance events revealed increased sustained activity in auditory and parietal cortex in REG relative to RAND scenes, emerging ~400 ms of scene-onset. Over and above this, appearance in REG scenes was associated with increased responses relative to RAND scenes. The effect of temporal structure on appearance-evoked responses was delayed when listeners were focused on the scenes relative to when listening passively, consistent with the notion that attention reduces ‘surprise’. Overall, the results implicate a mechanism that tracks predictability of multiple concurrent sources to facilitate active and passive ASA.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.19113.001">http://dx.doi.org/10.7554/eLife.19113.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.19113.002</object-id><title>eLife digest</title><p>Everyday environments like a busy street bombard our ears with information. Yet most of the time, the human brain quickly and effortlessly makes sense of this information in a process known as auditory scene analysis. According to one popular theory, the brain is particularly sensitive to regularly repeating features in sensory signals, and uses those regularities to guide scene analysis. Indeed, many biological sounds contain such regularities, like the pitter-patter of footsteps or the fluttering of bird wings.</p><p>In most previous studies that investigated whether regularity guides auditory scene analysis in humans, listeners attended to one sound stream that repeated slowly. Thus, it was unclear how regularity might benefit scene analysis in more realistic settings that feature many sounds that quickly change over time.</p><p>Sohoglu and Chait presented listeners with cluttered, artificial auditory scenes comprised of several sources of sound. If the scenes contained regularly repeating sound sources, the listeners were better able to detect new sounds that appeared partway through the scenes. This shows that auditory scene analysis benefits from sound regularity.</p><p>To understand the neurobiological basis of this effect, Sohoglu and Chait also recorded the brain activity of the listeners using a non-invasive technique called magnetoencephalography. This activity increased when the sound scenes featured regularly repeating sounds. It therefore appears that the brain prioritized the repeating sounds, and this improved the ability of the listeners to detect new sound sources.</p><p>When the listeners actively focused on listening to the regular sounds, their brain response to new sounds occurred later than seen in volunteers who were not actively listening to the scene. This was unexpected as delayed brain responses are not usually associated with active focusing. However, this effect can be explained if active focusing increases the expectation of new sounds appearing, because previous research has shown that expectation reduces brain responses.</p><p>The experiments performed by Sohoglu and Chait used a relatively simple form of sound regularity (tone pips repeating at equal time intervals). Future work will investigate more complex forms of regularity to understand the kinds of sensory patterns to which the brain is sensitive.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.19113.002">http://dx.doi.org/10.7554/eLife.19113.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>scene analysis</kwd><kwd>magnetoencephalography</kwd><kwd>predictive coding</kwd><kwd>change detection</kwd><kwd>attention</kwd><kwd>surprise</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/K003399/1</award-id><principal-award-recipient><name><surname>Chait</surname><given-names>Maria</given-names></name><name><surname>Sohoglu</surname><given-names>Ediz</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Brain responses in humans demonstrate that the analysis of crowded acoustic scenes is based on a mechanism that infers the predictability of sensory information and up-regulates processing for reliable signals.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Natural scenes are highly structured, containing statistical regularities in both space and time and over multiple scales (<xref ref-type="bibr" rid="bib43">Julesz, 1981</xref>; <xref ref-type="bibr" rid="bib66">Portilla and Simoncelli, 2000</xref>; <xref ref-type="bibr" rid="bib30">Geisler, 2008</xref>; <xref ref-type="bibr" rid="bib55">McDermott et al., 2013</xref>; <xref ref-type="bibr" rid="bib82">Theunissen and Elie, 2014</xref>). A growing body of work suggests that the human brain is sensitive to this statistical structure (<xref ref-type="bibr" rid="bib67">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib61">Näätänen et al., 2001</xref>; <xref ref-type="bibr" rid="bib6">Bar, 2004</xref>; <xref ref-type="bibr" rid="bib63">Oliva and Torralba, 2007</xref>; <xref ref-type="bibr" rid="bib15">Costa-Faidella et al., 2011</xref>; <xref ref-type="bibr" rid="bib29">Garrido et al., 2013</xref>; <xref ref-type="bibr" rid="bib62">Okazawa et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Barascud et al., 2016</xref>) and uses it for efficient scene analysis (<xref ref-type="bibr" rid="bib85">Winkler et al., 2009</xref>; <xref ref-type="bibr" rid="bib3">Andreou et al., 2011</xref>; <xref ref-type="bibr" rid="bib9">Bendixen, 2014</xref>). Uncovering the process by which this occurs, and how sensory predictability interacts with attention, is a key challenge in sensory neuroscience across modalities (<xref ref-type="bibr" rid="bib85">Winkler et al., 2009</xref>; <xref ref-type="bibr" rid="bib75">Summerfield and de Lange, 2014</xref>; <xref ref-type="bibr" rid="bib77">Summerfield and Egner, 2016</xref>).</p><p>The current state of understanding is limited by at least two factors: (1) most studies of sensory predictability and its effects on behavior have used slow presentation rates thus enabling conscious reflection of stimulus expectancy. As a consequence, relatively little is known about the neural underpinning of predictability processing on the rapid time scales relevant to perception of natural objects. (2) In most cases, predictability has been studied when participants attend to a single object (<xref ref-type="bibr" rid="bib59">Murray et al., 2002</xref>; <xref ref-type="bibr" rid="bib4">Arnal et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib13">Chennu et al., 2013</xref>; <xref ref-type="bibr" rid="bib9">Bendixen, 2014</xref>) – a far cry from the complex scenes in which we normally operate. We therefore do not understand whether/how statistical structure is extracted from complex, crowded scenes. The present work addresses both of these issues in the context of an auditory scene.</p><p>To understand how statistical structure facilitates perceptual analysis of acoustic scenes, we use an ecologically relevant paradigm (change detection) that captures the challenges of natural listening in crowded environments (<xref ref-type="bibr" rid="bib10">Cervantes Constantino et al., 2012</xref>; <xref ref-type="bibr" rid="bib73">Sohoglu and Chait, 2016</xref>). In this paradigm, listeners are presented with multiple concurrent acoustic sources and on occasional trials, a new source appears partway into the ongoing scene (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). By varying the temporal patterning of scene sources, we can create conditions in which the scenes are characterized by statistically regular or random structure and measure the effect of this manipulation on listeners’ ability to detect the appearance of new sources within the unfolding soundscape.<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.19113.003</object-id><label>Figure 1.</label><caption><title>Stimuli and behavior.</title><p>(<bold>A</bold>) Examples of REG and RAND scenes. The plots represent ‘auditory’ spectrograms, equally spaced on a scale of ERB-rate (<xref ref-type="bibr" rid="bib58">Moore and Glasberg, 1983</xref>). Channels are smoothed to obtain a temporal resolution similar to the Equivalent Rectangular Duration (<xref ref-type="bibr" rid="bib65">Plack and Moore, 1990</xref>). Black arrows indicate appearing sources. In these examples, the appearing source is temporally regular. The stimulus set also included scenes in which the appearing source was temporally random (see Materials and methods). (<bold>B</bold>) Behavioral results (d’ and detection time) as a function of scene temporal structure (REG versus RAND). These are shown for each type of scene change (when the appearing source was temporally regular or when random). Error bars represent within-subject standard error of the mean (SEM; <xref ref-type="bibr" rid="bib51">Loftus and Masson, 1994</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.19113.003">http://dx.doi.org/10.7554/eLife.19113.003</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-19113-fig1-v1"/></fig></p><p>The behavioral response pattern reveals that perceptual analysis of such scenes is enhanced by the presence of regular statistical structure, as assessed by listeners’ ability to detect source appearance. One possible explanation for this effect is that neural responses to regularly repeating scene components adapt (decrease over time) more than to random components. Indeed, perceptual influences of statistical structure have often been attributed to neural adaptation (e.g. 'stimulus specific adaptation'; <xref ref-type="bibr" rid="bib54">May et al., 1999</xref>; <xref ref-type="bibr" rid="bib40">Jääskeläinen et al., 2004</xref>; <xref ref-type="bibr" rid="bib34">Haenschel et al., 2005</xref>; <xref ref-type="bibr" rid="bib15">Costa-Faidella et al., 2011</xref>; ; <xref ref-type="bibr" rid="bib44">Khouri and Nelken, 2015</xref>). Accordingly, the relative change in neural response to a new spectral component (that is, the appearing source) will be larger and thus more detectable in regular versus random scenes (<xref ref-type="bibr" rid="bib78">Summerfield et al., 1987</xref>; <xref ref-type="bibr" rid="bib36">Hartmann and Goupell, 2006</xref>; <xref ref-type="bibr" rid="bib23">Erviti et al., 2011</xref>). By this account, statistical structure does not modulate the magnitude of neural response to a new event per se. Rather, improved detection is attributed exclusively to decreased neural responses occurring before the appearance of the new source. Indeed, in a mismatch negativity paradigm, <xref ref-type="bibr" rid="bib15">Costa-Faidella (2011)</xref> demonstrated that neural responses to repeating (‘standard’) tones adapt more in temporally regular than random sequences without accompanying changes in response to new (‘deviant’) tones (see also <xref ref-type="bibr" rid="bib72">Schwartze et al., 2011</xref>, <xref ref-type="bibr" rid="bib71">2013</xref>; <xref ref-type="bibr" rid="bib79">Tavano et al., 2014</xref>).</p><p>However, other work has shown that statistically regular patterns can be associated with increased neural responses (<xref ref-type="bibr" rid="bib34">Haenschel et al., 2005</xref>; <xref ref-type="bibr" rid="bib45">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib13">Chennu et al., 2013</xref>; <xref ref-type="bibr" rid="bib39">Hsu et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Kouider et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Barascud et al., 2016</xref>). These effects have been interpreted to reflect a mechanism that tracks the level of predictability or ‘precision’ of the sensory input, a measure inversely related to the uncertainty or entropy of a variable. This mechanism is hypothesized to enable the up-regulation of processing for information that is reliable and likely to indicate genuine events in the environment (<xref ref-type="bibr" rid="bib24">Feldman and Friston, 2010</xref>; <xref ref-type="bibr" rid="bib86">Zhao et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Auksztulewicz and Friston, 2015</xref>; <xref ref-type="bibr" rid="bib7">Barascud et al., 2016</xref>). Importantly, the up-regulation of neural processing in these accounts is hypothesized to lead to increased neural responses for regular scenes before source appearance, as well as an increased error (‘surprise’) response evoked by the new source.</p><p>In the current study we adjudicate between adaptation and precision accounts using magnetoencephalography (MEG) recordings of brain activity. Given the ongoing debate about how the neural influence of statistical regularity might depend on attention (<xref ref-type="bibr" rid="bib42">Jones and Boltz, 1989</xref>; <xref ref-type="bibr" rid="bib61">Näätänen et al., 2001</xref>; <xref ref-type="bibr" rid="bib76">Summerfield and Egner, 2009</xref>; <xref ref-type="bibr" rid="bib85">Winkler et al., 2009</xref>; <xref ref-type="bibr" rid="bib24">Feldman and Friston, 2010</xref>; <xref ref-type="bibr" rid="bib45">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib9">Bendixen, 2014</xref>; <xref ref-type="bibr" rid="bib70">Schröger et al., 2015</xref>), we do this in the context of passive listening (listeners engaged in an unrelated visual task) as well as active listening (listeners actively detecting source appearance). Our results provide evidence in support of precision accounts: we show that brain responses to ongoing acoustic scenes, and to new sources appearing within those scenes, increase in the presence of regular statistical structure. Strikingly, the effect of regularity on appearance detection is delayed when listeners are actively focused on the scenes rather than listening passively. This latter finding suggests (somewhat counter intuitively) that active listening can counteract the influence of regularity but is consistent with attention acting to reduce ‘surprise’ (<xref ref-type="bibr" rid="bib74">Spratling, 2008</xref>; <xref ref-type="bibr" rid="bib13">Chennu et al., 2013</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral data</title><p>Listeners’ source appearance detection performance in the Active group is shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. Listeners were more accurate and quicker to detect source apperance when the scene structure was temporally regular (REG) versus random (RAND; d’ F(1,12) = 100.7, p&lt;0.001; detection times F(1,12) = 17.61, p&lt;0.01). This effect occurred independently of the temporal structure of the appearing component (d’ F(1,12) = 0.075, p=0.789; detection times F(1,12) = 4.23, p=0.062). Additionally, listeners were quicker (by ~27 ms) to detect source appearance when it was temporally regular (detection times F(1,12) = 5.70, p&lt;0.050), although this effect did not extend to d’ (F(1,12) = 2.29, p=0.156). Thus temporally regular scenes are associated with enhanced detection performance and in a manner independent of the temporal structure of the appearing source. Overall, the mean hit rate was high (mean = 76.1%, ranging from 57 to 97% across listeners) and mean false alarm rate low (mean = 6.25%, ranging from 0 to 18.8%).</p></sec><sec id="s2-2"><title>MEG data</title><sec id="s2-2-1"><title>Scene-evoked response</title><p>The neural response evoked by scene onset (i.e. prior to any scene change) is characterized by a series of deflections at around 80, 110 and 200 ms, with topographies corresponding to the commonly observed M50, M100 and M200 onset-response components (<xref ref-type="bibr" rid="bib22">Eggermont and Ponton, 2002</xref>). From around 300 ms post onset, the response settles to a sustained amplitude.</p><p>We searched for differences between responses to the onset of REG and RAND scenes using cluster-based permutation statistics (shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>). Scene temporal structure had a significant effect on the evoked response from 436 ms in the Passive group and from 476 ms in the Active group, involving an increase in the sustained response for REG versus RAND conditions (temporal clusters with FWE corrected significance are indicated as thick horizontal green bars in <xref ref-type="fig" rid="fig2">Figure 2</xref>; uncorrected clusters are shown as thin light-green bars). The topographical patterns for REG and RAND conditions (averaged over the 500–800 ms period of the sustained response) were qualitatively similar in both Passive and Active groups (also shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>).<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.19113.004</object-id><label>Figure 2.</label><caption><title>RMS time-course of the scene-evoked response showing the main effect of scene temporal structure (REG versus RAND).</title><p>Thick horizontal green lines indicate time points for which there were significant differences between REG and RAND conditions (p&lt;0.05 FWE corrected at the cluster level; Thin light-green lines show uncorrected clusters). Purple lines indicate (jackknife-estimated) latencies of the onset of the REG versus RAND effect (horizontal and vertical portions indicate mean and jackknife-corrected standard error, respectively). Also shown are topographical patterns at the time of the sustained response (500–800 ms post scene onset), which are characterized by a dipole-like pattern over the temporal region in each hemisphere indicating downward flowing current in auditory cortex (red = source; blue = sink).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.19113.004">http://dx.doi.org/10.7554/eLife.19113.004</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-19113-fig2-v1"/></fig></p><p>To test whether the onset latency of the scene structure effect was significantly different between groups, we used a jackknife resampling procedure previously shown to be highly sensitive to latency effects (<xref ref-type="bibr" rid="bib56">Miller et al., 1998</xref>; <xref ref-type="bibr" rid="bib83">Ulrich and Miller, 2001</xref>). This involved repeatedly resampling the grand averaged RMS time-course and for each subsample, computing the earliest latency at which the REG versus RAND difference was larger than variability in the baseline period (see Materials and methods). Mean onset latencies for each group are shown as vertical purple lines in <xref ref-type="fig" rid="fig2">Figure 2</xref> (289 ms for Passive; 412 ms for Active). Although the scene structure effect emerged on average, 123 ms earlier in the Passive versus Active groups, there was no significant difference in onset latency between groups (jackknife adjusted two-sample t(25) = −1.35, p=0.188). Neither was there a main effect of group (p=0.48) or scene structure by group interaction (p=0.31) when conducting ANOVA on the magnitude of the sustained response (averaged from 500–800 ms). This was also the case for earlier time-windows during the M50, M100 and M200 components (all p’s &gt;0.19).</p><p>In summary, when MEG responses are timelocked to scene onset, regular scene structure results in an increased MEG response from around 400 ms post scene onset. Furthermore, the scene-evoked response shows no evidence of attentional modulation, either in terms of an overall difference between Passive and Active groups or the interaction between scene structure and group.</p></sec></sec><sec id="s2-3"><title>Appearance-evoked response</title><p>The appearance-evoked response is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Note that these data have been baseline corrected relative to the 200 ms period prior to the appearance event. Thus, effects reported in this section are specific to the appearance-evoked response and not merely a reflection of the pre-existing REG versus RAND effect observed for the scene-evoked response.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.19113.005</object-id><label>Figure 3.</label><caption><title>Appearance-evoked response.</title><p>(<bold>A</bold>) RMS time-course of the appearance-evoked response showing the main effect of scene temporal structure (REG versus RAND). Thick horizontal green lines indicate time points for which there were significant differences in RMS between REG and RAND conditions (p&lt;0.05 FWE corrected at the cluster level; Thin light-green lines show uncorrected clusters). Purple lines indicate (jackknife-estimated) latencies of the onset of the REG versus RAND effect (horizontal and vertical portions indicate mean and jackknife-corrected standard error, respectively). Also shown are topographical patterns at the time of the appearance-evoked M50 (72–112 ms), M100 (144–188 ms) and M200 (232–360 ms) components. (<bold>B</bold>) Mean RMS over the appearance-evoked M50 period (712–112 ms). Asterisk indicates the significant (p&lt;0.05) interaction ([REG&gt;RAND]&gt;[Passive&gt;Active]). Error bars represent within-subject standard error of the mean (computed separately for Passive and Active groups. (<bold>C</bold>) Same as panel A but showing main effect of appearing source structure (temporally regular versus random). See also <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for the MEG time-course averaged over selected sensors responsive to the appearance-evoked M50 component.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.19113.005">http://dx.doi.org/10.7554/eLife.19113.005</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-19113-fig3-v1"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.19113.006</object-id><label>Figure 3—figure supplement 1.</label><caption><title>MEG time-course averaged over selected sensors responsive to the appearance-evoked M50 component.</title><p>(<bold>A</bold>) MEG from sensors showing positive signal at the time of the appearance-evoked M50 component. Thick horizontal green lines indicate time points for which there were significant differences in MEG amplitude between REG and RAND conditions (p&lt;0.05 FWE corrected at the cluster level; Thin light-green lines show uncorrected clusters). (<bold>B</bold>) MEG from sensors showing negative signal at the time of the appearance-evoked M50 component.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.19113.006">http://dx.doi.org/10.7554/eLife.19113.006</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-19113-fig3-figsupp1-v1"/></fig></fig-group></p><p>In the Active group, the appearance-evoked response is characterized by a typical pattern of M50/M100/M200 deflections frequently observed at sound onset (as seen above) and following changes within an ongoing sound sequence (<xref ref-type="bibr" rid="bib53">Martin and Boothroyd, 2000</xref>; <xref ref-type="bibr" rid="bib33">Gutschalk et al., 2004</xref>; <xref ref-type="bibr" rid="bib12">Chait et al., 2008</xref>; <xref ref-type="bibr" rid="bib73">Sohoglu and Chait, 2016</xref>). Although the responses here are characterized by later latencies (around 90, 150 and 300 ms, respectively) than those typically observed in other studies that report similar deflections. This may be due to the higher complexity of the present stimuli, which is known to lead to delayed responses (see e.g. <xref ref-type="bibr" rid="bib12">Chait et al., 2008</xref>; <xref ref-type="bibr" rid="bib73">Sohoglu and Chait, 2016</xref>). M50 and M200 deflections are also observed in the Passive group but we note with interest the absence of a prominent M100 component, consistent with previous reports of this component being particularly sensitive to attention and/or task-related demands (<xref ref-type="bibr" rid="bib1">Ahveninen et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib46">Königs and Gutschalk, 2012</xref>; <xref ref-type="bibr" rid="bib73">Sohoglu and Chait, 2016</xref>).</p><p>As shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, cluster-based statistics showed a significant effect of scene structure on the appearance-evoked response from 96 ms in the Passive group and from 260 ms in the Active group, both involving an increased neural response for REG versus RAND conditions. This effect was apparent in the Passive group already at the earliest M50 component while in the Active group, it was confined to later components of the evoked response (M200 at corrected significance; M100 uncorrected). As shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, the topographical patterns for REG and RAND conditions were qualitatively similar in both Passive and Active groups.</p><p>As described previously, the appearance-evoked response was derived by baseline correcting relative to the 200 ms period prior to source appearance and therefore the measured effect of REG versus RAND is distinct to that observed for the scene-evoked response. To confirm this, we also analyzed matched trials in which there was no change (no appearing source; shown as transparent traces in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). For this analysis, no effect of REG versus RAND was observed, confirming that scene structure modulates the appearance-evoked response in addition to the scene-evoked response.</p><p>The cluster-based permutation statistics above imply an interaction between scene structure and group involving an earlier effect of scene structure in Passive versus Active groups. To directly test this interaction, we estimated the onset latency of the scene structure effect using the jackknife procedure and assessed whether this latency differed significantly between groups. The scene structure effect was estimated to occur on average, 55 ms earlier in Passive versus Active groups (mean onset latency = 87 ms for Passive; 142 ms for Active). This difference was confirmed significant using a jackknife adjusted independent samples test (t(25) = −3.96, p&lt;0.001). This analysis is consistent with the cluster-based permutation statistics above also suggesting a scene structure effect on the early M50 component only in the Passive group. To further characterize this scene structure by group interaction on the M50 peak, a post-hoc between-group t-test (one-tailed) was conducted on the difference in MEG response between REG and RAND conditions at the time of the appearance-evoked M50 (72–112 ms). As shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref>, the difference in MEG response between REG and RAND conditions was significantly stronger in Passive versus Active groups (t(25) = 1.78, p&lt;0.05.).</p><p>An alternative explanation of the interaction between group and scene structure is possible if the M50 and M100 peaks reflect independent but temporally and spatially overlapping components. By this account, the increased response for REG versus RAND scenes at the M50 does not differ between Passive and Active groups. Rather, REG scenes result in an increased M100 in Active listeners that causes a reduction in the M50 (due to their opposite polarities; see topographic plots in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). To explore this possibility, we selected the twenty most positive and twenty most negative channels at the time of the M50 deflection (72–112 ms; pooling over REG and RAND conditions). The MEG signal was then averaged across channels within these two (positive and negative) groupings and the resulting time-courses analyzed (shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). As this analysis is based on the mean (rather than RMS) neural response across channels, the polarity of the signal is preserved and thus potentially provides a more accurate representation of the underlying dynamics. Furthermore, the selection of channels based on the M50 deflection would be expected to attenuate interfering responses from the M100 component. As shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, consistent with the earlier RMS analysis, the M50 showed a larger response for REG versus RAND scenes in Passive but not in Active listeners. This is despite the M100 showing no evidence of modulation by scene structure in Active listeners (even at an uncorrected threshold of p&lt;0.05), making it unlikely the pattern of results reflect a suppression of the M50 by the M100.</p><p>The appearance-evoked response as a function of the temporal structure of the appearing source was also analyzed and is shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. Despite listeners’ detection times being somewhat quicker when the appearing source was temporally regular versus random (by ~27 ms on average across the group; shown earlier in <xref ref-type="fig" rid="fig1">Figure 1B</xref>), no significant differences in MEG response were observed in Passive or Active groups. Neither was there a significant interaction between scene and appearing source structure. However, we cannot rule out modulation of more temporally variable neural processes not captured by the evoked analysis employed here. Since the behavioral effects were only observed in detection times, it is also possible that the relevant brain activity is masked by motor response-related processes.</p></sec><sec id="s2-4"><title>Source reconstruction</title><p>Finally, we localized the neural generators of the scene structure effect. As shown in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, the scene-evoked response (averaged from 500 to 800 ms) showed greater source power for REG versus RAND scenes in both hemispheres of the superior temporal lobe, including primary auditory cortex, planum temporale and the superior temporal gyrus (peak voxel locations are reported in <xref ref-type="table" rid="tbl1">Table 1</xref>). An additional distinct cluster of activation is observed in left post central gyrus of the superior parietal lobe.<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.19113.007</object-id><label>Figure 4.</label><caption><title>Source reconstruction.</title><p>(<bold>A</bold>) Main effect of scene temporal structure at the time of the sustained portion of the scene-evoked response (500–800 ms post scene onset). Statistical map is overlaid onto an MNI space template brain, viewed over the left and right hemispheres. Color-bar indicates statistical threshold. (<bold>B</bold>) [REG&gt;RAND]&gt;[Passive&gt;Active] interaction at the time of the appearance-evoked M50 component (72–112 ms post appearance).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.19113.007">http://dx.doi.org/10.7554/eLife.19113.007</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-19113-fig4-v1"/></fig><table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.19113.008</object-id><label>Table 1.</label><caption><p>Peak voxel locations (in MNI space) and summary statistics from source reconstruction. Activations for the scene-evoked analysis are for the REG&gt;RAND contrast (500–800 ms post scene onset) while those for the appearance-evoked analysis are for the [REG&gt;RAND]&gt;[Passive&gt;Active] interaction contrast (72–112 ms post appearance). Activations have been thresholded using the same parameters as for <xref ref-type="fig" rid="fig4">Figure 4</xref> (p&lt;0.001 for scene-evoked; p&lt;0.01 for appearance-evoked) but with an additional cluster extent threshold of n &gt; 15 voxels (for display purposes).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.19113.008">http://dx.doi.org/10.7554/eLife.19113.008</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th colspan="3" valign="bottom">MNI Coordinates</th></tr><tr><th valign="bottom">Analysis</th><th valign="bottom">Region</th><th valign="bottom">Side</th><th valign="bottom">Extent</th><th valign="bottom">t-value</th><th valign="bottom">x</th><th valign="bottom">y</th><th valign="bottom">z</th></tr></thead><tbody><tr><td valign="bottom">Scene-evoked</td><td valign="bottom">Planum Temporale/Parietal Operculum</td><td valign="bottom">Left</td><td valign="bottom">1418</td><td valign="bottom">5.2779</td><td valign="bottom">−48</td><td valign="bottom">−28</td><td valign="bottom">16</td></tr><tr><td valign="bottom">(500-800 ms post scene onset)</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">4.364</td><td valign="bottom">−62</td><td valign="bottom">−50</td><td valign="bottom">14</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">3.9777</td><td valign="bottom">−52</td><td valign="bottom">−30</td><td valign="bottom">-4</td></tr><tr><td valign="bottom"/><td valign="bottom">Postcentral Gyrus</td><td valign="bottom">Left</td><td valign="bottom">204</td><td valign="bottom">4.8082</td><td valign="bottom">−32</td><td valign="bottom">−36</td><td valign="bottom">64</td></tr><tr><td valign="bottom"/><td valign="bottom">Supramarginal Gyrus</td><td valign="bottom">Right</td><td valign="bottom">704</td><td valign="bottom">4.2469</td><td valign="bottom">64</td><td valign="bottom">−24</td><td valign="bottom">24</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">3.7176</td><td valign="bottom">44</td><td valign="bottom">−6</td><td valign="bottom">16</td></tr><tr><td valign="bottom"/><td valign="bottom">Planum Temporale</td><td valign="bottom">Right</td><td valign="bottom">582</td><td valign="bottom">3.9459</td><td valign="bottom">64</td><td valign="bottom">−16</td><td valign="bottom">6</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">3.9252</td><td valign="bottom">46</td><td valign="bottom">−26</td><td valign="bottom">6</td></tr><tr><td valign="bottom"/><td valign="bottom">Precentral Gyrus</td><td valign="bottom">Right</td><td valign="bottom">19</td><td valign="bottom">3.6051</td><td valign="bottom">60</td><td valign="bottom">6</td><td valign="bottom">18</td></tr><tr><td valign="bottom">Appearance-evoked</td><td valign="bottom">Precentral Gyrus</td><td valign="bottom">Left</td><td valign="bottom">190</td><td valign="bottom">3.2219</td><td valign="bottom">−50</td><td valign="bottom">−6</td><td valign="bottom">44</td></tr><tr><td valign="bottom">(72-112 ms post appearance)</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">2.9902</td><td valign="bottom">−34</td><td valign="bottom">6</td><td valign="bottom">38</td></tr><tr><td valign="bottom"/><td valign="bottom">Precentral Gyrus/Central Operculum</td><td valign="bottom">Right</td><td valign="bottom">711</td><td valign="bottom">3.1966</td><td valign="bottom">56</td><td valign="bottom">0</td><td valign="bottom">10</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">3.0153</td><td valign="bottom">56</td><td valign="bottom">4</td><td valign="bottom">−10</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">2.9101</td><td valign="bottom">36</td><td valign="bottom">−8</td><td valign="bottom">16</td></tr><tr><td valign="bottom"/><td valign="bottom">Middle Temporal Gyrus</td><td valign="bottom">Right</td><td valign="bottom">157</td><td valign="bottom">2.9859</td><td valign="bottom">58</td><td valign="bottom">−2</td><td valign="bottom">−24</td></tr><tr><td valign="bottom"/><td valign="bottom">Middle Temporal Gyrus</td><td valign="bottom">Right</td><td valign="bottom">55</td><td valign="bottom">2.6951</td><td valign="bottom">52</td><td valign="bottom">−54</td><td valign="bottom">8</td></tr><tr><td valign="bottom"/><td valign="bottom">Precentral Gyrus</td><td valign="bottom">Right</td><td valign="bottom">21</td><td valign="bottom">2.6444</td><td valign="bottom">54</td><td valign="bottom">−4</td><td valign="bottom">40</td></tr><tr><td valign="bottom"/><td valign="bottom">Postcentral Gyrus</td><td valign="bottom">Left</td><td valign="bottom">16</td><td valign="bottom">2.5982</td><td valign="bottom">-30</td><td valign="bottom">−34</td><td valign="bottom">68</td></tr></tbody></table></table-wrap></p><p>For the appearance-evoked response, we focused on the scene structure by group interaction ([REG&gt;RAND] &gt; [Passive&gt;Active]) that emerged during the early M50 component. As shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref>, this effect localized to similar regions as for the scene-evoked response: superior/middle temporal lobe (albeit in the right hemisphere only) and post central gyrus. Additional activation is observed more anteriorly in the pre central gyrus, extending into the middle frontal gyrus.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The present study used psychophysics and MEG recordings of brain activity to understand how regular temporal structure facilitates auditory scene analysis. We demonstrate that listeners’ ability to detect the appearance of a new source was enhanced in temporally regular scenes. These behavioral benefits of statistical structure on scene analysis are associated with increased neural responses occurring before as well as after source appearance.</p><sec id="s3-1"><title>Adaptation versus precision</title><p>Around 400 ms following scene onset, we observed an increase in the sustained MEG response for scenes consisting of regularly structured, relative to randomly fluctuating sources. This finding is opposite to what would be expected based on adaptation i.e. <italic>decreased</italic> neural responses for temporally regular events, which has previously been observed for isolated tone sequences (<xref ref-type="bibr" rid="bib15">Costa-Faidella et al., 2011</xref>; <xref ref-type="bibr" rid="bib71">Schwartze et al., 2013</xref>; <xref ref-type="bibr" rid="bib79">Tavano et al., 2014</xref>). It is however consistent with a mechanism that infers the precision (predictability) of sensory input and uses this information to up-regulate neural processing towards more reliable sensory signals (<xref ref-type="bibr" rid="bib24">Feldman and Friston, 2010</xref>; <xref ref-type="bibr" rid="bib86">Zhao et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Auksztulewicz and Friston, 2015</xref>; <xref ref-type="bibr" rid="bib7">Barascud et al., 2016</xref>). Indeed, it has recently been demonstrated that the magnitude of sustained MEG activity (from naïve distracted listeners) tracks the predictability of rapid tone sequences (<xref ref-type="bibr" rid="bib7">Barascud et al., 2016</xref>). In that study, regularity was characterized by a spectral pattern repeating over time within a single ongoing tone sequence. Although distinct to the temporal regularity studied here, the ensuing effect on MEG response is strikingly similar to the sustained effect we observe. Importantly, the current findings demonstrate mechanisms that automatically (irrespective of directed attention) and rapidly (within 400 ms of scene onset) encode regularities distributed over many concurrent sources, typical of natural listening environments.</p><p>If the auditory system can form precise models about the content of ongoing scenes, novel events that violate those models would evoke greater neural responses and be perceived as more salient. Indeed, listeners were better and faster at detecting an appearing source in regular versus random scenes. The MEG response in naïve, passively listening subjects revealed a large (22%) increase in the evoked response starting from the very first response deflection (M50 component) following source appearance. Importantly, this effect occurred over and above that observed prior to the appearance event, demonstrating bottom-up driven ‘surprise’ responses tightly linked to the predictability of the ongoing scene context. Interestingly, the REG&gt;RAND effect emerged substantially later when participants were actively attending to the appearance events. More discussion of that is below.</p><p>Overall, the results demonstrate that the enhanced detection performance observed in behavior is not solely the result of changes in neural responses occurring prior to source appearance (cf. adaptation accounts; <xref ref-type="bibr" rid="bib54">May et al., 1999</xref>; <xref ref-type="bibr" rid="bib40">Jääskeläinen et al., 2004</xref>) but also due to enhanced neural responses to novel events themselves. This is again what would be expected based on precision accounts and is also consistent with animal physiology work showing that the magnitude of responses in single neurons of auditory cortex to new (‘deviant’) tones is larger than expected based on simple adaptation to previously repeated (‘standard’) tones alone (<xref ref-type="bibr" rid="bib44">Khouri and Nelken, 2015</xref>).</p></sec><sec id="s3-2"><title>Neural sources</title><p>Source reconstruction suggests that neural responses in a network of brain regions are modulated by scene temporal structure, including early auditory regions in the superior temporal lobe but also left parietal cortex (post central gyrus). This is consistent with evidence from neuroimaging (<xref ref-type="bibr" rid="bib68">Rao et al., 2001</xref>; <xref ref-type="bibr" rid="bib16">Coull and Nobre, 2008</xref>; <xref ref-type="bibr" rid="bib2">Andreou et al., 2015</xref>), electrophysiology (<xref ref-type="bibr" rid="bib48">Leon et al., 2003</xref>; <xref ref-type="bibr" rid="bib41">Janssen and Shadlen, 2005</xref>) and lesion studies (<xref ref-type="bibr" rid="bib35">Harrington et al., 1998</xref>; <xref ref-type="bibr" rid="bib8">Battelli et al., 2008</xref>) implicating a specific role for left parietal cortex in temporal processing. Parietal cortex has also been associated with figure-ground processing when the figure is defined by temporally repeatable spectral components in an otherwise randomly structured background (<xref ref-type="bibr" rid="bib81">Teki et al., 2011</xref>; <xref ref-type="bibr" rid="bib80">Teki et al., 2016</xref>). Thus, together the current study and previous findings suggest parietal cortex may be part of a wider network (along with auditory cortical regions) that codes the temporal structure of acoustic scenes. Alternatively, the parietal activity changes we observe may reflect a more domain-general increase in bottom-up saliency attributable to regularity (<xref ref-type="bibr" rid="bib14">Corbetta and Shulman, 2002</xref>; <xref ref-type="bibr" rid="bib86">Zhao et al., 2013</xref>).</p><p>We note however that although <xref ref-type="bibr" rid="bib7">Barascud et al. (2016)</xref> report effects of statistical structure in early auditory regions (like the current findings), they did not observe changes in MEG and fMRI responses in parietal cortex. Instead, spectral regularity modulated activity in the inferior frontal gyrus. This is may suggest a degree of neural specialization for the particular type of regularity encoded e.g. temporal-based involving parietal cortex versus spectral-based involving inferior frontal regions. Future work is required however to determine whether temporal and spectral regularities are encoded by distinct neural substrates (e.g. by contrasting neural effects of temporal and spectral regularities in the same experiment).</p></sec><sec id="s3-3"><title>Role of attention</title><p>Following scene onset (prior to new source appearance), the neural influence of regularity showed no evidence of attentional modulation (the strength of the scene-evoked response to regularly and randomly structured scenes was statistically indistinguishable in passive compared with active listening subjects). This suggests that the brain automatically encodes scene regularities, irrespective of directed attention. After the appearance of a new source, however, regularity and attention had an interactive influence on the evoked response; whereas the first cortical deflection of the appearance-evoked response (M50) increased in regular scenes during passive listening, this effect was confined to later deflections (M100 and M200) when listeners actively detected source appearance.</p><p>How the neural influence of regularity might depend on attention is the subject of ongoing debate (<xref ref-type="bibr" rid="bib42">Jones and Boltz, 1989</xref>; <xref ref-type="bibr" rid="bib61">Näätänen et al., 2001</xref>; <xref ref-type="bibr" rid="bib76">Summerfield and Egner, 2009</xref>, <xref ref-type="bibr" rid="bib77">2016</xref>; <xref ref-type="bibr" rid="bib85">Winkler et al., 2009</xref>; <xref ref-type="bibr" rid="bib24">Feldman and Friston, 2010</xref>; <xref ref-type="bibr" rid="bib45">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib9">Bendixen, 2014</xref>; <xref ref-type="bibr" rid="bib75">Summerfield and de Lange, 2014</xref>; <xref ref-type="bibr" rid="bib70">Schröger et al., 2015</xref>). One proposal is that attention (like regularity) acts to determine the inferred precision of sensory input (<xref ref-type="bibr" rid="bib26">Friston, 2009</xref>; <xref ref-type="bibr" rid="bib7">Barascud et al., 2016</xref>). In this view, attention increases precision (and neural responses) when sensory signals are task-relevant. In this case inferred precision is changed not by the intrinsic structure of the stimulus (e.g. whether temporally regular or random) but by the behavioral goals of the listener. As precision is hypothesized to have a multiplicative (gain) influence on neural activity, this account would have predicted attentional enhancement of the regularity effect. Indeed, <xref ref-type="bibr" rid="bib39">Hsu et al. (2014)</xref> demonstrated greater EEG responses for predictable (ascending) pitch patterns, which was most apparent when those patterns were embedded in an attended stream. In contrast to this pattern, attention in our study delayed the influence of regularity on appearance-related responses.</p><p>How then might the current attentional effect be explained? We suggest that attention in our study acted as a form of expectation. That is, when listeners actively detected source appearance, scene changes were relatively more expected. If change-related responses reflect the amount of ‘surprise’ given the preceding stimulus context, then they should diminish when change is expected and counteract the precision-mediated increase from regularity. Thus, although counterintuitive, the later benefit from regularity when listeners are actively seeking source appearance is consistent with attention acting to reduce surprise.</p><p>In the auditory modality, the mismatch negativity (MMN) response is often interpreted as reflecting ‘surprise’ (<xref ref-type="bibr" rid="bib60">Näätänen et al., 2007</xref>; <xref ref-type="bibr" rid="bib28">Garrido et al., 2009</xref>). The M50 effect we observe occurs earlier and with a distinct topography to the MMN, but may relate to novelty effects on the so-called ‘middle-latency’ responses (~40 ms) revealed in other work (<xref ref-type="bibr" rid="bib11">Chait et al., 2007</xref>, <xref ref-type="bibr" rid="bib12">2008</xref>; <xref ref-type="bibr" rid="bib31">Grimm et al., 2011</xref>; <xref ref-type="bibr" rid="bib69">Recasens et al., 2014</xref>).</p><p>While the proposal that attention acts to reduce surprise may appear at odds with the widespread view of attention playing a distinct functional role to expectation (<xref ref-type="bibr" rid="bib76">Summerfield and Egner, 2009</xref>, <xref ref-type="bibr" rid="bib77">2016</xref>), one associated with enhanced neural processing of attended signals (<xref ref-type="bibr" rid="bib19">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib27">Fritz et al., 2003</xref>), it is consistent with previous observations. In <xref ref-type="bibr" rid="bib13">Chennu et al. (2013)</xref>, listeners were presented with tone sequences containing regularities unfolding over multiple (local and global) timescales. When listeners were instructed to detect deviant tones on a local timescale, the mismatch negativity component indexing that local regularity was attenuated compared with when listeners detected global deviants. The authors interpreted this suppression effect as reflecting reduced surprise from 'top-down expectation (or bias) and consequent attentional focus'. Similarly, <xref ref-type="bibr" rid="bib74">Spratling (2008)</xref> argues that attention and expectation are part of the same general class of top-down signal that act in a similar fashion to modulate perceptual processing. Thus, in this view, the distinction between attention and expectation is blurred and whether these phenomena result in increased or decreased neural processes will depend on the precise details of the stimuli and behavioral demands (<xref ref-type="bibr" rid="bib70">Schröger et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Henson, 2016</xref>). In this respect, we note that previous investigations of regularity and attention employed static or relatively slow-evolving stimuli (1–5 Hz) and often containing a single perceptual object (image of a face or tone sequence). This may have enabled conscious awareness of stimulus content, involving distinct processes to those relevant to the rapidly evolving and complex scenes employed here and, arguably, to the perceptual challenges faced in natural environments.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Two groups of participants were tested after being informed of the study’s procedure, which was approved by the research ethics committee of University College London. The two groups differed in whether participants’ attention was directed away (‘Passive’ group) or towards (‘Active’ group) auditory stimulation (see Procedure section below). The Passive group comprised 14 (6 female) participants aged between 19 and 34 years (mean = 23.6, SD = 4.68). All but one of these participants was right-handed. The Active group comprised 13 (7 female), different, right-handed participants aged between 18 and 33 years (mean = 24.3, SD = 4.91). All reported normal hearing, normal or corrected-to-normal vision, and had no history of neurological disorders. There were no significant differences between groups in terms of gender (two-tailed χ(1) =. 326, p=0.568) or age (two-tailed t(25) = 0.35, p= 0.73).</p><sec id="s4-1-1"><title>Stimuli</title><p>Stimuli were 2500–3500 ms duration artificial acoustic ‘scenes’ populated by seven to eight streams of pure-tones designed to model auditory sources (shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Each of these sources had a unique carrier frequency (drawn from a pool of fixed values spaced at 2*ERB between 200 and 2800 Hz; <xref ref-type="bibr" rid="bib58">Moore and Glasberg, 1983</xref>) and temporal structure (see below). Previous experiments have demonstrated that these scenes are perceived as composite ‘sound-scapes’ in which individual sources can be perceptually segregated and selectively attended to, and are therefore good models for listening in natural acoustic scenes (<xref ref-type="bibr" rid="bib10">Cervantes Constantino et al., 2012</xref>). The large spectral separation between neighboring sources (at least two ERBs) was chosen to minimize energetic masking at the peripheral stages of auditory processing (<xref ref-type="bibr" rid="bib57">Moore, 1987</xref>). Signals were synthesized with a sampling rate of 44,100 Hz and shaped with a 30 ms raised cosine onset and offset ramp. They were delivered diotically to the subjects' ears with tubephones (EARTONE 3A 10 Ω, Etymotic Research, Inc) and adjusted to a comfortable listening level.</p><p>As shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>, a scene change involving the appearance of a new source, could occur partway through the stimulus. The timing of source appearance varied randomly (uniformly distributed between 1000 ms and 2000 ms post scene onset). To facilitate evoked response analysis, the interval between the time of source appearance and scene offset was fixed at 1500 ms. In the other half of scenes presented, no change occurred (‘No Change’). The specific configuration of carrier frequencies and temporal modulation patterns varied randomly across scenes. To enable a controlled comparison between conditions, scenes with and without appearing sources were derived from the same configurations of carrier frequencies and modulation patterns, and then presented in random order during the experiment.</p><p>The duration of the tone-pips comprising each source (varying uniformly between 22 and 167 ms) and the silent interval between tone-pips (varying uniformly between 1 and 167 ms) were chosen independently. In ‘Regular’ (REG) scenes, these tone/silence intervals were fixed so that the temporal structure was regular. This pattern mimics the regularly modulated temporal properties of many natural sounds. In ‘Random’ (RAND) scenes, tone duration was also fixed but the silent intervals between successive tones varied randomly (with the same distribution as REG scenes i.e. 1–167 ms) resulting in an irregular pattern. Importantly, the above manipulation of scene temporal structure was applied independently of the regularity of the appearing source: Appearing sources could be regular or random (equal proportion), independently of the regularity of the rest of the scene (REG or RAND; equal proportion). Stimuli were randomly ordered during each of eight presentation blocks of 96 trials. The inter-stimulus interval varied randomly between 900 and 1100 ms.</p></sec><sec id="s4-1-2"><title>Procedure</title><p>Stimulus delivery was controlled with Cogent software (<ext-link ext-link-type="uri" xlink:href="http://www.vislab.ucl.ac.uk/cogent.php">http://www.vislab.ucl.ac.uk/cogent.php</ext-link>). In the Passive group, participants were naïve to the sounds and engaged in an incidental visual task while looking at a central fixation cross. Participants in this group were instructed to press a button (with their right hand) each time a brief (100 ms duration) image of a pre-defined (target) object appeared on the display at fixation. The target was different on each block and was presented rarely (20%) amongst a stream of non-target images. The inter-image interval ranged from around 500 to 4000 ms and was randomly timed with respect to auditory stimulation. Hit rates ranged from 81 to 95% with false alarm rates below 1%, confirming engagement with the task. In the Active group, participants were instructed to listen carefully to the sounds while looking at a central fixation cross and press a button (with their right hand) as soon as they detected a change in each acoustic scene. Before the experiment, participants in both groups completed a brief (~2.5 min) practice session to familiarize themselves with the task.</p></sec></sec><sec id="s4-2"><title>Behavioral statistical analysis</title><p>d’ scores were obtained for the Active group by first computing for each subject and condition, the hit rate (proportion of source appearances correctly detected) and false alarm rate (proportion of ‘No Change’ trials for which responses were made). Following this, each d’ score was computed as the difference in the z-transformed hit rate and false alarm rate. Detection time was measured between the time of new source appearance and the subject’s key press.</p></sec><sec id="s4-3"><title>MEG data acquisition and pre-processing</title><p>Magnetic fields were recorded with a CTF-275 MEG system, with 274 functioning axial gradiometers arranged in a helmet shaped array. Electrical coils were attached to three anatomical fiducial points (nasion and left and right pre-auricular), in order to continuously monitor the position of each participant’s head with respect to the MEG sensors.</p><p>The MEG data were analyzed in SPM12 (Wellcome Trust Centre for Neuroimaging, London, UK) and FieldTrip (Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen, the Netherlands) software implemented in Matlab. The data were downsampled to 250 Hz, low-pass filtered at 30 Hz and epoched −200 to 800 ms relative to scene onset (to obtain the scene-evoked response) or −200 to 400 ms relative to the time of the appearance event (to obtain the appearance-evoked response). This epoch encompassed detection-related brain processes leading up to the initiation of the behavioral response in the Active group, which ranged from 465 to 911 ms across participants and conditions. After epoching, the data were baseline-corrected relative to the 200 ms period prior to scene onset (for the scene-evoked data) or prior to the time of source appearance (for the appearance-evoked data).</p><p>Subsequent preprocessing differed depending on whether the analysis was conducted in sensor- or source-space. For sensor-space analysis, any trials in which the data deviated by more than three standard deviations from the mean were discarded. Following outlier removal, Denoising Source Separation (DSS) was applied to maximize reproducibility of the evoked response across trials (<xref ref-type="bibr" rid="bib18">de Cheveigné and Simon, 2008</xref>; <xref ref-type="bibr" rid="bib17">de Cheveigné and Parra, 2014</xref>). For each subject, the first two DSS components (i.e., the two ‘most reproducible’ components; determined −200 to 800 ms relative to scene onset) were retained and used to project both the scene-evoked and appearance-evoked data back into sensor-space, which were then averaged across trials. For source-space analysis, DSS was not performed. Instead, the data were robust averaged across trials to downweight outlying samples (<xref ref-type="bibr" rid="bib84">Wager et al., 2005</xref>; <xref ref-type="bibr" rid="bib37">Litvak et al., 2011</xref>). To remove any high-frequency components that were introduced to the data by the robust averaging procedure, low-pass filtering was repeated after averaging.</p><p>Note that although images were presented only in the Passive group, auditory and visual events were temporally uncorrelated. Thus, in both Passive and Active groups, our MEG measures are expected to reflect primarily auditory (and not visual) evoked activity.</p></sec><sec id="s4-4"><title>MEG statistical analysis</title><p>MEG data across the sensor array were summarized as the root mean square (RMS) across sensors for each time sample within the epoch period, reflecting the instantaneous magnitude of neuronal responses. Group-level paired t-tests were performed for each time sample while controlling the family-wise error (FWE) rate using a non-parametric (cluster-based) permutation procedure based on 5000 iterations (<xref ref-type="bibr" rid="bib52">Maris and Oostenveld, 2007</xref>). Reported effects were obtained by using a cluster defining height threshold of p&lt;0.05 with a cluster size threshold of p&lt;0.05 (FWE corrected), unless otherwise stated.</p><p>Statistical tests of evoked response latency differences were conducted on subsamples of the grand averaged RMS time-course using the jackknife procedure (<xref ref-type="bibr" rid="bib21">Efron, 1981</xref>). In the jackknife procedure, the grand averaged data are resampled <italic>n</italic> times (with <italic>n</italic> being the number of participants) while omitting one participant from each subsample. Statistical reliability of an effect can then be assessed using standard tests (e.g. t-test), not across individual participants, but across subsamples of the grand average. This technique has been shown to be superior to computing latency differences from individual participant data because of the higher signal-to-noise ratio associated with grand averages (<xref ref-type="bibr" rid="bib56">Miller et al., 1998</xref>; <xref ref-type="bibr" rid="bib83">Ulrich and Miller, 2001</xref>). Jackknife-estimated latencies of the scene structure effect were determined by first computing the difference waveform between REG and RAND scenes and then for each jackknife subsample, computing the first latency at which the magnitude of the difference waveform deviated by more than three standard deviations from the mean RMS across time in the baseline period (−200 to 0 ms). When using the jackknife procedure, t-statistics were corrected following the procedure in <xref ref-type="bibr" rid="bib56">Miller et al. (1998)</xref> (multiplication of the subsample standard error by a factor of <italic>n</italic>-1).</p><p>To determine the underlying brain sources of the sensor-space effects, we used a distributed method of source reconstruction, implemented within the parametric empirical Bayes framework of SPM12 (<xref ref-type="bibr" rid="bib64">Phillips et al., 2005</xref>; <xref ref-type="bibr" rid="bib49">Litvak and Friston, 2008</xref>; <xref ref-type="bibr" rid="bib37">Henson et al., 2011</xref>). Participant-specific forward models were computed using a Single Shell model and sensor positions projected onto an MNI space template brain by minimizing the sum of squared differences between the digitized fiducials and the MNI template. For inversion of the forward model, we used the ‘LOR’ routine in SPM12, which assumes that all sources are activated with equal apriori probability and with weak correlation to neighboring sources. This was applied to the entire epoch (−200 to 800 ms for scene-evoked data; −200 to 400 ms for appearance-evoked data).</p><p>Source solutions were constrained to be consistent across subjects (pooled over Passive and Active groups), which has been shown to improve group-level statistical power (<xref ref-type="bibr" rid="bib49">Litvak and Friston, 2008</xref>; <xref ref-type="bibr" rid="bib37">Henson et al., 2011</xref>). In brief, this procedure involves 1) realigning and concatenating sensor-level data across subjects 2) estimating a single source solution for all subjects 3) using the resulting group solution as a Bayesian prior on individual subject inversions. Thus, this method exploits the availability of repeated measurements (from different subjects) to constrain source reconstruction. Importantly, however, this procedure does not bias activation differences between conditions in a given source.</p><p>Significant effects from sensor-space were localized within the brain (in MNI space, constrained to gray matter) after summarizing source power in the 0–30 Hz range for each participant and time-window of interest using a Morlet wavelet projector (<xref ref-type="bibr" rid="bib25">Friston et al., 2006</xref>). Given that the goal of source reconstruction was to localize the neural generators of sensor-space effects previously identified as significant, statistical maps of source activity are displayed with uncorrected voxelwise thresholds (<xref ref-type="bibr" rid="bib32">Gross et al., 2012</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was supported by a BBSRC project grant BB/K003399/1 awarded to MC. We are grateful to Letty Manyande for technical support during data collection and to Alain de Cheveigné for advice with Denoising Source Separation.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>ES, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>MC, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Experimental procedures were approved by the research ethics committee of University College London, and written informed consent was obtained from each participant. Subjects were paid for their participation.</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahveninen</surname><given-names>J</given-names></name><name><surname>Hämäläinen</surname><given-names>M</given-names></name><name><surname>Jääskeläinen</surname><given-names>IP</given-names></name><name><surname>Ahlfors</surname><given-names>SP</given-names></name><name><surname>Huang</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>FH</given-names></name><name><surname>Raij</surname><given-names>T</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Vasios</surname><given-names>CE</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Attention-driven auditory cortex short-term plasticity helps segregate relevant sounds from noise</article-title><source>PNAS</source><volume>108</volume><fpage>4182</fpage><lpage>4187</lpage><pub-id pub-id-type="doi">10.1073/pnas.1016134108</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andreou</surname><given-names>LV</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sensitivity to the temporal structure of rapid sound sequences - An MEG study</article-title><source>NeuroImage</source><volume>110</volume><fpage>194</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.01.052</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andreou</surname><given-names>LV</given-names></name><name><surname>Kashino</surname><given-names>M</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The role of temporal regularity in auditory segregation</article-title><source>Hearing Research</source><volume>280</volume><fpage>228</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2011.06.001</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Transitions in neural oscillations reflect prediction errors generated in audiovisual speech</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>797</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1038/nn.2810</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auksztulewicz</surname><given-names>R</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attentional Enhancement of Auditory Mismatch Responses: a DCM/MEG Study</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu323</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Visual objects in context</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>617</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1038/nrn1476</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barascud</surname><given-names>N</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns</article-title><source>PNAS</source><volume>113</volume><fpage>E616</fpage><lpage>E625</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508523113</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battelli</surname><given-names>L</given-names></name><name><surname>Walsh</surname><given-names>V</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The 'when' parietal pathway explored by lesion studies</article-title><source>Current Opinion in Neurobiology</source><volume>18</volume><fpage>120</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2008.08.004</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bendixen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Predictability effects in auditory scene analysis: a review</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.3389/fnins.2014.00060</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cervantes Constantino</surname><given-names>F</given-names></name><name><surname>Pinggera</surname><given-names>L</given-names></name><name><surname>Paranamana</surname><given-names>S</given-names></name><name><surname>Kashino</surname><given-names>M</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Detection of appearing and disappearing objects in complex acoustic scenes</article-title><source>PLoS ONE</source><volume>7</volume><elocation-id>e46167</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0046167</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chait</surname><given-names>M</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Processing asymmetry of transitions between order and disorder in human auditory cortex</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>5207</fpage><lpage>5214</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0318-07.2007</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chait</surname><given-names>M</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Auditory temporal edge detection in human auditory cortex</article-title><source>Brain Research</source><volume>1213</volume><fpage>78</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2008.03.050</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chennu</surname><given-names>S</given-names></name><name><surname>Noreika</surname><given-names>V</given-names></name><name><surname>Gueorguiev</surname><given-names>D</given-names></name><name><surname>Blenkmann</surname><given-names>A</given-names></name><name><surname>Kochen</surname><given-names>S</given-names></name><name><surname>Ibáñez</surname><given-names>A</given-names></name><name><surname>Owen</surname><given-names>AM</given-names></name><name><surname>Bekinschtein</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Expectation and attention in hierarchical auditory prediction</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>11194</fpage><lpage>11205</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0114-13.2013</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>3</volume><fpage>201</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1038/nrn755</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa-Faidella</surname><given-names>J</given-names></name><name><surname>Baldeweg</surname><given-names>T</given-names></name><name><surname>Grimm</surname><given-names>S</given-names></name><name><surname>Escera</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Interactions between &quot;what&quot; and &quot;when&quot; in the auditory system: temporal predictability enhances repetition suppression</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>18590</fpage><lpage>18597</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2599-11.2011</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coull</surname><given-names>J</given-names></name><name><surname>Nobre</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dissociating explicit timing from temporal expectation with fMRI</article-title><source>Current Opinion in Neurobiology</source><volume>18</volume><fpage>137</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2008.07.011</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Parra</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Joint decorrelation, a versatile tool for multichannel data analysis</article-title><source>NeuroImage</source><volume>98</volume><fpage>487</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.05.068</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Denoising based on spatial filtering</article-title><source>Journal of Neuroscience Methods</source><volume>171</volume><fpage>331</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2008.03.015</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neural mechanisms of selective visual attention</article-title><source>Annual Review of Neuroscience</source><volume>18</volume><fpage>193</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>PNAS</source><volume>109</volume><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Efron</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Nonparametric estimates of standard error: The jackknife, the bootstrap and other methods</article-title><source>Biometrika</source><volume>68</volume><fpage>589</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1093/biomet/68.3.589</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggermont</surname><given-names>JJ</given-names></name><name><surname>Ponton</surname><given-names>CW</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The neurophysiology of auditory perception: from single units to evoked potentials</article-title><source>Audiology and Neuro-Otology</source><volume>7</volume><fpage>71</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1159/000057656</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erviti</surname><given-names>M</given-names></name><name><surname>Semal</surname><given-names>C</given-names></name><name><surname>Demany</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Enhancing a tone by shifting its frequency or intensity</article-title><source>The Journal of the Acoustical Society of America</source><volume>129</volume><fpage>3837</fpage><lpage>3845</lpage><pub-id pub-id-type="doi">10.1121/1.3589257</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>H</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attention, uncertainty, and free-energy</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><fpage>215</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2010.00215</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Mattout</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian estimation of evoked and induced responses</article-title><source>Human Brain Mapping</source><volume>27</volume><fpage>722</fpage><lpage>735</lpage><pub-id pub-id-type="doi">10.1002/hbm.20214</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The free-energy principle: a rough guide to the brain?</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>293</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.04.005</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname><given-names>J</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Klein</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Rapid task-related plasticity of spectrotemporal receptive fields in primary auditory cortex</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>1216</fpage><lpage>1223</lpage><pub-id pub-id-type="doi">10.1038/nn1141</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrido</surname><given-names>MI</given-names></name><name><surname>Kilner</surname><given-names>JM</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The mismatch negativity: a review of underlying mechanisms</article-title><source>Clinical Neurophysiology</source><volume>120</volume><fpage>453</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2008.11.029</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrido</surname><given-names>MI</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Outlier responses reflect sensitivity to statistical structure in the human brain</article-title><source>PLoS Computational Biology</source><volume>9</volume><elocation-id>e1002999</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002999</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual perception and the statistical properties of natural scenes</article-title><source>Annual Review of Psychology</source><volume>59</volume><fpage>167</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.58.110405.085632</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grimm</surname><given-names>S</given-names></name><name><surname>Escera</surname><given-names>C</given-names></name><name><surname>Slabu</surname><given-names>L</given-names></name><name><surname>Costa-Faidella</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Electrophysiological evidence for the hierarchical organization of auditory change detection in the human brain</article-title><source>Psychophysiology</source><volume>48</volume><fpage>377</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2010.01073.x</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Barnes</surname><given-names>GR</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Hillebrand</surname><given-names>A</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Jerbi</surname><given-names>K</given-names></name><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Maess</surname><given-names>B</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Taylor</surname><given-names>JR</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Good practice for conducting and reporting MEG research</article-title><source>NeuroImage</source><volume>65</volume><fpage>349</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.10.001</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutschalk</surname><given-names>A</given-names></name><name><surname>Patterson</surname><given-names>RD</given-names></name><name><surname>Scherg</surname><given-names>M</given-names></name><name><surname>Uppenkamp</surname><given-names>S</given-names></name><name><surname>Rupp</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Temporal dynamics of pitch in human auditory cortex</article-title><source>NeuroImage</source><volume>22</volume><fpage>755</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.01.025</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haenschel</surname><given-names>C</given-names></name><name><surname>Vernon</surname><given-names>DJ</given-names></name><name><surname>Dwivedi</surname><given-names>P</given-names></name><name><surname>Gruzelier</surname><given-names>JH</given-names></name><name><surname>Baldeweg</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Event-related brain potential correlates of human auditory sensory memory-trace formation</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>10494</fpage><lpage>10501</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1227-05.2005</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrington</surname><given-names>DL</given-names></name><name><surname>Haaland</surname><given-names>KY</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical networks underlying mechanisms of time perception</article-title><source>Journal of Neuroscience </source><volume>18</volume><fpage>1085</fpage><lpage>1095</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartmann</surname><given-names>WM</given-names></name><name><surname>Goupell</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Enhancing and unmasking the harmonics of a complex tone</article-title><source> Journal of the Acoustical Society of America</source><volume>120</volume><fpage>2142</fpage><lpage>2157</lpage><pub-id pub-id-type="doi">10.1121/1.2228476</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Wakeman</surname><given-names>DG</given-names></name><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A parametric empirical bayesian framework for the EEG/MEGinverse problem: generativemodels for multi-subject and multi-modal integration</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.3389/fnhum.2011.00076</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Repetition suppression to faces in the fusiform face area: A personal and dynamic journey</article-title><source>Cortex</source><volume>80</volume><fpage>174</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.09.012</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>YF</given-names></name><name><surname>Hämäläinen</surname><given-names>JA</given-names></name><name><surname>Waszak</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Both attention and prediction are necessary for adaptive neuronal tuning in sensory processing</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><fpage>152</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2014.00152</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jääskeläinen</surname><given-names>IP</given-names></name><name><surname>Ahveninen</surname><given-names>J</given-names></name><name><surname>Bonmassar</surname><given-names>G</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Ilmoniemi</surname><given-names>RJ</given-names></name><name><surname>Levänen</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>FH</given-names></name><name><surname>May</surname><given-names>P</given-names></name><name><surname>Melcher</surname><given-names>J</given-names></name><name><surname>Stufflebeam</surname><given-names>S</given-names></name><name><surname>Tiitinen</surname><given-names>H</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Human posterior auditory cortex gates novel sounds to consciousness</article-title><source>PNAS</source><volume>101</volume><fpage>6809</fpage><lpage>6814</lpage><pub-id pub-id-type="doi">10.1073/pnas.0303760101</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janssen</surname><given-names>P</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A representation of the hazard rate of elapsed time in macaque area LIP</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1038/nn1386</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>MR</given-names></name><name><surname>Boltz</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Dynamic attending and responses to time</article-title><source>Psychological Review</source><volume>96</volume><fpage>459</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.96.3.459</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Julesz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Textons, the elements of texture perception, and their interactions</article-title><source>Nature</source><volume>290</volume><fpage>91</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1038/290091a0</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khouri</surname><given-names>L</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Detecting the unexpected</article-title><source>Current Opinion in Neurobiology</source><volume>35</volume><fpage>142</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.08.003</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Rahnev</surname><given-names>D</given-names></name><name><surname>Jehee</surname><given-names>JF</given-names></name><name><surname>Lau</surname><given-names>HC</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Attention reverses the effect of prediction in silencing sensory signals</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>2197</fpage><lpage>2206</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr310</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Königs</surname><given-names>L</given-names></name><name><surname>Gutschalk</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Functional lateralization in auditory cortex under informational masking and in silence</article-title><source>European Journal of Neuroscience</source><volume>36</volume><fpage>3283</fpage><lpage>3290</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2012.08240.x</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kouider</surname><given-names>S</given-names></name><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Le Stanc</surname><given-names>L</given-names></name><name><surname>Charron</surname><given-names>S</given-names></name><name><surname>Fievet</surname><given-names>AC</given-names></name><name><surname>Barbosa</surname><given-names>LS</given-names></name><name><surname>Gelskov</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural dynamics of prediction and surprise in infants</article-title><source>Nature Communications</source><volume>6</volume><fpage>8537</fpage><pub-id pub-id-type="doi">10.1038/ncomms9537</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leon</surname><given-names>MI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Representation of time by neurons in the posterior parietal cortex of the macaque</article-title><source>Neuron</source><volume>38</volume><fpage>317</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00185-5</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Electromagnetic source reconstruction for group studies</article-title><source>NeuroImage</source><volume>42</volume><fpage>1490</fpage><lpage>1498</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.06.022</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Mattout</surname><given-names>J</given-names></name><name><surname>Kiebel</surname><given-names>S</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name><name><surname>Kilner</surname><given-names>J</given-names></name><name><surname>Barnes</surname><given-names>G</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Penny</surname><given-names>W</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>EEG and MEG data analysis in SPM8</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1155/2011/852961</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loftus</surname><given-names>GR</given-names></name><name><surname>Masson</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Using confidence intervals in within-subject designs</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>1</volume><fpage>476</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.3758/BF03210951</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>BA</given-names></name><name><surname>Boothroyd</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Cortical, auditory, evoked potentials in response to changes of spectrum and amplitude</article-title><source> Journal of the Acoustical Society of America</source><volume>107</volume><fpage>2155</fpage><lpage>2161</lpage><pub-id pub-id-type="doi">10.1121/1.428556</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>May</surname><given-names>P</given-names></name><name><surname>Tiitinen</surname><given-names>H</given-names></name><name><surname>Ilmoniemi</surname><given-names>RJ</given-names></name><name><surname>Nyman</surname><given-names>G</given-names></name><name><surname>Taylor</surname><given-names>JG</given-names></name><name><surname>Näätänen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Frequency change detection in human auditory cortex</article-title><source>Journal of Computational Neuroscience</source><volume>6</volume><fpage>99</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1023/A:1008896417606</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Schemitsch</surname><given-names>M</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Summary statistics in auditory perception</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>493</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1038/nn.3347</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>J</given-names></name><name><surname>Patterson</surname><given-names>T</given-names></name><name><surname>Ulrich</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Jackknife-based method for measuring LRP onset latency differences</article-title><source>Psychophysiology</source><volume>35</volume><fpage>99</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1111/1469-8986.3510099</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Psychophysics of normal and impaired hearing</article-title><source>British Medical Bulletin</source><volume>43</volume><fpage>887</fpage><lpage>908</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>BC</given-names></name><name><surname>Glasberg</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Suggested formulae for calculating auditory-filter bandwidths and excitation patterns</article-title><source>Journal of the Acoustical Society of America</source><volume>74</volume><fpage>750</fpage><lpage>753</lpage><pub-id pub-id-type="doi">10.1121/1.389861</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>SO</given-names></name><name><surname>Kersten</surname><given-names>D</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Schrater</surname><given-names>P</given-names></name><name><surname>Woods</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Shape perception reduces activity in human primary visual cortex</article-title><source>PNAS</source><volume>99</volume><fpage>15164</fpage><lpage>15169</lpage><pub-id pub-id-type="doi">10.1073/pnas.192579399</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Näätänen</surname><given-names>R</given-names></name><name><surname>Paavilainen</surname><given-names>P</given-names></name><name><surname>Rinne</surname><given-names>T</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The mismatch negativity (MMN) in basic research of central auditory processing: a review</article-title><source>Clinical Neurophysiology</source><volume>118</volume><fpage>2544</fpage><lpage>2590</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2007.04.026</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Näätänen</surname><given-names>R</given-names></name><name><surname>Tervaniemi</surname><given-names>M</given-names></name><name><surname>Sussman</surname><given-names>E</given-names></name><name><surname>Paavilainen</surname><given-names>P</given-names></name><name><surname>Winkler</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>&quot;Primitive intelligence&quot; in the auditory cortex</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>283</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01790-2</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okazawa</surname><given-names>G</given-names></name><name><surname>Tajima</surname><given-names>S</given-names></name><name><surname>Komatsu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Image statistics underlying natural texture selectivity of neurons in macaque V4</article-title><source>PNAS</source><volume>112</volume><fpage>E351</fpage><lpage>E360</lpage><pub-id pub-id-type="doi">10.1073/pnas.1415146112</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The role of context in object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>520</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.09.009</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Mattout</surname><given-names>J</given-names></name><name><surname>Rugg</surname><given-names>MD</given-names></name><name><surname>Maquet</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>An empirical Bayesian solution to the source reconstruction problem in EEG</article-title><source>NeuroImage</source><volume>24</volume><fpage>997</fpage><lpage>1011</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.10.030</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plack</surname><given-names>CJ</given-names></name><name><surname>Moore</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Temporal window shape as a function of frequency and level</article-title><source> Journal of the Acoustical Society of America</source><volume>87</volume><fpage>2178</fpage><lpage>2187</lpage><pub-id pub-id-type="doi">10.1121/1.399185</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portilla</surname><given-names>J</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A parametric texture model based on joint statistics of complex wavelet coefficients</article-title><source>International Journal of Computer Vision</source><volume>40</volume><fpage>49</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1023/A:1026553619983</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>SM</given-names></name><name><surname>Mayer</surname><given-names>AR</given-names></name><name><surname>Harrington</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The evolution of brain activation during temporal processing</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>317</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1038/85191</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recasens</surname><given-names>M</given-names></name><name><surname>Grimm</surname><given-names>S</given-names></name><name><surname>Capilla</surname><given-names>A</given-names></name><name><surname>Nowak</surname><given-names>R</given-names></name><name><surname>Escera</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Two sequential processes of change detection in hierarchically ordered areas of the human auditory cortex</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>143</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs295</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schröger</surname><given-names>E</given-names></name><name><surname>Marzecová</surname><given-names>A</given-names></name><name><surname>SanMiguel</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention and prediction in human audition: a lesson from cognitive psychophysiology</article-title><source>European Journal of Neuroscience</source><volume>41</volume><fpage>641</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1111/ejn.12816</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartze</surname><given-names>M</given-names></name><name><surname>Farrugia</surname><given-names>N</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dissociation of formal and temporal predictability in early auditory evoked potentials</article-title><source>Neuropsychologia</source><volume>51</volume><fpage>320</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.09.037</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartze</surname><given-names>M</given-names></name><name><surname>Rothermich</surname><given-names>K</given-names></name><name><surname>Schmidt-Kassow</surname><given-names>M</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Temporal regularity effects on pre-attentive and attentive processing of deviance</article-title><source>Biological Psychology</source><volume>87</volume><fpage>146</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2011.02.021</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural dynamics of change detection in crowded acoustic scenes</article-title><source>NeuroImage</source><volume>126</volume><fpage>164</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.050</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spratling</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Reconciling predictive coding and biased competition models of cortical function</article-title><source>Frontiers in Computational Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.10.004.2008</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Expectation in perceptual decision making: neural and computational mechanisms</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/nrn3863</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Expectation (and attention) in visual cognition</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>403</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.06.003</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Feature-based attention and feature-based expectation</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>401</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.03.008</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>Q</given-names></name><name><surname>Sidwell</surname><given-names>A</given-names></name><name><surname>Nelson</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Auditory enhancement of changes in spectral amplitude</article-title><source> Journal of the Acoustical Society of America</source><volume>81</volume><fpage>700</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1121/1.394838</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavano</surname><given-names>A</given-names></name><name><surname>Widmann</surname><given-names>A</given-names></name><name><surname>Bendixen</surname><given-names>A</given-names></name><name><surname>Trujillo-Barreto</surname><given-names>N</given-names></name><name><surname>Schröger</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal regularity facilitates higher-order sensory predictions in fast auditory sequences</article-title><source>European Journal of Neuroscience</source><volume>39</volume><fpage>308</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1111/ejn.12404</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teki</surname><given-names>S</given-names></name><name><surname>Barascud</surname><given-names>N</given-names></name><name><surname>Picard</surname><given-names>S</given-names></name><name><surname>Payne</surname><given-names>C</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural Correlates of Auditory Figure-Ground Segregation Based on Temporal Coherence</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3669</fpage><lpage>3680</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw173</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teki</surname><given-names>S</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>von Kriegstein</surname><given-names>K</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Brain bases for auditory stimulus-driven figure-ground segregation</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>164</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3788-10.2011</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Elie</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural processing of natural sounds</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>355</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn3731</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulrich</surname><given-names>R</given-names></name><name><surname>Miller</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Using the jackknife-based scoring method for measuring LRP onset effects in factorial designs</article-title><source>Psychophysiology</source><volume>38</volume><fpage>816</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.1111/1469-8986.3850816</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wager</surname><given-names>TD</given-names></name><name><surname>Keller</surname><given-names>MC</given-names></name><name><surname>Lacey</surname><given-names>SC</given-names></name><name><surname>Jonides</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Increased sensitivity in neuroimaging analyses using robust regression</article-title><source>NeuroImage</source><volume>26</volume><fpage>99</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.01.011</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname><given-names>I</given-names></name><name><surname>Denham</surname><given-names>SL</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Modeling the auditory scene: predictive regularity representations and perceptual objects</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>532</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.09.003</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>J</given-names></name><name><surname>Al-Aidroos</surname><given-names>N</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention is spontaneously biased toward regularities</article-title><source>Psychological Science</source><volume>24</volume><fpage>667</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.1177/0956797612460407</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.19113.009</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Reviewing editor</role><aff id="aff3"><institution>University of Oxford</institution>, <country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Detecting and representing predictable structure during auditory scene analysis&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers – Inyong Choi (Reviewer #1) and Alexander Gutschalk (Reviewer #2) – and the evaluation has been overseen by Andrew King as the Senior and Reviewing Editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>Using a paradigm designed to simulate the complexity of natural listening environments, this study shows that listeners are more accurate and faster in their ability to detect a new sound source among multiple concurrent sources when the acoustic scene changes in a temporally regular and therefore predictable way than if the changes occur randomly. This behavioral advantage was accompanied by larger and more sustained neural responses, as assessed using magnetoencephalography recordings of brain activity. The change-evoked response was initially weaker in actively attending than passively listening subjects.</p><p>Although the reviewers agree that this is high quality and interesting work, they have raised several concerns that will need to be addressed before a final decision can be made.</p><p>Essential revisions:</p><p>1) It is stated in the third paragraph of the section “MEG statistical analysis” that &quot;source solutions were constrained to be consistent across participants.&quot; Please provide more detail about how this was done.</p><p>2) The paradigm is sometimes discussed as &quot;detection of a new stream&quot; and sometimes as &quot;change detection&quot;. The reviewers recommend that you stick with one description and suggest that this might be referred to as the detection of a new stream in an auditory background.</p><p>3) The results suggest that the change/stream onset response is weaker under attention, with the exception of later deflections. The discussion for potential reasons (reduction of surprise under attention) is plausible and interesting. However, could it not be that the amplitude reduction of the positive response is simply caused by the appearance of the negative response and the cancellation between the two (cf. <xref ref-type="fig" rid="fig3">Figure 3</xref> in Sohoglu and Chait, 2016)? This could be explored e.g. by calculating difference waves between attended and not-attended conditions.</p><p>4) Parietal activity is discussed here in the context of regularity processing. However, in Barascud, Chait et al. 2016 – which is also related to regularity detection – frontal, but no parietal activity was observed. You should probably be more cautious in the interpretation of these different patterns of activity in a distributed source model.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.19113.010</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>1) It is stated in the third paragraph of the section “MEG statistical analysis” that &quot;source solutions were constrained to be consistent across participants.&quot; Please provide more detail about how this was done.</italic> </p><p>We now provide additional information on this procedure. Group-optimization of subject-specific source inversions is a widely used feature of SPM’s Bayesian source reconstruction tools (for full details of the method, see Litvak and Friston 2008 NeuroImage; Henson et al. 2011 Front Hum Neurosci. For example applications, see Furl et al. 2011 NeuroImage; Spitzer et al. 2011 PNAS; Sohoglu et al. 2012 J Neurosci).</p><p>We now provide more details of this procedure in the revised manuscript (fourth paragraph of the section “MEG statistical analysis”):</p><p>“Source solutions were constrained to be consistent across subjects (pooled over Passive and Active groups), which has been shown to improve group-level statistical power (Litvak and Friston, 2008; Henson et al., 2011). In brief, this procedure involves 1) realigning and concatenating sensor-level data across subjects 2) estimating a single source solution for all subjects 3) using the resulting group solution as a Bayesian prior on individual subject inversions. Thus, this method exploits the availability of repeated measurements (from different subjects) to constrain source reconstruction. Importantly, however, this procedure does not bias activation differences between conditions in a given source.”</p><p> <italic>2) The paradigm is sometimes discussed as &quot;detection of a new stream&quot; and sometimes as &quot;change detection&quot;. The reviewers recommend that you stick with one description and suggest that this might be referred to as the detection of a new stream in an auditory background.</italic> </p><p>We agree that using a single description would enhance clarity. We have followed the reviewers’ suggestion and now use ‘detection of appearing source’ throughout the revised manuscript.</p><p><italic>3) The results suggest that the change/stream onset response is weaker under attention, with the exception of later deflections. The discussion for potential reasons (reduction of surprise under attention) is plausible and interesting. However, could it not be that the amplitude reduction of the positive response is simply caused by the appearance of the negative response and the cancellation between the two (cf. <xref ref-type="fig" rid="fig3">Figure 3</xref> in Sohoglu and Chait, 2016)? This could be explored e.g. by calculating difference waves between attended and not-attended conditions.</italic></p><p>We thank the reviewers for this excellent point. In the revised manuscript, we include additional analysis (Results section, subsection “Appearance-evoked response”) and a new supplementary figure (Figure 3—figure supplement 1) to address this issue:</p><p>“An alternative explanation of the interaction between group and scene structure is possible if the M50 and M100 peaks reflect independent but temporally and spatially overlapping components. […] This is despite the M100 showing no evidence of modulation by scene structure in Active listeners (even at an uncorrected threshold of p&lt;0.05), making it unlikely the pattern of results reflect a suppression of the M50 by the M100.”</p><p> <italic>4) Parietal activity is discussed here in the context of regularity processing. However, in Barascud, Chait et al. 2016 – which is also related to regularity detection – frontal, but no parietal activity was observed. You should probably be more cautious in the interpretation of these different patterns of activity in a distributed source model.</italic></p><p>We agree that the parietal activity should be interpreted within the limits of MEG source localization accuracy and that to conclusively determine whether there are genuine differences in the neural substrates between studies, the two types of regularity (temporal-based in current study versus spectral-based in Barascud, Chait et al. 2016) would need to be statistically contrasted in a single experiment.</p><p>This is now acknowledged in the revised manuscript (Discussion section, subsection “Neural sources”):</p><p>“We note however that although Barascud et al. (2016) report effects of statistical structure in early auditory regions (like the current findings), they did not observe changes in MEG and fMRI responses in parietal cortex. Instead, spectral regularity modulated activity in the inferior frontal gyrus. This is may suggest a degree of neural specialization for the particular type of regularity encoded e.g. temporal-based involving parietal cortex versus spectral-based involving inferior frontal regions. Future work is required however to determine whether temporal and spectral regularities are encoded by distinct neural substrates (e.g. by contrasting neural effects of temporal and spectral regularities in the same experiment).”</p></body></sub-article></article>