<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">10094</article-id><article-id pub-id-type="doi">10.7554/eLife.10094</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-38480"><name><surname>Dordek</surname><given-names>Yedidyah</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-47563"><name><surname>Soudry</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-38481"><name><surname>Meir</surname><given-names>Ron</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-34799"><name><surname>Derdikman</surname><given-names>Dori</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3677-6321</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="other" rid="par-6"/><xref ref-type="other" rid="par-7"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Faculty of Electrical Engineering</institution>, <institution>Technion – Israel Institute of Technology</institution>, <addr-line><named-content content-type="city">Haifa</named-content></addr-line>, <country>Israel</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Rappaport Faculty of Medicine and Research Institute</institution>, <institution>Technion – Israel Institute of Technology</institution>, <addr-line><named-content content-type="city">Haifa</named-content></addr-line>, <country>Israel</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Statistics</institution>, <institution>Columbia University</institution>, <addr-line><named-content content-type="city">New York</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Center for Theoretical Neuroscience</institution>, <institution>Columbia University</institution>, <addr-line><named-content content-type="city">New York</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing editor</role><aff id="aff5"><institution>Brown University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>daniel.soudry@gmail.com</email> (DS);</corresp><corresp id="cor2"><email>derdik@technion.ac.il</email> (DD)</corresp><fn fn-type="con" id="equal-contrib"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="pub" publication-format="electronic"><day>08</day><month>03</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e10094</elocation-id><history><date date-type="received"><day>15</day><month>07</month><year>2015</year></date><date date-type="accepted"><day>08</day><month>03</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Dordek et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Dordek et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-10094-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.10094.001</object-id><p>Many recent models study the downstream projection from grid cells to place cells, while recent data have pointed out the importance of the feedback projection. We thus asked how grid cells are affected by the nature of the input from the place cells. We propose a single-layer neural network with feedforward weights connecting place-like input cells to grid cell outputs. Place-to-grid weights are learned via a generalized Hebbian rule. The architecture of this network highly resembles neural networks used to perform Principal Component Analysis (PCA). Both numerical results and analytic considerations indicate that if the components of the feedforward neural network are non-negative, the output converges to a hexagonal lattice. Without the non-negativity constraint, the output converges to a square lattice. Consistent with experiments, grid spacing ratio between the first two consecutive modules is −1.4. Our results express a possible linkage between place cell to grid cell interactions and PCA.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.001">http://dx.doi.org/10.7554/eLife.10094.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.10094.002</object-id><title>eLife digest</title><p>Long before the invention of GPS systems, ships used a technique called dead reckoning to navigate at sea. By tracking the ship’s speed and direction of movement away from a starting point, the crew could estimate their position at any given time. Many believe that some animals, including rats and humans, can use a similar process to navigate in the absence of external landmarks. This process is referred to as “path integration”.</p><p>It is commonly believed that the brain’s navigation system is based on such path integration in two key regions: the entorhinal cortex and the hippocampus. Most models of navigation assume that a network of grid cells in the entorhinal cortex processes information about an animal’s speed and direction of movement. The grid cell network estimates the animal’s future position and relays this information to cells in the hippocampus called place cells. Individual place cells then fire whenever the animal reaches a specific location.</p><p>However, recent work has shown that information also flows from place cells back to grid cells. Further experiments have suggested that place cells develop before grid cells. Also, inactivating place cells eliminates the hexagonal patterns that normally appear in the activity of the grid cells.</p><p>Using a computational model, Dordek, Soudry et al. now show that place cell activity could in principle trigger the formation of the grid cell network, rather than vice versa. This is achieved using a process that resembles a common statistical algorithm called principal component analysis (PCA). However, this only works if place cells only excite grid cells and never inhibit their activity, similar to what is known from the anatomy of these brain regions. Under these circumstances, the model shows hexagonal patterns emerging in the activity of the grid cells, with similar properties to those patterns observed experimentally.</p><p>These results suggest that navigation may not depend solely on grid cells processing information about speed and direction of movement, as assumed by path integration models. Instead grid cells may rely on position-based input from place cells. The next step is to create a single model that combines the flow of information from place cells to grid cells and vice versa.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.002">http://dx.doi.org/10.7554/eLife.10094.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>bat</kwd><kwd>grid cell</kwd><kwd>place cell</kwd><kwd>hippocampus</kwd><kwd>entorhinal</kwd><kwd>navigation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd><kwd>Mouse</kwd><kwd>Rat</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Ollendroff center of the Department of Electrical Engineering, Technion</institution></institution-wrap></funding-source><award-id>Research fund</award-id><principal-award-recipient><name><surname>Dordek</surname><given-names>Yedidyah</given-names></name><name><surname>Meir</surname><given-names>Ron</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>Gruss Lipper Charitable Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Soudry</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>Intelligence Advanced Research Projects Activity</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Soudry</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>Personal Research Grant, 955/13</award-id><principal-award-recipient><name><surname>Derdikman</surname><given-names>Dori</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>New Faculty Equipment Grant, 1882/13</award-id><principal-award-recipient><name><surname>Derdikman</surname><given-names>Dori</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution>Rappaport Institute</institution></institution-wrap></funding-source><award-id>Personal Research Grant</award-id><principal-award-recipient><name><surname>Derdikman</surname><given-names>Dori</given-names></name></principal-award-recipient></award-group><award-group id="par-7"><funding-source><institution-wrap><institution>Allen and Jewel Prince Center for Neurodegenrative Disorders</institution></institution-wrap></funding-source><award-id>Research Grant</award-id><principal-award-recipient><name><surname>Derdikman</surname><given-names>Dori</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Computational modeling of the brain’s navigation system reveals that place cells can drive the formation of hexagonal patterns experimentally observed in grid cells activity.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The system of spatial navigation in the brain has recently received much attention (<xref ref-type="bibr" rid="bib5">Burgess, 2014</xref>; <xref ref-type="bibr" rid="bib27">Morris, 2015</xref>; <xref ref-type="bibr" rid="bib11">Eichenbaum, 2015</xref>). This system involves many regions, which seem to divide into two major classes: regions such as CA1 and CA3 of the hippocampus, which contain place cells (<xref ref-type="bibr" rid="bib28">O'Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="bib29">O'Keefe and Nadel, 1978</xref>), vs. regions, such as the medial-entorhinal cortex (MEC), the presubiculum and the parasubiculum, which contain grid cells, head-direction cells and border cells (<xref ref-type="bibr" rid="bib17">Hafting et al., 2005</xref>; <xref ref-type="bibr" rid="bib3">Boccara et al., 2010</xref>; <xref ref-type="bibr" rid="bib32">Sargolini et al., 2006</xref>; <xref ref-type="bibr" rid="bib35">Solstad et al., 2008</xref>; <xref ref-type="bibr" rid="bib33">Savelli et al., 2008</xref>). While the phenomenology of those cells is described in many studies (<xref ref-type="bibr" rid="bib10">Derdikman and Knierim, 2014</xref>; <xref ref-type="bibr" rid="bib40">Tocker et al., 2015</xref>), the manner in which grid cells are formed is quite enigmatic. Many mechanisms have been proposed. The details of these mechanisms differ, however, they mostly share in common the assumption that the animal’s velocity is the main input to the system (<xref ref-type="bibr" rid="bib10">Derdikman and Knierim, 2014</xref>; <xref ref-type="bibr" rid="bib47">Zilli, 2012</xref>; <xref ref-type="bibr" rid="bib15">Giocomo et al., 2011</xref>), such that positional information is generated by the integration of this input in time. This process is termed 'path integration' (PI) (<xref ref-type="bibr" rid="bib25">Mittelstaedt and Mittelstaedt, 1980</xref>). A notable exception to this class of models was suggested in a previous paper by <xref ref-type="bibr" rid="bib21">Kropff and Treves (2008)</xref>; and in a sequel to that paper (<xref ref-type="bibr" rid="bib34">Si and Treves, 2013</xref>), in which they demonstrated the emergence of grid cells from place cell inputs without using the rat's velocity as an input signal.</p><p>We note here that generating grid cells from place cells may seem at odds with the architecture of the network, since it is known that place cells reside at least one synapse downstream of grid cells (<xref ref-type="bibr" rid="bib45">Witter and Amaral, 2004</xref>). Nonetheless, there is current evidence that the feedback from place cells to grid cells is of great functional importance. Specifically, there is evidence that inactivation of place cells causes grid cells to disappear (<xref ref-type="bibr" rid="bib4">Bonnevie et al., 2013</xref>), and furthermore, it seems that, in development, place cells emerge before grid cells do (<xref ref-type="bibr" rid="bib23">Langston et al., 2010</xref>; <xref ref-type="bibr" rid="bib44">Wills et al., 2010</xref>). Thus, there is good motivation for trying to understand how the feedback from hippocampal place cells may contribute to grid cell formation.</p><p>In the present paper, we thus investigated a model of grid cell development from place cell inputs. We showed the resemblance between a feedforward network from place cells to grid cells to a neural network architecture previously used to implement the PCA algorithm (<xref ref-type="bibr" rid="bib30">Oja, 1982</xref>). We demonstrated, both analytically and through simulations, that the formation of grid cells from place cells using such a neural network could occur given specific assumptions on the input (i.e. zero mean) and on the nature of the feedforward connections (specifically, non-negative, or excitatory).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Comparing neural-network results to PCA</title><p>We initially considered the output of a single-layer neural network and of the PCA algorithm in response to the same inputs. These consisted of the temporal activity of a simulated agent moving around in a two-dimensional (2D) space (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; see Materials and methods for details). In order to mimic place cell activity, the simulated virtual space was covered by multiple 2D Gaussian functions uniformly distributed at random (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), which constituted the input. In order to calculate the principal components, we used a [Neuron x Time] matrix (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) after subtracting the temporal mean, generated from the trajectory of the agent as it moved through the place fields. Thus, we displayed a one-dimensional mapping of the two-dimensional activity, transforming the 2D activity into a 1D vector per input neuron. This resulted in the [Neuron X Neuron] covariance matrix (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), on which PCA was performed by evaluating the appropriate eigenvalues and eigenvectors.<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.003</object-id><label>Figure 1.</label><caption><title>Construction of the correlation matrix from behavior.</title><p>(<bold>A</bold>) Diagram of the environment. Black dots indicate places the virtual agent has visited. (<bold>B</bold>) Centers of place cells uniformly distributed in the environment. (<bold>C</bold>) The [Neuron X Time] matrix of the input-place cells. (<bold>D</bold>) Correlation matrix of (<bold>C</bold>) used for the PCA process.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.003">http://dx.doi.org/10.7554/eLife.10094.003</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig1-v2.tif"/></fig></p><p>To learn the grid cells, based on the place cell inputs, we implemented a single-layer neural network with a single output (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Input to output weights were governed by a Hebbian-like learning rule. As described in the Introduction (see also analytical treatment in the Methods section), this type of architecture induces the output’s weights to converge to the leading principal component of the input data.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.004</object-id><label>Figure 2.</label><caption><title>Neural network architecture with feedforward connectivity.</title><p>The input layer corresponds to place cells and the output to a single cell.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.004">http://dx.doi.org/10.7554/eLife.10094.004</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig2-v2.tif"/></fig></p><p>The agent explored the environment for a sufficiently long time allowing the weights to converge to the first principal component of the temporal input data. In order to establish a spatial interpretation of the eigenvectors (from PCA) or the weights (from the converged network) we projected both the PCA eigenvectors and the network weights onto the place cells space, producing corresponding spatial activity maps. The leading eigenvectors of the PCA and the network’s weights converged to square-like periodic spatial solutions (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>).<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.005</object-id><label>Figure 3.</label><caption><title>Results of PCA and of the networks' output (in different simulations).</title><p>(<bold>A</bold>) 1st 16 PCA eigenvectors projected on the place cells' input space. (<bold>B</bold>) Converged weights of the network (each result from different simulation, initial conditions and trajectory) projected onto place cells' space. Note that the 8 outputs shown here resemble linear combinations of components #1 to #4 in panel A.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.005">http://dx.doi.org/10.7554/eLife.10094.005</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig3-v2.tif"/></fig></p><p>Being a PCA algorithm, the spatial projections of the weights were periodic in space due to the covariance matrix of the input having a Toeplitz structure (<xref ref-type="bibr" rid="bib8">Dai et al., 2009</xref>) (a Toeplitz matrix has constant elements along each diagonal). Intuitively, the Toeplitz structure arises due to the spatial stationarity of the input. In fact, since we used periodic boundary conditions for the agent’s motion, the covariance matrix was a circulant matrix, and the eigenvectors were sinusoidal functions, with length constants determined by the scale of the box (<xref ref-type="bibr" rid="bib16">Gray, 2006</xref>) [a circulant matrix is defined by a single row (or column), and the remaining rows (or columns) are obtained by cyclic permutations. It is a special case of a Toeplitz matrix - see for example <xref ref-type="fig" rid="fig1">Figure 1D</xref>]. The covariance matrix was heavily degenerate, with approximately 90% of the variance accounted for by the first 15% of the eigenvectors (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The solution demonstrated a fourfold redundancy. This was apparent in the plotted eigenvalues (from the largest to the smallest eigenvalue, <xref ref-type="fig" rid="fig4">Figure 4A and C</xref>), which demonstrated a fourfold grouping-pattern. The fourfold redundancy can be explained analytically by the symmetries of the system – see analytical treatment of PCA in Methods section (specifically Figure 15C).<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.006</object-id><label>Figure 4.</label><caption><title>Eigenvalues and eigenvectors of the input's correlation matrix.</title><p>(<bold>A</bold>) Eigenvalue size (normalized by the largest, from large to small (<bold>B</bold>) Cumulative explained variance by the eigenvalues, with 90% of variance accounted for by the first 35 eigenvectors (out of 625). (<bold>C</bold>) Amplitude of leading 32 eigenvalues, demonstrating that they cluster in groups of 4 or 8. Specifically, the first four clustered groups correspond respectively (from high to low) to groups A,B,C &amp; D In <xref ref-type="fig" rid="fig15">Figure 15C</xref>, which have the same redundancy (4,8,4 &amp; 4).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.006">http://dx.doi.org/10.7554/eLife.10094.006</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig4-v2.tif"/></fig></p><p>In summary, both the direct PCA algorithm and the neural network solutions developed periodic structure. However, this periodic structure was not hexagonal but rather had a square-like form.</p></sec><sec id="s2-2"><title>Adding a non-negativity constraint to the PCA</title><p>It is known that most synapses from the hippocampus to the MEC are excitatory (<xref ref-type="bibr" rid="bib45">Witter and Amaral, 2004</xref>). We thus investigated how a non-negativity constraint, applied to the projections from place cells to grid cells, affected our simulations. As demonstrated in the analytical treatment in the Methods section, we could expect to find hexagons when imposing the non-negativity constraint. Indeed, when adding this constraint, the outputs behaved in a different manner and converged to a <bold>hexagonal grid</bold>, similar to real grid cells. While it was straightforward to constrain the neural network, calculating non-negative PCA directly was a more complicated task due to the non-convex nature of the problem (<xref ref-type="bibr" rid="bib26">Montanari and Richard, 2014</xref>; <xref ref-type="bibr" rid="bib22">Kushner and Clark, 1978</xref>).</p><p>In the network domain, we used a simple rectification rule for the learned feedforward weights, which constrained their values to be non-negative. For the direct non-negative PCA calculation, we used the raw place cells activity (after spatial or temporal mean normalization), as inputs to three different iterative numerical methods: NSPCA (Nonnegative Sparse PCA), AMP (Approximate Message Passing) and FISTA (Fast Iterative Threshold and Shrinkage) based algorithms (see Materials and methods section).</p><p>In both cases, we found that hexagonal grid cells emerged in the output layer (plotted as spatial projection of weights and eigenvectors: <xref ref-type="fig" rid="fig5">Figure 5A–B</xref>, <xref ref-type="fig" rid="fig6">Figure 6A–B</xref>, <xref ref-type="other" rid="media1">Video 1</xref>, <xref ref-type="other" rid="media2">Video 2</xref>). When we repeated the process over many simulations (i.e. new trajectories and random initializations of weights) we found that the population as a whole consistently converged to hexagonal grid-like responses, while similar simulations with the unconstrained version did not (compare <xref ref-type="fig" rid="fig3">Figure 3</xref> to <xref ref-type="fig" rid="fig5">Figure 5</xref>–<xref ref-type="fig" rid="fig6">Figure 6</xref>).<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.007</object-id><label>Figure 5.</label><caption><title>Output of the neural network when weights are constrained to be non-negative.</title><p>(<bold>A</bold>) Converged weights (from different simulations) of the network projected onto place cells space. See an example of a simulation in <xref ref-type="other" rid="media1">Video 1</xref>. (<bold>B</bold>) Spatial autocorrelations of (<bold>A</bold>). See an example of the evolution of autorcorrelation in simulation in <xref ref-type="other" rid="media2">Video 2</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.007">http://dx.doi.org/10.7554/eLife.10094.007</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig5-v2.tif"/></fig><media content-type="glencoe play-in-place height-250 width-310" id="media1" mime-subtype="mp4" mimetype="video" xlink:href="elife-10094-media1.mp4"><object-id pub-id-type="doi">10.7554/eLife.10094.008</object-id><label>Video 1.</label><caption><title>Evolution in time of the network's weights.</title><p>625 Place-cells used as input. Video frame shown every 3000 time steps up to t=1,000,000. Video converges to results similar to those of <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.008">http://dx.doi.org/10.7554/eLife.10094.008</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media2" mime-subtype="mp4" mimetype="video" xlink:href="elife-10094-media2.mp4"><object-id pub-id-type="doi">10.7554/eLife.10094.009</object-id><label>Video 2.</label><caption><title>Evolution of autocorrelation pattern of network's weights shown in <xref ref-type="other" rid="media1">Video 1</xref>.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.009">http://dx.doi.org/10.7554/eLife.10094.009</ext-link></p></caption></media><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.010</object-id><label>Figure 6.</label><caption><title>Results from the non-negative PCA algorithm.</title><p>(<bold>A</bold>) Spatial projection of the leading eigenvector on input space. (<bold>B</bold>) Corresponding spatial autocorrelations. The different solutions are outcomes of multiple simulations with identical settings in a new environment and new random initial conditions.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.010">http://dx.doi.org/10.7554/eLife.10094.010</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig6-v2.tif"/></fig></p><p>In order to further assess the hexagonal grid emerging in the output, we calculated the mean (hexagonal) Gridness scores ([<xref ref-type="bibr" rid="bib32">Sargolini et al., 2006</xref>], which measure the degree to which the solution resembles a hexagonal grid [see Materials and methods]). We ran about 1500 simulations of the network (in each simulation, the network consisted of 625 place cell-like inputs and a single grid cell-like output), and found noticeable differences between the constrained and unconstrained cases. Namely, the Gridness score in the non-negatively constrained-weight simulations was significantly higher than in the unconstrained-weight case (Gridness = 1.07 ± 0.003 in the constrained case vs. 0.302 ± 0.003 in the unconstrained case. see <xref ref-type="fig" rid="fig7">Figure 7</xref>). A similar difference was observed with the direct non-negative PCA methods (1500 simulations, each with different trajectories, Gridness = 1.13 ± 0.0022 in the constrained case vs. 0.27 ± 0.0023 in the unconstrained case).<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.011</object-id><label>Figure 7.</label><caption><title>Histograms of Gridness values from network and PCA.</title><p>First row (<bold>A</bold>) + (<bold>C</bold>) corresponds to network results, and second row (<bold>B</bold>) + (<bold>D</bold>) to PCA. The left column histograms contain the 60° Gridness scores and the right one the 90° Gridness scores.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.011">http://dx.doi.org/10.7554/eLife.10094.011</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig7-v2.tif"/></fig></p><p>Another score we tested was a 'Square Gridness' score (see Materials and methods) where we measured the 'Squareness' of the solutions (as opposed to 'Hexagonality'). We found that the unconstrained network had a higher square-Gridness score while the constrained network had a lower square-Gridness score (<xref ref-type="fig" rid="fig7">Figure 7</xref>); for both the direct-PCA calculation (square-Gridness = 0.89 ± 0.0074 in the unconstrained case vs. 0.1 ± 0.006 in the constrained case) and the neural-network (square-Gridness = 0.073 ± 0.006 in the constrained case vs. 0.73 ± 0.008 in the unconstrained case).</p><p>All in all, these results suggest that when direct PCA eigenvectors and neural network weights were unconstrained they converged to periodic square solutions. However, when constrained to be non-negative, the direct PCA, and the corresponding neural network weights, both converged to a hexagonal solution.</p></sec><sec id="s2-3"><title>Dependence of the result on the structure of the input</title><p>We investigated the effect of different inputs on the emergence of the grid structure in the networks' output. We found that some manipulation of the input was necessary in orderto enable the implementation of PCA in the neural network. Specifically, PCA requires a zero-mean input, while simple Gaussian-like place cells do not possess this property. In order to obtain input with zero-mean, we either performed differentiation of the place cells’ activity in time, or used a Mexican-hat like (Laplacian) shape (See Materials and methods for more details on the different types of inputs). Another option we explored was the usage of positive-negative disks with a total sum of zero activity in space (<xref ref-type="fig" rid="fig8">Figure 8</xref>). The motivation for the use of Mexican-hat like transformations is their abundance in the nervous system (<xref ref-type="bibr" rid="bib43">Wiesel and Hubel, 1963</xref>; <xref ref-type="bibr" rid="bib12">Enroth-Cugell and Robson, 1966</xref>; <xref ref-type="bibr" rid="bib9">Derdikman et al., 2003</xref>).<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.012</object-id><label>Figure 8.</label><caption><title>Different types of spatial input used in our network.</title><p>(<bold>A</bold>) 2D Gaussian function, acting as a simple place cell. (<bold>B</bold>) Laplacian function or Mexican hat. (<bold>C</bold>) A positive (inner circle) - negative (outer ring) disk. While inputs as in panel A do not converge to hexagonal grids, inputs as in panels B or C do converge.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.012">http://dx.doi.org/10.7554/eLife.10094.012</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig8-v2.tif"/></fig></p><p>We found that usage of simple 2-D Gaussian-functions as inputs did not generate hexagonal grid cells as outputs (<xref ref-type="fig" rid="fig9">Figure 9</xref>). On the other hand, time-differentiated inputs, positive-negative disks or Laplacian inputs did generate grid-like output cells, both when running the non-negative PCA directly (<xref ref-type="fig" rid="fig6">Figure 6</xref>), or by simulating the non-negatively constrained Neural Network (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Another approach we used for obtaining zero-mean was to subtract the mean dynamically from every output individually (see Materials and methods). The latter approach, related to adaptation of the firing rate, was adopted from Kropff &amp; Treves (<xref ref-type="bibr" rid="bib21">Kropff and Treves, 2008</xref>), who used it to control various aspects of the grid cell's activity. In addition to controlling the firing rate of the grid cells, if applied correctly, the adaptation could be exploited to keep the output's activity stable, with zero-mean rates. We applied this method in our system and in this case the outputs converged to hexagonal grid cells as well, similarly to the previous cases (e.g. derivative in time, or Mexican hats as inputs; data not shown).<fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.013</object-id><label>Figure 9.</label><caption><title>Spatial projection of outputs’ weights in the neural network when inputs did not have zero mean (such as in <xref ref-type="fig" rid="fig8">Figure 8A</xref>).</title><p>(<bold>A</bold>) Various weights plotted spatially as projection onto place cells space. (<bold>B</bold>) Autocorrelation of (<bold>A</bold>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.013">http://dx.doi.org/10.7554/eLife.10094.013</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig9-v2.tif"/></fig></p><p>In summary, two conditions were required for the neural network to converge to spatial solutions resembling hexagonal grid cells: (1) non-negativity of the feedforward weights and (2) an effective zero-mean of the inputs (in time or space).</p></sec><sec id="s2-4"><title>Stability analysis</title><sec id="s2-4-1"><title>Convergence to hexagons from various initial spatial conditions</title><p>In order to numerically test the stability of the hexagonal solution, we initialized the network in different ways, randomly, using linear stripes, squares, rhomboids (squares on hexagonal lattice) and noisy hexagons. In all cases, the network converged to a hexagonal pattern (<xref ref-type="fig" rid="fig10">Figure 10</xref>; for squares and stripes, other shapes not shown here).<fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.014</object-id><label>Figure 10.</label><caption><title>Evolution in time of the networks’ solutions (upper rows) and their autocorrelations (lower rows).</title><p>The network was initialized in shapes of (<bold>A</bold>) Squares and of (<bold>B</bold>) stripes (linear).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.014">http://dx.doi.org/10.7554/eLife.10094.014</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig10-v2.tif"/></fig></p><p>We also ran the converged weights in a new simulation with novel trajectories and tested the Gridness scores, and the inter-trial stability in comparison to previous simulations. We found that the hexagonal solutions of the network remained stable although the trajectories varied drastically (data not shown).</p></sec></sec><sec id="s2-5"><title>Asymptotic stability of the equilibria</title><p>Under certain conditions (e.g., decaying learning rates and independent and identically distributed (i.i.d.) inputs), it was previously proved (<xref ref-type="bibr" rid="bib18">Hornik and Kuan, 1992</xref>), using techniques from the theory of stochastic approximation, that the system described here can be asymptotically analyzed in terms of (deterministic) Ordinary Differential Equations (ODE), rather than in terms of the stochastic recurrence equations. Since the ODE defining the converged weights is non-linear, we solved the ODEs numerically (see Materials and methods), by randomly initializing the weight vector. The asymptotic equilibria were reached much faster, compared to the outcome of the recurrence equations. Similarly to the recurrence equations, constraining the weights to be non-negative induced them to converge into a hexagonal shape while a non-constrained system produced square-like outcomes (<xref ref-type="fig" rid="fig11">Figure 11</xref>).<fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.015</object-id><label>Figure 11.</label><caption><title>Numerical convergence of the ODE to hexagonal results when weights are constrained.</title><p>(<bold>A</bold>) + (<bold>B</bold>): 60° and 90° Gridness score histograms. Each score represents a different weight vector of the solution J. (<bold>C</bold>) + (<bold>D</bold>): Spatial results for constrained and unconstrained scenarios, respectively. (<bold>E</bold>) + (<bold>F</bold>) Spatial autocorrelations of (<bold>C</bold>) + (<bold>D</bold>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.015">http://dx.doi.org/10.7554/eLife.10094.015</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10094-fig11-v2.tif"/></fig></p><p>Simulation was run 60 times, with 400 outputs per run. 60° Gridness score mean was 1.1 ± 0.0006 when weights were constrained and 0.29 ± 0.0005 when weights were unconstrained. 90° Gridness score mean was 0.006 ± 0.002 when weights were constrained and 0.8 ± 0.0017 when weights were unconstrained.</p></sec><sec id="s2-6"><title>Effect of place cell parameters on grid structure</title><p>A more detailed view of the resulting grid spacing showed that it was heavily dependent on the field widths of the place cells inputs. When the environment size was fixed and the output calculated per input size, the grid-spacing (distance between neighboring peaks) increased for larger place cell field widths.</p><p>To enable a fast parameter sweep over many place cell field widths (and large environment sizes), we took the steady state limit, and the limit of a high density of place cell locations, and used the fast FISTA algorithm to solve the non-negative PCA problem (see Materials and methods section).</p><p>We performed multiple simulations, and found that there was a simple linear dependency between the place field size and the output grid scale. For the case of periodic boundary conditions, we found that grid scale was S = 7.5sigma+0.85, where sigma was the width of the place cell field (<xref ref-type="fig" rid="fig12">Figure 12A</xref>). For a different set of simulations with zero boundary conditions, we achieved a similar relation: S=7.54sigma+0.62 (figure not shown). Grid scale was more dependent on place field size and less on box size (<xref ref-type="fig" rid="fig12">Figure 12H</xref>). We note that for very large environments, the effects of boundary conditions diminishes. At this limit, this linear relation between place field size and grid scale can be explained from analytical considerations (see Materials and methods section). Intuitively, this follows from dimensional analysis: given an infinite environment, at steady state the length scale of the place cell field width is the only length scale in the model, so any other length scale must be proportional to this scale. More precisely, we can provide a lower bound for the linear fit (<xref ref-type="fig" rid="fig12">Figure 12A</xref>), which depends only on the tuning curve of the place cells (see Materials and methods section). This lower bound was derived for periodic boundary conditions, but works well even with zero boundary conditions (not shown).<fig id="fig12" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.016</object-id><label>Figure 12.</label><caption><title>Effect of changing the place-field size in fixed arena (FISTA algorithm used; periodic boundary conditions and Arena size 500);</title><p>(<bold>A</bold>) Grid scale as a function of place field size (sigma); Linear fit is: Scale = 7.4 Sigma+0.62; the lower bound, equal to <inline-formula><mml:math id="inf1"><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, were <inline-formula><mml:math id="inf2"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is defined in <xref ref-type="disp-formula" rid="equ34">Equation 32</xref> in the Materials and methods section; (<bold>B</bold>) Grid orientation as a function of gridness; (<bold>C</bold>) Grid orientation as a function of sigma – scatter plot (blue stars) and mean (green line); (<bold>D</bold>) Histogram of grid orientations; (<bold>E</bold>) Mean gridness as a function of sigma; and (<bold>F</bold>) Histogram of mean gridness. (<bold>G</bold>) Gridness as a function of sigma and (arena-size/sigma) (zero boundary conditions). (<bold>H</bold>) Grid scale for the same parameters as in G.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.016">http://dx.doi.org/10.7554/eLife.10094.016</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig12-v2.tif"/></fig></p><p>Furthermore, we found that the grid orientation varied substantially for different place cell field widths, in the possible range of 0–15 degrees (<xref ref-type="fig" rid="fig12">Figure 12C,D</xref>). For small environments, the orientation strongly depended on the boundary conditions. However, as described in the Methods section, analytical considerations suggest that as the environment grows, the distribution of grid orientations becomes uniform in the range of 0–15 degrees, with a mean at 7.5°. Intuitively, this can be explained by rotational symmetry – when the environment size is infinite, all directions in the model are equivalent, and so we should get all orientations with equal probability, if we start the model from a uniformly random initialization. In addition, grid orientation was not a clear function of the gridness of the obtained grid cells (<xref ref-type="fig" rid="fig12">Figure 12B</xref>). For large enough place cells, gridness was larger than 1 (<xref ref-type="fig" rid="fig12">Figure 12E–G</xref>).</p></sec><sec id="s2-7"><title>Modules of grid cells</title><p>It is known that in reality grid cells form in modules of multiple spacings (<xref ref-type="bibr" rid="bib1">Barry et al., 2007</xref>; <xref ref-type="bibr" rid="bib37">Stensola et al., 2012</xref>). We tried to address this question of modules in several ways. First, we used different widths for the Gaussian/Laplacian input functions: Initially, we placed a heterogeneous population of widths in a given environment (i.e., uniformly random widths) and ran the single-output network 100 times. The distribution of grid spacings was almost comparable to the results of the largest width if applied alone, and did not exhibit module like behavior. This result is not surprising when thinking about a small place cell overlapping in space with a large place cell. Whenever the agent passes next to the small one, it activates both weights via synaptic learning. This causes the large firing field to overshadow the smaller one. Additionally, when using populations of only two widths of place fields, the grid spacings were dictated by the size of the larger place field (data not shown).</p><p>The second option we considered was to use a multi-output neural network, capable of computing all 'eigenvectors' rather than only the principal 'eigenvector' (where by 'eigenvector' we mean here the vectors achieved under the positivity constraint, and not the exact eigenvectors themselves). We used a hierarchical network implementation introduced by <xref ref-type="bibr" rid="bib31">Sanger, 1989</xref> (see Materials and methods). Since the 1<sup>st</sup> output’s weights converged to the 1<sup>st</sup> 'eigenvector', the network (<xref ref-type="fig" rid="fig13">Figure 13A–B</xref>) provided to the subsequent outputs (2<sup>nd</sup>, 3<sup>rd</sup>, and so forth) a reduced-version of the data from which the projection of the 1<sup>st</sup> 'eigenvector' has been subtracted out. This process, reminiscent of Gram-Schmidt orthogonalization, was capable of computing all 'eigenvectors' (in the modified sense) of the input's covariance matrix. It is important to note though that, due to the non-negativity constraint, the vectors achieved in this way were not orthogonal, and thus it cannot be considered a real orthogonalization process, although, as explained in the Methods section, the process does aim for maximum difference between the vectors.<fig id="fig13" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.017</object-id><label>Figure 13.</label><caption><title>Hierarchial network capable of computing all 'principal components'.</title><p>(<bold>A</bold>) Each output is a linear sum of all inputs weighted by the corresponding learned weights. (<bold>B</bold>) Over time, the data the following outputs 'see' is the original data after subtration of the 1st 'eigenvector's' projection onto it. This is an iterative process causing all outputs' weights to converge to the 'prinipcal components' of the data.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.017">http://dx.doi.org/10.7554/eLife.10094.017</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig13-v2.tif"/></fig></p><p>When constrained to be non-negative, and using the same homogeneous 'place cells' as in the previous network, the networks' weights converged to hexagonal shapes. Here, however, we found that the smaller the 'eigenvalue' was (or the higher the principal component number) the denser the grid became. We were able to identify two main populations of grid-distance 'modules' among the hexagonal spatial solutions with high Gridness scores (&gt;0.7, <xref ref-type="fig" rid="fig14">Figure 14A–B</xref>). In addition, we found that the ratio between the distances of the modules was −1.4, close to the value of 1.42 found by Stensola et al. (<xref ref-type="bibr" rid="bib37">Stensola et al., 2012</xref>). Although we searched for additional such jumps, we could only identify this single jump, suggesting that our model can yield up to two 'modules' and not more. The same process was repeated using the direct PCA method, utilizing the covariance matrix of the data after simulation as input for the non-negative PCA algorithms, and considering their ability to calculate only the 1<sup>st</sup> 'eigenvector'. By iteratively projecting the 1<sup>st </sup>'eigenvector' on the simulation data and subtracting the outcome from the original data, we applied the non-negative PCA algorithm to the residual data obtaining the 2<sup>nd </sup>'eigenvector' of the original data. This 'eigenvector' now constituted the 1<sup>st</sup> eigenvector' of the new residual data (see Materials and methods). Applying this process to as many 'outputs' as needed, we obtained very similar results to the ones presented above using the neural network (data not shown).<fig id="fig14" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.018</object-id><label>Figure 14.</label><caption><title>Modules of grid cells.</title><p>(<bold>A</bold>) In a network with 50 outputs, the grid spacing per output is plotted with respect to the hierarchical place of the output. (<bold>B</bold>) The grid spacing of outputs with high Gridness score (&gt;0.7). The centroids have a ratio of close to <inline-formula><mml:math id="inf3"><mml:mrow><mml:mo>√</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>) + (<bold>D</bold>) Example of rate maps of outputs and their spatial autocorrelations for both of the modules.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.018">http://dx.doi.org/10.7554/eLife.10094.018</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig14-v2.tif"/></fig></p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In our work, we explored the nature and behavior of the feedback projections from place cells to grid cells. We shed light on the importance of this relation and showed, both analytically and in simulation, how a simple single-layer neural network could produce hexagonal grid cells when subjected to place cell-like temporal input from a randomly-roaming moving agent. We found that the network resembled a neural network performing PCA (<xref ref-type="bibr" rid="bib30">Oja, 1982</xref>), with the constraint that the weights were non-negative. Under these conditions, and also under the requirements that place cells have a zero mean in time or space, the first principal component in the 2D arena had a firing pattern resembling a hexagonal grid cell. Furthermore, we found that in the limit of very large arenas, grid orientation converged to a uniform distribution in range of 0–15°. When looking at additional components, grid scale tends to be discretely clustered, such that two modules emerge. This is partially consistent with current experimental findings (<xref ref-type="bibr" rid="bib37">Stensola et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">2015</xref>). Furthermore, the inhibitory connectivity between multiple grid cells is consistent with the known functional anatomy in this network (<xref ref-type="bibr" rid="bib7">Couey et al., 2013</xref>).</p><sec id="s3-1"><title>Place-to-Grid as a PCA network</title><p>As a consequence of the requirements for PCA to hold, we found that the place cell input needed to have a zero-mean, otherwise the output was not periodic. Due to the lack of the zero-mean property in 2D Gaussians, we used various approaches to impose zero-mean on the input data. The first, in the time domain, was to differentiate the input and use the derivatives (a random walk produces zero-mean derivatives) as inputs. Another approach was to dynamically subtract the mean in all iterations of the simulation. This approach was reminiscent of the adaptation procedure suggested in the Kropff &amp; Treves paper (<xref ref-type="bibr" rid="bib21">Kropff and Treves, 2008</xref>). A third approach, applied in the spatial domain was to use inputs with a zero-spatial mean such as Laplacians of Gaussians (Mexican hats in 2D, or differences-of-Gaussians) or negative – positive disks. Such Mexican-hat inputs are quite typical in the nervous system (<xref ref-type="bibr" rid="bib43">Wiesel and Hubel, 1963</xref>; <xref ref-type="bibr" rid="bib12">Enroth-Cugell and Robson, 1966</xref>; <xref ref-type="bibr" rid="bib9">Derdikman et al., 2003</xref>), although in the case of place cells it is not completely known how they are formed. They could be a result of interaction between place cells and the vast number of inhibitory interneurons in the local hippocampal network (<xref ref-type="bibr" rid="bib14">Freund and Buzsáki, 1996</xref>).</p><p>Another condition we found crucial, which was not part of the original PCA network, was a non-negativity constraint on the place-to-grid learned weights. While rather easy to implement in the network, adding this constraint to the non-convex PCA problem was harder to implement. Since the problem is NP-hard (<xref ref-type="bibr" rid="bib26">Montanari and Richard, 2014</xref>), we turned to numerical methods. We used three different algorithms (<xref ref-type="bibr" rid="bib26">Montanari and Richard, 2014</xref>; <xref ref-type="bibr" rid="bib46">Zass and Shashua, 2006</xref>; <xref ref-type="bibr" rid="bib2">Beck and Teboulle, 2009</xref>) to find the leading 'eigenvector' of every given temporal based input. As shown in the results section, both processes (i.e. direct PCA and the neural network) resulted in hexagonal outcomes when the non-negativity and zero-mean criteria were met. Note that the ease of use of the neural network for solving the positive PCA problem is a nice feature of the neural network implementation, and should be investigated further.</p><p>We also note that while our network focused on the projection from place cells to grid cells, we cannot preclude the importance of the reciprocal projection from grid cells to place cells. Further study will be needed to ‘close the loop’ and simultaneously consider both of these projections at once.</p></sec><sec id="s3-2"><title>Similar studies</title><p>We note that similar work has noticed the relation between place-cell-to-grid-cell transformation and PCA. Notably, <xref ref-type="bibr" rid="bib36">Stachenfeld et al., (2014)</xref> have demonstrated, from considerations related to reinforcement learning, that grid cells could be related to place cells through a PCA transformation. However, due to the unconstrained nature of their transformation, the resulting grid cells were square-like. Furthermore, there has been an endeavor to model the transformation from place cells to grid cells using independent-component-analysis (<xref ref-type="bibr" rid="bib13">Franzius et al., 2007</xref>).</p><p>We also note that there is now a surge of interest in the feedback projection from place cells to grid-cells, which is inverse to the anatomical downstream direction from grid cells to place cells (<xref ref-type="bibr" rid="bib45">Witter and Amaral, 2004</xref>) that has guided most of the models to-date (<xref ref-type="bibr" rid="bib47">Zilli, 2012</xref>; <xref ref-type="bibr" rid="bib15">Giocomo et al., 2011</xref>). In addition to several papers from the Treves group, in which the projection from place cells to grid cells is studied (<xref ref-type="bibr" rid="bib21">Kropff and Treves, 2008</xref>; <xref ref-type="bibr" rid="bib34">Si and Treves, 2013</xref>), there has been also recent work from other groups as well exploring this direction (<xref ref-type="bibr" rid="bib6">Castro and Aguiar, 2014</xref>; <xref ref-type="bibr" rid="bib39">Stepanyuk, 2015</xref>). As far as we are aware, none of the previous studies noted the importance of the non-negativity constraint and the requirement of zero mean input. Additionally, to the best of our knowledge, the analytic results and insights provided in this work (see Materials and methods) are novel, and provide a mathematically consistent explanation for the emergence of hexagonally-spaced grid cells.</p></sec><sec id="s3-3"><title>Predictions of our model</title><p>Based on the findings of this work, it is possible to make several predictions. First, the grid cells must receive zero-mean input over time to produce hexagonally shaped firing patterns. With all feedback projections from place cells being excitatory, the lateral inhibition from other neighboring grid cells might be the balancing parameter to achieve the temporal zero-mean (<xref ref-type="bibr" rid="bib7">Couey et al., 2013</xref>). Alternatively, an adaptation method, such as the one suggested in <xref ref-type="bibr" rid="bib21">Kropff and Treves, (2008)</xref> may be applied. Second, if indeed the grid cells are a lower dimensional representation of the place cells in a PCA form, the place-to-grid neural weights distribution should be similar across identically spaced grid cell populations. This is because all grid cells with similar spacing would have maximized the variance over the same input, resulting in similar spatial solutions. As an aside, we note that such a projection may be a source of phase-related correlations in grid cells (<xref ref-type="bibr" rid="bib40">Tocker et al., 2015</xref>). Third, we found a linear relation between the size of the place cells and the spacing between grid cells. Furthermore, the spacing of the grid cells is mostly determined by the size of the largest place cell – predicting that the feedback from large place cells is not connected to grid cells with small spacing. Fourth, we found modules of different grid spacings in a hierarchical network with the ratio of distances between successive units close to √2. This result is in accordance with the ratio reported in <xref ref-type="bibr" rid="bib37">Stensola et al., (2012)</xref>. However, we note that there is a difference between our results and experimental results because the analysis predicts that there should only be two modules, while the data show at least 5 modules, with a range of scales, the smallest and most numerous having approximately the scale of the smaller place fields found in the dorsal hippocampus (25–30 cm). Fifth, for large enough environments our model suggests that, from mathematical considerations, the grid orientation should approach a uniform orientation in the possible range of 0–15°. This is in discrepancy with experimental results which measure a peak at 7.5°, and not a uniform distribution (<xref ref-type="bibr" rid="bib38">Stensola et al., 2015</xref>). As noted, the discrepancies between our results and reality may relate to the fact that a more advanced model will have to take into account both the downstream projection from grid cells to place cells together with the upstream projection from place cells to grid cells discussed in this paper. Furthermore, such a model will have to take into account the non-uniform distribution of place-cell widths (<xref ref-type="bibr" rid="bib20">Kjelstrup et al., 2008</xref>).</p></sec><sec id="s3-4"><title>Why hexagons?</title><p>In light of our results, we further asked what is special about the hexagonal shape which renders it a stable solution. Past works have demonstrated that hexagonality is optimal in terms of efficient coding. Two recent papers have addressed the potential benefit of encoding by grid cells. <xref ref-type="bibr" rid="bib24">Mathis et al., (2015)</xref> considered the decoding of spatial information based on a grid-like periodic representation. Using lower bounds on the reconstruction error based on a Fisher information criterion, they demonstrated that hexagonal grids lead to the highest spatial resolution in two dimensions (extensions to higher dimensions were also provided). The solution is obtained by mapping the problem onto a circle packing problem. The work of <xref ref-type="bibr" rid="bib41">Wei et al., (2013)</xref> also took a decoding perspective, and showed that hexagonal grids minimize the number of neurons required to encode location with a given resolution. Both papers offer insights into the possible information theoretic benefits of the hexagonal grid solution. In the present paper, we were mainly concerned with a specific biologically motivated learning (development) mechanism that may yield such a solution. Our analysis suggests that the hexagonal patterns can arise as a solution that maximizes the grid cell output variance, under non-negativity constraints. In Fourier space, the solution is a hexagonal lattice with lattice constant near the peak of the Fourier transform of the place cell tuning curve (<xref ref-type="fig" rid="fig15">Figures 15</xref> and <xref ref-type="fig" rid="fig16">16</xref>; see Materials and methods).</p><p>To conclude, this work demonstrates how grid cells could be formed from a simple Hebbian neural network with place cells as inputs, without needing to rely on path-integration mechanisms.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All code was written in MATLAB, and can be obtained on <ext-link ext-link-type="uri" xlink:href="https://github.com/derdikman/Dordek-et-al.-Matlab-code.git">https://github.com/derdikman/Dordek-et-al.-Matlab-code.git</ext-link> or on request from authors.</p><sec id="s4-1"><title>Neural network architecture</title><p>We implemented a single-layer neural network with feedforward connections that was capable of producing a hexagonal-like output (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The feedforward connections were updated according to a self-normalizing version of a Hebbian learning rule referred to as the Oja rule (<xref ref-type="bibr" rid="bib30">Oja, 1982</xref>),<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>J</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>J</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf4"><mml:mrow><mml:msup><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> denotes the learning rate, <inline-formula><mml:math id="inf5"><mml:mrow><mml:mo/><mml:msubsup><mml:mi>J</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="inf6"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> weight and <inline-formula><mml:math id="inf7"><mml:mrow><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are the output and the <inline-formula><mml:math id="inf8"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> input of the network, respectively (all at time <inline-formula><mml:math id="inf9"><mml:mi>t</mml:mi></mml:math></inline-formula>). The weights were initialized randomly according to a uniform distribution and then normalized to have norm 1. The output <inline-formula><mml:math id="inf10"><mml:mrow><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> was calculated every iteration by summing up all pre-synaptic activity from the entire input neuron population. The activity of each output was processed through a sigmoidal function (e.g., <inline-formula><mml:math id="inf11"><mml:mrow><mml:mtext>tanh</mml:mtext></mml:mrow></mml:math></inline-formula>) or a simple linear function. Formally,<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msubsup><mml:mi>J</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where n is the number of input place cells. Since we were initially only concerned with the eigenvector associated with the largest eigenvalue, we did not implement a multiple-output architecture. In this formulation, in which no lateral weights were used, multiple outputs were equivalent to running the same setting with one output several times.</p><p>As discussed in the introduction, this kind of simple feedforward neural network with linear activation and a local weight update in the form of Oja’s rule (1) is known to perform <italic>Principal Components Analysis (PCA)</italic> (<xref ref-type="bibr" rid="bib30">Oja, 1982</xref>; <xref ref-type="bibr" rid="bib31">Sanger, 1989</xref>; <xref ref-type="bibr" rid="bib42">Weingessel and Hornik, 2000</xref>). In the case of a single output the feedforward weights converge to the principal eigenvector of the input's covariance matrix. With several outputs, and lateral weights, as described in the section on modules, the weights converge to the leading principal eigenvectors of the covariance matrix, or, in certain cases (<xref ref-type="bibr" rid="bib42">Weingessel and Hornik, 2000</xref>), to the subspace spanned by the principal eigenvectors. We can thus compare the results of the neural network to those of the mathematical procedure of PCA. Hence, in our simulation, we (1) let the neural networks' weights develop in real time based on the current place cell inputs. In addition, we (2) saved the input activity for every time step to calculate the input covariance matrix and perform (batch) PCA directly.</p><p>It is worth mentioning that the PCA solution described in this section can be interpreted differently based on the Singular Value Decomposition (SVD). Denoting by <inline-formula><mml:math id="inf12"><mml:mi>R</mml:mi></mml:math></inline-formula> the <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> spatio-temporal pattern of place cell activities (after setting the mean to zero), where <inline-formula><mml:math id="inf14"><mml:mi>T</mml:mi></mml:math></inline-formula> is the time duration and <inline-formula><mml:math id="inf15"><mml:mi>d</mml:mi></mml:math></inline-formula> is the number of place cells, the SVD decomposition (see <xref ref-type="bibr" rid="bib19">Jolliffe, 2002</xref>; sec. 3.5) for <inline-formula><mml:math id="inf16"><mml:mi>R</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="inf17"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mtext>ULA'</mml:mtext></mml:math></inline-formula>. For a matrix <inline-formula><mml:math id="inf18"><mml:mi>R</mml:mi></mml:math></inline-formula> of rank <inline-formula><mml:math id="inf19"><mml:mi>r</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf20"><mml:mi>L</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> diagonal matrix whose <inline-formula><mml:math id="inf22"><mml:mi>k</mml:mi></mml:math></inline-formula>th element is equal to <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, the square root of the <inline-formula><mml:math id="inf24"><mml:mi>k</mml:mi></mml:math></inline-formula>th eigenvalue of the covariance matrix <inline-formula><mml:math id="inf25"><mml:mtext>RR'</mml:mtext></mml:math></inline-formula> (computed in the PCA analysis), <inline-formula><mml:math id="inf26"><mml:mi>A</mml:mi></mml:math></inline-formula> is the <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> matrix with <inline-formula><mml:math id="inf28"><mml:mi>k</mml:mi></mml:math></inline-formula>th column equal to the <inline-formula><mml:math id="inf29"><mml:mi>k</mml:mi></mml:math></inline-formula>th eigenvector of <inline-formula><mml:math id="inf30"><mml:mtext>RR'</mml:mtext></mml:math></inline-formula>, and <inline-formula><mml:math id="inf31"><mml:mi>U</mml:mi></mml:math></inline-formula> is the <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> matrix whose <inline-formula><mml:math id="inf33"><mml:mi>k</mml:mi></mml:math></inline-formula>th column is <inline-formula><mml:math id="inf34"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>R</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Note that <inline-formula><mml:math id="inf35"><mml:mi>U</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> dimensional matrix whose <inline-formula><mml:math id="inf37"><mml:mi>k</mml:mi></mml:math></inline-formula>th column represents the temporal dynamics of the <inline-formula><mml:math id="inf38"><mml:mi>k</mml:mi></mml:math></inline-formula><sup>th</sup> grid cell. In other words, the SVD provides a decomposition of the place cell activity in terms of the grid cell activity, as opposed to the grid cell representation in terms of place cell activity we discussed so far. The network learns the spatial weights over place cells (the eigenvectors) as the connections weights from the place cells, and 'projection onto place cell space' (<inline-formula><mml:math id="inf39"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>R</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) is simply the firing rates of the output neuron plotted against the location of the agent.</p><p>The question we therefore asked was under what conditions, when using <italic>place cell-like</italic> inputs, a solution resembling hexagonal <italic>grid cells</italic> emerges. To answer this we used both the neural-network implementation and the direct calculation of the PCA coefficients.</p></sec><sec id="s4-2"><title>Simulation</title><p>We simulated an agent moving in a 2D virtual environment consisting of a square arena covered by <inline-formula><mml:math id="inf40"><mml:mi>n</mml:mi></mml:math></inline-formula> uniformly distributed 2D Gaussian-shaped place cells, organized on a grid, given by<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf41"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> represents the location of the agent. The variables <inline-formula><mml:math id="inf42"><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:math></inline-formula> constitute the temporal input from place cell <inline-formula><mml:math id="inf43"><mml:mi>i</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf44"><mml:mrow><mml:mo/><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf45"><mml:mo mathvariant="bold"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo/><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are the <inline-formula><mml:math id="inf46"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> place cell’s field center and width, respectively (see variations on this input structure below). In order to eliminate boundary effects, periodic boundary conditions were assumed. The virtual agent moved about in a random walk scheme (see Appendix) and explored the environment (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The place cell centers were assumed to be uniformly distributed (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) and shared the same standard deviation <inline-formula><mml:math id="inf47"><mml:mrow><mml:mo/><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula>. The activity of all place cells as a function of time <inline-formula><mml:math id="inf48"><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> was dependent on the stochastic movement of the agent, and formed a [<italic>Neuron x Time</italic>] matrix (<inline-formula><mml:math id="inf49"><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, with T- being the time dimension, see <xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>The simulation was run several times with different input arguments (see <xref ref-type="table" rid="tbl1">Table 1</xref>). The agent was simulated for <inline-formula><mml:math id="inf50"><mml:mi>T</mml:mi></mml:math></inline-formula> time steps, allowing the neural network's weights to develop and reach a steady state by using the learning rule (<xref ref-type="disp-formula" rid="equ1 equ2">Equations 1,2</xref>) and the input (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) data. The simulation parameters are listed below and include parameters related to the environment, simulation, agent and network variables.<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.019</object-id><label>Table 1.</label><caption><p>List of variables used in simulation.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.019">http://dx.doi.org/10.7554/eLife.10094.019</ext-link></p></caption><table frame="hsides" rules="groups"><tbody><tr><td valign="top"><bold>Environment:</bold></td><td valign="top">Size of arena</td><td valign="top">Place cells field width</td><td valign="top">Place cells distribution</td></tr><tr><td valign="top"><bold>Agent:</bold></td><td valign="top">Velocity (angular &amp; linear)</td><td valign="top">Initial position</td><td valign="top">-------------------</td></tr><tr><td valign="top"><bold>Network:</bold></td><td valign="top"># Place cells/ #Grid cells</td><td valign="top">Learning rate</td><td valign="top">Adaptation variable (if used)</td></tr><tr><td valign="top"><bold>Simulation:</bold></td><td valign="top">Duration (time)</td><td valign="top">Time step</td><td valign="top">-------------------</td></tr></tbody></table></table-wrap></p><p>To calculate the PCA directly, we used the MATLAB function <bold><italic>Princomp</italic></bold> in order to evaluate the <inline-formula><mml:math id="inf51"><mml:mi>n</mml:mi></mml:math></inline-formula> principal eigenvectors <inline-formula><mml:math id="inf52"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and corresponding eigenvalues of the input covariance matrix. As mentioned in the Results section, there exists a near fourfold redundancy in the eigenvectors (X-Y axis and in phase). <xref ref-type="fig" rid="fig3">Figure 3</xref> demonstrates this redundancy by plotting the eigenvalues of the covariance matrix. The output response of each eigenvector <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> corresponding to a 2D input location <inline-formula><mml:math id="inf54"><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi>Φ</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mi>q</mml:mi><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo> </mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>n</mml:mi></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf55"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf56"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are the <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> components of the centers of the individual place cell fields. Unless otherwise mentioned, we used place cells in a rectangular grid, such that a place cell is centered at each pixel of the image (that is – number of place cells equals the number of image pixels).</p></sec><sec id="s4-3"><title>Non-negativity constraint</title><p>Projections between place cells and grid cells are known to be primarily excitatory (<xref ref-type="bibr" rid="bib45">Witter and Amaral, 2004</xref>), thus if we aim to mimic the biological circuit, a non-negativity constraint should be added to the feedforward weights in the neural network. While implementing a non-negativity constraint in the neural network is rather easy (a simple rectification rule in the weight dynamics, such that weights which are smaller than 0 are set to 0), the equivalent condition for calculating non-negative Principal Components is more intricate. Since this problem is non-convex and, in general, NP-hard (<xref ref-type="bibr" rid="bib26">Montanari and Richard, 2014</xref>), a numerical procedure was imperative. We used three different algorithms for this purpose.</p><p>The first (<xref ref-type="bibr" rid="bib46">Zass and Shashua, 2006</xref>) named NSPCA (Nonnegative Sparse PCA) is based on coordinate-descent. The algorithm computes a non-negative version of the covariance matrix's eigenvectors and relies on solving a numerical optimization problem, converging to a local maximum starting from a random initial point. The local nature of the algorithm did not guarantee a convergence to a global optimum (recall that the problem is non-convex). The algorithm's inputs consisted of the place cell activities’ covariance matrix, <italic>α</italic> - a balancing parameter between reconstruction and orthonormality, <italic>β</italic> – a variable which controls the amount of sparseness required, and an initial solution vector. For the sake of generality, we set the initial vector to be uniformly random (and normalized), <italic>α</italic> was set to a relatively high value – 10<sup>4</sup> and since no sparseness was needed, <italic>β</italic> was set to zero.</p><p>The second algorithm (<xref ref-type="bibr" rid="bib26">Montanari and Richard, 2014</xref>) does not require any simulation parameters except an arbitrary initialization. It works directly on the inputs and uses a message passing algorithm to define an iterative algorithm to approximately solve the optimization problem. Under specific assumptions it can be shown that the algorithm asymptotically solves the problem (for large input dimensions).</p><p>The third algorithm we use is the parameter free Fast Iterative Threshold and Shrinkage algorithm FISTA (<xref ref-type="bibr" rid="bib2">Beck and Teboulle, 2009</xref>). As described later in this section, this algorithm is the fastest of the three, and allowed us rapid screening of parameter space.</p></sec><sec id="s4-4"><title>Different variants of input structure</title><p>Performing PCA on raw data requires the subtraction of the data mean. Some thought was required in order to determine how to perform this subtraction in the case of the neural network.</p><p>One way to perform the subtraction in the time domain was to dynamically subtract the mean during simulation by using the discrete 1<sup>st</sup> or 2<sup>nd</sup> derivatives of the inputs in time [i.e. from <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, <inline-formula><mml:math id="inf58"><mml:mo>∆</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>r</mml:mi><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:math></inline-formula>]. Under conditions of an isotropic random walk (namely, given any starting position, motion in all directions is equally likely) it is clear that <inline-formula><mml:math id="inf59"><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>∆</mml:mo><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. Another option for subtracting the mean in the time domain was the use of an adaptation variable, as was initially introduced by <xref ref-type="bibr" rid="bib21">Kropff and Treves, (2008)</xref>. Although originally exploited for control over the firing rate, it can be viewed as a variable that represents subtraction of a weighted sum of the firing rate history. Instead of using the inputs <inline-formula><mml:math id="inf60"><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:math></inline-formula> directly in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> to compute the activation <inline-formula><mml:math id="inf61"><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:math></inline-formula>, an intermediate adaptation variable <inline-formula><mml:math id="inf62"><mml:mrow><mml:msubsup><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>δ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> was used (<inline-formula><mml:math id="inf63"><mml:mi>δ</mml:mi></mml:math></inline-formula> being the relative significance of the present temporal sample) as<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:msubsup><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mover><mml:mi>ψ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:msup><mml:mover><mml:mi>ψ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mover><mml:mi>ψ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>It is not hard to see that for <italic>i.i.d.</italic> variables <inline-formula><mml:math id="inf64"><mml:mrow><mml:mo/><mml:msubsup><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, the sequence <inline-formula><mml:math id="inf65"><mml:msup><mml:mover><mml:mi>ψ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup></mml:math></inline-formula> converges for large <inline-formula><mml:math id="inf66"><mml:mi>t</mml:mi></mml:math></inline-formula> to the mean of <inline-formula><mml:math id="inf67"><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:math></inline-formula>. Thus, when <inline-formula><mml:math id="inf68"><mml:mrow><mml:mtext>t</mml:mtext><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula> we find that <inline-formula><mml:math id="inf69"><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, specifically, the adaptation variable is of zero asymptotic mean.</p><p>The second method we used to enforce a zero mean input was simply to create it in advance. Rather than using 2D Gaussian functions (i.e. [<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>]) as inputs we used 2D difference-of-Gaussians (all <inline-formula><mml:math id="inf70"><mml:mi>σ</mml:mi></mml:math></inline-formula> are equal in x and y axis):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:math></disp-formula></p><p>where the constants <inline-formula><mml:math id="inf71"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are set so the integral of the given Laplacian function is zero (if the environment size is not too small, then <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>≈</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). Therefore, if we assume a random walk that covers the entire environment uniformly, the temporal mean of the input would be zero as well. Such input data can be inspired by similar behavior of neurons in the retina and the lateral-geniculate nucleus (<xref ref-type="bibr" rid="bib43">Wiesel and Hubel, 1963</xref>; <xref ref-type="bibr" rid="bib12">Enroth-Cugell and Robson, 1966</xref>). Finally, we implemented another input data type; positive-negative disks (see Appendix). Analogously to the difference-of-Gaussians function, the integral over input is zero so the same goal (zero-mean) was achieved. It is worthwhile noting that subtracting a constant from a simple Gaussian function is not sufficient since at infinity it does not reach zero.</p></sec><sec id="s4-5"><title>Quality of solution and Gridness</title><p>In order to test the hexagonality of the results we used a hexagonal <italic>Gridness score</italic> (<xref ref-type="bibr" rid="bib32">Sargolini et al., 2006</xref>). The Gridness score of the spatial fields was calculated from a cropped ring of their autocorrelogram including the six maxima closest to the center. The ring was rotated six times, <inline-formula><mml:math id="inf74"><mml:mrow><mml:msup><mml:mrow><mml:mn>30</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> per rotation, reaching in total angles of <inline-formula><mml:math id="inf75"><mml:mrow><mml:mo/><mml:msup><mml:mrow><mml:mn>30</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>60</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo/><mml:msup><mml:mrow><mml:mn>90</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>120</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>150</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. Furthermore, for every rotated angle the Pearson correlation with the original un-rotated map was obtained. Denoting by <inline-formula><mml:math id="inf76"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>γ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> the correlation for a specific rotation angle <inline-formula><mml:math id="inf77"><mml:mrow><mml:mo/><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula>, the final Gridness score was (<xref ref-type="bibr" rid="bib21">Kropff and Treves, 2008</xref>):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>60</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>120</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>30</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>90</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>150</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>In addition to this 'traditional' score we used a <italic>Squareness</italic> Gridness score in order to examine how square-like the results are spatially. The special reference to the square shape was driven by the tendency of the spatial solution to converge to a rectangular shape when no constrains were applied. The Squareness Gridness score is similar to the hexagonal one, but now the cropped ring of the autocorrelogram is rotated <inline-formula><mml:math id="inf78"><mml:mrow><mml:mo/><mml:msup><mml:mrow><mml:mn>45</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> every iteration to reach angles of <inline-formula><mml:math id="inf79"><mml:mrow><mml:mo/><mml:msup><mml:mrow><mml:mn>45</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mtext/><mml:msup><mml:mrow><mml:mn>90</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mtext/><mml:msup><mml:mrow><mml:mn>135</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. As before, denoting <inline-formula><mml:math id="inf80"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>γ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as the correlation for a specific rotation angle <inline-formula><mml:math id="inf81"><mml:mrow><mml:mo/><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula> the new Gridness score was calculated as:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mi>S</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo> </mml:mo><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>90</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>45</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>135</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>All errors calculated in gridness measures are SEM (Standard Error of the Mean).</p></sec><sec id="s4-6"><title>Hierarchical networks and modules</title><p>As described in the Results section, we were interested to check whether a hierarchy of outputs could explain the module phenomenon described for real grid cells. We replaced the single-output network with a hierarchical, multiple outputs network, which is capable of computing all 'principal components' of the input data while maintaining the non-negativity constraint as before. The network, introduced by <xref ref-type="bibr" rid="bib31">Sanger, 1989</xref>, computes each output as a linear summation of the weighted inputs similar to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. However, the weights are now calculated according to:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>ψ</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>ψ</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:munderover><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>The first term in the parenthesis when <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> was the regular Hebb-Oja derived rule. In other words, the first output calculated the first non-negative 'principal component' (in inverted commas due to the non-negativity) of the data. Following the first one, the weights of each output received a back projection from the previous outputs. This learning rule applied to the data in a similar manner to the Gram-Schmidt process, subtracting the 'influence' of the previous 'principal components' on the data and recalculating the appropriate 'principal components' of the updated input data.</p><p>In a comparable manner, we applied this technique to the input data <inline-formula><mml:math id="inf83"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math></inline-formula> in order to obtain non-negative 'eigenvectors' from the direct nonnegative-PCA algorithms. We found <inline-formula><mml:math id="inf84"><mml:msub><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> by subtracting from the data the projection of <inline-formula><mml:math id="inf85"><mml:msub><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> on it,<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Next, we computed <inline-formula><mml:math id="inf86"><mml:mo mathvariant="bold"/><mml:msub><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, the first non-negative 'principal component' of <inline-formula><mml:math id="inf87"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula>, and similarly the subsequent ones.</p></sec><sec id="s4-7"><title>Stability of hexagonal solutions</title><p>In order to test the stability of the solutions we obtained under all types of conditions, we applied the ODE method (<xref ref-type="bibr" rid="bib22">Kushner and Clark, 1978</xref>; <xref ref-type="bibr" rid="bib18">Hornik and Kuan, 1992</xref>; <xref ref-type="bibr" rid="bib42">Weingessel and Hornik, 2000</xref>) to the PCA feature extraction algorithm introduced in pervious sections. This method allows one to asymptotically replace the stochastic update equations describing the neural dynamics by smooth differential equations describing the average asymptotic behavior. Under appropriate conditions, the stochastic dynamics converge with probability one to the solution of the ODEs. Although originally this approach was designed for a more general architecture (including lateral connections and asymmetric updating rules), we used a restricted version for our system. In addition, the following analysis is accurate solely for linear output functions. However, since our architecture works well with either linear or non-linear output functions, the conclusions are valid.</p><p>We can rewrite the relevant updating equations of the linear neural network (in matrix form), (see [<xref ref-type="bibr" rid="bib42">Weingessel and Hornik, 2000</xref>] <xref ref-type="disp-formula" rid="equ16 equ17 equ18 equ19 equ20">Equations 15–19</xref>):<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:msup><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>J</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mi>Δ</mml:mi><mml:msup><mml:mi>J</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>J</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>In our case we set<disp-formula id="equ14"><mml:math id="m14"><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>Φ</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Consider the following assumptions</p><list list-type="order"><list-item><p>The input sequence <inline-formula><mml:math id="inf88"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> consists of independent identically distributed, bounded random variables with zero-mean.</p></list-item><list-item><p><inline-formula><mml:math id="inf89"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a positive number sequence satisfying: <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="false"><mml:munder><mml:mo>∑</mml:mo><mml:mi>t</mml:mi></mml:munder></mml:mstyle><mml:mo> </mml:mo><mml:msup><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mo/><mml:mo/><mml:mo/><mml:msup><mml:mrow><mml:mstyle displaystyle="false"><mml:munder><mml:mo>∑</mml:mo><mml:mi>t</mml:mi></mml:munder></mml:mstyle><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&lt;</mml:mo><mml:mi>∞</mml:mi></mml:math></inline-formula>.</p></list-item></list><p>A typical suitable sequence is <inline-formula><mml:math id="inf91"><mml:mrow><mml:mo/><mml:msup><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mo/><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>…</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>For long times, we denote<disp-formula id="equ15"><label>(14)</label><mml:math id="m15"><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>ψ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>J</mml:mi><mml:mo>⋅</mml:mo><mml:mi>r</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>J</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>r</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>J</mml:mi><mml:mi>Σ</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="equ16"><label>(15)</label><mml:math id="m16"><mml:munder><mml:mi>lim</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:munder><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>ψ</mml:mi><mml:msup><mml:mi>ψ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>J</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>J</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>J</mml:mi><mml:mi>Σ</mml:mi><mml:msup><mml:mi>J</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>The penultimate equalities in these equations used the fact that the weights converge with probability one to their average value, resulting from the solution of the ODEs. Following <xref ref-type="bibr" rid="bib42">Weingessel and Hornik, (2000)</xref>, we can analyze <xref ref-type="disp-formula" rid="equ12 equ13">Equations 12,13</xref> under the above assumptions, via their asymptotically equivalent associated ODEs<disp-formula id="equ17"><label>(16)</label><mml:math id="m17"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>J</mml:mi><mml:mi>Σ</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>J</mml:mi><mml:mi>Σ</mml:mi><mml:msup><mml:mi>J</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mi>J</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>with equilibria at<disp-formula id="equ18"><label>(17)</label><mml:math id="m18"><mml:mi>J</mml:mi><mml:mi>Σ</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>J</mml:mi><mml:mi>Σ</mml:mi><mml:msup><mml:mi>J</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mi>J</mml:mi><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>We solved it numerically by exploiting the same covariance matrix and initializing with random weights <inline-formula><mml:math id="inf92"><mml:mrow><mml:mo/><mml:mi>J</mml:mi></mml:mrow></mml:math></inline-formula>. In line with our previous findings, we found that constraining <inline-formula><mml:math id="inf93"><mml:mi>J</mml:mi></mml:math></inline-formula> to be non-negative (by a simple cut-off rule) resulted in a hexagonal shape (in the projection of <inline-formula><mml:math id="inf94"><mml:mi>J</mml:mi></mml:math></inline-formula> onto the place cells space; <xref ref-type="fig" rid="fig11">Figure 11</xref>). In contrast, when the weights were not constrained they converged to square-like results.</p></sec><sec id="s4-8"><title>Steady state analysis</title><p>From this point onwards, we focus on the case of a single output, in which <inline-formula><mml:math id="inf95"><mml:mi>J</mml:mi></mml:math></inline-formula> is a row vector, unless stated otherwise. In the unconstrained case, from <xref ref-type="disp-formula" rid="equ18">Equation 17</xref> any <inline-formula><mml:math id="inf96"><mml:mi>J</mml:mi></mml:math></inline-formula> which is a normalized eigenvector of <inline-formula><mml:math id="inf97"><mml:mi>Σ</mml:mi></mml:math></inline-formula> would be a fixed point. However, from <xref ref-type="disp-formula" rid="equ17">Equation 16</xref>, only the principal eigenvector, which is the solution to the following optimization problem<disp-formula id="equ19"><label>(18)</label><mml:math id="m19"><mml:msub><mml:mi>max</mml:mi><mml:mrow><mml:mi>J</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi>J</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mi>J</mml:mi><mml:mi>Σ</mml:mi><mml:msup><mml:mi>J</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math></disp-formula></p><p>would correspond to a stable fixed point. This is the standard PCA problem. By adding the constraint <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>J</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> we get the non-negative PCA problem.</p><p>To speed up simulation and simplify analysis we make further simplifications.</p><p>First, we assume that the agent’s random movement is ergodic (e.g., an isotropic random walk in a finite box as we used in our simulation), uniform and covering the entire environment, so that<disp-formula id="equ20"><label>(19)</label><mml:math id="m20"><mml:mi>J</mml:mi><mml:mi>Σ</mml:mi><mml:msup><mml:mi>J</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msup><mml:mi>ψ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:munder><mml:msup><mml:mi>ψ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf99"><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> denotes location vector (in contrast to <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is the random process corresponding to the location of the agent), <inline-formula><mml:math id="inf101"><mml:mi>S</mml:mi></mml:math></inline-formula> is the entire environment, and <inline-formula><mml:math id="inf102"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the size of the environment.</p><p>Second, we assume that the environment <inline-formula><mml:math id="inf103"><mml:mi>S</mml:mi></mml:math></inline-formula> is uniformly and densely covered by identical place cells, each of which has the same a tuning curve <inline-formula><mml:math id="inf104"><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> (which integrates to zero). In this case, the activity of the linear grid cell becomes a convolution operation<disp-formula id="equ21"><label>(20)</label><mml:math id="m21"><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">'</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">−</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">'</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">'</mml:mo><mml:mo mathvariant="bold">,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf105"><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is the synaptic weight connecting to the place cell at location <inline-formula><mml:math id="inf106"><mml:mstyle mathvariant="bold"><mml:mtext>x</mml:mtext></mml:mstyle></mml:math></inline-formula>.</p><p>Thus, we can write our objective as<disp-formula id="equ22"><label>(21)</label><mml:math id="m22"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:msub><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:msub><mml:msup><mml:mi>ψ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:msub><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">'</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">−</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">'</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">'</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">x</mml:mtext></mml:math></disp-formula></p><p>under the constraint that the weights are normalized<disp-formula id="equ23"><label>(22)</label><mml:math id="m23"><mml:mfrac><mml:mn>1</mml:mn><mml:mfenced close="|" open="|"><mml:mi>S</mml:mi></mml:mfenced></mml:mfrac><mml:msub><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:msub><mml:msup><mml:mi>J</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo><mml:mo> </mml:mo><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where either <inline-formula><mml:math id="inf107"><mml:mi>J</mml:mi><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">ℝ</mml:mi></mml:math></inline-formula> (PCA) or <inline-formula><mml:math id="inf108"><mml:mi>J</mml:mi><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo><mml:mo> </mml:mo><mml:mo>≥</mml:mo><mml:mo> </mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> ('non-negative PCA').</p><p>Since we expressed the objective using a convolution operation (different boundary conditions can be assumed), it can be solved numerically considerably faster. In the non-negative case, we used the parameter free Fast Iterative Threshold and Shrinkage algorithm [FISTA (<xref ref-type="bibr" rid="bib2">Beck and Teboulle, 2009</xref>); in which we do not use shrinkage, since we only have hard constraints], where the gradient was calculated efficiently using convolutions.</p><p>Moreover, as we show in the following sections, if we assume periodic boundary conditions and use Fourier analysis, we can analytically find the PCA solutions, and obtain important insight on the non-negative PCA solutions.</p></sec><sec id="s4-9"><title>Fourier notation</title><p>Any continuously differentiable function <inline-formula><mml:math id="inf109"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, defined over <inline-formula><mml:math id="inf110"><mml:mi>S</mml:mi><mml:mo>≜</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mi>D</mml:mi></mml:msup></mml:math></inline-formula>, a 'box' region in <inline-formula><mml:math id="inf111"><mml:mi>D</mml:mi></mml:math></inline-formula> dimensions, with periodic boundary conditions, can be written using a Fourier series<disp-formula id="equ24"><label>(23)</label><mml:math id="m24"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mover><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>≜</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:msub><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>⋅</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>≜</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>∈</mml:mo><mml:mover><mml:mi>S</mml:mi><mml:mo mathvariant="italic">^</mml:mo></mml:mover></mml:mrow><mml:mrow/></mml:munderover><mml:mover><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>⋅</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf112"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the volume of the box and<disp-formula id="equ25"><mml:math id="m25"><mml:mrow><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>≜</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>π</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>m</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mi>π</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℤ</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>is the reciprocal lattice of <italic>S</italic> in <bold>k</bold>-space (frequency space).</p></sec><sec id="s4-10"><title>PCA solution</title><p>Assuming periodic boundary conditions, we use Parseval’s identity, and the properties of the convolution, to transform the steady state objective (<xref ref-type="disp-formula" rid="equ22">Equation 21</xref>) to its simpler form in the Fourier domain,<disp-formula id="equ26"><label>(24)</label><mml:math id="m26"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mi>S</mml:mi><mml:mrow/></mml:msubsup><mml:msup><mml:mi>ψ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mtext>d</mml:mtext><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>∈</mml:mo><mml:mover><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Similarly, the normalization constraint can also be written in the Fourier domain,<disp-formula id="equ27"><label>(25)</label><mml:math id="m27"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mi>S</mml:mi><mml:mrow/></mml:msubsup><mml:msup><mml:mi>J</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mtext>d</mml:mtext><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>∈</mml:mo><mml:mover><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Maximizing the objective <xref ref-type="disp-formula" rid="equ26">Equation 24</xref> under this constraint in the Fourier domain, we immediately get that any solution is a linear combination of the Fourier components,<disp-formula id="equ28"><label>(26)</label><mml:math id="m28"><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo>)</mml:mo><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>where<disp-formula id="equ29"><label>(27)</label><mml:math id="m29"><mml:msub><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo mathvariant="bold">*</mml:mo></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>argmax</mml:mi><mml:mrow><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>∈</mml:mo><mml:mover><mml:mi>S</mml:mi><mml:mo mathvariant="italic">^</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>and <inline-formula><mml:math id="inf113"><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> satisfies the normalization constraint. In the original space, the Fourier components are<disp-formula id="equ30"><label>(28)</label><mml:math id="m30"><mml:mi>J</mml:mi><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>·</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mi>ϕ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a free parameter that determines the phase. Also, since <inline-formula><mml:math id="inf115"><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> should assume real values, it is composed of real Fourier components<disp-formula id="equ31"><label>(29)</label><mml:math id="m31"><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo mathvariant="bold">*</mml:mo></mml:msub><mml:mo>⋅</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mi>ϕ</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo mathvariant="bold">*</mml:mo></mml:msub><mml:mo>⋅</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mi>ϕ</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo mathvariant="bold">*</mml:mo></mml:msub><mml:mo mathvariant="bold">⋅</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>+</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:math></disp-formula></p><p>This is a valid solution, since <inline-formula><mml:math id="inf116"><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is a real-valued function, <inline-formula><mml:math id="inf117"><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and therefore <inline-formula><mml:math id="inf118"><mml:mo>-</mml:mo><mml:msub><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo></mml:msub><mml:mo>∈</mml:mo><mml:mo> </mml:mo><mml:msup><mml:msub><mml:mi>argmax</mml:mi><mml:mrow><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>∈</mml:mo><mml:mover><mml:mi mathvariant="normal">S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mover><mml:mi mathvariant="normal">r</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msup><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:math></inline-formula>.</p></sec><sec id="s4-11"><title>PCA solution for a difference of Gaussians tuning curve</title><p>In this paper we focused on the case where <inline-formula><mml:math id="inf119"><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> has the shape of a difference of Gaussians (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>),<disp-formula id="equ32"><label>(30)</label><mml:math id="m32"><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo> </mml:mo></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>‖</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo> </mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>‖</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf120"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf121"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are some positive normalization constants, set so that <inline-formula><mml:math id="inf122"><mml:msub><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> (see appendix). The Fourier transform of <inline-formula><mml:math id="inf123"><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:math></inline-formula> is also a difference of Gaussians<disp-formula id="equ33"><label>(31)</label><mml:math id="m33"><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo>‖</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo>‖</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf124"><mml:mo>∀</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>∈</mml:mo><mml:mover><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>, as we show in the appendix. Therefore the value of the Fourier domain objective only depends on the radius <inline-formula><mml:math id="inf125"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and all solutions <inline-formula><mml:math id="inf126"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> have the same radius <inline-formula><mml:math id="inf127"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. If <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>L</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>, then the <inline-formula><mml:math id="inf129"><mml:mi>k</mml:mi></mml:math></inline-formula>-lattice <inline-formula><mml:math id="inf130"><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> becomes dense (<inline-formula><mml:math id="inf131"><mml:mrow><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>→</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>) and this radius is equal to<disp-formula id="equ34"><label>(32)</label><mml:math id="m34"><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mfenced close="]" open="["><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>which is a unique maximizer, that can be easily obtained numerically.</p><p>Notice that if we multiply the place cell field width by some positive constant <inline-formula><mml:math id="inf132"><mml:mi>c</mml:mi></mml:math></inline-formula>, then the solution <inline-formula><mml:math id="inf133"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> will be divided by <inline-formula><mml:math id="inf134"><mml:mi>c</mml:mi></mml:math></inline-formula>. The grid spacing, proportional to <inline-formula><mml:math id="inf135"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, would therefore also be multiplied by <inline-formula><mml:math id="inf136"><mml:mi>c</mml:mi></mml:math></inline-formula>. This entails a linear dependency between the place cell field width and the grid cell spacing, in the limit of a large box size <inline-formula><mml:math id="inf137"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. When the box has a finite size, <italic>k</italic>-lattice discretization also has a (usually small) effect on the grid spacing.</p><p>In that case, all solutions <inline-formula><mml:math id="inf138"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> are restricted to be on the finite lattice <inline-formula><mml:math id="inf139"><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>. Therefore, the solutions <inline-formula><mml:math id="inf140"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> are the points on the lattice <inline-formula><mml:math id="inf141"><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> for which the radius <inline-formula><mml:math id="inf142"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is closest to <inline-formula><mml:math id="inf143"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig15">Figure 15B,C</xref>).<fig id="fig15" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.020</object-id><label>Figure 15.</label><caption><title>PCA k-space analysis for a difference of Gaussians tuning curve.</title><p>(<bold>A</bold>) The 1D tuning curve <inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) The 1D tuning curve Fourier transform <inline-formula><mml:math id="inf145"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The black circles indicate k-lattice points. The PCA solution, <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, is given by the circles closest to <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, the peak of <inline-formula><mml:math id="inf148"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (red cross). (<bold>C</bold>) A contour plot of the 2D tuning curve Fourier transform <inline-formula><mml:math id="inf149"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In 2D k-space the peak of <inline-formula><mml:math id="inf150"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> becomes a circle (red), and the k-lattice <inline-formula><mml:math id="inf151"><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> is a square lattice (black circles). The lattice point can be partitioned into equivalent groups. Several such groups are marked in blue on the lattice. For example, the PCA solution Fourier components lie on the four lattice points closest to the circle, denoted A1-4. Note the grouping of A,B,C &amp; D (4,8,4 and 4, respectively) corresponds to the grouping of the 20 highest principal components in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Parameters: <inline-formula><mml:math id="inf152"><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>7.5</mml:mn><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.020">http://dx.doi.org/10.7554/eLife.10094.020</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig15-v2.tif"/></fig></p></sec><sec id="s4-12"><title>The degeneracy of the PCA solution</title><p>The number of real-valued PCA solutions (degeneracy) in 1D is two, as there are exactly two maxima, <inline-formula><mml:math id="inf153"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf154"><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>. The phase <inline-formula><mml:math id="inf155"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, determines how the components at <inline-formula><mml:math id="inf156"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf157"><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> are linearly combined.</p><p>However, there are more maxima in the 2D case. Specifically, given a maximum <inline-formula><mml:math id="inf158"><mml:mstyle mathvariant="bold"><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mstyle></mml:math></inline-formula>, we can write <inline-formula><mml:math id="inf159"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf160"><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="normal">ℤ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. Usually there are 7 other different points with the same radius: <inline-formula><mml:math id="inf161"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>,<inline-formula><mml:math id="inf162"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>,<inline-formula><mml:math id="inf163"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>,<inline-formula><mml:math id="inf164"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>,<inline-formula><mml:math id="inf165"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>,<inline-formula><mml:math id="inf166"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf167"><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, so we will have a degeneracy of eight (corresponding to the symmetries of a square box). This is case of points in group B, shown in <xref ref-type="fig" rid="fig15">Figure 15C</xref>.</p><p>However, we can also get a different degeneracy. First, if either <inline-formula><mml:math id="inf168"><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo>±</mml:mo><mml:mi>n</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf170"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> we will have a degeneracy of 4, since then some of the original eight points will coincide (groups A,C and D in <xref ref-type="fig" rid="fig15">Figure 15C</xref>). Second, additional points <inline-formula><mml:math id="inf171"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can exist such that <inline-formula><mml:math id="inf172"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, (Pythagorean triplets with the same hypotenuse) – for example, <inline-formula><mml:math id="inf173"><mml:mrow><mml:msup><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mn>20</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>25</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mn>7</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mn>24</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. These points will also appear in groups of four or eight.</p><p>Therefore, we will always have a degeneracy which is some multiple of 4. Note that in the full network simulation, the degeneracy is not exact. This is due to the perturbation noise from the agent’s random walk as well as the non-uniform sampling of the place cells.</p></sec><sec id="s4-13"><title>The PCA solution with a non-negative constraint</title><p>Next, we add the non-negativity constraint <inline-formula><mml:math id="inf174"><mml:mi>J</mml:mi><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. As mentioned earlier, this constraint renders the optimization problem NP-hard, and prevents us from a complete analytical solution. We therefore combine numerical and mathematical analysis, in order to gain intuition as to why</p><list list-type="order"><list-item><p>Locally optimal 2D solutions are hexagonal.</p></list-item><list-item><p>These solutions have a grid spacing near (<inline-formula><mml:math id="inf175"><mml:mrow><mml:mn>4</mml:mn><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf176"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula> is the peak of <inline-formula><mml:math id="inf177"><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced></mml:math></inline-formula>).</p></list-item><list-item><p>The average grid alignment is approximately 7.5°, for large environments.</p></list-item><list-item><p>Why grid cells have modules, and what is their spacing.</p></list-item></list></sec><sec id="s4-14"><title>1D Solutions</title><p>Our numerical results indicate that the Fourier components of any locally optimal 1D solution of non-negative PCA have the following structure:</p><list list-type="order"><list-item><p>There is a non-negative 'DC component' (<italic>k</italic> <italic>=</italic> 0).</p></list-item><list-item><p>The maximal non-DC component, (<italic>k</italic> ≠ 0) is <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf179"><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:math></inline-formula> is 'close' (more details below) to <inline-formula><mml:math id="inf180"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula>, the peak of <inline-formula><mml:math id="inf181"><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>.</p></list-item><list-item><p>All other non-zero Fourier components are <inline-formula><mml:math id="inf182"><mml:msubsup><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>∞</mml:mo></mml:msubsup></mml:math></inline-formula>, weaker harmonies of <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:math></inline-formula>.</p></list-item></list><p>This structure suggests that the component at <inline-formula><mml:math id="inf184"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> aims to maximize the objective, while the other components guarantee the non-negativity of the solution <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In order to gain some analytical intuition as to why this is the case, we first examine the limit case that <inline-formula><mml:math id="inf186"><mml:mrow><mml:mi>L</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf187"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is highly peaked at <inline-formula><mml:math id="inf188"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula>. In that case the Fourier objective (<xref ref-type="disp-formula" rid="equ26">Equation 24</xref>) simply becomes <inline-formula><mml:math id="inf189"><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. For simplicity, we will rescale our units so that <inline-formula><mml:math id="inf190"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, and the objective becomes <inline-formula><mml:math id="inf191"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. Therefore, the solution must include a Fourier component at <inline-formula><mml:math id="inf192"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> or the objective would be zero. The other components exist only to maintain the non-negativity constraint, since if they increase in magnitude, then the objective, which is proportional to <inline-formula><mml:math id="inf193"><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, must decrease to compensate (due to the normalization constraint – <xref ref-type="disp-formula" rid="equ27">Equation 25</xref>). Note that these components must include a positive 'DC component' at <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, or else <inline-formula><mml:math id="inf195"><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:munder><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:munder><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>∝</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, which contradicts the constraints. To find all the Fourier components, we examine a solution composed of only a few (<inline-formula><mml:math id="inf196"><mml:mi>M</mml:mi></mml:math></inline-formula>) components<disp-formula id="equ35"><mml:math id="m35"><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msub><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Clearly, we can set <inline-formula><mml:math id="inf197"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, or otherwise, the objective would be zero. Also, we must have<disp-formula id="equ36"><mml:math id="m36"><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>min</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msub><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Otherwise, the solution would be either (1) negative or (2) non-optimal, since we can decrease <inline-formula><mml:math id="inf198"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and increase <inline-formula><mml:math id="inf199"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>For <inline-formula><mml:math id="inf200"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, we immediately get that, in the optimal solution, <inline-formula><mml:math id="inf201"><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf202"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> does not matter). For <inline-formula><mml:math id="inf203"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf204"><mml:mn>4</mml:mn></mml:math></inline-formula> a solution is harder to find directly, so we performed a parameter grid search over all the free parameters (<inline-formula><mml:math id="inf205"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf206"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) in those components. We found that the optimal solution (which maximizes the objective <inline-formula><mml:math id="inf207"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>), had the following form<disp-formula id="equ37"><label>(33)</label><mml:math id="m37"><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf208"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is a free parameter. This form results from a parameter grid search for <inline-formula><mml:math id="inf209"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf210"><mml:mn>4</mml:mn></mml:math></inline-formula>, under the assumption that <inline-formula><mml:math id="inf211"><mml:mrow><mml:mi>L</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf212"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is highly peaked. However, our numerical results in the general case (<xref ref-type="fig" rid="fig16">Figure 16A</xref>), using the FISTA algorithm, indicate that the locally optimal solution does not change much even if <inline-formula><mml:math id="inf213"><mml:mi>L</mml:mi></mml:math></inline-formula> is finite, and <inline-formula><mml:math id="inf214"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is not highly peaked. Specifically, it has a similar form<disp-formula id="equ38"><label>(34)</label><mml:math id="m38"><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf215"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is rapidly decaying (<xref ref-type="fig" rid="fig16">Figure 16A</xref>), effectively only the first few components are non-negligible, as in <xref ref-type="disp-formula" rid="equ37">Equation 33</xref>. This can also be seen in the value of the objective obtained in the parameter scan<disp-formula id="equ39"><label>(35)</label><mml:math id="m39"><mml:mtable><mml:mtr><mml:mtd><mml:mi>M</mml:mi></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>3</mml:mn></mml:mtd><mml:mtd><mml:mn>4</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mtd><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mn>6</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mn>0.2367</mml:mn></mml:mtd><mml:mtd><mml:mn>0.2457</mml:mn></mml:mtd><mml:mtd><mml:mn>0.2457</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where the contribution of additional high frequency components to the objective quickly becomes negligible. In fact, the value of the objective cannot increase above <inline-formula><mml:math id="inf216"><mml:mrow><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula>, as we explain in the next section.</p><p>And so, the main difference between <xref ref-type="disp-formula" rid="equ37 equ38">Equations 33 and 34</xref> is the base frequency, <inline-formula><mml:math id="inf217"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, which is slightly different from <inline-formula><mml:math id="inf218"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula>. As explained in the appendix, the relation between <inline-formula><mml:math id="inf219"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf220"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula> depends on the <inline-formula><mml:math id="inf221"><mml:mi>k</mml:mi></mml:math></inline-formula>-lattice discretization, as well as on the properties of <inline-formula><mml:math id="inf222"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.<fig id="fig16" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.021</object-id><label>Figure 16.</label><caption><title>Fourier components of Non-negative PCA on the <inline-formula><mml:math id="inf223"><mml:mi>k</mml:mi></mml:math></inline-formula>-lattice.</title><p>(<bold>A</bold>) 1D solution (blue) includes: a DC component (<inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), a maximal component with magnitude near <inline-formula><mml:math id="inf225"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> (red line), and weaker harmonics of the maximal component. (<bold>B</bold>) 2D solution includes: a DC component (<italic><bold>k</bold></italic> = (0,0)), a hexgaon of strong components with radius near <inline-formula><mml:math id="inf226"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> (red circle), and weaker components on the lattice of the strong components. White dots show underlying <inline-formula><mml:math id="inf227"><mml:mi>k</mml:mi></mml:math></inline-formula>-lattice. We used a difference of Gaussians tuning curve, with parameters <inline-formula><mml:math id="inf228"><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>7.5</mml:mn><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>, and the FISTA algorithm.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.021">http://dx.doi.org/10.7554/eLife.10094.021</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig16-v2.tif"/></fig></p></sec><sec id="s4-15"><title>2D Solutions</title><p>The 1D properties, described in the previous section, generalize to the 2D case in the following manner:</p><list list-type="order"><list-item><p>There is a non-negative DC component <inline-formula><mml:math id="inf229"><mml:mtext>(</mml:mtext><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>=</mml:mo><mml:mtext>(</mml:mtext><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mtext>))</mml:mtext></mml:math></inline-formula>.</p></list-item><list-item><p>A small 'basis set' of components <inline-formula><mml:math id="inf230"><mml:msubsup><mml:mfenced close="}" open="{"><mml:msubsup><mml:mi mathvariant="bold">k</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:math></inline-formula>with similar amplitudes, and with similar radii <inline-formula><mml:math id="inf231"><mml:mfenced close="||" open="||"><mml:msubsup><mml:mi mathvariant="bold">k</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:math></inline-formula> which are all 'close' to <inline-formula><mml:math id="inf232"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula> (details below).</p></list-item><list-item><p>All other non-zero Fourier components are weaker, and restricted to the lattice</p></list-item></list><p><disp-formula id="equ40"><mml:math id="m40"><mml:msub><mml:mfenced close="}" open="{"><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>∈</mml:mo><mml:mover><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo mathvariant="bold">∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">i</mml:mi><mml:mo mathvariant="bold">=</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:munderover><mml:msub><mml:mi>n</mml:mi><mml:mi mathvariant="bold">i</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">k</mml:mi><mml:mo mathvariant="bold">*</mml:mo><mml:mrow><mml:mo mathvariant="bold">(</mml:mo><mml:mi mathvariant="bold">i</mml:mi><mml:mo mathvariant="bold">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="normal">ℤ</mml:mi><mml:mi>B</mml:mi></mml:msup></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Interestingly, given these properties of the solution we already get hexagonal patterns, as we explain next.</p><p>Similarly to the 1D case, the difference between <inline-formula><mml:math id="inf233"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf234"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula> is affected by lattice discretization, and the curvature of <inline-formula><mml:math id="inf235"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> near <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula>. To simplify matters, we focus first on the simple case that <inline-formula><mml:math id="inf237"><mml:mrow><mml:mi>L</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf238"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is sharply peaked around <inline-formula><mml:math id="inf239"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula>. Therefore, the Fourier objective becomes <inline-formula><mml:math id="inf240"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>, so the only Fourier components that appear in the objective are <inline-formula><mml:math id="inf241"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, which have radius <inline-formula><mml:math id="inf242"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula>. We examine the values this objective can have.</p><p>All the base components have the same radius. This implies, according to the Crystallographic restriction theorem in 2D, that the only allowed lattice angles (in the range between 0 and 90 degrees) are 0, 60 and 90 degrees. Therefore, there are only three possible lattice types in 2D. Next, we examine the value of the objective for each of these lattice types:</p><p>1) Square lattice, in which <inline-formula><mml:math id="inf243"><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>†</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>†</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, up to a rotation. In this case,<disp-formula id="equ41"><mml:math id="m41"><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mrow/></mml:msubsup><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and the value of the objective is bounded above by <inline-formula><mml:math id="inf244"><mml:mrow><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula> (see proof in appendix).</p><p>2) 1D lattice, in which <inline-formula><mml:math id="inf245"><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, up to a rotation. This is a special case of the square lattice, with a subset of <inline-formula><mml:math id="inf246"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mrow/></mml:msubsup></mml:mrow></mml:math></inline-formula>equal to zero, so we can write, as we did in the 1D case<disp-formula id="equ42"><mml:math id="m42"><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi><mml:mrow/></mml:msubsup></mml:mrow></mml:mstyle><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Therefore, the same objective upper bound, <inline-formula><mml:math id="inf247"><mml:mrow><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula>, holds. Note that some of the solutions we found numerically are close to this bound (<xref ref-type="disp-formula" rid="equ39">Equation 35</xref>).</p><p>3) Hexagonal lattice, in which the base components are<disp-formula id="equ43"><mml:math id="m43"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>up to a rotation by some angle <inline-formula><mml:math id="inf248"><mml:mi>α</mml:mi></mml:math></inline-formula>. Our parameter scans indicate that the objective value cannot surpass <inline-formula><mml:math id="inf249"><mml:mrow><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> in any solution composed of only the base hexgonal components <inline-formula><mml:math id="inf250"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold">k</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:msubsup></mml:math></inline-formula> and a DC component. However, taking into account also some higher order lattice components, we can find a better solution, with an objective value of <inline-formula><mml:math id="inf251"><mml:mrow><mml:mn>0.2558</mml:mn></mml:mrow></mml:math></inline-formula>. Though this is not necessarily the optimal solution, it surpasses any possible solutions on the other lattice types (bounded below <inline-formula><mml:math id="inf252"><mml:mrow><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula>, as we proved in the appendix). Specifically, this solution is composed of the base vectors <inline-formula><mml:math id="inf253"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>*</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> and their harmonics<disp-formula id="equ44"><mml:math id="m44"><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>8</mml:mn></mml:munderover><mml:msub><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo mathvariant="bold">*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo mathvariant="bold">⋅</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf254"><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">2k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf255"><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">2k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf256"><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>6</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">2k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf257"><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>7</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf258"><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>8</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. Also, <inline-formula><mml:math id="inf259"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6449</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf260"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.292</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf261"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>5</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>6</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.0101</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf262"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>7</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>8</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.134</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>Thus, any optimal solution must be on the hexagonal lattice, given our approximations. In practice, the lattice hexagonal basis vectors do not have exactly the same radius, and, as in the 1D case, this radius is somewhat smaller then <inline-formula><mml:math id="inf263"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, due to the lattice discretization, and due to that <inline-formula><mml:math id="inf264"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is not sharply peaked. However, the resulting solution lattice is still approximately hexagonal in <inline-formula><mml:math id="inf265"><mml:mi>k</mml:mi></mml:math></inline-formula>-space. For example, this can be seen in the numerically obtained solution in <xref ref-type="fig" rid="fig16">Figure 16B</xref> – where the strongest non-DC Fourier components form an approximate hexagon near <inline-formula><mml:math id="inf266"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, from the Fourier components A, defined in <xref ref-type="fig" rid="fig17">Figure 17</xref>.<fig id="fig17" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.022</object-id><label>Figure 17.</label><caption><title>The modules in Fourier space.</title><p>As in <xref ref-type="fig" rid="fig15">Figure 15C</xref>, we see a contour plot of the 2D tuning curve Fourier transform <inline-formula><mml:math id="inf267"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the k-space the peak of <inline-formula><mml:math id="inf268"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (red circle), and the k-lattice <inline-formula><mml:math id="inf269"><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> (black circles). The lattice points can be divided into approximately hexgonal shaped groups. Several such groups are marked in blue on the lattice. For example, group A and B are optimal since they are nearest to the red circle. The next best (with the highest-valued contours) group of points, which have an approximate hexgonal shape, is C. Note that group C has a k-radius of approximately the optimal radius times <inline-formula><mml:math id="inf270"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> (cyan circle). Parameters: <inline-formula><mml:math id="inf271"><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>7.5</mml:mn><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.022">http://dx.doi.org/10.7554/eLife.10094.022</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-fig17-v2.tif"/></fig></p><sec id="s4-15-1"><title>Grid spacing</title><p>In general, we get a hexagonal grid pattern in <inline-formula><mml:math id="inf272"><mml:mi>x</mml:mi></mml:math></inline-formula>-space. If all base Fourier components have a radius of <inline-formula><mml:math id="inf273"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, then the grid spacing in <inline-formula><mml:math id="inf274"><mml:mi>x</mml:mi></mml:math></inline-formula>-space would be <inline-formula><mml:math id="inf275"><mml:mrow><mml:mn>4</mml:mn><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Since the radius of the basis vectors can be smaller than <inline-formula><mml:math id="inf276"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, the value of <inline-formula><mml:math id="inf277"><mml:mrow><mml:mn>4</mml:mn><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a lower bound to the actual grid spacing (as demonstrated in <xref ref-type="fig" rid="fig12">Figure 12A</xref>), up to lattice discretization effects.</p></sec><sec id="s4-15-2"><title>Grid alignment</title><p>The angle of the hexagonal grid, <inline-formula><mml:math id="inf278"><mml:mi>α</mml:mi></mml:math></inline-formula>, is determined by the directions of the hexagonal vectors. An angle <inline-formula><mml:math id="inf279"><mml:mi>α</mml:mi></mml:math></inline-formula> is possible, if there exists a <inline-formula><mml:math id="inf280"><mml:mi>k</mml:mi></mml:math></inline-formula>-lattice point <inline-formula><mml:math id="inf281"><mml:mi mathvariant="bold">k</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> (with <inline-formula><mml:math id="inf282"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> integers), for which <inline-formula><mml:math id="inf283"><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mi>L</mml:mi></mml:mfrac><mml:mo>&gt;</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mo>*</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>, and then <inline-formula><mml:math id="inf284"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mi>arctan</mml:mi><mml:mfrac><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>. Since the hexagonal lattice has rotational symmetry of <inline-formula><mml:math id="inf285"><mml:mrow><mml:msup><mml:mrow><mml:mn>60</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, we can restrict <inline-formula><mml:math id="inf286"><mml:mi>α</mml:mi></mml:math></inline-formula> to be in the range <inline-formula><mml:math id="inf287"><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mn>30</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>≤</mml:mo><mml:mi>α</mml:mi><mml:mo>≤</mml:mo><mml:msup><mml:mrow><mml:mn>30</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. The grid alignment, which is the minimal angle of the grid with the box boundaries is given by<disp-formula id="equ45"><label>(36)</label><mml:math id="m45"><mml:mrow><mml:mtext>Grid</mml:mtext><mml:mtext> </mml:mtext><mml:mtext>aligment</mml:mtext><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>α</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>9</mml:mn><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>α</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mn>60</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>α</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>30</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>α</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which is limited to the range <inline-formula><mml:math id="inf288"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, since <inline-formula><mml:math id="inf289"><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mn>30</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>≤</mml:mo><mml:mi>α</mml:mi><mml:mo>≤</mml:mo><mml:msup><mml:mrow><mml:mn>30</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. There are usually several possible grid alignments which are (approximately) rotated versions of each other (i.e., different <inline-formula><mml:math id="inf290"><mml:mi>α</mml:mi></mml:math></inline-formula>). Note that, due to the <inline-formula><mml:math id="inf291"><mml:mi>k</mml:mi></mml:math></inline-formula>-lattice discretization, different alignments can result in slightly different objective values. However, the numerical algorithms we used to solve the optimization problem reached many possible grid alignments with a positive probability (<xref ref-type="fig" rid="fig12">Figure 12C</xref>), since we started from a random initialization and converged to a local minimum.</p><p>In the limit <inline-formula><mml:math id="inf292"><mml:mrow><mml:mi>L</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>, the grid alignment will become uniform in the range <inline-formula><mml:math id="inf293"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and the average grid alignment is <inline-formula><mml:math id="inf294"><mml:mrow><mml:msup><mml:mrow><mml:mn>7.5</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-15-3"><title>Hierarchical networks and modules</title><p>There are multiple routes to generalize non-negative PCA with multiple vectors. In this paper we chose to do so using a 'Gramm-Schmidt' like process, which can be written in the following way. First we define,<disp-formula id="equ46"><label>(37)</label><mml:math id="m46"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">,</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">−</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo> </mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>‖</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">−</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo> </mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>‖</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo mathvariant="bold">−</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>J</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>and then, recursively, this process recovers non-negative 'eigenvectors' by subtracting out the previous components, similarly to Sanger’s multiple PCA algorithm (<xref ref-type="bibr" rid="bib31">Sanger, 1989</xref>), and enforcing the non-negativity constraint.<disp-formula id="equ47"><label>(38)</label><mml:math id="m47"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mtext>,</mml:mtext><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mtext>,</mml:mtext><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mtext>,</mml:mtext><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>)</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>)</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>)</mml:mo><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">z</mml:mtext><mml:mspace linebreak="newline"/><mml:msub><mml:mi>J</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo> </mml:mo><mml:msub><mml:mi>max</mml:mi><mml:mrow><mml:mi>J</mml:mi><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mfenced close="|" open="|"><mml:mi>S</mml:mi></mml:mfenced></mml:mfrac><mml:msub><mml:mo>∫</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:msup><mml:mi>J</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mo>∫</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">x</mml:mtext><mml:msup><mml:mfenced><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mtext>,</mml:mtext><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>)</mml:mo><mml:mi>J</mml:mi><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>)</mml:mo><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">y</mml:mtext></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mtext>.</mml:mtext></mml:math></disp-formula></p><p>To analyze this, we write the objectives we maximize in the Fourier domain, using Parseval’s Theorem.</p><p>For <italic>n</italic> =1, we recover the old objective (<xref ref-type="disp-formula" rid="equ26">Equation 24</xref>):<disp-formula id="equ48"><label>(39)</label><mml:math id="m48"><mml:munder><mml:mo>∑</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi><mml:mo mathvariant="normal">∈</mml:mo><mml:mover><mml:mi mathvariant="italic">S</mml:mi><mml:mo mathvariant="italic">^</mml:mo></mml:mover></mml:mstyle></mml:munder><mml:msup><mml:mfenced close="|" open="|"><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>For <italic>n</italic> =2, we get<disp-formula id="equ49"><label>(40)</label><mml:math id="m49"><mml:munder><mml:mo>∑</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi><mml:mo mathvariant="normal">∈</mml:mo><mml:mover><mml:mi mathvariant="italic">S</mml:mi><mml:mo mathvariant="italic">^</mml:mo></mml:mover></mml:mstyle></mml:munder><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>q</mml:mi><mml:mo mathvariant="normal">∈</mml:mo><mml:mover><mml:mi mathvariant="italic">S</mml:mi><mml:mo mathvariant="italic">^</mml:mo></mml:mover></mml:mstyle></mml:munder><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">q</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">q</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf295"><mml:msup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> is the complex conjugate of <inline-formula><mml:math id="inf296"><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>. This objective is similar to the original one, except that it penalizes <inline-formula><mml:math id="inf297"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> if its components are similar to those of <inline-formula><mml:math id="inf298"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. As <italic>n</italic> increases the objective becomes more and more complicated, but as before, it contains terms which penalize <inline-formula><mml:math id="inf299"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> if its components are similar to any of the previous solutions (<italic>i.e.</italic>, <inline-formula><mml:math id="inf300"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf301"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>). This form suggests that each new 'eigenvector' tends to occupy new points in the Fourier lattice (similarly to unconstrained PCA solutions).</p><p>For example, the numerical solution shown in <xref ref-type="fig" rid="fig16">Figure 16B</xref> is composed of the Fourier lattice components in group A, defined in <xref ref-type="fig" rid="fig17">Figure 17</xref>. A completely equivalent solution would be in group B (it is just a 90 degrees rotation of the first). The next 'eigenvectors' should then include other Fourier-lattice components outside groups A and B. Note that components with smaller <inline-formula><mml:math id="inf302"><mml:mi>k</mml:mi></mml:math></inline-formula>-radius cannot be arranged to be hexagonal (not even approximately), so they will have a low gridness score. In contrast, the next components with higher <inline-formula><mml:math id="inf303"><mml:mi>k</mml:mi></mml:math></inline-formula>-radius (e.g., group C) can form an approximately hexagonal shape together, and would appear as an additional grid cell 'module'. The grid spacing of this new module will decrease by <inline-formula><mml:math id="inf304"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula>, since the new <inline-formula><mml:math id="inf305"><mml:mi>k</mml:mi></mml:math></inline-formula>-radius is about <inline-formula><mml:math id="inf306"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> times larger than the <inline-formula><mml:math id="inf307"><mml:mi>k</mml:mi></mml:math></inline-formula>-radius of groups A and B.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Alexander Mathis and Omri Barak for comments on the manuscript, and Gilad Tocker and Matan Sela for helpful discussions and advice. The research was supported by the Israel Science Foundation grants 955/13 and 1882/13, by a Rappaport Institute grant, and by the Allen and Jewel Prince Center for Neurodegenerative Disorders of the Brain. The work of RM and Y D was partially supported by the Ollendorff Center of the Department of Electrical Engineering, Technion. DD is a David and Inez Myers Career Advancement Chair in Life Sciences fellow. The work of DS was partially supported by the Gruss Lipper Charitable Foundation, and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>YD, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>DS, Conception and design, Analysis and interpretation of data, Drafting or revising the article, Analytical derivations in Materials and methods and in Appendix</p></fn><fn fn-type="con" id="con3"><p>RM, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con4"><p>DD, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Hayman</surname><given-names>R</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Experience-dependent rescaling of entorhinal grids</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>682</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.1038/nn1905</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>A</given-names></name><name><surname>Teboulle</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A fast iterative shrinkage-thresholding algorithm for linear inverse problems</article-title><source>SIAM Journal on Imaging Sciences</source><volume>2</volume><fpage>183</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1137/080716542</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boccara</surname><given-names>CN</given-names></name><name><surname>Sargolini</surname><given-names>F</given-names></name><name><surname>Thoresen</surname><given-names>VH</given-names></name><name><surname>Solstad</surname><given-names>T</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Grid cells in pre- and parasubiculum</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>987</fpage><lpage>994</lpage><pub-id pub-id-type="doi">10.1038/nn.2602</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnevie</surname><given-names>T</given-names></name><name><surname>Dunn</surname><given-names>B</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Derdikman</surname><given-names>D</given-names></name><name><surname>Kubie</surname><given-names>JL</given-names></name><name><surname>Roudi</surname><given-names>Y</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Grid cells require excitatory drive from the hippocampus</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>309</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1038/nn.3311</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The 2014 Nobel Prize in Physiology or Medicine: a spatial model for cognitive neuroscience</article-title><source>Neuron</source><volume>84</volume><fpage>1120</fpage><lpage>1125</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.009</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castro</surname><given-names>L</given-names></name><name><surname>Aguiar</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A feedforward model for the formation of a grid field where spatial information is provided solely from place cells</article-title><source>Biological Cybernetics</source><volume>108</volume><fpage>133</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1007/s00422-013-0581-3</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Couey</surname><given-names>JJ</given-names></name><name><surname>Witoelar</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>SJ</given-names></name><name><surname>Zheng</surname><given-names>K</given-names></name><name><surname>Ye</surname><given-names>J</given-names></name><name><surname>Dunn</surname><given-names>B</given-names></name><name><surname>Czajkowski</surname><given-names>R</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Roudi</surname><given-names>Y</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Recurrent inhibitory circuitry as a mechanism for grid formation</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>318</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1038/nn.3310</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>H</given-names></name><name><surname>Geary</surname><given-names>Z</given-names></name><name><surname>Kadanoff</surname><given-names>LP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Asymptotics of eigenvalues and eigenvectors of Toeplitz matrices</article-title><source>Journal of Statistical Mechanics: Theory and Experiment</source><volume>2009</volume><fpage>P05012</fpage><pub-id pub-id-type="doi">10.1088/1742-5468/2009/05/P05012</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derdikman</surname><given-names>D</given-names></name><name><surname>Hildesheim</surname><given-names>R</given-names></name><name><surname>Ahissar</surname><given-names>E</given-names></name><name><surname>Arieli</surname><given-names>A</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Imaging spatiotemporal dynamics of surround inhibition in the barrels somatosensory cortex</article-title><source>Journal of Neuroscience</source><volume>23</volume><fpage>3100</fpage><lpage>3105</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Derdikman</surname><given-names>D</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><source><italic>Space,Time and Memory in the Hippocampal Formation</italic></source><publisher-loc>Vienna</publisher-loc><publisher-name>Springer Vienna</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-7091-1292-2</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Perspectives on 2014 nobel prize</article-title><source>Hippocampus</source><volume>25</volume><fpage>679</fpage><lpage>681</lpage><pub-id pub-id-type="doi">10.1002/hipo.22445</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enroth-Cugell</surname><given-names>C</given-names></name><name><surname>Robson</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>The contrast sensitivity of retinal ganglion cells of the cat</article-title><source>The Journal of Physiology</source><volume>187</volume><fpage>517</fpage><lpage>552</lpage></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franzius</surname><given-names>M</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Wiskott</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Slowness and sparseness lead to place, head-direction, and spatial-view cells</article-title><source>PLoS Computational Biology</source><volume>3</volume><elocation-id>e166</elocation-id><lpage>1622</lpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.0030166</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freund</surname><given-names>TF</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Interneurons of the hippocampus</article-title><source>Hippocampus</source><volume>6</volume><fpage>347</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1996)6:4&amp;amp;lt;347::AID-HIPO1&amp;amp;gt;3.0.CO;2-I</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giocomo</surname><given-names>LM</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Computational models of grid cells</article-title><source>Neuron</source><volume>71</volume><fpage>589</fpage><lpage>603</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.07.023</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gray</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Toeplitz and circulant matrices: A review</article-title><source>Now Publishers Inc</source><volume>2</volume><fpage>155</fpage><lpage>239</lpage></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/nature03721</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hornik</surname><given-names>K</given-names></name><name><surname>Kuan</surname><given-names>C-M</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Convergence analysis of local feature extraction algorithms</article-title><source>Neural Networks</source><volume>5</volume><fpage>229</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(05)80022-X</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jolliffe</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Principal component analysis</article-title><source>Wiley Online Library</source><pub-id pub-id-type="doi">10.1002/0470013192.bsa501</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kjelstrup</surname><given-names>KB</given-names></name><name><surname>Solstad</surname><given-names>T</given-names></name><name><surname>Brun</surname><given-names>VH</given-names></name><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Finite scale of spatial representation in the hippocampus</article-title><source>Science</source><volume>321</volume><fpage>140</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1126/science.1157086</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kropff</surname><given-names>E</given-names></name><name><surname>Treves</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The emergence of grid cells: Intelligent design or just adaptation?</article-title><source>Hippocampus</source><volume>18</volume><fpage>1256</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1002/hipo.20520</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kushner</surname><given-names>HJ</given-names></name><name><surname>Clark</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="1978">1978</year><chapter-title>Stochastic Approximation Methods for Constrained and Unconstrained Systems</chapter-title><source><italic>Applied Mathematical Sciences, Vol. 26</italic></source><publisher-loc>New York, NY</publisher-loc><publisher-name>Springer New York</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4684-9352-8</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langston</surname><given-names>RF</given-names></name><name><surname>Ainge</surname><given-names>JA</given-names></name><name><surname>Couey</surname><given-names>JJ</given-names></name><name><surname>Canto</surname><given-names>CB</given-names></name><name><surname>Bjerknes</surname><given-names>TL</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Development of the spatial representation system in the rat</article-title><source>Science</source><volume>328</volume><fpage>1576</fpage><lpage>1580</lpage><pub-id pub-id-type="doi">10.1126/science.1188210</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Stemmler</surname><given-names>MB</given-names></name><name><surname>Herz</surname><given-names>AVM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probable nature of higher-dimensional symmetries underlying mammalian grid-cell activity patterns</article-title><source>eLife</source><volume>4</volume><elocation-id>e05979</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05979</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mittelstaedt</surname><given-names>M-L</given-names></name><name><surname>Mittelstaedt</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Homing by path integration in a mammal</article-title><source>Naturwissenschaften</source><volume>67</volume><fpage>566</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1007/BF00450672</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Montanari</surname><given-names>A</given-names></name><name><surname>Richard</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Non-negative principal component analysis: Message passing algorithms and sharp asymptotics</article-title><uri xlink:href="http://arxiv.org/abs/1406.4775">http://arxiv.org/abs/1406.4775</uri></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The mantle of the heavens: Reflections on the 2014 Nobel Prize for medicine or physiology</article-title><source>Hippocampus</source><volume>25</volume><pub-id pub-id-type="doi">10.1002/hipo.22455</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage></element-citation></ref><ref id="bib29"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>O'Keefe</surname><given-names>J</given-names></name><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1978">1978</year><chapter-title>The hippocampus as a cognitive map</chapter-title><publisher-loc>Oxford</publisher-loc><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oja</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>A simplified neuron model as a principal component analyzer</article-title><source>Journal of Mathematical Biology</source><volume>15</volume><fpage>267</fpage><lpage>273</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanger</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Optimal unsupervised learning in a single-layer linear feedforward neural network</article-title><source>Neural Networks</source><volume>2</volume><fpage>459</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1016/0893-6080(89)90044-0</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sargolini</surname><given-names>F</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Conjunctive representation of position, direction, and velocity in entorhinal cortex</article-title><source>Science</source><volume>312</volume><fpage>758</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1126/science.1125572</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savelli</surname><given-names>F</given-names></name><name><surname>Yoganarasimha</surname><given-names>D</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Influence of boundary removal on the spatial representations of the medial entorhinal cortex</article-title><source>Hippocampus</source><volume>18</volume><fpage>1270</fpage><lpage>1282</lpage><pub-id pub-id-type="doi">10.1002/hipo.20511</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Si</surname><given-names>B</given-names></name><name><surname>Treves</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A model for the differentiation between grid and conjunctive units in medial entorhinal cortex</article-title><source>Hippocampus</source><volume>23</volume><fpage>1410</fpage><lpage>1424</lpage><pub-id pub-id-type="doi">10.1002/hipo.22194</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solstad</surname><given-names>T</given-names></name><name><surname>Boccara</surname><given-names>CN</given-names></name><name><surname>Kropff</surname><given-names>E</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representation of geometric borders in the entorhinal cortex</article-title><source>Science</source><volume>322</volume><fpage>1865</fpage><lpage>1868</lpage><pub-id pub-id-type="doi">10.1126/science.1166466</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Design Principles of the Hippocampal Cognitive Map</article-title><source>NIPS Proceedings</source><fpage>2528</fpage><lpage>2536</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stensola</surname><given-names>H</given-names></name><name><surname>Stensola</surname><given-names>T</given-names></name><name><surname>Solstad</surname><given-names>T</given-names></name><name><surname>Frøland</surname><given-names>K</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The entorhinal grid map is discretized</article-title><source>Nature</source><volume>492</volume><fpage>72</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1038/nature11649</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stensola</surname><given-names>T</given-names></name><name><surname>Stensola</surname><given-names>H</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Shearing-induced asymmetry in entorhinal grid cells</article-title><source>Nature</source><volume>518</volume><fpage>207</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1038/nature14151</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stepanyuk</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Self-organization of grid fields under supervision of place cells in a neuron model with associative plasticity</article-title><source>Biologically Inspired Cognitive Architectures</source><volume>13</volume><fpage>48</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/j.bica.2015.06.006</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tocker</surname><given-names>G</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Derdikman</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Grid cells correlation structure suggests organized feedforward projections into superficial layers of the medial entorhinal cortex</article-title><source>Hippocampus</source><volume>25</volume><fpage>1599</fpage><lpage>1613</lpage><pub-id pub-id-type="doi">10.1002/hipo.22481</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>X-X</given-names></name><name><surname>Prentice</surname><given-names>J</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The sense of place: grid cells in the brain and the transcendental number e</article-title><uri xlink:href="http://arxiv.org/abs/1304.0031">http://arxiv.org/abs/1304.0031</uri></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weingessel</surname><given-names>A</given-names></name><name><surname>Hornik</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Local PCA algorithms</article-title><source>Neural Networks, IEEE Transactions</source><volume>11</volume><fpage>1242</fpage><lpage>1250</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiesel</surname><given-names>TN</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Effects of visual deprivation on morphology and physiology of cells in the cat's lateral geniculate body</article-title><source>Journal of Neurophysiology</source><volume>26</volume><fpage>978</fpage><lpage>993</lpage></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wills</surname><given-names>TJ</given-names></name><name><surname>Cacucci</surname><given-names>F</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>O'Keefe</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Development of the hippocampal cognitive map in preweanling rats</article-title><source>Science</source><volume>328</volume><fpage>1573</fpage><lpage>1576</lpage><pub-id pub-id-type="doi">10.1126/science.1188224</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Amaral</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2004">2004</year><chapter-title>Hippocampal Formation</chapter-title><person-group person-group-type="editor"><name><surname>Paxinos</surname> <given-names>G</given-names></name></person-group><source><italic>The Rat Nervous System</italic></source><edition>3rd ed</edition><publisher-loc>San Diego, CA</publisher-loc><publisher-name>Elsevier</publisher-name><fpage>635</fpage><lpage>704</lpage><pub-id pub-id-type="doi">10.1016/B978-012547638-6/50022-5</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zass</surname><given-names>R</given-names></name><name><surname>Shashua</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Nonnegative Sparse PCA</article-title><source>NIPS Proceedings</source><fpage>1561</fpage><lpage>1568</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zilli</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Models of grid cell spatial firing published 2005-2011</article-title><source>Frontiers in Neural Circuits</source><volume>6</volume><pub-id pub-id-type="doi">10.3389/fncir.2012.00016</pub-id></element-citation></ref></ref-list><app-group><app id="app1"><title>Appendix</title><boxed-text><sec id="s36" sec-type="appendix"><title>Movement schema of agent and environment data</title><p>The relevant simulation data used:</p><p><table-wrap id="tblu1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th valign="top">Size of arena 10X10</th><th valign="top">Place cells field width: 0.75</th><th valign="top">Place cells distribution: <break/>uniform</th></tr></thead><tbody><tr><td valign="top">Velocity: 0.25 (linear), <break/>0.1-6.3 (angular)</td><td valign="top"># Place cells: 625</td><td valign="top">Learning rate: 1/(t+1e5)</td></tr></tbody></table></table-wrap></p><p>The agent was moved around the virtual environment according to:<disp-formula id="equ50"><mml:math id="m50"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>modulo</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>ω</mml:mi><mml:mo>⋅</mml:mo><mml:mi>Z</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>ν</mml:mi><mml:mo>⋅</mml:mo><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>ν</mml:mi><mml:mo>⋅</mml:mo><mml:mi>sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf308"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the current direction angle, <inline-formula><mml:math id="inf309"><mml:mi>ω</mml:mi></mml:math></inline-formula> is the angular velocity, <inline-formula><mml:math id="inf310"><mml:mi>Z</mml:mi><mml:mo>-</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf311"><mml:mi>N</mml:mi></mml:math></inline-formula> is the standard normal Gaussian distribution, <inline-formula><mml:math id="inf312"><mml:mi>ν</mml:mi></mml:math></inline-formula> is the linear velocity, and <inline-formula><mml:math id="inf313"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the current position of the agent. Edges were treated as periodic – when agent arrived to one side of box it was teleported to the other side.</p></sec><sec id="s37" sec-type="appendix"><title>Positive-negative disks</title><p>Positive-negative disks are used with the following activity rules:<disp-formula id="equ51"><mml:math id="m51"><mml:mo/><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:msubsup><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:msqrt></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf314"><mml:msub><mml:mi>c</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> are the centers of the disks, and <inline-formula><mml:math id="inf315"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are the radii of the inner circle and the outer ring, respectively. The constant value in the negative ring was chosen to yield zero integral over the disk.</p></sec><sec id="s38" sec-type="appendix"><title>Fourier transform of the difference of Gaussians function</title><p>Here we prove that if we are given a difference of Gaussians function,<disp-formula id="equ52"><mml:math id="m52"><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi><mml:mo mathvariant="normal">·</mml:mo><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>in which the appropriate normalization constants for a bounded box are</p><p><inline-formula><mml:math id="inf316"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mrow><mml:mi>exp</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>,</p><p>with <inline-formula><mml:math id="inf317"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msubsup><mml:mrow><mml:mi>exp</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> being the cumulative normal distribution, then <inline-formula><mml:math id="inf318"><mml:mo>∀</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo>∈</mml:mo><mml:mover><mml:mi>S</mml:mi><mml:mo mathvariant="italic">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mfenced close="}" open="{"><mml:mfenced><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:mfrac></mml:mrow></mml:mfenced></mml:mfenced><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo mathvariant="italic">,</mml:mo><mml:mo mathvariant="italic">…</mml:mo><mml:mo mathvariant="italic">,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="normal">ℤ</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula>, we have<disp-formula id="equ53"><mml:math id="m53"><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo><mml:mo>≜</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mfenced close="|" open="|"><mml:mi>S</mml:mi></mml:mfenced></mml:mfrac><mml:msub><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:msub><mml:mo> </mml:mo><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>)</mml:mo><mml:mo> </mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>·</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:mi>d</mml:mi><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mfenced close="|" open="|"><mml:mi>S</mml:mi></mml:mfenced></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:mtext>(</mml:mtext><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mtext>)</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo> </mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo mathvariant="bold">·</mml:mo><mml:mtext mathvariant="bold">k</mml:mtext><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Note that the normalization constants <inline-formula><mml:math id="inf319"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> vanish after the Fourier transform, and that in the limit <inline-formula><mml:math id="inf320"><mml:mrow><mml:mi>L</mml:mi><mml:mo>≫</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> we obtain the standard result for the Fourier transform of an unbounded Gaussian distribution.</p><p>Proof: For simplicity of notation, we assume <inline-formula><mml:math id="inf321"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. However, the calculation is identical for any <inline-formula><mml:math id="inf322"><mml:mi>D</mml:mi></mml:math></inline-formula>.<disp-formula id="equ54"><mml:math id="m54"><mml:mstyle><mml:mtable columnalign="left left left"><mml:mtr><mml:mtd><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:msub><mml:mo>∫</mml:mo><mml:mi>S</mml:mi></mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi mathvariant="bold">k</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:mfenced close="]" open="["><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd/><mml:mtd><mml:mo> </mml:mo><mml:mo>·</mml:mo><mml:mo> </mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:msup><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>L</mml:mi></mml:msubsup><mml:mi>d</mml:mi><mml:mi>z</mml:mi><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:msup><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>L</mml:mi></mml:msubsup><mml:mi>d</mml:mi><mml:mi>z</mml:mi><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:math></disp-formula></p><p>To solve this integral, we define<disp-formula id="equ55"><mml:math id="m55"><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>L</mml:mi></mml:msubsup><mml:mo> </mml:mo><mml:mi>d</mml:mi><mml:mi>z</mml:mi><mml:mo> </mml:mo><mml:mi>exp</mml:mi><mml:mfenced><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo> </mml:mo><mml:mi>cos</mml:mi><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Its derivative is<disp-formula id="equ56"><mml:math id="m56"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:msub><mml:msup><mml:mi>I</mml:mi><mml:mo>'</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>L</mml:mi></mml:msubsup><mml:mi>d</mml:mi><mml:mi>z</mml:mi><mml:mi>z</mml:mi><mml:mo> </mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mtext>sin</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mtext>sin</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo mathsize="15px">|</mml:mo></mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>L</mml:mi></mml:msubsup><mml:mi>d</mml:mi><mml:mi>z</mml:mi><mml:mo> </mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where in the last equality we used the fact that <inline-formula><mml:math id="inf323"><mml:mrow><mml:mi>a</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf324"><mml:mi>n</mml:mi></mml:math></inline-formula> is an integer) if <inline-formula><mml:math id="inf325"><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. Solving this differential equation, we obtain<disp-formula id="equ57"><mml:math id="m57"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ58"><mml:math id="m58"><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>L</mml:mi></mml:msubsup><mml:mi>d</mml:mi><mml:mi>z</mml:mi><mml:mo> </mml:mo><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:msqrt><mml:mo> </mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mi>L</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:msqrt><mml:mo> </mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mi>L</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>substituting this into our last expression for <inline-formula><mml:math id="inf326"><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> we obtain<disp-formula id="equ59"><mml:math id="m59"><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mi>k</mml:mi><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>which is what we wanted to prove.</p></sec><sec id="s39" sec-type="appendix"><title>Why the 'unconstrained' <inline-formula><mml:math id="inf327"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is a lower bound on 'constrained' <inline-formula><mml:math id="inf328"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula></title><p>In section 'The PCA solution with a non-negative constraint – 1D Solutions', we mention that the main difference between the Equations (<xref ref-type="disp-formula" rid="equ37 equ38">Equations 33 and 34</xref>) is the base frequency, <inline-formula><mml:math id="inf329"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, which is slightly different from <inline-formula><mml:math id="inf330"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>Here we explain how the relation between <inline-formula><mml:math id="inf331"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf332"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> depends on the <inline-formula><mml:math id="inf333"><mml:mi>k</mml:mi></mml:math></inline-formula>-lattice discretization, as well as the properties of <inline-formula><mml:math id="inf334"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The discretization effect is similar to the unconstrained case, and can cause a difference of at most <inline-formula><mml:math id="inf335"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> between <inline-formula><mml:math id="inf336"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf337"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>. However, even if <inline-formula><mml:math id="inf338"><mml:mrow><mml:mi>L</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>, we can expect <inline-formula><mml:math id="inf339"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> to be slightly smaller than <inline-formula><mml:math id="inf340"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>. To see that, suppose we have a solution as described above, with <inline-formula><mml:math id="inf341"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf342"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is a small perturbation (which does not affect the non-negativity constraint). We write the perturbed <inline-formula><mml:math id="inf343"><mml:mi>k</mml:mi></mml:math></inline-formula>-space objective of this solution<disp-formula id="equ60"><mml:math id="m60"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf344"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is the peak of <inline-formula><mml:math id="inf345"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, if <inline-formula><mml:math id="inf346"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is monotonically decreasing for <inline-formula><mml:math id="inf347"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> (as is the case for the difference of Gaussians function), then any positive perturbation <inline-formula><mml:math id="inf348"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> would decrease the objective.</p><p>However, a sufficiently small negative perturbation <inline-formula><mml:math id="inf349"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> would improve the objective. We can see this from the Taylor expansion of the objective,<disp-formula id="equ61"><mml:math id="m61"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:msub><mml:mi>m</mml:mi><mml:mi>δ</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In which the derivative <inline-formula><mml:math id="inf350"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is zero for <inline-formula><mml:math id="inf351"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (since <inline-formula><mml:math id="inf352"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is the peak of <inline-formula><mml:math id="inf353"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) and negative for <inline-formula><mml:math id="inf354"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, if <inline-formula><mml:math id="inf355"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is monotonically decreasing for <inline-formula><mml:math id="inf356"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>. If we gradually increase the magnitude of this negative perturbation <inline-formula><mml:math id="inf357"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, at some point the objective will stop increasing and start decreasing, because, for the difference of Gaussians function, <inline-formula><mml:math id="inf358"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> increases more sharply for <inline-formula><mml:math id="inf359"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> than it decreases for <inline-formula><mml:math id="inf360"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>. We can thus treat the 'unconstrained' <inline-formula><mml:math id="inf361"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> as an upper bound for the 'constrained' <inline-formula><mml:math id="inf362"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, if we ignore discretization effects (<italic>i.e.</italic>, the limit <inline-formula><mml:math id="inf363"><mml:mrow><mml:mi>L</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s40" sec-type="appendix"><title>Upper bound on the constrained objective – 2D square lattice</title><p>In the Methods section 'The PCA solution with a non-negative constraint – 2D Solutions' we examine different 2D lattice-based solutions, including a square lattice, which is a solution of the following constrained minimization problem<disp-formula id="equ62"><mml:math id="m62"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>max</mml:mi><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup></mml:msub><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>s</mml:mtext><mml:mtext>.t</mml:mtext><mml:mtext>.:</mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mrow><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mrow/></mml:msubsup><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Here we prove that the objective is bounded above by <inline-formula><mml:math id="inf364"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>, <italic>i.e.</italic>, <inline-formula><mml:math id="inf365"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>≤</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p><p>Without loss of generality we assume <inline-formula><mml:math id="inf366"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (we can always shift and scale <inline-formula><mml:math id="inf367"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula>). We denote<disp-formula id="equ63"><mml:math id="m63"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow/></mml:msubsup><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow/></mml:msubsup><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ64"><mml:math id="m64"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mrow/></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We examine the domain <inline-formula><mml:math id="inf368"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. We denote by <inline-formula><mml:math id="inf369"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mo>±</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> the regions in which <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is positive or negative, respectively. Note that<disp-formula id="equ65"><mml:math id="m65"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>mod</mml:mi><mml:mo> </mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>mod</mml:mi><mml:mo> </mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>so both regions have the same area, and<disp-formula id="equ66"><label>(A1)</label><mml:math id="m66"><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:munder><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:munder><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>From the non-negativity constraint (2), we must have<disp-formula id="equ67"><label>(A2)</label><mml:math id="m67"><mml:mo>∀</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Also, note that <inline-formula><mml:math id="inf371"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf372"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> do not share any common Fourier (cosine) components. Therefore, they are orthogonal, with the inner product being the integral of their product on the region <inline-formula><mml:math id="inf373"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula></p><p><inline-formula><mml:math id="inf374"><mml:mstyle><mml:mtable columnalign="left left left"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:munder></mml:mstyle><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:munder></mml:mstyle><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:msup><mml:mi>C</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mrow/></mml:msup></mml:munder></mml:mstyle><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≤</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:munder></mml:mstyle><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:munder></mml:mstyle><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:math></inline-formula>,</p><p>where in the last line we used the bound from <xref ref-type="disp-formula" rid="equ67">Equation (A2)</xref>, and then <xref ref-type="disp-formula" rid="equ66">Equation (A1)</xref>. Using this result, together with Cauchy-Schwartz inequality, we have<disp-formula id="equ68"><mml:math id="m68"><mml:mstyle><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:munder><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>≤</mml:mo><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:munder><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>≤</mml:mo><mml:msqrt><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:munder><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:munder><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:msqrt><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula></p><p>So,<disp-formula id="equ69"><mml:math id="m69"><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:munder><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>≤</mml:mo><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:munder><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>Summing this equation with <xref ref-type="disp-formula" rid="equ67">Equation (A2),</xref> squared and integrated over the region <inline-formula><mml:math id="inf375"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, and dividing by <inline-formula><mml:math id="inf376"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, we obtain<disp-formula id="equ70"><mml:math id="m70"><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:munder><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>≤</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:munder><mml:mo>∫</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:munder><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>using the orthogonality of the Fourier components (<italic>i.e.</italic>, Parseval’s theorem) to perform the integrals over <inline-formula><mml:math id="inf377"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf378"><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, we get<disp-formula id="equ71"><mml:math id="m71"><mml:mn>2</mml:mn><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>≤</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mover><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Lastly, plugging in the normalization constraint <inline-formula><mml:math id="inf379"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, we find<disp-formula id="equ72"><mml:math id="m72"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>≥</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>as required.</p></sec></boxed-text></app></app-group></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.10094.027</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing editor</role><aff id="aff6"><institution>Brown University</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your work entitled &quot;Extracting grid characteristics from spatially distributed place cell inputs using non-negative PCA&quot; for peer review at <italic>eLife</italic>. Your submission has been favorably evaluated by Timothy Behrens (Senior editor), Reviewing editor Michael Frank, and two reviewers.</p><p>The reviewers have discussed the reviews with one another and the Reviewing editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The authors present analytical and neural network simulations which demonstrate that, under certain constraints, the first principal component of place cell firing patterns in a 2D arena resemble hexagonal grid cell firing patterns. Specifically, these constraints require that the principal components be constrained to non-negativity, and that the place cell inputs have a zero mean in time or space. In addition, they suggest that interesting features of grid cell firing, namely the tendency for grid scale to be discretely clustered, and for the grid to align at approximately 7 degrees to the walls of a square arena. This is an important result which adds to a growing body of recent studies that have emphasised the importance of the projection from place cells to grid cells, as opposed to the grid cell to place cell projection that has been the subject of several previous theoretical studies.</p><p>As you will see however, the Reviewers felt that it was important to provide some intuition and cogent explanation about how the results were obtained, and several questions arose along these lines. It is not clear how the dual constraints of (eigenvector) non-negativity and orthogonality are actually satisfied, nor is it clear why they get 7degree alignment or a ratio of 1.4 for the jump in grid scale.</p><p>In addition to the revisions articulated below, the following suggestion arose from discussion amongst the Reviewers/editors.</p><p><xref ref-type="fig" rid="fig13">Figure 13</xref> of the manuscript tracks the evolution of the network from different initial conditions.</p><p>One might go some way in clarifying the questions of how and why the observed phenomena occur by tracking individual objective measures, in a manner similar to <xref ref-type="fig" rid="fig13">Figure 13</xref>.</p><p>To wit, one of the algorithms you use trades off two objectives:</p><p>(i) maximize the variance of the projection of the data onto the [non-negative] principal component</p><p>(ii) enforce approximate orthogonormality by minimizing || I – e<sup>T</sup> e||, where I is the identity matrix, and e is the normalized &quot;eigenvector,&quot; subject to the non-negativity constraint.</p><p>In addition, one ought to quantify the degree of non-negativity. There are several ways one might go about this, but one approach would be to normalize the eigenvector to unit length, then take the norm of the vector composed of only the positive entries and subtract the norm of the vector of negative entries.</p><p>Essential revisions:</p><p>1) This manuscript is of sufficient general interest for publication in <italic>eLife</italic>, although there is a general lack of detail when describing the methodology. There is also significant overlap with a recent paper at NIPS 2014 (Stachenfeld et al., 2014). I find both papers (Dordek et al., submitted, and Stachenfeld et al.) extremely interesting, and they both have different focusses. But I do think that there needs to be some link to/discussion of Stachenfeld et al.</p><p>2) Perhaps the most important issue is that they provide little intuition into the interesting subsidiary results. There is a section on &quot;Why Hexagons?&quot; but none on why other aspects of the results occur. This becomes more important given the fact that Stachenfeld also see the basic result. Why do grid scales have a ratio of 1.4 (indeed is this figure reliable)? Why do they see a non-zero alignment of grids to borders (and is this figure reliable)? Alignment angle appears to depend on place field size, but how does overall place cell firing coverage (and covariance) vary with place field size? How does the alignment angle covary with gridness (the very nice grid-like patterns they show tend to look aligned)?</p><p>3) &quot;Due to the isotropic nature of the data (generated by the agent's motion), there was a 2D redundancy in the X-Y plane in conjunction with a dual phase redundancy in 1D, resulting in a fourfold total redundancy of every solution&quot; – this statement requires unpacking. Presumably the dual phase redundancy is caused by the periodic boundary conditions? What are the implications for non-periodic boundary conditions? This sentence needs to be explained to the non-expert reader.</p><p>4) Similarly, <xref ref-type="fig" rid="fig4">Figure 4A</xref> is cryptic – how does the figure show fourfold redundancy? The authors also use a reduced number of place cells to obtain this result (225 vs. 625 in other simulations) – what are the reasons for this change? This should be explained and justified in the text.</p><p>5) More generally, when changing place field size, what happens to the number of place cells – presumably the density of firing overlap (covariance) needs to be maintained as this will strongly affect the results?</p><p>6) It is not clear how the non-negative weights are implemented alongside zero mean inputs, and we could not gain any additional insight from the Methods section. This issue needs to be elaborated upon in the text, otherwise it seems implicitly contradictory. Does it imply that the output GC can have negative firing rates? i.e. are there positive and negative place cell firing rates (with a mean of zero), coupled with positive place cell to grid cell synaptic weights, to give both negative and positive inputs to the output grid cell? Presumably the time step to time step changes in grid cell firing rate between negative and positive values will have a large impact on the direction of synaptic weight change – is this the case?</p><p>7) <xref ref-type="fig" rid="fig7">Figure 7C, D</xref>; also 14B – the analytical PCA results for constrained weights appear to exhibit a bimodal distribution of 90 degree gridness scores – what in the data accounts for this? Can the authors provide any comment or insight?</p><p>&quot;The dependency was almost linear&quot; – presumably there is a mathematical reason for this, based on the relationship between the covariance matrix and eigenvectors? Can the authors provide any comment or insight into this relationship?</p><p>8) The methods are generally lacking in detail that would allow these simulations to be replicated, based on details provided in the manuscript (see specific details below). The authors should ensure that sufficient detail to allow replication is incorporated. They may also like to consider uploading their code to ModelDB, or a similar online repository, for the sake of transparency and to allow independent replication.</p><p>9) The representation of space lies on a torus in this model, which means that the eigenvectors should be periodic and the spatial frequencies quantized. We fail to grasp why, in <xref ref-type="fig" rid="fig3">Figure 3</xref>, the first spatial frequency should be 2 and not 1 (i.e., we have two peaks in each direction), and why the eigenvectors 3B is periodic, but in 3A they are not.</p><p>10) The eigenvectors of a correlation matrix are orthogonal. If we normalize these eigenvectors and furthermore enforce an additional constraint that the entries of each eigenvector must be positive semidefinite, then the &quot;synaptic weights&quot; of different eigenvectors should not overlap. In other words, if an entry in the first eigenvector is non-zero, it ought to be zero in all other eigenvectors.</p><p>11) While the algorithms the authors use relax this last constraint, it is not clear how skewing or shearing a square pattern that results from PCA helps satisfy the dual constraints of non-negativity and orthogonality.</p><p>This is a key point in the paper, and begs for an intuitive explanation, or at least more analysis or quantification of the degree to which the constraints are satisfied.</p><p>12) The finding that the spacings corresponding to successive eigenvectors obey ~ <inline-formula><mml:math id="inf380"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> also calls for an explanation. Why should this ratio of spacings best satisfy non-negativity, orthogonality, and periodicity?</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Extracting grid characteristics from spatially distributed place cell inputs using non-negative PCA&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Timothy Behrens (Senior editor), Reviewing editor Michael Frank, and two reviewers, one of whom is Neil Burgess. The manuscript has been improved but both reviewers have identified some remaining issues that need to be addressed before acceptance, as articulated in the comments below:</p><p><italic>Reviewer #1:</italic> </p><p>The authors have worked hard to provide further intuition and interpretation to their results, with much success, although there are still some points that should be clarified, see comments below.</p><p>There is also a plain error that should be removed. The authors state &quot;the distribution of grid orientations becomes uniform in the range of 0-15 degrees, with a mean (and median) at 7.5 degrees, similar to experimental data [Stensola et al., 2015]&quot;. On the contrary the experimental data shows a very non-uniform distribution clearly peaked at about 7.5 degrees, which is quite different. Also, as a circular variable limited to [0,15) – albeit one for which 16 degrees corresponds to 14 degrees rather than 1 degree – the 'mean' and 'median' they refer to are undefined if the distribution is uniform. Thus the authors should acknowledge that they do not provide an explanation for this aspect of the experimental data, and some of the text/ results on this point could be removed, most importantly in the Abstract.</p><p>At the start of describing the main result (Results, first paragraph) is not clear that the &quot;1d mapping of the 2d activity&quot; is caused by the trajectory of the rat moving through the place fields.</p><p>The next part of the description of their PCA analysis as Eigenvectors and their spatial interpretation could also be improved in terms of intuition. It would be worth mentioning that the PCA, in addition to being calculated as the Eigenvectors of the matrix of temporal covariance between inputs, can also be thought of in terms of singular value decomposition of the (appropriately normalised) input activity. I.e. the input to the network (temporal sequences of place cell activity) can be decomposed into an outer product of (spatial) weights over place cell inputs (their Eigenvectors) and temporally varying weights over these Eigenvectors. The network learns the spatial weights over place cells (the Eigenvectors) as the connections weights from the place cells, and &quot;projection onto place cell space&quot; is simply the firing rates of the output neuron plotted against the location of the rat.</p><p>In the second paragraph of the subsection “Adding a non-Negativity constraint to the PCA “it is important to remind the reader that the &quot;raw place cells activity&quot; is not the final input to the PCA numerical methods – spatial or temporal mean normalization has to happen first.</p><p>It would be useful to have a surface plot showing Gridness vs. PC Density vs. PC Width. I have found it hard to replicate the Gridness distribution and wonder how general it is to the precise place cell inputs used? The reference to <xref ref-type="fig" rid="fig10">Figure 10A</xref> in the third paragraph of the subsection “Effect of place cell parameters on grid structure “is incorrect (I think) – not showing the dependence on the width of the place fields.</p><p>Regarding the presence of &quot;modules&quot; of grid cells (subsection “Modules of grid cells”), it is not clear if the interpretation is that different modules correspond to different grid scales, or different Eigenvectors (even if having the same scale). Assuming the latter, it seems that the analysis predicts that there should only be two modules, with the scale of the first module approximately 7.5x the largest place field inputs, and the second module (~<inline-formula><mml:math id="inf381"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> smaller) existing only to enable non-negative PCA solutions. This deserves discussion, given that the data show at least 5 modules, with a range of scales, the smallest and most numerous having approximately the scale of the smaller place fields found in the dorsal hippocampus (25-30 cm).</p><p><italic>Reviewer #2:</italic> </p><p>The new material, starting with the subsection “Steady state analysis “is great, and just the thing I was asking for, namely an understanding as to <italic>why</italic> hexagonal patterns result under a non-negative PCA learning rule.</p><p>There are a few things I'd like to understand better, though:</p><p>The last equation in the section “2D Solutions”: should read <italic>ϕ<sub>1</sub> = ϕ<sub>2</sub> = ϕ<sub>3</sub></italic>n 2 π/3, n ϵ Z. e.g. <italic>ϕ<sub>1</sub> = ϕ<sub>2</sub> = ϕ<sub>3</sub></italic>= 0 would be a solution.</p><p>As written, <italic>ϕ<sub>1</sub> = ϕ<sub>2</sub> = ϕ<sub>3</sub></italic>, is not the maximum. Besides, this more general case would allow solutions that are unlike the ones observed in experiments.</p><p>This brings me to</p><p>In the subsection “2D solutions”:</p><p>&quot;To do this [maximize the Fourier components inside the basis set,] we must maximize the minimal value of the cosine components for the basis set&quot;</p><p>I am afraid this argument escapes me, even though its role is crucial.</p><p>While I think I understand the Fourier domain argument up to this point in the text, how exactly does maximizing the minimum value in real space of the function guarantee that the Fourier components in Eq. 24 are maximized?</p><p>Eq. 34: why is the upper limit here ∞, when it was <italic>M</italic> in Eq. 33?</p><p>I'd like some clarification from the authors before I can make a judgment call as to whether the argument is likely to be correct.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.10094.028</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>As you will see however, the Reviewers felt that it was important to provide some intuition and cogent explanation about how the results were obtained, and several questions arose along these lines. It is not clear how the dual constraints of (eigenvector) non-negativity and orthogonality are actually satisfied, nor is it clear why they get 7degree alignment or a ratio of 1.4 for the jump in grid scale.</italic> Re-analysis demonstrates that we tend to get values which are on average a little lower than 7 degrees (Author response <xref ref-type="fig" rid="fig3">images 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>). <underline>However</underline>, the range of possible values is from 0 to 15 degrees, and we demonstrate analytically (in the new extended Methods section) that in the case of a very large box, the distribution of orientation angles becomes uniform in the interval 0-15, with a mean at 7.5 degrees, consistent with experimental data.</p><p>The jump of 1.4 results from the change in the circular components of the solution in Fourier space (See especially new <xref ref-type="fig" rid="fig17">Figure 17</xref> in Methods section of paper). From running more simulations and also from analytical considerations, we believe that within our theoretical framework there is only one jump, and thus this model can really only explain 2 modules and not more (see analytical derivation for more intuition on this point).</p><p>In addition to the new extended Methods section, we have updated the Abstract and Results (throughout) to account for these changes.</p><p><italic>In addition to the revisions articulated below, the following suggestion arose from discussion amongst the Reviewers/editors. […]</italic></p><p> <italic>In addition, one ought to quantify the degree of non-negativity. There are several ways one might go about this, but one approach would be to normalize the eigenvector to unit length, then take the norm of the vector composed of only the positive entries and subtract the norm of the vector of negative entries.</italic> </p><p>We thank the editors and reviewers for raising this essential issue. In our understanding, the main issue is to comprehend to what extent we can combine maximal -variance, orthogonality, and positivity together, when looking at multiple outputs (such as in old <xref ref-type="fig" rid="fig11">Figures 11</xref> and <xref ref-type="fig" rid="fig12">12</xref>, or new <xref ref-type="fig" rid="fig13">Figures 13</xref> and <xref ref-type="fig" rid="fig14">14</xref>). As we demonstrate now, the outputs from the method are not orthogonal (<xref ref-type="fig" rid="fig18">Author response image 1</xref>), and this we believe solves the conundrum. We now added a short comment about this into the Results section: “It is important to note though that, due to the non-negativity constraint, the vectors achieved in this way were not orthogonal, and thus it cannot be considered a real orthogonalization process, although as explained in the Methods section, the process does aim for maximum difference between the vectors”<fig id="fig18" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.023</object-id><label>Author response image 1.</label><caption><title>Demonstrates that output vectors are not orthogonal.</title><p>Matrix of cosine values between every two vectors.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.023">http://dx.doi.org/10.7554/eLife.10094.023</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-resp-fig1-v2.tif"/></fig></p><p> <italic>Essential revisions: 1) This manuscript is of sufficient general interest for publication in eLife, although there is a general lack of detail when describing the methodology.</italic> </p><p>Additional detail has been added to the Methods. Furthermore, extensive sections have been added to the Methods in order to provide an analytical motivation to the whole paper (from the subsection “Steady state analysis”). See detailed points below in the section related to Methods. Furthermore, we have uploaded our MATLAB code to a public server, as described below.</p><p><italic>There is also significant overlap with a recent paper at NIPS 2014 (Stachenfeld et al., 2014). I find both papers (Dordek et al., submitted, and Stachenfeld et al) extremely interesting, and they both have different focusses. But I do think that there needs to be some link to/discussion of Stachenfeld et al.</italic> </p><p>Stachnfeld et al. arrive to similar ideas in a very nice paper based on a reinforcement learning model of place cells. Unlike our paper, their paper does not contain the positivity constraint, and thus the “grid cells” that results from the PCA procedure they use are square rather than hexagonal. Furthermore, they did not provide a theoretical analysis of the nature provided in the revision. We now discuss their paper in the Discussion, as follows: “We note that similar work has noticed the relation between place-cell-to-grid-cell transformation and PCA.[…] However, due to the unconstrained nature of their transformation, the outputted grid cells were square-like.”</p><p><italic>2) Perhaps the most important issue is that they provide little intuition into the interesting subsidiary results. There is a section on &quot;Why Hexagons?&quot; but none on why other aspects of the results occur. This becomes more important given the fact that Stachenfeld also see the basic result. Why do grid scales have a ratio of 1.4 (indeed is this figure reliable)?</italic> </p><p>Following the new analytical derivation in the Methods section (from the subsection “Steady state analysis”), we can now explain the jump of scale as a hop on the Fourier-space lattice (new <xref ref-type="fig" rid="fig17">Figure 17</xref> in paper).</p><p>We have now re-run the multiple solutions in order to demonstrate a jump between two consecutive scales. However, we find that it is harder to see the jump to the next smaller third scale, and thus we conclude that the method only partially accounts for the effect of modules. This is now noted in the Results as follows: “[…]we found that the ratio between the distances of the modules was ~1.4, close to the value of 1.42 found by Stensola et al. [Stensola et al., 2012]. Although we searched for additional such jumps, we could only identify this single jump, suggesting that our model can yield up to two “modules” and not more.”</p><p><italic>Why do they see a non-zero alignment of grids to borders (and is this figure reliable)?</italic> </p><p>As we now show in our analytical derivation, the non-zero alignment can arise from the relation between the lattice points in the Fourier domain and the optimal-solution circle. This is now explainedthoroughly in the new sections added to the Methods from the subsection “Steady state analysis” and in <xref ref-type="fig" rid="fig15">Figure 15</xref>–<xref ref-type="fig" rid="fig16">16</xref> in paper.</p><p><italic>Alignment angle appears to depend on place field size, but how does overall place cell firing coverage (and covariance) vary with place field size?</italic> </p><p>In our previous submission we manipulated the place field size while keeping the size of the arena and number of place cells constant. Thus the coverage has increased as a function of the increase in size. Here (<xref ref-type="fig" rid="fig19">Author response image 2</xref>) we show an example of increasing both the place field size and the size of the arena, while keeping the place-cell-size/arena size constant, such that the total coverage increases (a place cell on every pixel).</p><p><italic>How does the alignment angle covary with gridness (the very nice grid-like patterns they show tend to look aligned)?</italic> </p><p>In <xref ref-type="fig" rid="fig19">Author response image 2b</xref> we plot grid orientation as a function of gridness. As can be seen the two quantities are not strongly related:<fig id="fig19" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.024</object-id><label>Author response image 2.</label><caption><title>Example of changing width of place cells (σ) and correspondingly changing arena size, such that there is a fixed ratio: arena_size/σ = 40.</title><p>(FISTA algorithm was used for positive PCA, with 0 boundary conditions.) (<bold>a</bold>) Grid scale as a function of σ (place cell width); (<bold>b</bold>) Grid orientation as a function of σ. (<bold>c</bold>) Histogram of grid orientations (<bold>d</bold>) Mean gridness as a function of σ. (<bold>e</bold>) Histogram of mean gridness.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.024">http://dx.doi.org/10.7554/eLife.10094.024</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-resp-fig2-v2.tif"/></fig><fig id="fig20" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.025</object-id><label>Author response image 3.</label><caption><title>Effect of changing the place-field size in a constant boundary (FISTA algorithm used; zero boundary conditions and Arena size 500);</title><p>(<bold>A</bold>) Grid scale as a function of place field size (σ); Linear fit is: Scale = 7.4. Σ+0.62; the lower bound is equal to , where <inline-formula><mml:math id="inf382"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula> is defined in Eq. 32 in the Methods section; (<bold>B</bold>) Grid orientation as a function of gridness (green line is the mean); (<bold>C</bold>) Grid orientation as a function of σ – scatter plot (blue stars) and mean (green line); (<bold>D</bold>) Histogram of grid orientations; (<bold>E</bold>) Mean gridness as a function of σ; and (<bold>F</bold>) Histogram of mean gridness.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.025">http://dx.doi.org/10.7554/eLife.10094.025</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-resp-fig3-v2.tif"/></fig></p><p><italic>3) &quot;Due to the isotropic nature of the data (generated by the agent's motion), there was a 2D redundancy in the X-Y plane in conjunction with a dual phase redundancy in 1D, resulting in a fourfold total redundancy of every solution&quot; – this statement requires unpacking. Presumably the dual phase redundancy is caused by the periodic boundary conditions?</italic> </p><p>We have now devoted a large section in the Methods to analytical considerations, which can explain the fourfold redundancy. Thus we changed the wording of the above paragraph to the following: “The solution demonstrated a fourfold redundancy (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). This was apparent in the plotted eigenvalues (from largest to the smallest eigenvalue, <xref ref-type="fig" rid="fig4">Figure 4A and 4C</xref>), which demonstrated a fourfold grouping-pattern. The fourfold redundancy can be explained analytically by the symmetries of the system – see PCA analysis in Methods section.”</p><p><italic>What are the implications for non-periodic boundary conditions? This sentence needs to be explained to the non-expert reader.</italic> </p><p>In the case of very large arenas, it does not matter what the boundary conditions are. In smaller arenas, the nature of the boundary conditions (i.e. how the place cells behave at the boundary) has an effect. It seems from our simulations that the boundary conditions matter less for the scale of the grid and more for the grid orientation (Compare Author response <xref ref-type="fig" rid="fig3">images 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>). We now note in the Results section: “[…]for very large environments the effects of boundary conditions diminish.”.<fig id="fig21" position="float"><object-id pub-id-type="doi">10.7554/eLife.10094.026</object-id><label>Author response image 4.</label><caption><title>Effect of changing the place-field size in a constant boundary (FISTA algorithm used; periodic boundary conditions and Arena size 500);</title><p>(<bold>A</bold>) Grid scale as a function of place field size (σ); Linear fit is: Scale = 7.4 Σ+0.62; the lower bound, equal to <inline-formula><mml:math id="inf383"><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula>, were <inline-formula><mml:math id="inf384"><mml:msub><mml:mi>k</mml:mi><mml:mo>†</mml:mo></mml:msub></mml:math></inline-formula> is defined in Eq. 32 in the Methods section; (<bold>B</bold>) Grid orientation as a function of gridness; (<bold>C</bold>) Grid orientation as a function of σ – scatter plot (blue stars) and mean (green line); (<bold>D</bold>) Histogram of grid orientations; (<bold>E</bold>) Mean gridness as a function of σ; and (<bold>F</bold>) Histogram of mean gridness.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10094.026">http://dx.doi.org/10.7554/eLife.10094.026</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-10094-resp-fig4-v2.tif"/></fig></p><p>&lt;/<xref ref-type="fig" rid="fig21">Author response image 4</xref> title/legend&gt;</p><p><italic>4) Similarly, <xref ref-type="fig" rid="fig4">Figure 4A</xref> is cryptic – how does the figure show fourfold redundancy? The authors also use a reduced number of place cells to obtain this result (225 vs. 625 in other simulations) – what are the reasons for this change? This should be explained and justified in the text.</italic> We have improved the figure by increasing the resolution and using the same number of points for both sub-figures. Now the fourfold redundancy is quite clear (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). We updated <xref ref-type="fig" rid="fig4">Figure 4</xref> to contain all 625 values, and have also added an additional panel to emphasize the fourfold redundancy.</p><p><italic>5) More generally, when changing place field size, what happens to the number of place cells – presumably the density of firing overlap (covariance) needs to be maintained as this will strongly affect the results?</italic> </p><p><xref ref-type="fig" rid="fig19">Author response image 2</xref> above demonstrates that results are not strongly dependent on the place cell overlap.</p><p> <italic>6) It is not clear how the non-negative weights are implemented alongside zero mean inputs, and we could not gain any additional insight from the Methods section. This issue needs to be elaborated upon in the text, otherwise it seems implicitly contradictory. Does it imply that the output GC can have negative firing rates? i.e. are there positive and negative place cell firing rates (with a mean of zero), coupled with positive place cell to grid cell synaptic weights, to give both negative and positive inputs to the output grid cell?</italic> </p><p>The grid cells were not clipped to zero so they did obtain negative values. In order to realize this in real neurons we will need to assume a general baseline firing which will account for the fact that real neurons do not have negative firing rates.</p><p><italic>Presumably the time step to time step changes in grid cell firing rate between negative and positive values will have a large impact on the direction of synaptic weight change – is this the case?</italic> </p><p>Yes, this is indeed the case, as far as we understand the comment.</p><p> <italic>7) <xref ref-type="fig" rid="fig7">Figure 7C, D</xref>; also 14B – the analytical PCA results for constrained weights appear to exhibit a bimodal distribution of 90 degree gridness scores – what in the data accounts for this? Can the authors provide any comment or insight?</italic> </p><p>The bi-modal distribution was a result of the formation of two populations, one with small orientations to one of the walls and another with small orientation to the other wall. We believe this is not an essential result, but cannot provide a full explanation for this phenomenon.</p><p> <italic>&quot;The dependency was almost linear&quot; – presumably there is a mathematical reason for this, based on the relationship between the covariance matrix and eigenvectors? Can the authors provide any comment or insight into this relationship?</italic> </p><p>This relation is now explained from the added analytical model. The predicted value provides a tight lower bound on the actual relation, as can be seen from the results of simulations in <xref ref-type="fig" rid="fig20">Author response image 3</xref> and <xref ref-type="fig" rid="fig21">4</xref> above, where the actual values were that the grid scale is about 7.5 times the width of the place field.</p><p>Intuitively, the linear dependency between the place cell width and the grid cell spacing must be true in the limit of infinite box size, from dimensional analysis: the length units of the grid cell spacing must be the same length units of the place cell width, since these are the only length units in the model. This is explained mathematically in the unconstrained case by Equation 32, as we detail below that equation:</p><p>&quot;Notice that if we multiply the place cell width by some positive constant c, then the solution 2pi/k† will be divided by c. […] When the box has finite size, k-lattice discretization also has a (usually small) effect on the grid spacing.&quot;</p><p>A similar reasoning can be applied to the constrained case.</p><p> <italic>8) The methods are generally lacking in detail that would allow these simulations to be replicated, based on details provided in the manuscript (see specific details below). The authors should ensure that sufficient detail to allow replication is incorporated. They may also like to consider uploading their code to ModelDB, or a similar online repository, for the sake of transparency and to allow independent replication.</italic> </p><p>We have followed the advice given and have now uploaded our code to <underline>t</underline></p><p>This is also noted at the beginning of the Methods section.</p><p><italic>9) The representation of space lies on a torus in this model, which means that the eigenvectors should be periodic and the spatial frequencies quantized. We fail to grasp why, in <xref ref-type="fig" rid="fig3">Figure 3</xref>, the first spatial frequency should be 2 and not 1 (i.e., we have two peaks in each direction),</italic> </p><p>Indeed the spatial frequencies are quantized by the box dimensions. However, the eigenvectors periodicity is not determined by the box, as we now explain mathematically in extended Methods section. Even in the case that the box size is infinite, we get a finite periodic solution, with spatial frequency equal to the peak of the Fourier transform of the place cell tuning curve – given in Equation 32. When the box size is finite this spatial frequency is also affected by the quantization due to the box dimensions (<xref ref-type="fig" rid="fig15">Figure 15</xref>).</p><p><italic>and why the eigenvectors 3B is periodic, but in 3A they are not.</italic> </p><p>In <xref ref-type="fig" rid="fig3">Figure 3B</xref> we show multiple examples of the 1<sup>st</sup> principal component as output from the network, while in <xref ref-type="fig" rid="fig3">Figure 3A</xref> we show the first 16 components of the PCA algorithm. Thus both figures do not show the same thing. Generally, the PCA components need not appear periodic (i.e., repeat more than once in some directions), though they do obey periodic boundary conditions. For example, PCA components 5-12 in <xref ref-type="fig" rid="fig3">Figure 3A</xref> do not appear periodic since they are composed of the Fourier components B1-8 in <xref ref-type="fig" rid="fig15">Figure 15C</xref>.</p><p> <italic>10) The eigenvectors of a correlation matrix are orthogonal. If we normalize these eigenvectors and furthermore enforce an additional constraint that the entries of each eigenvector must be positive semidefinite, then the &quot;synaptic weights&quot; of different eigenvectors should not overlap. In other words, if an entry in the first eigenvector is non-zero, it ought to be zero in all other eigenvectors.</italic> </p><p>In the constrained case, the solutions maximizing the variance subject to the non-negativity constraint are no longer eigenvectors of any matrix. Thus, the second such solution described in the section “Hierarchical networks and modules” need not be orthogonal to the first, and the issues raised by the reviewers does not arise. We have added a note on this issue in the Results section: “It is important to note though that due to the non-negativity constraint the vectors achieved this way were not orthogonal, and thus it cannot be considered a real orthogonalization process.”. See <xref ref-type="fig" rid="fig18">Author response image 1</xref>, demonstrating that the different vectors outputted from the simulation are indeed not orthogonal.</p><p><italic>11) While the algorithms the authors use relax this last constraint, it is not clear how skewing or shearing a square pattern that results from PCA helps satisfy the dual constraints of non-negativity and orthogonality. This is a key point in the paper, and begs for an intuitive explanation, or at least more analysis or quantification of the degree to which the constraints are satisfied.</italic> </p><p>We believe the above reply addresses this issue. See also <xref ref-type="fig" rid="fig18">Author response image 1</xref>.</p><p> <italic>12) The finding that the spacings corresponding to successive eigenvectors obey ~</italic> <inline-formula><mml:math id="inf385"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> <italic>also calls for an explanation. Why should this ratio of spacings best satisfy non-negativity, orthogonality, and periodicity?</italic></p><p>We now provide an analytical motivation for this in the new revised Methods section of the paper. In short, the change of scale occurs when “jumping” from the first optimal circle in the Fourier space to next available Fourier lattice points which can form a hexagonal shape (<xref ref-type="fig" rid="fig17">Figure 17</xref> in paper).</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p> <italic>Reviewer #1: The authors have worked hard to provide further intuition and interpretation to their results, with much success, although there are still some points that should be clarified, see comments below. There is also a plain error that should be removed. The authors state &quot;the distribution of grid orientations becomes uniform in the range of 0-15 degrees, with a mean (and median) at 7.5 degrees, similar to experimental data [Stensola et al., 2015]&quot;. On the contrary the experimental data shows a very non-uniform distribution clearly peaked at about 7.5 degrees, which is quite different. Also, as a circular variable limited to [0,15) – albeit one for which 16 degrees corresponds to 14 degrees rather than 1 degree – the 'mean' and 'median' they refer to are undefined if the distribution is uniform. Thus the authors should acknowledge that they do not provide an explanation for this aspect of the experimental data, and some of the text/ results on this point could be removed, most importantly in the Abstract.</italic> </p><p>We agree that the experimental data is non-uniform, and clearly peaked around 7.5<sup>o</sup>. We have thus erased this point from the abstract, and removed it throughout the paper.</p><p><italic>At the start of describing the main result (Results, first paragraph) is not clear that the &quot;1d mapping of the 2d activity&quot; is caused by the trajectory of the rat moving through the place fields.</italic> </p><p>This point is now clarified in the second paragraph of subsection “Comparing Neural-Network results to PCA).</p><p> <italic>The next part of the description of their PCA analysis as Eigenvectors and their spatial interpretation could also be improved in terms of intuition. It would be worth mentioning that the PCA, in addition to being calculated as the Eigenvectors of the matrix of temporal covariance between inputs, can also be thought of in terms of singular value decomposition of the (appropriately normalised) input activity. I.e. the input to the network (temporal sequences of place cell activity) can be decomposed into an outer product of (spatial) weights over place cell inputs (their Eigenvectors) and temporally varying weights over these Eigenvectors. The network learns the spatial weights over place cells (the Eigenvectors) as the connections weights from the place cells, and &quot;projection onto place cell space&quot; is simply the firing rates of the output neuron plotted against the location of the rat.</italic> </p><p>Thanks for this good point. This alternative interpretation using SVD has now been added to the in the Methods section.</p><p><italic>In the second paragraph of the subsection “Adding a non-Negativity constraint to the PCA “it is important to remind the reader that the &quot;raw place cells activity&quot; is not the final input to the PCA numerical methods – spatial or temporal mean normalization has to happen first.</italic> </p><p>This reminder has now been added in the third paragraph of the section “Adding a non-Negativity constraint to the PCA”.</p><p><italic>It would be useful to have a surface plot showing Gridness vs. PC Density vs. PC Width. I have found it hard to replicate the Gridness distribution and wonder how general it is to the precise place cell inputs used?</italic> </p><p>We have now added the following subpanels to <xref ref-type="fig" rid="fig12">Figure 12</xref>, in which we vary the ratio between the box size and the size of the input place fields. As can be seen, as long as the box and place field sizes are large enough, the Gridness is &gt; 1 (left panel). Furthermore, the scale of the grid is not highly dependent on the size of the box (right panel).</p><p><italic>The reference to <xref ref-type="fig" rid="fig10">Figure 10A</xref> in the third paragraph of the subsection “Effect of place cell parameters on grid structure “is incorrect (I think) – not showing the dependence on the width of the place fields.</italic> </p><p>Typo has been corrected to “<xref ref-type="fig" rid="fig12">Figure 12A</xref>”.</p><p> <italic>Regarding the presence of &quot;modules&quot; of grid cells (subsection “Modules of grid cells”), it is not clear if the interpretation is that different modules correspond to different grid scales, or different Eigenvectors (even if having the same scale). Assuming the latter, it seems that the analysis predicts that there should only be two modules, with the scale of the first module approximately 7.5x the largest place field inputs, and the second module (~</italic><inline-formula><mml:math id="inf386"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> <italic>smaller)</italic></p><p><italic>existing only to enable non-negative PCA solutions. This deserves discussion, given that the data show at least 5 modules, with a range of scales, the smallest and most numerous having approximately the scale of the smaller place fields found in the dorsal hippocampus (25-30cm).</italic> </p><p>We agree. We believe that a thorough understanding of the module phenomenon will require further study. Mainly two points need to be added to a future model: (a) Looking at a non-uniform distribution of place-cell sizes, similar to reality and (b) adding the feedforward projection from grid cells to place cells, thus “closing the loop”. This point has now been added to the Discussion.</p><p><italic>Reviewer #2: The last equation in the section “2D Solutions”: should read ϕ<sub>1</sub> = ϕ<sub>2</sub> = ϕ<sub>3</sub>n 2 π/3, n ϵ Z. e.g. ϕ<sub>1</sub> = ϕ<sub>2</sub> = ϕ<sub>3</sub></italic></p><p><italic>= 0 would be a solution. As written, ϕ<sub>1</sub> = ϕ<sub>2</sub> = ϕ<sub>3</sub>, is not the maximum. Besides, this more general case would allow solutions that are unlike the ones observed in experiments.</italic> </p><p>This part was removed due to our revision, in response to the next comment. In any case, this was a typo (which happened due to an inaccurate change of notation from cos(k <italic><sub>i</sub></italic> ·(x-x<sub>o</sub>)) to cos(k <italic><sub>i</sub></italic> ·x+<italic>ϕ<sub>i</sub></italic>). It should have been “<italic>ϕ<sub>i</sub></italic> = k<italic><sub>i</sub></italic> ·x<sub>0</sub>, for some x<sub>0</sub>”.</p><p><italic>This brings me to In the subsection “2D solutions”: &quot;To do this [maximize the Fourier components inside the basis set,] we must maximize the minimal value of the cosine components for the basis set&quot; I am afraid this argument escapes me, even though its role is crucial.</italic></p><p><italic>While I think I understand the Fourier domain argument up to this point in the text, how exactly does maximizing the minimum value in real space of the function guarantee that the Fourier components in Eq. 24 are maximized?</italic></p><p>This argument was indeed incorrect (it is true if can we neglect the higher order harmonics in each lattice, unfortunately this cannot be done, as we verified numerically). We thank the reviewer for noting this. We changed the sub-section “2D solutions” (in section “The PCA solution with a non-negative constraint”) to correct this. First, we prove (in the appendix section “Upper bound on the constrained objective – 2D square lattice”) that the objective is bounded from above by 0.25 in any non-hexagonal 2D lattice with a single grid length (i.e., a square or 1D lattice). Note this remains true even if we consider higher order harmonics. Second, we give an example for a hexagonal lattice (found via a numerical scan) which achieves an objective value higher than 0.25. Thus, given our approximations, this shows that any optimal solution must be a hexagonal lattice</p><p><italic>Eq. 34: why is the upper limit here ∞, when it was M in Eq. 33?</italic></p><p>The two equations result from two different numerical procedures and assumptions. We revised the text near the two equations to explain this point:</p><p>“This form (Eq. 33) results from a parameter grid search for <italic>M</italic> =1, 2,3 and 4, under the assumption that <italic>L</italic> ⟶ ∞ and <italic>r</italic>ˆ <italic>(k)</italic> is highly peaked. […] Specifically, it has similar form (Eq. 34).”</p><p>The main point is to show that both give very similar results. In other words, that our approximate description, using the simplifying assumptions and finite <italic>M</italic>, is closely related to the solution without approximations. Additional details were added to the text to clarify this important point. Also, to improve focus, we relegated to the appendix some less important details from that section</p><p>(on the origin of the difference between <italic>k</italic><sub>†</sub> and <italic>k</italic><sub>*</sub>).</p></body></sub-article></article>