<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">31134</article-id><article-id pub-id-type="doi">10.7554/eLife.31134</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Encoding sensory and motor patterns as time-invariant trajectories in recurrent neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-86051"><name><surname>Goudar</surname><given-names>Vishwa</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6612-3076</contrib-id><email>vishwa.goudar@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-13701"><name><surname>Buonomano</surname><given-names>Dean V</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8528-9231</contrib-id><email>dbuono@ucla.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Departments of Neurobiology</institution><institution>University of California, Los Angeles</institution><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Integrative Center for Learning and Memory</institution><institution>University of California, Los Angeles</institution><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Departments of Psychology</institution><institution>University of California, Los Angeles</institution><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-28203"><name><surname>Druckmann</surname><given-names>Shaul</given-names></name><role>Reviewing Editor</role><aff id="aff4"><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>14</day><month>03</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e31134</elocation-id><history><date date-type="received" iso-8601-date="2017-08-10"><day>10</day><month>08</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-02-19"><day>19</day><month>02</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Goudar et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Goudar et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-31134-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.31134.001</object-id><p>Much of the information the brain processes and stores is temporal in nature—a spoken word or a handwritten signature, for example, is defined by how it unfolds in time. However, it remains unclear how neural circuits encode complex time-varying patterns. We show that by tuning the weights of a recurrent neural network (RNN), it can recognize and then transcribe spoken digits. The model elucidates how neural dynamics in cortical networks may resolve three fundamental challenges: first, encode multiple time-varying sensory <italic>and</italic> motor patterns as stable neural trajectories; second, generalize across relevant spatial features; third, identify the same stimuli played at different speeds—we show that this temporal invariance emerges because the recurrent dynamics generate neural trajectories with appropriately modulated angular velocities. Together our results generate testable predictions as to how recurrent networks may use different mechanisms to generalize across the relevant spatial and temporal features of complex time-varying stimuli.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>temporal scaling</kwd><kwd>recurrent neural networks</kwd><kwd>neural dynamics</kwd><kwd>sensorimotor</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NSF IIS-1420897</award-id><principal-award-recipient><name><surname>Buonomano</surname><given-names>Dean V</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006785</institution-id><institution>Google</institution></institution-wrap></funding-source><award-id>Google Faculty Research Award</award-id><principal-award-recipient><name><surname>Buonomano</surname><given-names>Dean V</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>MH60163</award-id><principal-award-recipient><name><surname>Buonomano</surname><given-names>Dean V</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A recurrent network model trained to transcribe temporally scaled spoken digits into handwritten digits proposes that the brain flexibly encodes time-varying stimuli as neural trajectories that can be traversed at different speeds.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Many, if not most, of the tasks the brain performs are inherently temporal in nature: from recognizing and generating complex spatiotemporal patterns—such as a phoneme sequence that composes a word—to creating temporal expectations of when an event will occur (<xref ref-type="bibr" rid="bib43">Mauk and Buonomano, 2004</xref>; <xref ref-type="bibr" rid="bib49">Nobre et al., 2007</xref>; <xref ref-type="bibr" rid="bib25">Ivry and Schlerf, 2008</xref>; <xref ref-type="bibr" rid="bib45">Merchant et al., 2013</xref>; <xref ref-type="bibr" rid="bib24">Hopfield, 2015</xref>). In contrast to the representation of an object in a static image, such as a picture of a face, robust decoding and encoding of a time-varying patterns must rely on the spatiotemporal dependencies inherent to the pattern. How does the brain accomplish this? One highly influential theory in neuroscience holds that information is stored as fixed-point attractors that emerge in the dynamic activity of the brain’s recurrently connected circuits (<xref ref-type="bibr" rid="bib23">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="bib22">Hopfield and Tank, 1986</xref>; <xref ref-type="bibr" rid="bib3">Amit and Brunel, 1997</xref>; <xref ref-type="bibr" rid="bib66">Wang, 2001</xref>). Two limitations of this framework are: (1) It fails to capture the temporal aspect of stored information, thus forcing many computational models to ‘spatialize’ time—that is, they treat the temporal component of time-varying patterns as additional spatial dimensions (<xref ref-type="bibr" rid="bib52">Rabiner, 1989</xref>; <xref ref-type="bibr" rid="bib64">Waibel et al., 1989</xref>; <xref ref-type="bibr" rid="bib15">Elman, 1990</xref>; <xref ref-type="bibr" rid="bib21">Hinton et al., 2012</xref>; <xref ref-type="bibr" rid="bib47">Mnih et al., 2015</xref>); (2) it does not capture a fundamental feature of how the brain processes temporal information: temporal invariance. For example, humans readily recognize temporally warped—compressed or dilated—speech or music.</p><p>While the mechanisms that underlie the brain’s ability to perform a broad range of spatiotemporal tasks in the sensory and motor domains are not known, there is mounting theoretical and experimental evidence that our ability to tell time on the sub-second scale, and represent time-varying patterns, relies on the inherent <italic>continuous</italic> dynamics, and computational potential, of recurrent neural networks (<xref ref-type="bibr" rid="bib53">Rabinovich et al., 2008</xref>; <xref ref-type="bibr" rid="bib7">Buonomano and Maass, 2009</xref>; <xref ref-type="bibr" rid="bib40">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Crowe et al., 2014</xref>; <xref ref-type="bibr" rid="bib9">Carnevale et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Li et al., 2016</xref>). This framework alleviates the restrictions imposed by traditional discrete-time or fixed-point attractor models, by relying on the recurrent connections to implicitly maintain an ongoing memory of the pattern. Indeed, recent computational studies have established that by tuning the weights within recurrent neural network (RNN) models, it is possible to <italic>robustly</italic> store and generate complex time-varying motor patterns (<xref ref-type="bibr" rid="bib30">Laje and Buonomano, 2013</xref>; <xref ref-type="bibr" rid="bib2">Abbott et al., 2016</xref>; <xref ref-type="bibr" rid="bib57">Rajan et al., 2016</xref>). What remains unknown is whether the same approach can be used to robustly discriminate time-varying sensory patterns. Indeed, this poses a challenging problem because in order to effectively process spatiotemporal stimuli the dynamics of an RNN must be sensitive to the relevant spatial and temporal features of the sensory stimuli, while being able to generalize across their natural spatial and temporal variations.</p><p>Neuroscientists have typically distinguished between sensory and motor areas; but it is well established that activity in sensory areas is strongly influenced by motor behavior, and that sensory stimuli can modulate activity in motor areas—furthermore, some brain areas are characterized as being sensorimotor (<xref ref-type="bibr" rid="bib14">Doupe and Kuhl, 1999</xref>; <xref ref-type="bibr" rid="bib4">Ayaz et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Chang et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Schneider and Mooney, 2015</xref>; <xref ref-type="bibr" rid="bib11">Cheung et al., 2016</xref>). Computationally, sensory and motor processing are understood to have disparate requirements—during sensory processing, network dynamics should primarily be driven by the sensory inputs; in contrast, during motor processing neural dynamics should be autonomous and driven primarily by recurrent interactions. It remains unclear how a single network could accomplish both tasks, and couple them when necessary. Indeed, to date, no previous models have shown that the same network can satisfy the requirements for both sensory and motor processing. Here we show that the same RNN can reliably function in both a sensory and motor regime. Specifically the same RNN can convert complex time-varying sensory patterns into motor patterns—thus performing a transcription task in which spoken digits are identified and read out as ‘handwritten’ digits.</p><p>In the temporal domain, understanding how the brain recognizes temporally compressed or dilated patterns represents a long-standing challenge. This temporal invariance is particularly evident in our ability to recognize temporally warped speech or music (<xref ref-type="bibr" rid="bib46">Miller et al., 1984</xref>; <xref ref-type="bibr" rid="bib59">Sebastián-Gallés et al., 2000</xref>). Our results suggest that one advantage of storing time-varying patterns as neural trajectories within RNNs is that, under the appropriate conditions, this strategy naturally accounts for temporal invariance.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We first asked if a single RNN could perform a complex sensory-motor task: transcribing spoken digits into handwritten digits. A <italic>continuous-time</italic> firing-rate RNN with randomly assigned sparse recurrent connections was used (<xref ref-type="bibr" rid="bib60">Sompolinsky et al., 1988</xref>). The strengths of these recurrent connections were initialized to be relatively strong; thus, before training the network was in a so-called high-gain regime (<italic>g</italic> = 1.6) that is characterized by self-perpetuating and chaotic activity (Materials and Methods). The transcription task is divided into a sensory and a motor epoch. During the sensory epoch, the RNN is presented with a spoken digit, and over the ensuing motor epoch the RNN drives an output pattern transcribing the presented digit via three motor outputs <italic>x</italic>, <italic>y</italic>, and <italic>z</italic>—activity in the <italic>x</italic> and <italic>y</italic> units determines the 2D coordinates of a ‘pen on paper’, while <italic>z</italic> determines if the ‘pen’ is in contact with the ‘paper’ or not. <xref ref-type="fig" rid="fig1">Figure 1A</xref> illustrates the network architecture and transcription task for the digit ‘2’. Successful performance of this transcription task requires that the RNN: 1) encode spoken digits with a set of neural trajectories in high-dimensional phase space; 2) autonomously generate digit-specific trajectories during the motor epoch, in order to drive the digit-specific output patterns; and most importantly 3) generate trajectories that are stable so they may encode each digit’s sensory pattern, sensory-motor transition, and motor pattern in a manner that is invariant not just to background noise but also to spatiotemporal variations of the spoken digits, including temporal warping.</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.31134.002</object-id><label>Figure 1.</label><caption><title>Trained RNNs perform a sensorimotor spoken-to-handwritten digit transcription task</title><p>(<bold>A</bold>) Transcription task. The spectrogram of a spoken digit, e.g. ‘two’, is transformed to a 12-channel cochleogram that serves as the continuous-time input to a RNN during the sensory epoch of each trial. During the motor epoch, the output units must transform the high-dimensional RNN activity into a low-dimensional ‘handwritten’ motor pattern corresponding to the spoken digit (the z output unit indicates whether the pen is in contact with the ‘paper’). The colors of the output pattern (right panel) represent time (as defined in the ‘Input’ panel). (<bold>B</bold>) Overlaid outputs of a trained RNN (<italic>N</italic> = 4000) for five sample utterances of each of 10 digits. For all digits, each output pattern is color-coded to the bounding box of the corresponding cochleogram (inset). Sample utterances shown are a mix of trained (*) and novel utterances, and span the range of utterance durations in the dataset. (<bold>C</bold>) Transcription performance of three different types of RNNs on novel utterances. Performance was based on images of the output as classified by a deep CNN handwritten digit classifier. The control groups include untrained RNNs (‘reservoir’) and RNNs trained only during the motor epoch (i.e., just to reproduce the handwritten patterns; see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Output unit training was performed identically for all networks. Bars represent mean values over three replications, and error bars indicate standard errors of the mean. Line indicates chance performance (10%). The RNNs were trained on 90 utterances (10 digits, three subjects, three utterances per subject per digit). They were then tested on 410 novel utterances (across five speakers, including two novel speakers), with 10 test trials per utterance. <italic>I<sub>0</sub></italic> was set to 0.5 during network training (if applicable), and to 0.05 during output training and testing.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig1-v1"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31134.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Transcription performance of an RNN trained only during the motor epoch.</title><p>Overlaid outputs of a motor-trained RNN (<italic>N</italic> = 4000) for five sample utterances of each of the 10 digits used in the spoken-to-handwritten digit transcription task. Transcription performance of the motor-trained RNN is poorer than its sensorimotor trained counterpart (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Each output pattern is color-coded to the bounding box of the corresponding cochleogram (insets). Sample utterances shown are a mix of trained (*) and novel utterances, and are the same ones shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. The network was trained on 90 utterances (10 digits, three subjects, three utterances per subject per digit). <italic>I<sub>0</sub></italic> was set to 0.5 during network training, and to 0.05 during output training and testing.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig1-figsupp1-v1"/></fig></fig-group><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-31134-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.31134.004</object-id><label>Video 1.</label><caption><title>A trained RNN performs the sensorimotor spoken-to-handwritten digit transcription task on novel utterances.</title><p>A trained RNN (N = 4000) and its output units perform the transcription task on five novel utterances (five different speakers). The last two utterances illustrate RNN performance on same-digit utterances of different durations (i.e. temporal invariance).Top Left: Input to the RNN (with audio). Moving gray bar indicates the current time step. Bottom Left: Evolution of RNN activity in a subset of the network (100 units). Right: Evolution of the output. The location of the pen is imprinted in red when the z co-ordinate is greater than 0.5, and plotted in gray otherwise.</p></caption></media><p>We attempted to satisfy these three conditions by training the RNN using a supervised learning rule, in which the recurrent units were trained to robustly reproduce their ‘innate’ patterns of activity (<xref ref-type="bibr" rid="bib30">Laje and Buonomano, 2013</xref>)—that is, those generated in the untrained network—using the recursive-least-squares learning rule (<xref ref-type="bibr" rid="bib20">Haykin, 2002</xref>; <xref ref-type="bibr" rid="bib61">Sussillo and Abbott, 2009</xref>). In essence, the RNN is trained to reproduce one digit-specific ‘innate’ trajectory in response to all training utterances of a given digit. For example, the pattern of activity produced in response to a ‘template’ utterance of a given digit from the beginning of the sensory epoch to the end of the motor epoch is taken as the innate trajectory; the network is then trained to reproduce this trajectory in response to other utterances of the same digit. Furthermore, it is trained to do so regardless of the initial state of the network’s units, and in the presence of continuous background noise (Materials and Methods). Only after training the RNN (<italic>recurrent training</italic>), are the output units trained to generate the handwriting patterns representing each of the digits during the motor epoch (<italic>output training</italic>)—this separation of the recurrent and output training phases, while not necessary, allows for rapid training and retraining of the outputs to produce arbitrary motor patterns, without retraining the recurrent weights. Notably, the temporal separation of sensory and motor learning is observed in some forms of sensori-motor learning (<xref ref-type="bibr" rid="bib14">Doupe and Kuhl, 1999</xref>). The output training phase is performed using standard supervised methods (Materials and Methods). <xref ref-type="fig" rid="fig1">Figure 1B</xref> shows the outputs of a trained network cross-tested on ten digits (0–9) across five speakers. Following training, the network successfully transcribes the utterances used during training (marked by asterisks). More importantly, performance generalizes to novel utterances and speakers in the dataset despite significant variations in the duration and spatiotemporal structure across utterances (<xref ref-type="video" rid="video1">Video 1</xref>).</p><p>To quantify performance, we need an objective measure of the quality of the motor output for each test utterance. This itself represents a classification task, wherein images of RNN-generated transcriptions must be assigned to one of the ten possible digits. Rather than use a human-based performance measure, we used a standard deep convolutional neural network (CNN) (<xref ref-type="bibr" rid="bib32">LeCun et al., 2015</xref>) to rate the performance of trained RNNs (Materials and Methods)—i.e., the CNN was used to determine if each ‘handwritten’ digit was correct or not. Performance of trained RNNs was 98.7% on a test set of 410 novel stimuli (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Since untrained RNNs (‘reservoir networks’) are in and of themselves capable of performing many interesting computations (<xref ref-type="bibr" rid="bib39">Maass et al., 2002</xref>; <xref ref-type="bibr" rid="bib26">Jaeger and Haas, 2004</xref>; <xref ref-type="bibr" rid="bib37">Lukoševičius and Jaeger, 2009</xref>), it is important to determine how much of this performance is dependent on the training of the RNNs. We thus examined a control group wherein the three output units are trained as they were for the trained RNNs, but the RNN itself was not trained (the ‘reservoir’). The performance was very poor (20%), in large part because during the motor epoch a reservoir RNN is operating in an autonomous mode that is chaotic—making it difficult for the output units to learn to produce the target output patterns. We also examined the effects of training an RNN only during the motor epoch, resulting in a performance of 71%. This significant improvement over the reservoir networks is a consequence of the fact that a network driven by external inputs can encode sensory stimuli despite a lack of sensory epoch training, due to the intrinsic ability of RNNs to encode sensory stimuli and the stabilizing influence of the external inputs (see below). Yet, despite the improvement in performance, some digits were almost always misclassified (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The difference in performance between the trained RNNs (sensory and motor training) and the exclusively motor trained RNNs, confirms the importance of tuning the recurrent weights to the sensory discrimination component of the task (see below).</p><sec id="s2-1"><title>Stability of the neural trajectories</title><p>RNNs operating in high-gain regimes are prone to chaotic behavior (<xref ref-type="bibr" rid="bib60">Sompolinsky et al., 1988</xref>; <xref ref-type="bibr" rid="bib55">Rajan et al., 2010a</xref>) because the strong recurrent feedback of these networks rapidly amplifies any noise or perturbations. It is thus critical to demonstrate that the above performance is robust to background noise and perturbations during the sensory and motor epochs. The distinction between epochs is critical since the RNN is operating in fundamentally different regimes during the sensory and motor epochs. During the sensory epoch the recurrently generated internal dynamics are partially suppressed (or ‘clamped’) as a result of the external input, yet during the motor epoch all activity is internally generated. To examine the stability of these sensory and motor ‘object’ representations, we briefly perturbed the internal dynamics of the RNN during either the sensory or motor epochs. Sensory epoch perturbations were introduced half way through the presentation of each utterance, while motor epoch perturbations were introduced at the 10% mark of the motor epoch. <xref ref-type="fig" rid="fig2">Figure 2A</xref> shows the activity of a hundred units from a trained RNN (and the resulting output), when it is presented with the digit ‘three’ and strongly perturbed (amplitude = 2) in the motor epoch. The resulting transcription briefly deviates from an unperturbed one (grey backdrop), but recovers mid-voyage. The sensory epoch is less sensitive to perturbations—as would be expected because the presence of the external input serves as a stabilizing influence (<xref ref-type="bibr" rid="bib55">Rajan et al., 2010a</xref>). During the motor epoch the RNN is operating autonomously as a ‘dynamic attractor’, that is, once bumped off its trajectory it maintains a memory of its current voyage and is able to return to the original trajectory (<xref ref-type="bibr" rid="bib30">Laje and Buonomano, 2013</xref>) (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Performance measurements using the CNN classifier reveal a graceful degradation across a wide range of perturbation magnitudes, and confirm the superior robustness of the sensory epoch (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.31134.005</object-id><label>Figure 2.</label><caption><title>Digit transcription is robust to perturbations during the sensory and motor epochs.</title><p>(<bold>A</bold>) Schematic of a perturbation experiment. The motor trajectory of a trained RNN (<italic>N</italic> = 2100; 100 sample units shown) for the spoken digit ‘three’, is perturbed with a 25 ms pulse (amplitude = 2). The pulse (blue arrow) causes a disruption of the network trajectory, but the trajectory quickly recovers and returns to the dynamic attractor, as is evident from the plots on the right comparing the output unit values and the transcribed pattern for a trial with (red forefront) and without (gray backdrop) the perturbation. (<bold>B</bold>) Sample motor patterns generated by the network in response to perturbations of increasing magnitude, applied either during the sensory or motor epochs. Sensory epoch perturbations were applied halfway into the epoch, while motor epoch perturbations were applied at the 10% mark of the epoch (indicated by a blue dot). (<bold>C</bold>) Impact of perturbations on transcription performance (measured by the deep CNN classifier) for test utterances. Bars represent mean performance over ten trials, with a different (randomly selected) perturbation pulse applied at each trial. Line indicates chance performance. The performance measures establish that the encoding trajectories are stable to background noise perturbations, with transcription performance degrading gracefully as the perturbation magnitude increases. Furthermore, at all perturbation magnitudes, the sensory encodings are more robust than their motor counterparts due to the suppressive effects of the sensory input. The network was trained on 30 utterances (3 utterances of each digit by one subject) and tested on 70 (7 utterances of each digit by one subject). <italic>I<sub>0</sub></italic> was set to 0.25 during network training, to 0.01 during output training, and to 0 during testing except for the duration of the perturbation pulse.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig2-v1"/></fig></sec><sec id="s2-2"><title>RNN training sculpts network dynamics to enhance discrimination</title><p>How does tuning the recurrent weights allow the RNN to stably encode both sensory and motor information in neural trajectories, despite considerable spatiotemporal differences between digit utterances (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, insets)? Firing rate traces of sample units in a trained RNN show more similar patterns of activity in response to different utterances of the same digit, when compared to a reservoir RNN (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>). To better examine the structure of the population representations, we can visualize and compare the neural trajectories in response to multiple utterances (learned and novel) of the same digit, during the sensory and motor epochs, in principal component analysis (PCA) subspace. The trajectories produced in the untrained network by utterances of the digits ‘six’ and ‘eight’, during the sensory epoch, occupied a fairly large abutting volume of PCA subspace (<xref ref-type="fig" rid="fig3">Figure 3C</xref>); and during the motor epoch the trajectories were highly variable—the network strongly and perpetually amplifies the differences between utterances, because it is in a chaotic regime. In contrast, in the trained RNN, sensory epoch trajectories of each digit were restricted to a narrower volume, and better separated from the volume representing the other digit (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). During the motor epoch, the trajectories for different utterances of the same digit were constrained to a much narrower tube—reflecting the dynamic attractor—and better separated from the motor trajectories of the other digit. The set of all within-digit trajectories can be thought of as populating a hypertube that delimits the volume of phase space traversed by the digit. During the sensory epoch, this hypertube of trajectories represents a memory of a family of related spatiotemporal objects: the spoken digit. During the motor epoch the network is autonomous, and the hypertube of all within-digit trajectories narrows to a dynamic attractor that represents the ‘motor memory’ of the corresponding handwritten digit.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.31134.006</object-id><label>Figure 3.</label><caption><title>Trained RNNs generate convergent continuous neural trajectories in response to different instances of the same spatiotemporal object.</title><p>(<bold>A–B</bold>) Neural activity patterns of three sample units of a reservoir (<bold>A</bold>) and trained (<bold>B</bold>) network, in response to a trained and a novel utterance each of the digits ‘six’ (red traces) and ‘eight’ (blue traces) during the sensory (top) and motor (bottom) epochs. Utterances of similar duration were chosen for each digit, to allow for a direct comparison, without temporal warping, of the corresponding pair of sensory epoch traces. (<bold>C–D</bold>) Projections in PCA space of the sensory and motor trajectories for 10 utterances each of the digits ‘six’ and ‘eight’ generated by the reservoir (<bold>C</bold>) and trained (<bold>D</bold>) networks. Colored spheres represent time intervals of 150 ms. Compared to the reservoir network, in the trained RNN the activity patterns in response to different utterances of the same digit are closer, and the trajectories in response to different digits are better separated. Both networks were composed of 2100 units. Network training was performed with 30 utterances (one subject, 10 digits, three utterances per digit). <italic>I<sub>0</sub></italic> was set to 0.5 during network training, and to 0 while recording trajectories for the analysis.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig3-v1"/></fig><p>It is well established that cortical circuits undergo experience-dependent plasticity—a process that seems to result in the optimization or specialization of those circuits to the tasks the animal is exposed to (<xref ref-type="bibr" rid="bib8">Buonomano and Merzenich, 1998</xref>; <xref ref-type="bibr" rid="bib12">Crist et al., 2001</xref>; <xref ref-type="bibr" rid="bib16">Feldman and Brecht, 2005</xref>; <xref ref-type="bibr" rid="bib28">Karmarkar and Dan, 2006</xref>). The above results establish that training the RNN does improve discrimination performance—the recurrent weights are tuned to the task at hand—but leaves open the question of how exactly this is accomplished. To answer this question, we asked if the Euclidean distances between trajectories in response to different utterances of the same digit (within-digit distance), and utterances of different digits (between-digit distance), were significantly altered in comparison to the reservoir (untrained) RNN. Training significantly decreases the mean within-digit distances during the sensory epoch (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Importantly, in doing so, it does not diminish the large separations between trajectories of different digits. The same effect, much enhanced, is observed during the motor epoch, a consequence of the formation of dynamic attractors. Successful formation of these attractors is strongly influenced by an unambiguous and stable sensory epoch-motor epoch transition of the network dynamics. Such a transition relies on two factors: (i) a low within-digit separation at the end of the sensory epoch trajectory, ensuring initial conditions at the beginning of the motor epoch that lie within the initial basin of attraction of the motor pattern-encoding dynamic attractor; (ii) a between-digit separation that is substantially larger than the within-digit separation of sensory epoch trajectories, particularly at the end of the sensory epoch, to ensure robust and divergent sensory-epoch-motor epoch transitions for different digits.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.31134.007</object-id><label>Figure 4.</label><caption><title>Trained RNNs encode both sensory and motor objects as well separated neural trajectories.</title><p>(<bold>A</bold>) Euclidean distance between trajectories of the same digit (within-digit) versus those of different digits (between-digit). At each time step, the trajectory distances represent the mean and SD (shading) over pairs of one hundred utterances (one subject, 10 digits, 10 utterances per digit). During the sensory epoch, training brings trajectories for the same digit closer together, while maintaining a large separation between trajectories of different digits, thereby improving discriminability. A similar, but stronger, effect is observed during the motor epoch. (<bold>B</bold>) Comparison of the eigenspectrum of the recurrent weight matrix (<bold><italic>W<sup>R</sup></italic></bold>) in a reservoir and trained network. Both networks were composed of 2100 units. Network training was performed with 30 utterances (one subject, 10 digits, three utterances per digit). <italic>I<sub>0</sub></italic> was set to 0.5 during network training, and to 0 while recording trajectories for the analysis.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig4-v1"/></fig><p>The finding that training sculpts the dynamics of the RNN during the sensory epoch is also a critical one because it shows that even though RNN activity is governed in part by the external input, the internal connections are critical: tuning them effectively improves the interaction between the sensory input and the internally generated dynamics. This improvement is expressed as a collapsing of the family of trajectories representing a digit into a narrower hypertube. These results follow as a direct consequence of the supervised recurrent training paradigm (Materials and Methods)—a single innate trajectory serves as a common target for different training utterances of a digit, thereby inducing changes in the network’s recurrent dynamics that are necessary to encode the different utterances along similar neural trajectories; However, different digits are encoded in rapidly divergent trajectories because the target trajectories for each digit are generated by an untrained chaotic network. Further evidence that training dramatically influences the weight matrix of the RNN is demonstrated by the change in its eigenspectrum (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Because the network is initialized with normally distributed weights with a gain of 1.6 (Materials and Methods), the eigenspectrum of the reservoir’s weight matrix lies in a circle of radius approximately 1.6. Training results in a compression of those eigenvalues with real part larger than one, bringing the maximal real part of the eigenvalues closer to one. In a linear network, when all eigenvalues have a real part less than one, it implies that the network’s activity will decay to zero when it operates autonomously; however our network is nonlinear and does not operate exclusively in the autonomous mode, so interpretations of the eigenspectrum of the weight matrix are not straightforward—nevertheless the compression of the eigenspectrum is consistent with the increased stability of the network during the sensory and motor epochs (<xref ref-type="bibr" rid="bib56">Rajan and Abbott, 2006</xref>; <xref ref-type="bibr" rid="bib50">Ostojic, 2014</xref>).</p></sec><sec id="s2-3"><title>Balance between recurrent and input dynamics is crucial for discrimination</title><p>The above results provide insights as to how a single neural circuit can function in two seemingly distinct computational modes: sensory and motor. Sensory discrimination requires circuits to be highly responsive to external stimuli in order to categorize inputs into discrete classes. In contrast, motor tasks require autonomous generation of spatiotemporal patterns, and thus need the network dynamics to be somewhat resistant to external inputs. In the network described here, this balance depends on the recurrent weights and the magnitude of the external drive.</p><p>The recurrent weights must be strong—that is the network must be in a high-gain regime—for two reasons. First, network dynamics in high-gain regimes are high-dimensional (<xref ref-type="bibr" rid="bib54">Rajan et al., 2010b</xref>), which naturally yields well-separated trajectories for different digits. A goal of training then, is to achieve recurrent suppression of within-digit separation, while retaining the large between-digit separation (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Second, as noted earlier, a network operating in a high-gain regime is inherently capable of generating the self-perpetuating activity required in the motor epoch, which is then stabilized by the training procedure. Thus during the sensory epoch, the RNN is operating in a regime that is both sensitive to external input and influenced by strong internal recurrent dynamics. In the motor mode, the RNN operates autonomously, generating locally stable trajectories that serve as a high-dimensional ‘engine’ that drives arbitrary low dimensional output patterns.</p><p>The ability of the network to meaningfully process input patterns during the sensory epoch, so that they are easy to discriminate, also depends on the strength of the inputs. To better understand the impact of input amplitude on the encoding trajectories and their discriminability, before and after training, we parametrically varied the input amplitude and studied the resulting RNN dynamics during the sensory epoch. In the reservoir, both within and between-digit distances diminish as the input amplitude increases (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left panel), consistent with a previous study showing that strong inputs can progressively dominate and override the internal dynamics of an RNN (<xref ref-type="bibr" rid="bib55">Rajan et al., 2010a</xref>). In the extreme, input-dominated regimes effectively void the recurrent weights and render training useless (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, right panel). The poor discriminability at high input amplitudes is further confirmed by dimensionality measurements of sensory epoch trajectories (Materials and Methods), which mirror the low-dimensionality of the input cochleograms, regardless of training (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). In contrast, low-input amplitude regimes are dominated by the RNN’s internal dynamics. Reservoirs in this regime produce chaotic high-dimensional dynamics and are insufficiently sensitivity to external inputs; thus the within- and between-digit distances are similarly high (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left). Training strongly alters these dynamics, lowering its dimensionality (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). However, as a consequence of poor input-sensitivity, these changes fail to improve discrimination—trained RNNs in this regime still sustain similar within and between-digit distances (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, right). RNNs trained at intermediate input amplitudes (0.5, 5) are both sensitive to the input and able to discriminate between digits, because their sensory epoch trajectories are shaped both by the input and internal dynamics. It is thus critical that the input drive be strong enough to influence ongoing activity but not strong enough to ‘erase’ recent information encoded in the current trajectory.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.31134.008</object-id><label>Figure 5.</label><caption><title>Trajectory separation in reservoir and trained RNNs as a function of input amplitude.</title><p>(<bold>A</bold>) Comparison of mean within- and between-digit distances of the sensory epoch trajectories in reservoir and trained networks (N = 2100) at different input amplitudes. Bars represent mean of the time-averaged distances for all utterances of all digits (one subject, 10 digits, 10 utterances per digit). (<bold>B</bold>) Dimensionality of sensory epoch trajectories in the reservoir and trained networks at different input amplitudes. Simulations, including training, in all other figures were performed at an input amplitude of 5 (gray highlight). Dashed lines indicate the dimensionality of the motor epoch trajectories in the reservoir (blue) and trained (red) networks, when the input amplitude was 5. Network training was performed with 30 utterances (one subject, 10 digits, three utterances per digit). The networks were trained with <italic>I<sub>0</sub></italic> set to 0.25, and trajectories were recorded with <italic>I<sub>0</sub></italic> set to 0.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig5-v1"/></fig></sec><sec id="s2-4"><title>Mechanisms underlying spatial (Spectral) Generalization</title><p>The attracting dynamics of the network result in each digit being represented in a narrow hypertube. These representations must be robust across both spectral and temporal (changes in speed) differences in the utterances of each digit. Next, we separately examine the mechanisms underlying spatial and temporal generalization. Variation in the spectral structure of digits—spectral noise (<xref ref-type="fig" rid="fig6">Figure 6A</xref>)—is qualitatively different from background noise. First, spectral noise exhibits time-dependent structured relationships with the input signal (they may be correlated, anti-correlated or tend to occur along specific directions during specific parts of the input). For example, the spectral differences between utterances of one phoneme within a digit will be different than those of another. Invariance then requires the network to isolate and integrate the signal while suppressing signal-dependent spectral noise. Second, spectral noise is a form of correlated noise. The noise in each input channel is simultaneously delivered to multiple recurrent units via common input projections. Moreover, the spectral noise in different channels, particularly neighboring ones, exhibit structured time-dependent relationships. Third, spectral noise is proportional in magnitude to the input amplitude, and therefore tends to be much stronger than the background noise that is typically injected during training and testing.</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.31134.009</object-id><label>Figure 6.</label><caption><title>Robustness to spectral noise depends on the spatiotemporal structure of the inputs that the network is exposed to during training.</title><p>(<bold>A</bold>) Spectral noise in the inputs to an RNN (N = 2100) during presentations of digit zero. Sample noise is the difference between the external input (each row reflects net external input to a unit in the RNN) for a sample and template utterance (corresponding cochleogram inputs are shown as insets; see Materials and Methods for the definition of template utterance). Absolute values of external inputs are shown here for better visualization. (<bold>B</bold>) Coordinates of spectral noise in PCA space used to construct noisy utterances for digit zero. First three Principal component (PC) scores of the sample noise (left). Scores, where the basis was constructed as a random shuffle of the sample noise PC loadings (middle) and an orthonormal set orthogonal to the sample noise PC loadings (right). (<bold>C–D</bold>) Projections in PCA space of the external input for a template utterance, a sample utterance, and utterances with artificial noise created from the shuffled basis noise (<bold>C</bold>) and orthogonal basis noise (<bold>D</bold>). The respective artificial external inputs are shown as insets. (<bold>E</bold>) Comparison of mean within-digit distances of the trajectories for the different natural (two trained, blue and seven untrained, cyan) and artificial (30 shuffled, yellow and orthogonal, red each) utterances from the template utterance. Bars represent mean of the time-averaged distances for respective utterances of all digits (one subject, 10 digits). Error bars indicate standard errors of the mean over the digits. (<bold>F–G</bold>) Magnitudes of components that contribute to the total within-digit distance measured in (<bold>E</bold>) in the reservoir (<bold>F</bold>) and trained (<bold>G</bold>) networks. Bars represent mean of the time-averaged values of the components for respective utterances of all digits (one subject, 10 digits). Error bars indicate standard errors of the mean over the digits. The network was trained with <italic>I<sub>0</sub></italic> set to 0.25, and trajectories were recorded with <italic>I<sub>0</sub></italic> set to 0.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig6-v1"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31134.010</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Schematic description of the decomposition of trajectories and trajectory separation into recurrent and input components.</title><p>(<bold>A</bold>) Left panel. Schematic of the evolution of a trajectory <italic>x(t)</italic>. Right panel. The evolution of the trajectory in phase space between time steps <italic>t</italic> and <italic>t + 1</italic> <inline-formula><mml:math id="inf1"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, can be decomposed into three component vectors: recurrent (red), input (green), and a passive component (black, which always points towards the origin). The sum of these vectors must equal the net movement of the trajectory between those time steps (see Materials and Methods for definitions of <italic>b</italic> and <italic>R</italic>). (<bold>B</bold>) Left panel. The total deviation between two trajectories <bold><italic>x<sup>1</sup>(t)</italic></bold> and <bold><italic>x<sup>2</sup>(t)</italic></bold> at time step <italic>t</italic> (<italic>Δx(t))</italic>, can be viewed similarly as being composed of the recurrent (<italic>ΔR(t))</italic>, input (<italic>ΔI(t))</italic> and total (<italic>Δx(t-1))</italic> deviations at time <italic>t-1</italic>. Right panel. The total deviation at <italic>t</italic>, in turn, can be decomposed into the recurrent and input deviations at the previous time steps (i.e. Equation S1 may be viewed as a recurrence relationship). Thus, the total deviation at <italic>t + 1</italic> can be decomposed into recurrent and input deviation histories (<inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively, where the bar represents the exponentially weighted sum of the history up to time <italic>t</italic>). The vectors associated with <bold><italic>ΔR</italic></bold> and <bold><italic>ΔI</italic></bold> are plotted at different time points for visualization purposes.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig6-figsupp1-v1"/></fig></fig-group><p>To better characterize how spectral noise invariance emerges during training, we measured network responses while naturally and artificially varying the structure of the spectral noise. Any confounding effects from temporal invariance were avoided by training and testing the network on temporally normalized utterances, by warping them to the duration of the template utterance for the respective digit. Responses to standard natural test utterances (those not presented during training) were contrasted with two types of artificially created utterances: those with shuffled spectral noise and orthogonal spectral noise (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Shuffled spectral noise was created by shuffling the PCA axes (basis) of the spectral noise (Materials and Methods). In other words, novel utterances were generated by reordering the directions of spectral noise in phase space (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). This procedure scrambled the relationships between input signal and noise, while constraining the artificial spectral noise to the same subspace as natural spectral noise. Orthogonal spectral noise was composed of linear bases that were orthogonal to natural spectral noise (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, see Materials and Methods). In addition to scrambling the input signal-noise relationships, this procedure also scrambled the relationships between input channels. Care was taken while constructing these artificial inputs, to ensure that the temporal structure, dimensionality and magnitude of the spectral noise were identical to that observed in natural utterances.</p><p>Measurements of the within-digit trajectory distances revealed a progressive increase from trained utterances, to untrained utterances, to utterances with shuffled noise, to utterances with orthogonal noise (<xref ref-type="fig" rid="fig6">Figure 6E</xref>). In contrast, the untrained (reservoir) network exhibited deviations that were large and fairly similar across all conditions, implying that the trained network was able to identify and suppress natural spectral noise present in the utterances, but not artificial noise of similar magnitude and structure. We further dissected these results with a novel analysis based on the decomposition of the RNN drive into its input and recurrent components, which allowed trajectories to be decomposed into the subspaces wherein the network integrates external (input subspace) and recurrent (recurrent subspace) inputs (Materials and Methods). The analysis showed that in untrained networks, both natural and artificial spectral noise introduced deviations in the recurrent subspace that were large and unsuppressed (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). In contrast, in trained RNNs the recurrent dynamics suppressed natural spectral noise, but not artificial noise (<xref ref-type="fig" rid="fig6">Figure 6G</xref>). Training also resulted in a recurrent subspace that suppressed spectral noise induced deviations in the input subspace. Whereas the untrained network integrated recurrent and external inputs in orthogonal subspaces, training the network rotated the recurrent subspace such that spectral noise induced trajectory deviations in the recurrent subspace were anti-correlated with the deviations in the input subspace, thereby resulting in their suppression (Recurrent-Input interaction in <xref ref-type="fig" rid="fig6">Figure 6F–G</xref>).</p><p>Together, these results suggest that recurrent plasticity may play a crucial role in identifying and suppressing the natural spatial variations of stimuli. In our model, this is achieved via supervised training with a common within-digit target trajectory, which exposes the spatiotemporal distribution of the spectral noise in the sensory input, and in doing so, allows for effective sampling from this distribution. Training then shapes the basins of attraction or hypertubes around the spoken digit-encoding trajectories based on this distribution, resulting in effective, recurrently-driven suppression of naturally occurring spectral noise.</p></sec><sec id="s2-5"><title>Encoding stimuli as neural trajectories allows for temporal generalization (Scaling)</title><p>As mentioned above, a signature and little understood feature of how the brain processes time-varying patterns pertains to temporal warping (temporal invariance), whereby temporally compressed or dilated input signals can be identified as the same pattern. Thus, an important test for computational models of sensory processing of time-varying stimuli, is whether they are able to account for temporal invariance. We hypothesized that one advantage of encoding time-varying stimuli as continuous neural trajectories, is that it naturally addresses the problem of temporal warping; specifically, that compressed or dilated stimuli generate similar neural trajectories that play out at different ‘speeds’ (<xref ref-type="bibr" rid="bib35">Lerner et al., 2014</xref>; <xref ref-type="bibr" rid="bib44">Mello et al., 2015</xref>). To test this hypothesis, we trained and tested an RNN on a dataset of temporally warped spoken digits.</p><p>Specifically, the input pattern, <bold><italic>y</italic></bold>(<italic>t</italic>), of a single utterance of each digit from a speaker of the TI-46 dataset was artificially stretched or compressed by a time warp factor α (<bold><italic>y</italic></bold>(<inline-formula><mml:math id="inf4"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula><italic>t)</italic>), while retaining its spatial structure. <xref ref-type="fig" rid="fig7">Figure 7A</xref> (left panel) shows the input structure for two utterances of the digit ‘nine’, one warped to twice (α = 2, 2x or 200% warp) and the other to half (α = 0.5, 0.5x or 50% warp) the duration of the original utterance.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.31134.011</object-id><label>Figure 7.</label><caption><title>Invariance of encoding trajectories to temporally warped spoken digits.</title><p>(<bold>A</bold>) Temporally warped input cochleograms for an utterance of the digit ‘nine’ (left panel), warped by a factor of 2x (upper row) and 0.5x (lower row). Distance matrices between the trajectories produced by the warped and reference stimuli for the two control networks and the sensorimotor trained network (right panels). Distance matrices are accompanied by the corresponding sonograms on each axis, and highlight the comparison between the sensory trajectories of the warped and reference utterance (outlined box). A deep blue (zero-valued) diagonal through a distance matrix indicates that the sensory trajectory encoding the warped input overlaps with the reference trajectory. Thus, the sensorimotor trained RNN is more invariant (deeper blue diagonal) to temporal warping of the input than the motor-trained and reservoir RNNs. (<bold>B</bold>) Mean time-averaged Euclidean distance between sensory trajectories encoding warped and reference utterances of the 10 digits in each of the three networks, over a range of warp factors. Dashed gray line indicates mean distance between sensory trajectories encoding warped and reference utterances of different digits in the sensorimotor trained network. Error bars (not visible) indicate standard errors of the mean over 10 test trials. Gray arrows indicate the warp factors at which the networks were trained. (<bold>C</bold>) Transcription performance (measured by the deep CNN classifier) of the three networks on utterances warped over a range of warp factors. The results corroborate the measurements in (<bold>B</bold>) and show that a sensorimotor trained RNN is more resistant to temporal warping. All networks were composed of 2100 units. Network training was performed with 30 utterances (one subject, 10 digits, one utterance per digit, three warps per utterance). <italic>I<sub>0</sub></italic> was set to 0.25 during network training, and to 0.05 during output training and testing. Reference trajectories for the distance analysis were recorded with <italic>I<sub>0</sub></italic> set to 0.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig7-v1"/></fig><p>The RNN was trained on three warped utterances of each digit (warp factors of 0.7, 1 and 1.4) and tested on warp factors in the range 0.5 to 2 (a four-fold factor that approximates the natural speed variation of speech). To discern the effect of a temporally warped input on the encoding trajectory, we constructed a matrix of the distances between trajectories produced by the reference (α = 1) and a warped utterance (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). Each element of such a matrix measures the distance between the two trajectories at a corresponding pair of time points. In the case of perfect warping (e.g., when the reference trajectory and the trajectory at an α≠1 overlap exactly in phase space), a diagonal line of zero distances (deep blue) would be observed during the sensory epoch (shaded region), with slope proportional to the warp factor. Trajectories produced by temporally warped inputs in the sensorimotor trained RNN were closer to the reference compared to their counterparts in the reservoir or motor-trained RNN. In other words, tuning the recurrent weights dramatically improved the ability of the network to reproduce the same neural trajectories, despite being driven by inputs at different speeds. We quantified this effect by measuring the average distance between the reference and warped trajectories during the sensory epoch over different warp factors (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). In comparison to the two control networks, the trained RNN maintains a much smaller distance between trajectories across the four-fold range of temporal warping—including warp factors outside the range used during training. These results confirm the hypothesis that training produces a modulation of the internal dynamics by the external input that renders the encoding trajectories invariant to temporal warping of the input patterns. The impact of this result is borne out in the network’s performance on the digit transcription task, as measured by the CNN classifier (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). Training during the sensory and motor epochs dramatically improves ‘extrapolation’—that is generalization to speeds outside the range of training speeds; however, training during the motor epoch alone is sufficient to produce very good ‘interpolation’ (novel speeds within the training set range) generalization.</p></sec><sec id="s2-6"><title>Mechanisms underlying temporal generalization (temporal scaling)</title><p>It is surprising that a RNN with strong recurrent dynamics can represent temporally scaled stimuli with similar trajectories. Specifically, the strong intrinsic dynamics of the network (governed by the recurrent weights) must somehow match the temporal scale of the input. One possibility is that the network achieves this by scaling the linear speed of its trajectories (i.e. the magnitude of <inline-formula><mml:math id="inf5"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) in inverse proportion to the warp factor. However, a comparison of the time-averaged linear speed of trajectories encoding digits at different warps deviates from this relationship in both the untrained and trained networks (<xref ref-type="fig" rid="fig8">Figure 8A</xref>), which, in contrast to our earlier results (<xref ref-type="fig" rid="fig4">Figures 4A</xref> and <xref ref-type="fig" rid="fig7">7A</xref>), suggests that the trajectories may not be temporally invariant. This contradiction arises from the incorrect assumption that the trajectories have little or no curvature. <xref ref-type="fig" rid="fig8">Figure 8B</xref> schematizes the two alternatives for temporal scaling of trajectories with curvature: constant speed/variable distance traversed, versus variable speed/constant distance traversed. At one extreme, a reference trajectory (black curve) can be slowed down while conserving its linear speed by increasing its radius of curvature, and therefore the distance it traverses through phase space (green curve). At the other extreme, the reference may be slowed by appropriately scaling down its linear speed while conserving its radius of curvature (yellow curve), thereby generating temporally invariant trajectories that traverse identical distances.</p><fig-group><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.31134.012</object-id><label>Figure 8.</label><caption><title>Mechanism of temporal scaling invariance.</title><p>(<bold>A</bold>) Time-averaged linear speed (<bold>v</bold>) during the sensory epoch trajectories in the reservoir and trained networks (N = 2100) compared to the ideal linear speed, over a range of warp factors. The speeds are normalized to the 1x warp. (<bold>B</bold>) Schematic representation of two hypotheses for trajectories that exhibit temporal warp invariance via warp-dependent angular speeds (ω): constant linear speed/variable distance (green) and variable linear speed/constant distance (yellow) (<bold>C</bold>) Projections in PCA space of external input for warped utterances of digit one (left) and the corresponding sensory epoch trajectories of the trained network (right), over a 100 ms duration of the 1x warp trajectory and the corresponding utterance interval of the remaining trajectories. Over this short duration, over 90% of the variance in the external input and the trajectories was captured. (<bold>D</bold>) Mean linear speed (left) and cumulative distance traversed (time integral of the linear speed; right) in the recurrent subspace of phase space of a reservoir and trained network, compared to predictions from the constant speed and constant distance hypotheses. The measures are normalized to the 1x warp. In all panels error bars indicate SEM over the digits. The network was trained with <italic>I<sub>0</sub></italic> set to 0.25, and trajectories were recorded with <italic>I<sub>0</sub></italic> set to 0.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig8-v1"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31134.013</object-id><label>Figure 8—figure supplement 1.</label><caption><title>Phase space relationships in the input and recurrent subspaces as a function of the warp factor.</title><p>(<bold>A</bold>) Projections in PCA space of the input (left) and recurrent (right) subspace trajectories that compose the sensory epoch trajectories of a trained network (N = 2100), in response to warped utterances of digit one, over a 100 ms duration of the 1x warp trajectory and the corresponding utterance interval of the remaining trajectories. The contribution of the leaky integrated external inputs to the overall trajectory is termed the input subspace trajectory (<inline-formula><mml:math id="inf6"><mml:mi>b</mml:mi><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ7">Equation 4</xref>), and that of the recurrent dynamics is termed the recurrent subspace trajectory (<inline-formula><mml:math id="inf7"><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo> <mml:mi/><mml:mi>b</mml:mi><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ7">Equation 4</xref>). While the external inputs are originally inseparable in phase space (<xref ref-type="fig" rid="fig8">Figure 8C</xref>), following integration by the RNN, these trajectories appear to separate out into parallel trajectories. A similar separation is observed in the recurrent subspace. (<bold>B</bold>) Time-averaged linear speed of sensory epoch trajectories in the input subspace of phase space where the inputs are leaky integrated (as in (<bold>A</bold>)), in comparison to the linear speed if the inputs were perfectly integrated. Whereas perfect integration produces constant speed trajectories (<xref ref-type="fig" rid="fig8">Figure 8B</xref>), leaky integration instead results in trajectories that are a balance of constant speed and constant distance. The speeds are normalized to the 1x warp. Error bars indicate standard errors of the mean over the digits. (<bold>C</bold>) Time-averaged within-digit separation of reservoir and trained sensory epoch trajectories at each warp factor, from the respective 1x warp trajectory, along a common time-dependent direction (left), establishes that they are parallel. Following temporal alignment, at each time step, the separation between a test trajectory and the corresponding 1x warp trajectory is projected onto the direction of separation between the 0.7x and 1x warp trajectories (solid), or the 1.4x and 1x warp trajectories (dashed), depending on the input warp factor for the test trajectory (directions indicated by arrows in (<bold>A</bold>)). Monotonically increasing separation from the 1x warp as a function of the input warp factor establishes that the trajectories are parallel over the manifold. The separation along these manifolds explains a majority of the variance in the overall separation of trajectories from the 1x warp trajectory (right), although the explained variance is lowest for the recurrent dynamics of the reservoir owing to its partially chaotic dynamics. Error bars indicate standard errors of the mean over the digits. (<bold>D</bold>) Mean range of recurrent unit firing rates (defined as <inline-formula><mml:math id="inf8"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">mean</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) during sensory epoch trajectories of the trained network, over a range of warp factors. Error bars indicate standard errors of the mean over the digits. The network was trained with <italic>I<sub>0</sub></italic> set to 0.25, and trajectories were recorded with <italic>I<sub>0</sub></italic> set to 0.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig8-figsupp1-v1"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31134.014</object-id><label>Figure 8—figure supplement 2.</label><caption><title>Temporal invariance performance and mechanism in a network trained with Backpropagation through time (BPTT).</title><p>(<bold>A</bold>) Mean time-averaged Euclidean distance between sensory trajectories encoding warped and reference utterances of the 10 digits, in a network trained with BPTT, over a range of warp factors. Dashed gray line indicates mean distance between sensory trajectories encoding warped and reference utterances of different digits. Error bars indicate standard errors of the mean over 10 test trials. End-to-end training of a 2100 unit network was performed on the transcription task with the BPTT algorithm applied via the ADAM optimizer (Materials and Methods), with a dataset of 30 artificially warped utterances as in <xref ref-type="fig" rid="fig7">Figures 7</xref>–<xref ref-type="fig" rid="fig8">8</xref> (one subject, 10 digits, one utterance per digit, three warps per utterance). <italic>I<sub>0</sub></italic> was set to 0.25 during network training. Gray arrows indicate the warp factors at which the network was trained. (<bold>B</bold>) Transcription performance of the network (measured by the deep CNN classifier) on utterances warped over a range of warp factors. Error bars indicate standard errors of the mean over 10 test trials. The network exhibits good interpolation and extrapolation performance, but its overall performance is weaker than the network trained with ‘innate’ learning (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). (<bold>C</bold>) Time-averaged linear speed of the sensory epoch trajectories in the BPTT trained network compared to the ideal speed, over a range of warp factors. The speeds are normalized to the 1x warp. (<bold>D</bold>) Mean range of recurrent unit firing rates (defined as <inline-formula><mml:math id="inf9"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">mean</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) during sensory epoch trajectories of the network, over a range of warp factors. Error bars indicate standard errors of the mean over the digits. (<bold>E</bold>) Mean linear speed (top) and cumulative distance traversed (time integral of the linear speed; bottom) in the recurrent subspace of phase space of the network, compared to predictions from the constant speed and constant distance hypotheses. The measures are normalized to the 1x warp. Error bars indicate SEM over the digits. (<bold>F</bold>) Projections in PCA space of the input (left) and recurrent (right) subspace trajectories that compose the sensory epoch trajectories of the trained network, in response to warped utterances of digit one, over a 100 ms duration of the 1x warp trajectory and the corresponding utterance interval of the remaining trajectories. (<bold>G</bold>) Time-averaged within-digit separation of the trained sensory epoch trajectories at each warp factor, from the respective 1x warp trajectory, along a common time-dependent direction (left), establishes that they are parallel. Following temporal alignment, at each time step, the separation between a test trajectory and the corresponding 1x warp trajectory is projected onto the direction of separation between the 0.7x and 1x warp trajectories (solid), or the 1.4x and 1x warp trajectories (dashed), depending on the input warp factor for the test trajectory (directions indicated by arrows in (<bold>F</bold>)). Monotonically increasing separation from the 1x warp as a function of the input warp factor establishes that the trajectories are parallel over the manifold. The separation along these manifolds explains a majority of the variance in the overall separation of trajectories from the 1x warp trajectory (right). Error bars indicate SEM over the digits. Network performance (panels A and B) was evaluated with <italic>I<sub>0</sub></italic> set to 0.05. Reference trajectories for the distance analysis (<bold>A</bold>) were recorded with <italic>I<sub>0</sub></italic> set to 0. Trajectories for panels (<bold>E</bold>) thru (<bold>F</bold>) were also recorded with <italic>I<sub>0</sub></italic> set to 0.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31134-fig8-figsupp2-v1"/></fig></fig-group><p>Temporally-scaled utterances produce overlapping trajectories in PCA subspace (<xref ref-type="fig" rid="fig8">Figure 8C</xref>, left panel)—reflecting the scaling of their linear velocities while traversing constant distances (a direct consequence of how the artificially warped utterances were constructed). In contrast, the RNN trajectories produced by these scaled utterances are spatially distinct with warp-dependent separations (<xref ref-type="fig" rid="fig8">Figure 8C</xref>, right panel). Moreover, they exhibit warp-dependent curvature, with slower trajectories incurring larger curvature radii. Together with the results in <xref ref-type="fig" rid="fig8">Figure 8A</xref>, this suggests that the mechanisms underlying temporal scaling in our model fall between the two extreme hypotheses, implying that: (i) that the average linear speed and curvature radius of trajectories encoding utterances at different warps adjust in a manner that appropriately modulates their angular speed and therefore their timescale; and (ii) that the trajectories are parallel to each other, demonstrating that they encode the digit similarly. <xref ref-type="fig" rid="fig8">Figure 8D</xref> presents measurements of the time-averaged linear speed and the total distance traversed in the recurrent subspace of phase space, by the trajectories in the trained and untrained networks over a range of warps. The latter is a heuristic measure of the time-averaged curvature radius, hinging on the fact that the circumference (or distance traversed) linearly scales with the radius. A comparison of these measurements for the two networks to the expected values in constant speed- and constant distance trajectories, confirms that both networks produce trajectories that lie in between these two extremes. That is, temporal scaling is achieved by an approximate balance of both hypotheses, e.g., at low speeds (digits spoken slowly) the trajectories are slower and longer. Furthermore, training alters this balance to reduce the within-digit separation, by modulating the trajectory speeds more strongly. Measurements to establish whether the trajectories are parallel also agree with these results (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1C</xref>).</p><p>But how do identical inputs (except for their duration) generate these parallel trajectories (e.g., <xref ref-type="fig" rid="fig8">Figure 8C</xref>)? To answer this question, we first measured the relationship between the input and recurrent subspaces in the trained network, and discovered that they are orthogonal to one another (data not shown), implying that the integration of the external inputs is independent of the integration of the recurrent inputs. In the absence of subspace interactions, it is easy to see that external inputs with different speeds are integrated into spatially distinct input subspace trajectories (e.g. <inline-formula><mml:math id="inf10"><mml:mrow><mml:mo>∫</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, wherein high speed/frequency signals integrate to produce trajectories with small magnitudes) (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1A</xref>). As with the recurrent subspace trajectories, the resulting input subspace trajectories are parallel to each other and modulate their angular speed via a combination of changes to their curvature radii and linear speed (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1A–C</xref>). Ultimately, the differentiated curvature and linear velocity that modulate the angular velocity and therefore the timescales of recurrent subspace trajectories, are directly shaped by these input subspace dynamics (<xref ref-type="bibr" rid="bib55">Rajan et al., 2010a</xref>).</p><p>To determine if this solution to the temporal invariance problem is specific to the ‘innate’ training algorithm, or likely a general property of trained RNNs, we trained a RNN with a gradient-descent approach in which the error was based solely on the output units (Materials and Methods, <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). While the interpolation and extrapolation performance of this network was inferior to the network trained with ‘innate’ learning (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2B</xref>), the underlying mechanism was the same: warped utterances were encoded along parallel trajectories (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2F–G</xref>) with warp-dependent curvature and linear velocity (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2E</xref>) that subserved temporal scaling. Thus, two very different training algorithms resulted in similar solutions to the temporal invariance problem.</p><p>In summary, these results demonstrate that the integration of inputs at different warps separates out the resulting trajectories in the input subspace, and consequently in the recurrent subspace. In both subspaces the network dynamics generate trajectories with warp-dependent linear velocities and curvature, which appropriately modulate their angular velocity, resulting in them traversing spatially distinct yet parallel paths at warp-dependent timescales. This confirms the hypothesis that an inherent computational advantage of encoding time-varying sensory stimuli in neural trajectories is that it can naturally address the temporal invariance problem.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The brain naturally encodes, recognizes, and generates complex time-varying patterns, and can seamlessly process temporally scaled patterns. Despite our poor understanding of these processes, theoretical evidence increasingly suggests that the dynamics of recurrently connected circuits in the brain are critical to representing time-varying patterns. For example, so-called reservoir computing approaches propose that complex high-dimensional spatiotemporal patterns are represented in the dynamics inherent to randomly connected recurrent neural networks (<xref ref-type="bibr" rid="bib26">Jaeger and Haas, 2004</xref>; <xref ref-type="bibr" rid="bib7">Buonomano and Maass, 2009</xref>; <xref ref-type="bibr" rid="bib61">Sussillo and Abbott, 2009</xref>). A shortcoming of this approach, however, is that the recurrent connections in these networks are not plastic; thus in contrast to actual cortical circuits, the random recurrent neural network (the ‘reservoir’) does not adapt or optimize to the task at hand. Additionally, fully tapping into the computational potential of randomly connected RNNs has proven difficult because they are susceptible to chaos (<xref ref-type="bibr" rid="bib60">Sompolinsky et al., 1988</xref>; <xref ref-type="bibr" rid="bib65">Wallace et al., 2013</xref>). However, progress has been made on both accounts (<xref ref-type="bibr" rid="bib42">Martens and Sutskever, 2011</xref>; <xref ref-type="bibr" rid="bib63">Vogels et al., 2011</xref>; <xref ref-type="bibr" rid="bib30">Laje and Buonomano, 2013</xref>). Here, we extend these results to better understand well-known aspects of the brain’s ability to represent time-varying sensory <italic>and</italic> motor patterns—we demonstrate how experience-dependent plasticity could reshape the dynamics of reservoir RNNs to qualitatively improve representations by endowing them with such essential properties as generalization to novel exemplars, sensory-motor association and transformation, and temporal invariance.</p><p>Previous models of temporal scaling have enlisted neural and synaptic dynamics to achieve temporal invariance of both sensory stimuli (<xref ref-type="bibr" rid="bib19">Gütig and Sompolinsky, 2009</xref>) and the generation of motor responses (<xref ref-type="bibr" rid="bib48">Murray and Escola, 2017</xref>). In the model proposed by Gutig and Sompolinsky, temporally invariant recognition of compressed and dilated sensory stimuli is based on a synaptic shunting mechanism, wherein the effective integration time of a post-synaptic cell is modulated by the total conductance of its afferent synapses, which changes as a monotonic function of the input warp—thus temporal invariance is ultimately reduced to a cellular property. In contrast, in the model described here temporal invariance emerges from a network property: the formation of parallel neural trajectories traversed at different speeds. While the formation of such trajectories may be surprising given the sensitivity and nonlinear nature of RNNs, we show that this capacity is the consequence of scale-dependent angular velocities of the encoding trajectories. This property of RNN encoding, which is observable to a lesser extent even in reservoir networks (<xref ref-type="fig" rid="fig8">Figure 8D</xref>), stems from its leaky integration of scaled inputs (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1B</xref>) and the input-driven suppression of its chaotic recurrent activity (<xref ref-type="bibr" rid="bib55">Rajan et al., 2010a</xref>). Indeed, network training enhances this property by suppressing recurrent amplification of scale-driven deviations in the encoding trajectories, producing proximal, parallel within-digit trajectories with linear velocities that are more strongly scale-dependent (<xref ref-type="fig" rid="fig8">Figure 8D</xref>, <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1C</xref>, <xref ref-type="fig" rid="fig8s2">2E and G</xref>). The effects of such a suppression are evident even in networks subjected to motor-only training, where we observe a considerable decrease in the within-digit distances (<xref ref-type="fig" rid="fig7">Figure 7B</xref>) and consequently strong interpolation generalization (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). Nevertheless, it remains the case that training across both the sensory and motor epochs, not only improves interpolation and extrapolation generalization, but also strongly enhances cross-utterance and cross-speaker generalization (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>Our results also demonstrate how training an RNN reveals a computationally powerful pattern recognition regime, one that helps generalize the encoding of learned time-varying patterns to novel exemplars. As stated earlier, recognizing different instances of the same digit as one and the same, while discriminating between different digits requires that networks actively and differentially sculpt the within and between class trajectories—the separation between trajectories representing similar patterns must be kept to a minimum, while that between trajectories representing different patterns must be amplified. A previous study has analytically shown that untrained random recurrent networks (reservoir networks) process inputs in one of three computational regimes: a chaotic regime, wherein any separation between trajectories is strongly amplified by the network’s chaotic dynamics; a regime with weak recurrent weights that is input-dominated with little computational power; and a critical regime wherein the input and internal dynamics are balanced (<xref ref-type="bibr" rid="bib6">Bertschinger and Natschläger, 2004</xref>). Here, we show that training alters these regimes in very different ways (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Crucially, relative to the separation between the input patterns, reservoir networks in the critical regime amplify the separation between trajectories representing similar patterns more strongly than between different patterns. In contrast, trained networks separate trajectories according to the patterns they are representing (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Tuning the recurrent weights reshapes the internal dynamics of the network, enabling it to redirect or suppress deviations from the target trajectories (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Ultimately, it is this property of trained networks that allows them to encode complex sensory patterns and effectively generalize across natural utterances and speakers.</p><p>Three experimentally tractable predictions emerge from the above results. First, neural trajectories will be stable in response to local perturbations potentially administered through optogenetic stimulation. Importantly, and in agreement with an earlier analysis (<xref ref-type="bibr" rid="bib55">Rajan et al., 2010a</xref>), the neural trajectories elicited during sensory stimuli will be more resistant to perturbations than the neural trajectories unfolding during the motor epoch of sensory-motor tasks. A second prediction, which to the best of our knowledge is a novel one, relates to the structure of noise. Specifically, trained networks fail at generalizing to unfamiliar spectral noise patterns, thereby generating the prediction that trajectories in sensory areas will diverge more when presented stimuli composed of artificial or unfamiliar spectral noise patterns. This would indicate that the stimulus-response function of complex sensory neurons are not just sensitive to natural stimulus features (<xref ref-type="bibr" rid="bib62">Theunissen and Elie, 2014</xref>), but also natural noise features. To test this, an animal could be trained to recognize natural sounds with noise injected along some principal components of the stimulus spectrogram but not others, and then tested on noise injected along the held-out PCA dimensions. A comparison of the trajectories encoding trained and tested sounds should reveal a larger divergence in the neural encoding of the test sounds. Third, and most specifically, the model predicts a clear neural correlate of the encoding of temporally scaled stimuli—the observation that slower stimuli yield trajectories with larger curvature radii implies that the neural population activity should exhibit larger fluctuations in their firing rates in response to slower stimuli; In other words, at the neuronal level the range of minimal to maximal firing rate should be larger (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1D</xref> and <xref ref-type="fig" rid="fig8s2">2D</xref>).</p><p>These experimental predictions are critical to validate the model’s implementation of complex and invariant sensorimotor computations as stable neural trajectories. However, even if validated a number questions remain to be addressed. Most notably, how can the recurrent synaptic strengths be tuned to develop stable trajectories in a biologically plausible manner? The learning rule used here, coupled with its requirement of an explicit target for each recurrent unit, make it biologically implausible. However, while it is important that the sensorimotor representations are encoded as locally stable trajectories, the structure of the target trajectories themselves are essentially arbitrary. It is therefore conceivable that arbitrary yet locally-stable encoding trajectories may emerge from unsupervised learning. A second related issue is that to achieve strong temporal invariance, our model had to be trained over a range of sensory stimuli speeds. Again, it is not clear if this would represent a biologically plausible scenario—to help address this question, it will be important for future research to determine if the ability to recognize stimuli independent of speed is learned through experience. Finally, questions relating to the learning capacity of networks capable of strong temporal invariance, and the expedience of sensory-epoch training for temporal invariance remain open.</p><p>The computational potential of continuous time RNNs in high-gain regimes has long been recognized, but it has been challenging to tap into this potential because of their inherently chaotic behavior and the difficulties in training them (<xref ref-type="bibr" rid="bib5">Bengio et al., 1994</xref>; <xref ref-type="bibr" rid="bib51">Pearlmutter, 1995</xref>). Step-by-step, progress has been made on how to capture the computational potential of RNNs (<xref ref-type="bibr" rid="bib26">Jaeger and Haas, 2004</xref>; <xref ref-type="bibr" rid="bib61">Sussillo and Abbott, 2009</xref>; <xref ref-type="bibr" rid="bib42">Martens and Sutskever, 2011</xref>; <xref ref-type="bibr" rid="bib30">Laje and Buonomano, 2013</xref>). Here, we establish that a further feature of trained RNNs is their ability to not only encode spatiotemporal objects, but also to perform complex sensorimotor tasks and address the long-standing problem of temporal warping. These results are consistent with the proposal that that spatiotemporal objects are not encoded as fixed-point attractors, but as locally stable neural trajectories (dynamic attractors).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Network model</title><p>The dynamics of the RNN was comprised of <italic>N</italic> nonlinear continuous-time firing rate units modeled as:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msubsup><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula><italic>x<sub>i</sub></italic> represents the state of neuron <italic>i</italic>, and <italic>r<sub>i</sub></italic> its ‘firing rate’. The time constant, <inline-formula><mml:math id="inf11"><mml:mi>τ</mml:mi></mml:math></inline-formula>, of each unit was set to 25 ms. <bold><italic>W<sup>R</sup></italic></bold> is the weight matrix representing the recurrent connectivity of the network. The recurrent connectivity was uniformly random (but with no autapses) with a connection probability (<italic>p<sub>c</sub></italic>) of 0.2. The weights of these synapses were initialized from an independent Gaussian distribution with zero mean and SD equal to <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>g</mml:mi><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mi>N</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>, where <italic>g</italic> represents the ‘gain’ of the network. RNNs were initialized to a high-gain regime (<italic>g</italic> = 1.6), which generates chaotic self-perpetuating activity in the absence of external input or noise (<xref ref-type="bibr" rid="bib60">Sompolinsky et al., 1988</xref>).</p><p>The M-dimensional vector <bold><italic>y(t)</italic></bold> represents the time-varying sensory input to the RNN. The fixed input weight matrix, <bold><italic>W<sup>I</sup></italic></bold>, tonotopically projects this input vector onto the RNN—input channel <italic>k</italic> is projected onto units (k-1)×N/M + 1 thru k × N/M. The weights of these connections were drawn from an independent Gaussian distribution with zero mean and unit variance. <bold><italic>W<sup>I</sup> y(t)</italic></bold> represents the net external input to each RNN. The handwriting was modeled with three output units (<italic>o<sub>1</sub></italic>, <italic>o<sub>2</sub></italic>, <italic>o<sub>3</sub></italic>) that represented the x, y and z co-ordinates of a pen on paper. Finally, each unit of the RNN also received an independent background noise current, <bold><italic>I<sup>noise</sup></italic></bold>, modeled as additive Gaussian white noise with SD <italic>I<sub>0</sub></italic>.</p><p>As is standard, the output neurons were simulated as a weighted linear sum of all the units in the RNN:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>O</mml:mi></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula>where the output weight matrix, <bold><italic>W<sup>o</sup></italic></bold>, was initialized from an independent Gaussian distribution with zero mean and SD <inline-formula><mml:math id="inf13"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. All simulations were performed with a time step of 1 ms.</p></sec><sec id="s4-2"><title>Simulations and training</title><p>Each trial consisted of a time window comprised of a sensory and a motor epoch. We defined the sensory epoch as the period in which an external stimulus is presented—starting at <italic>t</italic> = 0 and lasting the duration of the utterance. The motor epoch was defined as a period beginning 300 ms after the sensory epoch ended and lasting the duration of the target motor pattern (the appropriate handwritten digit).</p><p>Training proceeded in three steps (described in detail below): (1) target trajectories were generated for each utterance of each digit in the training subset of the dataset; (2) the recurrent units were trained to reproduce the target trajectories; and (3) the output units were trained to produce the handwritten spatiotemporal patterns. The trained network was then tested on novel (and trained) utterances. In steps 2, 3 and during testing, each trial began at <italic>t</italic>=-100 ms with the network initialized to a random state (<italic>x<sub>i</sub></italic> values drawn from a uniform distribution between −1 and 1). In <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig7">7</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> and <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2A–2B</xref>, testing was performed with the noise amplitude (<italic>I<sub>0</sub></italic>) set to 0.05. For the perturbation analysis (<xref ref-type="fig" rid="fig2">Figure 2</xref>), a 25 ms ‘perturbation pulse’ was introduced during the sensory or motor epoch of each trial, with <italic>I<sub>0</sub></italic> set to the desired perturbation magnitude for the duration of the pulse. Noise was omitted in these simulations to allow for a direct assessment of the effects of the perturbation pulse. Similarly, for the simulations in <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6</xref>, <xref ref-type="fig" rid="fig8">Figure 8</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1</xref> and <xref ref-type="fig" rid="fig8s2">2C</xref>–<xref ref-type="fig" rid="fig8s2">2G</xref>, noise was omitted (<italic>I<sub>0</sub></italic> = 0) so that the impact of training on the generalization and discriminability of an RNN’s encodings, but not its background noise invariance, could be direct evaluated.</p><p>Simulations of the untrained ‘reservoir’ control network skipped steps 1 and 2, while simulations of the motor-trained control network limited recurrent network training (step 2) to a duration starting 150 ms after the end of the sensory epoch and lasting until the end of the motor epoch.</p></sec><sec id="s4-3"><title>Input structure</title><p>We used spoken digits from the TI-46 spoken word corpus (<xref ref-type="bibr" rid="bib41">Mark Liberman et al., 1993</xref>) to train and test networks on the transcription task. Specifically, our dataset was composed of the spoken digits ‘zero’ thru ‘nine’, uttered 10 times each, by each of five female subjects. Spoken digits from the corpus were decoded, end-pointed, resampled to 12 kHz, and converted to spectrograms with Matlab’s specgram function. The spectrograms were preprocessed with Lyon’s passive ear model of the human cochlea (<xref ref-type="bibr" rid="bib38">Lyon, 1982</xref>), as implemented by the auditory toolbox (Malcolm Slaney), to generate analog ‘cochleograms’ composed of 12 analog frequency bands, or channels, ranging from 0 to 6 kHz. Finally, the cochleograms were smoothed with a 20<sup>th</sup> order 1D median filter (Matlab’s medfilt1 function), normalized to a maximum of 1 (i.e., they were normalized by the maximal value of all utterances and digits), and scaled by an input amplitude. During the sensory epoch of each trial, the input, <bold><italic>y(t)</italic></bold> (M = 12), took values from the cochleogram corresponding to the utterance that was to be presented to the RNN. At all other times of the trial, <bold><italic>y(t)</italic></bold> was set to 0. The input amplitude was set to five in all simulations except in <xref ref-type="fig" rid="fig5">Figure 5</xref>, where it was parametrically varied. For the temporal warping simulations (<xref ref-type="fig" rid="fig7">Figures 7</xref>–<xref ref-type="fig" rid="fig8">8</xref>, <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1</xref>–<xref ref-type="fig" rid="fig8s2">2</xref>) the cochleograms were compressed or dilated by the warping factor <italic>α</italic> through linear interpolation.</p></sec><sec id="s4-4"><title>Innate trajectories and RNN training</title><p>Training was performed with the ‘innate-learning’ approach—which uses the Recursive Least Squares (RLS) rule to train each unit of the recurrent network to match the pattern generated by the untrained network (<xref ref-type="bibr" rid="bib30">Laje and Buonomano, 2013</xref>). For each subject used in the training set, a single template utterance of each digit was presented to the untrained RNN (the reservoir) in the absence of background noise, and the resulting trajectory served as a target (‘innate’) trajectory. The template utterance of each digit was chosen as one with the median duration among all the utterances of the digit by the subject. Target sensory epoch trajectories for other training utterances of the digit by the same subject were generated by linearly warping this innate trajectory to match the utterance durations.</p><p>To achieve the formation of digit-specific dynamic attractors, a single target motor trajectory was adopted across subjects and utterances for each digit. This target, comprising of the sensory-to-motor transition and motor epoch population activity, was generated from the template utterance by a single arbitrarily chosen subject, by allowing the untrained RNN to autonomously generate activity following template stimulus offset, in the absence of background noise, for a duration equal to the sum of the sensory-to-motor transition duration (300 ms) and the motor epoch duration for the digit. Sensorimotor targets for each training utterance were composed by concatenating the common motor target to the end of the corresponding sensory targets. The initial network state was chosen at random while harvesting the target trajectory for each digit. Target trajectories for template utterances of the same digit by different speakers were harvested starting the network at the same initial state.</p><p>All networks were trained with three utterances of each digit for each of the subjects in the training set. The networks in <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> were trained on three subjects and tested on five, while those in <xref ref-type="fig" rid="fig2">Figures 2</xref>–<xref ref-type="fig" rid="fig8">8</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1</xref>–<xref ref-type="fig" rid="fig8s2">2</xref> were trained and tested on one subject. The network size, <italic>N</italic>, strongly impacted its capacity, particularly for generalization. Accordingly, we trained larger networks to robustly address cross-speaker generalization (<italic>N</italic> = 4000) relative to those trained for cross-utterance generalization within a single speaker (<italic>N</italic> = 2100).</p><p>Network training was performed by modifying the recurrent weight matrix, <bold><italic>W<sup>R</sup></italic></bold>, with the Recursive Least Squares learning rule (<xref ref-type="bibr" rid="bib20">Haykin, 2002</xref>). The rule was simultaneously applied to 90% of the units in the network (randomly selected). Training was conducted by iterating through all utterances of the digits in the training set over multiple trials, starting each trial at a random initial condition and continuously injecting the network with background noise (<bold><italic>I<sup>noise</sup></italic></bold>). Training concluded when the error in the activity of the rate units asymptoted (generally between 100 and 150 trials).</p></sec><sec id="s4-5"><title>Output training</title><p>The spatiotemporal target patterns that comprise the handwritten digits were sampled from a Wacom Bamboo Pen Tablet, as the digits ‘0’ thru ‘9’ were individually stenciled on it. For each handwritten digit, the x and y coordinates of the pen were sampled at approximately 50 Hz, low-pass filtered, and resampled with interpolation to 1 kHz (corresponding to the 1 ms simulation time step). The target values for <italic>o<sub>1</sub></italic> and <italic>o<sub>2</sub></italic> were set to 0 from the start of each trial until the beginning of the motor epoch. During the motor epoch, they were set to the pen’s 2D coordinates for the corresponding digit, and reset to 0 between the end of the motor epoch and the end of the trial. The target for <italic>o<sub>3</sub></italic> (z co-ordinate) was a step function, set to one during the motor epoch and 0 at all other times. To train the output units, the Recursive Least Squares learning rule was applied to the readout weights in <bold><italic>W<sup>O</sup></italic></bold> (<xref ref-type="bibr" rid="bib20">Haykin, 2002</xref>; <xref ref-type="bibr" rid="bib26">Jaeger and Haas, 2004</xref>; <xref ref-type="bibr" rid="bib61">Sussillo and Abbott, 2009</xref>; <xref ref-type="bibr" rid="bib30">Laje and Buonomano, 2013</xref>). Output training was performed for 25 trials per utterance of each digit in the training set, while the RNN was continuously injected with background noise.</p><p>In each test trial, the motor output was recorded from the values of <italic>o<sub>1</sub></italic> and <italic>o<sub>2</sub></italic>, whenever the value of <italic>o<sub>3</sub></italic> was greater than 0.5 (i.e. when ‘the pen contacted the paper’). At the end of the trial, a 28 × 28 pixel grayscale image of the ‘handwritten’ output (pen width = 2 pixels) was labeled as the transcription for the corresponding digit. An objective determination of the transcription’s accuracy was made by comparing this label to one assigned to the image by a CNN classifier for handwritten digits. This classification was performed with the LeNet-5 CNN classifier (<xref ref-type="bibr" rid="bib33">Lecun et al., 1998</xref>) implemented with the Caffe deep learning framework (<xref ref-type="bibr" rid="bib27">Jia et al., 2014</xref>). The CNN was first trained on the MNIST database of handwritten digits (<xref ref-type="bibr" rid="bib34">LeCun et al., 1998</xref>), and its output layer was then fine-tuned on the stenciled digits that we used as the output targets.</p></sec><sec id="s4-6"><title>Gradient descent training</title><p>In <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>, we evaluate the mechanism underlying temporal scaling in a network trained with gradient descent. The recurrent rate units and output units were modeled as in <xref ref-type="disp-formula" rid="equ1 equ2 equ3">Equations 1-3</xref>, with weights of the input and output units initialized as in the reservoir network. Target values for the output units were assigned as described above. No target values were specified for the recurrent rate unit activity. Instead, all input, recurrent and output weights of the network were updated at the end of each training trial, with weight update values for the trial determined based on the squared-error of the outputs averaged across time and output units. Specifically, the error was transformed into weight updates via the backpropagation through time (BPTT) algorithm applied via ADAM optimization (<xref ref-type="bibr" rid="bib29">Kingma and Ba, 2014</xref>). To evade the vanishing and exploding gradients problem that commonly hinder gradient descent training of RNNs, <bold><italic>W<sup>R</sup></italic></bold> was initialized as an all-to-all orthogonal matrix of random values (<xref ref-type="bibr" rid="bib31">Le et al., 2015</xref>), and gradient magnitudes were restricted via gradient clipping (<xref ref-type="bibr" rid="bib18">Graves, 2013</xref>). Furthermore, <bold><italic>W<sup>R</sup></italic></bold> was initialized to high-gain regime (<italic>g</italic> = 1.6), as this lead to faster convergence (5000 trials per utterance). The network was trained with continuous injection of background noise. Training was performed with the Tensorflow library (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>), and output performance was evaluated with the CNN classifier described above.</p></sec><sec id="s4-7"><title>Trajectory analysis</title><p>In <xref ref-type="fig" rid="fig4">Figures 4</xref>–<xref ref-type="fig" rid="fig5">5</xref>, the sensory (motor) epoch within-digit distances were calculated from the Euclidean distance, at each time step, between the sensory (motor) epoch trajectory for each utterance, and its nearest neighbor from among the trajectories produced by the training utterances of the same digit. Similarly, the sensory (motor) epoch between-digit distances were calculated from the Euclidean distance, at each time step, between the sensory (motor) epoch trajectory for each utterance, and its nearest neighbor from among the sensory (motor) epoch trajectories encoding training utterances of all other digits. Similarly, in <xref ref-type="fig" rid="fig6">Figure 6E</xref>, sensory epoch within-digit distances were calculated as the Euclidean distance between the sensory epoch trajectory for each tested external input, and the one encoding the template utterance. Finally, trajectory distances in <xref ref-type="fig" rid="fig7">Figure 7B</xref> and <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2A</xref> were also calculated in a similar fashion: at each time step, the Euclidean distance was calculated between the trajectory encoding an utterance at warp factor <italic>α</italic> (<italic>I<sub>0</sub></italic> = 0.05), and its nearest neighbor along the trajectory encoding the reference utterance of the same digit (<italic>I<sub>0</sub></italic> = 0). The time-average of these distances were then summarized over 10 trials for the plots in these figures.</p><p>The nature and robustness of spectral generalization in trained RNNs was probed in <xref ref-type="fig" rid="fig6">Figure 6</xref>, by artificially altering the external inputs. In order to assess spectral generalization independent of temporal invariance, for each digit, the duration of all its utterances were time-normalized by warping the respective cochleograms to their median duration. The RNN and its outputs were trained on three time-normalized utterances for each of the ten digits, and all test simulations were performed with a common initial state. The spectral generalization of an RNN was tested with artificially constructed external inputs to the network that were qualitatively similar to external inputs for time-normalized natural utterances, but with altered spectral noise structure. The spectral noise in an utterance was defined as the difference between the external inputs of the utterance and the respective template utterance (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Network responses to time-normalized trained and untrained natural utterances were compared to two artificial controls: external inputs with shuffled noise and with orthogonal noise. Inputs with shuffled noise were constructed from the Principal Component Analysis (PCA) loadings and scores of the spectral noise in untrained natural utterances as follows: (i) the loading vectors were permuted; (ii) shuffled noise was generated by multiplying the spectral noise scores by the shuffled loading vectors; (iii) this shuffled noise was added to the external input of the corresponding template utterance. Inputs with orthogonal noise were also constructed from the PCA loadings and scores of the spectral noise in untrained natural utterances, except instead of permuting the loading vectors, they were replaced by an orthonormal set of vectors, generated via Gram-Schmidt orthogonalization, each of which was orthogonal to the set of spectral noise PCA loading vectors. For this a random vector was generated and QR factorization was performed on the set of vectors comprised of the PCA loadings and the random vector, producing a new vector that was orthogonal to the PCA loadings. This procedure was repeated to generate a set of such vectors equal in cardinality to the PCA loading set. The random vectors were then orthogonalized relative to each other in a similar manner, and then normalized to produce an orthonormal set. External inputs with shuffled and orthogonal noise were constructed from each untrained natural utterance, with as few PCs as were necessary to explain 99% of the variance in its spectral noise (typically 11 PCs). For each digit, the network was tested with 30 shuffled and orthogonal noise inputs, each based on a randomly chosen untrained natural utterance.</p><p>The dimensionality measure shown in <xref ref-type="fig" rid="fig5">Figure 5</xref> was calculated as <inline-formula><mml:math id="inf14"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <italic>λ<sub>k</sub></italic> represents eigenvalues of the equal-time cross-correlation matrix of network activity, expressed as a fraction of their sum (<xref ref-type="bibr" rid="bib54">Rajan et al., 2010b</xref>). The eigenvalues were calculated on a concatenation of the sensory epoch trajectories for all utterances (trained and novel) of all digits.</p></sec><sec id="s4-8"><title>RNN decomposition</title><p>In <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig8">Figure 8</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1</xref>–<xref ref-type="fig" rid="fig8s2">2</xref>, trajectories and their distances from each other were decomposed into the constituent input and recurrent subspaces of phase space. These are derived from the state variable <bold><italic>x(t)</italic></bold> (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), rather than the rate variable <bold><italic>r(t)</italic></bold> (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) that was used to measure Euclidean distances:</p><p>From Euler’s Method:<disp-formula id="equ4"><mml:math id="m4"><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ5"><mml:math id="m5"><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msup><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo> <mml:mi/><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>Let <inline-formula><mml:math id="inf15"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>,<inline-formula><mml:math id="inf16"> <mml:mi/><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>, <inline-formula><mml:math id="inf17"><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msup><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>, and <inline-formula><mml:math id="inf18"><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula><disp-formula id="equ6"><mml:math id="m6"><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo> <mml:mi/><mml:mi>b</mml:mi><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo> <mml:mi/><mml:mi>b</mml:mi><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>Solving this recurrence relationship,<disp-formula id="equ7"><label>(4)</label><mml:math id="m7"><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo> <mml:mi/><mml:mi>b</mml:mi><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo> <mml:mi/><mml:mi>b</mml:mi><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="inf19"><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> is the initial state of the network. We denote the first two terms (<inline-formula><mml:math id="inf20"><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo> <mml:mi/><mml:mi>b</mml:mi><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>) as network activity in the recurrent subspace (recurrent subspace trajectory) and the last term (<inline-formula><mml:math id="inf21"><mml:mi>b</mml:mi><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>) as network activity in the input subspace (input subspace trajectory).</p><p>When the input durations for all utterances of a digit are time-normalized, as in <xref ref-type="fig" rid="fig6">Figure 6</xref>, the squared Euclidean distance between the state variables <inline-formula><mml:math id="inf22"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced> <mml:mi/></mml:math></inline-formula>for two sensory epoch trajectories encoding utterances <inline-formula><mml:math id="inf24"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is given by:<disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>From <xref ref-type="disp-formula" rid="equ7">Equation 4</xref>, it follows that:<disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Assuming common initial state and rearranging:<disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Let <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Then,<disp-formula id="equ11"><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ12"><label>(5)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>.</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>.</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>.</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></disp-formula>where the first (<inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>.</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>) and second (<inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>.</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>) terms denote the squared distance in recurrent subspace and input subspace, respectively. The third term (<inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>2</mml:mn><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>.</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>) is based on the interaction between the two subspaces: If the recurrent and input subspaces are orthogonal to each other, then this term will be zero; otherwise, it indicates whether the deviations in the recurrent subspace serve to amplify or suppress the spectral noise.</p><p>In <xref ref-type="fig" rid="fig8">Figure 8</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1</xref>–<xref ref-type="fig" rid="fig8s2">2</xref>, all test simulations were performed with a common initial state. The linear speed of the trajectory in neural phase space, and in its input and recurrent subspaces, were calculated as the time-averaged magnitude, or L<sub>2</sub>-norm, of the instantaneous change in network state (<inline-formula><mml:math id="inf31"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>), and its input and recurrent subspaces projections, respectively (<xref ref-type="disp-formula" rid="equ7">Equation 4</xref>). Similarly, the total distance traversed in each subspace was calculated as the sum of the magnitude of instantaneous change in network state in the respective subspace, over the duration of the warped utterance.</p></sec><sec id="s4-9"><title>Analysis of parallel trajectories</title><p>In <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1</xref>–<xref ref-type="fig" rid="fig8s2">2</xref>, we present evidence in support of parallel trajectories produced by the network dynamics in response to warped utterances. For each digit, the trajectories were first temporally aligned to the 100% (1x) warp, by matching the time indices of the respective warped utterances to the 100% reference. The following procedure was then independently applied to the temporally aligned recurrent and input subspace trajectories. One separation vector, <bold>S<sub>0.7x</sub></bold>(t<sub>aligned</sub>) (<bold>S<sub>1.4x</sub></bold>(t<sub>aligned</sub>)), was calculated at each aligned time point of the trajectory at the fast 0.7x (slow 1.4x) warp, as the difference between the population state (<inline-formula><mml:math id="inf32"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula>) of the 0.7x (1.4x) warp and 1x warp trajectories. The separation vectors were then normalized to unit length, denoted as <bold>S<sub>0.7x</sub></bold><sup>norm</sup>(t<sub>aligned</sub>) (<bold>S<sub>1.4x</sub></bold><sup>norm</sup>(t<sub>aligned</sub>)). Finally, the projected separations at each fast (slow) warp were generated by calculating the respective separation vectors and projecting them onto the normalized separation vectors of the 0.7x (1.4x) warp trajectory. For example, projected separations at the 0.5x warp, PS<sub>0.5x</sub>(t<sub>aligned</sub>), were generated by projecting the separation vectors for the 0.5x warp trajectory, onto <bold>S<sub>0.7x</sub></bold><sup>norm</sup>(t<sub>aligned</sub>), to give PS<sub>0.5x</sub>(t<sub>aligned</sub>)=<bold>S<sub>0.5x</sub></bold>(t<sub>aligned</sub>).<bold>S<sub>0.7x</sub></bold><sup>norm</sup>(t<sub>aligned</sub>). The variance explained by these projections was calculated as the ratio between the total population variance of the projected and overall separation.</p></sec><sec id="s4-10"><title>Trajectory visualization</title><p>For <xref ref-type="fig" rid="fig3">Figure 3</xref>, PCA was performed on a concatenation of the trajectories generated by both the reservoir and the trained network in response to all utterances of the digits ‘six’ and ‘eight’ by a single subject. Trajectories were then individually projected onto three principal components (PCs) and plotted in 3D. The sensory (motor) trajectories were projected onto PCs 2–4 (PCs 1–3). PC one was not used in plotting the sensory trajectories, because it captured features common to both spoken digits. Similarly, in <xref ref-type="fig" rid="fig6">Figure 6</xref>, PCA was performed on a concatenation of the external inputs for the template utterance of digit zero, a natural test utterance of digit zero by the same subject, and one artificial test utterance each (shuffled and orthogonal noise) derived from these utterances. The external inputs were then projected on the first three PCs and plotted in 3D. In <xref ref-type="fig" rid="fig8">Figure 8</xref>, PCA was performed on concatenations of temporally-aligned 100 ms segments (i.e. segments aligned to 200–300 ms of the 1x warp) of the external input for digit one at warps of 0.57x, 0.7<underline>x,</underline> 0.87x, 1x, 1.15x, 1.4x and 1.74x. The external input segments where then projected onto the first three PCs and plotted in 3D. The same procedure was followed in plotting the PCA projections of population state responses in <xref ref-type="fig" rid="fig8">Figure 8</xref>, and of the recurrent and input subspace population state in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1</xref>–<xref ref-type="fig" rid="fig8s2">2</xref>.</p></sec><sec id="s4-11"><title>Source code</title><p>Code for the simulation and training of the RNN model, cochleogram data and a set of trained weight matrices are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vgoudar/SensoriMotorTranscription">https://github.com/vgoudar/SensoriMotorTranscription</ext-link> (<xref ref-type="bibr" rid="bib17">Goudar and Buonomano, 2018</xref>). A copy is archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/SensoriMotorTranscription">https://github.com/elifesciences-publications/SensoriMotorTranscription</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Nicholas Hardy, Omri Barak, Sean Escola, Alexandre Rivkind and Jonathan Kadmon for helpful discussions, and Dharshan Kumaran for comments on an earlier version of this manuscript.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Writing—original draft, Writing—review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.31134.015</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-31134-transrepform-v1.docx"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Abadi</surname> <given-names>M</given-names></name><name><surname>Agarwal</surname> <given-names>A</given-names></name><name><surname>Barham</surname> <given-names>P</given-names></name><name><surname>Brevdo</surname> <given-names>E</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Citro</surname> <given-names>C</given-names></name><name><surname>Corrado</surname> <given-names>GS</given-names></name><name><surname>Davis</surname> <given-names>A</given-names></name><name><surname>Dean</surname> <given-names>J</given-names></name><name><surname>Devin</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tensorflow: Large-scale machine learning on heterogeneous distributed systems</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.04467">https://arxiv.org/abs/1603.04467</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>DePasquale</surname> <given-names>B</given-names></name><name><surname>Memmesheimer</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Building functional networks of spiking model neurons</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>350</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1038/nn.4241</pub-id><pub-id pub-id-type="pmid">26906501</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amit</surname> <given-names>DJ</given-names></name><name><surname>Brunel</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex</article-title><source>Cerebral Cortex</source><volume>7</volume><fpage>237</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1093/cercor/7.3.237</pub-id><pub-id pub-id-type="pmid">9143444</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayaz</surname> <given-names>A</given-names></name><name><surname>Saleem</surname> <given-names>AB</given-names></name><name><surname>Schölvinck</surname> <given-names>ML</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Locomotion controls spatial integration in mouse visual cortex</article-title><source>Current Biology</source><volume>23</volume><fpage>890</fpage><lpage>894</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.04.012</pub-id><pub-id pub-id-type="pmid">23664971</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Simard</surname> <given-names>P</given-names></name><name><surname>Frasconi</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Learning long-term dependencies with gradient descent is difficult</article-title><source>IEEE Transactions on Neural Networks</source><volume>5</volume><fpage>157</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1109/72.279181</pub-id><pub-id pub-id-type="pmid">18267787</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertschinger</surname> <given-names>N</given-names></name><name><surname>Natschläger</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Real-time computation at the edge of chaos in recurrent neural networks</article-title><source>Neural Computation</source><volume>16</volume><fpage>1413</fpage><lpage>1436</lpage><pub-id pub-id-type="doi">10.1162/089976604323057443</pub-id><pub-id pub-id-type="pmid">15165396</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname> <given-names>DV</given-names></name><name><surname>Maass</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>State-dependent computations: spatiotemporal processing in cortical networks</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>113</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/nrn2558</pub-id><pub-id pub-id-type="pmid">19145235</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname> <given-names>DV</given-names></name><name><surname>Merzenich</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical plasticity: from synapses to maps</article-title><source>Annual Review of Neuroscience</source><volume>21</volume><fpage>149</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.21.1.149</pub-id><pub-id pub-id-type="pmid">9530495</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carnevale</surname> <given-names>F</given-names></name><name><surname>de Lafuente</surname></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Parga</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamic control of response criterion in premotor cortex during perceptual detection under temporal uncertainty</article-title><source>Neuron</source><volume>86</volume><fpage>1067</fpage><lpage>1077</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.04.014</pub-id><pub-id pub-id-type="pmid">25959731</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>EF</given-names></name><name><surname>Niziolek</surname> <given-names>CA</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name><name><surname>Nagarajan</surname> <given-names>SS</given-names></name><name><surname>Houde</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Human cortical sensorimotor network underlying feedback control of vocal pitch</article-title><source>PNAS</source><volume>110</volume><fpage>2653</fpage><lpage>2658</lpage><pub-id pub-id-type="doi">10.1073/pnas.1216827110</pub-id><pub-id pub-id-type="pmid">23345447</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname> <given-names>C</given-names></name><name><surname>Hamiton</surname> <given-names>LS</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The auditory representation of speech sounds in human motor cortex</article-title><source>eLife</source><volume>5</volume><elocation-id>e12577</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12577</pub-id><pub-id pub-id-type="pmid">26943778</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crist</surname> <given-names>RE</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name><name><surname>Gilbert</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Learning to see: experience and attention in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>519</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1038/87470</pub-id><pub-id pub-id-type="pmid">11319561</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowe</surname> <given-names>DA</given-names></name><name><surname>Zarco</surname> <given-names>W</given-names></name><name><surname>Bartolo</surname> <given-names>R</given-names></name><name><surname>Merchant</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic representation of the temporal and sequential structure of rhythmic movements in the primate medial premotor cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>11972</fpage><lpage>11983</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2177-14.2014</pub-id><pub-id pub-id-type="pmid">25186744</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doupe</surname> <given-names>AJ</given-names></name><name><surname>Kuhl</surname> <given-names>PK</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Birdsong and human speech: common themes and mechanisms</article-title><source>Annual Review of Neuroscience</source><volume>22</volume><fpage>567</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.22.1.567</pub-id><pub-id pub-id-type="pmid">10202549</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elman</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Finding structure in time</article-title><source>Cognitive Science</source><volume>14</volume><fpage>179</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog1402_1</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname> <given-names>DE</given-names></name><name><surname>Brecht</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Map plasticity in somatosensory cortex</article-title><source>Science</source><volume>310</volume><fpage>810</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1126/science.1115807</pub-id><pub-id pub-id-type="pmid">16272113</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Goudar</surname> <given-names>V</given-names></name><name><surname>Buonomano</surname> <given-names>DV</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>SensoriMotor Transcription</data-title><source>GitHub</source><version designator="d5f40d2">d5f40d2</version><ext-link ext-link-type="uri" xlink:href="https://github.com/vgoudar/SensoriMotorTranscription">https://github.com/vgoudar/SensoriMotorTranscription</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Graves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Generating sequences with recurrent neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1308.0850">https://arxiv.org/abs/1308.0850</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gütig</surname> <given-names>R</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Time-warp-invariant neuronal processing</article-title><source>PLoS Biology</source><volume>7</volume><elocation-id>e1000141</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000141</pub-id><pub-id pub-id-type="pmid">19582146</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Haykin</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Adaptive Filter Theory</source><publisher-loc>Upper Saddle River</publisher-loc><publisher-name>Prentice Hall</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname> <given-names>G</given-names></name><name><surname>Deng</surname> <given-names>L</given-names></name><name><surname>Yu</surname> <given-names>D</given-names></name><name><surname>Dahl</surname> <given-names>G</given-names></name><name><surname>Mohamed</surname> <given-names>A-rahman</given-names></name><name><surname>Jaitly</surname> <given-names>N</given-names></name><name><surname>Senior</surname> <given-names>A</given-names></name><name><surname>Vanhoucke</surname> <given-names>V</given-names></name><name><surname>Nguyen</surname> <given-names>P</given-names></name><name><surname>Sainath</surname> <given-names>T</given-names></name><name><surname>Kingsbury</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</article-title><source>IEEE Signal Processing Magazine</source><volume>29</volume><fpage>82</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1109/MSP.2012.2205597</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname> <given-names>JJ</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Computing with neural circuits: a model</article-title><source>Science</source><volume>233</volume><fpage>625</fpage><lpage>633</lpage><pub-id pub-id-type="doi">10.1126/science.3755256</pub-id><pub-id pub-id-type="pmid">3755256</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Neural networks and physical systems with emergent collective computational abilities</article-title><source>PNAS</source><volume>79</volume><fpage>2554</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id><pub-id pub-id-type="pmid">6953413</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Understanding emergent dynamics: Using a collective activity coordinate of a neural network to recognize time-varying patterns</article-title><source>Neural Computation</source><volume>27</volume><fpage>2011</fpage><lpage>2038</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00768</pub-id><pub-id pub-id-type="pmid">26313598</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ivry</surname> <given-names>RB</given-names></name><name><surname>Schlerf</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dedicated and intrinsic models of time perception</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>273</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.04.002</pub-id><pub-id pub-id-type="pmid">18539519</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeger</surname> <given-names>H</given-names></name><name><surname>Haas</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication</article-title><source>Science</source><volume>304</volume><fpage>78</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1126/science.1091277</pub-id><pub-id pub-id-type="pmid">15064413</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jia</surname> <given-names>Y</given-names></name><name><surname>Shelhamer</surname> <given-names>E</given-names></name><name><surname>Donahue</surname> <given-names>J</given-names></name><name><surname>Karayev</surname> <given-names>S</given-names></name><name><surname>Long</surname> <given-names>J</given-names></name><name><surname>Girshick</surname> <given-names>R</given-names></name><name><surname>Guadarrama</surname> <given-names>S</given-names></name><name><surname>Darrell</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Caffe: Convolutional Architecture for Fast Feature Embedding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1408.5093">https://arxiv.org/abs/1408.5093</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karmarkar</surname> <given-names>UR</given-names></name><name><surname>Dan</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Experience-dependent plasticity in adult visual cortex</article-title><source>Neuron</source><volume>52</volume><fpage>577</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.11.001</pub-id><pub-id pub-id-type="pmid">17114043</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>D</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laje</surname> <given-names>R</given-names></name><name><surname>Buonomano</surname> <given-names>DV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Robust timing and motor patterns by taming chaos in recurrent neural networks</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>925</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nn.3405</pub-id><pub-id pub-id-type="pmid">23708144</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Le</surname> <given-names>QV</given-names></name><name><surname>Jaitly</surname> <given-names>N</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A simple way to initialize recurrent networks of rectified linear units</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1504.00941">https://arxiv.org/abs/1504.00941</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lecun</surname> <given-names>Y</given-names></name><name><surname>Bottou</surname> <given-names>L</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Haffner</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Gradient-based learning applied to document recognition</article-title><source>Proceedings of the IEEE</source><volume>86</volume><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Cortes</surname> <given-names>C</given-names></name><name><surname>Burges</surname> <given-names>CJC</given-names></name></person-group><year iso-8601-date="1998">1998</year><data-title>Training sets</data-title><source>The MNIST Database of Handwritten Digits</source><pub-id pub-id-type="archive" xlink:href="http://yann.lecun.com/exdb/mnist/">MNIST</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerner</surname> <given-names>Y</given-names></name><name><surname>Honey</surname> <given-names>CJ</given-names></name><name><surname>Katkov</surname> <given-names>M</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal scaling of neural responses to compressed and dilated natural speech</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>2433</fpage><lpage>2444</lpage><pub-id pub-id-type="doi">10.1152/jn.00497.2013</pub-id><pub-id pub-id-type="pmid">24647432</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>N</given-names></name><name><surname>Daie</surname> <given-names>K</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Druckmann</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Robust neuronal dynamics in premotor cortex during motor planning</article-title><source>Nature</source><volume>532</volume><fpage>459</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1038/nature17643</pub-id><pub-id pub-id-type="pmid">27074502</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lukoševičius</surname> <given-names>M</given-names></name><name><surname>Jaeger</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reservoir computing approaches to recurrent neural network training</article-title><source>Computer Science Review</source><volume>3</volume><fpage>127</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.cosrev.2009.03.005</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lyon</surname> <given-names>RF</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>A Computational Model of Filtering, Detection, and Compression in the Cochlea</article-title><conf-name><italic>Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP</italic></conf-name><fpage>1282</fpage><lpage>1285</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname> <given-names>W</given-names></name><name><surname>Natschläger</surname> <given-names>T</given-names></name><name><surname>Markram</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title><source>Neural Computation</source><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage><pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname> <given-names>V</given-names></name><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Mark Liberman</surname> <given-names>RA</given-names></name><name><surname>Church</surname> <given-names>K</given-names></name><name><surname>Fox</surname> <given-names>E</given-names></name><name><surname>Hafner</surname> <given-names>C</given-names></name><name><surname>Klavans</surname> <given-names>J</given-names></name><name><surname>Marcus</surname> <given-names>M</given-names></name><name><surname>Mercer</surname> <given-names>B</given-names></name><name><surname>Pedersen</surname> <given-names>J</given-names></name><name><surname>Roossin</surname> <given-names>P</given-names></name><name><surname>Don Walker</surname> <given-names>SW</given-names></name><name><surname>Zampolli</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1993">1993</year><data-title>TI 46-Word LDC93S9</data-title><source>Linguistic Data Consortium</source><pub-id pub-id-type="accession" xlink:href="https://catalog.ldc.upenn.edu/ldc93s9">LDC93S9</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Martens</surname> <given-names>J</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Learning Recurrent Neural Networks with Hessian-Free Optimization</article-title><source>Proceedings of the 28th International Conference on Machine Learning</source><conf-name>The International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mauk</surname> <given-names>MD</given-names></name><name><surname>Buonomano</surname> <given-names>DV</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The neural basis of temporal processing</article-title><source>Annual Review of Neuroscience</source><volume>27</volume><fpage>307</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144247</pub-id><pub-id pub-id-type="pmid">15217335</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mello</surname> <given-names>GB</given-names></name><name><surname>Soares</surname> <given-names>S</given-names></name><name><surname>Paton</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A scalable population code for time in the striatum</article-title><source>Current Biology</source><volume>25</volume><fpage>1113</fpage><lpage>1122</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.02.036</pub-id><pub-id pub-id-type="pmid">25913405</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merchant</surname> <given-names>H</given-names></name><name><surname>Harrington</surname> <given-names>DL</given-names></name><name><surname>Meck</surname> <given-names>WH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural basis of the perception and estimation of time</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>313</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170349</pub-id><pub-id pub-id-type="pmid">23725000</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>JL</given-names></name><name><surname>Grosjean</surname> <given-names>F</given-names></name><name><surname>Lomanto</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Articulation rate and its variability in spontaneous speech: a reanalysis and some implications</article-title><source>Phonetica</source><volume>41</volume><fpage>215</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1159/000261728</pub-id><pub-id pub-id-type="pmid">6535162</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname> <given-names>V</given-names></name><name><surname>Kavukcuoglu</surname> <given-names>K</given-names></name><name><surname>Silver</surname> <given-names>D</given-names></name><name><surname>Rusu</surname> <given-names>AA</given-names></name><name><surname>Veness</surname> <given-names>J</given-names></name><name><surname>Bellemare</surname> <given-names>MG</given-names></name><name><surname>Graves</surname> <given-names>A</given-names></name><name><surname>Riedmiller</surname> <given-names>M</given-names></name><name><surname>Fidjeland</surname> <given-names>AK</given-names></name><name><surname>Ostrovski</surname> <given-names>G</given-names></name><name><surname>Petersen</surname> <given-names>S</given-names></name><name><surname>Beattie</surname> <given-names>C</given-names></name><name><surname>Sadik</surname> <given-names>A</given-names></name><name><surname>Antonoglou</surname> <given-names>I</given-names></name><name><surname>King</surname> <given-names>H</given-names></name><name><surname>Kumaran</surname> <given-names>D</given-names></name><name><surname>Wierstra</surname> <given-names>D</given-names></name><name><surname>Legg</surname> <given-names>S</given-names></name><name><surname>Hassabis</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Human-level control through deep reinforcement learning</article-title><source>Nature</source><volume>518</volume><fpage>529</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1038/nature14236</pub-id><pub-id pub-id-type="pmid">25719670</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>JM</given-names></name><name><surname>Escola</surname> <given-names>GS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning multiple variable-speed sequences in striatum via cortical tutoring</article-title><source>eLife</source><volume>6</volume><elocation-id>e26084</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26084</pub-id><pub-id pub-id-type="pmid">28481200</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nobre</surname> <given-names>A</given-names></name><name><surname>Correa</surname> <given-names>A</given-names></name><name><surname>Coull</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The hazards of time</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>465</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.07.006</pub-id><pub-id pub-id-type="pmid">17709239</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ostojic</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Two types of asynchronous activity in networks of excitatory and inhibitory spiking neurons</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>594</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1038/nn.3658</pub-id><pub-id pub-id-type="pmid">24561997</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearlmutter</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Gradient calculations for dynamic recurrent neural networks: a survey</article-title><source>IEEE Transactions on Neural Networks</source><volume>6</volume><fpage>1212</fpage><lpage>1228</lpage><pub-id pub-id-type="doi">10.1109/72.410363</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabiner</surname> <given-names>LR</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>A tutorial on hidden Markov models and selected applications in speech recognition</article-title><source>Proceedings of the IEEE</source><volume>77</volume><fpage>257</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1109/5.18626</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinovich</surname> <given-names>M</given-names></name><name><surname>Huerta</surname> <given-names>R</given-names></name><name><surname>Laurent</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>NEUROSCIENCE: transient dynamics for neural processing</article-title><source>Science</source><volume>321</volume><fpage>48</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1126/science.1155564</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rajan</surname> <given-names>K</given-names></name><name><surname>Abbott</surname> <given-names>L</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2010">2010b</year><article-title>Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</article-title><conf-name><italic>Advances in Neural Information Processing Systems 23 (NIPS 2010)</italic></conf-name><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/4051-inferring-stimulus-selectivity-from-the-spatial-structure-of-neural-network-dynamics">https://papers.nips.cc/paper/4051-inferring-stimulus-selectivity-from-the-spatial-structure-of-neural-network-dynamics</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname> <given-names>K</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2010">2010a</year><article-title>Stimulus-dependent suppression of chaos in recurrent neural networks</article-title><source>Physical Review E</source><volume>82</volume><elocation-id>011903</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.82.011903</pub-id><pub-id pub-id-type="pmid">20866644</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname> <given-names>K</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Eigenvalue spectra of random matrices for neural networks</article-title><source>Physical Review Letters</source><volume>97</volume><elocation-id>188104</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.97.188104</pub-id><pub-id pub-id-type="pmid">17155583</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname> <given-names>K</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Recurrent network models of sequence generation and memory</article-title><source>Neuron</source><volume>90</volume><fpage>128</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.009</pub-id><pub-id pub-id-type="pmid">26971945</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname> <given-names>DM</given-names></name><name><surname>Mooney</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Motor-related signals in the auditory system for listening and learning</article-title><source>Current Opinion in Neurobiology</source><volume>33</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.03.004</pub-id><pub-id pub-id-type="pmid">25827273</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sebastián-Gallés</surname> <given-names>N</given-names></name><name><surname>Dupoux</surname> <given-names>E</given-names></name><name><surname>Costa</surname> <given-names>A</given-names></name><name><surname>Mehler</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Adaptation to time-compressed speech: phonological determinants</article-title><source>Perception &amp; Psychophysics</source><volume>62</volume><fpage>834</fpage><lpage>842</lpage><pub-id pub-id-type="doi">10.3758/BF03206926</pub-id><pub-id pub-id-type="pmid">10883588</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Crisanti</surname> <given-names>A</given-names></name><name><surname>Sommers</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Chaos in random neural networks</article-title><source>Physical Review Letters</source><volume>61</volume><fpage>259</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.61.259</pub-id><pub-id pub-id-type="pmid">10039285</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating coherent patterns of activity from chaotic neural networks</article-title><source>Neuron</source><volume>63</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.018</pub-id><pub-id pub-id-type="pmid">19709635</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname> <given-names>FE</given-names></name><name><surname>Elie</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural processing of natural sounds</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>355</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn3731</pub-id><pub-id pub-id-type="pmid">24840800</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name><name><surname>Zenke</surname> <given-names>F</given-names></name><name><surname>Clopath</surname> <given-names>C</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title><source>Science</source><volume>334</volume><fpage>1569</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1126/science.1211095</pub-id><pub-id pub-id-type="pmid">22075724</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waibel</surname> <given-names>A</given-names></name><name><surname>Hanazawa</surname> <given-names>T</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name><name><surname>Shikano</surname> <given-names>K</given-names></name><name><surname>Lang</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Phoneme recognition using time-delay neural networks</article-title><source>IEEE Transactions on Acoustics, Speech, and Signal Processing</source><volume>37</volume><fpage>328</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1109/29.21701</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname> <given-names>E</given-names></name><name><surname>Maei</surname> <given-names>HR</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Randomly connected networks have short temporal memory</article-title><source>Neural Computation</source><volume>25</volume><fpage>1408</fpage><lpage>1439</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00449</pub-id><pub-id pub-id-type="pmid">23517097</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Synaptic reverberation underlying mnemonic persistent activity</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>455</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01868-3</pub-id><pub-id pub-id-type="pmid">11476885</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.31134.017</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Druckmann</surname><given-names>Shaul</given-names></name><role>Reviewing Editor</role><aff id="aff5"><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Encoding Sensory and Motor Patterns as Time-Invariant Trajectories in Recurrent Neural Networks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Richard Ivry as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>This manuscript reports a computational study in which a recurrent neural network was trained to categorize spatio-temporal input stimuli and produce spatio-temporal patterns as responses. More specifically, the study used spoken digits as input stimuli, and transcriptions of these digits as outputs. The authors show that the trained network is robust to variations in the inputs and to external perturbations. In particular, they show that the network generalizes to untrained utterances of digits, and when trained on utterances of different length is able to deal with temporal stretching and compression of the stimuli. These results are interpreted in terms of stimulus-elicited neural trajectories, which show a degree of temporal invariance.</p><p>The paper is clearly a well-described, well-thought out computational experiment in artificial recurrent neural networks. However, all reviewers felt that its biological conclusions remain unclear. On a computational level, we know from much work in machine learning, where the task that the authors consider would be called a sequence-to-sequence task, that recurrent neural networks are indeed able to instantiate such computations (e.g., Sutskever et al., 2014). We also know, as the authors themselves report, that alternative mechanisms for dealing with time warping exist.</p><p>Accordingly, just the fact that the trained networks could solve the task is not in and of itself a strong biological finding. There are three ways the reviewers feel the paper could be improved, in order of importance:</p><p>1) What is the novel biological insight offered by the study? At present there is little detail on this. One prediction appears in the text: &quot;Specifically, the observation that slower stimuli yield trajectories with larger curvature radii implies that the neural population activity should exhibit larger fluctuations in their firing rates in response to slower stimuli&quot;. However, in order for that to be a true prediction one would need to show that this is a necessary, or at least generic property, for instance by studying multiple different types of tasks involving temporal warping, or showing somehow that without this increase in curvature radius the trajectories cannot be learned. Moreover, the effect size for this prediction shown in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1D</xref> appears quite mild (25% increase in range across the full span of warping). both generally and at the mechanistic level.</p><p>2) What is the extent and nature of the generalization that these networks are capable of. The authors demonstrate some interpolation and extrapolation of time-warping as well as an ability to reject certain kinds of noise. A stronger test might be for instance whether a network given a training set with naturally spoken examples all time-warped to the same duration, would still be able to generalize to stimuli of different durations?</p><p>3) How generic are the results? Is this approach different from previous generic deep learning results? Can the authors describe better the underlying mechanism, Can they convincingly say for instance that the mechanism that they describe in <xref ref-type="fig" rid="fig8">Figure 8</xref> for example is typical, or necessary? That it would remain the same given different choices about training paradigms, etc.</p><p>We realize that these are difficult points to address (especially point three). We feel these will be the best ways to strengthen the biological conclusions of the paper, even if a completely full answer cannot be achieved.</p><p>To help you in addressing these three main concerns, please find below more detailed reviewer remarks, culled from the original review. Please don't feel you have to address each of these as separate points, I add them here to help you understand in more detail the three main points to be addressed which we listed above.</p><p>Major concerns:</p><p>1) The network was trained in two steps in which the recurrent weights were trained and then the output weights were trained. I understand the rationale as it is stated in the main text. My question is whether this two-step training is necessary. In a biological case, the network would likely not be trained in such a way. Training would happen presumably all at once based only on the correctness of the output. It would be interesting to show how the network performs when the recurrent and output weights are trained simultaneously. What are the main differences in functionality the network can achieve? Also, a little more text on the rationale for the training procedure would be helpful to reader in the main text.</p><p>2) One of the main novelties of the paper is the temporal invariance of the network. The authors state that other models have been developed that can also account for temporal invariance. However, these other models are not fully introduced, forcing the reader to go to the literature to make a comparison. It would be helpful if the text contained a longer introduction to other models and a discussion of how, in functional terms and experimental predictions, the new model presented here compares to and differs from the published models cited in the text.</p><p>3) The paper emphasizes the idea of generalizing to different speeds of stimuli. The current results show that when trained on stimuli of different durations, generalization can occur. However, it is unclear if this generalization is a natural consequence of encoding stimuli as trajectories or requires training on different durations. That is, if the network is given a training set with naturally spoken examples all time-warped to the same duration, would the network be able to generalize to stimuli of different durations? The answer to this question helps the reader better understand the scope of the generalization that can occur.</p><p>4) How does the performance of the network differ for things like generalization, robustness to noise, etc. as a function of the size of the network? Different figures seem to use different size networks (n = 2100 in <xref ref-type="fig" rid="fig3">Figure 3 and n</xref> = 4000 in <xref ref-type="fig" rid="fig1">Figure 1</xref>), but it was not immediately clear how the network size was chosen.</p><p>5) One weakness of the paper is that the extent of mechanistic exploration of the RNN and experimental predictions is rather limited, at least in my opinion. I do not have specific suggestions for how to improve on this point. One possibility is further examination of the recurrent weight matrix to look for specific structure and dynamics that could generate additional experimental predictions. Given that I do not have specific questions here, I do not expect that something must be done in this area. I simply note this point because the impact of the paper will be enhanced if more mechanistic insight into how the RNN functions or into experimental predictions could be added.</p><p>6) The relation between this work and the substantial progress in deep learning on general sequence-to-sequence tasks (for example, &quot;Sequence to sequence learning with neural networks&quot; by Sutskever et al., 2014 and the dozens of subsequent papers that built upon it) is unclear. Most of these works in deep learning use backpropagation through time (BPTT) to train their networks, an algorithm which is conceptually very distinct from their tamed chaos method. Given the excellent track record of BPTT on many real-world sequence-to-sequence tasks, it seems that the point that the brain can use recurrent dynamics to solve such tasks is already evident. Perhaps the authors feel their approach is more appropriate in some way? If so, it would be important to compare the performance of the tamed chaos method with that of BPTT on more complex tasks and understand the relative advantages and shortcomings between them (for example, is there a task that one method can easily achieve but the other can't? Or is there some feature of the dynamics/networks that are learnt that appears more reasonable for one than the other). Moreover, given these two largely distinct methods to train RNNs, an important question that needs to be addressed is how the mechanisms by which generalization occurs in each method differ from each other. In other words, it is not clear which aspects of the mechanisms they identified in this paper are unique to the tamed chaos method or common to more general classes of algorithms to train RNNs. This paper provides a rather detailed analysis of how their particular training method enables generalization, but, for their analysis to be more useful in understanding how the RNNs in the brain operate, showing that this is a general mechanism in RNNs would be desired.</p><p>7) It was unclear how to interpret the fact that the motor-only-training networks were also robust to temporal warping (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). In fact, for time warping that is in between trained values, the interpolation regime, as the authors call it the motor-only-training networks were at 100% performance and one of the papers the authors cite (Lerner et al., 2014) shows that there is temporal scaling in human hemodynamic imaging mainly in the behavioral range, that is in the interpolation range. Given the chaotic nature of the networks pre-training wouldn't motor-trained-only networks have initial conditions that are almost as different for within digit than between digit? If this is true and the networks can still learn to be at 100% wouldn't that imply that the sensory training is in some sense redundant? I see the greater separation in <xref ref-type="fig" rid="fig7">Figure 7B</xref> but it is hard to evaluate whether the extra separation is important or overkill.</p><p>8) In general the treatment of temporal warping somewhat confusing. The text is written as if the solution to a time warped input is necessarily to generate the same trajectory running at a different speed. This doesn't have to be the case, in general all that is needed is that whatever dynamics happen, at the transition between the sensory and motor phases the within digit distance (including time warps, different utterances, etc.) be smaller than the between digit distance. The easiest way for this to be achieved is to arrive at the exact same point for different time warps. However, this is not necessary, nor is it what the network actually ends up doing (<xref ref-type="fig" rid="fig8">Figure 8</xref>). It was therefore very difficult for me to interpret the significance of <xref ref-type="fig" rid="fig8">Figure 8</xref>, that the network arrives at a mid-way solution between constant speed and constant distance. Is there something particularly interesting about this midpoint? Is there a reason it arrived at this compromise between these two possible mechanisms? Is it influenced by the way the authors generate the target trajectories by scaling?</p><p>9) The authors make one main biological prediction: &quot;Specifically, the observation that slower stimuli yield trajectories with larger curvature radii implies that the neural population activity should exhibit larger fluctuations in their firing rates in response to slower stimuli&quot;. However, in order for that to be a true prediction at least in my mind one would need to show that this is a necessary, or at least generic property, for instance by studying multiple different types of tasks involving temporal warping, or showing somehow that without this increase in curvature radius the trajectories cannot be learned. Moreover, the effect size for this prediction shown in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1D</xref> appears quite mild (25% increase in range across the full span of warping).</p><p>10) Focusing on a specific set of realistic, complex stimuli and outputs is certainly a clear proof of concept of the task. Yet, the shortcoming of such an approach is that it is not immediately clear which part of the reported results are to some extent general, and what is in contrast specific to the particular implementation of the trained network (which relies on a very specific training procedure, and no doubt involves some fine-tuning). I guess what I am missing is a simplified, computational description of the underlying mechanism. Right now, I am left wondering how much details matter, and what happens when some details are changed. For instance, it seems that the specific implementation used in the paper requires that the motor action starts at a fixed time after the stimulus, and that it could not easily accommodate a variable delay between the stimulus and the motor output (e.g. specified by a go cue). Providing a simplified computational description of the mechanism, and examples of extensions/limitations would greatly improve the paper.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for submitting a revision of your article &quot;Encoding Sensory and Motor Patterns as Time-Invariant Trajectories in Recurrent Neural Networks&quot; for consideration by <italic>eLife</italic>.</p><p>We found the manuscript improved. In particular, the inclusion of the back propagation trained network results that came up with a similar regime to the intrinsic trajectory training was informative and useful.</p><p>However, this addition does not fully address the three summary issues raised in the original decision letter. We recognize that these are very difficult to address, and hence a complete answer may not be possible. However, we think it would be appropriate to be more explicit about the limitations of the current work/approach. As one reviewer noted, the current manuscript presents the problems as more solved than warranted and thought it would be useful to note issues that remain relatively unsolved. Your response letter takes more of an approach along these lines than the Discussion section in the revision. For example, in the biological predictions part, it would be helpful to cleanly separate previously known and relatively generic predictions, such as input driven suppression of variability and generalization being challenging, from those that are more specific to the model such as the larger curvature radii. We think this more tempered approach will make the paper more impactful, helping lay out some issues in need of future exploration</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.31134.018</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>There are three ways the reviewers feel the paper could be improved, in order of importance:</p><p>1) What is the novel biological insight offered by the study? At present there is little detail on this. One prediction appears in the text: &quot;Specifically, the observation that slower stimuli yield trajectories with larger curvature radii implies that the neural population activity should exhibit larger fluctuations in their firing rates in response to slower stimuli&quot;. However, in order for that to be a true prediction one would need to show that this is a necessary, or at least generic property, for instance by studying multiple different types of tasks involving temporal warping, or showing somehow that without this increase in curvature radius the trajectories cannot be learned. Moreover, the effect size for this prediction shown in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1D</xref> appears quite mild (25% increase in range across the full span of warping). both generally and at the mechanistic level.</p></disp-quote><p>It is of course critical for a computational study to generate, and clearly enunciate, predictions, and we have now clarified what we view as the three most important predictions in the Discussion. Specifically:</p><p>i) Neural trajectories will be stable in response to local perturbations—potentially administered through optogenetic stimulation—and importantly that the neural trajectories elicited during sensory stimuli will be more resistant to perturbations than the neural trajectories unfolding during the motor epoch of sensory-motor tasks.</p><p>ii) Slower stimuli yield trajectories with larger curvature radii, implying that the neural population activity should exhibit larger fluctuations in their firing rates in response to slower stimuli.</p><p>iii) Trained networks fail at generalizing to unfamiliar spectral noise patterns, thereby predicting that trajectories in sensory areas will diverge more when presented with stimuli composed of artificial or unfamiliar spectral noise patterns. This prediction could be tested, for example, by training an animal to recognize sounds with noise injected on some principal components of the stimulus spectrogram but not others, and then tested on noise injected along the held-out PC dimensions.</p><p>We should clarify that the model does not predict that “without an increase in curvature radius the trajectories cannot be learned”. But rather that temporally invariant encoding of sensory patterns is a result of temporally scaled neural trajectories, wherein the change in the trajectory curvatures radius underlying this temporal scaling is significantly above zero (variable speed/constant distance) and below the warp factor of the sensory pattern (constant speed/variable distance). The reviewer correctly noticed that <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> predicts a change in firing rate of approximately 25% across the range of warps tested. This is because we predict that temporal scaling is a result of both a change in curvature and a change in linear speed (<xref ref-type="fig" rid="fig8">Figure 8A</xref>), and therefore the expected change in the curvature radius (and consequently the firing rate range) should be less than the warp factor. While the magnitude of the change, may indeed make this prediction harder to test, detecting changes in firing rates of 25% is well within the range of experimental studies. For example the effects of attention on firing rate generally fall, on average, within this range (Gregoriou et al., 2014; Maunsell and Treue, 2006).</p><disp-quote content-type="editor-comment"><p>2) What is the extent and nature of the generalization that these networks are capable of. The authors demonstrate some interpolation and extrapolation of time-warping as well as an ability to reject certain kinds of noise. A stronger test might be for instance whether a network given a training set with naturally spoken examples all time-warped to the same duration, would still be able to generalize to stimuli of different durations?</p></disp-quote><p>This is an important question that is partially addressed in point RC7 (see below) and in the motorepoch only training shown in <xref ref-type="fig" rid="fig7">Figure 7C</xref>. Specifically, a strength of the RNN approach is that encoding spatiotemporal stimuli in continuous neural trajectories endows it with some intrinsic sensory temporal invariance. Thus, training the network on utterances of the same duration results in good temporal scaling (similar to the M-trained RNN in <xref ref-type="fig" rid="fig7">Figure 7C</xref>, see also the Reservoir control <xref ref-type="fig" rid="fig8">Figure 8D</xref>). However, it remains the case that training across different durations significantly improves interpolation and extrapolation (as shown in <xref ref-type="fig" rid="fig7">Figure 7B-C</xref>). We have now expanded our discussion of the ability of RNNs to exhibit intrinsic temporal scaling.</p><disp-quote content-type="editor-comment"><p>3) How generic are the results? Is this approach different from previous generic deep learning results? Can the authors describe better the underlying mechanism, Can they convincingly say for instance that the mechanism that they describe in <xref ref-type="fig" rid="fig8">Figure 8</xref> for example is typical, or necessary? That it would remain the same given different choices about training paradigms, etc.</p></disp-quote><p>This point, raised in more detail under point RC6 below, relates to determining whether our results and predictions are general or specific to the learning rule we used. To test whether our results depend on the “innate” learning framework, we trained a recurrent network, similar to the ones used in our study, on the sensorimotor task with backpropagation through time (BPTT) applied via ADAM optimization (Kingma and Ba, 2014), a common training approach in the deep learning literature. Training and testing the network with temporally warped utterances (as in <xref ref-type="fig" rid="fig8">Figure 8</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>) yielded qualitatively similar results. Specifically, temporal scaling was observed across a wide range of speeds, however, performance was weaker than that of our innate training approach (compare <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref> with <xref ref-type="fig" rid="fig7">Figure 7</xref>). As stressed in RC6 below, this result strengthens our prediction relating to the mechanisms underlying temporal scaling.</p><p>Regarding the comparison with standard deep-learning approaches we would argue that there are there are fundamental differences. Specifically, standard deep-learning approaches generally do not operate in continuous time nor address the issue of dynamic/transient attractors and chaos. For example, the seminal 2014 paper by Sutskever et al. dealt with translation of sentences from one language to another, the inputs in their case were not auditory and time is non-continuous— as a result, temporal invariance was not addressed in their work. But deep learning has, of course, been extensively applied to the domain of artificial speech recognition (Chan et al., 2016; Graves et al., 2006), either using standard feedforward CNN networks or RNNs. However, the RNN models have focused primarily on LSTM networks, and thus their architecture is farther removed from biological realism:</p><p>* LSTM units possess state-dependent input and output gates, effectively yielding statedependent, time-varying and arbitrary effective time-constants per cell. As a result, temporal scaling within these models may result from LSTM gating dynamics, wherein the effective time-constants of the cells are warp-dependent. However, to the best of our knowledge, neurons or neural populations with this state-dependent and time-varying time constants have not been experimentally observed.</p><p>* Deep learning models often enlist bidirectional LSTMs to allow for current inputs to be processed in light of past <italic>and future</italic> context. While this serves to improve efficiency, it is certainly not biologically plausible.</p><disp-quote content-type="editor-comment"><p>Major concerns:</p><p>1) The network was trained in two steps in which the recurrent weights were trained and then the output weights were trained. I understand the rationale as it is stated in the main text. My question is whether this two-step training is necessary. In a biological case, the network would likely not be trained in such a way. Training would happen presumably all at once based only on the correctness of the output. It would be interesting to show how the network performs when the recurrent and output weights are trained simultaneously. What are the main differences in functionality the network can achieve? Also, a little more text on the rationale for the training procedure would be helpful to reader in the main text.</p></disp-quote><p>This is a good question, although it is not clear if separate stages of training in sensory-motor tasks should be considered less biological. For example, in sensory-motor learning in the song bird, there are clear distinctions between learning of the sensory component (the template) and motor learning—indeed the same is probably true in speech (Doupe and Kuhl, 1999). Nevertheless, it is not the case that learning has to be performed in two steps. Indeed one could interpret our training framework as having parallel plasticity, with learning rates of the recurrent weights being much larger than that of the output weights. Furthermore, we now establish that end-to-end training of the network with BPTT does not alter the mechanism underlying temporal invariance. We have now added text elaborating on this point.</p><disp-quote content-type="editor-comment"><p>2) One of the main novelties of the paper is the temporal invariance of the network. The authors state that other models have been developed that can also account for temporal invariance. However, these other models are not fully introduced, forcing the reader to go to the literature to make a comparison. It would be helpful if the text contained a longer introduction to other models and a discussion of how, in functional terms and experimental predictions, the new model presented here compares to and differs from the published models cited in the text.</p></disp-quote><p>We thank the reviewer for this suggestion. We have now updated the Discussion section with a more detailed descriptions of other models. Specifically, the main other biologically realistic model for temporally invariant processing of sensory patterns proposes synaptic shunting as a mechanism that amounts to a modulation of the time constants of the neurons by the warped input—in other words it is more akin to a cellular mechanism than a network mechanism (Gutig and Sompolinsky, 2009).</p><disp-quote content-type="editor-comment"><p>3) The paper emphasizes the idea of generalizing to different speeds of stimuli. The current results show that when trained on stimuli of different durations, generalization can occur. However, it is unclear if this generalization is a natural consequence of encoding stimuli as trajectories or requires training on different durations. That is, if the network is given a training set with naturally spoken examples all time-warped to the same duration, would the network be able to generalize to stimuli of different durations? The answer to this question helps the reader better understand the scope of the generalization that can occur.</p></disp-quote><p>Please see response #2 above.</p><disp-quote content-type="editor-comment"><p>4) How does the performance of the network differ for things like generalization, robustness to noise, etc. as a function of the size of the network? Different figures seem to use different size networks (n = 2100 in <xref ref-type="fig" rid="fig3">Figure 3 and n</xref> = 4000 in <xref ref-type="fig" rid="fig1">Figure 1</xref>), but it was not immediately clear how the network size was chosen.</p></disp-quote><p>The size of the network certainly affects its capacity, particularly for generalization. The reason for using a larger network in <xref ref-type="fig" rid="fig1">Figure 1</xref> was to address cross-speaker generalization. The smaller network size used in the other figures was not as effective when trained and tested across speakers. We now explain this in Materials and methods section of the paper.</p><disp-quote content-type="editor-comment"><p>5) One weakness of the paper is that the extent of mechanistic exploration of the RNN and experimental predictions is rather limited, at least in my opinion. I do not have specific suggestions for how to improve on this point. One possibility is further examination of the recurrent weight matrix to look for specific structure and dynamics that could generate additional experimental predictions. Given that I do not have specific questions here, I do not expect that something must be done in this area. I simply note this point because the impact of the paper will be enhanced if more mechanistic insight into how the RNN functions or into experimental predictions could be added.</p></disp-quote><p>We share the reviewer’s feelings here, and we think the point reflects the deeper question of what it means to “understand” an RNN. Indeed, this was a concern raised in the first review, and we believe the mechanistic analyses including the decomposition analysis that was added to the first revision is a strong approach—but admittedly the field as a whole must address the larger issue at stake.</p><disp-quote content-type="editor-comment"><p>6) The relation between this work and the substantial progress in deep learning on general sequence-to-sequence tasks (for example, &quot;Sequence to sequence learning with neural networks&quot; by Sutskever et al., 2014 and the dozens of subsequent papers that built upon it) is unclear. Most of these works in deep learning use backpropagation through time (BPTT) to train their networks, an algorithm which is conceptually very distinct from their tamed chaos method. Given the excellent track record of BPTT on many real-world sequence-to-sequence tasks, it seems that the point that the brain can use recurrent dynamics to solve such tasks is already evident. Perhaps the authors feel their approach is more appropriate in some way? If so, it would be important to compare the performance of the tamed chaos method with that of BPTT on more complex tasks and understand the relative advantages and shortcomings between them (for example, is there a task that one method can easily achieve but the other can't? Or is there some feature of the dynamics/networks that are learnt that appears more reasonable for one than the other). Moreover, given these two largely distinct methods to train RNNs, an important question that needs to be addressed is how the mechanisms by which generalization occurs in each method differ from each other. In other words, it is not clear which aspects of the mechanisms they identified in this paper are unique to the tamed chaos method or common to more general classes of algorithms to train RNNs. This paper provides a rather detailed analysis of how their particular training method enables generalization, but, for their analysis to be more useful in understanding how the RNNs in the brain operate, showing that this is a general mechanism in RNNs would be desired.</p></disp-quote><p>This is a very important point. As mentioned above (Summary point #3), the differences between the two techniques are more than just the training rule. Specifically, in regard to the treatment of time. E.g., in Sutskever <italic>et al. 2014,</italic> time is not explicitly present, and in many of the other models the temporal structure of the input is “spatialized” (i.e., at time step t discrete time step inputs from t-n, t-n-1, …, t, are presented to the network)—and thus does not capture how the brain represents time. Nevertheless to address the reviewers question we have now trained our network using a version of BPTT. Under some conditions (random orthogonal initialization of weight matrices with a high gain) BPTT resulted in good temporal scaling via similar mechanisms (temporal scaling of neural trajectories by changes in curvature radius)—but overall performance, as measured by CNN “handwritten” classification, was inferior. These results establish that the same solution was arrived at using different training methods, implying that our predictions are fairly general (even if some rules result in better performance than others). This result is consistent with our stance that these models are meant to capture computational principles of biological of recurrent neural networks, but the learning rules by which biological circuits reach these regimes remain to be elucidated.</p><p>These results are presented in <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>.</p><disp-quote content-type="editor-comment"><p>7) It was unclear how to interpret the fact that the motor-only-training networks were also robust to temporal warping (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). In fact, for time warping that is in between trained values, the interpolation regime, as the authors call it the motor-only-training networks were at 100% performance and one of the papers the authors cite (Lerner et al. 2014) shows that there is temporal scaling in human hemodynamic imaging mainly in the behavioral range, that is in the interpolation range. Given the chaotic nature of the networks pre-training wouldn't motor-trained-only networks have initial conditions that are almost as different for within digit than between digit? If this is true and the networks can still learn to be at 100% wouldn't that imply that the sensory training is in some sense redundant? I see the greater separation in <xref ref-type="fig" rid="fig7">Figure 7B</xref> but it is hard to evaluate whether the extra separation is important or overkill.</p></disp-quote><p>The network is initialized to a chaotic state which means it is chaotic in the absence of any external input, but as has been described, an external input can suppress this chaotic activity (Rajan et al., 2010). A nice feature of our RNN approach is that it does show some degree of intrinsic temporal scaling because the external input can partially “clamp” the internal dynamics (of course, sensory-epoch training leads to substantial performance improvements in terms of generalization to spectral noise and to different speakers (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) as well as improved temporal scaling (<xref ref-type="fig" rid="fig7">Figure 7</xref>)). This was discussed in the Results (<italic>Stability of the Neural Trajectories</italic>), and we now expand upon it in the Discussion section.</p><disp-quote content-type="editor-comment"><p>8) In general the treatment of temporal warping somewhat confusing. The text is written as if the solution to a time warped input is necessarily to generate the same trajectory running at a different speed. This doesn't have to be the case, in general all that is needed is that whatever dynamics happen, at the transition between the sensory and motor phases the within digit distance (including time warps, different utterances, etc.) be smaller than the between digit distance. The easiest way for this to be achieved is to arrive at the exact same point for different time warps. However, this is not necessary, nor is it what the network actually ends up doing (<xref ref-type="fig" rid="fig8">Figure 8</xref>). It was therefore very difficult for me to interpret the significance of <xref ref-type="fig" rid="fig8">Figure 8</xref>, that the network arrives at a mid-way solution between constant speed and constant distance. Is there something particularly interesting about this midpoint? Is there a reason it arrived at this compromise between these two possible mechanisms? Is it influenced by the way the authors generate the target trajectories by scaling?</p></disp-quote><p>This is a fascinating question regarding the nature of the network’s dynamics. The reviewer is certainly correct that other solutions could be used during the sensory-epoch. But it is more complicated than requiring that “at the transition between the sensory and motor phases the within digit distance (including time warps, different utterances, etc.) be smaller than the between digit distance”. Specifically, there is an interaction between this distance and the basin of attraction of the motor dynamic attractor: it is possible that at the end of the sensory epoch, despite being closer to the other within-digit trajectories, a trajectory may not end up within the basin of attraction of the corresponding digit’s motor dynamic attractor. Good performance will depend on the relationship between the size of this basin and the within digit-distance at the end of the sensory epoch. In other words, the size of the basin of attraction poses a stronger constraint on the withindigit distance. Since both of these characteristics of the network’s dynamics are affected by training, it appears that the network finds the best tradeoff. We now further highlight this interaction.</p><disp-quote content-type="editor-comment"><p>9) The authors make one main biological prediction: &quot;Specifically, the observation that slower stimuli yield trajectories with larger curvature radii implies that the neural population activity should exhibit larger fluctuations in their firing rates in response to slower stimuli&quot;. However, in order for that to be a true prediction at least in my mind one would need to show that this is a necessary, or at least generic property, for instance by studying multiple different types of tasks involving temporal warping, or showing somehow that without this increase in curvature radius the trajectories cannot be learned. Moreover, the effect size for this prediction shown in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1D</xref> appears quite mild (25% increase in range across the full span of warping).</p></disp-quote><p>Please see the response to Summary point #1.</p><disp-quote content-type="editor-comment"><p>10) Focusing on a specific set of realistic, complex stimuli and outputs is certainly a clear proof of concept of the task. Yet, the shortcoming of such an approach is that it is not immediately clear which part of the reported results are to some extent general, and what is in contrast specific to the particular implementation of the trained network (which relies on a very specific training procedure, and no doubt involves some fine-tuning). I guess what I am missing is a simplified, computational description of the underlying mechanism. Right now, I am left wondering how much details matter, and what happens when some details are changed. For instance, it seems that the specific implementation used in the paper requires that the motor action starts at a fixed time after the stimulus, and that it could not easily accommodate a variable delay between the stimulus and the motor output (e.g. specified by a go cue). Providing a simplified computational description of the mechanism, and examples of extensions/limitations would greatly improve the paper.</p></disp-quote><p>We thank the reviewer for this comment. The addition of a model using a different learning rule addresses, to some degree, the question of generality (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). But the reviewer is correct that in its current implementation the model does not adapt well to a variable sensory-motor delay. Arguably, in biological systems such delayed output invokes additional working memory dynamics, which could be incorporated by training the network to converge to a fixed-point attractor before the go signal. Regarding the presentation of a reduced model, this is something we have struggled with. Specifically, it is challenging to present a reduced model of something that we view as a truly emergent phenomenon (indeed, emergent phenomena can be defined as those that are not easily reducible). Thus, this relates to point RC5: what does it mean to understand the computations that emerge from a RNN. In our view many of the complex computations the brain performs are indeed emergent phenomena, and as such the field will need to address the question of how to study, perturb, establish causality, and understand these networks. Hopefully, this paper takes an initial step in this direction, by highlighting the challenge, showing that RNNs can solve complex sensori-motor tasks, and offering a tool to understand these dynamics (the RNN decomposition).</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Thank you for submitting a revision of your article &quot;Encoding Sensory and Motor Patterns as Time-Invariant Trajectories in Recurrent Neural Networks&quot; for consideration by eLife.</p><p>We found the manuscript improved. In particular, the inclusion of the back propagation trained network results that came up with a similar regime to the intrinsic trajectory training was informative and useful.</p><p>However, this addition does not fully address the three summary issues raised in the original decision letter. We recognize that these are very difficult to address, and hence a complete answer may not be possible. However, we think it would be appropriate to be more explicit about the limitations of the current work/approach. As one reviewer noted, the current manuscript presents the problems as more solved than warranted and thought it would be useful to note issues that remain relatively unsolved. Your response letter takes more of an approach along these lines than the Discussion section in the revision. For example, in the biological predictions part, it would be helpful to cleanly separate previously known and relatively generic predictions, such as input driven suppression of variability and generalization being challenging, from those that are more specific to the model such as the larger curvature radii. We think this more tempered approach will make the paper more impactful, helping lay out some issues in need of future exploration</p></disp-quote><p>We thank the reviewers for their further feedback, and for raising the point pertaining to making explicit statements about the limitations of the model, and issues that remain unaddressed. We, of course, did not mean to imply in the text that questions relating to how recurrent neural networks perform sensorimotor computations are solved, and hopefully made that clear in the letter, if not in the Discussion section. We have now added an additional paragraph to the Discussion section about limitations and future directions (see below). Finally, we have also clarified the predictions of the model with the goal of helping the reader understand how specific they are likely to be to the current model.</p><p>“These experimental predictions are critical to validate the model’s implementation of complex and invariant sensorimotor computations as stable neural trajectories. However, even if validated a number questions remain to be addressed. Most notably, how can the recurrent synaptic strengths be tuned to develop stable trajectories in a biologically plausible manner? The learning rule used here, coupled with its requirement of an explicit target for each recurrent unit, make it biologically implausible. However, while it is important that the sensorimotor representations are encoded as locally stable trajectories, the structure of the target trajectories themselves are essentially arbitrary. It is therefore conceivable that arbitrary yet locally-stable encoding trajectories may emerge from unsupervised learning. A second related issue is that to achieve strong temporal invariance, our model had to be trained over a range of sensory stimuli speeds. Again, it is not clear if this would represent a biologically plausible scenario—to help address this question, it will be important for future research to determine if the ability to recognize stimuli independent of speed is learned through experience. Finally, questions relating to the learning capacity of networks capable of strong temporal invariance, and the expedience of sensory-epoch training for temporal invariance remain open.”</p></body></sub-article></article>