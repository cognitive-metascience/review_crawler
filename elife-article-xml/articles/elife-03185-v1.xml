<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">03185</article-id><article-id pub-id-type="doi">10.7554/eLife.03185</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Vocalization–whisking coordination and multisensory integration of social signals in rat auditory cortex</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-13437"><name><surname>Rao</surname><given-names>Rajnish P</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-13604"><name><surname>Mielke</surname><given-names>Falk</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-13605"><name><surname>Bobrov</surname><given-names>Evgeny</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-13454"><name><surname>Brecht</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Bernstein Center for Computational Neuroscience</institution>, <institution>Humboldt University of Berlin</institution>, <addr-line><named-content content-type="city">Berlin</named-content></addr-line>, <country>Germany</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Berlin School of Mind and Brain</institution>, <institution>Humboldt University of Berlin</institution>, <addr-line><named-content content-type="city">Berlin</named-content></addr-line>, <country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Häusser</surname><given-names>Michael</given-names></name><role>Reviewing editor</role><aff><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>michael.brecht@bccn-berlin.de</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>08</day><month>12</month><year>2014</year></pub-date><pub-date pub-type="collection"><year>2014</year></pub-date><volume>3</volume><elocation-id>e03185</elocation-id><history><date date-type="received"><day>24</day><month>04</month><year>2014</year></date><date date-type="accepted"><day>01</day><month>11</month><year>2014</year></date></history><permissions><copyright-statement>Copyright © 2014, Rao et al</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Rao et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-03185-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.03185.001</object-id><p>Social interactions involve multi-modal signaling. Here, we study interacting rats to investigate audio-haptic coordination and multisensory integration in the auditory cortex. We find that facial touch is associated with an increased rate of ultrasonic vocalizations, which are emitted at the whisking rate (∼8 Hz) and preferentially initiated in the retraction phase of whisking. In a small subset of auditory cortex regular-spiking neurons, we observed excitatory and heterogeneous responses to ultrasonic vocalizations. Most fast-spiking neurons showed a stronger response to calls. Interestingly, facial touch-induced inhibition in the primary auditory cortex and off-responses after termination of touch were twofold stronger than responses to vocalizations. Further, touch modulated the responsiveness of auditory cortex neurons to ultrasonic vocalizations. In summary, facial touch during social interactions involves precisely orchestrated calling-whisking patterns. While ultrasonic vocalizations elicited a rather weak population response from the regular spikers, the modulation of neuronal responses by facial touch was remarkably strong.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.001">http://dx.doi.org/10.7554/eLife.03185.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.03185.002</object-id><title>eLife digest</title><p>Rats are highly social creatures, preferring to live in large groups within an established hierarchy. Social interactions range from play, mating, and parental care to displays of aggression and dominance and involve the use of odors, touch, and vocal calls. Touch typically takes the form of snout-to-snout contact, while most vocalizations are ultrasonic, with calls of different frequencies used to signal alarm or pleasure.</p><p>To date, most studies of rat vocalizations have involved playback of recorded calls to anaesthetized animals, and relatively little is known about how freely moving rats respond to calls. Rao et al. have now addressed this question by recording video footage of rats interacting with other animals or with objects and then using electrodes to record signals in the brains of these rats.</p><p>The video footage revealed that rats produce more vocal calls during social interactions than they do during non-social interactions. Moreover, bursts of calls appear to signal the beginning and end of bouts of snout-to-snout contact, suggesting that rodent communication involves the coordinated use of both tactile and vocal cues. Surprisingly, electrode recordings from the part of the brain that responds to sound—the auditory cortex—revealed that most neurons in this region did not respond to ultrasonic calls. However, a type of neuron called a fast-spiking neuron did respond strongly to these calls.</p><p>The work of Rao et al. shows that information from multiple senses is directly combined early in the processing of sensory information. Exactly why tactile stimuli should inhibit the auditory cortex is not clear, but there is some evidence that this may increase the rat's sensitivity to sounds. Further experiments are required to test this possibility and to determine how integrating information from multiple senses affects rodent behavior. This will help us to understand how the brain generates coherent social behaviour from signals arriving through distinct sensory channels.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.002">http://dx.doi.org/10.7554/eLife.03185.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>whisking</kwd><kwd>ultrasonic vocalization</kwd><kwd>social interaction</kwd><kwd>facial touch</kwd><kwd>auditory cortex</kwd><kwd>multisensory integration</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Bernstein Center for Computational Neuroscience Berlin</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Brecht</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Bundesministerium für Bildung und Forschung</institution></institution-wrap></funding-source><award-id>Forderkennzeichen 01GQ1001A</award-id><principal-award-recipient><name><surname>Brecht</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002839</institution-id><institution>Charité Universitätsmedizin Berlin</institution></institution-wrap></funding-source><award-id>Neurocure</award-id><principal-award-recipient><name><surname>Brecht</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Brecht</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>Gottfried Wilhelm Leibniz Prize</award-id><principal-award-recipient><name><surname>Brecht</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution>Humboldt Universität zu Berlin</institution></institution-wrap></funding-source><award-id>Berlin School of Mind and Brain Scholarship</award-id><principal-award-recipient><name><surname>Bobrov</surname><given-names>Evgeny</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.0</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Snout-to-snout contact modulates the response of rat auditory cortex to calls from other animals, indicating that the multisensory nature of social interaction is directly represented in the rat brain.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Rats are highly social animals that display complex behaviors (aggression, dominance, mating, parental care, and play [<xref ref-type="bibr" rid="bib2">Barnett, 1958</xref>]), which involve the use of multi-modal signaling and sensing (<xref ref-type="bibr" rid="bib9">Brecht and Freiwald, 2012</xref>). These social interactions are initially characterized by anogenital sniffing followed by facial contacts which occur at a constant rate (<xref ref-type="bibr" rid="bib48">Wolfe et al., 2011</xref>). Facial contacts have also been shown to be involved in dominance-related behaviors in naturalistic settings (<xref ref-type="bibr" rid="bib6">Blanchard et al., 2001</xref>). Facial contacts consist of extensive whisker-to-whisker and snout-to-snout touch episodes, largely mediated by macrovibrissae (<xref ref-type="bibr" rid="bib48">Wolfe et al., 2011</xref>). The use of macrovibrissae results in a strong activation of the barrel cortex and, importantly, the neurons appear to represent the social context of these interactions by distinct firing rates (<xref ref-type="bibr" rid="bib8">Bobrov et al., 2014</xref>).</p><p>Ultrasonic vocalizations (USVs) form another important component of rodent social interactions (<xref ref-type="bibr" rid="bib12">Brudzynski and Pniak, 2002</xref>; <xref ref-type="bibr" rid="bib49">Wright et al., 2010</xref>). Rats produce two distinct classes of USVs, 22 kHz alarm calls and 50 kHz appetitive vocalizations (<xref ref-type="bibr" rid="bib31">McGinnis and Vakulenko, 2003</xref>; <xref ref-type="bibr" rid="bib10">Brudzynski, 2009</xref>), which serve as indicators of negative and positive affective states, respectively (<xref ref-type="bibr" rid="bib30">Knutson et al., 2002</xref>). While the alarm calls are elicited by a range of threats (dominant or aggressive conspecifics, predators, and aversive stimuli [<xref ref-type="bibr" rid="bib10">Brudzynski, 2009</xref>]), the 50-kHz calls are produced in anticipation of/in response to direct social contact (<xref ref-type="bibr" rid="bib7">Blanchard et al., 1993</xref>; <xref ref-type="bibr" rid="bib4">Bialy et al., 2000</xref>; <xref ref-type="bibr" rid="bib12">Brudzynski and Pniak, 2002</xref>), mating (<xref ref-type="bibr" rid="bib4">Bialy et al., 2000</xref>), and homo-specific (rough-and-tumble) or hetero-specific (tickling) play behaviors (<xref ref-type="bibr" rid="bib14">Burgdorf et al., 2008</xref>). Further, playback experiments have demonstrated that 50-kHz vocalizations can induce approach behavior (<xref ref-type="bibr" rid="bib47">Wöhr and Schwarting, 2007</xref>).</p><p>Studies on the neuronal representation of USVs have largely relied on playback experiments, for example, to demonstrate the induction of c-fos expression (<xref ref-type="bibr" rid="bib37">Sadananda et al., 2008</xref>). Reliable and preferential responses to USVs have been reported upon playback in anaesthetized (<xref ref-type="bibr" rid="bib28">Kim and Bao, 2013</xref>) or awake rats (<xref ref-type="bibr" rid="bib15">Carruthers et al., 2013</xref>). However in the awake auditory cortex, a sparse representation of playback sounds has been demonstrated (<xref ref-type="bibr" rid="bib22">Hromádka et al., 2008</xref>). Little is known about the representation of the whole repertoire of conspecific calls in awake, behaving animals. Likewise, there is little information about the integration of multisensory social signals in the auditory cortex, with the notable exception of experience-dependent modulation of auditory responses by natural odors in mice (<xref ref-type="bibr" rid="bib16">Cohen et al., 2011</xref>). To address these issues, we employed the gap paradigm (<xref ref-type="bibr" rid="bib48">Wolfe et al., 2011</xref>; <xref ref-type="bibr" rid="bib43">von Heimendahl et al., 2012</xref>; <xref ref-type="bibr" rid="bib8">Bobrov et al., 2014</xref>), wherein a combination of USVs and facial touch enables the study of multisensory coordination and integration of social signals in the auditory cortex. Specifically, we pose the following questions: (i) How are calls and whisking related on coarse and fine time scales? (ii) How does auditory cortex respond to calls in interacting animals? (iii) How does auditory cortex respond to facial touch? (iv) Are responses to calls in the auditory cortex modulated by touch?</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To study the neuronal representation of multisensory signaling during social interactions in the gap paradigm (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), we combined low- and high-speed videography with acoustic and neuronal data acquisition. Rats engaged in extensive facial touch that involved whisker-to-whisker (n = 2894, <xref ref-type="fig" rid="fig1">Figure 1B</xref>, left) and snout-to-snout contacts (n = 2232, <xref ref-type="fig" rid="fig1">Figure 1B</xref>, right) with social and non-social stimuli (i.e., conspecifics and objects/plastinated rats, respectively). The interactions were spontaneous, frequent (1.76 events/min), and usually sustained (2.86 ± 2.68 s, mean ± SD; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>, <xref ref-type="other" rid="media1">Video 1</xref>). A total of 56,525 USVs were identified and classified into categories described earlier for rats in a social context (<xref ref-type="bibr" rid="bib49">Wright et al., 2010</xref>), that is, trill, complex, flat (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), upward ramp, downward ramp, inverted u, split, and short (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). A few fear calls were also observed but these were restricted to two animals (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). Analysis of call properties revealed characteristic mean frequencies, bandwidths, and call durations (<xref ref-type="table" rid="tbl1">Table 1</xref>). Intensity measurements from the four microphones were used to assign a source to the majority of USVs (80%). Analysis of false positive rates estimated that &gt;4 out of 5 calls were correctly assigned to the source (see ‘Materials and methods’).<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.03185.003</object-id><label>Figure 1.</label><caption><title>Facial touch is associated with increased ultrasonic vocalizations.</title><p>(<bold>A</bold>) Social interactions between subject and stimulus rats placed across a gap were documented using low- (30 Hz) and high-speed (250 Hz) cameras under infra-red (IR) illumination. USVs were recorded using four ultrasound microphones. (<bold>B</bold>) Facial touch consists of whisker (left) and snout contacts (right). Whisker tracking was facilitated by the use of tags (red circle; t<sub>0</sub> = time of first whisker contact). (<bold>C</bold>) Representative spectrograms of three major 50 kHz USV call categories emitted during social interactions, that is, trill, complex, and flat calls. (<bold>D</bold>) Raster plot indicating the relationship between facial touch episodes (gray bars, aligned to start, indicated by arrow) and individual USVs (vertical rasters) in a sample session. (<bold>E</bold>) USVs emitted in three scenarios, by the ‘Subject alone’, by all interacting partners after introduction of stimulus rats (‘Social setting’), and during ‘Facial touch’ were predominantly trill, complex, and flat calls. The proportion of trills (red) increased in the social contexts while flats (green) showed a reverse trend. Complex calls (blue) remained constant. (<bold>F</bold>) All call types were vocalized at a higher rate during facial touch compared to outside of it (even though stimuli were present). (<bold>G</bold>) Population PSTH aligned to the onset of facial touch indicates an increase in vocalization associated with whisker contact (bin size: 500 ms). (<bold>H</bold>) The end of whisker contact is associated with a sharp decrease in vocalization (bin size: 500 ms). (<bold>I</bold>) Interactions with non-social stimuli (objects/plastinated rats) had lower calling rates and no touch-associated modulation (bin size: 500 ms).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.003">http://dx.doi.org/10.7554/eLife.03185.003</ext-link></p></caption><graphic xlink:href="elife-03185-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03185.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Facial touch is associated with increased ultrasonic vocalizations.</title><p>(<bold>A</bold>) Distribution of durations of whisker-to-whisker (mean ± SD: 2.86 ± 2.68 s; left) and snout-to-snout (mean ± SD: 2.15 ± 2.7 s; right) episodes during facial touch when rats interact with conspecifics in the gap paradigm. (<bold>B</bold>) Representative spectrogram of upward ramp (ur), downward ramp (dr), inverted u (iu), split (sp), short (sh), and fear call (fc) emitted during social interactions (also present with the fear call is another overlapping call towards the end). (<bold>C</bold>) Histogram showing proportions (%) of various call categories emitted by the ‘Subject alone’ during baseline period, after introduction of stimulus rats (‘Social setting’), and during ‘Facial touch’. (<bold>D</bold>) Calling rates for various call categories during facial touch vs outside of touch showing a generalized increase in calling rates.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.004">http://dx.doi.org/10.7554/eLife.03185.004</ext-link></p></caption><graphic xlink:href="elife-03185-fig1-figsupp1-v1.tif"/></fig></fig-group><media content-type="glencoe play-in-place height-250 width-310" id="media1" mime-subtype="avi" mimetype="video" xlink:href="elife-03185-media1.avi"><object-id pub-id-type="doi">10.7554/eLife.03185.005</object-id><label>Video 1.</label><caption><title>Temporal coordination of whisking and vocalization.</title><p>During facial touch in the gap paradigm, rats engage in extensive whisker-to-whisker and snout-to-snout contacts (representative high-speed video slowed down by a factor of 16, bottom panel). In addition, ultrasonic vocalizations produced by the subject rat (left) are shown as spectrograms (slowed down by a factor of 16, top panel). The stimulus animal (right) was anaesthetized to remove confounds of the source of sound. Whiskers of the subject rat were tracked and whisking angles were calculated. A trace of the whisking angle indicates protraction (positive angles) and retraction (negative angles, middle panel). Onsets of individual ultrasonic vocalizations are indicated as vertical rasters in the whisker trace, indicating that vocalization and whisking appear to be temporally coordinated, with a bias towards the retraction phase of the whiskers.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.005">http://dx.doi.org/10.7554/eLife.03185.005</ext-link></p></caption></media><table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.03185.006</object-id><label>Table 1.</label><caption><p>Properties of USV categories emitted during social interactions.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.006">http://dx.doi.org/10.7554/eLife.03185.006</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Call type</th><th rowspan="2">n</th><th colspan="3">Frequency (kHz)</th><th colspan="3">Band width (kHz)</th><th colspan="3">Duration (ms)</th></tr><tr><th>Mean</th><th>± SEM</th><th>20th, 80th quantiles</th><th>Mean</th><th>± SEM</th><th>20th, 80th quantiles</th><th>Mean</th><th>± SEM</th><th>20th, 80th quantiles</th></tr></thead><tbody><tr><td>Trill</td><td>16,549</td><td>63.1</td><td>0.2</td><td>51.7, 76.9</td><td>10.3</td><td>0.1</td><td>5.1, 12.6</td><td>50.9</td><td>0.2</td><td>30.2, 67.1</td></tr><tr><td>Complex</td><td>13,662</td><td>54.8</td><td>0.1</td><td>48.1, 62.9</td><td>7.8</td><td>0.1</td><td>3.9, 8.7</td><td>39.0</td><td>0.2</td><td>23.8, 49.9</td></tr><tr><td>Flat</td><td>13,656</td><td>49.0</td><td>0.1</td><td>38.6, 56.0</td><td>5.2</td><td>0.1</td><td>2.7, 5.3</td><td>44.1</td><td>0.3</td><td>21.7, 56.4</td></tr><tr><td>Upward ramp</td><td>1280</td><td>51.6</td><td>0.4</td><td>45.7, 59.8</td><td>6.5</td><td>0.2</td><td>3.5, 8.4</td><td>26.8</td><td>0.3</td><td>15.9, 36.5</td></tr><tr><td>Downward ramp</td><td>258</td><td>48.0</td><td>1.0</td><td>31.7, 59.7</td><td>5.5</td><td>0.4</td><td>3.2, 6.8</td><td>19.9</td><td>0.8</td><td>11.6, 25.6</td></tr><tr><td>Inverted u</td><td>501</td><td>55.5</td><td>0.7</td><td>45.0, 67.4</td><td>5.7</td><td>0.2</td><td>3.2, 7.2</td><td>16.4</td><td>0.5</td><td>11.0, 18.4</td></tr><tr><td>Short</td><td>26</td><td>49.6</td><td>3.1</td><td>36.1, 65.1</td><td>4.1</td><td>0.3</td><td>3.1, 5.4</td><td>13.7</td><td>0.9</td><td>10.9, 15.6</td></tr><tr><td>Split</td><td>1072</td><td>45.2</td><td>0.5</td><td>34.0, 57.2</td><td>7.4</td><td>0.5</td><td>3.1, 7.3</td><td>68.1</td><td>1.4</td><td>38.9, 87.8</td></tr><tr><td>Fear call</td><td>299</td><td>28.0</td><td>0.5</td><td>22.9, 31.0</td><td>2.4</td><td>0.5</td><td>1.1, 2.3</td><td>206.5</td><td>16.6</td><td>30.2, 353.7</td></tr></tbody></table></table-wrap></p><sec id="s2-1"><title>Facial touch is associated with increased ultrasonic vocalization</title><p>To study the relationship between calling and social interactions, we aligned the USVs to the onsets of the facial touch episodes (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). We analyzed three scenarios: (i) when the subject rat was present alone on the setup (‘Subject alone’), (ii) after the introduction of stimulus rats during which interactions took place (‘Social setting’), and (iii) specifically during the facial touch episodes (‘Facial touch’). In all these scenarios, a vast majority of USVs (∼80%) were composed of just three call categories, that is, trill, complex, and flat (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Interestingly, the proportion of trills was greater in social contexts whereas the proportion of flats was greater when the animals were alone. A similar trend was observed in some of the minor call categories (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>). While the baseline calling rate was low when the subject rat was alone (0.13 Hz), a substantial increase (0.80 Hz) occurred in the social setting. Within the facial touch episodes, a further increase in vocalization (1.50 Hz) was observed. Indeed, the calling rate for all call categories increased during facial touch compared to periods outside of it (<xref ref-type="fig" rid="fig1">Figure 1F</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D</xref>), with trills showing the highest increase. Analysis of peri-stimulus time histograms (PSTHs) also revealed an increase in calling associated with the onset (<xref ref-type="fig" rid="fig1">Figure 1G</xref>) and a sharp decrease with the offset (<xref ref-type="fig" rid="fig1">Figure 1H</xref>) of facial touch. For interactions with non-social stimuli, the calling rate was lower (0.36 Hz) and there was no increase in calling during touch (<xref ref-type="fig" rid="fig1">Figure 1I</xref>). On the whole, this increase in calling rate indicates a role for USVs in social communication in our paradigm.</p></sec><sec id="s2-2"><title>Calling and whisking are temporally coordinated</title><p>Since facial touch was associated with whisker contacts and extensive vocalization, we wondered if whisking and calling were temporally related. To assess this, we tracked precise whisker positions in high-speed videos and mapped the call times onto this information. As shown in the representative example (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), calls were typically emitted in the retraction phase of the whisking cycle of the call emitter (top trace), whereas there was no systematic relationship of calling to the whisking of the interacting partner (bottom trace). Hence, when all whisking traces were aligned and averaged relative to calls, the whisking rhythmicity was preserved in the resulting call-triggered whisking average (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). We observed that the relationship of call onsets to whisking phase was distributed significantly non-uniformly relative to whisking for call emitter (n = 664, p = 0.0014, <xref ref-type="fig" rid="fig2">Figure 2C</xref>, top, Hodges–Anje test) but not for interacting partner (n = 705, p = 0.06, <xref ref-type="fig" rid="fig2">Figure 2C</xref>, bottom). There also appears to be a substantial bias for calls during the retraction phase of the emitter's whisking (381 in retraction vs 283 in protraction, <xref ref-type="fig" rid="fig2">Figure 2C</xref>, top) unlike for calls during the interacting partner's whisking (352 in retraction vs 353 in protraction, <xref ref-type="fig" rid="fig2">Figure 2C</xref>, bottom).<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.03185.007</object-id><label>Figure 2.</label><caption><title>Whisking and vocalization are coordinated during facial touch.</title><p>(<bold>A</bold>) Sample whisker traces of a call emitter (top) when aligned to its own vocalizations (gray bars) indicate a correlation with the retraction phase of the whisker. This was not the case with the whisker trace of the interacting partner (bottom). (<bold>B</bold>) Call-triggered whisking average wherein the average of all whisking traces is aligned with respect to the emitter's call onset. (<bold>C</bold>) Distribution of call onsets were significantly non-uniform (Hodges–Anje test) relative to whisking cycle of emitter itself (top) but not for the interacting partner (bottom). There appears to be a substantial bias for calls during the retraction phase of the emitter's whisking (top) unlike for calls during the interacting partner's whisking (bottom). (<bold>D</bold>) Predominant whisking rates determined by power spectral density of whisker traces during social touch is 8.1 Hz (left). Distribution of time intervals between whisking cycles within one animal (auto) correspondingly shows a peak at ca. 112 ms (middle). Points of maximum protraction were used for binning. Inter-animal (cross) whisking interval distribution suggests that the interacting animals do not whisk with a fixed phase relationship (right). (<bold>E</bold>) Power spectral density analysis to determine the predominant calling rates revealed a 7.6 Hz predominant frequency component (left). The distribution of time intervals between start of two subsequent vocalizations peaks around 144 ms (‘auto’, middle). The effect is not as pronounced when triggering to calls that were assigned to a different animal (‘cross’, right).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.007">http://dx.doi.org/10.7554/eLife.03185.007</ext-link></p></caption><graphic xlink:href="elife-03185-fig2-v1.tif"/></fig></p><p>We next analyzed the predominant whisking and vocalization rates and observed a peak at ∼8 Hz for both by power spectral density analysis (<xref ref-type="fig" rid="fig2">Figure 2D,E</xref>, left panels). Analysis of the distribution of time intervals between whisking cycles revealed a peak at 112 ms within animals (‘auto’, <xref ref-type="fig" rid="fig2">Figure 2D</xref>, middle) but not across animals (‘cross’, <xref ref-type="fig" rid="fig2">Figure 2D</xref>, right). This was consistent with previous observations that whisking is not coordinated across animals (<xref ref-type="bibr" rid="bib48">Wolfe et al., 2011</xref>). Similarly, in the analysis of the distribution of time intervals between the start of two subsequent vocalizations of the putative call emitter, a prominent peak was observed at 144 ms (‘auto’, <xref ref-type="fig" rid="fig2">Figure 2E</xref>, middle). Such rhythmicity was not obvious when the call onsets across interaction partners were analyzed (‘cross’, <xref ref-type="fig" rid="fig2">Figure 2E</xref>, right), suggesting that the coordination of whisking and vocalization is restricted to individuals. This analysis is constrained by the fact that ∼1 in 5 calls is incorrectly assigned to a particular call source. Sessions with a single source of USVs (where subject rats interact with plastinated rats/objects) would ideally provide unambiguous evidence as there is only one source of USVs. However, since very few calls are produced in such sessions, we presented subject rats with anaesthetized stimulus rats to elicit extensive vocalization. Analysis of tracked whiskers in the high-speed videos relative to call onset shows that the temporal coordination of calling and whisking is indeed observable (<xref ref-type="other" rid="media1">Video 1</xref>).</p></sec><sec id="s2-3"><title>Responses to ultrasonic vocalizations in auditory cortex are excitatory and cell type dependent</title><p>To study the neuronal representation of USVs, we performed extracellular tetrode recordings in the auditory cortex (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A–C</xref>) of awake, behaving rats (four females, four males) while they interacted with conspecifics. Single units were identified based on separation and stability criteria (see ‘Materials and methods’) and classified as putative regular-spiking (RS) or fast-spiking (FS) neurons (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1D–H</xref>). Analysis of PSTHs triggered to the call onsets revealed the presence of several neurons in the auditory cortex that responded strongly to the calls vocalized within that session (<xref ref-type="fig" rid="fig3">Figure 3A,C</xref>; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.03185.008</object-id><label>Figure 3.</label><caption><title>Responses to USVs in auditory cortex are excitatory and cell-type dependent.</title><p>(<bold>A</bold>) Representative PSTHs showing the response of a RS neuron to all USVs (top left) and to individual call categories that is, trill (top right), complex (bottom left), and flat (bottom right) (bin size: 10 ms). (<bold>B</bold>) Population data showing responses of RS neurons plotted as firing rate in response to USVs vs baseline (left, Wilcoxon signed rank test). The population showed a significant modulation in firing rate in response to USVs. The mean response indices ± SEM to either all calls or the major call categories also reveal little or no overall modulation (right). (<bold>C</bold>) Response of a representative FS neuron showing a stronger modulation to all calls and also to complex and trill calls (bin size: 10 ms). (<bold>D</bold>) Almost all FS neurons were also significantly upregulated by USVs (left, Wilcoxon signed rank test) and this was evident in their mean response indices ± SEM to all calls and to the various call categories (right).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.008">http://dx.doi.org/10.7554/eLife.03185.008</ext-link></p></caption><graphic xlink:href="elife-03185-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03185.009</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Locations of recording sites in the auditory cortex and cell-type classification.</title><p>(<bold>A</bold>) Photomicrograph of cytochrome oxidase stained brain section showing electrolytic lesions (arrowheads) along two tetrode tracts (solid lines) passing through the auditory cortex (bregma: −3.72 mm; magnification: 2×; scale bar: 1 mm). Dotted lines demarcate the sub-regions: auditory cortex dorsal (AuD), primary auditory cortex (Au1), and auditory cortex ventral (AuV). (<bold>B</bold>) Locations of recording sites across the auditory cortices of four female and four male subject rats. Bregma positions are as indicated in the rat brain atlas (reproduced with permission from <xref ref-type="bibr" rid="bib33">Paxinos and Watson, 2007</xref>, The Rat Brain in Stereotaxic Coordinates, sixth Edition, copyright Elsevier 2007, All Rights Reserved). (<bold>C</bold>) Location of recording sites as in (<bold>B</bold>). (<bold>D</bold>) Comparison of firing rates with full spike width indicated a bi-modal distribution of auditory cortex units. (<bold>E</bold>) A similar comparison with the second half width also resulted in a bi-modal distribution. (<bold>F</bold>) Using these features, k-means clustering of units with two clusters resulted in well separated populations (indicated by dotted line). (<bold>G</bold>) Spikes from these two populations resulted in well-defined average spike shapes, with the longer waveforms (orange) being classified as putative regular-spiking (RS) neurons and the thinner waveforms (green) classified as putative fast-spiking (FS) neurons. (<bold>H</bold>) Overall firing rates of these two classes were also highly differentiated (median firing rates: RS: 4.4 Hz vs FS: 11.1 Hz).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.009">http://dx.doi.org/10.7554/eLife.03185.009</ext-link></p></caption><graphic xlink:href="elife-03185-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03185.010</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Responses of auditory cortex neurons to ultrasonic vocalizations are heterogeneous.</title><p>(<bold>A</bold>–<bold>E</bold>) Representative PSTHs showing the response of RS neurons to all USVs and to individual call categories that is, trill, complex, and flat (bin size: 10 ms). (<bold>F</bold>–<bold>H</bold>) Representative PSTHs showing the response of FS neurons to all USVs and to individual call categories that is, trill, complex, and flat (bin size: 10 ms). Neurons that appear to be more strongly modulated by one call category than others are indicated by an asterisk (*).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.010">http://dx.doi.org/10.7554/eLife.03185.010</ext-link></p></caption><graphic xlink:href="elife-03185-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03185.011</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Population responses of auditory cortex neurons to different USV call categories.</title><p>(<bold>A</bold>) Population response of RS neurons (top) to various call categories compared to baseline firing rates showed significant modulation by trill, complex, and flat categories (Wilcoxon signed rank test). FS neurons (bottom) were also significantly modulated by trill category. (<bold>B</bold>) Scatter plot showing significant response of RS neurons to the ‘best call’, that is, maximal excitatory response when compared to the baseline firing rate (Wilcoxon signed rank test).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.011">http://dx.doi.org/10.7554/eLife.03185.011</ext-link></p></caption><graphic xlink:href="elife-03185-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03185.012</object-id><label>Figure 3—figure supplement 4.</label><caption><title>Population responses to own vs stimulus calls and in various sub-regions of the auditory cortex.</title><p>(<bold>A</bold>) RS neurons showed significant modulation to USVs of stimulus but not own calls (top) whereas FS neurons are modulated by own but not stimulus calls (bottom, Wilcoxon signed rank test). (<bold>B</bold>) Sub-region-wise comparison showed that AuD and Au1 RS neurons were significantly modulated by USVs (top) whereas Au1 FS neurons alone were modulated by USVs (bottom, Wilcoxon signed rank test).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.012">http://dx.doi.org/10.7554/eLife.03185.012</ext-link></p></caption><graphic xlink:href="elife-03185-fig3-figsupp4-v1.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03185.013</object-id><label>Figure 3—figure supplement 5.</label><caption><title>Auditory cortex neurons do not show any locking to the phase of whisking.</title><p>(<bold>A</bold>) Polar plots showing the preferred phase of auditory cortex neurons triggered to own (left) and stimulus whisking (right, Rayleigh vector lengths 0.1–0.5 with 0.2 indicated in red). Neurons that passed the shuffling test criteria are indicated in green. At the individual level neurons that have a Rayleigh vector length &gt;0.2 and have passed the shuffling test are considered to be locked to whisking. At the population level, there appears to be no significant locking of spiking with either the retraction or protraction phase of whisking.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.013">http://dx.doi.org/10.7554/eLife.03185.013</ext-link></p></caption><graphic xlink:href="elife-03185-fig3-figsupp5-v1.tif"/></fig></fig-group></p><p>Strikingly, most auditory cortex neurons showed little or no response to calls while ∼10% of RS neurons showed heterogeneous and excitatory responses (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A–E</xref>). Some RS neurons had a fast onset-associated response (response latency &lt;25 ms of call onset; <xref ref-type="fig" rid="fig3">Figure 3A</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A,B</xref>), while the others showed a more sustained response (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C–E</xref>). In addition, a few late responders were also observed (response latency &gt;50 ms after call onset; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2E</xref>). USVs elicited a small (∼5%) but significant increase in the population of RS neurons sampled (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left), as reflected in their firing rates (mean response to all calls ± SEM: 9.3 ± 0.82 Hz vs baseline: 8.8 ± 0.77 Hz; n = 172, p = 0.0006, Wilcoxon signed rank test). The median firing rates however were lower (response to calls: 6.06 Hz vs baseline: 5.42 Hz). This would suggest that the overall increase in the mean firing rate was contributed to by a small fraction of neurons within the population. This is in line with the observation that most neurons had little or no response to USVs despite extensive sampling of the auditory cortex across several animals (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B,C</xref>).</p><p>FS neurons on the other hand showed a robust excitatory modulation by USVs (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2F–H</xref>). Most FS neurons had a sustained response to calls (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2F</xref>). In addition, few cells with sharp short latency (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2G</xref>) or delayed (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2H</xref>) responses were also observed. The mean firing rates showed a consistent upregulation (response to calls: 19.11 ± 3.94 Hz vs baseline: 16.92 ± 4.07 Hz, n = 23, p = 0.006, Wilcoxon signed rank test), which is seen in the scatter plots (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, left).</p><p>Analysis of call category specificity revealed that many neurons (RS and FS) responded to more than one of the major call categories (<xref ref-type="fig" rid="fig3">Figure 3A,C</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A,C,D,F–H</xref>). Neurons more strongly modulated by one call category than others were also observed (indicated by asterisks; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>). Population level analysis showed that there is a significant modulation of RS neurons by all the major call categories (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A</xref>, top row). FS neurons however appear to be significantly modulated by trills alone (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A</xref>, bottom row). The lack of significant modulation by complex and flat calls is probably an artifact caused by the small sample size as most FS neurons show activation in the scatter plots (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A</xref>, bottom row). We next wanted to check if the population as a whole has a greater preference for one of the call categories. For this, we computed responses to the ‘best call’ that is, the major call type that elicits the highest modulation. While this scenario biased towards larger effects shows a highly significant modulation by the ‘best call’ (p &lt; 0.0001, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>), there appears to be no overwhelming population of trill-/complex-/flat-preferring neurons.</p><p>To compare the responses across cell types and call categories, we computed response indices for each neuron (see ‘Materials and methods’). The mean response indices of RS neurons to all calls and the major call categories showed a very small positive modulation (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, right), with no preference to any specific call category. Mean response indices of FS neurons, on the other hand, showed a robust positive modulation to all calls and to individual call categories (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, right). Also, the response indices of FS neurons were significantly different from those of RS neurons (p &lt; 0.0001, unpaired Mann–Whitney test).</p><p>We also determined that while RS neurons significantly responded to calls from the stimulus animals, this was not the case with the subject animal's own calls (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4A</xref>, top row). However, a pair-wise comparison of each RS neuron's response to own vs stimulus calls was not significantly different (p = 0.85, Wilcoxon signed rank test), suggesting that this was a population level effect. Interestingly, the reverse was true for the FS neurons, which showed a clear preference for own calls but not for the stimulus calls (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4A</xref>, bottom row). Pairwise comparisons of FS neuronal responses showed a significant difference (p = 0.03), suggesting that they could indeed discriminate between own and stimulus calls. We also compared the responses across various auditory cortex sub-fields, that is, auditory cortex dorsal (AuD), primary auditory cortex (Au1), and auditory cortex ventral (AuV). AuD and Au1 RS neurons as well as Au1 FS neurons were significantly modulated by USVs. However, the apparent lack of modulation in AuV RS, AuD, and AuV FS neurons is confounded by the small sample sizes (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4B</xref>).</p><p>Previously, having observed a correlation between whisking and calling, we next tested if the neuronal firing rate is correlated with the whisker position. Towards this, whisker tracking was performed on high-speed videos to determine whisker angle and phase. Spike time stamps aligned to this information was used to generate polar plots and compute Rayleigh vector lengths for each neuron which showed &gt;10 spikes each in touch episodes. The phase locking was also tested with a shuffling test that is insensitive to the spike count (see ‘Materials and methods’). At the population level, there was no strong preference to the whisking phase (<xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5A</xref>). A few cells that appeared to have a strong locking to the retraction phase were also strong responders to USVs, suggesting that this phase locking is an artifact from locking of whisking and vocalization.</p></sec><sec id="s2-4"><title>Facial touch evokes inhibition and off-responses in the primary auditory cortex</title><p>Unexpectedly, we observed that facial touch resulted in an inhibition of several RS (<xref ref-type="fig" rid="fig4">Figure 4A–D</xref>) and FS (<xref ref-type="fig" rid="fig4">Figure 4E–G</xref>) neurons. Exemplary spike raster (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) and PSTHs of RS (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) and FS (<xref ref-type="fig" rid="fig4">Figure 4E</xref>) neurons triggered to the onset (left) and end (right) of facial touch show the inhibition elicited due to touch and an off-response, which usually occurred within a 200-ms window at the end of touch. A substantial fraction of neurons in our dataset showed this distinct off-response (∼19%). This phenomenon appears to occur in Au1 (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) but not in the other auditory cortex sub-regions (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A,B</xref>).<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.03185.014</object-id><label>Figure 4.</label><caption><title>Facial touch evokes inhibition and off-responses in primary auditory cortex (Au1).</title><p>(<bold>A</bold>) Schematic showing the spiking activity (rasters) of a representative Au1 RS neuron aligned to a facial touch episode (gray bar). Facial touch onset results in an inhibition in firing rate and is associated with an increase upon the end of touch. (<bold>B</bold>) Representative PSTHs of a RS neuron triggered to onset (left) and offset (right) of facial touch demonstrating the inhibitory effect during touch and an off-response at the end of touch (bin size: 77 ms). (<bold>C</bold>) Population response of RS neurons plotted as firing rate after touch onset vs baseline (left) shows a significant inhibition due to facial touch. However, the off-response (in a 200 ms window after the end of facial touch) led to an increase in firing rate which was significantly higher than in touch (right, Wilcoxon signed rank test). (<bold>D</bold>) Mean response indices ± SEM of RS neurons during facial touch and off-response were significantly different (Wilcoxon signed rank test). (<bold>E</bold>) Representative PSTHs of a FS neuron triggered to facial touch onset (left) and offset (right) demonstrating a similar inhibitory effect due to touch and the off-response at the end of touch (bin size: 77 ms). (<bold>F</bold>) Population response of FS neurons showing a significant inhibition (Wilcoxon signed rank test) due to touch (left) that was released at the end of touch (right). (<bold>G</bold>) Mean response indices ± SEM of FS neurons were also significantly different (Wilcoxon signed rank test) between facial touch and off-response windows.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.014">http://dx.doi.org/10.7554/eLife.03185.014</ext-link></p></caption><graphic xlink:href="elife-03185-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03185.015</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Facial touch does not elicit inhibition in AuD and AuV; whisker/snout touch does not lead to measurable changes in sound intensities.</title><p>(<bold>A</bold>) Facial touch onset-associated inhibition (left) and offset associated increase in firing rate (right) do not occur in AuD RS neurons. (<bold>B</bold>) Similarly, these phenomena were also not observed in AuV RS neurons. (<bold>C</bold>) Plots of change in relative power measured during social interactions revealed no change when triggered to whisker touch (left) and snout touch (middle), whereas when triggered to USVs (right) a large increase was observed. Dotted lines denote the pre- and post-triggered averages.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.015">http://dx.doi.org/10.7554/eLife.03185.015</ext-link></p></caption><graphic xlink:href="elife-03185-fig4-figsupp1-v1.tif"/></fig></fig-group></p><p>In Au1 RS neurons, the firing rates reduced from 8.18 ± 1.03 Hz to 7.82 ± 1.04 Hz during the touch episode (n = 105, p = 0.0067, Wilcoxon signed rank test, <xref ref-type="fig" rid="fig4">Figure 4C</xref>, left). Compared to touch, the firing rate in the off-response window was significantly higher at 8.49 ± 1.1 Hz (n = 105, p = 0.0019, <xref ref-type="fig" rid="fig4">Figure 4C</xref>, right). Unexpectedly, this ∼10% increase in the population response was about twice as large as the excitatory responses elicited by USVs in RS cells. Similarly, for Au1 FS neurons, the baseline firing rates decreased from 14.6 ± 4.18 Hz to 13.82 ± 4.03 Hz due to touch (n = 15, p = 0.0302, <xref ref-type="fig" rid="fig4">Figure 4F</xref>, left). The firing rate during the off-response window was higher at 17.03 ± 4.71 Hz when compared to the touch episode (p = 0.0043, <xref ref-type="fig" rid="fig4">Figure 4F</xref>, right). The off-response to facial touch is remarkable as it was the largest population response we observed in the auditory cortex. Mean response indices for Au1 RS neurons (<xref ref-type="fig" rid="fig4">Figure 4D</xref>) showed that the overall inhibition due to touch was reversed after the touch episode (p = 0.0329). A similar inhibition at touch onset and reversal at the end of touch were also evident in Au1 FS neurons (p = 0.0026, <xref ref-type="fig" rid="fig4">Figure 4G</xref>).</p><p>It could be hypothesized that these responses in Au1 are brought about not by the touch itself but by the sounds produced due to touch. To test this, we measured the sound intensities triggered to whisker and snout touch and did not observe any increase in intensity. USVs lead to a large increase as expected (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref>).</p></sec><sec id="s2-5"><title>Responses to calls are modulated by facial touch</title><p>Given the robust inhibition observed in Au1, we next asked if facial touch modulates the responses of Au1 neurons to USVs. Indeed, we observed that in RS neurons, the response to calls was modulated by touch (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). The population response is also evident by an increased modulation by USVs in touch (greater scatter, <xref ref-type="fig" rid="fig5">Figure 5B</xref>, right) as compared to neurons responding to calls out of touch (<xref ref-type="fig" rid="fig5">Figure 5B</xref> left). Pairwise comparison of each neuron's response to USVs in (right) and out (left) of touch revealed a highly significant differential modulation (p = 0.0038, Wilcoxon signed rank test, <xref ref-type="fig" rid="fig5">Figure 5B</xref>). This was also evident in the frequency distribution histograms of the response indices. While response indices of RS neurons to calls out of touch were largely centered around zero indicating little or no modulation (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, left), they were more spread out during touch (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, right) indicating significantly greater modulation (p = 0.0072, Kolmogorov–Smirnov test). However, in FS neurons, responsiveness to calls appeared to be less strongly modulated by touch at the individual (<xref ref-type="fig" rid="fig5">Figure 5D</xref>) and population levels (p = 0.3303, <xref ref-type="fig" rid="fig5">Figure 5E</xref>). Correspondingly, the spread of activation indices did not differ in and out of touch (p = 0.3752, <xref ref-type="fig" rid="fig5">Figure 5F</xref>).<fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.03185.016</object-id><label>Figure 5.</label><caption><title>Stronger and variable modulation of responses to vocalizations by facial touch.</title><p>(<bold>A</bold>) Representative PSTHs of a RS neuron showing a stronger response to vocalizations during facial touch (right) compared to out of touch (left, bin size: 10 ms). (<bold>B</bold>) Population response of RS neurons also demonstrated an increased modulation to vocalizations (both excitation and inhibition) during touch (right) compared to outside of it (left), which were significantly different when subjected to a pair-wise comparison (Wilcoxon signed rank test). (<bold>C</bold>) Distribution of response indices showed that a large fraction of RS neurons were not modulated by vocalizations when out of touch (left) while a significant amount of modulation occurred during touch (right, Kolmogorov–Smirnov test). (<bold>D</bold>) Representative PSTHs of a FS neuron showing a higher response to vocalizations during touch (right) compared to the response when out of touch (left, bin size: 10 ms). (<bold>E</bold>) Population response of FS neurons however did not show any significant difference (Wilcoxon signed rank test) in modulation to vocalizations either in (right) or out (left) of touch. (<bold>F</bold>) Distribution of response indices in FS neurons also did not show any significant differences in modulation (Kolmogorov–Smirnov test) either in (right) or out (left) of touch.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.016">http://dx.doi.org/10.7554/eLife.03185.016</ext-link></p></caption><graphic xlink:href="elife-03185-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03185.017</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Sampling bias does not account for modulation of responsiveness to calls during touch; calls both from own and stimulus animals lead to increased modulation in primary auditory cortex during touch.</title><p>(<bold>A</bold>) Distribution of response indices from bootstrapped data out of touch (left) shows that a large fraction of RS neurons are not modulated by vocalizations. This was significantly different (Kolmogorov–Smirnov test) from the response during facial touch (right) where many cells showed increased modulation. (<bold>B</bold>) Similar analysis of the distribution of response indices of FS neurons revealed that there was no significant differences (Kolmogorov–Smirnov test) between bootstrapped out of touch (left) and in touch (right) data. (<bold>C</bold>) Distribution of response indices shows that many RS neurons exhibit higher modulation to own calls during touch (right), which was significantly different (Kolmogorov–Smirnov test) from the modulation to calls out of touch (left). (<bold>D</bold>) A similar distribution of responses indices to stimulus calls was also observed in RS neurons out of (left) and in (right) touch (Kolmogorov–Smirnov test).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.017">http://dx.doi.org/10.7554/eLife.03185.017</ext-link></p></caption><graphic xlink:href="elife-03185-fig5-figsupp1-v1.tif"/></fig></fig-group></p><p>The apparent increased modulation in responsiveness to calls during touch could be due to a sampling artifact as only a fraction of calls (19.8%) occur during touch. To rule out this potential confound, we applied bootstrapping analysis (see ‘Materials and methods’) to compute the mean response indices for our data set and plotted the frequency distribution histograms as before. These bootstrapped data show that neuronal responses in RS (but not FS) neurons were strongly modulated during touch, independent of the number of samples (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A,B</xref>). Another possible reason for this greater modulation could be the higher effective intensity of vocalizations in touch when the animals were closely juxtaposed. This would suggest that the response of RS neurons would be stronger to stimulus calls in touch while the response to the subject's own calls would not show much modulation (under the assumption that the subject rat vocalizes and perceives its own calls at roughly the same intensities). There, however, does not appear to be any greater responsiveness to stimulus calls during touch. This would suggest that the intensity differences are probably not responsible for the higher modulation during touch (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C,D</xref>). Taken together, these results suggest that the responsiveness to USVs in the auditory cortex is modulated by facial touch.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Multi-modal social signaling</title><p>Social interactions consist of complex behaviors which employ a range of multi-modal signaling and sensing (<xref ref-type="bibr" rid="bib9">Brecht and Freiwald, 2012</xref>). Social transmission of food preference which relies on the combined use of olfactory and gustatory cues (<xref ref-type="bibr" rid="bib19">Galef and Wigmore, 1983</xref>) is probably the most extensively studied example of this. In addition, the use of unimodal signals including vision (ear wiggling [<xref ref-type="bibr" rid="bib17">Erskine, 1989</xref>]), smell (cheek gland pheromones, [<xref ref-type="bibr" rid="bib24">Kannan and Archunan, 2001</xref>], and somatosensation [<xref ref-type="bibr" rid="bib6">Blanchard et al., 2001</xref>]) has also been reported. However, little is known about the multisensory integration of these signals, which would play a critical role in facilitating social interactions. In this inquiry, we study interacting rats to investigate audio-haptic coordination and multisensory integration in the auditory cortex. We demonstrate that facial touch during social interactions is associated with an increased production of USVs. We observe a temporal coordination of vocalization and whisking, with calls being associated with the retraction of whiskers. USVs elicited excitatory responses in a small fraction of RS neurons in the auditory cortex, whereas almost all FS neurons showed a strong activation. Facial touch however resulted in a robust inhibition of the primary auditory cortex. Moreover, we observed a remarkable off-response at the end of the touch episode, possibly reflecting a release of touch-induced inhibition, which was surprisingly the largest response modulation observed in our study. Finally, it appears that the response of auditory cortex neurons to USVs is modulated by facial touch.</p></sec><sec id="s3-2"><title>Role of USVs in social signaling</title><p>Previous studies from our laboratory have used the gap paradigm to study social interactions and have demonstrated that spontaneously interacting rats use their whiskers for extensive facial interactions (<xref ref-type="bibr" rid="bib48">Wolfe et al., 2011</xref>; <xref ref-type="bibr" rid="bib43">von Heimendahl et al., 2012</xref>). Interestingly, the social context (sex and sexual status) of these interactions appears to be represented in the barrel cortex neurons (<xref ref-type="bibr" rid="bib8">Bobrov et al., 2014</xref>). A little explored component of these interactions has been USVs, which are known to contribute to species-specific social signaling (<xref ref-type="bibr" rid="bib12">Brudzynski and Pniak, 2002</xref>; <xref ref-type="bibr" rid="bib11">Brudzynski, 2013</xref>). A reduction in social interaction and ultrasonic communication has also been reported in a mouse model of monogenic heritable autism (<xref ref-type="bibr" rid="bib23">Jamain et al., 2008</xref>). Indeed, using the gap paradigm, we observed an increase in the rate of vocalizations during the presentation of social (but not non-social) stimuli. Interestingly, the calling rate also showed a sharp decrease at the end of interactions.</p><p>We classified the USVs into specific call types largely based on an earlier description (<xref ref-type="bibr" rid="bib49">Wright et al., 2010</xref>). We observe that a vast majority of calls in our paradigm belong to one of the three categories that is, trill, complex, and flat. Further, the proportion of trills and flats appear to be modulated by the presence of conspecifics. These results are in line with the finding that there is a greater prevalence of flats in singly tested rats whereas trill vocalization is increased in pair-tested rats (<xref ref-type="bibr" rid="bib49">Wright et al., 2010</xref>). A role for trills in social contexts has also been reported during play behavior (<xref ref-type="bibr" rid="bib39">Schwarting et al., 2007</xref>). Taken together, these results would suggest that specific call categories could play a role during specific components of social interactions. This idea is supported by a recent study which reports that male mice emit distinct vocalizations when females leave the environment (<xref ref-type="bibr" rid="bib50">Yang et al., 2013</xref>).</p></sec><sec id="s3-3"><title>Temporal coordination of vocalization and whisking</title><p>Since facial touch involves the extensive use of whiskers and USVs, we employed high-speed videography to track the position of the whiskers with high temporal resolution and mapped the vocalization data onto this information. We observed a locking of calling to whisking, with calls being produced during the retraction phase of the whisking cycle. The retraction phase has been correlated to the exhalation phase of breathing (<xref ref-type="bibr" rid="bib32">Moore et al., 2013</xref>), which would correspond to call production. Previous studies have shown that in addition to a correlation of respiration and USV production (<xref ref-type="bibr" rid="bib36">Roberts, 1972</xref>; <xref ref-type="bibr" rid="bib35">Riede, 2011</xref>), sniffing and whisking are also tightly correlated (<xref ref-type="bibr" rid="bib46">Welker, 1964</xref>; <xref ref-type="bibr" rid="bib34">Ranade et al., 2013</xref>). The neural substrate of such coordination is thought to lie in the joint architecture of pattern generators for whisking and breathing (<xref ref-type="bibr" rid="bib32">Moore et al., 2013</xref>). Further, our results on the ∼8 Hz vocalization rate are in line with an earlier report that there is a selective increase in the representation of sounds repeated at an ethological rate (<xref ref-type="bibr" rid="bib29">Kim and Bao, 2009</xref>). Our findings on the temporally precise audio-haptic coordination in social signaling are reminiscent of much earlier findings on the multisensory orchestration of sensory acquisition (<xref ref-type="bibr" rid="bib45">Welker, 1971</xref>).</p></sec><sec id="s3-4"><title>Responses of regular-spiking neurons to ultrasonic vocalizations</title><p>RS neurons in the auditory cortex largely had little or no response to USVs. A very small fraction (∼10%) however showed robust responses to calls. When we did not observe more numerous responses to USVs, we initially wondered if the freely interacting animals simply did not hear many of the USVs. However, this explanation can be ruled out as: (i) responses to both own and stimulus were the same even though the subject rats were obviously always close to their own calls and (ii) FS cells responded robustly and significantly to calls. To our knowledge, this is the first report of neuronal responses to conspecific calls in awake behaving rats. Previous reports on the neuronal encoding of USVs in the rat auditory cortex have employed playback experiments in anesthetized preparations (<xref ref-type="bibr" rid="bib28">Kim and Bao, 2013</xref>) or awake rats (<xref ref-type="bibr" rid="bib15">Carruthers et al., 2013</xref>). While these studies report reliable population responses to vocalizations, our data are very similar to another study that shows that there is a sparse representation of sounds in the unanaesthetized auditory cortex (<xref ref-type="bibr" rid="bib22">Hromádka et al., 2008</xref>). Using a method that is not biased to neuronal activity (cell-attached recordings), the authors report that &lt;5% of neurons responded robustly to sounds at any instant. Further, they also report that narrow-spiking inter-neurons are highly responsive.</p><p>Of the strongly modulated neurons, we observe a range of responses: most cells displayed early (&lt;25 ms after call onset) increases in firing rates, while a few late responders (&gt;50 ms after call onset) were also observed. Responses tended to be sharp or sustained, with a few cells showing robust modulation by one or more call categories. At the population level, the major call categories evoked strong activation in the RS cell population whereas FS neurons appear to be modulated only by trills. Also, there does not appear to be any population level preference for one of these call types with relation to their best response.</p><p>Interestingly, RS neurons seem to prefer the stimulus calls whereas FS neurons were strongly modulated by the animal's own calls. However, a cell-wise analysis of response to own vs stimulus calls does not show any difference, suggesting that this phenomenon is brought out at the population level. The role of FS neurons in blanking out the animal's own calls and facilitating increased response to stimulus calls would be a tempting speculation. There also appears to be auditory cortex sub-region-wise differences, in that AuD and Au1 respond significantly to calls. Despite the temporal coordination of whisking and calling, we do not observe any strong preference of the auditory cortex to the whisking phase at the population level. Rare examples of neurons strongly locked to retraction could be artifacts from locking of whisking and vocalization.</p></sec><sec id="s3-5"><title>Response of auditory cortex to facial touch</title><p>We next analyzed the response of auditory cortex neurons to facial touch and observed an unexpected inhibition. This was particularly evident as an off-response; wherein at the end of the touch episode, a burst of firing was observed in the PSTHs indicating a possible release from inhibition. This was observed both in RS and FS neurons, but only in those from Au1. These observations are potentially confounded by the fact that whisker/snout contacts could in themselves produce sounds. Indeed, high sensitivity to broadband stimuli has been reported in the cat auditory cortex (<xref ref-type="bibr" rid="bib3">Bar-Yosef and Nelken, 2007</xref>). Sound intensity measurements were performed but no change was detected due to whisker movement and contacts, at least at the microphones. Both social and non-social touch induce inhibition, suggesting that the modulation we observe is likely to be tactile in nature.</p><p>An anatomical basis for these observations would lie in the direct connectivity between somatosensory and auditory cortices (<xref ref-type="bibr" rid="bib13">Budinger et al., 2006</xref>). Even prior to the cortical processing, multisensory integration of sound and touch has been shown to occur in the dorsal cochlear nucleus (<xref ref-type="bibr" rid="bib51">Young et al., 1995</xref>; <xref ref-type="bibr" rid="bib25">Kanold and Young, 2001</xref>; <xref ref-type="bibr" rid="bib41">Shore, 2005</xref>). It has also been suggested that a possible role for touch in the auditory areas is to enhance responses to non-self vocalizations while at the same time suppressing responses to self-generated sounds such as calls or respiration (<xref ref-type="bibr" rid="bib42">Shore and Zhou, 2006</xref>). Multisensory integration of visual (<xref ref-type="bibr" rid="bib5">Bizley et al., 2007</xref>) and olfactory information (<xref ref-type="bibr" rid="bib16">Cohen et al., 2011</xref>) in the auditory cortex has been demonstrated in ferrets and rats, respectively. Evidence for multisensory integration in the primary auditory cortex also comes from studies on the modulation by touch (<xref ref-type="bibr" rid="bib18">Fu et al., 2003</xref>; <xref ref-type="bibr" rid="bib26">Kayser et al., 2005</xref>) and vision (<xref ref-type="bibr" rid="bib27">Kayser et al., 2007</xref>) in primates.</p></sec><sec id="s3-6"><title>Modulation of responses to USVs by touch</title><p>The extensive modulation of primary auditory cortex by touch implies a possible role for touch-induced inhibition during social interactions where there is an increased vocalization. To test this, we analyzed the responsiveness of Au1 neurons to USVs in and out of touch. Interestingly, RS neurons exhibited a greater modulation to calls during touch as compared to out of touch. However, this was not the case with FS neurons. This large modulation of call responsiveness by touch could be construed as a sampling issue as calls during touch are fewer than those out of touch. However, bootstrapping analysis rules out this interpretation. Similarly, a higher intensity of stimulus calls during touch (due to close proximity) is less likely to be a contributing factor. Interestingly, there are several lines of evidence to suggest that inhibition in the auditory cortex could actually be responsible for an increased responsiveness to auditory stimuli. It has been reported that balanced inhibition underlies tuning and sharpens spike timing in auditory cortex (<xref ref-type="bibr" rid="bib44">Wehr and Zador, 2003</xref>) and that inhibition might contribute to auditory processing (<xref ref-type="bibr" rid="bib20">Hamilton et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Shamma, 2013</xref>). In the light of these findings, it would be interesting to understand how multisensory response modulation of auditory cortex neurons affects the behavior of interacting animals.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>Wistar rats (45- to 60-day old, female and male) were commercially procured (Harlan, Eystrup, Germany) and housed with a 12:12 hr inverted light/dark cycle and <italic>ad libitum</italic> access to food and water. While implanted (‘subject’) rats were housed individually after surgery, ‘stimulus’ rats were housed in groups of 2–3. After a 1-week post shipment recovery, rats were handled for 2–3 days, following which they were habituated to the behavioral setup for 3–4 days. All experimental procedures were performed in accordance to German regulations on animal welfare (Permit no. G0259/09).</p></sec><sec id="s4-2"><title>Low-speed videography and behavioral analysis</title><p>The gap paradigm (<xref ref-type="bibr" rid="bib48">Wolfe et al., 2011</xref>; <xref ref-type="bibr" rid="bib8">Bobrov et al., 2014</xref>) was used to study social interactions between conspecifics, wherein facial touch episodes occur freely across a gap between rats placed on two platforms (30 cm × 25 cm, <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The platforms (enclosed by 35 cm high walls on three sides) were elevated (20 cm) and placed in a Faraday cage to reduce electrical noise. The gap between the platforms was set to 20 ± 2 cm, depending on the size of the interacting partners. The entire setup was enclosed with black curtains and the room was darkened during experiments. An overhead low-speed camera (30 Hz) was used for continuous videography under infrared illumination (ABUS, Wetter, Germany). In most experiments, each recording session consisted of a 5 min baseline during which the subject rat was alone in the setup, a 5 min interaction time when stimulus animals/objects were presented across the gap, and another 5 min baseline at the end of interactions. On each recording day, 2–7 such recording sessions were conducted with various combinations of stimuli presented in a pseudo-random order. Stimuli presented included female and male conspecifics (60- to 120-day old), an object (Styrofoam block) and plastinated rats. These stimuli were presented either individually or in pairs (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Foam mats on the stimulus platform were changed between recording sessions to minimize olfactory cues. Offline analyses of videos were used to identify episodes of social facial interactions, and the following behavioral events were scored for: whisker overlap onset (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, left), snout touch onset (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, right), snout touch offset, and whisker overlap offset. The time of placement/removal of stimuli into/out of the setup was also scored for.</p></sec><sec id="s4-3"><title>Sound recording</title><p>Ultrasonic vocalizations produced by the rats were recorded using four microphones (condenser ultrasound CM16/CMPA, frequency range 10–200 kHz, Avisoft Bioacoustics, Berlin, Germany) placed under the elevated platforms (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Data were acquired using UltraSoundGate 416H at a sampling rate of 250 kHz and 16-bit resolution using Avisoft-RECORDER USGH software (Avisoft Bioacoustics, Berlin, Germany).</p></sec><sec id="s4-4"><title>Sound analysis</title><p>Acoustic analysis was performed using Avisoft SASLab Pro (Avisoft Bioacoustics, Berlin, Germany). Spectrograms were generated using the following fast Fourier transform (FFT) parameters: length of 1024 points and an overlap of 93.75% (FlatTop window, 100% frame size). The spectrograms had a frequency resolution of 244 Hz and a time resolution of 0.256 ms. Custom-written MATLAB codes (<xref ref-type="supplementary-material" rid="SD1-data">Source code 1</xref>) were also used to generate spectrograms using the Blackman–Harris window which resulted in a clearer appearance of frequency-modulated calls with steeper decays and a higher resolution (MathWorks, Natick, MA, USA). FFT parameters were: 500 points length and 80% overlap, resulting in 58 Hz frequency resolution and 0.4 ms time resolution. Start and end times of calls were manually set, and call category assignment was done as per an earlier classification (<xref ref-type="bibr" rid="bib49">Wright et al., 2010</xref>). A total of 56,525 USVs were identified and classified by two different experimenters while being blind to the behavioral events. A small fraction of calls (5.8%) could not be unambiguously assigned into any one of these categories and were classified as ‘unspecified’. In addition to call duration, mean frequency and band widths were computed. A subset (48,170 vocalizations) was acquired alongside recordings from the auditory cortex and used to compute neuronal responses to USVs.</p><p>To assign call source, low-speed videos of periods with only one animal present were superimposed with ultrasound recordings from all four channels. Analysis of these videos demonstrated a high degree of vocalization directionality. Hence, a comparison of call intensities on each of the channels was used to assign the source. A majority of USVs (80%) were assigned to either the subject or stimulus rats while 20% remained unassigned (and subsequently omitted from all emitter-dependent analyses). To check if the fraction of unassigned calls was greater during touch episodes where close facial proximity could obscure the exact source, we analyzed 11,194 calls that occurred during touch. Only 17.8% of these were unassigned, which is similar to the overall unassigned fraction, ruling out any obfuscation of source assignment due to touch. Noise (generally not directional) or simultaneous calls (rare) perturb this algorithm in principle, but visual inspection of spectrograms showed that such interferences were the exception.</p><p>To estimate the accuracy of this method, we analyzed the false positive rates (i.e., calls wrongly assigned to stimuli) in two conditions: (i) recordings of subject rats interacting with plastinated rats and objects: in 13 recordings with 1833 calls, 1286 calls (70%) were correctly assigned to the subject, 284 calls (15%) were wrongly assigned to stimuli, and 263 calls (14%) were unassigned, (ii) recordings when subject rats were present alone on the setup: out of 4299 calls, 3406 calls (79%) were correctly assigned, 196 calls (5%) were wrongly assigned to stimuli, and 697 calls (16%) were unassigned. Having excluded the unassigned calls from emitter-dependant analyses, we hence estimate that &gt;4 out of 5 calls were assigned to the correct source.</p><p>To compare intensity changes due to whisker touch, snout touch, and USVs, power spectral densities (PSD, as a function of time and frequency) were computed around these triggers. Integration over time verified that in the analyzed time window, power spectrum was indifferent to the trigger and frequency distribution was identical before and after it. Power was then cumulated over frequencies, which yields relative intensity as a function of time. Since the intensity values span several orders of magnitude, the geometric mean was used for comparison. Same number of triggers was randomly chosen to circumvent sample size issues. Interactions with vocalizations occurring in the time frame of interest as well as vocalizations that occurred simultaneously or in succession were excluded. All remaining episodes were averaged and traces plotted relative to the average during the pre-trigger phase. The pre-trigger average PSD was comparable across triggers (in the order of 10<sup>−8</sup> V<sup>2</sup> Hz<sup>−1</sup>).</p></sec><sec id="s4-5"><title>High-speed videography and whisker tracking</title><p>Whiskers of both subject and stimulus rats were tagged under 2–4% isoflurane anesthesia with small spherical drops of high-viscosity epoxy glue (Dymax 3021 UV-adhesive, Dymax Europe, Wiesbaden, Germany) that was hardened using ultraviolet light (Bluewave 50, Dymax Europe, Wiesbaden, Germany). The tag was covered with silver paint and fixed with superglue. Stimulus rats also received a black dot on the head to facilitate head tracking. Social interactions were recorded using a high-speed camera (A504k, Basler AG, Ahrensburg, Germany) at 250 Hz with 1280 × 1024 pixels. Video frames were streamed directly to a PCIe 1429 express card (National Instruments Corporation, Austin, TX, USA). Acquisition was controlled by custom-written Labview (National Instruments Corporation, Austin, TX, USA) programs. Whisker (one per animal) tracking was performed for a subset of 58 interactions (194 s of video) where the whiskers were clearly visible.</p><p>Tracking was done as has been described earlier (<xref ref-type="bibr" rid="bib48">Wolfe et al., 2011</xref>; <xref ref-type="bibr" rid="bib8">Bobrov et al., 2014</xref>) using a custom-written code (<xref ref-type="supplementary-material" rid="SD2-data">Source code 2</xref>). The process consisted of manual setting of head center and nose positions, automated contour detection and setting of whisker base and tag positions. Head axis (posterior to anterior) and whisker direction (outwards from whisker base) were acquired with custom-written MATLAB software. The head–whisker-angle was defined as zero in the orthogonal position, with protraction taking positive values up to a theoretical maximum of 180° and retraction being negative analogously. Whisker traces, that is, time series of whisker angle, were processed for signal cleansing, quality criteria, and signal decomposition as described elsewhere (<xref ref-type="bibr" rid="bib21">Hill et al., 2011</xref>). This includes up-sampling to 1000 Hz and 1–25 Hz bandpass filtering (fourth order Butterworth filter). Whisker cycles were disregarded when not between 50 and 250 ms length or when amplitude was &lt;7.5°. Hilbert transform yielded the phase within the whisking cycle starting at maximum protraction (0°), via maximum retraction (back-directed movement up to 180°) and return (≤360°). Whisking cycle phases at the onset of calls were analyzed for both the calling animal and the one towards which the calls were putatively directed.</p><p>To compute the locking of neuronal firing to whisker positions, spike time stamps for each cell were aligned to whisker traces and binned in a circular fashion before being normalized for phase occupancy. To the resulting polar plots, circular Rayleigh statistics were applied to calculate the Rayleigh vector (the summation of unit vectors in all individual angle observations to find a tendency in overall direction). The following criteria was set to identify putative phase locked cells: (i) minimum number of spikes (10) in a tracked episode, (ii) Rayleigh vector greater than 0.2, and (iii) shuffling test (see below) showing significance at a confidence interval of 5%. The shuffling test is necessary because regular tests, such as Rayleigh or Hodges–Ajne test for non-uniformity of circular data, are sensitive to very low- or high-spike numbers because of the extreme (high or low) variance among bins. Also, the number of bins can affect the test result. To test independently of these, the spike train was randomly time shifted relative to the whisker phase trace. The Rayleigh vector was computed again and this procedure repeated 10,000 times to simulate a random set of Rayleigh vectors from the spike series. If the original Rayleigh vector was in the 95% quantile of that distribution, it was presumed to be phase locked.</p></sec><sec id="s4-6"><title>Analysis of whisking and calling periodicity</title><p>To analyze periodicity of calling, the intervals between successive calls were computed. For whisking cycles, the point of maximum protraction served as a marker. The spectral power density of whisking and calling was determined using Welch's method (‘pwelch’, MATLAB signal processing toolbox) in the range of 4–25 Hz. This was applied to the approximately continuous whisker trace and to the outline of the calling interval histogram, the latter with lower resolution and less power due to the rough discretization.</p></sec><sec id="s4-7"><title>Electrophysiology</title><p>Neuronal activity was recorded using a chronic microdrive (Harlan 8-drive, Neuralynx, Bozeman, MT, USA) consisting of eight independently movable tetrodes (arranged in a 4 × 2 array). The tetrodes were fashioned out of 12.5-µm diameter nichrome wire (California Fine Wire Company, Grover Beach, CA, USA) and gold-plated to a resistance of 250–300 kΩ (nanoZ, Neuralynx, Bozeman, MT, USA). The microdrives were implanted on 8 ‘subject’ rats (four male, four female) spanning the following Bregma locations: −3.12 to −6.00 mm AP; 6.5 to 7.00 mm ML (<xref ref-type="bibr" rid="bib33">Paxinos and Watson, 2006</xref>).</p><p>For implanting the drive, rats were subjected to ketamine (100 mg/kg body wt)/xylazine (7.5 mg/kg body wt) anesthesia. Booster doses of anaesthetics were administered as required. Body temperature was maintained with a heating pad and continuously monitored by a rectal probe (Stoelting, Wood Dale, IL, USA). After securing the animal's head onto a stereotactic apparatus (Narashige Scientific Instrument Lab., Tokyo, Japan), lidocaine was injected subcutaneously under the scalp. After retraction of the temporal muscle, the cleaned skull surface was treated with a UV-activated etchant-cum-glue (Optibond All-In-One, Kerr Italia, Salerno, Italy). Gold-plated screws were fixed away from the craniotomy site to anchor the drive and provide for grounding. After craniotomy and durectomy, the microdrive was positioned on the brain and the area was covered with 1% agarose. The microdrive was secured with dental cement (Paladur, Heraeus Kulzer, Hanau, Germany).</p><p>Tetrodes were lowered into the brain and recordings typically began 1–2 days after surgery. Tetrodes were advanced by a minimum of 80 µm between recording days to ensure that new high quality units were sampled during the course of the experiment. After passing through a unity-gain headstage, signals were transmitted via a tether cable to an amplifier (Digital Lynx, Neuralynx, Bozeman, MT, USA). Spike signals were amplified (10×), digitized at 32 kHz, and bandpass-filtered between 0.6 and 6 kHz. Events that crossed a user-set threshold were recorded for 1 ms (250 µs before and 750 µs after voltage peak).</p><p>At the end of the experiment, subject rats were deeply anaesthetized using ketamine/xylazine and electrolytic lesions (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>) were performed by injecting 10 µA negative current through the tetrodes for 10 s (nanoZ, Neuralynx, Bozeman, MT, USA). Transcardiac perfusion was performed with cold phosphate buffer and 4% paraformaldehyde. The brains were dissected out and post-fixed in 4% paraformaldehyde (overnight). Coronal sections (150 µm) were stained for cytochrome oxidase and visualized using light microscopy to identify the lesions. Recording depths (determined by number of microdrive turns) were used to identify exact recording sites and cortical layers (<xref ref-type="bibr" rid="bib33">Paxinos and Watson, 2006</xref>) relative to lesions after accounting for shrinkage during tissue processing using Neurolucida (MBF Bioscience, Williston, VT, USA).</p></sec><sec id="s4-8"><title>Spike sorting and clustering</title><p>Amplitude and principal components were used for offline spike sorting using the semiautomatic clustering algorithm KlustaKwik (KD Harris, Rutgers University, Newark, NJ, USA). Manual correction and refinement were applied with MClust (AD Redish, University of Minnesota, Minneapolis, MN, USA) using MATLAB (MathWorks, Natick, MA, USA). Spike features (energy and first derivative of energy) were used for separation. Inclusion criteria for single units were determined by refractory period, separation quality, and stability. Inter-spike interval histograms with minimal or no contamination in the first 2 ms were indicative of a single unit. Separation quality was determined by L-ratio (&lt;0.2) and isolation distance (&gt;15) (<xref ref-type="bibr" rid="bib38">Schmitzer-Torbert et al., 2005</xref>). Stability of the units was quantified as follows: for each recording session, time periods outside interactions (as many in number and length as interactions on that day) were selected. This was performed on randomly distributed periods for 1000 permutations, and a linear correlation between time and firing rate was calculated. Average Pearson's R value was used as a measure of stability (higher R means stronger drift) and units with a value &gt;0.4 were excluded from analysis.</p></sec><sec id="s4-9"><title>Cell classification</title><p>Spike shapes were used to classify units as putative regular-spiking (RS) or fast-spiking (FS) neurons. RS neurons have been shown to have wider action potentials while FS neurons on the other hand have narrower action potentials (<xref ref-type="bibr" rid="bib1">Atencio and Schreiner, 2008</xref>). Auditory cortex single unit waveforms normalized by peak voltage were used to compute a whole host of features (height, trough, width parameters). Of these, two closely related features, full spike width and second half width showed bi-modal distributions (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1D,E</xref>). For this analysis, the waveforms were compared to the widest spike in the dataset (which was designated the value of 1). Consequently, the spike widths of thinner spikes get assigned with negative values (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1D,E</xref>). k-means clustering of units with two clusters resulted in well separated populations (indicated by dotted line, [<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1F</xref>]), albeit indicating a high correlation between these two features (arising possibly due to the spike waveforms being narrower than measured, which in turn is determined by filter settings). Indeed, spikes from these two populations resulted in well-defined average spike shapes (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1G</xref>) and were well differentiated with respect to firing rates (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1H</xref>).</p></sec><sec id="s4-10"><title>Response indices</title><p>To quantify the response of each neuron, we computed the average firing rate in the following response windows: in call duration alone and in a call duration + 25 ms window. For RS neurons, pair-wise comparison of basal firing rates were significantly different to the firing rates in the call duration alone (p = 0.0049) and call duration + 25 ms (p = 0.0006, Wilcoxon matched pairs signed rank test) response windows. However for FS neurons, the response was significantly different in the call duration + 25 ms window (p = 0.006), while it was not the case during the call duration alone (p = 0.0605). We also computed the onset responses in various time windows (0–25, 26–50, 51–75, 76–100 ms after call onset) as reported earlier (<xref ref-type="bibr" rid="bib22">Hromádka et al., 2008</xref>). For RS neurons, we found significant increase in all four time windows while for FS it was significant only for the 26–50 and 51–75 ms time windows. It must be noted that since the stimuli are not all of the same duration (for e.g., trills: 50.9 ± 26.2 ms, mean ± SD, <xref ref-type="table" rid="tbl1">Table 1</xref>), the ideal response windows should span the entire call durations. Thus it appears that the call duration + 25 ms response window sufficiently well describes the responses of both RS and FS neurons in our study and this was used to compute the neuronal responses to USVs. For facial touch, firing rate was defined as average firing rate during all interactions with an interaction partner. The off-response due to facial touch was calculated as the mean firing rate in a 200 ms window after the end of the touch episode. These firing rates were compared with a matched baseline period, which was as long as the USVs/interactions and shifted −10,000 ms relative to these on the spike train (jumping over any periods which contained a USV or interaction). For each neuron, a response index was calculated as follows: Response Index = (<italic>in</italic> − <italic>out</italic>)/(<italic>in</italic> + <italic>out</italic>), where <italic>in</italic> and <italic>out</italic> are the firing rates during a call/interaction and baseline firing rates, respectively.</p></sec><sec id="s4-11"><title>Statistics</title><p>Data were analyzed using Prism 6 (GraphPad Software Inc., La Jolla, CA, USA) or MATLAB (MathWorks, Natick, MA, USA) and are presented as mean ± SEM unless stated otherwise. Whisking phase preference of vocalizations was tested by Hodges–Ajne test. Since most of the data were not normally distributed (as determined by D'Agostino &amp; Pearson omnibus normality test), differences between groups were tested with Wilcoxon signed rank test for paired data and Mann–Whitney U test for unpaired data. Comparison of distribution widths was performed by Kolmogorov–Smirnov test.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by Humboldt Universität zu Berlin, the Bernstein Center for Computational Neuroscience Berlin, the German Federal Ministry of Education and Research (BMBF, Förderkennzeichen 01GQ1001A), and Neurocure. EB was a recipient of a Berlin School of Mind &amp; Brain scholarship. MB was a recipient of a European Research Council grant and the Gottfried Wilhelm Leibniz Prize. We thank Brigitte Geue, Undine Schneeweiß, Maik Kunert, and Juliane Steger for technical assistance; Viktor Bahr and Dominik Spicher for software development; Christian Ebbesen for sharing data; Carolin Mende for sharing art work; Ann Clemens, Christian Ebbesen, and members of the Brecht lab for valuable comments.</p></ack><sec sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>RPR, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>EB, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>FM, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con4"><p>MB, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All experimental procedures were performed in accordance to German regulations on animal welfare (Permit No. G0259/09).</p></fn></fn-group></sec><sec sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.03185.018</object-id><label>Source code 1.</label><caption><title>USV Classification.</title><p>Custom-written MATLAB codes to generate spectrograms from multi-channel ultrasonic recordings. The codes also enable semi-automatic identification and manual classification of rat ultrasonic vocalisations.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.018">http://dx.doi.org/10.7554/eLife.03185.018</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-03185-code1-v1.zip"/></supplementary-material><supplementary-material id="SD2-data"><object-id pub-id-type="doi">10.7554/eLife.03185.019</object-id><label>Source code 2.</label><caption><title>Whisker Tracking.</title><p>Custom-written MATLAB codes to manually track rat whisker and head positions to compute whisking parameters (angle, phase etc).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03185.019">http://dx.doi.org/10.7554/eLife.03185.019</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-03185-code2-v1.zip"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atencio</surname><given-names>CA</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year>2008</year><article-title>Spectrotemporal processing differences between auditory cortical fast-spiking and regular-spiking neurons</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>3897</fpage><lpage>3910</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5366-07.2008</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>SA</given-names></name></person-group><year>1958</year><article-title>An analysis of social behaviour in wild rats</article-title><source>Proceedings of the Zoological Society of London</source><volume>130</volume><fpage>107</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1111/j.1096-3642.1958.tb00565.x</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar-Yosef</surname><given-names>O</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><year>2007</year><article-title>The effects of background noise on the neural responses to natural sounds in cat primary auditory cortex</article-title><source>Frontiers in Computational Neuroscience</source><volume>1</volume><fpage>3</fpage><pub-id pub-id-type="doi">10.3389/neuro.10.003.2007</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bialy</surname><given-names>M</given-names></name><name><surname>Rydz</surname><given-names>M</given-names></name><name><surname>Kaczmarek</surname><given-names>L</given-names></name></person-group><year>2000</year><article-title>Precontact 50-kHz vocalizations in male rats during acquisition of sexual experience</article-title><source>Behavioral Neuroscience</source><volume>114</volume><fpage>983</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.1037/0735-7044.114.5.983</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Nodal</surname><given-names>FR</given-names></name><name><surname>Bajo</surname><given-names>VM</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year>2007</year><article-title>Physiological and anatomical evidence for multisensory interactions in auditory cortex</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2172</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl128</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchard</surname><given-names>RJ</given-names></name><name><surname>Dulloog</surname><given-names>L</given-names></name><name><surname>Markham</surname><given-names>C</given-names></name><name><surname>Nishimura</surname><given-names>O</given-names></name><name><surname>Nikulina Compton</surname><given-names>J</given-names></name><name><surname>Jun</surname><given-names>A</given-names></name><name><surname>Han</surname><given-names>C</given-names></name><name><surname>Blanchard</surname><given-names>DC</given-names></name></person-group><year>2001</year><article-title>Sexual and aggressive interactions in a visible burrow system with provisioned burrows</article-title><source>Physiology &amp; Behavior</source><volume>72</volume><fpage>245</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/S0031-9384(00)00403-0</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchard</surname><given-names>RJ</given-names></name><name><surname>Yudko</surname><given-names>EB</given-names></name><name><surname>Blanchard</surname><given-names>DC</given-names></name><name><surname>Taukulis</surname><given-names>HK</given-names></name></person-group><year>1993</year><article-title>High-frequency (35–70 kHz) ultrasonic vocalizations in rats confronted with anesthetized conspecifics: effects of gepirone, ethanol, and diazepam</article-title><source>Pharmacology, Biochemistry, and Behavior</source><volume>44</volume><fpage>313</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.1016/0091-3057(93)90467-8</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bobrov</surname><given-names>E</given-names></name><name><surname>Wolfe</surname><given-names>J</given-names></name><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Brecht</surname><given-names>M</given-names></name></person-group><year>2014</year><article-title>The representation of social facial touch in rat barrel cortex</article-title><source>Current Biology</source><volume>24</volume><fpage>109</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.11.049</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brecht</surname><given-names>M</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><year>2012</year><article-title>The many facets of facial interactions in mammals</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>259</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.12.003</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brudzynski</surname><given-names>SM</given-names></name></person-group><year>2009</year><article-title>Communication of adult rats by ultrasonic vocalization: biological, sociobiological, and neuroscience approaches</article-title><source>ILAR Journal</source><volume>50</volume><fpage>43</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1093/ilar.50.1.43</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brudzynski</surname><given-names>SM</given-names></name></person-group><year>2013</year><article-title>Ethotransmission: communication of emotional states through ultrasonic vocalization in rats</article-title><source>Current Opinion in Neurobiology</source><volume>23</volume><fpage>310</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.01.014</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brudzynski</surname><given-names>SM</given-names></name><name><surname>Pniak</surname><given-names>A</given-names></name></person-group><year>2002</year><article-title>Social contacts and production of 50-kHz short ultrasonic calls in adult rats</article-title><source>Journal of Comparative Psychology</source><volume>116</volume><fpage>73</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1037/0735-7036.116.1.73</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Budinger</surname><given-names>E</given-names></name><name><surname>Heil</surname><given-names>P</given-names></name><name><surname>Hess</surname><given-names>A</given-names></name><name><surname>Scheich</surname><given-names>H</given-names></name></person-group><year>2006</year><article-title>Multisensory processing via early cortical stages: Connections of the primary auditory cortical field with other sensory systems</article-title><source>Neuroscience</source><volume>143</volume><fpage>1065</fpage><lpage>1083</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2006.08.035</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgdorf</surname><given-names>J</given-names></name><name><surname>Kroes</surname><given-names>RA</given-names></name><name><surname>Moskal</surname><given-names>JR</given-names></name><name><surname>Pfaus</surname><given-names>JG</given-names></name><name><surname>Brudzynski</surname><given-names>SM</given-names></name><name><surname>Panksepp</surname><given-names>J</given-names></name></person-group><year>2008</year><article-title>Ultrasonic vocalizations of rats (<italic>Rattus norvegicus</italic>) during mating, play, and aggression: Behavioral concomitants, relationship to reward, and self-administration of playback</article-title><source>Journal of Comparative Psychology</source><volume>122</volume><fpage>357</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1037/a0012889</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carruthers</surname><given-names>IM</given-names></name><name><surname>Natan</surname><given-names>RG</given-names></name><name><surname>Geffen</surname><given-names>MN</given-names></name></person-group><year>2013</year><article-title>Encoding of ultrasonic vocalizations in the auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>109</volume><fpage>1912</fpage><lpage>1927</lpage><pub-id pub-id-type="doi">10.1152/jn.00483.2012</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Rothschild</surname><given-names>G</given-names></name><name><surname>Mizrahi</surname><given-names>A</given-names></name></person-group><year>2011</year><article-title>Multisensory integration of natural odors and sounds in the auditory cortex</article-title><source>Neuron</source><volume>72</volume><fpage>357</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.08.019</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erskine</surname><given-names>MS</given-names></name></person-group><year>1989</year><article-title>Solicitation behavior in the estrous female rat: a review</article-title><source>Hormones and Behavior</source><volume>23</volume><fpage>473</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1016/0018-506X(89)90037-8</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>KM</given-names></name><name><surname>Johnston</surname><given-names>TA</given-names></name><name><surname>Shah</surname><given-names>AS</given-names></name><name><surname>Arnold</surname><given-names>L</given-names></name><name><surname>Smiley</surname><given-names>J</given-names></name><name><surname>Hackett</surname><given-names>TA</given-names></name><name><surname>Garraghty</surname><given-names>PE</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year>2003</year><article-title>Auditory cortical neurons respond to somatosensory stimulation</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>7510</fpage><lpage>7515</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galef</surname><given-names>BG</given-names><suffix>Jnr</suffix></name><name><surname>Wigmore</surname><given-names>SW</given-names></name></person-group><year>1983</year><article-title>Transfer of information concerning distant foods: a laboratory investigation of the “information-centre” hypothesis</article-title><source>Animal Behaviour</source><volume>31</volume><fpage>748</fpage><lpage>758</lpage><pub-id pub-id-type="doi">10.1016/S0003-3472(83)80232-2</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>LS</given-names></name><name><surname>Sohl-Dickstein</surname><given-names>J</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Carels</surname><given-names>VM</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Bao</surname><given-names>S</given-names></name></person-group><year>2013</year><article-title>Optogenetic activation of an inhibitory network enhances feedforward functional connectivity in auditory cortex</article-title><source>Neuron</source><volume>80</volume><fpage>1066</fpage><lpage>1076</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.017</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>DN</given-names></name><name><surname>Curtis</surname><given-names>JC</given-names></name><name><surname>Moore</surname><given-names>JD</given-names></name><name><surname>Kleinfeld</surname><given-names>D</given-names></name></person-group><year>2011</year><article-title>Primary motor cortex reports efferent control of vibrissa motion on multiple timescales</article-title><source>Neuron</source><volume>72</volume><fpage>344</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.09.020</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hromádka</surname><given-names>T</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year>2008</year><article-title>Sparse representation of sounds in the unanesthetized auditory cortex</article-title><source>PLOS Biology</source><volume>6</volume><fpage>e16</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.0060016</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jamain</surname><given-names>S</given-names></name><name><surname>Radyushkin</surname><given-names>K</given-names></name><name><surname>Hammerschmidt</surname><given-names>K</given-names></name><name><surname>Granon</surname><given-names>S</given-names></name><name><surname>Boretius</surname><given-names>S</given-names></name><name><surname>Varoqueaux</surname><given-names>F</given-names></name><name><surname>Ramanantsoa</surname><given-names>N</given-names></name><name><surname>Gallego</surname><given-names>J</given-names></name><name><surname>Ronnenberg</surname><given-names>A</given-names></name><name><surname>Winter</surname><given-names>D</given-names></name><name><surname>Frahm</surname><given-names>J</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name><name><surname>Bourgeron</surname><given-names>T</given-names></name><name><surname>Ehrenreich</surname><given-names>H</given-names></name><name><surname>Brose</surname><given-names>N</given-names></name></person-group><year>2008</year><article-title>Reduced social interaction and ultrasonic communication in a mouse model of monogenic heritable autism</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>105</volume><fpage>1710</fpage><lpage>1715</lpage><pub-id pub-id-type="doi">10.1073/pnas.0711555105</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kannan</surname><given-names>S</given-names></name><name><surname>Archunan</surname><given-names>G</given-names></name></person-group><year>2001</year><article-title>Rat cheek gland compounds: behavioural response to identified compounds</article-title><source>Indian Journal of Experimental Biology</source><volume>39</volume><fpage>887</fpage><lpage>891</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanold</surname><given-names>PO</given-names></name><name><surname>Young</surname><given-names>ED</given-names></name></person-group><year>2001</year><article-title>Proprioceptive information from the pinna provides somatosensory input to cat dorsal cochlear nucleus</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>7848</fpage><lpage>7858</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name><name><surname>Augath</surname><given-names>M</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year>2005</year><article-title>Integration of touch and sound in auditory cortex</article-title><source>Neuron</source><volume>48</volume><fpage>373</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.09.018</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name><name><surname>Augath</surname><given-names>M</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year>2007</year><article-title>Functional imaging reveals visual modulation of specific fields in auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>1824</fpage><lpage>1835</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4737-06.2007</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Bao</surname><given-names>S</given-names></name></person-group><year>2013</year><article-title>Experience-dependent overrepresentation of ultrasonic vocalization frequencies in the rat primary auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>1087</fpage><lpage>1096</lpage><pub-id pub-id-type="doi">10.1152/jn.00230.2013</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Bao</surname><given-names>S</given-names></name></person-group><year>2009</year><article-title>Selective increase in representations of sounds repeated at an ethological rate</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>5163</fpage><lpage>5169</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0365-09.2009</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knutson</surname><given-names>B</given-names></name><name><surname>Burgdorf</surname><given-names>J</given-names></name><name><surname>Panksepp</surname><given-names>J</given-names></name></person-group><year>2002</year><article-title>Ultrasonic vocalizations as indices of affective states in rats</article-title><source>Psychological Bulletin</source><volume>128</volume><fpage>961</fpage><lpage>977</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.128.6.961</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGinnis</surname><given-names>MY</given-names></name><name><surname>Vakulenko</surname><given-names>M</given-names></name></person-group><year>2003</year><article-title>Characterization of 50-kHz ultrasonic vocalizations in male and female rats</article-title><source>Physiology &amp; Behavior</source><volume>80</volume><fpage>81</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/S0031-9384(03)00227-0</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>JD</given-names></name><name><surname>Deschênes</surname><given-names>M</given-names></name><name><surname>Furuta</surname><given-names>T</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Smear</surname><given-names>MC</given-names></name><name><surname>Demers</surname><given-names>M</given-names></name><name><surname>Kleinfeld</surname><given-names>D</given-names></name></person-group><year>2013</year><article-title>Hierarchy of orofacial rhythms revealed through whisking and breathing</article-title><source>Nature</source><volume>497</volume><fpage>205</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1038/nature12076</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paxinos</surname><given-names>G</given-names></name><name><surname>Watson</surname><given-names>C</given-names></name></person-group><year>2006</year><source>The rat brain in stereotaxic coordinates: hard cover edition</source><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranade</surname><given-names>S</given-names></name><name><surname>Hangya</surname><given-names>B</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><year>2013</year><article-title>Multiple modes of phase locking between sniffing and whisking during active exploration</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>8250</fpage><lpage>8256</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3874-12.2013</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riede</surname><given-names>T</given-names></name></person-group><year>2011</year><article-title>Subglottal pressure, tracheal airflow, and intrinsic laryngeal muscle activity during rat ultrasound vocalization</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>2580</fpage><lpage>2592</lpage><pub-id pub-id-type="doi">10.1152/jn.00478.2011</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>LH</given-names></name></person-group><year>1972</year><article-title>Correlation of respiration and ultrasound production in rodents and bats</article-title><source>Journal of Zoology</source><volume>168</volume><fpage>439</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7998.1972.tb01360.x</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadananda</surname><given-names>M</given-names></name><name><surname>Wöhr</surname><given-names>M</given-names></name><name><surname>Schwarting</surname><given-names>RK</given-names></name></person-group><year>2008</year><article-title>Playback of 22-kHz and 50-kHz ultrasonic vocalizations induces differential c-fos expression in rat brain</article-title><source>Neuroscience Letters</source><volume>435</volume><fpage>17</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2008.02.002</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmitzer-Torbert</surname><given-names>N</given-names></name><name><surname>Jackson</surname><given-names>J</given-names></name><name><surname>Henze</surname><given-names>D</given-names></name><name><surname>Harris</surname><given-names>K</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year>2005</year><article-title>Quantitative measures of cluster quality for use in extracellular recordings</article-title><source>Neuroscience</source><volume>131</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2004.09.066</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarting</surname><given-names>RK</given-names></name><name><surname>Jegan</surname><given-names>N</given-names></name><name><surname>Wöhr</surname><given-names>M</given-names></name></person-group><year>2007</year><article-title>Situational factors, conditions and individual variables which can determine ultrasonic vocalizations in male adult Wistar rats</article-title><source>Behavioural Brain Research</source><volume>182</volume><fpage>208</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2007.01.029</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year>2013</year><article-title>Inhibition mediates top-down control of sensory processing</article-title><source>Neuron</source><volume>80</volume><fpage>838</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.007</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shore</surname><given-names>SE</given-names></name></person-group><year>2005</year><article-title>Multisensory integration in the dorsal cochlear nucleus: unit responses to acoustic and trigeminal ganglion stimulation</article-title><source>The European Journal of Neuroscience</source><volume>21</volume><fpage>3334</fpage><lpage>3348</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2005.04142.x</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shore</surname><given-names>SE</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name></person-group><year>2006</year><article-title>Somatosensory influence on the cochlear nucleus and beyond</article-title><source>Hearing Research</source><volume>216–217</volume><fpage>90</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2006.01.006</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Heimendahl</surname><given-names>M</given-names></name><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Brecht</surname><given-names>M</given-names></name></person-group><year>2012</year><article-title>Weak and nondiscriminative responses to conspecifics in the rat hippocampus</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>2129</fpage><lpage>2141</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3812-11.2012</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wehr</surname><given-names>M</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year>2003</year><article-title>Balanced inhibition underlies tuning and sharpens spike timing in auditory cortex</article-title><source>Nature</source><volume>426</volume><fpage>442</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1038/nature02116</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welker</surname><given-names>C</given-names></name></person-group><year>1971</year><article-title>Microelectrode delineation of fine grain somatotopic organization of (SmI) cerebral neocortex in albino rat</article-title><source>Brain Research</source><volume>26</volume><fpage>259</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1016/S0006-8993(71)80004-5</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welker</surname><given-names>W</given-names></name></person-group><year>1964</year><article-title>Analysis of sniffing of the albino rat</article-title><source>Behaviour</source><volume>22</volume><fpage>223</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1163/156853964X00030</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wöhr</surname><given-names>M</given-names></name><name><surname>Schwarting</surname><given-names>RK</given-names></name></person-group><year>2007</year><article-title>Ultrasonic communication in rats: can playback of 50-kHz calls induce approach behavior?</article-title><source>PLOS ONE</source><volume>2</volume><fpage>e1365</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0001365</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>J</given-names></name><name><surname>Mende</surname><given-names>C</given-names></name><name><surname>Brecht</surname><given-names>M</given-names></name></person-group><year>2011</year><article-title>Social facial touch in rats</article-title><source>Behavioral Neuroscience</source><volume>125</volume><fpage>900</fpage><lpage>910</lpage><pub-id pub-id-type="doi">10.1037/a0026165</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname><given-names>JM</given-names></name><name><surname>Gourdon</surname><given-names>JC</given-names></name><name><surname>Clarke</surname><given-names>PB</given-names></name></person-group><year>2010</year><article-title>Identification of multiple call categories within the rich repertoire of adult rat 50-kHz ultrasonic vocalizations: effects of amphetamine and social context</article-title><source>Psychopharmacology</source><volume>211</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1007/s00213-010-1859-y</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Loureiro</surname><given-names>D</given-names></name><name><surname>Kalikhman</surname><given-names>D</given-names></name><name><surname>Crawley</surname><given-names>JN</given-names></name></person-group><year>2013</year><article-title>Male mice emit distinct ultrasonic vocalizations when the female leaves the social interaction arena</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>7</volume><fpage>159</fpage><pub-id pub-id-type="doi">10.3389/fnbeh.2013.00159</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>ED</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>Conley</surname><given-names>RA</given-names></name></person-group><year>1995</year><article-title>Somatosensory effects on neurons in dorsal cochlear nucleus</article-title><source>Journal of Neurophysiology</source><volume>73</volume><fpage>743</fpage><lpage>765</lpage></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.03185.020</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Häusser</surname><given-names>Michael</given-names></name><role>Reviewing editor</role><aff><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for sending your work entitled “Vocalization-whisking coordination and multisensory integration of social signals in rat auditory cortex” for consideration at <italic>eLife.</italic> Your article has been favorably evaluated by a Senior editor and 3 reviewers, one of whom is a member of our Board of Reviewing Editors.</p><p>The Reviewing editor and the other reviewers discussed their comments before we reached this decision, and the Reviewing editor has assembled the following comments to help you prepare a revised submission.</p><p>By analyzing responses in the auditory cortex of a rat interacting with another rat, the authors provide some interesting and novel evidence suggesting that whisker touch is coordinated with ultrasonic vocalization, and that responses in auditory cortex are modulated by touch. These are exciting results. However, the manuscript suffers from the fact that it is divided into two almost-independent parts, the first one dealing with the relation between whisking and vocalizations during facial interactions, and the second one describing the physiological correlates in auditory cortex. It would be desirable to further analyze the relationship between behavior and physiology, as described below. In addition, some of the major conclusions are not fully supported by the data that is presented.</p><p>Here is a summary of the major issues that need to be addressed in revision:</p><p>1) In the first part of the manuscript, the authors provide a detailed description of the type of calls rats emit during social interactions and the whisking behavior to which such calls are synchronized. However, this detailed information is not taken into account for the analysis of the physiological correlates. It would be interesting to know whether distinct types of calls can differentially modulate auditory cortex responses and whether there is a correlation between whisker position and modulation of firing rate.</p><p>2) The distinction between social and non-social touch is emphasized throughout the paper: “Surprisingly it appears that social but not non-social stimuli elicit this touch-related inhibition”. Yet the corresponding data is not shown. It is important that the data are presented as it is an unexpected result which will ultimately require further investigation. Furthermore, considering that during social touch other variables such as olfactory cues may be changing, this brings into question the conclusions of this paper that the effect is purely touch-mediated.</p><p>3) The authors should analyze neuronal responses to calls at higher temporal resolution (in and out of touch) rather than just integrating of longer temporal windows even extending beyond the end of the calls. Are there time locked on or off responses? How is the signal to noise ratio (e.g. Z-score) affected by touch? What is the effect of touch in a non-social setting? Bin size should be indicated.</p><p>4) The authors pooled recordings obtained from different sub-divisions of auditory cortex and recordings obtained from male and female animals. This may potentially introduce confounds. The authors should either limit their recordings to male or female and to A1 or AuV, or else, the data set should be extended to compare conditions.</p><p>5) The lack of responses by RS neurons could be accounted for by mismatch in frequency selectivity between the calls and the frequency responses of the neurons. For example, the Geffen lab (the Carruthers 2013 paper cited in the Introduction) used similar sounds and saw quite robust responses to vocalizations in rat A1. The simplest possibility is that the authors are recording in part of A1 that is not sensitive to ultrasonic frequencies; a change in 500 um can make a big difference in terms of the frequencies that neurons respond to, but it is difficult to tell from the coronal slices they show and the coordinates they describe, and they don't appear to do any kind of tone frequency mapping to figure out exactly where they are in A1. In addition, there could be a suboptimal choice of analysis windows (which were very long relative to the standard responses in auditory cortex). This issue can be resolved by more detailed analysis of the auditory responses.</p><p>6) The responses to touch in auditory cortex could be accounted for by sounds produced by the touch. The auditory system may be extremely sensitive to low-level broadband stimuli (see in the cat Bar-Yosef et al. 2007 and related papers). This could be resolved by (1) sensitive sound recordings (standard mics would probably not be sensitive enough for that); or (2) recordings from auditory cortex of acutely deafened rats.</p><p>7) The Discussion is very 'cortico-centric' – the auditory system has somatosensory-auditory interactions as early as the cochlear nucleus. This should also be considered in the Discussion.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.03185.021</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>We have provided new data, analyses and made revisions in the text and figures. Therefore, we believe that the revised manuscript is significantly strengthened after addressing key concerns expressed by the reviewers. Before we go on to provide a detailed response, we are listing below a summary of all of the major inclusions in the revised manuscript:</p><p>1) Data acquired from two more animals (our overall data set now stands at 4 males and 4 females)</p><p>2) Performed additional analysis to determine optimal response windows for modulation of auditory cortex neurons by ultrasonic vocalizations. We would particularly like to thank one of the reviewers for suggesting that we analyse earlier time windows. Indeed, we see several strong responses to calls and have reported this in the paper.</p><p>3) We also present detailed supplementary figures describing the heterogeneity of responses to calls seen in the auditory cortex, and responses to the different call categories.</p><p>4) We also present auditory cortex sub-region wise analyses, which have thrown up some interesting observations in relation to the effects of touch.</p><p>5) Extensive whisker tracking was also performed on of high-speed videos in order to analyse the relationship between whisking, calling and neuronal responses.</p><p><italic>1) In the first part of the manuscript, the authors provide a detailed description of the type of calls rats emit during social interactions and the whisking behavior to which such calls are synchronized. However, this detailed information is not taken into account for the analysis of the physiological correlates. It would be interesting to know whether distinct types of calls can differentially modulate auditory cortex responses and whether there is a correlation between whisker position and modulation of firing rate.</italic></p><p>The referee raises two important points here. First, the referee notes that we did not analyse the relationship between auditory cortex activity and whisking. We agree with the referee on the importance of this issue. In order to address this, we performed whisker tracking of high-speed videos and analysed the auditory cortex activity relative to the phase of whisking. As shown in <xref ref-type="fig" rid="fig3s5">Figure 3–figure supplement 5</xref>, we did not observe any systematic relationship between whisking phase and auditory cortex activity at the population level (in the 127 cells for which we obtained whisking data). A few cells did show a strong locking of firing rates with the retraction phase of the whisker, but this appeared be an artefact of the temporal coordination between whisking and calling. Indeed, these cells also showed a strong response to USVs.</p><p>Second, the referee asks about call type specific responses in our data set and we have now included a new supplementary figure with this information (<xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2</xref>). In summary, RS neurons often show responses to more than one call type, whereas we do see examples of them being strongly modulated by a single call type. FS neurons on the other hand in general appear to less call selective, though examples to the contrary can also be seen. The panels in this supplementary figure also document the heterogeneity in responses to calls (early, late, sharp and sustained) that we observed.</p><p>Changes:</p><p>1) We performed whisker tracking along with auditory cortex recordings. The results thereof are shown in <xref ref-type="fig" rid="fig3s5">Figure 3–figure supplement 5</xref>.</p><p>2) Inclusion of <xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2</xref>, which shows call-specific responses.</p><p><italic>2) The distinction between social and non-social touch is emphasized throughout the paper: “Surprisingly it appears that social but not non-social stimuli elicit this touch-related inhibition”. Yet the corresponding data is not shown. It is important that the data are presented as it is an unexpected result which will ultimately require further investigation. Furthermore, considering that during social touch other variables such as olfactory cues may be changing, this brings into question the conclusions of this paper that the effect is purely touch-mediated.</italic></p><p>In our earlier data set, we had observed a strong trend to this effect and had therefore made the claim. With the addition of new data, we however do not observe any significant modulation by social vs. non-social touch. We therefore retract the claim. While the effects of smell are important and certainly cannot be ruled out, the fact that both social and non-social touch induces inhibition suggests that the effect we observe is tactile in nature.</p><p>The statement has been deleted.</p><p><italic>3) The authors should analyze neuronal responses to calls at higher temporal resolution (in and out of touch) rather than just integrating of longer temporal windows even extending beyond the end of the calls. Are there time locked on or off responses? How is the signal to noise ratio (e.g. Z-score) affected by touch? What is the effect of touch in a non-social setting? Bin size should be indicated.</italic></p><p>First, the referee comments on the interesting idea of call responsiveness in and out of touch, given the strong effects of facial touch that we observe in the primary auditory cortex (Au1). Indeed in <xref ref-type="fig" rid="fig5">Figure 5</xref>, we report a much greater modulation of the neurons by USVs when in touch compared to out of it. Furthermore, this effect is seen in the regular-spiking but not fast-spiking neurons. Second, the referee asks about time locked on or off responses. We have included a supplementary figure (<xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2</xref>) to demonstrate several example of time locked on responses. As far as off responses are concerned, we saw no clear examples, possibly due of the different call durations. One could however speculate that the sustained and delayed responses are possibly due to calls of various durations eliciting an off response in some cells. In our paradigm of freely interacting animals, the number of ‘trials’ (facial interactions) and ‘calls’ depends on the animals leading to very variable sampling episodes and consequently varying levels of significance. We think that for such a data set, our statistical assessment (which does not build on z-scores) is informative. Specifically, we show plots of raw spike rates, compute response indices and apply non-parametric tests to assess the significance of the responses. The referee also raises the issue of non-social touch, to which we have responded above.</p><p>Changes:</p><p>1) Analysis of neuronal response to calls in primary auditory cortex at higher temporal resolution is shown as earlier in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p><p>2) Representative examples of time locked responses are shown in the new supplementary figure (<xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2</xref>).</p><p>3) The claim regarding differences between social and non-social touch has been deleted.</p><p>4) Bin sizes have been indicated in all the figure legends.</p><p><italic>4) The authors pooled recordings obtained from different sub-divisions of auditory cortex and recordings obtained from male and female animals. This may potentially introduce confounds. The authors should either limit their recordings to male or female and to A1 or AuV, or else, the data set should be extended to compare conditions.</italic></p><p>This suggestion by the referee indeed proved to be a great idea. Upon separating our dataset into the sub-regions of the auditory cortex, we observed the touch-evoked inhibition in Au1 but not in AuD or AuV (<xref ref-type="fig" rid="fig4s1">Figure 4–figure supplement 1A, B</xref>). Therefore, we have restricted out analysis of neuronal responses to calls in and out of touch to Au1 (<xref ref-type="fig" rid="fig5">Figure 5</xref>). In addition, we have also expanded our analyses to show the responsiveness of neurons from these sub-regions to USVs (<xref ref-type="fig" rid="fig3s4">Figure 3–figure supplement 4B</xref>). The suggestion to compare sex differences entails a large number of possible permutations and combinations (e.g. differences in calling behaviour and neuronal responses in relation to sex, estrous state, in and out of touch etc.). Inclusion of all this data is outside the scope of this manuscript and we would like to present this data in a separate manuscript.</p><p>Changes:</p><p>1) New supplementary figure (<xref ref-type="fig" rid="fig3s4">Figure 3–figure supplement 4B</xref>) included showing sub-region wise differences in responsiveness to USVs.</p><p>2) New supplementary figure (<xref ref-type="fig" rid="fig4s1">Figure 4–figure supplement 1A, B</xref>) included showing sub-region wise differences in responsiveness to touch.</p><p>3) Data in <xref ref-type="fig" rid="fig5">Figure 5</xref> restricted to Au1 as touch effects are only observed here.</p><p><italic>5) The lack of responses by RS neurons could be accounted for by mismatch in frequency selectivity between the calls and the frequency responses of the neurons. For example, the Geffen lab (the Carruthers 2013 paper cited in the Introduction) used similar sounds and saw quite robust responses to vocalizations in rat A1. The simplest possibility is that the authors are recording in part of A1 that is not sensitive to ultrasonic frequencies; a change in 500 um can make a big difference in terms of the frequencies that neurons respond to, but it is difficult to tell from the coronal slices they show and the coordinates they describe, and they don't appear to do any kind of tone frequency mapping to figure out exactly where they are in A1. In addition, there could be a suboptimal choice of analysis windows (which were very long relative to the standard responses in auditory cortex). This issue can be resolved by more detailed analysis of the auditory responses.</italic></p><p>The referee raises the important issue of under-/sub-optimal sampling of the auditory cortex and states that this could be an issue with the lack of responses. We have now included new supplementary panels (<xref ref-type="fig" rid="fig3s1">Figure 3–figure supplement 1B, C</xref>) to show the location of our recording sites over 8 animals and a range of Bregma values. Despite this, as the referee points out, we could still have been missing the USV responsive cells by being just slightly away. We did take up the suggestion of suboptimal choice of analysis windows and have indeed found much better responses to calls in shorter response windows. We would like to thank the reviewer for this particular suggestion. We analysed responses to calls in three shorter time windows: in the call duration alone, call duration + 25 ms and call duration + 50 ms. It must be noted here that since the stimuli are of varying durations (e.g. trills: 50.9 ± 26.2 ms, mean ± SD), the ideal response windows should span the entire call duration. We find strong responses in the call duration + 25 ms window for both RS and FS neurons and have used this to analyse the rest of the data (see Materials and Methods). In addition, we confirmed this fact by looking for onset responses in 0-25, 26-50, 51-75 and 76-100 ms windows as had been analysed earlier (<xref ref-type="bibr" rid="bib22">Hromádka et al., 2008</xref>). We now not only show several examples of neurons activated strongly by USVs (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2</xref>), but also report a significant modulation of the RS neurons by calls at the population level too (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p>Changes:</p><p>1) New supplementary panel (<xref ref-type="fig" rid="fig3s1">Figure 3–figure supplement 1B, C</xref>) indicating the spread of the recording sites across the auditory cortex (in 4 females and 4 males).</p><p>2) New analyses to identify call duration + 25 ms window as the optimal response window.</p><p>3) Confirmation by use different of onset response time windows (0-25, 26-50, 51-75 and 76-100 ms).</p><p>4) Inclusion of panels to show examples of neurons activated strongly by USVs (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2</xref>).</p><p>5) Reanalysis of modulation of the RS neurons by calls at the population level using the new response window (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p><italic>6) The responses to touch in auditory cortex could be accounted for by sounds produced by the touch. The auditory system may be extremely sensitive to low-level broadband stimuli (see in the cat Bar-Yosef et al. 2007 and related papers). This could be resolved by (1) sensitive sound recordings (standard mics would probably not be sensitive enough for that); or (2) recordings from auditory cortex of acutely deafened rats.</italic></p><p>The referee raises the possibility of the ‘sound of touch’ being responsible for the inhibition observed due to touch. We agree with the referee that this fact cannot be completely ruled out. To control for this in our paradigm, we measured the sound intensity levels in our recordings when triggered to the following events: onset of whisker-touch, onset of snout-touch and onset of USVs. While whisker-touch and snout-touch elicited little or no change of recorded audio power across all frequencies, USVs expectedly led to a large increase (<xref ref-type="fig" rid="fig4s1">Figure 4–figure supplement 1C</xref>). Thus, at least according to the microphones, touch is not associated with increased sound pressure levels. We did not perform deafening experiments suggested by the referee because we currently lack the additional animal ethics permit required. We are also concerned that such a manipulation might alter patterns of multisensory integration and hence be less conclusive that anticipated.</p><p>Changes:</p><p>1) New supplementary panel showing the sound intensity levels in our recordings triggered to onset of whisker-touch, snout-touch and USVs (<xref ref-type="fig" rid="fig4s1">Figure 4–figure supplement 1C</xref>).</p><p>2) Text has been added in the relevant Results and Discussion sections to state these results and the limitations of these analyses.</p><p><italic>7) The Discussion is very 'cortico-centric' – the auditory system has somatosensory-auditory interactions as early as the cochlear nucleus. This should also be considered in the Discussion.</italic></p><p>We agree that the previous manuscript had been cortico-centric. Yes, the role of cochlear nucleus deserves mention and we have included this. We thank the referee for pointing this out!</p><p>The relevant references have been cited in the Discussion.</p></body></sub-article></article>