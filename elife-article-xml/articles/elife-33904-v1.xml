<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">33904</article-id><article-id pub-id-type="doi">10.7554/eLife.33904</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Differential temporal dynamics during visual imagery and perception</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-101986"><name><surname>Dijkstra</surname><given-names>Nadine</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1423-9277</contrib-id><email>n.dijkstra@donders.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-84254"><name><surname>Mostert</surname><given-names>Pim</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-103001"><name><surname>Lange</surname><given-names>Floris P de</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-103002"><name><surname>Bosch</surname><given-names>Sander</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-98793"><name><surname>van Gerven</surname><given-names>Marcel AJ</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Donders Institute for Brain, Cognition and Behaviour</institution><institution>Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-2436"><name><surname>Kastner</surname><given-names>Sabine</given-names></name><role>Reviewing Editor</role><aff id="aff2"><institution>Princeton University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>29</day><month>05</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e33904</elocation-id><history><date date-type="received" iso-8601-date="2017-11-30"><day>30</day><month>11</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-04-30"><day>30</day><month>04</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Dijkstra et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Dijkstra et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-33904-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.33904.001</object-id><p>Visual perception and imagery rely on similar representations in the visual cortex. During perception, visual activity is characterized by distinct processing stages, but the temporal dynamics underlying imagery remain unclear. Here, we investigated the dynamics of visual imagery in human participants using magnetoencephalography. Firstly, we show that, compared to perception, imagery decoding becomes significant later and representations at the start of imagery already overlap with later time points. This suggests that during imagery, the entire visual representation is activated at once or that there are large differences in the timing of imagery between trials. Secondly, we found consistent overlap between imagery and perceptual processing around 160 ms and from 300 ms after stimulus onset. This indicates that the N170 gets reactivated during imagery and that imagery does not rely on early perceptual representations. Together, these results provide important insights for our understanding of the neural mechanisms of visual imagery.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.33904.002</object-id><title>eLife digest</title><p>If someone stops you in the street to ask for directions, you might find yourself picturing a particular crossing in your mind’s eye as you explain the route. This ability to mentally visualize things that we cannot see is known as visual imagery. Neuroscientists have shown that imagining an object activates some of the same brain regions as looking at that object. But do these regions also become active in the same order when we imagine rather than perceive?</p><p>Our ability to see the world around us depends on light bouncing off objects and entering the eye, which converts it into electrical signals. These signals travel to an area at the back of the brain that processes basic visual features, such as lines and angles. The electrical activity then spreads forward through the brain toward other visual areas, which perform more complex processing. Within a few hundred milliseconds of light entering the eye, the brain generates a percept of the object in front of us.</p><p>So, does the brain perform these same steps when we mentally visualize an object? Dijkstra et al. measured brain activity in healthy volunteers while they either imagined faces and houses, or looked at pictures of them. Electrical activity spread from visual areas at the back of the brain to visual areas nearer the front as the volunteers looked at the pictures. But this did not happen when the volunteers imagined the faces and houses. Contrary to perception, the different brain areas did not seem to become activated in any apparent order. Instead, the brain areas active during imagining were those that only became active during perception after 130 milliseconds. This is the time at which brain areas responsible for complex visual processing become active when we look at objects.</p><p>These findings shed new light on how we see with our mind’s eye. They suggest that when we imagine an object, the brain activates the entire representation of that object at once rather than building it up in steps. Understanding how the brain forms a mental image in real time could help us develop new technologies, such as brain-computer interfaces. These devices aim to interpret patterns of brain activity and display the output on a computer. Such equipment could help people with paralysis to communicate.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>mental imagery</kwd><kwd>visual perception</kwd><kwd>multivariate analysis</kwd><kwd>MEG</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>639.072.513</award-id><principal-award-recipient><name><surname>Bosch</surname><given-names>Sander</given-names></name><name><surname>van Gerven</surname><given-names>Marcel AJ</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>678286</award-id><principal-award-recipient><name><surname>Lange</surname><given-names>Floris P de</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>452-13-016</award-id><principal-award-recipient><name><surname>Lange</surname><given-names>Floris P de</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>406-13-001</award-id><principal-award-recipient><name><surname>Mostert</surname><given-names>Pim</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>In contrast to perception, during visual imagery, there are no clear time-locked processing stages and imagery specifically overlaps with perceptual processing around 160 ms after stimulus onset and from 300 ms onwards.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p>Visual imagery is the ability to generate visual experience in the absence of the associated sensory input. This ability plays an important role in various cognitive processes such as (working) memory, spatial navigation, mental rotation, and reasoning about future events (<xref ref-type="bibr" rid="bib36">Kosslyn et al., 2001</xref>). When we engage in visual imagery, a large network covering parietal, frontal and occipital areas becomes active (<xref ref-type="bibr" rid="bib19">Ganis et al., 2004</xref>; <xref ref-type="bibr" rid="bib28">Ishai et al., 2000</xref>). Multivariate fMRI studies have shown that imagery activates similar distributed representations in the visual cortex as perception for the same content (<xref ref-type="bibr" rid="bib48">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="bib38">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib2">Albers et al., 2013</xref>). There is a gradient in this representational overlap, in which higher, anterior visual areas show more overlap between imagery and perception than lower, posterior visual areas (<xref ref-type="bibr" rid="bib38">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Horikawa and Kamitani, 2017</xref>). The overlap in low-level visual areas furthermore depends on the amount of visual detail required by the task (<xref ref-type="bibr" rid="bib37">Kosslyn and Thompson, 2003</xref>; <xref ref-type="bibr" rid="bib8">Bergmann et al., 2016</xref>) and the experienced imagery vividness (<xref ref-type="bibr" rid="bib2">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Dijkstra et al., 2017a</xref>). Thus, much is known about the spatial features of neural representations underlying imagery. However, the temporal features of these representations remain unclear.</p><p>In contrast, the temporal properties of perceptual processing are well studied. Perception is a highly dynamic process during which representations change rapidly over time before arriving at a stable percept. After signals from the retina reach the cortex, activation progresses up the visual hierarchy starting at primary, posterior visual areas and then spreading towards secondary, more anterior visual areas over time (<xref ref-type="bibr" rid="bib51">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib39">Lerner et al., 2001</xref>; <xref ref-type="bibr" rid="bib55">Van Essen et al., 1992</xref>). First, simple features such as orientation and spatial frequency are processed in posterior visual areas (<xref ref-type="bibr" rid="bib26">Hubel and Wiesel, 1968</xref>) after which more complex features such as shape and eventually semantic category are processed more anteriorly (<xref ref-type="bibr" rid="bib43">Maunsell and Newsome, 1987</xref>; <xref ref-type="bibr" rid="bib57">Vogels and Orban, 1996</xref>; <xref ref-type="bibr" rid="bib50">Seeliger et al., 2017</xref>). After this initial feedforward sweep, feedback from anterior to posterior areas is believed to further sharpen the visual representation over time until a stable percept is achieved (<xref ref-type="bibr" rid="bib11">Cauchoix et al., 2014</xref>; <xref ref-type="bibr" rid="bib6">Bastos et al., 2015</xref>; <xref ref-type="bibr" rid="bib5">Bastos et al., 2012</xref>).</p><p>However, the temporal dynamics of visual imagery remain unclear. During imagery, there is no bottom-up sensory input. Instead, visual areas are assumed to be activated by top-down connections from fronto-parietal areas (<xref ref-type="bibr" rid="bib44">Mechelli et al., 2004</xref>; <xref ref-type="bibr" rid="bib16">Dijkstra et al., 2017b</xref>). Given the absence of bottom-up input, it might be that during imagery, high-level representations in anterior visual areas are activated first, after which activity is propagated down to more low-level areas to fill in the visual details. This would be in line with the reverse hierarchy theory (<xref ref-type="bibr" rid="bib1">Ahissar and Hochstein, 2004</xref>; <xref ref-type="bibr" rid="bib24">Hochstein and Ahissar, 2002</xref>). Alternatively, there may be no ordering such that during imagery, perceptual representations at different levels of the hierarchy are reinstated simultaneously. Furthermore, the temporal profile of the overlap between imagery and perception remains unclear. The finding that imagery relies on similar representations in low-level visual cortex as perception (<xref ref-type="bibr" rid="bib2">Albers et al., 2013</xref>) suggests that imagery might already show overlap with the earliest stages of perceptual processing. On the other hand, it is also possible that more processing is necessary before the perceptual representation has a format that can be accessed by imagery, which would be reflected by overlap arising later in time.</p><p>In the current study, we investigated these questions by tracking the neural representations of imagined and perceived stimuli over time. We combined magnetoencephalography (MEG) with multivariate decoding. First, we investigated the temporal dynamics within perception and imagery by exploring the stability and recurrence of activation patterns over time (<xref ref-type="bibr" rid="bib32">King and Dehaene, 2014</xref>; <xref ref-type="bibr" rid="bib3">Astrand et al., 2015</xref>). Second, we investigated the temporal overlap by exploring which time points during perception generalized to imagery.</p><sec id="s1" sec-type="results"><title>Results</title><sec id="s1-1"><title>Behavioral results</title><p>Twenty-five participants executed a retro-cue task in which they perceived and imagined faces and houses and rated their experienced imagery vividness on each trial using a sliding bar (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). Prior to scanning, participants filled in the Vividness of Visual Imagery Questionnaire, which is a measure of people’s imagery ability (<xref ref-type="bibr" rid="bib42">Marks, 1973</xref>). There was a significant correlation between VVIQ and averaged vividness ratings (<italic>r</italic> = −0.45, p=0.02), which indicates that people with a higher imagery vividness as measured by the VVIQ also rated their imagery as more vivid on average during the experiment. Participants reported relatively high vividness on average (49.6 ± 26.6 on a scale between −150 and +150, where 0 is the starting point of the slider and positive numbers indicate high vividness and negative numbers indicate low vividness). There was no significant difference in vividness ratings between faces (54.0 ± 29.7) and houses (48.7 ± 26.7; <italic>t</italic>(24) = 1.46, p=0.16). To ensure that participants were imagining the correct images, on 7% of the trials participants had to indicate which of four exemplars they imagined. The imagined exemplar was correctly identified in 89.8% (±5.4%) of the catch trials, indicating that participants performed the task correctly. There was also no significant difference between the two stimulus categories in the percentage of correct catch trials (faces: 90.9 ± 6.6, houses: 88.8 ± 7.1; <italic>t</italic>(24) = −1.25, p=0.22). Furthermore, correctly identified catch trials were experienced as more vivid (52.06 ± 33.1) than incorrectly identified catch trials (24.31 ± 33.84; <italic>t</italic>(23) = 3.11, p=0.0049), giving support to the criterion validity of the identification task. We had to drop one subject for this comparison because this person did not have any incorrect trials.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.33904.003</object-id><label>Figure 1.</label><caption><title>Experimental design.</title><p>Two images were presented for 0.8 seconds each, with a random inter-stimulus interval (ISI) between 400 and 600 ms. After the second image, a mask with random noise was on screen for 500 ms. The retro-cue indicating which of the two images the participants had to imagine was shown for 500 ms. Subsequently, a frame was presented for 3.5 s within which the participants imagined the cued stimulus. After this, they rated their experienced vividness on a continuous scale. On a random subset (7%) of trials, the participants indicated which of four exemplars they imagined that trial. The face stimuli were adapted from the multiracial face database (courtesy of Michael J Tarr, Center for the Neural Basis of Cognition and Department of Psychology, Carnegie Mellon University (<ext-link ext-link-type="uri" xlink:href="http://www.tarrlab.org">http://www.tarrlab.org</ext-link>) and available at <ext-link ext-link-type="uri" xlink:href="http://wiki.cnbc.cmu.edu/Face_Place">http://wiki.cnbc.cmu.edu/Face_Place</ext-link> under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-sa/3.0/">https://creativecommons.org/licenses/by-nc-sa/3.0/</ext-link>)).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-33904-fig1-v1"/></fig></sec><sec id="s1-2"><title>Representational dynamics during perception and imagery</title><p>To uncover the temporal dynamics of category representations during perception and imagery, we decoded the category from the MEG signal over time. The results are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Testing and training on the same time points revealed that during perception, significantly different patterns of activity for faces and houses were present from 73 ms after stimulus onset with the peak accuracy at 153 ms (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, left). During imagery, category information could be decoded significantly from 540 ms after retro-cue onset, with the peak at 1073 ms (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). The generation of a visual representation from a cue thus seems to take longer than the activation via bottom-up sensory input. Furthermore, imagery decoding resulted in a much lower accuracy than perception decoding (peak perception is at ~90%, peak imagery at ~60%). This is also observed in fMRI decoding studies (<xref ref-type="bibr" rid="bib48">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="bib38">Lee et al., 2012</xref>) and is probably due to the fact that imagery is a less automatic process than perception, which leads to higher between trial variation in neural activations. Note that, to allow better comparison between perception and imagery, we only showed the first 1000 ms after cue onset during imagery (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for the results throughout the entire imagery period).</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.33904.004</object-id><label>Figure 2.</label><caption><title>Decoding performance of perception and imagery over time.</title><p>(<bold>A</bold>) Decoding accuracy from a classifier that was trained and tested on the same time points. Filled areas and thick lines indicate significant above chance decoding (cluster corrected, p&lt;0.05). The shaded area represents the standard error of the mean. The dotted line indicates chance level. For perception, zero signifies the onset of the stimulus, for imagery, zero signifies the onset of the retro-cue. (<bold>B</bold>) Temporal generalization matrix with discretized accuracy. Training time is shown on the vertical axis and testing time on the horizontal axis. Significant clusters are indicated by black contours. Numbers indicate time points of interest that are discussed in the text. (<bold>C</bold>) Proportion of time points of the significant time window that had significantly lower accuracy than the diagonal, that is specificity of the neural representation at each time point during above chance diagonal decoding (<bold>D</bold>) Source level contribution to the classifiers at selected training times. Source data for the analyses reported here are available in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>.</p><p><supplementary-material id="fig2sdata1"><object-id pub-id-type="doi">10.7554/eLife.33904.008</object-id><label>Figure 2—source data 1.</label><caption><title>Temporal dynamics within perception and imagery.</title><p>The files '...Acc.mat' contain the subjects x time x time decoding accuracy within perception and imagery. The files '...Sig.mat' contain the time x time cluster p values of the comparison of these accuracies with chance level.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-33904-fig2-data1-v1.zip"/></supplementary-material></p><p><supplementary-material id="fig2sdata2"><object-id pub-id-type="doi">10.7554/eLife.33904.009</object-id><label>Figure 2—source data 2.</label><caption><title>Decoding from eye tracker data. </title><p>The file '..eyeAcc.mat' contains the subject x time decoding accuracies from the x and y eye tracker channels within perception and imagery. The file '.. EyeSig.mat' contains the cluster based p values of these accuracies compared to chance level. </p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-33904-fig2-data2-v1.zip"/></supplementary-material></p><p><supplementary-material id="fig2sdata3"><object-id pub-id-type="doi">10.7554/eLife.33904.010</object-id><label>Figure 2—source data 3.</label><caption><title>Vividness median split.</title><p>The file '...ImaVivSplit.mat contains the subjects x time x time decoding accuracy within imagery for the high vividness and low vividness group and the time x time p values of the comparison between the two groups. The file '...PercVivSplit.mat' contains the same for the decoding within perception. </p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-33904-fig2-data3-v1.zip"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-33904-fig2-v1"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.33904.005</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Decoding results throughout the entire imagery period.</title><p>(<bold>A</bold>) Temporal generalization matrix. Training time is shown on the vertical axis and testing time on the horizontal axis. (<bold>B</bold>) Decoding accuracy from a classifier that was trained and tested on the same time points during imagery. (<bold>C</bold>) For each testing point, the proportion of time points that resulted in significantly lower accuracy than the diagonal decoding at that time point, that is the temporal specificity of the representations over time. Source data for the analyses reported here are available in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-33904-fig2-figsupp1-v1"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.33904.006</object-id><label>Figure 2—figure supplement 2.</label><caption><title>Decoding on eye tracker data.</title><p>(<bold>A</bold>) Decoding accuracy over time on eye tracker data during perception. Filled areas and thick lines indicate significant above chance decoding (cluster corrected, p&lt;0.05). The shaded area represents the standard error of the mean. The dotted line indicates chance level. (<bold>B</bold>) Decoding accuracy over time on eye tracker data during imagery. (<bold>C</bold>) Correlation over participants between eye tracker decoding accuracy and brain decoding accuracy, averaged over the period during which eye tracker decoding was significant. Source data for the analyses reported here are available in <xref ref-type="supplementary-material" rid="fig2sdata2">Figure 2—source data 2</xref>.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-33904-fig2-figsupp2-v1"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.33904.007</object-id><label>Figure 2—figure supplement 3.</label><caption><title>Differences in decoding accuracy between high and low vivid participants during perception and imagery.</title><p>None of the differences were significant after correction for multiple comparisons. (<bold>A</bold>) Decoding accuracy from a classifier that was trained and tested on the same time points during perception and (<bold>C</bold>) during imagery. The red line denotes the accuracy for the high vividness group, the blue line the accuracy for the low vividness group. (<bold>B</bold>) Difference between participants with high and low vividness on temporal generalization accuracy during perception (<bold>B</bold>), and (<bold>D</bold>) imagery. Reddish colors indicate higher accuracy for the high vividness group and blueish colors indicate higher accuracy for the low vividness group. Source data for the analyses reported here are available in <xref ref-type="supplementary-material" rid="fig2sdata3">Figure 2—source data 3</xref>.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-33904-fig2-figsupp3-v1"/></fig></fig-group><p>To reveal the generalization of representations over time, classifiers were trained on one time point and tested on all other time points (<xref ref-type="bibr" rid="bib33">King and Dehaene, 2014</xref>) (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Furthermore, to investigate the temporal specificity of the representations at each time point, we calculated the proportion of off-diagonal classifiers that had a significantly lower accuracy than the diagonal classifier of that time point (<xref ref-type="bibr" rid="bib3">Astrand et al., 2015</xref>) (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; see Materials and methods).</p><p>During perception, distinct processing stages can be distinguished (<xref ref-type="fig" rid="fig2">Figure 2B–C</xref>, left). During the first stage, between 70 and 120 ms, diagonal decoding was significant and there was very high temporal specificity. This indicates sequential processing with rapidly changing representations (<xref ref-type="bibr" rid="bib32">King and Dehaene, 2014</xref>). During this time period, the classifier mostly relied on activity in posterior visual areas (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, left). Therefore, these results are consistent with initial feedforward stimulus processing. In the second stage, around 160 ms, the classifier generalized to neighboring points as well as testing points after 250 ms. The associated sources are spread out over the ventral visual stream (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, left), indicating that high-level representations are activated at this time. In the third stage, around 210 ms, we again observed high temporal specificity (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, left) and a gap in generalization to 160 ms (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, left). This pattern could reflect feedback to low-level visual areas. Finally, from 300 ms onwards there is a broad off-diagonal generalization pattern that also generalizes to time points around 160 ms and an associated drop in temporal specificity (<xref ref-type="fig" rid="fig2">Figure 2B–C</xref>, left). This broad off-diagonal pattern likely reflects stabilization of the visual representation.</p><p>In contrast, during imagery, we did not observe any clear distinct processing stages. Instead, there was a broad off-diagonal generalization throughout the entire imagery period (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, right; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>). Already at the onset of imagery decoding, there was high generalization and low specificity (<xref ref-type="fig" rid="fig2">Figure 2B–C</xref>, right). This indicates that the neural representation during imagery remains highly stable (<xref ref-type="bibr" rid="bib32">King and Dehaene, 2014</xref>). The only change seems to be in decoding strength, which first increases and then decreases over time (Fig. S1B), indicating that either representations at those times are weaker or that they are more variable over trials. The sources that contributed to classification were mostly located in the ventral visual stream and there was also some evidence for frontal and parietal contributions (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, right).</p><p>Even though we attempted to remove eye movements from our data as well as possible (see Materials and methods), it is theoretically possible that eye movements which systematically differed between the conditions caused part of the neural signal that was picked up by the decoding analyses (<xref ref-type="bibr" rid="bib45">Mostert et al., 2017</xref>). In order to investigate this possibility, we tried to decode the stimulus category from the X and Y position of the eyes as measured with an eye tracker. The results for this analysis are shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>. During imagery, eye tracker decoding was at chance level for all time points, indicating that there were no condition-specific eye movements during imagery (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>). However, during perception, eye tracker decoding was significant from 316 ms onwards (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>), indicating that differences in eye movements between the conditions may have driven (part of) the brain decoding. If this were the case, there would be a high, positive correlation between eye tracker decoding and brain decoding. <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C</xref> however shows that there was no such correlation, suggesting that our perception decoding results for that time window were not driven by eye movements.</p></sec><sec id="s1-3"><title>Temporal overlap between perception and imagery</title><p>To investigate when perceptual processing generalizes to imagery, we trained a classifier on one data segment and tested it on the other segment. We first trained a classifier during perception and then used this classifier to decode the neural signal during imagery (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>). Already around 350 ms after imagery cue onset, classifiers trained on perception data from 160, 700 and 960 ms after stimulus onset could significantly decode the imagined stimulus category (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). This is earlier than classification within imagery, which started at 540 ms after cue onset (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right). Considering the increased decoding accuracy during perception compared to imagery (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, left versus right), this difference might reflect an increase in signal-to-noise ratio (SNR) by training on perception compared to imagery.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.33904.011</object-id><label>Figure 3.</label><caption><title>Generalization between perception and imagery over time.</title><p>(<bold>A</bold>) Decoding accuracy from classifiers trained on perception and tested during imagery. The training time during perception is shown on the vertical axis and the testing time during imagery is shown on the horizontal axis. (<bold>B</bold>) Decoding accuracies for classifiers trained on the four stages during perception. (<bold>C</bold>) Decoding accuracy from classifiers trained on imagery and tested during perception. The training time during imagery is shown on the vertical axis and the testing time during perception is shown on the horizontal axis. (<bold>D</bold>) Decoding accuracies for different training times during imagery. Source data for the analyses reported here are available in <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>.</p><p><supplementary-material id="fig3sdata1"><object-id pub-id-type="doi">10.7554/eLife.33904.013</object-id><label>Figure 3—source data 1.</label><caption/><media mime-subtype="zip" mimetype="application" xlink:href="elife-33904-fig3-data1-v1.zip"/></supplementary-material></p><p><supplementary-material id="fig3sdata2"><object-id pub-id-type="doi">10.7554/eLife.33904.014</object-id><label>Figure 3—source data 2.</label><caption/><media mime-subtype="zip" mimetype="application" xlink:href="elife-33904-fig3-data2-v1.zip"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-33904-fig3-v1"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.33904.012</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Differences in cross-decoding accuracy between high and low vivid participants.</title><p>None of the differences were significant after correction for multiple comparisons. The difference between the two vividness groups on cross-decoding accuracy for (<bold>A</bold>) a classifier trained during perception and tested during imagery and (<bold>B</bold>) a classifier trained during imagery and tested during perception. Source data for the analyses reported here can be available in <xref ref-type="supplementary-material" rid="fig3sdata2">Figure 3—source data 2</xref>.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-33904-fig3-figsupp1-v1"/></fig></fig-group><p>Furthermore, the distinct processing stages found during perception (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, left) were also reflected in the generalization to imagery (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>). Perceptual processes around 160 ms and after 300 ms significantly overlapped with imagery (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, right plots). In contrast, processing at 90 ms did not generalize to any time point during imagery (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, top left). Perceptual processing at 210 ms showed intermittent generalization to imagery, with generalization at some time points and no generalization at other times (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, bottom left). Significant generalization at this time could also reflect the effects of smoothing over neighboring time points which are significant (see Materials and methods). This would mean that there is no real overlap at 210 ms but that this overlap is caused by overlap from earlier or later time points.</p><p>To further pinpoint when perception started to overlap with imagery, we performed an additional analysis in which we reversed the generalization: we trained classifiers on different time points during imagery and used these to classify perception data. This analysis revealed a similar pattern of high overlap with perception around 160 and after 300 ms and low overlap before 100 ms and around 210 ms (<xref ref-type="fig" rid="fig3">Figure 3C–D</xref>). Note that this profile is stable throughout imagery and is already present at the start of imagery, albeit with lower accuracies (<xref ref-type="fig" rid="fig3">Figure 3</xref>-D, bottom panel). Furthermore, the onset of perceptual overlap is highly consistent over the course of imagery: overlap starts around 130 ms, with the first peak at approximately 160 ms (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). In general, cross-classification accuracy was higher when training on imagery than when training on perception (<xref ref-type="fig" rid="fig3">Figure 3C</xref> vs. <xref ref-type="fig" rid="fig3">Figure 3A</xref>). This is surprising, because training on high SNR data (in our case, perception) is reported to lead to higher classification accuracy than training on low SNR data (<xref ref-type="bibr" rid="bib32">King and Dehaene, 2014</xref>) (imagery). This difference may reflect the fact that the perceptual representation contained more unique features than the imagery representation, leading to a lower generalization performance when training on perception.</p><p>We also investigated whether the temporal dynamics were influenced by imagery vividness by investigating whether the results of previous analyses were different for participants with high or low vividness (see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> for within perception and imagery decoding and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for cross-decoding). Decoding accuracy seemed to be higher in the high vividness group, however, none of the differences were significant after correction for multiple comparisons.</p></sec></sec><sec id="s2" sec-type="discussion"><title>Discussion</title><p>We investigated the temporal dynamics of category representations during perception and imagery, as well as the overlap between the two. We first showed that, compared to perception, imagery decoding became significant later, indicating that it takes longer to generate a visual representation based on purely top-down processes. Furthermore, whereas perception was characterized by high temporal specificity and distinct processing stages, imagery showed wide generalization and low temporal specificity from the onset. Finally, cross-decoding between perception and imagery revealed a very clear temporal overlap profile which was consistent throughout the imagery period. We observed overlap between imagery and perceptual processing starting around 130 ms, decreasing around 210 ms and increasing again from 300 ms after stimulus onset. This pattern was already present at the onset of imagery.</p><p>These findings cannot be explained by a clear cascading of activity up or down the visual hierarchy during imagery. If there was a clear order in activation of different areas, we would not have observed such wide temporal generalization at the start of imagery but instead a more diagonal pattern, as during the start of perception (<xref ref-type="bibr" rid="bib32">King and Dehaene, 2014</xref>). Furthermore, we found that the complete overlap with perception was already present at the onset of imagery.</p><p>One interpretation of our results is that during imagery the complete stimulus representation, including different levels of the hierarchy, is activated simultaneously. However, there was no overlap between imagery and perceptual processing until 130 ms after stimulus onset, when the feedforward sweep is presumably completed and high-level categorical information is activated for the first time (<xref ref-type="bibr" rid="bib29">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib10">Carlson et al., 2011</xref>; <xref ref-type="bibr" rid="bib53">Thorpe et al., 1996</xref>). Overlap between perception and imagery in low-level visual cortex depends on the imagery task and experienced vividness (<xref ref-type="bibr" rid="bib38">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib2">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">Kosslyn and Thompson, 2003</xref>). However, we did not observe a relationship between overlap at this time point and imagery vividness (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). This absence of early overlap seems to imply that, even though early visual cortex has been implicated in visual imagery, there is no consistent overlap between imagery and early perceptual processing. One explanation for this discrepancy is that representations in low-level visual areas first have to be sharpened by feedback connections (<xref ref-type="bibr" rid="bib35">Kok et al., 2012</xref>) before they have a format that is accessible by top-down imagery. Alternatively, early perceptual activity during imagery may be more brief and variable over time than high-level activation, leading to a cancelling out when averaging over trials.</p><p>The large temporal generalization at the onset of imagery might have been partly due to the nature of the task we used. Here, the start of imagery was based on the association between the cue and the items held in working memory, which could have led to an instantaneous initiation of the visual representation after the cue. It could be the case that visual representations are activated differently if imagery is operationalized as a more constructive process. A previous study has already showed that cueing imagery from long-term memory leads to less neural overlap with perception compared to cueing imagery from working memory (<xref ref-type="bibr" rid="bib27">Ishai et al., 2002</xref>). Perhaps the temporal dynamics of imagery from long-term memory are also different. It could be that if perceptual details have to be filled in from long-term memory, low-level areas are activated later, resulting in a more diagonal generalization pattern. Future studies comparing temporal generalization during imagery from short- and long-term memory are necessary to investigate this further.</p><p>An alternative explanation for the broad temporal generalization during imagery is that, compared to perception, imagery is less time-locked. If the process during imagery is shifted in time between trials, averaging over trials per time point would obscure the underlying temporal dynamics. This temporal uncertainty will have a different effect on different underlying processes. For example, if the underlying process would be entirely sequential, meaning that independent representations are activated after each other (like at the start of perception), temporal shifting between trials would smear out this sequence over time. This would result in a broader diagonal pattern, where the width of the diagonal is proportional to the temporal shift between trials. This means that the broad temporal generalization that we observed during imagery could represent a sequential process if there were temporal shifts of more than a second on average between trials. Alternatively, the underlying process could be only sequential in the onset, such that different areas become active after each other, but remain active throughout time (or get reactivated later in time). In this case, temporal shifts between trials that are proportional to the difference in onset between the two processes would entirely obscure this dynamic. Note that this would mean that the start of significant imagery classification did not reflect the actual imagery onset, but the first point in time that the representations were consistent enough over trials to lead to above chance classification. Stimulus representations could actually be initiated before 350 ms after cue onset, but we would be unable to classify them at these early time points due to jitter in the onset. We cannot confidently rule out temporal uncertainty as an explanation for the broad temporal generalization at the onset of imagery. To fully resolve this issue, future research should systematically explore the effect of temporal uncertainty on different underlying processes and analysis tools need to be developed that can account for variation in temporal dynamics between trials.</p><p>We observed clear overlap between imagery and perceptual processing around 160 ms after stimulus onset. The perceptual representation at this time likely reflects the face-specific N170. This component has been shown to be involved in face processing and appears between 130 to 170 ms after stimulus onset (<xref ref-type="bibr" rid="bib7">Bentin et al., 1996</xref>; <xref ref-type="bibr" rid="bib20">Halgren et al., 2000</xref>), which corresponds well with the timing of overlap with imagery reported here. The sources of the N170 are thought to be face selective areas in the ventral stream (<xref ref-type="bibr" rid="bib13">Deffke et al., 2007</xref>; <xref ref-type="bibr" rid="bib23">Henson et al., 2009</xref>), which also corresponds to the location of our source reconstruction at this time point. A previous study showed an increase of the N170 after imagery of a face, indicating that imagery activates similar representations as the N170 (<xref ref-type="bibr" rid="bib18">Ganis and Schendan, 2008</xref>). Here we confirm that idea and show that N170 representations are active during imagery throughout time. Furthermore, this time also showed long temporal generalization within perception, indicating that the N170 representations also remain active over time during perception.</p><p>The lack of generalization between imagery and perceptual processing around 210 ms after stimulus onset was unexpected. This time window also showed an increase in temporal specificity during perception, indicating rapidly changing representations. One possible interpretation is that around this time feedback from higher areas arrives in low-level visual cortex (<xref ref-type="bibr" rid="bib34">Koivisto et al., 2011</xref>; <xref ref-type="bibr" rid="bib49">Roelfsema et al., 1998</xref>). If low-level representations are indeed more transient, this would explain the decrease in consistent generalization. Another possibility is that processing at this time reflects an unstable combination of feedback and feedforward processes, which is resolved around 300 ms when representations become more generalized and again start to generalize to imagery. In line with this idea, processing from 300 ms after stimulus onset has been associated with percept stabilization (<xref ref-type="bibr" rid="bib4">Bachmann, 2006</xref>; <xref ref-type="bibr" rid="bib9">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib30">Kaiser et al., 2016</xref>). Future studies looking at changes in effective connectivity over time are needed to dissociate these interpretations.</p><p>Surprisingly, we did not observe any influences of experienced imagery vividness on the overlap between perception and imagery over time (Fig. S2). One explanation for this is that we used whole-brain signals for decoding whereas the relationship between overlap and vividness has only been found for a specific set of brain regions (<xref ref-type="bibr" rid="bib38">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib15">Dijkstra et al., 2017a</xref>). Furthermore, if there is indeed strong temporal variability during imagery this would make it difficult to find any effect of vividness on specific time points. More studies on imagery vividness using MEG are necessary to explore this matter further.</p><p>In conclusion, our findings suggest that, in contrast to perception, at the onset of imagery the entire visual representation is activated at once. This might partly be caused by the nature of our task, since visual representations were already present in working memory at the onset of imagery. However, more research is needed to fully explore the contribution of temporal uncertainty between trials to this broad temporal generalization. Furthermore, imagery consistently overlapped with perceptual processing around 160 ms and from 300 ms onwards. This reveals the temporal counterpart of the neural overlap between imagery and perception. The overlap around 160 ms points towards a re-activation of the N170 during imagery, whereas the lack of overlap with perceptual processes before 130 ms indicates that either imagery does not rely on early perceptual representations, or that these representations are more transient and variable over time. Together, these findings reveal important new insights into the neural mechanisms of visual imagery and its relation to perception.</p></sec><sec id="s3" sec-type="materials|methods"><title>Materials and methods</title><sec id="s3-1"><title>Participants</title><p>We assumed a medium effect size (d = 0.6) which, to reach a power of 0.8, required twenty four participants. To take into account drop-out, thirty human volunteers with normal or corrected-to-normal vision gave written informed consent and participated in the study. Five participants were excluded: two because of movement in the scanner (movement exceeded 15 mm), two due to incorrect execution of the task (less than 50% correct on the catch trials, as described below) and one due to technical problems. 25 participants (mean age 28.6, SD = 7.62) remained for the final analysis. The study was approved by the local ethics committee and conducted according to the corresponding ethical guidelines (CMO Arnhem-Nijmegen).</p></sec><sec id="s3-2"><title>Procedure and experimental design</title><p>Prior to scanning, participants were asked to fill in the Vividness of Visual Imagery Questionnaire (VVIQ): a 16-item questionnaire in which participants indicate their imagery vividness for a number of scenarios on a 5-point scale (<xref ref-type="bibr" rid="bib42">Marks, 1973</xref>). The VVIQ has been used in many imagery studies and is a well-validated measure of general imagery ability (<xref ref-type="bibr" rid="bib38">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib2">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Dijkstra et al., 2017a</xref>; <xref ref-type="bibr" rid="bib12">Cui et al., 2007</xref>). The score was summarized in a total between 16 and 80 (low score indicates high vividness). Subsequently, the participants practiced the experimental task for ten trials outside the scanner, after which they were given the opportunity to ask clarification questions about the task paradigm. If they had difficulty with the task, they could practice a second time with ten different trials.</p><p>The experimental task is depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref>. We adapted a retro-cue paradigm in which the cue was orthogonalized with respect to the stimulus identity (<xref ref-type="bibr" rid="bib21">Harrison and Tong, 2009</xref>). Participants were shown two images after each other, a face and a house, or a house and a face, followed by a retro-cue indicating which of the images had to be imagined. After the cue, a frame was shown in which the participants had to imagine the cued stimulus as vividly as possible. After this, they had to indicate their experienced imagery vividness by moving a bar on a continuous scale. The size of the scale together with the screen resolution led to discretized vividness values between −150 and +150. To prevent preparation of a motor response during imagery, which side (left or right) indicated high vividness, was pseudo-randomized over trials.</p><p>The face stimuli were adapted from the multiracial face database (courtesy of Michael J Tarr, Center for the Neural Basis of Cognition and Department of Psychology, Carnegie Mellon University (Pittsburgh, Pennsylvania), <ext-link ext-link-type="uri" xlink:href="http://www.tarrlab.org.">http://www.tarrlab.org.</ext-link> Funding provided by NSF award 0339122). The house stimuli were adapted from the Pasedena houses database collected by Helle and Perona (California Institute of Technology, Pasadena, California). We chose faces and houses because these two categories elicit very different neural responses throughout the visual hierarchy, during both perception and imagery (<xref ref-type="bibr" rid="bib28">Ishai et al., 2000</xref>; <xref ref-type="bibr" rid="bib17">Epstein et al., 2003</xref>; <xref ref-type="bibr" rid="bib31">Kanwisher et al., 1997</xref>), and are therefore expected to allow for high-fidelity tracking of their corresponding neural representations.</p><p>To ensure that participants were imagining the stimuli with great visual detail, both categories contained eight exemplars, and on 7% of the trials the participants had to indicate which of four exemplars they imagined (<xref ref-type="fig" rid="fig1">Figure 1</xref>, Catch trial). The exemplars were chosen to be highly similar in terms of low-level features to minimize within-class variability and increase between-class classification performance. We instructed participants to focus on vividness and not on correctness of the stimulus, to motivate them to generate a mental image including all visual features of the stimulus. The stimuli encompassed 2.7 × 2.7 visual degrees. A fixation bull’s-eye with a diameter of 0.1 visual degree was on screen throughout the trial, except during the vividness rating. In total, there were 240 trials, 120 per category, divided in ten blocks of 24 trials, lasting about 5 min each. After every block, the participant had the possibility to take a break.</p></sec><sec id="s3-3"><title>MEG recording and preprocessing</title><p>Data were recorded at 1200 Hz using a 275-channel MEG system with axial gradiometers (VSM/CTF Systems, Coquitlam, BC, Canada). For technical reasons, data from five sensors (MRF66, MLC11, MLC32, MLF62, MLO33) were not recorded. Subjects were seated upright in a magnetically shielded room. Head position was measured using three coils: one in each ear and one on the nasion. Throughout the experiment head motion was monitored using a real-time head localizer (<xref ref-type="bibr" rid="bib52">Stolk et al., 2013</xref>). If necessary, the experimenter instructed the participant back to the initial head position during the breaks. This way, head movement was kept below 8 mm in most participants. Furthermore, both horizontal and vertical electro-oculograms (EOGs), as well as an electrocardiogram (ECG) were recorded for subsequent offline removal of eye- and heart-related artifacts. Eye position and pupil size were also measured for control analyses using an Eye Link 1000 Eye tracker (SR Research).</p><p>Data were analyzed with MATLAB version R2017a and FieldTrip (<xref ref-type="bibr" rid="bib47">Oostenveld et al., 2011</xref>) (RRID: <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_004849">SCR_004849</ext-link>). Per trial, three events were defined. The first event was defined as 200 ms prior to onset of the first image until 200 ms after the offset of the first image. The second event was defined similarly for the second image. Further analyses focused only on the first event, because the neural response to the second image is contaminated by the neural response to the first image. Finally, the third event was defined as 200 ms prior to the onset of the retro-cue until 500 ms after the offset of the imagery frame. As a baseline correction, for each event, the activity during 300 ms from the onset of the initial fixation of that trial was averaged per channel and subtracted from the corresponding signals.</p><p>The data were down-sampled to 300 Hz to reduce memory and CPU load. Line noise at 50 Hz was removed from the data using a DFT notch filter. To identify artifacts, the variance of each trial was calculated. Trials with high variance were visually inspected and removed if they contained excessive artifacts. After artifact rejection, on average 108 perception face trials (±11), 107 perception house trials (±12) and 105 imagery face trials (±16) and 106 imagery house trials (±13) remained for analysis. To remove eye movement and heart rate artifacts, independent components of the MEG data were calculated and correlated with the EOG and ECG signals. Components with high correlations were manually inspected before removal. The eye tracker data was cleaned separately by inspecting trials with high variance and removing them if they contained blinks or other excessive artifacts.</p></sec><sec id="s3-4"><title>Decoding analyses</title><p>To track the neural representations within perception and imagery, we decoded the stimulus category from the preprocessed MEG signals during the first stimulus and after the retro-cue for every time point. To improve the signal-to-noise ratio, prior to classification, the data were averaged over a window of 30 ms centered on the time point of interest. We used a linear discriminant analysis (LDA) classifier with the activity from the 270 MEG sensors as features (see <xref ref-type="bibr" rid="bib46">Mostert et al., 2015</xref> for more details). A 5-fold cross-validation procedure was implemented where for each fold the classifier was trained on 80% of the trials and tested on the other 20%. To prevent a potential bias in the classifier, the number of trials per class was balanced per fold by randomly removing trials from the class with the most trials until the trial numbers were equal between the classes.</p></sec><sec id="s3-5"><title>Generalization across time and conditions</title><p>By training a classifier on one time point and then testing it on other time points, we were able to investigate the stability of neural representations over time. The resulting temporal generalization pattern gives information about the underlying processing dynamics. For instance, a mostly diagonal pattern reflects sequential processing of specific representations, whereas generalization from one time point towards another reflects recurrent or sustained activity of a particular process (<xref ref-type="bibr" rid="bib32">King and Dehaene, 2014</xref>). Here, we performed temporal generalization analyses during perception and during imagery to investigate the dynamics of the neural representations. Furthermore, to quantify the extent to which the representation at a given time point <italic>t</italic> was specific to that time point, we tested whether a classifier trained at time <italic>t</italic> and tested at time <italic>t</italic> (i.e. diagonal decoding) had a higher accuracy than a classifier trained at time <italic>t’</italic> and tested at time <italic>t</italic> (i.e. generalization). This shows whether there is more information at time <italic>t</italic> than can be extracted by the decoder <italic>t’</italic> (<xref ref-type="bibr" rid="bib3">Astrand et al., 2015</xref>; <xref ref-type="bibr" rid="bib33">King et al., 2014</xref>). We subsequently calculated, for each training time point, the proportion of testing time points that were significantly lower than the diagonal decoding, giving a measure of specificity for each time point. To avoid overestimating the specificity, we only considered the time window during which the diagonal classifiers were significantly above chance.</p><p>To investigate the overlap in neural representations between perception and imagery, a similar approach can be used. Here, we trained a classifier on different time points during perception and tested it on different time points during imagery and vice versa. This analysis shows when neural activity during perception contains information that can be used to dissociate mental representations during imagery and vice versa - that is which time points show representational overlap. For both the temporal generalization as well as the across condition generalization analysis, we also applied cross-validation to avoid overestimating generalization due to autocorrelation in the signals.</p><p>It has been shown that representational overlap between imagery and perception, as measured by fMRI, is related to experienced imagery vividness (<xref ref-type="bibr" rid="bib38">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib2">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Dijkstra et al., 2017a</xref>). To investigate this in the current study, we performed a median split on the averaged vividness across trials on the group level, which yielded a high vividness (<italic>N</italic> = 12, vividness: 71.64 ± 12.44) and a low vividness (<italic>N</italic> = 12, vividness: 27.25 ± 17.69) group. We produced the accuracy maps for all previous analyses separately for the two groups and compared the decoding accuracies of the two groups using cluster based permutation testing (see Statistical testing).</p></sec><sec id="s3-6"><title>Statistical testing</title><p>Decoding accuracy was tested against chance using two-tailed cluster-based permutation testing with 1000 permutations (<xref ref-type="bibr" rid="bib41">Maris and Oostenveld, 2007</xref>). In the first step of each permutation, clusters were defined by adjacent points that crossed a threshold of p&lt;0.05. The t-values were summed within each cluster, but separately for positive and negative clusters, and the largest of these were included in the permutation distributions. A cluster in the true data was considered significant if its p-value was less than 0.05 based on the permutations. Correlations with vividness were tested against zero on the group level using the same procedure.</p></sec><sec id="s3-7"><title>Source localization</title><p>In order to identify the brain areas that were involved in making the dissociation between faces and houses during perception and imagery, we performed source reconstruction. In the case of LDA classifiers, the spatial pattern that underlies the classification reduces to the difference in magnetic fields between the two conditions (see <xref ref-type="bibr" rid="bib22">Haufe et al., 2014</xref>). Therefore, to infer the contributing brain areas, we performed source analysis on the difference ERF between the two conditions.</p><p>For this purpose, T1-weighted structural MRI images were acquired using a Siemens 3T whole body scanner. Vitamin E markers in both ears indicated the locations of the head coils during the MEG measurements. The location of the fiducial at the nasion was estimated based on the anatomy of the ridge of the nose. The volume conduction model was created based on a single shell model of the inner surface of the skull. The source model was based on a reconstruction of the cortical surface created for each participant using FreeSurfer’s anatomical volumetric processing pipeline (RRID: <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001847">SCR_001847</ext-link>). MNE-suite (Version 2.7.0; RRID: <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_005972">SCR_005972</ext-link>) was subsequently used to infer the subject-specific source locations from the surface reconstruction. The resulting head model and source locations were co-registered to the MEG sensors.</p><p>The lead fields were rank reduced for each grid point by removing the sensitivity to the direction perpendicular to the surface of the volume conduction model. Source activity was obtained by estimating linearly constrained minimum variance (LCMV) spatial filters (<xref ref-type="bibr" rid="bib56">Van Veen et al., 1997</xref>). The data covariance was calculated over the interval of 50 ms to 1 s after stimulus onset for perception and over the entire segment for imagery. The data covariance was subsequently regularized using shrinkage with a regularization parameter of 0.01 (as described in <xref ref-type="bibr" rid="bib40">Manahova et al., 2017</xref>). These filters were then applied to the axial gradiometer data, resulting in an estimated two-dimensional dipole moment for each grid point over time. For imagery, the data were low-pass filtered at 30 Hz prior to source analysis to increase signal to noise ratio.</p><p>To facilitate interpretation and visualization, we reduced the two-dimensional dipole moments to a scalar value by taking the norm of the vector. This value reflects the degree to which a given source location contributes to activity measured at the sensor level. However, the norm is always a positive value and will therefore, due to noise, suffer from a positivity bias. To counter this bias, we employed a permutation procedure in order to estimate this bias. Specifically, in each permutation, the sign of half of the trials were flipped before averaging and projecting to source space. This way, we cancelled out the systematic stimulus-related part of the signal, leaving only the noise. Reducing this value by taking the norm thus provides an estimate of the positivity bias. This procedure was repeated 1000 times, resulting in a distribution of the noise. We took the mean of this distribution as providing the most likely estimate of the noise and subtracted this from the true, squared source signal. Furthermore, this estimate provides a direct estimate of the artificial amplification factor due to the depth bias. Hence, we also divided the data by the noise estimate to obtain a quantity that allowed visualization across cortical topography. For full details, see <xref ref-type="bibr" rid="bib40">Manahova et al. (2017)</xref>.</p><p>For each subject, the surface-based source points were divided into 74 atlas regions as extracted by FreeSurfer on the basis of the subject-specific anatomy (<xref ref-type="bibr" rid="bib14">Destrieux et al., 2010</xref>). To enable group-level estimates, the activation per atlas region was averaged over grid points for each participant. Group-level activations were then calculated by averaging the activity over participants per atlas region (<xref ref-type="bibr" rid="bib54">van de Nieuwenhuijzen et al., 2016</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors would like to thank Marius Peelen for his comments on an earlier version of the manuscript and Jean-Rémi King for his suggestions for analyses.</p></ack><sec id="s4" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing Editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Investigation, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Data curation, Supervision, Investigation, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Methodology, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Informed consent and consent to publish was obtained from all participants and the experiment was conducted according to the ethical guidelines provided by the CMO Arnhem-Nijmegen.</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.33904.015</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-33904-transrepform-v1.pdf"/></supplementary-material><sec id="s6" sec-type="data-availability"><title>Data availability</title><p>The data discussed in the manuscript are available at the Donders Repository (<ext-link ext-link-type="uri" xlink:href="https://data.donders.ru.nl/collections/shared/di.dcc.DSC_2017.00072_245?2">https://data.donders.ru.nl/collections/shared/di.dcc.DSC_2017.00072_245?2</ext-link>). To download the data, the user has to agree with the accompanying Data Use Agreement (DUA), which states, a.o. that the user will not attempt to identify participants from the data and will acknowledge the origin of the data in any resulting publication. The DUA can be signed and the data can be downloaded by clicking on 'Request access' and logging in using either an institutional or a social account (e.g. Google+, Twitter, LinkedIn).</p><p>The following dataset was generated:</p><p><related-object content-type="generated-dataset" id="dataset1" source-id="https://data.donders.ru.nl/collections/shared/di.dcc.DSC_2017.00072_245?2" source-id-type="uri"><collab collab-type="author">Nadine Dijkstra</collab><collab collab-type="author">Pim Mostert</collab><collab collab-type="author">Sander Bosch</collab><collab collab-type="author">Floris P de Lange</collab><collab collab-type="author">Marcel AJ van Gerven</collab><year>2018</year><source>Temporal dynamics of visual imagery and perception</source><ext-link ext-link-type="uri" xlink:href="https://data.donders.ru.nl/collections/shared/di.dcc.DSC_2017.00072_245?2">https://data.donders.ru.nl/collections/shared/di.dcc.DSC_2017.00072_245?2</ext-link><comment>Publicly available at Donders Repository</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahissar</surname> <given-names>M</given-names></name><name><surname>Hochstein</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The reverse hierarchy theory of visual perceptual learning</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>457</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.08.011</pub-id><pub-id pub-id-type="pmid">15450510</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albers</surname> <given-names>AM</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Toni</surname> <given-names>I</given-names></name><name><surname>Dijkerman</surname> <given-names>HC</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Shared representations for working memory and mental imagery in early visual cortex</article-title><source>Current Biology</source><volume>23</volume><fpage>1427</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id><pub-id pub-id-type="pmid">23871239</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astrand</surname> <given-names>E</given-names></name><name><surname>Ibos</surname> <given-names>G</given-names></name><name><surname>Duhamel</surname> <given-names>JR</given-names></name><name><surname>Ben Hamed</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Differential dynamics of spatial attention, position, and color coding within the parietofrontal network</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>3174</fpage><lpage>3189</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2370-14.2015</pub-id><pub-id pub-id-type="pmid">25698752</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bachmann</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><chapter-title>Microgenesis of perception: Conceptual, psychophysical, and neurobiological aspects</chapter-title><person-group person-group-type="editor"><name><surname>Ögmen</surname> <given-names>H</given-names></name><name><surname>Breitmeyer</surname> <given-names>B. G</given-names></name></person-group><source>The First Half Second: The Microgenesis and Temporal Dynamics of Unconscious and Conscious Visual Processes</source><publisher-name>MIT Press</publisher-name><fpage>11</fpage><lpage>33</lpage><pub-id pub-id-type="isbn">9780262051149</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname> <given-names>AM</given-names></name><name><surname>Usrey</surname> <given-names>WM</given-names></name><name><surname>Adams</surname> <given-names>RA</given-names></name><name><surname>Mangun</surname> <given-names>GR</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Canonical microcircuits for predictive coding</article-title><source>Neuron</source><volume>76</volume><fpage>695</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id><pub-id pub-id-type="pmid">23177956</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname> <given-names>AM</given-names></name><name><surname>Vezoli</surname> <given-names>J</given-names></name><name><surname>Bosman</surname> <given-names>CA</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Dowdall</surname> <given-names>JR</given-names></name><name><surname>De Weerd</surname> <given-names>P</given-names></name><name><surname>Kennedy</surname> <given-names>H</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual areas exert feedforward and feedback influences through distinct frequency channels</article-title><source>Neuron</source><volume>85</volume><fpage>390</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.018</pub-id><pub-id pub-id-type="pmid">25556836</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bentin</surname> <given-names>S</given-names></name><name><surname>Allison</surname> <given-names>T</given-names></name><name><surname>Puce</surname> <given-names>A</given-names></name><name><surname>Perez</surname> <given-names>E</given-names></name><name><surname>McCarthy</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Electrophysiological studies of face perception in humans</article-title><source>Journal of Cognitive Neuroscience</source><volume>8</volume><fpage>551</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.1162/jocn.1996.8.6.551</pub-id><pub-id pub-id-type="pmid">20740065</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergmann</surname> <given-names>J</given-names></name><name><surname>Genç</surname> <given-names>E</given-names></name><name><surname>Kohler</surname> <given-names>A</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name><name><surname>Pearson</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Smaller primary visual cortex is associated with stronger, but less precise mental imagery</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3838</fpage><lpage>3850</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv186</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname> <given-names>T</given-names></name><name><surname>Tovar</surname> <given-names>DA</given-names></name><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational dynamics of object vision: the first 1000 ms</article-title><source>Journal of Vision</source><volume>13</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1167/13.10.1</pub-id><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname> <given-names>TA</given-names></name><name><surname>Hogendoorn</surname> <given-names>H</given-names></name><name><surname>Kanai</surname> <given-names>R</given-names></name><name><surname>Mesik</surname> <given-names>J</given-names></name><name><surname>Turret</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>High temporal resolution decoding of object position and category</article-title><source>Journal of Vision</source><volume>11</volume><fpage>9</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1167/11.10.9</pub-id><pub-id pub-id-type="pmid">21920851</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cauchoix</surname> <given-names>M</given-names></name><name><surname>Barragan-Jason</surname> <given-names>G</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Barbeau</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neural dynamics of face detection in the wild revealed by MVPA</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>846</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3030-13.2014</pub-id><pub-id pub-id-type="pmid">24431443</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cui</surname> <given-names>X</given-names></name><name><surname>Jeter</surname> <given-names>CB</given-names></name><name><surname>Yang</surname> <given-names>D</given-names></name><name><surname>Montague</surname> <given-names>PR</given-names></name><name><surname>Eagleman</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Vividness of mental imagery: individual variability can be measured objectively</article-title><source>Vision Research</source><volume>47</volume><fpage>474</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.11.013</pub-id><pub-id pub-id-type="pmid">17239915</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deffke</surname> <given-names>I</given-names></name><name><surname>Sander</surname> <given-names>T</given-names></name><name><surname>Heidenreich</surname> <given-names>J</given-names></name><name><surname>Sommer</surname> <given-names>W</given-names></name><name><surname>Curio</surname> <given-names>G</given-names></name><name><surname>Trahms</surname> <given-names>L</given-names></name><name><surname>Lueschow</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>MEG/EEG sources of the 170-ms response to faces are co-localized in the fusiform gyrus</article-title><source>NeuroImage</source><volume>35</volume><fpage>1495</fpage><lpage>1501</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.01.034</pub-id><pub-id pub-id-type="pmid">17363282</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destrieux</surname> <given-names>C</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Dale</surname> <given-names>A</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</article-title><source>NeuroImage</source><volume>53</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.010</pub-id><pub-id pub-id-type="pmid">20547229</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname> <given-names>N</given-names></name><name><surname>Bosch</surname> <given-names>SE</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Vividness of visual imagery depends on the neural overlap with perception in visual Areas</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>1367</fpage><lpage>1373</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3022-16.2016</pub-id><pub-id pub-id-type="pmid">28073940</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname> <given-names>N</given-names></name><name><surname>Zeidman</surname> <given-names>P</given-names></name><name><surname>Ondobaka</surname> <given-names>S</given-names></name><name><surname>van Gerven</surname> <given-names>MAJ</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>Distinct top-down and bottom-up brain connectivity during visual perception and imagery</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>5677</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-05888-8</pub-id><pub-id pub-id-type="pmid">28720781</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname> <given-names>R</given-names></name><name><surname>Graham</surname> <given-names>KS</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Viewpoint-specific scene representations in human parahippocampal cortex</article-title><source>Neuron</source><volume>37</volume><fpage>865</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00117-X</pub-id><pub-id pub-id-type="pmid">12628176</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganis</surname> <given-names>G</given-names></name><name><surname>Schendan</surname> <given-names>HE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual mental imagery and perception produce opposite adaptation effects on early brain potentials</article-title><source>NeuroImage</source><volume>42</volume><fpage>1714</fpage><lpage>1727</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.07.004</pub-id><pub-id pub-id-type="pmid">18674625</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganis</surname> <given-names>G</given-names></name><name><surname>Thompson</surname> <given-names>WL</given-names></name><name><surname>Kosslyn</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Brain areas underlying visual mental imagery and visual perception: an fMRI study</article-title><source>Cognitive Brain Research</source><volume>20</volume><fpage>226</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2004.02.012</pub-id><pub-id pub-id-type="pmid">15183394</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halgren</surname> <given-names>E</given-names></name><name><surname>Raij</surname> <given-names>T</given-names></name><name><surname>Marinkovic</surname> <given-names>K</given-names></name><name><surname>Jousmäki</surname> <given-names>V</given-names></name><name><surname>Hari</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Cognitive response profile of the human fusiform face area as determined by MEG</article-title><source>Cerebral Cortex</source><volume>10</volume><fpage>69</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.1.69</pub-id><pub-id pub-id-type="pmid">10639397</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname> <given-names>SA</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title><source>Nature</source><volume>458</volume><fpage>632</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nature07832</pub-id><pub-id pub-id-type="pmid">19225460</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haufe</surname> <given-names>S</given-names></name><name><surname>Meinecke</surname> <given-names>F</given-names></name><name><surname>Görgen</surname> <given-names>K</given-names></name><name><surname>Dähne</surname> <given-names>S</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name><name><surname>Blankertz</surname> <given-names>B</given-names></name><name><surname>Bießmann</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title><source>NeuroImage</source><volume>87</volume><fpage>96</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.067</pub-id><pub-id pub-id-type="pmid">24239590</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henson</surname> <given-names>RN</given-names></name><name><surname>Mouchlianitis</surname> <given-names>E</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>MEG and EEG data fusion: simultaneous localisation of face-evoked responses</article-title><source>NeuroImage</source><volume>47</volume><fpage>581</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.04.063</pub-id><pub-id pub-id-type="pmid">19398023</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochstein</surname> <given-names>S</given-names></name><name><surname>Ahissar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>View from the top: hierarchies and reverse hierarchies in the visual system</article-title><source>Neuron</source><volume>36</volume><fpage>791</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01091-7</pub-id><pub-id pub-id-type="pmid">12467584</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horikawa</surname> <given-names>T</given-names></name><name><surname>Kamitani</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Generic decoding of seen and imagined objects using hierarchical visual features</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15037</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15037</pub-id><pub-id pub-id-type="pmid">28530228</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Receptive fields and functional architecture of monkey striate cortex</article-title><source>The Journal of Physiology</source><volume>195</volume><fpage>215</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1968.sp008455</pub-id><pub-id pub-id-type="pmid">4966457</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ishai</surname> <given-names>A</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual imagery of famous faces: effects of memory and attention revealed by fMRI</article-title><source>NeuroImage</source><volume>17</volume><fpage>1729</fpage><lpage>1741</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1330</pub-id><pub-id pub-id-type="pmid">12498747</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ishai</surname> <given-names>A</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Distributed neural systems for the generation of visual images</article-title><source>Neuron</source><volume>28</volume><fpage>979</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)00168-9</pub-id><pub-id pub-id-type="pmid">11163281</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname> <given-names>L</given-names></name><name><surname>Meyers</surname> <given-names>EM</given-names></name><name><surname>Leibo</surname> <given-names>JZ</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The dynamics of invariant object recognition in the human visual system</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1152/jn.00394.2013</pub-id><pub-id pub-id-type="pmid">24089402</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The neural dynamics of attentional selection in natural scenes</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>10522</fpage><lpage>10528</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1385-16.2016</pub-id><pub-id pub-id-type="pmid">27733605</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname> <given-names>N</given-names></name><name><surname>McDermott</surname> <given-names>J</given-names></name><name><surname>Chun</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id><pub-id pub-id-type="pmid">9151747</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id><pub-id pub-id-type="pmid">24593982</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Schurger</surname> <given-names>A</given-names></name><name><surname>Naccache</surname> <given-names>L</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Two distinct dynamic modes subtend the detection of unexpected sounds</article-title><source>PLoS ONE</source><volume>9</volume><elocation-id>e85791</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0085791</pub-id><pub-id pub-id-type="pmid">24475052</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koivisto</surname> <given-names>M</given-names></name><name><surname>Railo</surname> <given-names>H</given-names></name><name><surname>Revonsuo</surname> <given-names>A</given-names></name><name><surname>Vanni</surname> <given-names>S</given-names></name><name><surname>Salminen-Vaparanta</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Recurrent processing in V1/V2 contributes to categorization of natural scenes</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>2488</fpage><lpage>2492</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3074-10.2011</pub-id><pub-id pub-id-type="pmid">21325516</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname> <given-names>SM</given-names></name><name><surname>Ganis</surname> <given-names>G</given-names></name><name><surname>Thompson</surname> <given-names>WL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural foundations of imagery</article-title><source>Nature Reviews Neuroscience</source><volume>2</volume><fpage>635</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1038/35090055</pub-id><pub-id pub-id-type="pmid">11533731</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname> <given-names>SM</given-names></name><name><surname>Thompson</surname> <given-names>WL</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>When is early visual cortex activated during visual mental imagery?</article-title><source>Psychological Bulletin</source><volume>129</volume><fpage>723</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.129.5.723</pub-id><pub-id pub-id-type="pmid">12956541</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>SH</given-names></name><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Disentangling visual imagery and perception of real-world objects</article-title><source>NeuroImage</source><volume>59</volume><fpage>4064</fpage><lpage>4073</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.055</pub-id><pub-id pub-id-type="pmid">22040738</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerner</surname> <given-names>Y</given-names></name><name><surname>Hendler</surname> <given-names>T</given-names></name><name><surname>Ben-Bashat</surname> <given-names>D</given-names></name><name><surname>Harel</surname> <given-names>M</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A hierarchical axis of object processing stages in the human visual cortex</article-title><source>Cerebral Cortex</source><volume>11</volume><fpage>287</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1093/cercor/11.4.287</pub-id><pub-id pub-id-type="pmid">11278192</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Manahova</surname> <given-names>ME</given-names></name><name><surname>Mostert</surname> <given-names>P</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stimulus familiarity and expectation jointly modulate neural activity in the visual ventral stream</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/192518</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marks</surname> <given-names>DF</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Visual imagery differences in the recall of pictures</article-title><source>British Journal of Psychology</source><volume>64</volume><fpage>17</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8295.1973.tb01322.x</pub-id><pub-id pub-id-type="pmid">4742442</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname> <given-names>JH</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Visual processing in monkey extrastriate cortex</article-title><source>Annual Review of Neuroscience</source><volume>10</volume><fpage>363</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.10.030187.002051</pub-id><pub-id pub-id-type="pmid">3105414</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mechelli</surname> <given-names>A</given-names></name><name><surname>Price</surname> <given-names>CJ</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Ishai</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Where bottom-up meets top-down: neuronal interactions during perception and imagery</article-title><source>Cerebral Cortex</source><volume>14</volume><fpage>1256</fpage><lpage>1265</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh087</pub-id><pub-id pub-id-type="pmid">15192010</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mostert</surname> <given-names>P</given-names></name><name><surname>Albers</surname> <given-names>A</given-names></name><name><surname>Brinkman</surname> <given-names>L</given-names></name><name><surname>Todorova</surname> <given-names>L</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>de Lange</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Eye movement-related confounds in neural decoding of visual working memory representations</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/215509</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mostert</surname> <given-names>P</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dissociating sensory from decision processes in human perceptual decision making</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>18253</elocation-id><pub-id pub-id-type="doi">10.1038/srep18253</pub-id><pub-id pub-id-type="pmid">26666393</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname> <given-names>L</given-names></name><name><surname>Tsuchiya</surname> <given-names>N</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Reading the mind's eye: decoding category information during mental imagery</article-title><source>NeuroImage</source><volume>50</volume><fpage>818</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.084</pub-id><pub-id pub-id-type="pmid">20004247</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelfsema</surname> <given-names>PR</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Spekreijse</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Object-based attention in the primary visual cortex of the macaque monkey</article-title><source>Nature</source><volume>395</volume><fpage>376</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1038/26475</pub-id><pub-id pub-id-type="pmid">9759726</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeliger</surname> <given-names>K</given-names></name><name><surname>Fritsche</surname> <given-names>M</given-names></name><name><surname>Güçlü</surname> <given-names>U</given-names></name><name><surname>Schoenmakers</surname> <given-names>S</given-names></name><name><surname>Schoffelen</surname> <given-names>J-M</given-names></name><name><surname>Bosch</surname> <given-names>SE</given-names></name><name><surname>van Gerven</surname> <given-names>MAJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Convolutional neural network-based encoding and decoding of visual object recognition in space and time</article-title><source>NeuroImage</source><volume>17</volume><fpage>30586</fpage><lpage>30584</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.018</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A feedforward architecture accounts for rapid categorization</article-title><source>PNAS</source><volume>104</volume><fpage>6424</fpage><lpage>6429</lpage><pub-id pub-id-type="doi">10.1073/pnas.0700622104</pub-id><pub-id pub-id-type="pmid">17404214</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stolk</surname> <given-names>A</given-names></name><name><surname>Todorovic</surname> <given-names>A</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Online and offline tools for head movement compensation in MEG</article-title><source>NeuroImage</source><volume>68</volume><fpage>39</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.11.047</pub-id><pub-id pub-id-type="pmid">23246857</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname> <given-names>S</given-names></name><name><surname>Fize</surname> <given-names>D</given-names></name><name><surname>Marlot</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><volume>381</volume><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/381520a0</pub-id><pub-id pub-id-type="pmid">8632824</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Nieuwenhuijzen</surname> <given-names>ME</given-names></name><name><surname>van den Borne</surname> <given-names>EW</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Spatiotemporal dynamics of cortical representations during and after stimulus presentation</article-title><source>Frontiers in Systems Neuroscience</source><volume>10</volume><elocation-id>42</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2016.00042</pub-id><pub-id pub-id-type="pmid">27242453</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Anderson</surname> <given-names>CH</given-names></name><name><surname>Felleman</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Information processing in the primate visual system: an integrated systems perspective</article-title><source>Science</source><volume>255</volume><fpage>419</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1126/science.1734518</pub-id><pub-id pub-id-type="pmid">1734518</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname> <given-names>BD</given-names></name><name><surname>van Drongelen</surname> <given-names>W</given-names></name><name><surname>Yuchtman</surname> <given-names>M</given-names></name><name><surname>Suzuki</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>44</volume><fpage>867</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1109/10.623056</pub-id><pub-id pub-id-type="pmid">9282479</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname> <given-names>R</given-names></name><name><surname>Orban</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Coding of stimulus invariances by inferior temporal neurons</article-title><source>Progress in Brain Research</source><volume>112</volume><fpage>195</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(08)63330-0</pub-id><pub-id pub-id-type="pmid">8979830</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.33904.020</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kastner</surname><given-names>Sabine</given-names></name><role>Reviewing Editor</role><aff id="aff3"><institution>Princeton University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Differential temporal dynamics during visual imagery and perception&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Sabine Kastner as the Senior Editor. The following individual involved in review of your submission has agreed to reveal his identity: Nicholas Edward Myers (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>In this paper by Dijkstra et al., entitled &quot;Differential temporal dynamics during visual imagery and perception&quot;, the authors use machine learning methods (decoding) to compare the temporal dynamics of whole brain MEG signals during the perception of faces vs. houses on the one hand and the visual imagery of these same visual categories (faces vs. houses). Previous studies have described how much of the visual perception network is also recruited during visual imagery and how this recruitment depends on the specifics of the task design. The major novelty of this study is to address the temporal aspect of information flow in the relevant cortical networks. The methods are state of the art methods and the reported observations are straightforward.</p><p>The reviewers and editors thought that greater attempts need to be made to make a point with respect to our understanding of imagery. At the moment, this paper is methodologically sound, carefully done, and well written. However, we think it has a very descriptive character, presenting a result that is not entirely surprising (imagery decoding is later, weaker, and more all over the place) and offering two different interpretations. That's a bit unsatisfying. To also make a stronger theoretical contribution, we think the authors should make a greater effort to disentangle these hypotheses (and in addition, a third interpretation that came up during the review process).</p><p>Essential revisions:</p><p>1) How does the cross-temporal decoding of faces vs. houses compare to the cross-temporal decoding of faces (resp. houses) in each task. In other words, for each task, how does the cross-category signal variability compare to the within category signal variability in each condition. The reviewers and editors acknowledge that the authors go through a random permutation procedure to identify data-based chance level. Our question pertains to a better understanding of the neural bases of imagery: could it be that overall lower accuracies during imagery is due to a higher variability in within category information? This is a way to address whether the decay in decoding accuracy during imagery is due to weaker representations or increased representational variability.</p><p>2) When presenting the temporal overlap between perception and imagery analysis, and Figure 3, the authors mention that the reported results describing that a classifier trained on perception can extract information from imagery data much earlier than a classifier trained on imagery data, might be due to the fact that more trials are available to the trained. This is a crucial point. We strongly recommend that the authors down sample their perception data to make this analysis (both that of Figure 3A and 3C) more comparable to the data presented in Figure 2. In this absence of this control, it is difficult to interpret the presented result.</p><p>3) An important additional analysis that needs to be added is the analysis presented in Figure 2C on the high vividness and low vividness trials separately (on matched samples sizes). One expects there to have an overall lower decoding accuracy on low vividness trials. This analysis we expected to directly relate imagery decoding accuracy with the subjective report of imagery vividness. This would a be prior analysis to the one presented in Figure S2, addressing whether more overlap of imagery representation with perceptual representations correlated with higher vividness in imagery subjective experience (further discussed in the next point).</p><p>4) The analysis presented in Figure S2, correlating perception/imagery representational overlap with imagery vividness is a crucial analysis to strengthen the scope of the present paper. It is based onto a LDA, thus assuming a linear relationship between imagery vividness and similarity between perceptual and imagery representations. One can expect other possible relationships between the two: e.g. a one or nothing mapping or a sigmoid type of mapping. We recommend to the authors to explore this differently. One way of doing it would be to perform to separate analysis as in figure 3 for high vividness imagery trials and low vividness imagery trials. One expects a difference to support the subject experience of imagery, though this difference might be localized to some cortical sources (here, the authors will have to make sure to use the same number of trials from training and testing and for the high and low vividness comparison).</p><p>5) The conclusion states that the 'findings show that imagery is characterized by a different temporal profile than perception.' This conclusion doesn't sound entirely surprising, and makes us think that the authors may be selling themselves short. We think it would be important to highlight what the results actually say about the neural basis of imagery.</p><p>6) Furthermore, it might help the Discussion if the authors speculated how specific their results are to the task they chose, rather than imagery per se. In their task, imagery is always preceded by presentation of the exact stimulus to be imagined (along with a not-to-be imagined control stimulus), which means that participants will still have a lot of information about the imagined stimulus in visual working memory. This would not be the case if, for example, participants were suddenly prompted to imagine a novel object. Could this partially account for the good cross-generalization between perception and imagery? What if there had been a control condition requiring memory but no imagery? Without such a control, how much of their findings may be attributed to visual working memory, rather than imagery?</p><p>7) The clear decoding peak around 160 ms seems like it could be related to the N170 or M170 component. Since this component is so well-studied in terms of its role in face processing and its cortical sources in the ventral stream, it seems warranted to discuss this a bit more. Does the fact that the 160 ms peak cross-generalizes well across time and from perception to imagery indicate that face- or house-selective areas in the ventral stream are active at this time and then maintain activation later on, especially during imagery?</p><p>8) The authors raise two different explanations for their results, e.g., in the Abstract: &quot;These results indicate that during imagery either the complete representation is activated at once and does not include low-level visual areas, or the order in which visual features are activated is less fixed and more flexible than during perception.&quot; However, it is possible that there is a probably less exciting, yet more parsimonious explanation for the pattern of results.</p><p>Perception and imagery could just evoke the same processing cascade (which the authors just dismiss us &quot;unlikely&quot;; main text, third paragraph), with two restrictions: (a) Imagery might not &quot;start&quot; as early as perception, thus not involving the earliest stages of perceptual processing. Thus, the onset of imagery decoding is expected to be later than the onset of perception decoding. (b) Visual imagery is initiated in a top-down way; in contrast to the clear onset of a perceptual event, this initiation of imagery may vary substantially between trials (e.g., by an accumulation of temporal noise). Thus, the onset of imagery decoding is expected to have a smoother rise (obscuring the initial dynamics of the signal), and &quot;temporal generalization&quot; would increase a lot (as the evoked responses for single trials are relatively shifted across time).</p><p>Importantly, this explanation would neither suggest that in imagery complete representations are activated at once, nor that the order of processing steps is somehow altered as compared to perception. Note how also the &quot;dip&quot; in cross decoding at 210 ms is explained by this account, without evoking a more complicated explanation: If the neural representation at 210 ms after onset of the event is just very distinct (for whatever reason), the same signal scattered in time from trial to trial would impair cross-decoding specifically for this very time point (where temporal uncertainty hurts a lot). Do the authors think that their data are consistent with this alternative explanation, or are their data points that refute this account?</p><p>9) To dissociate the temporal uncertainty explanation and the explanations given by the authors, I would strongly suggest additional analyses that try to estimate – and potentially correct for – the temporal uncertainty in imagery-related activation. One such analysis could try to align the imagery trials based on the within-imagery decoding. For example, the authors could perform a leave-one-trial-out analysis where they train a classifier on all trials, and test the classifier on the remaining trial, while shifting the left-out trial in time until the maximum decoding accuracy can be reached (this analysis should probably be done by randomly subsampling a smaller amount of trials as the testing set to reach more reliable estimates). Then for each trial's imagery period, this optimal temporal shift can be used to re-epoch the signal. If the processing sequence is similar and just suffers from temporal scattering, this procedure should significantly improve the cross-decoding accuracy while decreasing temporal generalization.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.33904.021</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>[…] The reviewers and editors thought that greater attempts need to be made to make a point with respect to our understanding of imagery. At the moment, this paper is methodologically sound, carefully done, and well written. However, we think it has a very descriptive character, presenting a result that is not entirely surprising (imagery decoding is later, weaker, and more all over the place) and offering two different interpretations. That's a bit unsatisfying. To also make a stronger theoretical contribution, we think the authors should make a greater effort to disentangle these hypotheses (and in addition, a third interpretation that came up during the review process).</p></disp-quote><p>We agree with the reviewers and editors that the paper would benefit from more focus on the neural mechanisms of imagery.</p><p>Specifically, we have:</p><p>- Performed within-category decoding for both perception and imagery (issue 1). We have added an explanation for the difference in decoding accuracy between perception and imagery in the manuscript;</p><p>- Performed additional analyses on vividness (issues 3 and 4) and have added the results to the manuscript;</p><p>- Reworked the conclusion to better highlight the contribution of our findings to the field (issue 5);</p><p>- Added an explanation of the temporal generalization results with respect to task specificity (issue 6);</p><p>- Connected the temporal cross-decoding results to the N170 (issue 7);</p><p>- Performed additional analyses to try to accommodate the temporal uncertainty within imagery (issue 8 and 9) and added more discussion on this issue in the manuscript.</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) How does the cross-temporal decoding of faces vs. houses compare to the cross-temporal decoding of faces (resp. houses) in each task. In other words, for each task, how does the cross-category signal variability compare to the within category signal variability in each condition. The reviewers and editors acknowledge that the authors go through a random permutation procedure to identify data-based chance level. Our question pertains to a better understanding of the neural bases of imagery: could it be that overall lower accuracies during imagery is due to a higher variability in within category information? This is a way to address whether the decay in decoding accuracy during imagery is due to weaker representations or increased representational variability.</p></disp-quote><p>We agree with the reviewers that exploring the relationship between within-category and between-category decoding during both imagery and perception is interesting. However, we optimized the design of the current experiment for between-category decoding. Given that imagery produces a relatively noisy signal, we attempted to maximize our signal-to-noise ratio (SNR).</p><p>Ideally, there would be only one stimulus per category, which is maximally different from the stimulus in the other category, and we would obtain as many repetitions as possible. However, we also needed participants to stay engaged in the task and motivate them to form detailed mental images. Therefore, they performed a more challenging exemplar identification task.</p><p>The stimuli in this task consisted of 8 exemplars per stimulus category that were as similar as possible (faces: all female, dark, long hair; houses: all a slanted roof, and relatively square), but the two stimulus sets (faces versus houses) were very different. We had a maximum of 120 trials per category (before trial rejection), which gave enough power for low but reliable above chance classification within imagery. In contrast, for instance-based decoding we would have 120/8 = a maximum of 15 trials per class.</p><p>To illustrate this issue fully, we have run pairwise classifiers on all combinations of stimuli (16 x 16; 8 per class) for the peak decoding latency for perception (~160 ms) and for imagery (~1 s). As can be seen (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>, bottom plots), the decoding accuracies for between-category classification are considerably lower than before (perception was at 90% and is now at 70%, imagery was at 60% and is now at ~50%), probably due to the decrease in the number of trials for each comparison. Furthermore, within-class classification performance is slightly above chance for perception, but entirely at chance for imagery.</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.33904.017</object-id><label>Author response image 1.</label><caption><title>Within category decoding accuracy for perception (left) and imagery (right).</title><p>At the top the pairwise decoding accuracies for the different stimuli are shown where F are the face stimuli and H are the house stimuli. At the bottom, the averaged accuracies for the different comparisons (within faces, within houses, between categories) are shown as a distribution over participants.</p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-33904-resp-fig1-v1"/></fig><p>Considering the large drop in between-category decoding accuracy with fewer trials, it is impossible to fully investigate this question with our data. However, the fact that within-class decoding was above chance in perception and completely at chance for imagery suggests that there is no increase in within-class variability in imagery compared to perception. We believe that the overall lower accuracies during imagery compared to perception are due to an increase in between-trial variability during imagery due to its cognitive nature: perception is a more automatic process that will proceed in a similar way in every instance, whereas imagery is a demanding process that relies on a combination of different cognitive abilities (working memory retrieval, visualization, percept stabilization). There will likely be larger variation between trials for a cognitively more complex process.</p><p>We agree with the reviewers that more explanation for this difference in decoding accuracy is warranted and have added the following to the text:</p><p>“The generation of a visual representation from a cue thus seems to take longer than the activation via bottom-up sensory input. […] This is also observed in fMRI decoding studies (Reddy et al., 2010; Lee et al., 2012) and is probably due to the fact that imagery is a less automatic process than perception, which leads to higher between trial variation in neural activations.”</p><disp-quote content-type="editor-comment"><p>2) When presenting the temporal overlap between perception and imagery analysis, and Figure 3, the authors mention that the reported results describing that a classifier trained on perception can extract information from imagery data much earlier than a classifier trained on imagery data, might be due to the fact that more trials are available to the trained. This is a crucial point. We strongly recommend that the authors down sample their perception data to make this analysis (both that of Figure 3A and 3C) more comparable to the data presented in Figure 2. In this absence of this control, it is difficult to interpret the presented result.</p></disp-quote><p>We thank the reviewers for pointing out this issue. These results were written based on an earlier version of the analysis in which we did not yet apply cross-validation to the cross-decoding analysis, which meant that there were more data in this analysis compared to the within-condition decoding. However, we realized that this could lead to false positives due to autocorrelations in the signal. Therefore, for the final cross-decoding analyses which are reported here, we did apply cross-validation, leading to the same amount of training data as for the within-imagery (and within-perception) analyses. Therefore, the earlier decoding here cannot be explained by a difference in amount of training data.</p><p>To clarify this point, we have added the following sentence to the Materials and methods section “Generalization across time and conditions”:</p><p>“For both the temporal generalization as well as the across condition generalization analysis, we also applied cross-validation to avoid overestimating generalization due to autocorrelation in the signals.”</p><p>Even though the earlier decoding of imagery based on perception could not be due to a difference in amount of training data, it could still reflect differences in SNR between the two analyses. As can be seen from the higher decoding accuracy during perception (see previous point), perception has a higher SNR. Therefore, training (or testing) on perception will result in increased decoding performance. To clarify this point, we have added the following to the text:</p><p>“This is earlier than classification within imagery, which started at 540 ms after cue onset (Figure 2A, right). Considering the increased decoding accuracy during perception compared to imagery (Figure 2A, left versus right), this difference might reflect an increase in signal-to-noise ratio (SNR) by training on perception compared to imagery.”</p><disp-quote content-type="editor-comment"><p>3) An important additional analysis that needs to be added is the analysis presented in Figure 2C on the high vividness and low vividness trials separately (on matched samples sizes). One expects there to have an overall lower decoding accuracy on low vividness trials. This analysis we expected to directly relate imagery decoding accuracy with the subjective report of imagery vividness. This would a be prior analysis to the one presented in Figure S2, addressing whether more overlap of imagery representation with perceptual representations correlated with higher vividness in imagery subjective experience (further discussed in the next point).</p></disp-quote><p>We agree with the reviewer that it is important to explore the effects of vividness on the imagery decoding. We have explored the effects of vividness on decoding accuracy more thoroughly and reported the results (see our response to issue 4 below).</p><disp-quote content-type="editor-comment"><p>4) The analysis presented in Figure S2, correlating perception/imagery representational overlap with imagery vividness is a crucial analysis to strengthen the scope of the present paper. It is based onto a LDA, thus assuming a linear relationship between imagery vividness and similarity between perceptual and imagery representations. One can expect other possible relationships between the two: e.g. a one or nothing mapping or a sigmoid type of mapping. We recommend to the authors to explore this differently. One way of doing it would be to perform to separate analysis as in figure 3 for high vividness imagery trials and low vividness imagery trials. One expects a difference to support the subject experience of imagery, though this difference might be localized to some cortical sources (here, the authors will have to make sure to use the same number of trials from training and testing and for the high and low vividness comparison).</p></disp-quote><p>We agree with the reviewers, and have looked at decoding accuracy for high and low vividness separately.</p><p>When performing these analyses, we realized that prior to imagery the signal corresponding to the second image will be present more strongly in the data than the signal corresponding to the first image (partly due to signal bleed-in but also due to the well-known recency effect during working memory, see i.e. Morrison, Conway and Chein, 2014). Since we used a two-class design in which the class of the imagined stimulus either corresponds to the second image (in cue 2 trials) or to the other class (in cue 1 trials), this means that this bleed-in/recency <italic>increases</italic> the accuracy for cue 2 trials and <italic>decreases</italic> the accuracy for cue 1 trials, irrespective of any imagery. This effect will likely influence the accuracy throughout the imagery trial. Usually, one averages over all trials and thereby cancels out this effect. However, there are more high vividness cue 2 trials than cue 1 trials (probably due to the recency effect). This means that effects of vividness between trials within participants will likely be influenced by this bleed-in. Therefore, we decided to instead only look at group-level effects of vividness.</p><p>This also means that the analysis that we proposed earlier, in which we correlate the LDA distance to the vividness, might have been influenced by recency and/or bleed-in. We decided to remove that analysis from the paper and replace it by the group-level analysis.</p><p>We performed a median split on the averaged vividness across trials on the group level, which yielded a high vividness (<italic>N</italic> = 12, vividness: 71.64 ± 12.44) and a low vividness (<italic>N</italic> = 12, vividness: 27.25 ± 17.69) group. We produced the accuracy maps for all previous analyses and plotted the difference between high and low vividness in supplementary figures of the main figures (see Figure 2—figure supplement 3 and Figure 3—figure supplement 1).</p><p>There were no significant differences in any of the accuracies after correction for multiple comparisons. However, we do see that the effect generally goes in the expected direction: there is on average a higher decoding accuracy during imagery for the high vividness group compared to the low vividness group (Figure 2C and D). Surprisingly, this also seems to be the case for the accuracy during perception (panels A and B). This might reflect a general attention modulation or a working memory encoding effect. Furthermore, there seems to be an interesting pattern present in the cross-decoding matrix, especially when training on imagery (Figure 3B): overlap around 100 ms and around 210 ms during perception (x-axis) seems to be higher in the high vividness group. We have added these results to the paper. It is unfortunate that we do not find any significant effects of vividness. However, as we already mentioned in the Discussion, influences of vividness have only been found in very specific brain areas. It is quite possible that these may become negligible when looking at a whole brain signal. Furthermore, if the signal suffered from temporal uncertainty (see issues 8 and 9), then it would be even harder to find a consistent effect of vividness at specific time points.</p><disp-quote content-type="editor-comment"><p>5) The conclusion states that the 'findings show that imagery is characterized by a different temporal profile than perception.' This conclusion doesn't sound entirely surprising, and makes us think that the authors may be selling themselves short. We think it would be important to highlight what the results actually say about the neural basis of imagery.</p></disp-quote><p>We agree that we can provide a more explicit interpretation of the neural mechanisms of imagery. Several other reviewer comments relate to this issue and we have tried to accommodate all of them. Furthermore, we extended the emphasis of the paper also to the overlap in time between perception and imagery. We have reworked the Introduction to take this into account as follows:</p><p>“However, the temporal dynamics of visual imagery remain unclear. During imagery, there is no bottom-up sensory input. […] Second, we investigated the temporal overlap by exploring which time points during perception generalized to imagery.”</p><p>Furthermore, we have adapted the conclusion in the Discussion as follows:</p><p>“In conclusion, our findings suggest that, in contrast to perception, at the onset of imagery the entire visual representation is activated at once. […] Together, these findings reveal important new insights into the neural mechanisms of visual imagery and its relation to perception.”</p><disp-quote content-type="editor-comment"><p>6) Furthermore, it might help the Discussion if the authors speculated how specific their results are to the task they chose, rather than imagery per se. In their task, imagery is always preceded by presentation of the exact stimulus to be imagined (along with a not-to-be imagined control stimulus), which means that participants will still have a lot of information about the imagined stimulus in visual working memory. This would not be the case if, for example, participants were suddenly prompted to imagine a novel object. Could this partially account for the good cross-generalization between perception and imagery? What if there had been a control condition requiring memory but no imagery? Without such a control, how much of their findings may be attributed to visual working memory, rather than imagery?</p></disp-quote><p>We agree with the reviewers that it is important to speculate about to what extent our results are specific to the task we used. It has previously been reported that overlap between imagery and perception is indeed lower when imagery is cued from long-term memory (Ishai et al., 2002). Therefore, cueing from long-term memory would have likely also resulted in lower cross-decoding accuracy here. However, this would have been likely been caused by a number of memory-related processes (success of previous encoding, retrieval, etc.), which were not the focus of the current study. In contrast, our aim was to increase our understanding of the neural mechanisms of visual experience in the absence of bottom-up input and to what extent they are similar to when bottom-up input is present. To do this, we wanted to maximize the similarity between perception and imagery while only varying whether or not bottom-up input was present. This is what we attempted here by using a retro-cue paradigm, in which the participant saw the to-be-imagined item only seconds before. Furthermore, other research has shown that imagery and visual working memory rely on highly similar representations in visual cortex (Albers et al., 2013; Tong, 2013; Christophel et al., 2015; Lee and Baker, 2016). Accordingly, we do not aim to dissociate imagery and visual working memory here, but rather compare the temporal dynamics underlying visual experience in the presence and absence of sensory input.</p><p>However, it is possible that the temporal generalization pattern during imagery would be different if we cued from long-term memory, which would point towards different mechanisms for imagery from long-term memory versus imagery from working memory. To further connect our findings to the specifics of the task, we have added the following paragraph to the Discussion:</p><p>“The large temporal generalization at the onset of imagery might have been partly due to the nature of the task we used. […] Future studies comparing temporal generalization during imagery from short- and long-term memory are necessary to investigate this further.”</p><disp-quote content-type="editor-comment"><p>7) The clear decoding peak around 160 ms seems like it could be related to the N170 or M170 component. Since this component is so well-studied in terms of its role in face processing and its cortical sources in the ventral stream, it seems warranted to discuss this a bit more. Does the fact that the 160 ms peak cross-generalizes well across time and from perception to imagery indicate that face- or house-selective areas in the ventral stream are active at this time and then maintain activation later on, especially during imagery?</p></disp-quote><p>We thank the reviewers for pointing this out to us. Our findings can indeed be linked very clearly to the N170. Accordingly, we have added the following paragraph to the Discussion:</p><p>“We observed clear overlap between imagery and perceptual processing around 160 ms after stimulus onset. [...] Furthermore, this time also showed long temporal generalization within perception, indicating that the N170 representations also remain active over time during perception.”</p><disp-quote content-type="editor-comment"><p>8) The authors raise two different explanations for their results, e.g., in the Abstract: &quot;These results indicate that during imagery either the complete representation is activated at once and does not include low-level visual areas, or the order in which visual features are activated is less fixed and more flexible than during perception.&quot; However, it is possible that there is a probably less exciting, yet more parsimonious explanation for the pattern of results.</p><p>Perception and imagery could just evoke the same processing cascade (which the authors just dismiss us &quot;unlikely&quot;; main text, third paragraph), with two restrictions: (a) Imagery might not &quot;start&quot; as early as perception, thus not involving the earliest stages of perceptual processing. Thus, the onset of imagery decoding is expected to be later than the onset of perception decoding.</p></disp-quote><p>This third explanation is interesting, but we feel it is not in line with our findings: if there is a clear processing cascade during imagery one would expect a more diagonal pattern for the temporal generalization (unless there is a lot of temporal uncertainty between trials, see next point). Furthermore, one would expect that later time points during perception overlap with later time points during imagery, and the same for earlier time points. Even if the earliest time point that overlaps with imagery is later than the earliest time point during perception that contains information (i.e. ‘imagery starts later’), one would still observe a temporal shift in the overlap of different time points if there really was a processing cascade.</p><p>Alternatively, if the reviewers mean that imagery does not include the earliest time points, and only activates <italic>late</italic> visual representations, then that would refer to the cross-decoding results. We have reworked the Abstract to clarify these different explanations. Furthermore, in the Discussion we explained this point as follows:</p><p>“This absence of early overlap seems to imply that, even though early visual cortex has been implicated in visual imagery, there is no consistent overlap between imagery and early perceptual processing. […] Alternatively, early perceptual activity during imagery may be more brief and variable over time than high-level activation, leading to a cancelling out when averaging over trials.”</p><disp-quote content-type="editor-comment"><p>(b) Visual imagery is initiated in a top-down way; in contrast to the clear onset of a perceptual event, this initiation of imagery may vary substantially between trials (e.g., by an accumulation of temporal noise). Thus, the onset of imagery decoding is expected to have a smoother rise (obscuring the initial dynamics of the signal), and &quot;temporal generalization&quot; would increase a lot (as the evoked responses for single trials are relatively shifted across time).</p><p>Importantly, this explanation would neither suggest that in imagery complete representations are activated at once, nor that the order of processing steps is somehow altered as compared to perception. Note how also the &quot;dip&quot; in cross decoding at 210 ms is explained by this account, without evoking a more complicated explanation: If the neural representation at 210 ms after onset of the event is just very distinct (for whatever reason), the same signal scattered in time from trial to trial would impair cross-decoding specifically for this very time point (where temporal uncertainty hurts a lot). Do the authors think that their data are consistent with this alternative explanation, or are their data points that refute this account?</p></disp-quote><p>Please see our response to Issue 9.</p><disp-quote content-type="editor-comment"><p>9) To dissociate the temporal uncertainty explanation and the explanations given by the authors, I would strongly suggest additional analyses that try to estimate – and potentially correct for – the temporal uncertainty in imagery-related activation. One such analysis could try to align the imagery trials based on the within-imagery decoding. For example, the authors could perform a leave-one-trial-out analysis where they train a classifier on all trials, and test the classifier on the remaining trial, while shifting the left-out trial in time until the maximum decoding accuracy can be reached (this analysis should probably be done by randomly subsampling a smaller amount of trials as the testing set to reach more reliable estimates). Then for each trial's imagery period, this optimal temporal shift can be used to re-epoch the signal. If the processing sequence is similar and just suffers from temporal scattering, this procedure should significantly improve the cross-decoding accuracy while decreasing temporal generalization.</p></disp-quote><p>We agree that temporal uncertainty during imagery is a very important issue, which deserves more emphasis in this paper. We thank the reviewers for this suggestion and have run this analysis (full details can be found in Appendix A: “Temporal alignment on imagery accuracy”).</p><p>We found that the best shifts for each trial were distributed quite uniformly over time. This indicates that either the representation at imagery onset does indeed generalize broadly over time, or that there is a very wide distribution of temporal jitter across trials. If the latter were true and we did identify the onset of imagery in each trial, the resulting temporal generalization matrix should show a more diagonal pattern, indicating changing representations. This is not what we found: when training and testing on the realigned data, we still observed broad off-diagonal decoding. This seems to suggest that there is indeed broad temporal generalization, although it is possible that the decoding model on which we based the onset alignment did not reflect the imagery onset.</p><p>To further investigate this issue, we aligned the trials based on lagged correlations between the MEG signal, instead of decoding accuracy (see Appendix A: “Temporal alignment on lagged correlations”). Because this analysis does not make use of decoding accuracy for the onset shift, any observed increase in accuracy would be due to an increase in temporal alignment. We did not observe any increase in decoding accuracy after aligning the trials in this way. Furthermore, the broad off-diagonal temporal generalization remained. It is difficult to draw conclusions from this because the accuracy did not increase, indicating that perhaps the single-trial signals were too noisy to get reliable cross-correlations.</p><p>Finally, we explored the recently developed Temporally Unconstrained Decoding Analysis (TUDA; Vidaurre et al., 2018), which estimates the number of unique states that are needed to describe a neural process, while allowing variation in the exact timing of these states within each trial. We also corresponded with the first author of that paper regarding the method. Unfortunately, in its current form, TUDA is not suited to answer the questions that we have (for more info, see Appendix A: “Temporally unconstrained decoding analysis”).</p><p>In conclusion, we cannot confidently dissolve the temporal uncertainty issue with the current analysis tools. We agree that temporal uncertainty is an important alternative explanation for part of our findings. Furthermore, we feel that it is more parsimonious than our earlier suggestion that the order of low-level feature activations may be more flexible during imagery. Therefore, we have replaced that part of the Discussion with the following paragraph:</p><p>“An alternative explanation for the broad temporal generalization during imagery is that, compared to perception, imagery is less time-locked. […] To fully resolve this issue, future research should systematically explore the effect of temporal uncertainty on different underlying processes and analysis tools need to be developed that can account for variation in temporal dynamics between trials.”</p></body></sub-article></article>