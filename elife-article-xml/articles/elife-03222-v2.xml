<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">03222</article-id><article-id pub-id-type="doi">10.7554/eLife.03222</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Disparate substrates for head gaze following and face perception in the monkey superior temporal sulcus</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-13566"><name><surname>Marciniak</surname><given-names>Karolina</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-13878"><name><surname>Atabaki</surname><given-names>Artin</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-13879"><name><surname>Dicke</surname><given-names>Peter W</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-10206"><name><surname>Thier</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Cognitive Neurology</institution>, <institution>Hertie Institute for Clinical Brain Research, University of Tuebingen</institution>, <addr-line><named-content content-type="city">Tuebingen</named-content></addr-line>, <country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Romo</surname><given-names>Ranulfo</given-names></name><role>Reviewing editor</role><aff><institution>Universidad Nacional Autonoma de Mexico</institution>, <country>Mexico</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>marciniak.kar@gmail.com</email> (KM);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>thier@uni-tuebingen.de</email> (PT)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>14</day><month>07</month><year>2014</year></pub-date><pub-date pub-type="collection"><year>2014</year></pub-date><volume>3</volume><elocation-id>e03222</elocation-id><history><date date-type="received"><day>29</day><month>04</month><year>2014</year></date><date date-type="accepted"><day>11</day><month>07</month><year>2014</year></date></history><permissions><copyright-statement>© 2014, Marciniak et al</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Marciniak et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-03222-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.03222.001</object-id><p>Primates use gaze cues to follow peer gaze to an object of joint attention. Gaze following of monkeys is largely determined by head or face orientation. We used fMRI in rhesus monkeys to identify brain regions underlying head gaze following and to assess their relationship to the ‘face patch’ system, the latter being the likely source of information on face orientation. We trained monkeys to locate targets by either following head gaze or using a learned association of face identity with the same targets. Head gaze following activated a distinct region in the posterior STS, close to-albeit not overlapping with-the medial face patch delineated by passive viewing of faces. This ‘gaze following patch’ may be the substrate of the geometrical calculations needed to translate information on head orientation from the face patches into precise shifts of attention, taking the spatial relationship of the two interacting agents into account.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.001">http://dx.doi.org/10.7554/eLife.03222.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.03222.002</object-id><title>eLife digest</title><p>Gaze following—working out where someone else is looking, and then switching your attention to that position—is an important part of social behavior and learning. Additionally, it is thought to be an important step towards recognizing that others have a mind of their own. Humans mostly use eye position to work out the ‘gaze direction’ of someone else, whereas non-human primates rely instead on the orientation of the face. However, the neural circuits that control gaze following are thought to be similar in both.</p><p>Gaze following is a complex process that requires the brain to process a lot of different information. A face must be recognized, and its orientation worked out. A series of complex geometrical calculations must then be performed to work out the direction of the gaze, and how this relates to the position of the observer. Finally, the object of interest needs to be recognized and the attention of the observer focused on it.</p><p>In the monkey brain, there are six interconnected areas called face patch regions that respond when a monkey is shown a face. However, researchers do not understand how monkeys translate the information about face orientation gathered by these regions into information about where to look during gaze following.</p><p>Marciniak et al. performed functional magnetic resonance imaging on monkeys to track the flow of blood to different regions of the brain—the higher the blood flow, the more that area of the brain is working. To identify the location of their face patch regions, the monkeys first looked at faces. When the monkeys then performed a gaze following task, a region of the brain close to—but not overlapping—the face patches was activated. Marciniak et al. suggest this is the ‘gaze following patch’ where the brain performs the demanding calculations to translate face orientation into a position to look at.</p><p>As gaze following is important in social interactions, understanding the neural circuits behind it could help us understand social disorders.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.002">http://dx.doi.org/10.7554/eLife.03222.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>macaque monkey</kwd><kwd>gaze following</kwd><kwd>joint attention</kwd><kwd>monkey fMRI</kwd><kwd>face patches</kwd><kwd>social attention</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>TH 425/12-1</award-id><principal-award-recipient><name><surname>Thier</surname><given-names>Peter</given-names></name></principal-award-recipient></award-group><funding-statement>The funder had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A distinct cortical region serves head gaze following, and is needed to establish joint attention with others and to ultimately develop a theory of others' mind.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Successful social interactions require understanding of peer dispositions, desires, beliefs and intentions. A major step in developing this <italic>theory of</italic> (the other one's) <italic>mind</italic> (TOM) is our ability to shift our attention to the same location and/or object the other one is interested in, that is to establish <italic>joint attention</italic> (<xref ref-type="bibr" rid="bib4">Baron-Cohen, 1995</xref>). By associating our own object-related aspirations and intentions with the object, we may arrive at a viable theory of the other one's mind. In order to shift our attention to the right place or object, we rely on spatial information provided by peer body such as the direction of a pointing finger or the orientation of the head and the shoulder girdle. In case of humans, arguably the most important bodily cue, reliably providing extremely precise spatial information (<xref ref-type="bibr" rid="bib8">Bock et al., 2008</xref>), is peer eye gaze, a cue that can be easily retrieved even from quite some distance because of the high contrast border between the eye's dark center and periphery. The eyes of macaque monkeys and many other nonhuman primates (NHP) lack comparable contrasts (<xref ref-type="bibr" rid="bib27">Kobayashi and Kohshima, 1997</xref>), which is why eye gaze seems to be of little importance in this group (<xref ref-type="bibr" rid="bib34">Lorincz et al., 1999</xref>). While monkeys seem to lack a full-fledged TOM (<xref ref-type="bibr" rid="bib3">Anderson et al., 1996</xref>) (<xref ref-type="bibr" rid="bib19">Flombaum and Santos, 2005</xref> for a different view), they are nevertheless able to establish joint attention with conspecifics, largely relying on head gaze, that is the orientation of peer head (=face). Importantly, not only human but also monkey gaze following seems to be geometric: the observer identifies peer focus of attention by following his/her gaze towards the object of interest (<xref ref-type="bibr" rid="bib16">Emery et al., 1997</xref>; <xref ref-type="bibr" rid="bib17">Emery, 2000</xref>). In other words, rather than simply using the directional information provided by a gaze cue to shift attention out from the center until an object of potential interest is encountered, geometrical gaze following implies that a gaze vector is defined which is used to search for the object of interest (<xref ref-type="bibr" rid="bib12">Butterworth and Jarrett, 1991</xref>). Another important feature shared by the gaze following of monkeys and man is the dependence on social context. For instance, human observers tend to prefer gaze cues of those whom they feel close to <xref ref-type="bibr" rid="bib31">Liuzza et al. (2011)</xref>, while monkeys are particularly eager to follow the gaze of higher status conspecifics (<xref ref-type="bibr" rid="bib46">Shepherd et al., 2006</xref>). The availability of geometric gaze following in monkeys and man and the modulatory influence of context supports the idea that monkey and human gaze following may actually be closely related, sharing homologous substrates, although the choice of the relevant social cue—eye vs head—differs. In any case, perceiving peer eye or head gaze and converting it into a gaze vector is only a first step in a sequence of demanding computations that ultimately lead to the establishment of joint attention. This is a consequence of the fact that objects of interest may lie anywhere relative to the demonstrator and the observer. Only if the object were midway between the two, joint attention could be established by simply mirroring the demonstrator's gaze vector. However, as this specific location will be an exception rather than the rule, the object position will have to be transformed from a demonstrator-centered frame of reference (FOR) into an observer-centered FOR before a successful shift of attention can be programmed.</p><p>Previous work on gaze following suggests that the computational steps leading from the extraction of eye and/or head direction to shifts of attention are based on a distributed network of areas located in the superior temporal sulcus and the posterior parietal cortex. For instance, single unit recordings from monkey area LIP suggest that it is the major substrate of the shifts of attention which are prompted by social cues providing spatial information (<xref ref-type="bibr" rid="bib47">Shepherd et al., 2009</xref>) as well as by non-social spatial cues (<xref ref-type="bibr" rid="bib7">Bisley et al., 2011</xref>). On the other hand, BOLD imaging studies of human cortex have consistently singled out an area in the posterior superior temporal sulcus (pSTS) specifically activated by the processing of eye gaze cues leading to subsequent shifts of attention (<xref ref-type="bibr" rid="bib35">Materna et al., 2008a</xref>). BOLD imaging is also able to delineate a patch of cortex in the monkey STS activated by head gaze following, arguably offering similar functionality as the human pSTS region and perhaps even being homologous (<xref ref-type="bibr" rid="bib26">Kamphuis et al., 2009</xref>). Yet, what this specific functionality might be remains unclear. An obvious possibility is that this 'gaze following patch' may contribute to extracting relevant facial features. This possibility is supported by the fact that previous work on face processing has established a set of six disparate but interconnected 'face patches' distributed along the rostro-caudal extent of the monkey STS, that is found in the same general region as the gaze following patch (<xref ref-type="bibr" rid="bib48">Tsao et al., 2003</xref>, <xref ref-type="bibr" rid="bib49">2008</xref>). As previous work on face processing has suggested that these face patches form a hierarchically organized network with functionally specialized nodes (<xref ref-type="bibr" rid="bib38">Moeller et al., 2008</xref>; <xref ref-type="bibr" rid="bib20">Freiwald and Tsao, 2010</xref>), we reasoned that one of these nodes might actually correspond to the gaze following patch. Using fMRI to delineate the passive face patch network and the gaze following patch in the same rhesus monkeys, we here show that, surprisingly, this expectation is not met. Rather than being part of the face patch network, the gaze following patch remains outside the network, albeit close to one of its nodes, the medial face patch.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral results</title><p>In the first experiment, the two monkey subjects ('observers') were exposed to the portraits of fpur monkey individuals turning their head at one out of four different targets arranged along the horizontal. The two observers were instructed to either use head gaze orientation to identify the spatial target (gaze following) or, alternatively, to exploit learned associations between the identity of the seen faces and the individual targets (identity-matching condition) while ignoring head gaze (<xref ref-type="fig" rid="fig1">Figure 1A,B</xref>). The target choice was indicated by precise saccades to one of the four targets (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In other words, the two conditions were the same in terms of the visual information provided by the presented portraits as well as the motor behavior the visual stimuli prompted. However, they differed with regard to the facial cues used to prompt shifts of attention. We compared the accuracy of behavioral responses as measured by the percentages of correct choices for the gaze following and the identity-matching task, using data sets underlying the fMRI analysis. The accuracy levels were not significantly different in the two tasks (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Saccade latencies were also not different (<xref ref-type="fig" rid="fig1">Figure 1E</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.03222.003</object-id><label>Figure 1.</label><caption><title>Experimental paradigm and behavioral results (‘gaze following’ paradigm).</title><p>(<bold>A</bold>) Stimuli. 16 portraits used in the gaze following and identity matching tasks, arranged by the same identity (rows) or head orientations (columns, demonstrator's head orientation eccentricity indicated in brackets). The arrows point to the correct target dot in gaze following (red) and identity matching task (green). Arrows and the scale with the eccentricity of the target as seen by the observer were not visible during the experiment. Portraits and target bar were presented on an otherwise black background (here shown as gray for better visualization). (<bold>B</bold>) Sequence of events. Exemplary gaze following (left) and identity matching (right) trials. (<bold>C</bold>) Exemplary horizontal eye movements sampled during a typical fMRI run. The gray shaded horizontal area around 0° indicates the limits (±2°) of the fixation window, the red areas indicate gaze following blocks and the green ones identity matching blocks. White areas outline the 'fixation-only' blocks. (<bold>D</bold>) Median percentages of correct answers in gaze following (red) and identity matching blocks (green), pooled separately for each observer (M1: 138 blocks; M2: 150 blocks) in ‘gaze following’ paradigm. Error bars represent 95% confidence intervals. The difference was not significant (ns, Wilcoxon signed rank test: p=0.67 [M1], p=0.43 [M2]). Dashed line indicates the chance level in each task (25%). (<bold>E</bold>) Mean reaction times in gaze following (red) and identity matching blocks (green), pooled separately for the two observers (M1:138 blocks; M2: 150 blocks) in ‘gaze following’ paradigm. Error bars represent standard errors. The difference was not significant (ns, paired samples <italic>t</italic> test: p=0.08 [M1]; p=0.22 [M2]).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.003">http://dx.doi.org/10.7554/eLife.03222.003</ext-link></p></caption><graphic xlink:href="elife-03222-fig1-v2.tif"/></fig></p><p>The two control tasks used to rule out that monkeys might have managed to circumvent true gaze following in the gaze following task by resorting to learned associations between the position of the portraits on the screen and the spatial (or ordinal) position of the targets, were carried out only outside the scanner. We assigned responses to three possible categories, the first one consistent with gaze following, the second one comprising responses reflecting a workaround strategy (control experiment 1: learned spatial associations; control experiment 2: learned order associations) and the third one, neither consistent with category 1 nor 2 (<xref ref-type="fig" rid="fig2">Figure 2A,B</xref>; ‘Materials and methods’). In each and every case the number of responses in the gaze following category by far surpassed the numbers in the two other categories (<xref ref-type="fig" rid="fig2">Figure 2C,D</xref>). The dominance of gaze following was statistically highly significant (<xref ref-type="fig" rid="fig2">Figure 2C,D</xref>). These behavioral results clearly demonstrated that the observers M1 and M2 followed the gaze of the monkey portrait in the gaze following task. To solve the identity-matching task, the two monkeys obviously used individually adjusted strategies to identify the correct target. Depending on the behavioral context, they relied primarily on associations of individual identities with absolute spatial position or on relative spatial order (<xref ref-type="fig" rid="fig2s1 fig2s2 fig2s3 fig2s4">Figure 2—figure supplements 1–4</xref>).<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.03222.004</object-id><label>Figure 2.</label><caption><title>Control experiments.</title><p>(<bold>A</bold>) Testing for learned associations between head orientation and the spatial position of the target. Sequence of normal gaze following trials (I) with catch trials (II) where demonstrator portrait was shifted horizontally (here by 10°). Subject's responses in the catch trials were later classified into three categories (III): (1) The 'gaze following' category (red outline). (2) The 'learned spatial association' category (blue outline). (3) The 'other' category (gray outline). Dashed lines in the figures indicate the observer’s eye gaze. (<bold>B</bold>) Testing for associations between head orientation and the ordinal position of targets. Sequence of normal gaze following trials (I) with catch trials (II) where the 10° eccentricity targets maintained their standard spatial position but changed their ordinal position (II). The responses in catch trials were later classified into three categories (III): (1) The 'gaze following' category (red outline). (2) The 'learned order association' category (blue outline). (3) The 'other' category (gray outline). (<bold>C</bold>) The results of control Experiment 1 (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Mean percentages of responses classified as the 'gaze following' category (red column), the 'learned spatial associations' category (blue column) and in the 'other' category (gray column). Both monkeys showed significantly more responses in the 'gaze following' category than in the two other ones (repeated measures 1-way ANOVA, significant effect of the factor 'response category' (F<sub>2,54</sub> = 51.23, p&lt;0.001 [M1]; F<sub>2,32</sub> = 127.4, p&lt;0.001 [M2]). (<bold>D</bold>) The results of the control Experiment 2 (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Mean percentages of responses classified as the 'gaze following' category (red column), the 'learned order associations' category (blue column) and in the 'other' category (gray column). Both monkeys showed significantly more responses in the 'gaze following' category than in the two other ones (repeated measures 1-way ANOVA, significant effect of the factor 'response category' [F<sub>2,20</sub> = 47.8, p=0.001 (M1); F<sub>2,22</sub> = 132.2, p&lt;0.001 (M2)]); In [<bold>C</bold>] and [<bold>D</bold>] post hoc pairwise comparisons [with Bonferroni correction] are indicated with significance levels: ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05, not significant [ns]; n indicates the number of experimental repetitions. Error bars represent standard errors. M1 = monkey 1, M2 = monkey 2.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.004">http://dx.doi.org/10.7554/eLife.03222.004</ext-link></p></caption><graphic xlink:href="elife-03222-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03222.005</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Design of the control experiments testing for learned associations between facial identity and the spatial position of the target.</title><p>It was performed outside the MRI scanner in order to shed light on the strategies utilized to solve the identity matching task. The observer was confronted with a sequence of normal identity matching trials (I) with occasional catch trials (II, 6–12% share) in which the demonstrator portrait was shifted horizontally (shifts of −10°, −5°, 5° and 10° with respect to the default central position, the cartoon assumes −5° shift). In any case, the observer was asked to generate a saccade towards the target singled out by facial identity. In the case of ordinary identity matching trials the observer was rewarded contingent on the response. In the case of catch trials, a reward was granted on 50% of the catch trials, independent of the target chosen, provided the observer had stayed on the chosen target for at least 1 s. These responses were later classified into three strategy categories (III). (1) The 'learned absolute spatial associations' category (blue outline): target chosen according to its absolute spatial position associated with a given demonstrator facial identity, (invariant in space after demonstrator's portrait shift). (2) The 'learned relative spatial associations' category (red outline): target chosen according to its relative distance associated with a given demonstrator portrait's identity (now shifted horizontally together with the demonstrator portrait; the cartoon assumes a shift of the target from 10° to 5°). (3) The 'other' category (gray outline). Dashed lines in the figures indicate the observer's eye gaze.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.005">http://dx.doi.org/10.7554/eLife.03222.005</ext-link></p></caption><graphic xlink:href="elife-03222-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03222.006</object-id><label>Figure 2—figure supplement 2.</label><caption><title>Design of the control experiment testing for associations between the facial identity and the ordinal position of targets.</title><p>Again it was performed outside the MRI scanner in order to shed light on the strategies utilized to solve the identity matching task. Also here the observer was confronted with a sequence of normal identity matching trials (I) with occasional catch trials (II, 6–12% share). In catch trials the outer targets stayed in their standard locations of −10° and 10° respectively, whereas inner targets (normally at −5° and 5° respectively) were shifted further out to −15° and 15° eccentricity respectively. As a consequence of this shift, the 10° eccentricity targets maintained their standard spatial position but changed their ordinal position (from 1 and 4 to 2 and 3) (II). Again, the responses in catch trials later classified into three categories (III): (1) The 'learned order associations' strategy (target chosen according to its ordinal position in the target order with respect to a given demonstrator's identity, shifted from 10° to 15° on the cartoon, blue outline). (2) The 'learned absolute spatial associations' category (target chosen according to its absolute spatial position associated with a given demonstrator's identity, keeping his position at 10° on the cartoon, red outline), (3) The 'other' strategy.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.006">http://dx.doi.org/10.7554/eLife.03222.006</ext-link></p></caption><graphic xlink:href="elife-03222-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03222.007</object-id><label>Figure 2—figure supplement 3.</label><caption><title>The results of the control experiment testing for associations between demonstrator's identity and target location (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</title><p>Mean percentages of responses classified as falling into the 'learned absolute spatial associations' category (blue column), the 'learned relative spatial associations' category (red column) and in the 'other' category (gray column). Both monkeys exhibited significantly more responses in the 'learned absolute spatial associations' category than in the two other categories (repeated measures 1-way ANOVA, significant effect of the factor 'response category' [F<sub>2,18</sub> = 30.6, p&lt;0.001 (M1); F<sub>2,18</sub> = 104.2; p&lt;0.001 (M2)]; post hoc pairwise comparisons [with Bonferroni correction] are presented on the graph with their significance levels: ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05, not significant [ns]; n indicates the number of experimental repetitions). Error bars represent standard errors.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.007">http://dx.doi.org/10.7554/eLife.03222.007</ext-link></p></caption><graphic xlink:href="elife-03222-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03222.008</object-id><label>Figure 2—figure supplement 4.</label><caption><title>The results of the control experiment testing for associations between demonstrator's identity and the ordinal position of targets (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</title><p>Mean percentages of responses classified as falling in the 'learned order associations' category (blue column), the 'learned absolute spatial associations' category (red column), and the 'other' category (gray column). We found differences in the preferred categories between two observers (repeated measures one-way ANOVA [F<sub>2,18</sub> = 170.9, p&lt;0.001 (M1); F<sub>2,22</sub> = 175.9, p=0.01 (M2)], post hoc pairwise comparisons [with Bonferroni correction] are presented on the graph with their significance levels: *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, not significant [ns]. n indicates the number of experimental repetitions). Error bars represent standard errors. M1 = monkey 1, M2 = monkey 2.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.008">http://dx.doi.org/10.7554/eLife.03222.008</ext-link></p></caption><graphic xlink:href="elife-03222-fig2-figsupp4-v2.tif"/></fig></fig-group></p></sec><sec id="s2-2"><title>Comparison of BOLD responses to gaze following and identity matching (Experiment 1)</title><p>To identify brain regions specifically activated when the observers relied on gaze orientation, we looked for voxels showing significantly larger BOLD responses to gaze following than to identity matching. Analyzing the whole brain of M1 for the 'gaze following &gt; identity matching' BOLD contrast, the only region showing a significant contrast was a patch of voxels (='gaze following (GF) patch') located unilaterally on the lower bank of the right STS, between the interaural line and 2 mm posterior to it (A0-P2), near to the dorsal end of the inferior occipital sulcus (<xref ref-type="fig" rid="fig3">Figure 3</xref>). We reasoned that the absence of a significant BOLD contrast on the opposite site might have been the consequence of a poorer signal-to-noise ratio (SNR) on the left side, possibly due to the positioning of the eye-tracking camera in front of the left eye. To test it, we performed additional experiments in M1 with a focal coil centered on the posterior left STS, expected to be less affected by the camera. Yet, also with this coil we did not obtain a significant BOLD contrast in the area sampled by the coil, which included the region corresponding to the activated GF patch on the opposite side. Interestingly, despite the fact that the focal coil was centered on the left STS, it allowed us to confirm the right posterior STS BOLD contrast obtained in a whole-brain analysis of the data collected with the bilateral coil (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Finally, also the possibility that the unilateral BOLD pattern might be secondary to differences in the behavior directed at the two sides can be excluded. A comparison of responses to the demonstrator's gaze directed to the left and to the right respectively did not reveal any differences in accuracy (‘Materials and methods’).<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.03222.009</object-id><label>Figure 3.</label><caption><title>'Gaze following vs identity matching' BOLD contrast.</title><p>(<bold>A</bold>) Lateral views of the partially inflated hemispheres of monkeys M1 and M2 with significant (p&lt;0.005, uncorrected, 5 contiguous voxels) BOLD 'gaze following vs identity matching' contrasts. A = anterior, P = posterior, L = left, R = right, sts = superior temporal sulcus, ios = inferior occipital sulcus, lus = lunate sulcus, ls = lateral sulcus. (<bold>B</bold>) Coronal sections through the brains of monkeys M1 and M2 with corresponding significant BOLD contrast from (<bold>A</bold>). The color scale bar gives the t-scores indicating the size of significant BOLD contrasts. The numbers in the left corners of each section indicate the distance from the vertical interaural plane of each monkey. L = left, R = right.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.009">http://dx.doi.org/10.7554/eLife.03222.009</ext-link></p></caption><graphic xlink:href="elife-03222-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03222.010</object-id><label>Figure 3—figure supplement 1.</label><caption><title>'Gaze following vs identity matching' BOLD contrast evoked in Experiment 1 using a unilateral small coil placed on the left hemisphere of M1.</title><p>Coronal sections based on T1 weighted anatomical scans cut through the brain M1 with significant BOLD responses mapped onto the sections. (<bold>A</bold>) Absence of significant activity at a significance level p&lt;0.005, uncorrected, 5 contiguous voxel. (<bold>B</bold>) Significance level p&lt;0.05, uncorrected, 5 contiguous voxels. The numbers in the left corner of each section indicate the AP-distance from interaural plane of the monkey. Notice that the BOLD response is contralateral to the position of the coil. L = left, R = right.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.010">http://dx.doi.org/10.7554/eLife.03222.010</ext-link></p></caption><graphic xlink:href="elife-03222-fig3-figsupp1-v2.tif"/></fig></fig-group></p><p>Since the only significant BOLD activation yielded by the whole-brain analysis of M1 was in the STS, we focused our scanning onto the temporal lobes of M2, using a bilateral and a unilateral coil configuration (‘Materials and methods’). By this approach we revealed a significant BOLD contrast for gaze following compared to identity matching in the lower bank of the STS on both sides, around 1 mm anterior to the interaural line (A1), near the dorsal end of the inferior temporal sulcus (<xref ref-type="fig" rid="fig3">Figure 3</xref>). The GF patch in the right hemisphere of M2 was shifted by 1–2 mm anterior to the coordinates of the GF patch in M1. It is important to emphasize that the patches were singular in both monkeys and located in the same general part of the STS. This strongly suggests that the slight shift is a manifestation of interindividual variability and thus does not question the spatial identity of the GF patch in the two monkeys. However, it is harder to explain the fact that M2 in the left STS had 3 disparate patches of gaze following-associated BOLD. In terms of their location, the posterior-most patch, which exhibited a much stronger peak BOLD signal than the other two, corresponds to the GF patch on the right side in terms of coordinates. On the other hand, the peak BOLD responses of the two more anterior patches located in the left STS, around A4 and A8 respectively, were much weaker, although consistent across the usage of the two different coil systems. We will reserve the term 'GF patch' to the posterior patch, consistently showing gaze following-associated BOLD activity and use the qualifier 'anterior' when discussing the two anterior patches in the left STS of M2.</p><p>For the 'identity matching &gt; gaze following' BOLD contrast we reached significance only at a level of p&lt;0.05 (uncorrected). In M1 the activity was found unilaterally on the lower lateral bank of the right STS around 17 mm anterior to the interaural line (A17). In M2 it was bilateral in the medial part of the STS around 22 mm anterior to the interaural line (A22).</p></sec><sec id="s2-3"><title>BOLD activation related to the perception of faces (Experiment 2)</title><p>Analyzing the whole brain of M1 and focusing on the temporal lobes of M2, we identified a pattern of face-specific BOLD activations consisting of a number of distinct patches. Their coordinates and their spatial layout corresponded to the face patches described by previous work (<xref ref-type="bibr" rid="bib48">Tsao et al., 2003</xref>, <xref ref-type="bibr" rid="bib49">2008</xref>; <xref ref-type="bibr" rid="bib38">Moeller et al., 2008</xref>) and at least partially confirmed by subsequent studies (<xref ref-type="bibr" rid="bib42">Pinsk et al., 2005</xref>; <xref ref-type="bibr" rid="bib6">Bell et al., 2009</xref>; <xref ref-type="bibr" rid="bib28">Ku et al., 2011</xref>): three bilateral anterior face-patches (anterior medial, AM; anterior lateral, AL; anterior fundus, AF), two middle bilateral face-patches (middle lateral, ML and middle fundus, MF) and one bilateral posterior face patch (posterior lateral, PL) (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The results were consistent across the two monkeys and consistent across the two different receiver coil systems used in M2 (focal unilateral coil vs bilateral coil). The least clear patches were AM, AF and PL, as the BOLD activation for those voxels reached significance only at a level of p&lt;0.05 (uncorrected), whereas the other clusters (ML, MF and AL) reached significance already on a level of p&lt;0.005 (uncorrected).<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.03222.011</object-id><label>Figure 4.</label><caption><title>Comparison of the patterns of BOLD responses to 'gaze following' and 'identity matching' with the face patch BOLD pattern, delineated by the passive viewing of faces.</title><p>(<bold>A</bold>) Lateral views of the partially inflated hemispheres of monkeys M1 and M2 with borders of significant BOLD responses. Face patches (orange) based on 'faces vs nonfaces' contrast (p&lt;0.05, uncorrected, 5 contiguous voxels) masked with an 'all non-scrambled vs all scrambled' objects' contrast (p&lt;0.05 uncorrected). The red contours: significant BOLD contrasts for the 'gaze following vs identity matching' comparison (p&lt;0.005, uncorrected, 5 contiguous voxels). The green contours: significant BOLD contrasts for the opposite, 'identity matching vs gaze following' comparison (p&lt;0.05, uncorrected, 5 contiguous voxels). A = anterior, P = posterior, L = left, R = right, sts = superior temporal sulcus, ios = inferior occipital sulcus, lus = lunate sulcus, ls = lateral sulcus. (<bold>B</bold>) Coronal sections through the brains of monkeys M1 and M2 with corresponding significant BOLD contrasts from (<bold>A</bold>). The numbers in the left corners indicate the distance from the vertical interaural plane of each monkey (positive values anterior, neg. posterior) (L = left, R = right).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.011">http://dx.doi.org/10.7554/eLife.03222.011</ext-link></p></caption><graphic xlink:href="elife-03222-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.03222.012</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Examples of stimuli used in the 'passive face perception' experiment.</title><p>(<bold>A</bold>) Biological and non-biological objects, (<bold>B</bold>) faces and scrambled versions of objects/ faces (lowermost row), (<bold>C</bold>) grid-scrambled images. The human faces were taken from the Nottingham Scans database (free for research use under the terms of a Creative Commons Attribution license, <ext-link ext-link-type="uri" xlink:href="http://pics.psych.stir.ac.uk/">http://pics.psych.stir.ac.uk</ext-link>), all other images were from a variety of freely available sources.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.03222.012">http://dx.doi.org/10.7554/eLife.03222.012</ext-link></p></caption><graphic xlink:href="elife-03222-fig4-figsupp1-v2.tif"/></fig></fig-group></p></sec><sec id="s2-4"><title>The spatial relationship of gaze following-related BOLD activity and the face patch system</title><p>To explore the relationship of the GF patch (Experiment 1) to the face patches found in Experiment 2, we projected the BOLD patterns obtained in the two experiments onto coronal sections and onto a partially unfolded 3D representation of the individual monkey's brain, based on anatomical MRI images. The GF patch in the posterior STS did not overlap with any of the face patches we identified (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Actually, the GF patch was posterior (with respect to the interaural line) relative to the middle face patches (ML and MF) and anterior to the posterior face patch (PL). Moreover, despite the slight difference between the location of the GF patch in the two monkeys the relative distance between this patch and the middle face patch (ML) matches in the two monkeys. In other words, the middle face patches of the two monkeys showed a comparable spatial offset to the GF patch. Unlike the major (posterior) GF patch, the additional anterior gaze following patches found in the left hemisphere of M2 only, overlapped with the MF face patch in this monkey. Interestingly, the weak anterior patch we had found for the opposite, 'identity matching vs gaze following' contrast, overlapped with anterior face patches: in M1 the unilateral identity patch on the lower bank of the STS around A17 right overlapped with the anterior lateral face patch (AL) in this monkey. In M2 the bilateral identity patches on the medial part of the STS around A22 overlapped with the anterior medial face patch (AM).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We asked monkeys to use head gaze orientation of a portrayed conspecific to identify the spatial target the portrayed monkey was looking at, and to overtly shift attention to the same target. If cued by an alternative instruction, the same subjects exploited learned association between portrait identities and specific target locations to shift attention while ignoring gaze.</p><p>Rather than to engage in geometrical head gaze following, the experimental animals might have learned to associate the head orientation of the portrayed monkey with particular target positions, either defined in absolute or in relative terms. We could rule out that the experimental animals relied on a workaround strategy based on such learned associations by presenting catch trials in behavioral control experiments in which spatial targets were presented in new associations with the portraits. In these catch trials, the experimental animals clearly preferred the target defined by head gaze of the portrayed monkey, rather than being misdirected to targets defined by one of the associations with portraits experienced in the training set. Hence, we can be sure that our experimental animals deployed geometrical head gaze following (<xref ref-type="bibr" rid="bib16">Emery et al., 1997</xref>; <xref ref-type="bibr" rid="bib17">Emery, 2000</xref>).</p><p>Using gaze direction rather than identity to shift attention was associated with a distinct patch of BOLD activity (GF patch) in a region of the posterior STS, on its lower bank around A1-P1, near the dorsal end of the inferior temporal sulcus, corresponding to cytoarchitectonic area TEO (<xref ref-type="bibr" rid="bib9">Bonin and Bailey, 1947</xref>; <xref ref-type="bibr" rid="bib50">Ungerleider and Desimone, 1986</xref>) and area PITd, the latter defined based on topographic and connectional data (<xref ref-type="bibr" rid="bib18">Felleman and Van Essen, 1991</xref>). The location of the GF patch in this study is more posterior than BOLD activity, interpreted as being gaze following related, in two other monkeys used in a previous study by <xref ref-type="bibr" rid="bib26">Kamphuis et al. (2009)</xref>. In that study, gaze following-related activity was found between A4.4-6.4 in one animal and A0.8-A3.3 in the other one. In other words, the anterior deviation of the location of the gaze following activity in the second monkey of Kamphuis et al. relative to the GF patch in the present study is not much larger than the interindividual differences in the present study (M1:A0-P2, M2: A2-P1). Hence, we may safely assert agreement. The question is if the much more anterior location of gaze following-related BOLD activity in the first monkey studied by Kamphuis et al. is a reflection of more substantial interindividual variability or, alternatively, a consequence of a more fundamental difference between this monkey and the others. Actually, a limitation of the Kamphuis et al. study was that because of a lack of behavioral controls the authors could not exclude that the experimental animals might not have resorted to a workaround strategy based on learned associations between portraits and targets. Moreover, the two conditions compared in order to identify gaze following-related BOLD activity involved subtle visual differences. Hence, a possibility is that the pattern evoked in the Kamphuis et al. study may have been influenced by variables other than gaze following and we may speculate that these variables may have played a larger role in the odd monkey.</p><p>Whereas in one monkey the GF patch could be delineated bilaterally in the STS in corresponding locations, much to our surprise, the gaze following-related activity in the other monkey was confined to the right hemisphere. Our attempts to lead this unilaterality back to differences in the quality of the MRI signal on the two sides or to differences in the behavior directed at the two sides, failed. Whatever the reason for the unilaterality may be, it seems specific to gaze following as testing the same monkey in a passive face perception task to delineate the face patch network did not reveal any major hemispheric differences. Previous behavioral studies on gaze following in healthy human subjects and patients with lesions (<xref ref-type="bibr" rid="bib45">Ricciardelli et al., 2002</xref>; <xref ref-type="bibr" rid="bib1">Akiyama et al., 2006a</xref>, <xref ref-type="bibr" rid="bib2">2006b</xref>) supports a specific role of the right hemisphere in gaze following and social cognition in general (<xref ref-type="bibr" rid="bib11">Brancucci et al., 2009</xref> for review). On the other hand, fMRI studies of gaze following in humans have yielded mixed results, with some showing mainly right hemisphere STS activation (<xref ref-type="bibr" rid="bib39">Pelphrey et al., 2003</xref>, <xref ref-type="bibr" rid="bib40">2004</xref>, <xref ref-type="bibr" rid="bib41">2005</xref>; <xref ref-type="bibr" rid="bib13">Calder et al., 2007</xref>; <xref ref-type="bibr" rid="bib29">Laube et al., 2011</xref>) and others bilateral pSTS activation (<xref ref-type="bibr" rid="bib44">Puce et al., 1998</xref>; <xref ref-type="bibr" rid="bib25">Hoffman and Haxby, 2000</xref>; <xref ref-type="bibr" rid="bib35">Materna et al., 2008a</xref>, <xref ref-type="bibr" rid="bib36">2008b</xref>). Hence, it is tempting to see certain analogies between the human data and our monkey findings and to conclude that gaze following in monkeys and man may in principal be bihemispheric, though the substrates on the left side are deployed to individually varying degrees. The idea of true hemispheric differences in monkeys receives further support from the fact that the left STS of monkey M2 not only showed a GF patch with coordinates matching those of the patch in the right hemisphere but in addition two more anterior patches without equivalents in the right hemisphere of this monkey and none of the two hemispheres of monkey M1. Actually, recent work on audio-vocal communication in macaque monkeys, emphasizing an advantage of the left hemisphere (<xref ref-type="bibr" rid="bib24">Heffner and Heffner, 1984</xref>; <xref ref-type="bibr" rid="bib22">Ghazanfar and Hauser, 2001</xref>; <xref ref-type="bibr" rid="bib43">Poremba et al., 2004</xref>), supports the idea that there is some hemispheric specialization in macaque monkeys. In any case, more subjects will have to be studied to decide if the seeming unilaterality in monkey M1 is more than an idiosyncrasy.</p><p>Using the same face patch localizer paradigm as used in previous work (<xref ref-type="bibr" rid="bib48">Tsao et al., 2003</xref>; <xref ref-type="bibr" rid="bib38">Moeller et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Tsao et al., 2008</xref>), we delineated the set of face patches with comparable coordinates and spatial layout. The only difference with respect to the later studies of Tsao et al. (<xref ref-type="bibr" rid="bib38">Moeller et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Tsao et al., 2008</xref>) was a somewhat weaker and less consistent activation of the anterior and posterior face patches. This difference is most probably a consequence of methodological differences: these later studies (<xref ref-type="bibr" rid="bib38">Moeller et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Tsao et al., 2008</xref>) relied on MION-based measurements of activity-related changes in blood volume, well-known to be more sensitive by a factor of 3 than the BOLD method deployed by us, using the same 3T scanner (<xref ref-type="bibr" rid="bib30">Leite et al., 2002</xref>). Actually, when relying on BOLD imaging of the monkey face patch system and deploying a comparably high significance threshold, also an earlier study of <xref ref-type="bibr" rid="bib48">Tsao et al. (2003)</xref> identified only a fraction of the face patches which were later demonstrated with MION. In this earlier study the strongest activity was found in the face patches in the fundus and lower bank of the middle STS (corresponding to ML and MF in <xref ref-type="bibr" rid="bib38">Moeller et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Tsao et al., 2008</xref>) and the face-patch located in rostral TE (corresponding to AL in <xref ref-type="bibr" rid="bib38">Moeller et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Tsao et al., 2008</xref>). The patch in the STS in area TEO (corresponding to PL in <xref ref-type="bibr" rid="bib38">Moeller et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Tsao et al., 2008</xref>) was not reliable across different days and other anterior face patches (AF and AM) were not reported. This pattern fits our results. Nevertheless, we clearly identified all the medial and posterior face patches described before (<xref ref-type="bibr" rid="bib38">Moeller et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Tsao et al., 2008</xref>) which were in the vicinity of our GF patch. This is important as our major finding is the complete separation of the GF patch from any of the neighboring face patches with ML/MF being closest to the GF patch. The face patch system in monkeys is largely bilateral (<xref ref-type="bibr" rid="bib48">Tsao et al., 2003</xref>, <xref ref-type="bibr" rid="bib49">2008</xref>). The fact that unlike face-related activity, the gaze following-related activity was unilateral in one of the two monkeys studied, further supports the notion of two distinct and anatomically separated systems. On the other hand, the two weak BOLD responses observed inconsistently much more anterior in conjunction with gaze following overlapped with the MF face patch. This overlap may suggest that MF may be more important for processing information on facial orientation than on facial identity. Physical proximity does not necessarily imply connectivity and close functional relationship. Yet, the properties of neurons in ML/MF are suggestive of a functional relationship: many face-selective cells are tuned to specific face (=head) orientations (<xref ref-type="bibr" rid="bib21">Freiwald et al., 2009</xref>; <xref ref-type="bibr" rid="bib20">Freiwald and Tsao, 2010</xref>). This is exactly the kind of information head gaze following builds on.</p><p>To identify the goal of the other one's gaze in the frame of reference of the observer or, alternatively, in a world-centered frame of reference shared by both agents, the spatial relationship of the two agents and the relationship of potential goal objects relative to the two agents needs to be taken into account also. Hence, it is intriguing to speculate that the GF patch could be the substrate of the geometrical calculations needed to establish this goal representation, to this end adding the required contextual information to the elementary face (=head) orientation information taken over from the ML/MF (<xref ref-type="bibr" rid="bib21">Freiwald et al., 2009</xref>). This idea receives additional support from the fact that microstimulation of parietal area LIP causes stimulation-induced BOLD responses in a part of the STS whose coordinates seem to correspond to those of our study GF patch (<xref ref-type="bibr" rid="bib14">Crapse et al., 2013</xref>). Area LIP is a well-established center of overt and covert shifts of attention guided by a wide variety of cues, including head gaze (<xref ref-type="bibr" rid="bib47">Shepherd et al., 2009</xref>; <xref ref-type="bibr" rid="bib7">Bisley et al., 2011</xref>).</p><p>Using a comparable approach to delineate the cortical substrates of eye gaze following in humans, gaze following related BOLD activity was described bilaterally in the posterior STS ('pSTS region') (<xref ref-type="bibr" rid="bib35">Materna et al., 2008a</xref>), later shown to be activated also by following other biological cues like finger pointing (<xref ref-type="bibr" rid="bib36">Materna et al., 2008b</xref>) and head orientation (<xref ref-type="bibr" rid="bib29">Laube et al., 2011</xref>). Importantly, presenting distracting eye gaze cue in conjunction with head gaze following led to a clear modulation of the BOLD signal in the pSTS (<xref ref-type="bibr" rid="bib29">Laube et al., 2011</xref>). We have not studied if changing eye orientation relative to the head changes gaze-related activity in the GF patch in monkeys. This must not be expected in view of the fact that monkeys seem to pay very little attention to the other one's eye when trying to establish joint attention. However, independent of the differing weights attributed to eye and head gaze in humans and monkeys, the geometrical calculations needed to pinpoint a spatial goal shared by the two interacting agents are comparable. This consideration may suggest that the regions in the monkey and human STS activated by gaze following are analogous and possibly even homologous.</p><p>In contrast to the absence of overlap between the GF patch and the neighboring face patches, the BOLD activity related to the usage of facial identity for target localization in the anterior parts of the STS overlapped with activity evoked by the passive observation of faces. The identity-associated signal coincided with AM in M2 and AL in M1. The anterior location of our identity matching BOLD activation fits the previous electrophysiological finding of identity-selective neurons in the anterior inferotemporal gyrus (<xref ref-type="bibr" rid="bib23">Hasselmo et al., 1989</xref>; <xref ref-type="bibr" rid="bib15">Eifuku et al., 2004</xref>; <xref ref-type="bibr" rid="bib20">Freiwald and Tsao, 2010</xref>). In accordance with the notion that these patches are involved in establishing facial identity, it was recently demonstrated that the percept of facial identity is disrupted by microstimulation of AM (and ML) face patches (<xref ref-type="bibr" rid="bib37">Moeller and Tsao, 2013</xref>).</p><p>In summary, by requiring monkeys to use head gaze to locate spatial targets we could identify a highly specific region in the posterior STS (GF patch) well separated from those parts of the STS known to process visual information on faces and heads. We propose that this region corresponds to the pSTS of humans devoted to eye gaze following. We furthermore suggest that this region may be the substrate of the geometrical calculations needed to translate head orientation into precise shifts of attention.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>Two rhesus monkeys (Macaca mulatta): monkey M1 (6 years, 8 kg); monkey M2 (8 years, 11 kg) were implanted with three cf-PEEK (carbon-fiber-enforced polyetheretherketone) tripods, each attached to the skull with six ceramic screws (Thomas Recording). Surgeries were carried out under combination anesthesia with isoflurane and remifentanyl with monitoring of all relevant vital parameters (body temperature, CO<sub>2</sub>, blood oxygen saturation, blood pressure, ECG). After surgery, monkeys were supplied with opioid analgesics (buprenorphin) until they fully recovered. Every effort was made to minimize discomfort and suffering. The study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to the guidelines of the German law regulating the usage of experimental animals and the protocols approved by the local institution in charge of experiments using animals (Regierungspraesidium Tuebingen, Abteilung Tierschutz, permit-number N1/08).</p></sec><sec id="s4-2"><title>Training and behavioral control</title><p>The monkeys were placed in an MR-compatible primate chair in a horizontal ('sphinx') posture. To immobilize the monkey's head, it was fixed to the chair by screwing the tripods to an acryl cap with an integrated massive cf-PEEK rod, which then was connected to the chair's frame. Before scanning they were trained in a mock scanner having the same geometry, projection screen and offering a similar acoustic environment as the real scanner. Hearing protection was guaranteed in the mock as well as in the real scanner by custom-made earmuffs with thick plastic foam filling. Habituating and subsequently training on the behavioral tasks took 6 months in M2 and 1 year in M1 until the mean accuracy scores in each task for the training session reached 80%. Eye position was tracked in real time using a scanner-compatible low-cost CMOS-infra-red camera (C-MOS-Kameramodul1(C-CAM-A), Conrad Elektronik, Germany) with infra-red emitting LEDs illuminating the monkey's eyes. The custom-made software running on a standard PC determined orientation of the center of the pupil with a spatial resolution of 0.5° visual angle and a temporal resolution of 50 Hz. Camera and power supply cables were equipped with radio frequency (RF)-eliminating filters to minimize RF interactions in the real scanner (<xref ref-type="bibr" rid="bib26">Kamphuis et al., 2009</xref>). Fluid rewards were supplied via a long and flexible tube, with the control unit, valve and reservoir installed outside the scanner room.</p></sec><sec id="s4-3"><title>Scanning</title><p>Monkeys were scanned in a horizontal clinical scanner, 3T MRT (Trio, Siemens, Erlangen, Germany). Functional images were acquired using two custom-made linear receiver surface coils (diameter 13 cm, 'Helmholtz-configuration'; whole-brain scanning in M1 and STS-ROI scanning in M2, later referred as 'bilateral coil') or one small surface coil (diameter 3 cm; STS-ROI scanning in M2 and additional left STS-ROI scanning in M1, later referred as 'focal coil'), placed above the temporal lobe (centered on the posterior STS). Each functional time series consisted of gradient-echoplanar whole-brain images (repetition time (TR) = 2000 ms; echo time (TE) = 28 ms; flip angle = 70°; 64 × 64 matrix; 1.31 × 2.42 × 1.31 mm voxels; 22 horizontal slices) or STS-ROI images (repetition time (TR) = 1000 ms; echo time (TE) = 28 ms: flip angle = 70°; 64 × 64 matrix; 1.31 × 2.42 × 1.31 mm voxels; 11 horizontal slices). For the purpose of the first experiment ('gaze following' paradigm) 10,364 whole brain volumes in 80 functional runs (see 'Visual stimuli' section for definition of a single functional run) were scanned in six scanning sessions (separate days of measurement) in monkey M1, using the bilateral coil. Additionally, 4400 STS volumes were scanned (55 functional runs, 2 scanning sessions) using the focal coil, centered on the left posterior STS in M1. In M2 a total of 12,280 right STS volumes were scanned in 75 functional runs: 6276 vol in 36 functional runs during 3 scanning sessions using the focal coil and 6004 in 39 functional runs during 3 scanning sessions with the bilateral coil. A total of 10,501 left STS volumes were scanned in 67 functional runs (4497 vol in 28 functional runs during 3 scanning sessions using the focal coil; 6004 vol in 39 functional runs during 3 scanning sessions with the bilateral coil). In the second experiment ('passive face perception' paradigm) a total of 4626 vol were scanned in 28 functional runs (3 scanning sessions) in M1 using whole-brain volumes and the bilateral coil. In M2 a total of 6106 STS volumes in 29 functional runs during 3 scanning sessions were collected from the right hemisphere (2724 vol in 13 functional runs during 1 scanning session using the focal coil; 3382 vol in 16 functional runs during 2 scanning sessions using the bilateral coil) and a total of 6297 STS volumes in 30 functional runs during 3 scanning sessions on the left hemisphere (2915 vol in 14 functional runs during 1 scanning session using the focal coil; 3382 vol in 16 functional runs during 2 scanning sessions using the bilateral coil).</p></sec><sec id="s4-4"><title>Visual stimuli</title><sec id="s4-4-1"><title>'Gaze following' paradigm (Experiment 1)</title><p>We used color images of demonstrator monkey faces (the largest height and width 5.6 × 5.6°) presented in the center, together with four targets (red dots, diameter 0.8°) drawn on a virtual horizontal line at −10°, −5°, 5° and 10° eccentricity as seen by the observer. The target eccentricities as seen by the demonstrator monkey were four times as large (i.e., −40°, −20°, +20°, +40°), reflecting the fact that the target plane was four times closer to the demonstrator than to the observer chosen to be closer to the demonstrator in order to demand large gaze shifts on his side (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We used photographs of different monkeys living in the same colony as the observers, taken while the individuals were sitting in the primate chair (head-fixed) with their head (and eyes) directed at a spatially well-defined object of attention. The raw images were processed using 'PAINT.NET' free software to erase the headholder and recording chambers from the portraits. They were mirrored horizontally in order to generate opposite head gaze directions. The stimuli were presented using an LCD projector (NEC GT 950, 1024 × 768 pixels) placed outside the scanner room. Images (32 × 24° visual angle) were back-projected via a 45° angled mirror on a translucent screen, inside the scanner bore at a distance of 60 cm from the monkey's eyes. Stimuli were presented in blocks consisting of 10–12 'orientation' trials (observer had to shift attention overtly), or 5–6 'fixation-only' trials. Blocks of 'fixation-only' trials were alternated with 'orientation' blocks based on gaze following (gf) or identity matching (im) task. Every functional run started with a 'fixation-only' block and contained two repetitions of each of two 'orientation' blocks (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) while the order of gf and im blocks was pseudo-randomized across functional runs. Each trial started with the presentation of the portrait of a monkey oriented straight ahead, not part of the group of four portraits used to shift attention, with a small fixation target on the portrayed monkey's forehead. The observer had to keep his eyes within a 4 × 4° window centered on the fixation target. 400 ms later, one of the portrayed monkeys, with its head turned to one of the four peripheral targets, appeared. Another 1200 ms later the fixation target turned off, telling the observer to make a saccade to one of the peripheral targets. The color and the shape of the central fixation cue told him which rule to apply in order to identify the correct peripheral target. In the case of a red circular fixation cue (0.8° diameter) the observer was required to saccade to the target the demonstrator was looking at (gaze following). In the case of a green rectangle (0.5 × 0.8°), the saccade target was identified by the learned association between the target locations and the four individual demonstrator monkeys whose portraits were shown (identity matching). As the portrait of each individual monkey could be shown in four different head gaze orientations, corresponding to the four target locations, the stimulus set involved 16 stimuli (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) and the one used in a particular trial was chosen at random. Finally, in 'fixation-only' trials, indicated by a blue circle (0.8° diameter) fixation target, the observer had to withhold any eye movements and stay on the fixation target location within the fixation window until the end of the trial. In the case of 'orientation' trials, the fixation window was removed for 0.5 s after the go signal to give the observer the opportunity to move his eyes to the new target. Thereafter it was reestablished at the new target location for 1.3 s. A juice reward was delivered at the end of the trial if the observer had satisfied the task requirements (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Stimulus presentation and recording of eye movement data was controlled using an open source recording and stimulation system (<ext-link ext-link-type="uri" xlink:href="http://nrec.neurologie.uni-tuebingen.de/nrec">nrec.neurologie.uni-tuebingen.de/nrec</ext-link>).</p><p>To assure that the observers actually used geometrical head gaze following in gaze following trials rather than resorting to potentially learned associations between head orientation as seen in the portrait and particular targets, we carried out two psychophysical control experiments outside the scanner (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Like the standard experiment described before also the control experiments consisted of gaze following, identity matching and 'fixation-only' trials, however with interspersed catch trials. The first control experiment was designed to exclude learned associations between head orientation and spatial position of targets: There, in 6–12% of the gaze following trials (catch trials, <xref ref-type="fig" rid="fig2">Figure 2A</xref>, [II]) intermingled randomly with regular gaze following trials (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, [I]), the stimulus monkey's portrait was shifted horizontally (shifts of −10°, −5°, 5° and 10°, with respect to the default central position (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, [II]). The observer was asked to perform a saccade towards one out of four target positions after the offset of the fixation cue that had prompted gaze following. A reward was given on 50% of the catch-trials, independent of the target chosen, provided the observer stayed on the chosen target for at least 1 s. A total of 28 (M1) and 17 (M2) experimental sessions were performed, yielding altogether 460 (M1) and 260 (M2) catch trials respectively. The second control experiment tested for associations between head orientation and the ordinal position of targets within the sequence of four. Here, in the catch trials, the outer targets stayed in their standard locations of −10° and 10° respectively, whereas inner targets (normally at −5° and 5° respectively) were shifted further out to −15° and 15° eccentricity respectively. Thus, the 10° eccentricity targets maintained their standard spatial position but changed their ordinal position (from 1 and 4 to 2 and 3), (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, [II]). All other aspects of this experiment corresponded to the ones given for control Experiment 1. A total of 11 (M1) and 12 (M2) experimental sessions were performed, yielding 50 (M1) and 51 (M2) catch trials respectively. Finally, to identify the strategies our monkey observers used in order to solve the identity-matching task, we repeated both control experiments outside the scanner with catch trials interspersed in identity matching trials (<xref ref-type="fig" rid="fig2s1 fig2s2 fig2s3 fig2s4">Figure 2—figure supplements 1–4</xref>). To this end the same procedure as described before for the control experiments related to the gaze following task were used. It was based on introducing catch trials interspersed in a regular identity-matching task. The catch trials were the same as those used in the control experiments testing for spatial associations and order associations in the gaze following task. A total number of 10 (M1) and 10 (M2) experimental repetitions were performed in the experiment testing for spatial associations with 150 (M1) and 70 (M2) catch trials. A total number of 10 (M1) and 12 (M2) experimental repetitions were performed in the control experiment testing for order associations with 150 (M1) and 50 (M2) catch trials.</p></sec><sec id="s4-4-2"><title>'Passive face perception' paradigm (Experiment 2)</title><p>We used a set of different categories of black and white pictures: faces (monkey faces, human faces) and non-face objects (human bodies without head visible, human-made tools, fruits, hands). A second set of images was generated by scrambling the original ones (Adobe Photoshop CS5, scramble filter, 20 × 20 randomly shuffled blocks, [<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>]). The human faces were taken from the Nottingham Scans database (free for research use under the terms of a Creative Commons Attribution license, <ext-link ext-link-type="uri" xlink:href="http://pics.psych.stir.ac.uk">http://pics.psych.stir.ac.uk</ext-link>). All other images were from a variety of freely available sources. They were chosen to be as similar as possible to the stimuli used in the previous studies (<xref ref-type="bibr" rid="bib48">Tsao et al., 2003</xref>). The stimuli were presented using the same setup as the one used for the ‘gaze following’ paradigm. Each image had a size of 11 × 11°, was presented for 1s on a black and white random dot background (pixel size 0.05°) and was repeated once in each functional run. The monkey was rewarded for keeping his eye gaze within a fixation window of (2 × 2°) centered on a central fixation cue dot (0.5° diameter). Short breaks of fixation (not longer than 100 ms, mostly associated with eye blinks) were tolerated. Stimuli were presented in blocks of 16 images, all chosen randomly from the category of face stimuli or, alternatively, the various categories of non-face stimuli. Blocks of face stimuli (monkey faces or human faces) alternated pseudo-randomly with blocks of non-face objects (fruits, tools or headless bodies or hands). Each of these blocks was preceded by a block consisting of the scrambled versions of the following block. In each functional run, the sequence of 'scrambled faces, faces, scrambled non-faces, non-faces' was repeated four times (in total 16 blocks and 256 images). The serial position of the category (faces, non-faces) within the sequence was balanced across all functional runs.</p></sec></sec><sec id="s4-5"><title>Data analysis</title><sec id="s4-5-1"><title>'Gaze following' paradigm (Experiment 1)</title><p>Eye movements records were analyzed offline (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) in order to assess task performance, defined as the percentage of correctly chosen targets, in both gaze following and identity matching task. Only functional runs with success rates exceeding 70% in the two tasks were considered for further BOLD fMRI analysis. The hypothesis of a significant difference in accuracy between tasks was evaluated by running a Wilcoxon signed rank test (a Kolmogorov–Smirnov test had shown that the data were not distributed normally; p=0.001 [M1], p=0.001 [M2]). Response times (RTs) were calculated as the time between cue offset and the onset of the monkey's first saccade, the latter defined by an eye velocity threshold (&gt;50°/s). Significant differences in RTs between the two tasks were detected with a paired <italic>t</italic> test. A Kolmogorov–Smirnov test did not show deviation from normality of 'gaze following': (M1: p=0.08, M2: p=0.06) and 'identity matching' (M1: p=0.2, M2: p=0.06) distributions. In order to test behavioral performance of M1 for gaze following to the left and to the right, we calculated separately the response accuracy to demonstrator's left and right gaze for each gaze following block (138 in total). As the Kolomogorov–Smirnov test had shown that the two data sets were not distributed normally (p=0.001), we used related-samples Wilcoxon signed rank test to assess the significance of the difference between the median response accuracies to the right (Median = 100%, 95% CI = 100–100%, n = 138) and to the left (Median = 100%, 95% CI = 93.07–106.93%, n = 138) sites. The difference was not significant (Z = 0.15, p=0.88). To test for spatial and/or order associations of the subject solving the gaze following task, we examined a subject's responses to catch trials in the two control experiments. The response was defined as the first horizontal saccade to one out of four targets after the offset of the central cue, followed by fixation on the target for at least 1 s. The responses were classified into 3 categories. In the first control experiment, testing for spatial associations, these were the 'gaze following' category (the chosen target position was congruent with gaze direction of the stimulus monkey's portrait, now shifted horizontally together with the stimulus monkey's portrait), the 'learned spatial association' category (the chosen target position corresponding to the correct one before the horizontal shift of the appropriate monkey's portrait, indicating that the monkey exploited a learned association between the given portrait and the absolute target position in space) and the 'other' category (the subject's response not compatible with any of the previous categories), (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, [III]). In the second control experiment testing for ordinal associations we defined the following response categories: the 'gaze following' category (the chosen target position was congruent with the gaze direction of the portrayed monkey, irrespective of the changed ordinal position), the 'learned order association' category (the target shifted further out, i.e., the one with the largest/smallest ordinal position, is preferred, although gaze is directed at the inner target positions, indicating that the monkey relies on learned associations between portraits and target ordinal position) and the 'other' category (the subject's response not compatible with any of the previous categories), (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, [III]). We used a repeated measures one-way ANOVA to compare the differences in the percentages of correct responses across the strategies for each subject (Kolmogorov–Smirnov test first confirmed that the data were normally distributed: p&gt;0.05 in M1 and M2).</p><p>To test for strategies used in order to solve the identity-matching task we classified the subject's responses in catch trials interspersed in a regular identity-matching task into three strategy categories. In the experiment testing for spatial associations (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), those were: 'learned absolute spatial associations' category (target chosen according to its absolute spatial position associated with a given demonstrator's facial identity), 'learned relative spatial associations' category (target chosen according to its relative distance with respect to a given demonstrator's portrait, associated with its facial identity) and 'other' category. In the experiment testing for ordinal associations (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), we identified: 'learned absolute spatial associations' category (target chosen according to its absolute spatial position associated with a given demonstrator portrait's identity), 'learned order associations' category (target chosen according to its ordinal position in the target order associated with a given demonstrator's identity) and 'other' category.</p><p>Functional images of both monkeys were analyzed using the statistical parametric mapping program package SPM8 (Wellcome, Department of Cognitive Neurology, London, UK, <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>) implemented in Matlab 7.9 (R2009b). Images were spatially aligned, co-registered with the mean functional image which in turn was co-registered with the structural image of the individual monkey and finally spatially smoothed with a Gaussian filter (3 mm full-width-half-maximum). We calculated response levels (β values) associated with each experimental task for each voxel using a statistical analysis based on the general linear model (GLM). The BOLD response during each experimental task block was modeled as a boxed-car covariate of variable length in SPM8 using a human canonical hemodynamic function. Assuming a standard human hemodynamic function seemed pragmatic as previous work has not suggested fundamentally different hemodynamic responses in monkeys and humans (for instance compare the similar BOLD responses of both species in auditory (<xref ref-type="bibr" rid="bib5">Bauman et al., 2010</xref>) and visual cortex (<xref ref-type="bibr" rid="bib10">Boynton et al., 1996</xref>; <xref ref-type="bibr" rid="bib32">Logothetis et al., 2001</xref>; <xref ref-type="bibr" rid="bib33">Logothetis, 2002</xref>)). Regressors representing the estimated head movements (translation and rotation; altogether six degrees of freedom) were added to the model as covariates of no interest to account for artifacts due to head movements during scanning. Contrast analysis comparing the gaze following and the identity matching conditions were carried out for both monkeys. Significant changes were assessed using t-statistics. In order to take the large number of data from each monkey into account, we used a fixed effect model to analyze each successful scanning day individually (a total of 6 days for M1, a total of 6 days for M2) and then analyzed the contrast images provided by each model using a second-level random effects analysis. We labeled a contrast as significant if a single-voxel threshold of p&lt;0.005 (uncorrected) was met in at least five contiguous voxels. To analyze the data collected in the 'focal coil' experiments in M1 experiments we were able to perform fixed effect analysis based on the complete data pool. In this case, two different statistical significance levels were compared, a single-voxel threshold of p&lt;0.005 (uncorrected) met in at least five contiguous voxels or, alternatively, a single-voxel threshold of p&lt;0.05 (uncorrected), again in at least five contiguous voxels (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). For the purpose of visualization we used Caret (<ext-link ext-link-type="uri" xlink:href="http://brainvis.wustl.edu/wiki/index.php/Caret:About">http://brainvis.wustl.edu/wiki/index.php/Caret:About</ext-link>), which offered a flattened reconstruction of the cortical surface gray matter onto which the statistical t-map was projected.</p></sec><sec id="s4-5-2"><title>‘Passive face perception’ paradigm (Experiment 2)</title><p>Eye movement data were analyzed to assess the accuracy of the fixation. Functional runs in which the monkey failed to stay inside the fixation window of (2 × 2°) in at least 85% of the trials were rejected. The preprocessing of the MRI data followed the procedure described above, the only difference was the size of the full-width-half-maximum of the Gaussian filter used for spatial smoothing (here 2 mm). To define face-selective regions, we calculated the contrast 'faces vs other objects' (not considering scrambled images). The output was masked with a contrast of 'both faces and other objects vs all their scrambled counterparts' (thresholded to p&lt;0.05, uncorrected) to identify voxels selective only for complex images rather than for simple visual patterns. This procedure was in accordance with the one described previously (<xref ref-type="bibr" rid="bib48">Tsao et al., 2003</xref>, <xref ref-type="bibr" rid="bib49">2008</xref>). Because of the smaller amount of data collected here in comparison with the ‘gaze following’ paradigm (Experiment 1), we performed a fixed-effect analysis pooling all data obtained for each subject monkey. Statistical significance was assumed if a single-voxel threshold of p&lt;0.005 (uncorrected) in at least five contiguous voxels (or p&lt;0.05 uncorrected, 5 contiguous voxels) was met.</p></sec></sec></sec></body><back><sec sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>KM, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>AA, Acquisition of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>PWD, Acquisition of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con4"><p>PT, Conception and design, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to the guidelines of the German law regulating the usage of experimental animals and the protocols approved by the local institution in charge of experiments using animals (Regierungspraesidium Tuebingen, Abteilung Tierschutz, permit-number N1/08). All surgery was performed under combination anesthesia involving isoflurane and remifentanyl and every effort was made to minimize discomfort and suffering.</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akiyama</surname><given-names>T</given-names></name><name><surname>Kato</surname><given-names>M</given-names></name><name><surname>Muramatsu</surname><given-names>T</given-names></name><name><surname>Saito</surname><given-names>F</given-names></name><name><surname>Nakachi</surname><given-names>R</given-names></name><name><surname>Kashima</surname><given-names>H</given-names></name></person-group><year>2006a</year><article-title>A deficit in discriminating gaze direction in a case with right superior temporal gyrus lesion</article-title><source>Neuropsychologia</source><volume>44</volume><fpage>161</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.05.018</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akiyama</surname><given-names>T</given-names></name><name><surname>Kato</surname><given-names>M</given-names></name><name><surname>Muramatsu</surname><given-names>T</given-names></name><name><surname>Saito</surname><given-names>F</given-names></name><name><surname>Umeda</surname><given-names>S</given-names></name><name><surname>Kashima</surname><given-names>H</given-names></name></person-group><year>2006b</year><article-title>Gaze but not arrows: a dissociative impairment after right superior temporal gyrus damage</article-title><source>Neuropsychologia</source><volume>44</volume><fpage>1804</fpage><lpage>1810</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.03.007</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>JR</given-names></name><name><surname>Montant</surname><given-names>M</given-names></name><name><surname>Schmitt</surname><given-names>D</given-names></name></person-group><year>1996</year><article-title>Rhesus monkeys fail to use gaze direction as an experimenter-given cue in an object-choice task</article-title><source>Behavioural Processes</source><volume>37</volume><fpage>47</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1016/0376-6357(95)00074-7</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Baron-Cohen</surname><given-names>S</given-names></name></person-group><year>1995</year><source>Mindblindness: an essay on autism and theory of mind</source><publisher-loc>Cambridge, Massachusetts, USA</publisher-loc><publisher-name>MIT Press/Bradford Books</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bauman</surname><given-names>S</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Rees</surname><given-names>A</given-names></name><name><surname>Hunter</surname><given-names>D</given-names></name><name><surname>Sun</surname><given-names>L</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name></person-group><year>2010</year><article-title>Characterisation of the BOLD response time course at different levels of the auditory pathway in non-human primates</article-title><source>NeuroImage</source><volume>50</volume><fpage>1099</fpage><lpage>1108</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.103</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>AH</given-names></name><name><surname>Hadj-Bouziane</surname><given-names>F</given-names></name><name><surname>Frihauf</surname><given-names>JB</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year>2009</year><article-title>Object representations in the temporal cortex of monkeys and humans as revealed by functional magnetic resonance imaging</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>688</fpage><lpage>700</lpage><pub-id pub-id-type="doi">10.1152/jn.90657.2008</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bisley</surname><given-names>JW</given-names></name><name><surname>Mirpour</surname><given-names>K</given-names></name><name><surname>Arcizet</surname><given-names>F</given-names></name><name><surname>Ong</surname><given-names>WS</given-names></name></person-group><year>2011</year><article-title>The role of the lateral intraparietal area in orienting attention and its implications for visual search</article-title><source>The European Journal of Neuroscience</source><volume>33</volume><fpage>1982</fpage><lpage>1990</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07700.x</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bock</surname><given-names>S</given-names></name><name><surname>Dicke</surname><given-names>PW</given-names></name><name><surname>Thier</surname><given-names>P</given-names></name></person-group><year>2008</year><article-title>How precise is gaze following in humans?</article-title><source>Vision Research</source><volume>48</volume><fpage>946</fpage><lpage>957</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.01.011</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bonin</surname><given-names>Gvon</given-names></name><name><surname>Bailey</surname><given-names>P</given-names></name></person-group><year>1947</year><source>The Neocortex of Macaca Mulatta</source><publisher-loc>Urbana, Illinois, USA</publisher-loc><publisher-name>University of Illinois Press</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boynton</surname><given-names>GM</given-names></name><name><surname>Engel</surname><given-names>SA</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year>1996</year><article-title>Linear systems analysis of functional magnetic resonance imaging in human V1</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>4207</fpage><lpage>4221</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brancucci</surname><given-names>A</given-names></name><name><surname>Lucci</surname><given-names>G</given-names></name><name><surname>Mazzatenta</surname><given-names>A</given-names></name><name><surname>Tommasi</surname><given-names>L</given-names></name></person-group><year>2009</year><article-title>Asymmetries of the human social brain in the visual, auditory and chemical modalities</article-title><source>Philosophical Transactions of the Royal Society of London Series B, Biological Sciences</source><volume>364</volume><fpage>895</fpage><lpage>914</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0279</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butterworth</surname><given-names>G</given-names></name><name><surname>Jarrett</surname><given-names>N</given-names></name></person-group><year>1991</year><article-title>What minds have in common is space: spatial mechanisms serving joint visual attention in infancy</article-title><source>British Journal of Developmental Psychology</source><volume>9</volume><fpage>55</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1111/j.2044-835X.1991.tb00862.x</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calder</surname><given-names>AJ</given-names></name><name><surname>Beaver</surname><given-names>JD</given-names></name><name><surname>Winston</surname><given-names>JS</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Jenkins</surname><given-names>R</given-names></name><name><surname>Eger</surname><given-names>E</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><year>2007</year><article-title>Separate coding of different gaze directions in the superior temporal sulcus and inferior parietal lobule</article-title><source>Current Biology</source><volume>17</volume><fpage>20</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.10.052</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Crapse</surname><given-names>TB</given-names></name><name><surname>Cheng</surname><given-names>X</given-names></name><name><surname>Tsao</surname><given-names>D</given-names></name></person-group><year>2013</year><source>A strong input to area LIP from two distinct regions in IT cortex revealed by combined fMRI and microstimulation</source><publisher-loc>San Diego, CA</publisher-loc><publisher-name>Society for Neuroscience</publisher-name><comment>Program No. 824.07. Neuroscience 2013 Abstracts. Online</comment></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eifuku</surname><given-names>S</given-names></name><name><surname>De Souza</surname><given-names>WC</given-names></name><name><surname>Tamura</surname><given-names>R</given-names></name><name><surname>Nishijo</surname><given-names>H</given-names></name><name><surname>Ono</surname><given-names>T</given-names></name></person-group><year>2004</year><article-title>Neuronal correlates of face identification in the monkey anterior temporal cortical areas</article-title><source>Journal of Neurophysiology</source><volume>91</volume><fpage>358</fpage><lpage>371</lpage><pub-id pub-id-type="doi">10.1152/jn.00198.2003</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emery</surname><given-names>NJ</given-names></name></person-group><year>2000</year><article-title>The eyes have it: the neuroethology, function and evolution of social gaze</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>24</volume><fpage>581</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.1016/S0149-7634(00)00025-7</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emery</surname><given-names>NJ</given-names></name><name><surname>Lorincz</surname><given-names>EN</given-names></name><name><surname>Perrett</surname><given-names>DI</given-names></name><name><surname>Oram</surname><given-names>MW</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year>1997</year><article-title>Gaze following and joint attention in rhesus monkeys (Macaca mulatta)</article-title><source>Journal of Comparative Psychology</source><volume>111</volume><fpage>286</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1037/0735-7036.111.3.286</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year>1991</year><article-title>Distributed hierarchical processing in primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flombaum</surname><given-names>JI</given-names></name><name><surname>Santos</surname><given-names>LR</given-names></name></person-group><year>2005</year><article-title>Rhesus monkeys attribute perceptions to others</article-title><source>Current Biology</source><volume>15</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.12.076</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year>2010</year><article-title>Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title><source>Science</source><volume>330</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1194908</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year>2009</year><article-title>A face feature space in the macaque temporal lobe</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1187</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1038/nn.2363</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Hauser</surname><given-names>MD</given-names></name></person-group><year>2001</year><article-title>The auditory behavior of primates: a neuroethological perspective</article-title><source>Current Opinion in Neurobiology</source><volume>11</volume><fpage>712</fpage><lpage>720</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(01)00274-4</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname><given-names>ME</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Baylis</surname><given-names>GC</given-names></name></person-group><year>1989</year><article-title>The role of expression and identity in the face-selective responses of neurons in the temporal visual cortex of the monkey</article-title><source>Behavioural Brain Research</source><volume>32</volume><fpage>203</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/S0166-4328(89)80054-3</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffner</surname><given-names>HE</given-names></name><name><surname>Heffner</surname><given-names>RS</given-names></name></person-group><year>1984</year><article-title>Temporal lobe lesions and perception of species-specific vocalizations by macaques</article-title><source>Science</source><volume>226</volume><fpage>75</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1126/science.6474192</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>EA</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year>2000</year><article-title>Distinct representations of eye gaze and identity in the distributed human neural system for face perception</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>80</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/71152</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamphuis</surname><given-names>S</given-names></name><name><surname>Dicke</surname><given-names>PW</given-names></name><name><surname>Thier</surname><given-names>P</given-names></name></person-group><year>2009</year><article-title>Neuronal substrates of gaze following in monkeys</article-title><source>The European Journal of Neuroscience</source><volume>29</volume><fpage>1732</fpage><lpage>1738</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2009.06730.x</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname><given-names>H</given-names></name><name><surname>Kohshima</surname><given-names>S</given-names></name></person-group><year>1997</year><article-title>Unique morphology of the human eye</article-title><source>Nature</source><volume>387</volume><fpage>767</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1038/42842</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ku</surname><given-names>SP</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Goense</surname><given-names>J</given-names></name></person-group><year>2011</year><article-title>FMRI of the face-processing network in the ventral temporal lobe of awake and anesthetized macaques</article-title><source>Neuron</source><volume>70</volume><fpage>352</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.048</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laube</surname><given-names>I</given-names></name><name><surname>Kamphuis</surname><given-names>S</given-names></name><name><surname>Dicke</surname><given-names>PW</given-names></name><name><surname>Thier</surname><given-names>P</given-names></name></person-group><year>2011</year><article-title>Cortical processing of head- and eye-gaze cues guiding joint social attention</article-title><source>NeuroImage</source><volume>54</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.08.074</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leite</surname><given-names>FP</given-names></name><name><surname>Tsao</surname><given-names>D</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Sasaki</surname><given-names>Y</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name><name><surname>Mandeville</surname><given-names>JB</given-names></name></person-group><year>2002</year><article-title>Repeated fMRI using iron oxide contrast agent in awake, behaving macaques at 3 Tesla</article-title><source>NeuroImage</source><volume>16</volume><fpage>283</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1110</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liuzza</surname><given-names>MT</given-names></name><name><surname>Cazzato</surname><given-names>V</given-names></name><name><surname>Vecchione</surname><given-names>M</given-names></name><name><surname>Crostella</surname><given-names>F</given-names></name><name><surname>Caprara</surname><given-names>GV</given-names></name><name><surname>Aglioti</surname><given-names>SM</given-names></name></person-group><year>2011</year><article-title>Follow my eyes: the gaze of politicians reflexively captures the gzae of ingroup voters</article-title><source>PLOS ONE</source><volume>6</volume><fpage>e25117</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0025117</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year>2002</year><article-title>The neural basis of the blood-oxygen-level-dependent functional magnetic resonance imaging signal</article-title><source>Philosophical Transactions of the Royal Society of London Series B, Biological Sciences</source><volume>357</volume><fpage>1003</fpage><lpage>10037</lpage><pub-id pub-id-type="doi">10.1098/rstb.2002.1114</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Pauls</surname><given-names>J</given-names></name><name><surname>Augath</surname><given-names>M</given-names></name><name><surname>Trinath</surname><given-names>T</given-names></name><name><surname>Oeltermann</surname><given-names>A</given-names></name></person-group><year>2001</year><article-title>Neurophysiological investigation of the basis of the fMRI signal</article-title><source>Nature</source><volume>412</volume><fpage>150</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1038/35084005</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorincz</surname><given-names>EN</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Perrett</surname><given-names>DI</given-names></name></person-group><year>1999</year><article-title>Visual cues for attention following in rhesus monkeys</article-title><source>Current Psychology of Cognition</source><volume>18</volume><fpage>973</fpage><lpage>1003</lpage></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Materna</surname><given-names>S</given-names></name><name><surname>Dicke</surname><given-names>PW</given-names></name><name><surname>Thier</surname><given-names>P</given-names></name></person-group><year>2008a</year><article-title>Dissociable roles of the superior temporal sulcus and the intraparietal sulcus in joint attention: a functional magnetic resonance imaging study</article-title><source>Journal of Cognitive Neuroscience</source><volume>20</volume><fpage>108</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1162/jocn.2008.20.1.108</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Materna</surname><given-names>S</given-names></name><name><surname>Dicke</surname><given-names>PW</given-names></name><name><surname>Thier</surname><given-names>P</given-names></name></person-group><year>2008b</year><article-title>The posterior superior temporal sulcus is involved in social communication not specific for the eyes</article-title><source>Neuropsychologia</source><volume>46</volume><fpage>2759</fpage><lpage>2765</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2008.05.016</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year>2008</year><article-title>Patches with links: a unified system for processing faces in the macaque temporal lobe</article-title><source>Science</source><volume>320</volume><fpage>1355</fpage><lpage>1359</lpage><pub-id pub-id-type="doi">10.1126/science.1157436</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year>2013</year><source>The effect of microstimulation of face patches ML and AM on the percept of facial identity in the macaque monkey</source><publisher-loc>San Diego, CA</publisher-loc><publisher-name>Society for Neuroscience</publisher-name><comment>Program No. 738.17. Neuroscience 2013 Abstracts. Online</comment></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelphrey</surname><given-names>KA</given-names></name><name><surname>Morris</surname><given-names>JP</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year>2005</year><article-title>Neural basis of eye gaze processing deficits in autism</article-title><source>Brain</source><volume>128</volume><fpage>1038</fpage><lpage>1048</lpage><pub-id pub-id-type="doi">10.1093/brain/awh404</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelphrey</surname><given-names>KA</given-names></name><name><surname>Singerman</surname><given-names>JD</given-names></name><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year>2003</year><article-title>Brain activation evoked by perception of gaze shifts; the influence of context</article-title><source>Neuropsychologia</source><volume>41</volume><fpage>156</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/S0028-3932(02)00146-X</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelphrey</surname><given-names>KA</given-names></name><name><surname>Viola</surname><given-names>RJ</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year>2004</year><article-title>When strangers pass: processing of mutual and averted gaze in the superior temporal sulcus</article-title><source>Psychological Science</source><volume>15</volume><fpage>598</fpage><lpage>603</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2004.00726.x</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinsk</surname><given-names>MA</given-names></name><name><surname>Desimone</surname><given-names>K</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year>2005</year><article-title>Representations of faces and body parts in macaque temporal cortex: a functional MRI study</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>102</volume><fpage>6996</fpage><lpage>7001</lpage><pub-id pub-id-type="doi">10.1073/pnas.0502605102</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poremba</surname><given-names>A</given-names></name><name><surname>Malloy</surname><given-names>MM</given-names></name><name><surname>Saunders</surname><given-names>RC</given-names></name><name><surname>Carson</surname><given-names>RE</given-names></name><name><surname>Herscovitch</surname><given-names>P</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year>2004</year><article-title>Species-specific calls evoke asymmetric activity in the monkey's temporal poles</article-title><source>Nature</source><volume>427</volume><fpage>448</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1038/nature02268</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>Bentin</surname><given-names>S</given-names></name><name><surname>Gore</surname><given-names>JC</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year>1998</year><article-title>Temporal cortex activation in humans viewing eye and mouth movements</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>2188</fpage><lpage>2199</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricciardelli</surname><given-names>P</given-names></name><name><surname>Ro</surname><given-names>T</given-names></name><name><surname>Driver</surname><given-names>J</given-names></name></person-group><year>2002</year><article-title>A left visual field advantage in perception of gaze direction</article-title><source>Neuropsychologia</source><volume>40</volume><fpage>769</fpage><lpage>777</lpage><pub-id pub-id-type="doi">10.1016/S0028-3932(01)00190-7</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepherd</surname><given-names>SV</given-names></name><name><surname>Deaner</surname><given-names>RO</given-names></name><name><surname>Platt</surname><given-names>ML</given-names></name></person-group><year>2006</year><article-title>Social status gates social attention in monkeys</article-title><source>Current Biology</source><volume>16</volume><fpage>119</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.02.013</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepherd</surname><given-names>SV</given-names></name><name><surname>Klein</surname><given-names>JT</given-names></name><name><surname>Deaner</surname><given-names>RO</given-names></name><name><surname>Platt</surname><given-names>ML</given-names></name></person-group><year>2009</year><article-title>Mirroring of attention by neurons in macaque parietal cortex</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>106</volume><fpage>9489</fpage><lpage>9494</lpage><pub-id pub-id-type="doi">10.1073/pnas.0900419106</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Knutsen</surname><given-names>TA</given-names></name><name><surname>Mandeville</surname><given-names>JB</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><year>2003</year><article-title>The representation of faces and objects in macaque cerebral cortex</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>989</fpage><lpage>995</lpage><pub-id pub-id-type="doi">10.1038/nn1111</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Freiwald</surname><given-names>W</given-names></name></person-group><year>2008</year><article-title>Comparing face patch systems in macaques and humans</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>49</volume><fpage>19514</fpage><lpage>19519</lpage><pub-id pub-id-type="doi">10.1073/pnas.0809662105</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year>1986</year><article-title>Cortical connections of visual area MT in the macaque</article-title><source>The Journal of Comparative Neurology</source><volume>248</volume><fpage>190</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1002/cne.902480204</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.03222.013</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Romo</surname><given-names>Ranulfo</given-names></name><role>Reviewing editor</role><aff><institution>Universidad Nacional Autonoma de Mexico</institution>, <country>Mexico</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for sending your work entitled “Disparate substrates for head gaze following and face perception in the monkey superior temporal sulcus” for consideration at <italic>eLife.</italic> Your article has been favorably evaluated by a Senior editor and 2 reviewers, one of whom is a member of our Board of Reviewing Editors.</p><p>The Reviewing editor and the other reviewers discussed their comments before we reached this decision, and the Reviewing editor has assembled the following comments to help you prepare a revised submission.</p><p>This study presents interesting fMRI data on the cerebral cortex region associated with the active head-gaze following in the behaving monkey. The main result is the finding of an activated region after subtraction of two tasks (active head-gaze-following and identity-matching) in the posterior STS that does not overlap the activated region of passive face recognition region. The results appear to clarify the neural mechanisms of gaze-following behavior. There are some points that the authors must address in a revised version of the manuscript:</p><p>1) The authors conclude that the gaze-following patch exists outside the passive face patch and that it could not be elicited directly. The authors should discuss this point. Discrepancy between the two monkeys that pSTS unilateral activity in one animal and bilateral activities in the other monkey could be argued from this point of view.</p><p>2) Previous studies done by the same group (<xref ref-type="bibr" rid="bib26">Kamphusis et al. 2009</xref>) demonstrated the active gaze-following exists in the middle STS but not in pSTS. The authors should discuss about differences between the two studies and should mention the study of Kamphsuis in the Discussion section. An obvious question is why the authors did not mention this previous study.</p><p>3) What cue the monkey used to choose one target out of 4 in the gaze-following test is a very difficult question. The study showed that the monkey used the gaze-direction, the face single angle of the monkey's photos as the cues. However, one problem to be solved still remains, the possibility that the monkey simply learned the association between the face angle and the order of the target from the face photo. The authors should discuss this possibility.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.03222.014</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>1) The authors conclude that the gaze-following patch exists outside the passive face patch and that it could not be elicited directly. The authors should discuss this point. Discrepancy between the two monkeys that pSTS unilateral activity in one animal and bilateral activities in the other monkey could be argued from this point of view</italic>.</p><p>As suggested, we added that the unilaterality of the gaze following-related activity in one of the monkeys vis-à-vis largely bilateral face-evoked activity is yet another finding supporting the idea that the gaze following patch is anatomically distinct from the face patches.</p><p><italic>2) Previous studies done by the same group (</italic><xref ref-type="bibr" rid="bib26"><italic>Kamphusis et al. 2009</italic></xref><italic>) demonstrated the active gaze-following exists in the middle STS but not in pSTS. The authors should discuss about differences between the two studies and should mention the study of Kamphsuis in the Discussion section. An obvious question is why the authors did not mention this previous study</italic>.</p><p>Actually, the Kamphuis et al study was mentioned in the previous version. However, we agree that the findings should have been compared in more detail. This deficiency is ironed out in the revised manuscript: the discussion now offers a paragraph on the comparison.</p><p><italic>3) What cue the monkey used to choose one target out of 4 in the gaze-following test is a very difficult question. The study showed that the monkey used the gaze-direction, the face single angle of the monkey's photos as the cues. However, one problem to be solved still remains, the possibility that the monkey simply learned the association between the face angle and the order of the target from the face photo. The authors should discuss this possibility</italic>.</p><p>This is an important concern, which we tried to address with the behavioral control experiments. The results obtained clearly rule out that our experimental animals had relied on learned associations. In the revised manuscript this important result receives additional attention by having added a paragraph in the Discussion.</p></body></sub-article></article>