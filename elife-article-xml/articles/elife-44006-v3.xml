<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">44006</article-id><article-id pub-id-type="doi">10.7554/eLife.44006</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Context-dependent signaling of coincident auditory and visual events in primary visual cortex</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-127914"><name><surname>Deneux</surname><given-names>Thomas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9330-7655</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-127916"><name><surname>Harrell</surname><given-names>Evan R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4376-9405</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-127915"><name><surname>Kempf</surname><given-names>Alexandre</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140827"><name><surname>Ceballo</surname><given-names>Sebastian</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140826"><name><surname>Filipchuk</surname><given-names>Anton</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-141232"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9211-1960</contrib-id><email>brice.bathellier@cnrs.fr</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department for Integrative and Computational Neuroscience (ICN)</institution><institution>Paris-Saclay Institute of Neuroscience (NeuroPSI), UMR9197 CNRS, University Paris Sud</institution><addr-line><named-content content-type="city">Gif-sur-Yvette</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Svoboda</surname><given-names>Karel</given-names></name><role>Reviewing Editor</role><aff><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>23</day><month>05</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e44006</elocation-id><history><date date-type="received" iso-8601-date="2018-11-29"><day>29</day><month>11</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-05-20"><day>20</day><month>05</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Deneux et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Deneux et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-44006-v3.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.44006.001</object-id><p>Detecting rapid, coincident changes across sensory modalities is essential for recognition of sudden threats or events. Using two-photon calcium imaging in identified cell types in awake, head-fixed mice, we show that, among the basic features of a sound envelope, loud sound onsets are a dominant feature coded by the auditory cortex neurons projecting to primary visual cortex (V1). In V1, a small number of layer 1 interneurons gates this cross-modal information flow in a context-dependent manner. In dark conditions, auditory cortex inputs lead to suppression of the V1 population. However, when sound input coincides with a visual stimulus, visual responses are boosted in V1, most strongly after loud sound onsets. Thus, a dynamic, asymmetric circuit connecting AC and V1 contributes to the encoding of visual events that are coincident with sounds.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual cortex</kwd><kwd>multisensory processing</kwd><kwd>calcium imaging</kwd><kwd>auditory cortex</kwd><kwd>cell types</kwd><kwd>chemogenetics</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>Retour Postdoc</award-id><principal-award-recipient><name><surname>Thomas</surname><given-names>Deneux</given-names></name><name><surname>Bathellier</surname><given-names>Brice</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>CDA</award-id><principal-award-recipient><name><surname>Harrell</surname><given-names>Evan R</given-names></name><name><surname>Bathellier</surname><given-names>Brice</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>ERC CoG</award-id><principal-award-recipient><name><surname>Kempf</surname><given-names>Alexandre</given-names></name><name><surname>Ceballo</surname><given-names>Sebastian</given-names></name><name><surname>Filipchuk</surname><given-names>Anton</given-names></name><name><surname>Bathellier</surname><given-names>Brice</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004963</institution-id><institution>Seventh Framework Programme</institution></institution-wrap></funding-source><award-id>Marie Curie CiG</award-id><principal-award-recipient><name><surname>Bathellier</surname><given-names>Brice</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Fondation pour l'Audition</institution></institution-wrap></funding-source><award-id>Lab grant</award-id><principal-award-recipient><name><surname>Harrell</surname><given-names>Evan R</given-names></name><name><surname>Bathellier</surname><given-names>Brice</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>École Doctorale Frontières du Vivant (FdV) - Programme Bettencourt</institution></institution-wrap></funding-source><award-id>Phd fellowship</award-id><principal-award-recipient><name><surname>Kempf</surname><given-names>Alexandre</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>Paris-Saclay University</institution></institution-wrap></funding-source><award-id>NeuroSaclay Brainscopes</award-id><principal-award-recipient><name><surname>Bathellier</surname><given-names>Brice</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution>Ecole des Neurosciences de Paris</institution></institution-wrap></funding-source><award-id>Phd fellowship</award-id><principal-award-recipient><name><surname>Ceballo</surname><given-names>Sebastian</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Abrupt sounds boost coincident visual cortex responses which could help localization of sudden audio-visual events.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Numerous multisensory illusions (<xref ref-type="bibr" rid="bib3">Bonath et al., 2007</xref>; <xref ref-type="bibr" rid="bib30">Maeda et al., 2004</xref>; <xref ref-type="bibr" rid="bib43">Sekuler et al., 1997</xref>; <xref ref-type="bibr" rid="bib44">Shams et al., 2000</xref>) show that audition and vision have strong perceptual bonds. For example, in the double flash illusion (<xref ref-type="bibr" rid="bib44">Shams et al., 2000</xref>) a brief sequence of two sounds played during a single visual flash leads to the perception of two flashes. While the mechanisms of cross-modal perceptual interactions remain unclear, the anatomy of the mammalian brain shows multiple sites of auditory-visual convergence. One of the best studied integrative regions is the superior colliculus, where visual and auditory cues are combined in the networks computing gaze direction (<xref ref-type="bibr" rid="bib47">Stein and Stanford, 2008</xref>; <xref ref-type="bibr" rid="bib50">Witten and Knudsen, 2005</xref>). In cortex, associative areas such as those in the parietal cortex (<xref ref-type="bibr" rid="bib37">Raposo et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Song et al., 2017</xref>) are not the only cortical sites demonstrating auditory-visual convergence: increasing evidence shows that functional auditory-visual interactions exist already in primary sensory cortex (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Iurilli et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Kayser et al., 2010</xref>; <xref ref-type="bibr" rid="bib21">Kayser et al., 2008</xref>; <xref ref-type="bibr" rid="bib32">Meijer et al., 2017</xref>). Moreover, axonal tracing studies indicate that numerous direct connections exist between primary auditory and visual cortex (<xref ref-type="bibr" rid="bib2">Bizley et al., 2007</xref>; <xref ref-type="bibr" rid="bib8">Driver and Noesselt, 2008</xref>; <xref ref-type="bibr" rid="bib9">Falchier et al., 2002</xref>; <xref ref-type="bibr" rid="bib12">Ghazanfar and Schroeder, 2006</xref>; <xref ref-type="bibr" rid="bib17">Innocenti et al., 1988</xref>; <xref ref-type="bibr" rid="bib26">Leinweber et al., 2017</xref>; <xref ref-type="bibr" rid="bib52">Zingg et al., 2014</xref>), suggesting that auditory-visual cross-talk is present before the associative stage. The existence of these direct connections in mice has initiated some experimental inquiry into their potential role. Recent studies indicate that auditory to visual connections are much stronger than their reciprocals, and that they provide inputs to visual cortex that can modulate visually driven activity (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Iurilli et al., 2012</xref>). Nevertheless, the computational role of primary auditory-visual connections remains unclear. First and foremost, information is lacking about the auditory features channeled from auditory to visual cortex and how they are integrated into the visual processing stream.</p><p>Mouse auditory cortex encodes a wide variety of acoustic features (<xref ref-type="bibr" rid="bib33">Mizrahi et al., 2014</xref>). Spectral content (<xref ref-type="bibr" rid="bib18">Issa et al., 2014</xref>; <xref ref-type="bibr" rid="bib20">Kanold et al., 2014</xref>) and temporal features such as modulations of frequency (<xref ref-type="bibr" rid="bib48">Trujillo et al., 2011</xref>) or intensity (<xref ref-type="bibr" rid="bib11">Gao and Wehr, 2015</xref>) are the most well-studied, and intensity variations occurring at sound onsets and offsets (<xref ref-type="bibr" rid="bib7">Deneux et al., 2016b</xref>; <xref ref-type="bibr" rid="bib40">Scholl et al., 2010</xref>) are particularly salient auditory features. The question of how sound frequency information would map onto visual cortex is a difficult one, because of the lack of perceptual and ethological data on the particular frequency cues that could potentially be associated with particular visual stimuli in mice. In contrast, temporal coincidence is known to be used for perceptually assigning auditory and visual stimuli to the same object and is implicated in the double flash (<xref ref-type="bibr" rid="bib45">Slutsky and Recanzone, 2001</xref>) and ventriloquist illusions (<xref ref-type="bibr" rid="bib3">Bonath et al., 2007</xref>; <xref ref-type="bibr" rid="bib38">Recanzone, 2009</xref>). Detection of temporal coincidence involves determining when sounds begin and end, and therefore might implicate neurons that encode particular intensity envelope features such as onsets and offsets. Also, covariations of the size of a visual input and sound intensity envelope are important for binding looming and receding auditory-visual stimuli (<xref ref-type="bibr" rid="bib49">Tyll et al., 2013</xref>). This suggests that there could be preferential cross-talk between some intensity envelope features and visual information. We have recently demonstrated that envelope features such as onsets, offsets and sustained temporal dynamics are encoded in separate cells with further selectivity for different sound amplitudes. Some neurons respond only to high amplitude, ‘loud’ sound onsets, whereas others only respond to low amplitude, ‘quiet’ onsets, and neurons responding to offsets and sustained phases are also tuned to precise intensity ranges (<xref ref-type="bibr" rid="bib7">Deneux et al., 2016b</xref>). Some neurons also encode combinations of these features. Such a coding scheme possibly allows constructing finer sound categories or interpretations. For example, mechanical shocks produce sounds that rise abruptly in intensity and tend to activate loud onset neurons, while more continuous events produce sounds that progressively ramp up in intensity and first activate quiet onset neurons. This complexity opens the possibility that some envelope features are more relevant to complement visual information related to external events. However, the precise intensity envelope features that characterize sound onsets and offsets which are transmitted through the direct connection between AC and V1 and how they impact visual processing remain unknown.</p><p>To address this question, we used two-photon calcium imaging and intersectional genetics to identify some of the intensity envelope features encoded by AC neurons that project to V1, and showed that V1-projecting neurons are predominantly tuned to loud onsets while other tested envelope features are less prominently represented, at least when comparing with supragranular cortical layers that lack V1-projecting cells. We also show that this cross-modal information impacts V1 in a context-dependent manner, with a net inhibitory effect in darkness and a net positive effect in the light, probably mediated by a subpopulation of L1 inhibitory neurons. Furthermore, we show that this mechanism also allows for a boosting of visual responses that occurs together with auditory events, particularly when those include loud sound onsets, increasing the saliency of visual events that co-occur with abrupt sounds.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Abrupt sound onsets are a dominant feature encoded by V1-projecting neurons</title><p>To evaluate if particular on- and offset features are preferentially channeled from AC to V1, we compared the feature encoding in the overall population of AC neurons and in AC neurons projecting to the primary visual cortex. To identify these V1-projecting cells, we injected a canine adenovirus (CAV) expressing Cre, which is retrogradely transported through axons, into V1. In the same mice, we injected into AC an adeno-associated virus (AAV) expressing GCAMP6s (<xref ref-type="bibr" rid="bib5">Chen et al., 2013b</xref>) in a Cre-dependent manner (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). As a result, GCAMP6s expression was obtained exclusively in AC neurons that project to V1. Consistent with previous reports, we observed that these neurons were located predominantly in layer 5 (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>). As a comparison, in another set of mice, we injected an AAV1-syn-GCAMP6s virus to express the calcium reporter in AC neurons, independent of cell type. However, we selected cells according to their vertical localization: upper layer 5 (370–430 µm, corresponding to the depth at which V1-projecting cells were imaged) and layer 2/3 (150–250 µm depth). Using two-photon microscopy, we imaged calcium responses at single cell resolution in these three sets of mice (layer 2/3: 4616 neurons, 29 sessions, 11 mice; upper layer 5: 7757 neurons, 12 sessions, four mice; V1-projecting: 2474 neurons, 15 sessions, three mice) while playing white noise sounds ramping up or down in intensity (range 50 to 85 dB SPL).</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.44006.002</object-id><label>Figure 1.</label><caption><title>Loud onset responses dominate in V1-projecting neurons.</title><p>(<bold>A</bold>) Viral expression strategy for GCAMP6s labeling of V1-projecting neurons in AC. (<bold>B</bold>) Epifluorescence image of an AC histological section (blue = DAPI, green = GCAMP6 s) showing V1-projecting neurons (cortical layers are matched to A). (<bold>C</bold>) Matrix of sound response correlation for all cells that could be assigned to a cluster for auditory cortex neurons that project to V1. The average response traces of each cluster (mean deconvolved calcium signal across cells) are shown on the right with a color code corresponding to the functional label given to the cluster. The bottom trace (black) shows the average response for the cells that were not assigned to a cluster (non- or weakly-responsive cells). (<bold>D</bold>) Top: Example of a 1 × 0.8 mm imaging field of view showing GCAMP6s-expressing neurons in upper layer 5 of auditory cortex (depth: 400 µm) with a magnification in the top-right inset. Bottom: Example raw calcium traces for four neurons during sound presentation. Red, black and gray traces are different samples for the same neuron. Scale bars 50% ΔF/F. Right: Average response traces of each cluster obtained with neurons recorded in upper layer 5 (370 to 430 µm depth). The color code corresponds to the functional label used in C. The black trace stands for non- or weakly-responsive cells. (<bold>E</bold>) Average response traces of each cluster obtained with neurons recorded in layer 2/3. (<bold>F</bold>) Fraction of neurons corresponding to each cluster shown in C-E. The functional cell class distributions are significantly different across the three anatomical cell types (L5, L2/3 or V1-projecting) as assessed together and pairwise across cell type using the χ<sup>2</sup> test of independence (p&lt;10<sup>−64</sup>). The fraction of loud onset cells is significantly different (χ<sup>2</sup> test of independence, performed pairwise across anatomical cell types p&lt;10<sup>−9</sup>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig1-v3.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44006.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Correlation matrices for all clustered neurons.</title><p>Matrix of sound response correlation for all cells that could be assigned to a cluster, (<bold>A</bold>) for auditory cortex neurons that project to V1 (same as <xref ref-type="fig" rid="fig1">Figure 1C</xref>), (<bold>B</bold>) for auditory cortex neurons in upper layer 5 (370 to 430 µm depth), (<bold>C</bold>) for auditory cortex neurons in layer 2/3 (150 to 250 µm depth). The average response trace of each cluster (mean deconvolved calcium signal across cells) is shown on the right with a color code corresponding to the functional label given to the cluster. The bottom trace (black) shows the average response for the cells that were not assigned to a cluster (non- or weakly-responsive cells).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig1-figsupp1-v3.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44006.004</object-id><label>Figure 1—figure supplement 2.</label><caption><title>Layer 5 responses to up- and down-ramps recorded using electrophysiology.</title><p>(<bold>A</bold>) Sketch of optical imaging of intrinsic signals experimental setup. (<bold>B</bold>) Localization of AC by intrinsic imaging. <italic>Left</italic>, 5 mm cranial window on the right hemisphere of an anesthetized mouse under green LED light. <italic>Middle and Right</italic>, Mean intrinsic signal response map and mean + single trial response traces for White Noise stimulation (WN) from the mouse shown on the left. Responses traces are derived from the region of interest delineated on the left image. (<bold>C</bold>) Sketch of silicon probe-based electrophysiology experiments in awake mice. (<bold>D</bold>) <italic>Left</italic>: Cranial window of the same animal as in B, after placing the probe into AC. Recordings were usually performed at 400, 600 700 and 800 µm in a single penetration for each animal. <italic>Middle</italic>: A raster plot and a peristimulus histogram of the responses to up- and down-ramp presentations for a sample single unit. <italic>Right</italic>: Spike waveform and autocorrelogram of spike train for the same unit. (<bold>E</bold>) Correlation-based, template matching method to detect the six main response types observed in two-photon calcium imaging (onset, offset, tonic response with a quiet or loud intensity tuning). Because of the small number of cells, only these prominent, simple response types were selected. They capture also some of the composite response types (ON, Loud Tonic + OFF) which are more difficult to capture using a binary model. For each unit (n = 410 isolated units in 4 mice and 15 recording locations), the correlation between its average response histogram and the six templates (represented by dashed lines) was computed. Each cell was assigned to the template to which its response was most correlated provided that the correlation was above 0.4 (112 cells passed this threshold). (<bold>F</bold>) Mean response profiles smoothed with a Gaussian filter, σ = 30 ms) for the different groups of cells identified by template matching. On the right, the heat map depicts the mean response of each unit to up- (left) and down- (right) ramps. Black and white triangles correspond to sound onsets and offsets respectively. Note that a small fraction of neurons classified as Loud or Quiet ON are equally responsive for both types of onset, corresponding to the ON cluster label defined for two-photon calcium imaging data. (<bold>G</bold>) Histogram of the fractions of neurons that were assigned to each of the six templates shown together with the number of neurons in each group. The plot shows that Loud ON responses types are dominant in layer 5, as seen with two-photon calcium imaging (<xref ref-type="fig" rid="fig1">Figure 1F</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig1-figsupp2-v3.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44006.005</object-id><label>Figure 1—figure supplement 3.</label><caption><title>AC neurons projecting to V1 are tuned both to intensity and frequency.</title><p>(<sc><bold>A</bold></sc>) Response profiles to up- and down-ramping white noise sounds (ramping range 60–85 dB SPL) and to 2s-long constant white noise stimuli at 50, 60 70 and 80 dB SPL, for 11 functional clusters of V1-projecting neurons labeled with different colors. Cell count per cluster is shown on the right. Some clusters respond to the same features and are labeled with similar colors (blue colors for Loud onsets, red colors for quiet onsets). The cluster labeled in black corresponds to all neurons that show no response to the stimuli. These clusters are derived from a dataset of 484 neurons, imaged in six sessions in two mice (different from the dataset shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>). Gray shading: up or down ramping sound presentation. The cluster responding only to loud or only to quiet onsets or offsets in ramped stimuli display amplitude tuning with the constant stimuli. (<bold>B</bold>) Response profiles to constant 2s-long pure tones (4, 6, 8, 10, 15, 24 kHz) at 60 (left) and 80 dB SPL (right) for 10 functional clusters of V1-projecting neurons labeled with different colors. Cell count per cluster is shown on the right. We represented clusters responding to the same intensity range and sound epoch (onset or offset) with similar colors (blue colors for Loud onsets, red colors for quiet onsets). The cluster labeled in black corresponds to all neurons that show no response to the stimuli. These clusters are derived from a dataset of 412 neurons imaged in 6 sessions in two mice. This data shows that neurons responding to a particular sound epoch and intensity range can be themselves tuned to different frequencies.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig1-figsupp3-v3.tif"/></fig></fig-group><p>After automated ROI extraction (<xref ref-type="bibr" rid="bib39">Roland et al., 2017</xref>), calcium signals were deconvolved (<xref ref-type="bibr" rid="bib1">Bathellier et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">Yaksi and Friedrich, 2006</xref>) to obtain a temporally more accurate estimate of actual neuronal firing rates by reverse-filtering the slow decay dynamics of calcium signals and smoothed with a sliding window filter (190 ms) to reduce fast noise. The resulting averaged response profiles were then submitted to a hierarchical clustering (<xref ref-type="bibr" rid="bib7">Deneux et al., 2016b</xref>) (see Materials and methods) to group together cells with similar response profiles and thereby identify different functional response types. In order to better appreciate the variety of responses, we over-clustered the data, based on the same homogeneity threshold for all datasets, so that most response types are represented by multiple clusters (<xref ref-type="fig" rid="fig1">Figure 1C–E</xref>). The clustering also identified a number of weakly or non-responsive cells that were not included in the analysis. We obtained 19, 31 and 21 functional clusters explaining 52.3 ± 7.2%, 51.9 ± 6.4% and 51.3 ± 6.5% of the total variance (mean ±STD) for V1-projecting, layer 5 and layer 2/3 cells, respectively. These high fractions of explained variance and the strong homogeneity of the clusters as seen in response correlation matrices (<xref ref-type="fig" rid="fig1">Figure 1C</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) indicate that the clustering provides an accurate dimensionality reduction of the dataset. Within these clusters, we identified a maximum of 9 response types (<xref ref-type="fig" rid="fig1">Figure 1C–E</xref>), including onset (ON), offset (OFF) and sustained (Tonic) responses with loud or quiet intensity tuning, and also clusters combining loud OFF and sustained responses (labeled as ‘loud OFF +tonic’) or quiet and loud ON responses (labeled as ‘ON’), as described previously (<xref ref-type="bibr" rid="bib7">Deneux et al., 2016b</xref>). Note that the terms ‘loud’ and ‘quiet’ are used here in their relative sense and correspond to high (~85 dB) and lower (~50 dB) sound pressure levels respectively, within the range used in this study. As a validation of the approach, many of these response types could also be observed in layer 5 targeted extracellular recordings in roughly the same proportion as in two-photon calcium imaging (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Interestingly, the distribution of these response types was different across layer 2/3, layer 5 and V1-projecting populations (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). While some slight differences could be seen for almost all response types, the largest discrepancies were observed for neurons signaling loud onsets which represented 48% of the clustered neurons in V1-projecting cells, and only 36% and 21% in layer 5 and layer 2/3, respectively (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). Thus, loud onset responses were enriched in layer 5 which contains most of the cells with direct projections to V1 and predominant in cells retrogradely labeled as V1-projecting neurons, indicating that AC is organized to favor the transfer of this particular sound envelope information even if other envelope features are also conveyed by V1-projecting cells (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), as well as some sound frequency information (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>).</p></sec><sec id="s2-2"><title>The sign of auditory responses in V1 depends on the illumination context</title><p>How do these structured AC inputs impact V1? One study in which mice received no visual input (dark environment) suggested an inhibitory effect (<xref ref-type="bibr" rid="bib19">Iurilli et al., 2012</xref>) while another study in which mice received visual stimuli described excitatory effects (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>). Thus, the sign of AC inputs to V1 pyramidal cells could be context-dependent. To address this question, we performed two-photon calcium imaging in head-fixed awake mice (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) using the calcium sensor GCAMP6s (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) expressed in V1 through stereotactic injection of an AAV-syn-GCAMP6s viral vector. The recordings were done either in complete darkness or in front of a gray screen at low luminance (0.57 cd/m<sup>2</sup>; when screen is on, the diffuse light allows the mouse to see its immediate surroundings). To monitor gaze stability, pupil position and diameter were tracked during the experiment (<xref ref-type="fig" rid="fig2">Figure 2C,D</xref>). V1 was identified using Fourier intrinsic imaging (<xref ref-type="bibr" rid="bib31">Marshel et al., 2011</xref>) as the largest retinotopic field in the visual areas (<xref ref-type="fig" rid="fig2">Figure 2E,F</xref>), and all two-photon imaging fields-of-view were mapped to the retinotopic field using blood vessel landmarks (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). All neurons imaged outside V1 (<xref ref-type="fig" rid="fig2">Figure 2E,F</xref>) were analyzed separately.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.44006.006</object-id><label>Figure 2.</label><caption><title>Retinotopically mapped two-photon imaging fields in V1.</title><p>(<bold>A</bold>) Sketch of the experimental setup. (<bold>B</bold>) (top) Example of a 0.5 × 0.5 imaging field of view showing GCAMP6s-expressing neurons in V1 (top with magnification in inset) and the ROIs that were automatically detected as putative neurons (bottom, see Materials and methods). (<bold>C</bold>) Examples of eye tracking images. (<bold>D</bold>) (top) Eye tracking showing a large saccade during a blank trial. The blue, purple, and yellow-red traces indicate apertures between eye lids, pupil diameter and x-y motion of pupil center. (bottom) Examples of raw GCAMP6s traces from individual neurons recorded concomitantly in V1 (each color is a single neuron, frame rate: 31 Hz) and population average (gray) showing saccade-related neuronal activity. (<bold>E</bold>) Example of a retinotopic map obtained with Fourier intrinsic imaging (see Materials and methods) and segregation between neuron locations inside (white) and outside (black) V1. The color code indicates the azimuth in the visual field. (<bold>F</bold>) Registered maps across nine mice.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig2-v3.tif"/></fig><p>As visual search behavior can be influenced by auditory stimuli (<xref ref-type="bibr" rid="bib47">Stein and Stanford, 2008</xref>), we first measured whether sounds motivated eye-movements. We found that sounds occasionally triggered predominantly horizontal saccadic eye movements, especially sounds with loud onsets (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Sound-induced changes in pupil diameter were also observed (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). In the lit condition, the saccades triggered responses in V1 (e.g. <xref ref-type="fig" rid="fig2">Figure 2D</xref>). Therefore, we excluded all trials with a saccade larger than mouse visual acuity (2° of visual angle) from our analyses (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). After this correction, we pooled together the activity of 18925 V1 neurons (35 sessions, nine mice) and observed that sounds alone trigger responses in supra-granular V1 neurons. The responses were stronger for the down-ramping sounds, similar to what we saw in the activity of V1-projecting AC neurons (<xref ref-type="fig" rid="fig3">Figure 3B,C</xref>). Strikingly however, we observed that the net population response was globally excitatory when mice were in the light and inhibitory when mice were in complete darkness (<xref ref-type="fig" rid="fig3">Figure 3B,C</xref>). The observed inhibition was seen consistently across cells and sound presentations, likely corresponding to a transient decrease in basal firing rates (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Interestingly, covering the contralateral eye was sufficient to obtain the same inhibition as in darkness (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>), suggesting a possible role of direct unilateral visual inputs in the context dependence of auditory responses in V1. As a sanity check, we verified that unfiltered micro-saccades did not cause the excitatory effects, and indeed the amplitude of these small saccades was not correlated with sound responses (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Also, excitatory responses to sounds in the light cannot be attributed to sound-induced pupil dilation as these movements occurred after V1 responses (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Taken together, our observations corroborate the idea that the sign of V1 auditory responses depends on the visual context (darkness vs dimly lit visual scene).</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.44006.007</object-id><label>Figure 3.</label><caption><title>Auditory responses in V1 switch sign in the presence of visual inputs.</title><p>(<bold>A</bold>) (left) Horizontal and vertical pupil movements recorded during three presentations of the down-ramp (gray shading). (center and right) Distribution of saccades across all mice (n = 7) and trials. The blue portion indicates trials where pupil movement did not exceed visual acuity in the mouse (2° of visual angle: 84% of all trials). The trials with larger eye movements (red) were removed from the analysis (trial filtering). Inset: average saccade responses to up-ramps and down-ramps, before (orange) and after (gray) trial filtering. (<bold>B</bold>) Averaged deconvolved calcium traces of V1 neurons in the light (light green) and in the dark (dark green) (6207 neurons, n = 17 sessions in seven mice). The purple line is the average pupil diameter. Left: Down ramp responses, Right: Up ramp responses. Up- and down ramping gray shadings indicate the timing of up- and down ramping sounds. (<bold>C</bold>) Average V1 responses are larger in the light than in the dark (n = 17 recording sessions in seven mice, Wilcoxon signed rank test). (<bold>D</bold>) Smoothed maps (Gaussian filter, σ = 320 µm) of the local responses to the down-ramping sound, averaged across sessions and animals after registration with respect to the retinotopic map. (<bold>E</bold>) Single trial saccade amplitudes (below the 2° visual acuity threshold) do not correlate with the amplitude of V1 population responses to sounds (Pearson correlation coefficient 0.05, p=0.42). (<bold>F</bold>) (top) Mean deconvolved signal for 2474 V1 projecting cells recorded in AC in light and in dark conditions. (bottom) Average onset response (0 to 500 ms) to up- and down-ramps for each recording session, showing no significant difference between light and dark conditions (Wilcoxon sign test, p=0.6 and p=1, n = 15 sessions).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig3-v3.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44006.008</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Robust inhibition in response to sounds in V1.</title><p>(<bold>A</bold>) Mean V1 population responses to a down-ramping sound in lit and dark conditions measured without neuropil subtraction (top), or with the conventional neuropil subtraction (factor 0.7 applied to neuropil signal, middle) or with a larger correction (factor 1 applied to neuropil signal, bottom). In all cases, a clear decrease in fluorescence is observed in the dark. (<bold>B</bold>) Heat map showing the deconvolved and raw calcium signal responses to the down-ramp for all recorded V1 neurons. Inhibition is observed robustly across cells. (<bold>C</bold>) Single-trial, raw calcium signal responses to a down-ramp (left) and spontaneous activity (right) for a cell inhibited (top) and another excited by the sound (bottom). The inhibition corresponds to both a decrease in fluorescence and a decrease in the occurrence of spontaneous positive calcium events. (<bold>D</bold>) Simulations of GCAMP6s ΔF/F signal for Poisson spike trains at 1 Hz and 2 Hz (see on top raster plots for 40 instantiations), with a 1 s period where all spikes are removed (inhibition). In the simulation, each spike produced a transient of peak amplitude ΔF/F = 10% and modeled as a difference of exponentials with rise and decay time constants of 70 ms and 1870 ms respectively. All parameters are derived from <xref ref-type="bibr" rid="bib6">Deneux et al. (2016a)</xref>. All transients were summed (convolution of the spike train with the single spike kernel) and Gaussian white noise (5% standard deviation) was added to produce the simulated calcium traces. The plots, for the 40 spike trains shown on the top, show that a 1 s firing pause during a low Poisson firing rate yielded trial-averaged (black trace) and single-trial traces highly compatible with inhibition traces observed in vivo, for example in C. Thus, inhibitory responses observed in calcium signals from V1 neurons could result from the downward modulation of a low, basal firing rate (of the order of 1 Hz) in each cell commonly observed in awake cortex (<xref ref-type="bibr" rid="bib15">Hromádka et al., 2008</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig3-figsupp1-v3.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44006.009</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Mean V1 responses (2226 neurons, n = 13 sessions in two additional mice) to a down-ramping sound (gray shading) in the dark, in the light and with the contralateral eye reversibly occluded as depicted in the sketch on the left.</title><p>This suggests that the absence of visual inputs to the visual hemifield corresponding to the imaged V1 area is sufficient to obtain sound-induced inhibition.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig3-figsupp2-v3.tif"/></fig></fig-group><p>To test whether this context-dependence was a general property of auditory inputs to non-auditory cortical areas or if it was specific to auditory inputs impacting V1, we measured auditory responses in the secondary visual or associative areas next to V1 which also receive inputs from AC (<xref ref-type="bibr" rid="bib52">Zingg et al., 2014</xref>). Unlike V1, these areas displayed excitatory responses in both the lit and dark conditions (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). We then wondered whether the context-dependence was a property arising in the circuit of V1 or if it is due to a modulation of auditory inputs by the light context. We imaged sound responses in V1-projecting neurons of auditory cortex specifically labeled using the CAV-Cre retrograde virus approach (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). We observed no response modulation between the dark and lit conditions (<xref ref-type="fig" rid="fig3">Figure 3F</xref>), indicating that context-dependent modulation arises in V1.</p><p>Finally, we verified that, as suggested by previous studies (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Iurilli et al., 2012</xref>), context-dependent auditory inputs to V1 are mediated by direct AC to V1 projections. Injections of the GABA-receptor agonist muscimol into AC during imaging almost completely abolished auditory responses in V1 and in the neighboring areas (<xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). We also used targeted chemogenetic inhibition (systemic CNO injections) of V1-projecting neurons in AC, expressing the hM4Di channel by co-injection of a CAV-Cre virus in V1 and an AAV8-flex-hM4di in AC (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This produced a decrease of sound responses which was consistent across sound repetitions for the imaged neural population (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). However, probably because this strategy impacts an incomplete subset of all V1-projecting neurons, the effects were smaller and more variable across experiments (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Taking these results together, we conclude that V1 implements a mechanism that inverts the impact of AC inputs depending on whether or not visual information is available.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.44006.010</object-id><label>Figure 4.</label><caption><title>Auditory responses in V1 are caused by direct AC projections.</title><p>(<bold>A</bold>) Schematics of the AC inactivation experiment (blue) and of the chemogenetic inactivation experiment in which V1-projecting neurons in AC are specifically silenced (magenta). (<bold>B</bold>) (top) Mean deconvolved calcium signal of V1 neurons in dark (dark green) or in light (light green) during saline injections (n = 765 neurons in four experiments and three mice) or muscimol inactivation of AC (n = 825 neurons in five experiments, same three mice). (bottom) Same as top graphs but for chemogenetic inactivation of V1-projecting neurons in AC (same n = 743 neurons before and after CNO activation, in three experiments and two mice). Shading indicates SEM. (<bold>C</bold>) Mean deconvolved calcium signal from the population responses shown in B for the saline (green) versus muscimol (blue) conditions and for Control (green) versus CNO (magenta) conditions (computed from 200 to 500 ms after sound onset for the negative responses in the dark, and from 200 to 1000 ms for the positive response in the light). The p-values are derived from a two-sided Wilcoxon sign test (n = 40 stimulus repetitions).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig4-v3.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44006.011</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Distribution of AC inactivation effects across experiments for the data shown in <xref ref-type="fig" rid="fig4">Figure 4B and C</xref>.</title><p>p-values indicate the result of a one-sided Wilcoxon signed-rank test for chemogenetic (n = 3, magenta) and muscimol (n = 5, blue) experiments.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig4-figsupp1-v3.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Layer 1 contains an interneuron sub-population delivering context-dependent inhibition</title><p>To investigate the circuit mechanisms of this context-dependence, we reasoned that the occurrence of context-dependent inhibition in L2/3 neurons could result from a cortical inhibitory population that receives auditory inputs but is itself inhibited in light condition. To identify such a population, we used GAD2-Cre x flex-TdTomato mice injected with AAV-syn-GCAMP6s to image identified excitatory and inhibitory neurons (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) in L1 and L2/3 where AC to V1 projections are concentrated (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>). We observed that in layer 2/3 both excitatory (n = 4348) and inhibitory neurons (n = 726) were globally inhibited by sounds in the dark and excited in the light, while the layer 1 interneuron population seemed globally unaffected in the dark and excited in the light (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.44006.012</object-id><label>Figure 5.</label><caption><title>Context-dependent inhibition by sound is mediated by a sub-population of L1 interneurons.</title><p>(<bold>A</bold>) In vivo two-photon images of GAD2-positive V1 neurons in L1 and L2/3 expressing td-Tomato. Superimposed are the contours of the active regions of interest identified by GCAMP6s imaging, blue: putative pyramidal cells, white: putative GABAergic cells. (<bold>B</bold>) Mean responses to the down-ramping sound (gray shading) for inhibitory and excitatory neurons in L2/3 and L1. (<bold>C</bold>) Fraction of neurons significantly excited (red) or inhibited (blue) by sounds for each layer and cell type (Wilcoxon rank sum test, p&lt;0.01: the dashed lines indicate chance level). (<bold>D</bold>) Scatter plot of the mean responses of all L1 inhibitory neurons to the down-ramping sound in the light against in the dark. Small gray dots indicate neurons that do not significantly respond in any condition. Larger, colored dots indicate significantly responding neurons. Dark blue indicates neurons significantly inhibited in the dark and responding more in light than in dark. Dark red dots indicate neurons significantly activated in the dark and responding more in dark than in light. Neurons responding equally in dark and light are marked with light blue (negative response) and light red (positive response) dots. Neurons that are non-responsive in the dark but respond more or less in the light are marked in white and dark, respectively. (<bold>E</bold>) Single trial deconvolved traces for an L1 interneuron responding to a down-ramping sound (gray shading) in the dark and inactive in the light. (<bold>F</bold>) Fractions of neurons responding significantly more in light than in dark (blue) or more in dark than in light (red) (Wilcoxon rank sum test, p&lt;0.01). (<bold>G</bold>) Fraction of neurons that can mediate context-dependent inhibition in each layer that is interneurons significantly excited by sounds in the dark and responding significantly less in the light than in the dark. (<bold>H</bold>) Simplified schematic of the proposed mechanism for context-dependent auditory responses in V1. Cells unaffected by visual context are not displayed.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig5-v3.tif"/></fig><p>However, the population trend concealed functionally distinct subpopulations. When we tested for significant positive or negative responses in the dark in single neurons (Wilcoxon rank-sum test, p&lt;0.01), we found a large fraction of neurons significantly inhibited, but also, a subpopulation of 12.8% of all L1 inhibitory neurons that were significantly activated in the dark (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). This positive response was rare in L2/3 with only 2.1% of inhibitory and 1% of excitatory cells significantly activated (chance levels = 1%, <xref ref-type="fig" rid="fig5">Figure 5C</xref>). When plotting the responses of L1 neurons in the dark against their responses in the light (<xref ref-type="fig" rid="fig5">Figure 5D</xref>), it is apparent that the L1 neurons that are excited by sounds in the dark tend to be less excited in the light (e.g. <xref ref-type="fig" rid="fig5">Figure 5E</xref>). In contrast, L1 neurons inhibited or unaffected by sounds in the dark became more activated in the light (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Consistently, statistical assessment showed that a significant percentage (5.3%) of L1 neurons (Wilcoxon rank-sum test, p&lt;0.01) respond less in light than in dark (<xref ref-type="fig" rid="fig5">Figure 5F</xref>), and that almost all these neurons are significantly excited in the dark (4.9%, <xref ref-type="fig" rid="fig5">Figure 5G</xref>). In contrast, based on the same tests, there was no significant population of L2/3 neurons that responded less in the light than in the dark (<xref ref-type="fig" rid="fig5">Figure 5F,G</xref>). Thus, we conclude that layer 1 contains the only supragranular subpopulation of GABAergic neurons that can provide a context-dependent, sound-induced inhibition gated by visual inputs. These neurons are thus good candidates to mediate the context-dependence of sound responses observed in the bulk of L2/3 V1 neurons (<xref ref-type="fig" rid="fig5">Figure 5H</xref>).</p></sec><sec id="s2-4"><title>Loud onsets boost the representation of coincident visual events</title><p>As we have shown that sounds can excite neurons in V1 in lit conditions, we next wanted to know the impact of AC inputs to V1 on the representations of coincident visual inputs. To this end, we recorded 9849 L2/3 neurons (23 sessions, seven mice) in mice injected with AAV-syn-GCAMP6s in V1. In these experiments, we presented the up-ramping and down-ramping auditory stimuli coincidently with a white looming or receding disk on the screen (sounds and visual stimuli were 2 s duration). The loudspeaker was placed such that auditory and visual stimuli came from the same direction. Unimodal stimuli were also delivered to assess additivity for the bimodal conditions.</p><p>We first observed that 11% of the V1 neurons displayed significant (Generalized Linear Model, p&lt;0.05) supra-additive boosting of their visual responses when coincident with the loud onset of down-ramps (e.g. <xref ref-type="fig" rid="fig6">Figure 6A</xref>), an effect even visible at the population level (<xref ref-type="fig" rid="fig6">Figure 6B</xref>) and dependent on AC as assessed with muscimol silencing (<xref ref-type="fig" rid="fig6">Figure 6D,E</xref>). Because looming and receding disks trigger different types of responses in V1, we performed a clustering in order to identify the main types of visual responses and sound-induced modulations (see Materials and methods, note that only the V1 neurons with high signal-to-noise and sufficient number of trials without eye movement, n = 499, were clustered). This analysis identified seven distinct clusters (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) capturing 49.5 ± 11.2% of the response variance. Four clusters (#1,4,6,7; 358 neurons) responded more to looming than receding disks, and three clusters (#2,3,5; 141 neurons) had the opposite preference. Four clusters (#1,2,6,7) were tightly direction-specific and responded only for their preferred stimulus, at its beginning or end, while the three others (#3,4,5) were less specific to direction, but also responded to specific parts of the stimulus (clusters 3 and 4: larger disk; cluster 5: smaller disk). This specificity was probably due to the location in the retinotopic field of the neurons with respect to the stimulus center.</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.44006.013</object-id><label>Figure 6.</label><caption><title>V1 neurons are boosted by the coincidence of loud auditory onsets with visual onsets.</title><p>(<bold>A</bold>) Sample neuron in V1. Raw GCAMP6s traces for a visual stimulus (blue), a sound (green) and a combination of both (red) show a large neuronal response for the bimodal condition compared to the additive prediction (dashed black line) based on unimodal responses. Individual trials (thin lines) and their average (thick lines) show that the multisensory responses are robust for this neuron. (<bold>B</bold>) Average deconvolved calcium responses (mean ±sem) in V1 for up- and down-ramping sounds (green), looming and receding visual stimuli (blue) and the four bimodal combinations. Black dashed lines: linear prediction. (<bold>C</bold>) Uni- and bimodal responses, displayed as in B., of the seven different functional cell types identified by hierarchical clustering. (<bold>D</bold>) Average deconvolved responses to the receding disk in absence (blue) and presence (red) of a simultaneous down-ramping sound, upon saline injection in AC (left, n = 765 neurons in four experiments and three mice) and upon muscimol inactivation of AC (right, n = 825 neurons in five experiments, same three mice). (<bold>E</bold>) Distribution of AC inactivation effects across experiments for the data shown in D, one-sided Wilcoxon signed-rank test (n = 5). Throughout the figure the ramping green shadings mark the presentation of up- or down-ramping sounds, and the ramping blue shadings the presentation of looming and receding visual stimuli.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig6-v3.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44006.014</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Bimodal responses in the area anteromedial to V1.</title><p>(<bold>A</bold>) Population responses (3076 neurons, nine sessions, four mice) to the four unimodal stimuli and four bimodal stimuli in the area anteromedial to V1 (see <xref ref-type="fig" rid="fig3">Figure 3D</xref>). Down-ramping sounds alone elicit strong onset responses which at the population level sum linearly with visual responses. (<bold>B</bold>) Result of the clustering of uni- and multimodal responses in the anteromedial area. Six clusters were identified, five of which integrate mostly linearly auditory and visual responses and only one (bottom right) shows non-linear auditory-visual responses. Interestingly also, one cluster shows auditory responses that are stronger than visual responses (top right), corroborating the idea that auditory input to the associative region anteromedial to V1 has also a driving role and not only a modulatory role as we observe in V1.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig6-figsupp1-v3.tif"/></fig></fig-group><p>The bimodal conditions revealed pronounced supra-additive responses. Down-ramping sounds boosted the responses of all cell clusters that responded to the onset of visual stimuli. (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, black arrows for clusters 1–5, bootstrap test, see Materials and methods). Very little boosting was observed with the quiet onset of up-ramps (<xref ref-type="fig" rid="fig6">Figure 6C</xref>), consistent with its reduced representation in AC to V1 projections (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Responses occurring towards the end of the stimulus were minimally (although significantly, cluster 5) boosted, probably because they correspond to auditory features (‘loud and quiet tonic’) that have less of an impact on V1 (see <xref ref-type="fig" rid="fig3">Figure 3B</xref>). Our data suggest that visual responses in V1 are boosted preferentially by the coincidence of loud sound onsets with respect to other basic envelope features. Note that a moderate response suppression was also observed, but only in cluster 7 (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, blue arrow). Moreover, the boosting of visual responses appeared to be a strong feature of V1, as the same analysis identified only one supra-linear cluster out of six clusters in the associative area medial to V1 (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p></sec><sec id="s2-5"><title>Interplay between AC inputs and neuronal non-linearities can explain all auditory-driven responses in V1</title><p>How can the same circuit implement context-dependent sound-driven inhibition and a boosting of specific visual responses? To propose a mechanism, we established a minimal model in which AC projections excite both a population of excitatory neurons (pyramidal cells) and a population of interneurons in V1 (putatively, the identified L1 population), consistent with connectivity studies (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>). The interneuron population directly inhibits excitatory neurons in our model (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). We used two critical mechanisms to render interneurons context-dependent: (i) a connection which inhibits interneurons in the presence of visual inputs (light modulation, <xref ref-type="fig" rid="fig7">Figure 7A</xref>, and (ii) a non-linear response function (response threshold). Then, provided that they are close to activation threshold (i.e. spike threshold) in the dark and that inhibition by visual inputs brings them well below threshold in the lit condition, interneurons will be less active in the light than in the dark. As a result, the excitatory population experiences a dominance of sound-driven excitation in the light and of sound-driven inhibition in the dark (<xref ref-type="fig" rid="fig7">Figure 7B,C</xref>) as observed experimentally. Biophysically, this mechanism requires a tonic inhibitory drive to the identified subset of L1 interneurons which could be provided by another inhibitory population. Interestingly, we observed that the subpopulation of L1 interneurons that are less responsive to sounds in light also have a smaller baseline fluorescence in light, an effect that is not observed for other L1 interneurons (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). This could be a possible manifestation of the tonic drive hypothesized in our model.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.44006.015</object-id><label>Figure 7.</label><caption><title>Non-linear inhibitory and excitatory neurons reproduce gated-inhibition and bimodal boosting.</title><p>(<bold>A</bold>) Sketch of a minimal model for the switch between negative and positive sound responses in dark vs in light and boost of visual responses to sounds. An inhibitory (I-neurons) population endowed with a non-linear response function (bottom) receives a positive auditory input and a negative visual input. When both inputs are active (lit condition), the auditory drive to I-neurons is below threshold and no inhibition is delivered to the excitatory non-linear neuronal population (E-neurons). In the dark condition, the visual modulation is reduced releasing sound triggered inhibition. Both I- and E-neurons are non-linear with a low and a high output gain. Auditory and visual inputs are typically converted with a low-gain, while coincident auditory and preferred-visual inputs pass the threshold and yield a high-gain output (boosting). (<bold>B</bold>) Input currents and output response of I-neurons during auditory simulation in light and dark conditions. <bold>Inset</bold>: Change in baseline fluorescence between light and dark conditions in L1 interneurons. A significant baseline decrease in light is observed for interneurons that are more sound responsive in dark than in light (i.e. putative I-neurons, red) as compared to interneurons that are more responsive in the light (Wilcoxon rank sum test, p=0.00989, n1 = 57 ‘dark &lt;light’ cells, n2 = 14 ‘light &lt;dark’ cells). (<bold>C</bold>) Same for E-neurons. (<bold>D</bold>) Input currents and output response of E-neurons during visual simulation alone (blue) and bimodal stimulation for a sub-threshold (non-preferred) visual input. Superimposed are the auditory responses in light and dark from panel (<bold>C</bold>) (<bold>E</bold>) Same as in D. but for a preferred, supra-threshold visual input.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44006-fig7-v3.tif"/></fig><p>We then showed that the same model can reproduce boosting of visual responses by sounds (<xref ref-type="fig" rid="fig7">Figure 7D,E</xref>) if excitatory neurons also have a simple non-linear response function. When excitatory neurons have a low, subthreshold output gain and a high, supra-threshold gain (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), it is possible to reproduce the observations that the response to sound alone (low gain regime) is weaker than the bimodal boosting effect (high-gain regime) (<xref ref-type="fig" rid="fig7">Figure 7D–E</xref>). With this design, we also reproduced the observation that boosting mainly occurs for the preferred visual stimulus (<xref ref-type="fig" rid="fig6">Figures 6C</xref> and <xref ref-type="fig" rid="fig7">7D–E</xref>), provided that the auditory and non-preferred visual inputs are driving excitatory neurons in the low-gain regime (<xref ref-type="fig" rid="fig7">Figure 7D</xref>) while the preferred input brings excitatory neurons close to or above the high-gain threshold (<xref ref-type="fig" rid="fig7">Figure 7E</xref>). In this case, the auditory response sums with a high gain for the preferred stimulus and a low gain for the non-preferred. While less conventional than a regular spike threshold, this mechanism could be biophysically implemented with non-linear dendritic processing. Apical dendrites of layer 2/3 pyramidal cells are known to produce large calcium spikes which boost somatic output when somatic activity coincides with excitatory inputs arriving in the apical tree (<xref ref-type="bibr" rid="bib24">Larkum et al., 2007</xref>). As AC axon terminals are concentrated in layer 1 where they likely contact the apical dendrites of pyramidal cells, sound-driven boosting of visual responses could rely on this dendritic phenomenon.</p><p>Taken together, our data and modeling indicate that a minimal circuit including a direct excitation from auditory cortex onto L2/3 pyramidal cells and an indirect inhibition via a subpopulation of L1 interneurons is sufficient to provide a context-dependent auditory modulation that emphasizes visual events coincident with loud sound onsets.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Using two-photon calcium imaging in identified excitatory and inhibitory neurons during auditory and visual stimulation, we demonstrated three important features of AC to V1 connections. First, we showed that V1-projecting neurons have a different bias in the distribution of sound envelope features they encode, as compared to layer 2/3 neurons in AC and also, although to a lesser extent, as compared to layer 5 in which they are located. In particular, the V1-projecting neuron population predominantly encodes loud sound onsets. Second, they are gated by a context-dependent inhibitory mechanism likely implemented by a subpopulation of L1 interneurons. Third, their impact on V1 representations is to produce a strong boosting of responses to dynamical visual stimuli coincident with different sound envelope features, particularly with loud sound onsets.</p><p>The shifts in the distribution of sound envelope features that we observed across layers and cell subpopulations is an interesting case of coding bias in different neural populations of auditory cortex (<xref ref-type="fig" rid="fig1">Figure 1</xref>). As observed earlier, in layer 2/3, there is a balance between the main neuron types that signal the beginning and the end of sounds with equivalent fractions of neurons responding to quiet or loud onsets and loud offsets (<xref ref-type="fig" rid="fig1">Figure 1F</xref>, note that ‘tonic +loud OFF’ neurons are included in this count. Quiet offset neurons are rare and not robustly detected across datasets. Detection of tonic responses by clustering is also variable.). In upper layer 5, the distribution becomes more biased toward loud onsets at the expense of offset neurons, a trend that is further increased in the layer 5-located V1-projecting cells in which half of the neurons respond to loud onsets and 10% of the cells to loud offsets. Similar results have been found in somatosensory cortex for neurons projecting to secondary somatosensory or motor cortex (<xref ref-type="bibr" rid="bib4">Chen et al., 2013a</xref>). In AC, it is well-established that specific acoustic features are related to the localization of neurons tangentially to the cortical surface in different subfields (<xref ref-type="bibr" rid="bib20">Kanold et al., 2014</xref>; <xref ref-type="bibr" rid="bib41">Schreiner, 1995</xref>; <xref ref-type="bibr" rid="bib42">Schreiner and Winer, 2007</xref>) and that, at least in higher mammals such as cats, particular auditory cortical fields may play different functional roles in behavior, such as sound localization (where) or identification (what) (<xref ref-type="bibr" rid="bib29">Lomber and Malhotra, 2008</xref>). Our results show that projection specificity can also be a determinant of feature selectivity, at least for some intensity modulation features at sound onsets and offsets.</p><p>As a result of these fine adjustments of feature distributions, layer 2/3 neurons in V1 are more impacted by abrupt sounds with loud onsets (e.g. down-ramps) than by sounds with softer onsets (e.g. up-ramps). This asymmetry is particularly striking for excitatory effects seen in light or with visual stimuli (<xref ref-type="fig" rid="fig3">Figures 3B</xref> and <xref ref-type="fig" rid="fig6">6</xref>), but less pronounced for inhibitory effects in the dark (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). It is thus possible that the projection targets also depend on encoded features. For example, L2/3 V1 pyramidal cells may receive projections almost exclusively from AC loud onset neurons while interneurons providing inhibition in the dark still receive a significant fraction of quiet onset and loud offset inputs.</p><p>The context-dependence of sound-triggered inhibitory and excitatory auditory responses in V1 is interesting in two respects. First, it reconciles conflicting reports of sound-induced inhibition or excitation in V1 (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Iurilli et al., 2012</xref>). Second, context-dependent inhibition could serve to decrease V1 activity in the dark when visual information is irrelevant for sound source localization, while leaving possibilities to integrate auditory and visual information in light conditions. The context-dependence of auditory responses in V1 is reminiscent of the recent observation that AC to V1 projections, activated by loud sounds and acting through L1, boost preferred orientation responses in V1 and inhibit non-preferred orientations (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>). The two effects share several properties including selectivity to high sound levels, the dominance of inhibition for weaker visual inputs and of excitation for stronger visual inputs, and the involvement of L1 neurons for mediating cross-modal inhibition. The main discrepancy is the apparent lack of visual input specificity of our illumination-dependent excitation of V1 neurons by AC inputs alone (i.e. no inhibition of responses to non-preferred visual stimuli). This could be due to differences in the type of visual inputs used. One important result appearing in our data is that only a fraction of all L1 interneurons have response properties compatible with the function of releasing a context-dependent, sound-triggered inhibition while these properties are not found in L2/3 neurons (<xref ref-type="fig" rid="fig5">Figure 5C,F,G</xref>). Thus, the silencing of visual processing by sounds in the dark is not a generic function of L1 interneurons, compatible with the observation that only VIP-negative interneurons receive AC inputs in V1 (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>) and the observation that L1 interneurons serve other functions (<xref ref-type="bibr" rid="bib14">Hattori et al., 2017</xref>), such as disinhibition of L2/3 in various contexts (<xref ref-type="bibr" rid="bib10">Fu et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Letzkus et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Pfeffer et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Pi et al., 2013</xref>). An interesting question that cannot be addressed with the current data is how the specific L1 interneurons that mediate sound-induced inhibition are themselves inhibited in the presence of visual inputs. Several putative mechanisms are compatible with the data, including disinhibition. We can however exclude that visual responses in AC (<xref ref-type="bibr" rid="bib34">Morrill and Hasenstaub, 2018</xref>) modulate a fraction of the AC input to V1, based on context-independent responses of retrogradely identified V1-projecting cells (<xref ref-type="fig" rid="fig3">Figure 3F</xref>).</p><p>Interestingly, our data obtained outside V1 indicate that, independent of the exact mechanism of context-dependence of auditory responses, this phenomenon is restricted to V1 (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). This indicates that AC projections to associative areas impact the local circuit differently than in V1. A possible explanation could be that projections to excitatory neurons are favored or that the layer 1 interneurons that provide strong sound-driven inhibition in V1 in the dark are nonexistent or excluded by AC projections in associative areas.</p><p>Another important phenomenon observed in our data is the boosting of visual responses coincident with loud sound onsets (<xref ref-type="fig" rid="fig6">Figure 6</xref>), an effect that could even potentially be amplified if visual stimuli led sounds by a few tens of milliseconds as observed in monkey auditory cortex (<xref ref-type="bibr" rid="bib21">Kayser et al., 2008</xref>). Our model suggests that this can result directly from the non-linear summation of visual and auditory inputs in V1 pyramidal cells, while sound-triggered inhibition is abolished by the visual context (<xref ref-type="fig" rid="fig7">Figure 7</xref>). The large amplitude of the sound-induced boosting as compared to the weaker responses observed when sounds are played without coincident visual input (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig5">5</xref> <italic>vs</italic> <xref ref-type="fig" rid="fig6">Figure 6</xref>, see also <xref ref-type="fig" rid="fig7">Figure 7</xref>) points toward a threshold mechanism, allowing for amplification of the auditory input only if sufficient visual input concomitantly arrives. Such a gated amplification mechanism could be implemented directly in L2/3 pyramidal neurons using the direct inputs they receive from auditory cortex (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="bib26">Leinweber et al., 2017</xref>). For example, amplification of L1 input by coincident somatic inputs triggering calcium spikes in apical dendrites has been described both in L5 (<xref ref-type="bibr" rid="bib23">Larkum et al., 1999</xref>) and L2/3 (<xref ref-type="bibr" rid="bib24">Larkum et al., 2007</xref>) pyramidal cells. While this is a good candidate cellular mechanism for the observed boosting effect, we cannot exclude alternative or complementary mechanisms such as a disinhibition mediated by a subpopulation of L1 interneurons (<xref ref-type="bibr" rid="bib16">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="bib25">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="bib35">Pfeffer et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Pi et al., 2013</xref>), a different population from the subpopulation providing direct inhibition (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Also, as down-ramping sounds are startling stimuli, a cholinergic input to visual cortex, known to increase cortical responses via disinhibition (<xref ref-type="bibr" rid="bib10">Fu et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Letzkus et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Letzkus et al., 2011</xref>) could also contribute to modulation of visual responses by sounds in V1. Independent of the mechanism, our data show that sound-induced boosting is a strong effect in V1 which tightly relates to coincidence with the loud onsets of sounds. An interesting hypothesis for the role of this mechanism, among other possibilities, is that this effect could provide a powerful way to highlight, in visual space, the visual events potentially responsible for the sound, and thus might be an element of the cortical computations related to the localization of sound sources (<xref ref-type="bibr" rid="bib47">Stein and Stanford, 2008</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>All animals used were 8–16 week-old male C57Bl6J and GAD2-Cre (Jax #010802) x RCL-TdT (Jax #007909) mice. All animal procedures were approved by the French Ethical Committee (authorization 00275.01). Animals were allocated in experimental groups without randomization and masking, but all scoring and analysis is done with computer programs.</p></sec><sec id="s4-2"><title>Two-photon calcium imaging in awake mice</title><p>Three to four weeks before imaging, mice were anaesthetized under ketamine medetomidine. A large craniotomy (5 mm diameter) was performed above the right primary visual cortex (V1) or the right auditory cortex (rostro-caudal, lateral and dorso-ventral stereotaxic coordinates from bregma, V1: −3.5, 2.5, 1 mm; AC: −2.5, 4, 2.2 mm). For the imaging of the auditory cortex (AC), the right masseter was removed before the craniotomy. We typically performed three to four injections of 150 nl (30 nl.min<sup>−1</sup>), AAV1.Syn.GCaMP6s.WPRE virus obtained from Vector Core (Philadelphia, PA) and diluted x10. For imaging V1-projecting neurons in AC, a stereotaxic injection of 5x diluted CAV-2-Cre viral solution (obtained from the Plateforme de Vectorologie de Montpellier) was performed just before the AC craniotomy. The craniotomy was sealed with a glass window and a metal post was implanted using cyanolite glue followed by dental cement. A few days before imaging, mice were trained to stand still, head-fixed under the microscope for 30–60 min/day. Then mice were imaged 1–2 hr/day. Imaging was performed using a two-photon microscope (Femtonics, Budapest, Hungary) equipped with an 8 kHz resonant scanner combined with a pulsed laser (MaiTai-DS, SpectraPhysics, Santa Clara, CA) tuned at 920 nm. We used 20x or 10x Olympus objectives (XLUMPLFLN and XLPLN10XSVMP), obtaining a field of view of 500 × 500, or 1000 × 1000 microns, respectively. To prevent light from the visual stimulation monitor from entering the microscope objective, we designed a ring silicon mask that covered the gap between the metal chamber and the objective. Images were acquired at 31.5 Hz during blocks of 5 s interleaved with 3 s intervals. A single stimulus was played in each block and stimulus order was randomized. All stimuli were repeated 20 times except drifting gratings (10 repetitions).</p><p>Drifting square gratings (2 Hz, 0.025 cyc/°) of eight different directions (0° from bottom to top, anti-clockwise, 45°, 90°, 135°, 180°, 225°, 270°, 315°), and a size- increasing (18° to 105° of visual angle) or decreasing white disk over a black background were presented on a screen placed 11 cm to the mouse left eye. The luminance of the black background and white disks was 0.57 and 200 cd/m<sup>2</sup>, respectively. The screen (10VG BeeTronics, 22 × 13 cm) was located 11 cm from the contra-lateral eye, thus covering 90° x 61° of its visual field including some of the binocular segment. For auditory cortex recording, we used 250 ms constant white noise sounds at four different intensity modulations, and two up- and down-intensity ramping sounds between 50 and 85 dB SPL. For all sounds, the intensity envelope was linearly ramped from or to 0 during 10 ms at the beginning and the end of the sound. Note that microscope scanners emit a constant background sound of about 45 dB SPL. For visual cortex experiments, we used only the two intensity ramps and added two frequency modulated sounds, going linearly from 8 kHz to 16 kHz and vice versa. Up- and down- amplitude ramps were combined with the increasing and decreasing disks to form four multisensory stimuli. The loudspeaker was placed just next to the stimulation screen, facing the contralateral eye. All stimuli were 2 s long. Auditory and visual stimuli were driven by Elphy (G. Sadoc, UNIC, France). All sounds were delivered at 192 kHz with a NI-PCI-6221 card (National Instruments) an amplifier and high-frequency loudspeakers (SA1 and MF1-S, Tucker-Davis Technologies, Alachua, FL). Sounds were calibrated in intensity at the location of the mouse ear using a probe microphone (Bruel and Kjaer).</p><p>For V1 recordings, auditory only stimulations were performed in two different contexts: either in complete darkness (screen turned off in the sound and light isolated box enclosing the microscope) or in dim light (screen turned on with black background, luminance measured to 0.57 candela per square meter).</p></sec><sec id="s4-3"><title>AC inactivation experiments</title><p>To perform muscimol inactivation of AC during V1 imaging a stereotaxic muscimol injection (100 or 150 nL, 1 mg/mL) was performed before imaging under light isoflurane (~1.5%) anesthesia via a hole drilled through cement and bone on the lateral side of the V1 cranial window. The hole was sealed with Kwik-Cast and the animal was allowed to recover 20 min from anesthesia in its cage before being placed under the microscope and imaged during auditory-visual stimulation. Saline controls were performed using the same procedure on the next day, imaging the same V1 region and a similar optical plane (the exact same neurons were not tracked across days but the imaging plane was as similar as possible). To perform chemogenetic inactivation of V1 projecting neurons in AC during V1 imaging, three stereotaxic injections of undiluted AAV8-hSyn-DIO-hM4D(Gi)-mCherry virus were done in AC and immediately after CAV-2-Cre virus (5x) was co-injected with the AAV1.Syn.GCaMP6s.WPRE virus in V1 during window implantation. Four to five weeks later, a control imaging session was performed, followed by a subcutaneous CNO injection (3 mg/kg) under light isoflurane anesthesia (~1.5%) in the imaging setup. After a 60 min waiting period in the setup, the animal was imaged again to monitor the effect of the chemogenetic manipulation in the exact same neurons as during the control session.</p></sec><sec id="s4-4"><title>Intrinsic optical imaging recordings</title><p>To localize the calcium-imaging recordings with respect to the global functional organization of the cortex, we performed intrinsic optical imaging experiments under isoflurane anesthesia (1%). The brain and blood vessels were illuminated through the cranial window by a red (intrinsic signal: wavelength 780 nm) or a green (blood vessel pattern: wavelength 525 nm) light-emitting diode.</p><p>To localize the visual cortex, the reflected light was collected at 15 Hz by a charge-coupled device (CCD) camera (Foculus IEEE 1394) coupled to the epifluorescence light path of the Femtonics microscope (no emission or excitation filter). A slow drifting bar protocol was used: a white vertical bar was drifting horizontally over the screen width for 10 cycles at 0.1 Hz, from left to right in half of the trials, and from right to left in the other half. After band-passing, the measured signals around 0.1 Hz, and determining their phase in each pixel and for each condition, both the preferred bar location and the hemodynamic delay could be determined for each pixel, yielding azimuth maps as in (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Similarly, elevation maps were obtained in a subset of the animals using a horizontal bar drifting vertically. These maps coincided with those obtained in previous studies (<xref ref-type="bibr" rid="bib31">Marshel et al., 2011</xref>), so we could determine V1 border as the limit between pixels displaying and not displaying retinotopy.</p><p>To localize the auditory cortex, the reflected light was collected at 20 Hz by a CCD camera (Smartek Vision, GC651M) attached to a custom-made macroscope. The focal plane was placed 400 µm below superficial blood vessels. A custom-made Matlab program controlled image acquisition and sound delivery. Sounds were trains of 20 white noise bursts or pure tone pips separated by smooth 20 ms gaps (down and up 10 ms linear ramps). We acquired a baseline and a response image (164 × 123 pixels,~3.7×2.8 mm) corresponding to the average images recorded 3 s before and 3 s after sound onset, respectively. The change in light reflectance (ΔR/R) was computed then averaged over the 20 trials for each sound frequency (4, 8, 16, 32 kHz, whitenoise). A 2D Gaussian filter (σ = 45.6 µm) was used to build the response map. The functional localization of the auditory cortex in this study corresponded to the response map produced by white noise.</p></sec><sec id="s4-5"><title>Eye tracking and trial filtering</title><p>Left eye measurements (eye size, pupil diameter, pupil movement) were made by tracking the eye using a CCD camera (Smartek Vision, GC651M). A Python software was used to capture images from the camera at 50 Hz, synchronized with the cortical recordings.</p><p>These movies were analyzed off-line using custom automatic Matlab programs that traced the contours, first of the eyelid, second of the pupil. The eye lid shape was approximated by two arcs and involved the estimation of 6 parameters (four for the coordinates of the two points where the arcs join, two for the y-coordinates of the crossings of the arcs with the vertical line at halfway between these two points; see <xref ref-type="fig" rid="fig2">Figure 2C</xref>, bounds for the parameters were set by hands and appear in yellow). The pupil shape was approximated by an ellipse, described by four parameters (center x and y, radius and eccentricity). Both estimations were performed by maximizing the difference between average luminance inside and outside the shape, as well as the luminance gradient normal to the shape boundary; in addition, they were inspected manually and corrected occasionally inside a dedicated graphical user interface. In the ‘dim light’ and ‘bimodal’ (but not the ‘dark’) contexts, it was observed that saccades correlated with population activity increases (e.g. <xref ref-type="fig" rid="fig2">Figure 2D</xref>), therefore we discarded all trials from these contexts displaying saccades. To do so, we computed the maximal distance between pupil center positions (<italic>x<sub>c</sub>,y<sub>c</sub></italic>) at different instants <italic>t</italic> and <italic>t’</italic> during the 2 s of stimulation <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo form="prefix">max</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn><mml:mtext>s</mml:mtext></mml:mrow></mml:munder><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> and discarded trials where the change in gaze exceeded the mouse visual acuity of 2°, that is where this distance exceeded 57.6 µm (assuming an eye radius of 1.65 mm).</p></sec><sec id="s4-6"><title>Calcium imaging data summary</title><p>The data obtained from the different imaging experiments consisted of the following.</p><p>A1 recordings of auditory responses: 4616 cortical neurons from 29 L2/3 imaging areas in 11 animals. In 18 sessions (seven mice), animals were also stimulated with looming visual stimuli and bimodal stimuli from which we observed no response or modulation. 7757 neurons from upper layer 5 recorded in 12 sessions across four mice during sound only protocols. 2474 neurons projecting to V1 recorded in 15 sessions across three mice during sound only protocols repeated in light or in dark conditions.</p><p>V1 recordings: 18,925 neurons from 35 L2/3 imaging areas in nine mice, and included the three ‘dark’, ‘dim light’ and ‘bimodal’ blocks in random order. Eye tracking was performed in 23 of these sessions (9849 neurons, seven mice; only these sessions where used for the analyses of the ‘dim light’ and ‘bimodal’ context, which required filtering out trials with saccades).</p><p>V1 recordings with labeling of Gad-positive interneurons: an additional 4348 neurons from 12 L2/3 imaging sessions (depth ranging from 140 to 300 microns), and 265 GAD2 positive neurons from 7 L1 imaging sessions (depth from 30 to 100 microns). All of these sessions had eye tracking, and included the ‘dark’ and ‘dim light’ blocks. 6 L2/3 sessions also included ‘bimodal’ blocks, whereas the 13 remaining sessions included a second ‘dim light’ block where the left (contralateral) eye was covered by a dark mask (<xref ref-type="fig" rid="fig3">Figure 3F</xref>).</p></sec><sec id="s4-7"><title>Calcium imaging pre-processing</title><p>Data analysis was performed with custom-made Matlab scripts available upon request. Every frame recorded was corrected for horizontal motion to a template image using rigid body registration (all sessions with visible z motion were discarded). Regions of interest were then automatically selected and human checked as the cell bodies of neurons with visually identifiable activity (<xref ref-type="bibr" rid="bib39">Roland et al., 2017</xref>) and the mean fluorescence signal F(t) was extracted for each region. We also estimated the local neuropil signal F<sub>np</sub>(t) for each neuron and subtracted a fraction of it from the neuron signal F<sub>c</sub>(t)=F(t) - 0.7 F<sub>np</sub>(t). This fraction was set to 0.7, according to a previous calibration for GCAMP6s in mouse visual cortex (<xref ref-type="bibr" rid="bib5">Chen et al., 2013b</xref>). Baseline fluorescence F<sub>c0</sub> was calculated as the minimum of a Gaussian-filtered trace over the 5 neighboring 5 s imaging blocks and fluorescence variations were computed as f(t) = (F<sub>c</sub>(t) - F<sub>c0</sub>)/F<sub>c0</sub>. Analyses were performed either on these normalized fluorescence signals, or on estimations of the firing rate obtained by temporal deconvolution (<xref ref-type="bibr" rid="bib1">Bathellier et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">Yaksi and Friedrich, 2006</xref>) as r(t)=f'(t) + f(t)/τ, in which f'(t) is the first derivative (computed at successive samples separated by ~30 ms) of f(t) and τ = 2 s, as estimated from the decays of the GCAMP6s fluorescent transients (<xref ref-type="bibr" rid="bib5">Chen et al., 2013b</xref>). This simple method efficiently corrects the strong discrepancy between fluorescence and firing rate time courses due to the slow decay of spike-triggered calcium rises (<xref ref-type="bibr" rid="bib1">Bathellier et al., 2012</xref>), and was preferred to more advanced deconvolution methods (<xref ref-type="bibr" rid="bib7">Deneux et al., 2016b</xref>) because it does not bias deconvolution towards absence of activity (i.e. interpreting small signal as noise) in cells displaying poor signal to noise ratio. However, it does not correct for the relatively slow rise time of GCAMP6s, producing a time delay on the order of 70 ms between peak firing rate and peak deconvolved signal.</p></sec><sec id="s4-8"><title>Calcium imaging analysis</title><p>Data analysis was performed with custom-made Matlab and Python scripts. Clustering of auditory responses in the auditory and visual cortices (<xref ref-type="fig" rid="fig1">Figure 1</xref>), and of bimodal conditions in the visual cortex (<xref ref-type="fig" rid="fig6">Figure 6</xref>) was performed using the following procedure. Deconvolved calcium responses were averaged across all valid trials (eye movement filtering). We then subtracted the baseline (average activity from 0 to 0.5 s before stimulus onset) and the average activity profile in trials with no stimulation. Hierarchical clustering was performed using the Euclidean metric and Ward method for computing distance between clusters. To determine the number of clusters, we moved down the clustering threshold (100 clusters for all AC datasets, 25 clusters for V1 data) until clusters became redundant (overclustering) as assessed visually. This method clusters neurons irrespective of whether they significantly responded to the stimuli. As a large number of neurons were not (or very weakly) responsive to the stimuli, a fraction of the clusters were merged into a cluster of ‘noisy’ non- or weakly responsive cells, which were eliminated from the final cluster list based on their heterogeneity measured as the mean correlation across the responses of each cell (only clusters with homogeneity larger than 0.4 were kept). These thresholds were chosen by visual inspection of the obtained clusters. Some clusters which were homogeneous but obviously captured a systematic perturbation of the signal (correlated noise) were also manually put in the non-responsive cluster. To make sure cell type distributions were not skewed by the fact that clustering outputs the most robust auditory responses (<xref ref-type="fig" rid="fig1">Figure 1</xref>), we re-aggregated neurons discarded as non-responsive if the mean correlation of their activity signature with any of the identified clusters was larger than 0.4. This procedure did not change the mean cluster response profiles and did not qualitatively impact the conclusions drawn on the distributions of the different clusters in V1-projecting and L2/3 neurons.</p></sec><sec id="s4-9"><title>Awake electrophysiology</title><p>Multi-electrode silicon probe recordings were done in mice already implanted with a cranial window above AC at least 2 or 3 weeks before the experiment. The brain was exposed by removing some of the dental cement and part of the cranial window’s cover slip with the animal under light isoflurane anesthesia (similar to intrinsic imaging). The brain was covered with Kwik-CastTM silicon (World precision Instruments) and the animal was removed from the setup to recover from anesthesia for at least 1 hr. Electrophysiological recordings were done using four shank Buzsaki32 silicon probes (Neuronexus). The animal was placed back on the setup-up and the Kwik-Cast was removed. The brain was then covered with warm Ringer’s solution. The insertion of the probe was controlled by a micromanipulator (MP-225, Butter Instrument). Recordings were usually performed at 400, 600 700 and 800 µm depths in each animal using a pre-amplifier and multiplexer coupled to a USB acquisition card (Intan Technologies).</p></sec><sec id="s4-10"><title>Electrophysiology data analysis</title><p>Single unit spikes were detected automatically using the klusa Suite (<ext-link ext-link-type="uri" xlink:href="https://github.com/kwikteam/phy">https://github.com/kwikteam/phy</ext-link>). Spikes were then sorted using KlustaKwik spike sorting algorithm (<xref ref-type="bibr" rid="bib13">Harris et al., 2000</xref>) (Klusta, <ext-link ext-link-type="uri" xlink:href="https://github.com/kwikteam/klusta">https://github.com/kwikteam/klusta</ext-link>). All posterior data analyses were performed using custom Python scripts. Firing rates were calculated in 25 ms time bins, averaging over 20 sound repetitions.</p></sec><sec id="s4-11"><title>Statistics</title><p>Data are displayed as mean and S.E.M. Sample size was chosen without explicit power analysis to large enough (n &gt; 4) to allow detection of significance with typical non-parametric tests.</p><p>To assess the significance of the relative distribution of functional cell types in V1-projecting neurons and in layer 2/3 neurons, we performed a bootstrap analysis. The null-hypothesis is that both populations have the same distribution of clusters. To simulate this hypothesis, we pooled together all neurons and performed 10,000 random partitions of the pool population into the number of V1-projecting and L2/3 neurons. For each partition, we computed the difference between the fractions of each cluster across the two partitions. This led to distributions of expected fractional differences for each cluster under the null hypothesis. The p-value was computed from the percentile of which the actual observed fractional difference was located.</p><p>Significant responses for individual neurons were detected using the non-parametric Wilcoxon rank-sum test. Raw calcium fluorescence traces were subtracted for pre-stimulus level, and averaged over a time window near the response peak. The vector of such responses for different trials was compared to the same computations performed on blank trials (unless responses to two different conditions were compared, such as auditory responses in the dark vs. in a dim light). Unless specified otherwise, we used a statistical threshold of 1% for detecting responding neurons; by definition this means that on average 1% of the neurons <italic>not responding</italic> to the condition would be detected as responding (false positive). Therefore, in all the histogram displays of the fractions of responding neurons (e.g. <xref ref-type="fig" rid="fig5">Figure 5C,F</xref>) we masked the first 1% as being potentially only false positive detections.</p><p>To assess the significance of supra- or sub-linear responses to audio-visual combinations in individual clusters resulting from the clustering of bimodal responses, we used a bootstrap consisting in shuffling the different trial repetitions. The null hypothesis is that the cluster’s average bimodal responses can be predicted as the sum of average visual and auditory responses, after baseline has been subtracted. To test this hypothesis, we first computed, for each neuron, individual ‘repetitions’ of the linear predictions by adding responses to one visual presentation and to one auditory presentation. Then we performed one million shuffles of the labels of ‘linear prediction’ and ‘bimodal response’ trials, yielding a distribution of the expected difference between their averages under the null hypothesis. The p-values were computed from the percentile in which the actual nonlinear difference was located.</p></sec><sec id="s4-12"><title>Model</title><p>Simulations were performed using a rate model with 2 or 3 populations and no synaptic delay. The spiking activity <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of population <italic>i</italic> followed the equation:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mtext>context</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>auditory</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>auditory</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>visual</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>visual</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <italic>t</italic> is time, <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mtext>auditory</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mtext>visual</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> are the auditory and visual input (displaying an exponentially decaying signal and a plateauing signal, respectively), <inline-formula><mml:math id="inf5"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mtext>auditory</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf6"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mtext>visual</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf7"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> the connectivities to input and to the other populations (see model schematics in <xref ref-type="fig" rid="fig7">Figure 7A</xref> for which connections are non-zero). <inline-formula><mml:math id="inf8"><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext>context</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is a baseline accounting for other inputs, and whose value can depend on the context ('dark' or 'light'). <inline-formula><mml:math id="inf9"><mml:mi>f</mml:mi></mml:math></inline-formula> is a nonlinear function consisting of two linear segments (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) that accounts for spiking threshold or further nonlinear (e.g. dendritic) computations. The first segment is not a constant zero accounts for the fact that the other inputs summarized in <inline-formula><mml:math id="inf10"><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext>context</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> are in fact stochastic and can lead the cell to fire even when its average potential is below threshold.</p></sec><sec id="s4-13"><title>Data and software availability</title><p>The data that support the findings of this study are freely available at <ext-link ext-link-type="uri" xlink:href="https://www.bathellier-lab.org/downloads">https://www.bathellier-lab.org/downloads</ext-link> or at Dryad, doi:10.5061/dryad.82r5q83. Custom analysis scripts are available in <xref ref-type="supplementary-material" rid="scode1">Source Code 1</xref>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank A Fleischmann, Y Frégnac and J Letzkus for helpful discussions and comments on the manuscript. We thank A Fleischmann for providing CAV2-Cre viruses. This work was supported by the Agence Nationale pour la Recherche (ANR ‘SENSEMAKER’), the Marie Curie FP7 Program (CIG 334581), the Human Brain Project (SP3 - WP5.2), the European Research Council (ERC CoG Deepen), the Fyssen foundation, the DIM ‘Region Ile de France’, the International Human Frontier Science Program Organization (CDA-0064–2015), the Fondation pour l’Audition (Laboratory grant), École Doctorale Frontières du Vivant (FdV) – Programme Bettencourt and the Paris-Saclay University (Lidex NeuroSaclay, IRS iCode and IRS Brainscopes).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing—original draft, Wrote the paper, Designed all experiments, Carried out all visual cortex aspects of experiments, Collected and analyzed the data, Assisted with preparing the manuscript</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Validation, Investigation, Methodology, Wrote the paper, Designed, carried out chemogenetic and muscimol experiments, Collected and analyzed the data</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Designed, carried out some auditory cortex two-photon imaging experiments, Collected and analyzed some of the data</p></fn><fn fn-type="con" id="con4"><p>Formal analysis, Investigation, Designed, carried out auditory cortex electrophysiology experiments, Collected and analyzed the data</p></fn><fn fn-type="con" id="con5"><p>Data curation, Investigation, Carried out some auditory cortex two-photon imaging experiments, Collected and pre-processed the data</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Data curation, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing—original draft, Project administration, Writing—review and editing, Wrote the paper, Designed all experiments, Performed some two-photon imaging experiments, and analyzed auditory cortex data, Supervised the project</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All animal procedures were approved by the French Ethical Committee (authorization 00275.01).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><object-id pub-id-type="doi">10.7554/eLife.44006.016</object-id><label>Source code 1.</label><caption><title>Custom analysis scripts.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-44006-code1-v3.zip"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.44006.017</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-44006-transrepform-v3.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The data that support the findings of this study are freely available at <ext-link ext-link-type="uri" xlink:href="https://www.bathellier-lab.org/downloads">https://www.bathellier-lab.org/downloads</ext-link> or at Dryad, doi:10.5061/dryad.82r5q83. Custom analysis scripts are available as a source code file.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Deneux</surname><given-names>T</given-names></name><name><surname>Harrell</surname><given-names>E</given-names></name><name><surname>Kempf</surname><given-names>A</given-names></name><name><surname>Ceballo</surname><given-names>S</given-names></name><name><surname>Filipchuk</surname><given-names>A</given-names></name><name><surname>Bathellier</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Data from: Context-dependent signaling of coincident auditory and visual events in primary visual cortex</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.82r5q83</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bathellier</surname> <given-names>B</given-names></name><name><surname>Ushakova</surname> <given-names>L</given-names></name><name><surname>Rumpel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Discrete neocortical dynamics predict behavioral categorization of sounds</article-title><source>Neuron</source><volume>76</volume><fpage>435</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.07.008</pub-id><pub-id pub-id-type="pmid">23083744</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname> <given-names>JK</given-names></name><name><surname>Nodal</surname> <given-names>FR</given-names></name><name><surname>Bajo</surname> <given-names>VM</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Physiological and anatomical evidence for multisensory interactions in auditory cortex</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2172</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl128</pub-id><pub-id pub-id-type="pmid">17135481</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonath</surname> <given-names>B</given-names></name><name><surname>Noesselt</surname> <given-names>T</given-names></name><name><surname>Martinez</surname> <given-names>A</given-names></name><name><surname>Mishra</surname> <given-names>J</given-names></name><name><surname>Schwiecker</surname> <given-names>K</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural basis of the ventriloquist illusion</article-title><source>Current Biology</source><volume>17</volume><fpage>1697</fpage><lpage>1703</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.08.050</pub-id><pub-id pub-id-type="pmid">17884498</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>JL</given-names></name><name><surname>Carta</surname> <given-names>S</given-names></name><name><surname>Soldado-Magraner</surname> <given-names>J</given-names></name><name><surname>Schneider</surname> <given-names>BL</given-names></name><name><surname>Helmchen</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Behaviour-dependent recruitment of long-range projection neurons in somatosensory cortex</article-title><source>Nature</source><volume>499</volume><fpage>336</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/nature12236</pub-id><pub-id pub-id-type="pmid">23792559</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>TW</given-names></name><name><surname>Wardill</surname> <given-names>TJ</given-names></name><name><surname>Sun</surname> <given-names>Y</given-names></name><name><surname>Pulver</surname> <given-names>SR</given-names></name><name><surname>Renninger</surname> <given-names>SL</given-names></name><name><surname>Baohan</surname> <given-names>A</given-names></name><name><surname>Schreiter</surname> <given-names>ER</given-names></name><name><surname>Kerr</surname> <given-names>RA</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name><name><surname>Looger</surname> <given-names>LL</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Kim</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title><source>Nature</source><volume>499</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1038/nature12354</pub-id><pub-id pub-id-type="pmid">23868258</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneux</surname> <given-names>T</given-names></name><name><surname>Kaszas</surname> <given-names>A</given-names></name><name><surname>Szalay</surname> <given-names>G</given-names></name><name><surname>Katona</surname> <given-names>G</given-names></name><name><surname>Lakner</surname> <given-names>T</given-names></name><name><surname>Grinvald</surname> <given-names>A</given-names></name><name><surname>Rózsa</surname> <given-names>B</given-names></name><name><surname>Vanzetta</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Accurate spike estimation from noisy calcium signals for ultrafast three-dimensional imaging of large neuronal populations in vivo</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12190</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12190</pub-id><pub-id pub-id-type="pmid">27432255</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneux</surname> <given-names>T</given-names></name><name><surname>Kempf</surname> <given-names>A</given-names></name><name><surname>Daret</surname> <given-names>A</given-names></name><name><surname>Ponsot</surname> <given-names>E</given-names></name><name><surname>Bathellier</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Temporal asymmetries in auditory coding and perception reflect multi-layered nonlinearities</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12682</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12682</pub-id><pub-id pub-id-type="pmid">27580932</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driver</surname> <given-names>J</given-names></name><name><surname>Noesselt</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multisensory interplay reveals crossmodal influences on 'sensory-specific' brain regions, neural responses, and judgments</article-title><source>Neuron</source><volume>57</volume><fpage>11</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.12.013</pub-id><pub-id pub-id-type="pmid">18184561</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falchier</surname> <given-names>A</given-names></name><name><surname>Clavagnier</surname> <given-names>S</given-names></name><name><surname>Barone</surname> <given-names>P</given-names></name><name><surname>Kennedy</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Anatomical evidence of multimodal integration in primate striate cortex</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>5749</fpage><lpage>5759</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-13-05749.2002</pub-id><pub-id pub-id-type="pmid">12097528</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname> <given-names>Y</given-names></name><name><surname>Tucciarone</surname> <given-names>JM</given-names></name><name><surname>Espinosa</surname> <given-names>JS</given-names></name><name><surname>Sheng</surname> <given-names>N</given-names></name><name><surname>Darcy</surname> <given-names>DP</given-names></name><name><surname>Nicoll</surname> <given-names>RA</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Stryker</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A cortical circuit for gain control by behavioral state</article-title><source>Cell</source><volume>156</volume><fpage>1139</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2014.01.050</pub-id><pub-id pub-id-type="pmid">24630718</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname> <given-names>X</given-names></name><name><surname>Wehr</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A coding transformation for temporally structured sounds within auditory cortical neurons</article-title><source>Neuron</source><volume>86</volume><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.004</pub-id><pub-id pub-id-type="pmid">25819614</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Is neocortex essentially multisensory?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>278</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.04.008</pub-id><pub-id pub-id-type="pmid">16713325</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Henze</surname> <given-names>DA</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name><name><surname>Hirase</surname> <given-names>H</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Accuracy of tetrode spike separation as determined by simultaneous intracellular and extracellular measurements</article-title><source>Journal of Neurophysiology</source><volume>84</volume><fpage>401</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.84.1.401</pub-id><pub-id pub-id-type="pmid">10899214</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hattori</surname> <given-names>R</given-names></name><name><surname>Kuchibhotla</surname> <given-names>KV</given-names></name><name><surname>Froemke</surname> <given-names>RC</given-names></name><name><surname>Komiyama</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Functions and dysfunctions of neocortical inhibitory neuron subtypes</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1199</fpage><lpage>1208</lpage><pub-id pub-id-type="doi">10.1038/nn.4619</pub-id><pub-id pub-id-type="pmid">28849791</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hromádka</surname> <given-names>T</given-names></name><name><surname>Deweese</surname> <given-names>MR</given-names></name><name><surname>Zador</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Sparse representation of sounds in the unanesthetized auditory cortex</article-title><source>PLOS Biology</source><volume>6</volume><elocation-id>e16</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060016</pub-id><pub-id pub-id-type="pmid">18232737</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ibrahim</surname> <given-names>LA</given-names></name><name><surname>Mesik</surname> <given-names>L</given-names></name><name><surname>Ji</surname> <given-names>XY</given-names></name><name><surname>Fang</surname> <given-names>Q</given-names></name><name><surname>Li</surname> <given-names>HF</given-names></name><name><surname>Li</surname> <given-names>YT</given-names></name><name><surname>Zingg</surname> <given-names>B</given-names></name><name><surname>Zhang</surname> <given-names>LI</given-names></name><name><surname>Tao</surname> <given-names>HW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cross-Modality sharpening of visual cortical processing through Layer-1-Mediated inhibition and disinhibition</article-title><source>Neuron</source><volume>89</volume><fpage>1031</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.01.027</pub-id><pub-id pub-id-type="pmid">26898778</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Innocenti</surname> <given-names>GM</given-names></name><name><surname>Berbel</surname> <given-names>P</given-names></name><name><surname>Clarke</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Development of projections from auditory to visual areas in the cat</article-title><source>The Journal of Comparative Neurology</source><volume>272</volume><fpage>242</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1002/cne.902720207</pub-id><pub-id pub-id-type="pmid">2456313</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issa</surname> <given-names>JB</given-names></name><name><surname>Haeffele</surname> <given-names>BD</given-names></name><name><surname>Agarwal</surname> <given-names>A</given-names></name><name><surname>Bergles</surname> <given-names>DE</given-names></name><name><surname>Young</surname> <given-names>ED</given-names></name><name><surname>Yue</surname> <given-names>DT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multiscale optical Ca2+ imaging of tonal organization in mouse auditory cortex</article-title><source>Neuron</source><volume>83</volume><fpage>944</fpage><lpage>959</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.009</pub-id><pub-id pub-id-type="pmid">25088366</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iurilli</surname> <given-names>G</given-names></name><name><surname>Ghezzi</surname> <given-names>D</given-names></name><name><surname>Olcese</surname> <given-names>U</given-names></name><name><surname>Lassi</surname> <given-names>G</given-names></name><name><surname>Nazzaro</surname> <given-names>C</given-names></name><name><surname>Tonini</surname> <given-names>R</given-names></name><name><surname>Tucci</surname> <given-names>V</given-names></name><name><surname>Benfenati</surname> <given-names>F</given-names></name><name><surname>Medini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sound-driven synaptic inhibition in primary visual cortex</article-title><source>Neuron</source><volume>73</volume><fpage>814</fpage><lpage>828</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.026</pub-id><pub-id pub-id-type="pmid">22365553</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanold</surname> <given-names>PO</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name><name><surname>Polley</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Local versus global scales of organization in auditory cortex</article-title><source>Trends in Neurosciences</source><volume>37</volume><fpage>502</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2014.06.003</pub-id><pub-id pub-id-type="pmid">25002236</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Petkov</surname> <given-names>CI</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual modulation of neurons in auditory cortex</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>1560</fpage><lpage>1574</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm187</pub-id><pub-id pub-id-type="pmid">18180245</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual enhancement of the information representation in auditory cortex</article-title><source>Current Biology</source><volume>20</volume><fpage>19</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.10.068</pub-id><pub-id pub-id-type="pmid">20036538</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larkum</surname> <given-names>ME</given-names></name><name><surname>Zhu</surname> <given-names>JJ</given-names></name><name><surname>Sakmann</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A new cellular mechanism for coupling inputs arriving at different cortical layers</article-title><source>Nature</source><volume>398</volume><fpage>338</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1038/18686</pub-id><pub-id pub-id-type="pmid">10192334</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larkum</surname> <given-names>ME</given-names></name><name><surname>Waters</surname> <given-names>J</given-names></name><name><surname>Sakmann</surname> <given-names>B</given-names></name><name><surname>Helmchen</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Dendritic spikes in apical dendrites of neocortical layer 2/3 pyramidal neurons</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>8999</fpage><lpage>9008</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1717-07.2007</pub-id><pub-id pub-id-type="pmid">17715337</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>S</given-names></name><name><surname>Kruglikov</surname> <given-names>I</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Fishell</surname> <given-names>G</given-names></name><name><surname>Rudy</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A disinhibitory circuit mediates motor integration in the somatosensory cortex</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1662</fpage><lpage>1670</lpage><pub-id pub-id-type="doi">10.1038/nn.3544</pub-id><pub-id pub-id-type="pmid">24097044</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leinweber</surname> <given-names>M</given-names></name><name><surname>Ward</surname> <given-names>DR</given-names></name><name><surname>Sobczak</surname> <given-names>JM</given-names></name><name><surname>Attinger</surname> <given-names>A</given-names></name><name><surname>Keller</surname> <given-names>GB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A sensorimotor circuit in mouse cortex for visual flow predictions</article-title><source>Neuron</source><volume>96</volume><elocation-id>1204</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.009</pub-id><pub-id pub-id-type="pmid">29216453</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Letzkus</surname> <given-names>JJ</given-names></name><name><surname>Wolff</surname> <given-names>SB</given-names></name><name><surname>Meyer</surname> <given-names>EM</given-names></name><name><surname>Tovote</surname> <given-names>P</given-names></name><name><surname>Courtin</surname> <given-names>J</given-names></name><name><surname>Herry</surname> <given-names>C</given-names></name><name><surname>Lüthi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A disinhibitory microcircuit for associative fear learning in the auditory cortex</article-title><source>Nature</source><volume>480</volume><fpage>331</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1038/nature10674</pub-id><pub-id pub-id-type="pmid">22158104</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Letzkus</surname> <given-names>JJ</given-names></name><name><surname>Wolff</surname> <given-names>SB</given-names></name><name><surname>Lüthi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Disinhibition, a circuit mechanism for associative learning and memory</article-title><source>Neuron</source><volume>88</volume><fpage>264</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.024</pub-id><pub-id pub-id-type="pmid">26494276</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lomber</surname> <given-names>SG</given-names></name><name><surname>Malhotra</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Double dissociation of 'what' and 'where' processing in auditory cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>609</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1038/nn.2108</pub-id><pub-id pub-id-type="pmid">18408717</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maeda</surname> <given-names>F</given-names></name><name><surname>Kanai</surname> <given-names>R</given-names></name><name><surname>Shimojo</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Changing pitch induced visual motion illusion</article-title><source>Current Biology</source><volume>14</volume><fpage>R990</fpage><lpage>R991</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.11.018</pub-id><pub-id pub-id-type="pmid">15589145</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshel</surname> <given-names>JH</given-names></name><name><surname>Garrett</surname> <given-names>ME</given-names></name><name><surname>Nauhaus</surname> <given-names>I</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specialization of seven mouse visual cortical areas</article-title><source>Neuron</source><volume>72</volume><fpage>1040</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.004</pub-id><pub-id pub-id-type="pmid">22196338</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijer</surname> <given-names>GT</given-names></name><name><surname>Montijn</surname> <given-names>JS</given-names></name><name><surname>Pennartz</surname> <given-names>CMA</given-names></name><name><surname>Lansink</surname> <given-names>CS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Audiovisual modulation in mouse primary visual cortex depends on Cross-Modal stimulus configuration and congruency</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8783</fpage><lpage>8796</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0468-17.2017</pub-id><pub-id pub-id-type="pmid">28821672</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mizrahi</surname> <given-names>A</given-names></name><name><surname>Shalev</surname> <given-names>A</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Single neuron and population coding of natural sounds in auditory cortex</article-title><source>Current Opinion in Neurobiology</source><volume>24</volume><fpage>103</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.09.007</pub-id><pub-id pub-id-type="pmid">24492086</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrill</surname> <given-names>RJ</given-names></name><name><surname>Hasenstaub</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual information present in infragranular layers of mouse auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>2854</fpage><lpage>2862</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3102-17.2018</pub-id><pub-id pub-id-type="pmid">29440554</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeffer</surname> <given-names>CK</given-names></name><name><surname>Xue</surname> <given-names>M</given-names></name><name><surname>He</surname> <given-names>M</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Scanziani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inhibition of inhibition in visual cortex: the logic of connections between molecularly distinct interneurons</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1068</fpage><lpage>1076</lpage><pub-id pub-id-type="doi">10.1038/nn.3446</pub-id><pub-id pub-id-type="pmid">23817549</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pi</surname> <given-names>HJ</given-names></name><name><surname>Hangya</surname> <given-names>B</given-names></name><name><surname>Kvitsiani</surname> <given-names>D</given-names></name><name><surname>Sanders</surname> <given-names>JI</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Kepecs</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical interneurons that specialize in disinhibitory control</article-title><source>Nature</source><volume>503</volume><fpage>521</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1038/nature12676</pub-id><pub-id pub-id-type="pmid">24097352</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname> <given-names>D</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A category-free neural population supports evolving demands during decision-making</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1784</fpage><lpage>1792</lpage><pub-id pub-id-type="doi">10.1038/nn.3865</pub-id><pub-id pub-id-type="pmid">25383902</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recanzone</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Interactions of auditory and visual stimuli in space and time</article-title><source>Hearing Research</source><volume>258</volume><fpage>89</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2009.04.009</pub-id><pub-id pub-id-type="pmid">19393306</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roland</surname> <given-names>B</given-names></name><name><surname>Deneux</surname> <given-names>T</given-names></name><name><surname>Franks</surname> <given-names>KM</given-names></name><name><surname>Bathellier</surname> <given-names>B</given-names></name><name><surname>Fleischmann</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Odor identity coding by distributed ensembles of neurons in the mouse olfactory cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e26337</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26337</pub-id><pub-id pub-id-type="pmid">28489003</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholl</surname> <given-names>B</given-names></name><name><surname>Gao</surname> <given-names>X</given-names></name><name><surname>Wehr</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Nonoverlapping sets of synapses drive on responses and off responses in auditory cortex</article-title><source>Neuron</source><volume>65</volume><fpage>412</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.020</pub-id><pub-id pub-id-type="pmid">20159453</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreiner</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Order and disorder in auditory cortical maps</article-title><source>Current Opinion in Neurobiology</source><volume>5</volume><fpage>489</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(95)80010-7</pub-id><pub-id pub-id-type="pmid">7488851</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreiner</surname> <given-names>CE</given-names></name><name><surname>Winer</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Auditory cortex mapmaking: principles, projections, and plasticity</article-title><source>Neuron</source><volume>56</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.013</pub-id><pub-id pub-id-type="pmid">17964251</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sekuler</surname> <given-names>R</given-names></name><name><surname>Sekuler</surname> <given-names>AB</given-names></name><name><surname>Lau</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Sound alters visual motion perception</article-title><source>Nature</source><volume>385</volume><elocation-id>308</elocation-id><pub-id pub-id-type="doi">10.1038/385308a0</pub-id><pub-id pub-id-type="pmid">9002513</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname> <given-names>L</given-names></name><name><surname>Kamitani</surname> <given-names>Y</given-names></name><name><surname>Shimojo</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Illusions. what you see is what you hear</article-title><source>Nature</source><volume>408</volume><elocation-id>788</elocation-id><pub-id pub-id-type="doi">10.1038/35048669</pub-id><pub-id pub-id-type="pmid">11130706</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slutsky</surname> <given-names>DA</given-names></name><name><surname>Recanzone</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Temporal and spatial dependency of the ventriloquism effect</article-title><source>Neuroreport</source><volume>12</volume><fpage>7</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1097/00001756-200101220-00009</pub-id><pub-id pub-id-type="pmid">11201094</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname> <given-names>YH</given-names></name><name><surname>Kim</surname> <given-names>JH</given-names></name><name><surname>Jeong</surname> <given-names>HW</given-names></name><name><surname>Choi</surname> <given-names>I</given-names></name><name><surname>Jeong</surname> <given-names>D</given-names></name><name><surname>Kim</surname> <given-names>K</given-names></name><name><surname>Lee</surname> <given-names>SH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A neural circuit for auditory dominance over visual perception</article-title><source>Neuron</source><volume>93</volume><fpage>1236</fpage><lpage>1237</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.02.026</pub-id><pub-id pub-id-type="pmid">28279357</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname> <given-names>BE</given-names></name><name><surname>Stanford</surname> <given-names>TR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multisensory integration: current issues from the perspective of the single neuron</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>255</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nrn2331</pub-id><pub-id pub-id-type="pmid">18354398</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trujillo</surname> <given-names>M</given-names></name><name><surname>Measor</surname> <given-names>K</given-names></name><name><surname>Carrasco</surname> <given-names>MM</given-names></name><name><surname>Razak</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Selectivity for the rate of frequency-modulated sweeps in the mouse auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>2825</fpage><lpage>2837</lpage><pub-id pub-id-type="doi">10.1152/jn.00480.2011</pub-id><pub-id pub-id-type="pmid">21849608</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyll</surname> <given-names>S</given-names></name><name><surname>Bonath</surname> <given-names>B</given-names></name><name><surname>Schoenfeld</surname> <given-names>MA</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Ohl</surname> <given-names>FW</given-names></name><name><surname>Noesselt</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural basis of multisensory looming signals</article-title><source>NeuroImage</source><volume>65</volume><fpage>13</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.09.056</pub-id><pub-id pub-id-type="pmid">23032489</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witten</surname> <given-names>IB</given-names></name><name><surname>Knudsen</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Why seeing is believing: merging auditory and visual worlds</article-title><source>Neuron</source><volume>48</volume><fpage>489</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.10.020</pub-id><pub-id pub-id-type="pmid">16269365</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yaksi</surname> <given-names>E</given-names></name><name><surname>Friedrich</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reconstruction of firing rate changes across neuronal populations by temporally deconvolved Ca2+ imaging</article-title><source>Nature Methods</source><volume>3</volume><fpage>377</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1038/nmeth874</pub-id><pub-id pub-id-type="pmid">16628208</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zingg</surname> <given-names>B</given-names></name><name><surname>Hintiryan</surname> <given-names>H</given-names></name><name><surname>Gou</surname> <given-names>L</given-names></name><name><surname>Song</surname> <given-names>MY</given-names></name><name><surname>Bay</surname> <given-names>M</given-names></name><name><surname>Bienkowski</surname> <given-names>MS</given-names></name><name><surname>Foster</surname> <given-names>NN</given-names></name><name><surname>Yamashita</surname> <given-names>S</given-names></name><name><surname>Bowman</surname> <given-names>I</given-names></name><name><surname>Toga</surname> <given-names>AW</given-names></name><name><surname>Dong</surname> <given-names>HW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural networks of the mouse neocortex</article-title><source>Cell</source><volume>156</volume><fpage>1096</fpage><lpage>1111</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2014.02.023</pub-id><pub-id pub-id-type="pmid">24581503</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.44006.021</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Svoboda</surname><given-names>Karel</given-names></name><role>Reviewing Editor</role><aff><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Mizrahi</surname><given-names>Adi</given-names> </name><role>Reviewer</role><aff><institution>The Hebrew University of Jerusalem</institution><country>Israel</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Holtmaat</surname><given-names>Anthony</given-names> </name><role>Reviewer</role><aff><institution>University of Geneva</institution><country>Switzerland</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Context-dependent signaling of coincident auditory and visual events in primary visual cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three reviewers and the evaluation has been overseen by a Reviewing Editor andAndrew King as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Adi Mizrahi (Reviewer #1); Anthony Holtmaat (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Psychophysical data suggest that the auditory and visual systems are intimately connected. This paper is about the circuit mechanisms that support these multimodal interactions. Deneux and colleagues perform an elegant set of experiments characterizing neuronal responses in specific cell types in both auditory and visual cortex to reveal a circuit that could support context-dependent integration of both modalities in the visual cortex. They find that neurons in auditory cortex that project to visual cortex are more likely to be responsive to loud sounds (and to loud sounds getting quieter). They show that these same sounds preferentially modulate the activity of neurons in the visual cortex, but that they do so in a context-dependent and layer-specific manner which depends on the ambient luminance. Some of the data are of high quality and the authors perform a range of controls that lend rigor and confidence to their study. This will be of great interest to a wide audience interested in sensory integration and cortical circuits.</p><p>However, the reviewers noted some major concerns that need to be addressed before publication. The reviewers believe that these issues can be addressed in a couple of months.</p><p>Conceptual issues:</p><p>Specificity for &quot;loud onsets sounds&quot; has not been demonstrated. Although the effects they describe may be stronger for loud onsets than for quiet onsets, the results seem more of a bias within a continuous distribution rather than true specificity.</p><p>Another major concern relates to the strong conclusion that the gating of context must occur in the visual cortex instead of in the inputs from the auditory cortex to the visual cortex. This does not seem supported by the data.</p><p>These concerns can be addressed by toning down the conclusions in the paper.</p><p>Essential revisions:</p><p>The concerns listed below require some changes in analysis or data presentation and possibly some experiments that can be done rapidly.</p><p>1) The authors state that the majority of V1-projecting neurons in AC originate in L5. However, when they image neurons in AC to monitor sound-evoked responses they compare these responses to L2/3 neurons. What was the rationale behind this comparison? Wouldn't it have been more interesting to compare with the 'overall' population in L5? It is critical to provide images and fluorescence traces in L5 neurons. Along similar lines, the authors do not describe all differences in the response types between V1-projecting AC neurons and the control population. It seems that cluster 7 (OFF-response type) is not present in V1-procting neurons. More histological data needs to be provided to sow which other targets the AC neurons have. This needs to be cleared up.</p><p>Also, in the realm of image analysis, the hierarchical clustering should be unpacked in a supplemental figure. How similar are neurons in each cluster? How much variance do the clusters explain etc.</p><p>It would have been more convincing to show data from mice where both V1-projecting and those that do not are imaged in the same mice (you already have the tools to do this). Imaging separate neurons in separate experiments (with and without specificity) is less compelling. It’s not absolutely necessary, but if possible, such an experiment (even for a limited number of example) could strengthen these results.</p><p>2) The new aspects of the modulation described here relate to the inhibition in darkness vs excitation in light, and the supralinear responses between perceptually matching stimuli. Therefore, the inhibitory responses need to be understood well, especially given that the 'switch' in the model strongly hinges on this finding.</p><p>However, the inhibitory effects of sound in darkness are a bit enigmatic and not very well supported by the figures. The authors provide mean deconvolved traces, but these are difficult to digest in the context of these types of responses, as this assumes that there is high baseline activity in V1 in darkness. Either there is a general but very consistent small decrease in all neurons, or a decreased response in a few neurons that are highly active under darkness – but then why the low variance.</p><p>For negative responses it is important to exclude the possibilities that technical issues have seeped in. For example, can the authors rule out changes due to vertical movements; and how much does the neuropil signal subtraction affect these responses? 0.7Fnp is a general and accepted rule for neuropil subtraction. However, neutral responses (i.e. no change) might be very sensitive to the 0.7x threshold (especially in densely labeled populations). Could the authors 'play' with these parameters to see how they affect the outcome (e.g. same analysis with and without NP correction or varying thresholds), and convincingly show at the level of individual neurons that their responses were indeed reduced?</p><p>Altogether, it is essential to provide example images of groups of neurons (preferably time lapse images) and traces of individual neurons. They should also compare baseline neuronal activity of those neurons that are inhibited versus those who are not, and possibly perform movement (z-plane) correction in images in which they have neurons labeled in red.</p><p>3) The authors try and exclude the possibility that a difference in arousal state between the dark and light trials could explain the V1 sign switching. However, it is unclear that closing one eye could simply test the influence of arousal, as this would by itself represent yet a different state of arousal. It is not simply a matter of being in light or darkness, as there are many factors in an experimental setup that determine arousal.</p><p>4) In Figure 4, the authors inhibited AC to test whether auditory responses in V1 are caused by AC projections. Where are the statistical comparisons for the DREADD experiments (n=3)? The authors report that this showed a &quot;similar effect, but less robust&quot; as compared to the muscimol experiment. Whereas this trend might be true for the experiments under light, this remains inconclusive for the DREADD experiments in darkness since in one animal the average responses were drastically reduced. The authors should report the statistical comparison for the DREADD experiment and increase the n if they feel that this addition is necessary to support the conclusions.</p><p>5) Why opt out of showing 'sound only' averages in Figure 6D? For unimodal stimuli was there any background stimulus in either modality (e.g. for sound only, dim light; and for light only, white noise)? If not, why is the sound-only inhibitory response in V1 (as seen in Figure 5) not reproduced in Figure 6?</p><p>6) The results of what they call context are the most interesting part of the paper. The claims of effect-specificity to loud onset sounds and looming stimuli are too strong. After all, they only tested a limited amount of stimuli, both auditory and visual. And even within this limited set, the effects are not binary.</p><p>7) The value of the minimal model of Figure 7 is unclear. Reproducing the empirical results with a model is a good starting point but eventually it has to provide something more (e.g. some hypotheses to test in the context of mechanism). Do they suggest specific biophysical mechanisms of the neurons are involved? This should be clarified.</p><p>8) The authors' major conclusion is that (from the Abstract) &quot;a small number of layer 1 interneurons gates this cross-modal information flow&quot;. They then design a model to demonstrate how this might work through the application of distinct gain conditions and a non-linear threshold. However, another possibility (as the authors acknowledge in the Discussion section) is that the gating might occur in the auditory cortex such that the inputs to the L1 population are only active in the dark. Evidence that the A1-&gt;V1 population is insensitive to ambient light conditions would add significant support to the authors assumed model.</p><p>9) The authors' model suggests that ambient light alters the excitability (and therefore gain) of L1 interneurons. This is a hypothesis that the authors could test by measuring F in baseline conditions (in the absence of auditory stimulation) in the light and dark. Evidence that the L1 neurons that are driven only in the dark have higher baseline F in the dark (while other less selective L1 interneurons do not show such strong modulation) would significantly strengthen the authors' argument.</p><p>10 The authors' proposed model assumes a functionally homogeneous input to V1. However, while the majority of V1-&gt;A1 neurons prefer down-ramp sounds, there are still a significant number that respond to up-ramps. In fact, up-ramp sounds are sufficient to drive suppression in the dark, though unlike down-ramp sounds do not evoke either excitation or inhibition in the light (at least not on average as shown in Figure 3B). This suggests that down-ramp preferring neurons may only contact L1 interneurons and not provide direct excitation to L2/3. Thus, the authors should make it clear that there are anatomical specializations that might also support the observed gating.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.44006.022</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Psychophysical data suggest that the auditory and visual systems are intimately connected. This paper is about the circuit mechanisms that support these multimodal interactions. Deneux and colleagues perform an elegant set of experiments characterizing neuronal responses in specific cell types in both auditory and visual cortex to reveal a circuit that could support context-dependent integration of both modalities in the visual cortex. They find that neurons in auditory cortex that project to visual cortex are more likely to be responsive to loud sounds (and to loud sounds getting quieter). They show that these same sounds preferentially modulate the activity of neurons in the visual cortex, but that they do so in a context-dependent and layer-specific manner which depends on the ambient luminance. Some of the data are of high quality and the authors perform a range of controls that lend rigor and confidence to their study. This will be of great interest to a wide audience interested in sensory integration and cortical circuits.</p><p>However, the reviewers noted some major concerns that need to be addressed before publication. The reviewers believe that these issues can be addressed in a couple of months.</p></disp-quote><p>To address these concerns, we have provided three new sets of experiments (imaging in layer 5, electrophysiology in layer 5, and imaging of V1 projecting cells in AC in lit vs dark conditions). We also performed the requested complementary analysis</p><disp-quote content-type="editor-comment"><p>Conceptual issues:</p><p>Specificity for &quot;loud onsets sounds&quot; has not been demonstrated. Although the effects they describe may be stronger for loud onsets than for quiet onsets, the results seem more of a bias within a continuous distribution rather than true specificity.</p></disp-quote><p>We agree that the results describe a different bias in the distribution of sound response types, rather than a strict specificity.</p><p>We thus carefully checked the text and made several changes in the Abstract and main text to avoid conveying the impression that there is strict specificity. For example, we have replaced ‘specific’ by ‘preferential’, wherever meaningful, or described, in accordance with the data, that loud onsets are a dominant feature among other features encoded in V1-projecting cells. Below are a few examples of modified text:</p><p>Abstract: “Using two-photon calcium imaging in identified cell types in awake, head-fixed mice, we show that, among the basic features of a sound envelope, loud sound onsets are a dominant feature coded by the auditory cortex neurons projecting to primary visual cortex (V1). […] when sound input coincides with a visual stimulus, visual responses are boosted in V1, most strongly after loud sound onsets.”</p><p>Introduction: “and [we] showed that, V1-projecting neurons are predominantly tuned to loud onsets while other tested envelope features are less prominently represented, at least when comparing with supragranular cortical layers that lack V1-projecting cells.…this mechanism also allows for a boosting of visual responses together with auditory events, particularly when those include loud sound onsets, increasing the saliency of visual events that co-occur with abrupt sounds.”</p><p>Results, subsection “Abrupt sound onsets are a dominant feature encoded by V1-projecting neurons”</p><p>Discussion section: “First, we showed that V1-projecting neurons have a different bias in the distribution of sound envelope features they encode, as compared to layer 2/3 neurons in AC and also, although to a lesser extent, as compared to layer 5 in which they are located. In particular, the V1’projecting neuron population predominantly encodes loud sound onsets.”</p><p>Plus a larger more detailed paragraph describing the changes in response type distribution for all cell subpopulations.</p><disp-quote content-type="editor-comment"><p>Another major concern relates to the strong conclusion that the gating of context must occur in the visual cortex instead of in the inputs from the auditory cortex to the visual cortex. This does not seem supported by the data.</p></disp-quote><p>We have done new experiments, recording V1 projecting cells in auditory cortex in light and dark conditions. These experiments show that the illumination context does not modulate activity of auditory cortex neurons. Thus, the contextual gating or modulation must happen in V1.</p><p>The results are shown in Figure 3F. Note that we have used this new dataset for the clustering analysis of V1 projecting AC neurons in Figure 1 which is thus also changed (and extended with data from AC layer 5).</p><disp-quote content-type="editor-comment"><p>These concerns can be addressed by toning down the conclusions in the paper.</p></disp-quote><p>We have toned down the conclusions related to specificity of loud onset. See our response to the first point above.</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The concerns listed below require some changes in analysis or data presentation and possibly some experiments that can be done rapidly.</p><p>1) The authors state that the majority of V1-projecting neurons in AC originate in L5. However, when they image neurons in AC to monitor sound-evoked responses they compare these responses to L2/3 neurons. What was the rationale behind this comparison? Wouldn't it have been more interesting to compare with the 'overall' population in L5? It is critical to provide images and fluorescence traces in L5 neurons. Along similar lines, the authors do not describe all differences in the response types between V1-projecting AC neurons and the control population. It seems that cluster 7 (OFF-response type) is not present in V1-procting neurons. More histological data needs to be provided to sow which other targets the AC neurons have. This needs to be cleared up.</p></disp-quote><p>We failed to achieve simultaneous imaging of V1 projecting and non-projecting cells in AC (see below), a separation which is actually hard to guaranty with viral techniques (partial infection). Thus, we had focused on a comparison between L2/3 (that contains almost no V1 projecting cells) in AC and identified V1 projecting cells. But indeed this leaves open the question whether L2/3 and L5 cells encode a different distribution of features. We have thus now provided in Figure 1 a quantification of the distribution of sound envelop features in upper L5. It shows, among other differences, an increase of the fraction of loud ON responses compared to layer 2/3. We clarified the conclusions by emphasizing the dominance of loud ON response in V1 projecting neurons (half of the cells), a situation different from in layer 2/3, but maybe partially inherited from an enrichment of loud ON responses in layer 5.</p><p>We observe differences in the distribution of responses type across all three datasets. For example, as the referees point out, quiet OFF neurons (former cluster 7) are generally very rare (see also Deneux et al., 2016) and are thus not seen in in all dataset, possibly because it was not picked up by the clustering given their sparseness. We provide now a small paragraph on these discrepancies in the Discussion:</p><p>“The shifts in the distribution of sound envelope features that we observed across layers and cell subpopulations is an interesting case of coding bias in different neural populations of auditory cortex (Figure 1). As observed earlier, in layer 2/3, there is a balance between the main neuron types that signal the beginning and the end of sounds with equivalent fractions of neurons responding to quiet or loud onsets and loud offsets (Figure 1F, note that ‘tonic+loud OFF’ neurons are included in this count. Quiet offset neurons are rare and not robustly detected across datasets. Detection of tonic responses by clustering is also variable.). In upper layer 5, the distribution becomes more biased towards loud onsets at the expense of offset neurons, a trend that is further increased in the layer 5-located V1-projecting cells in which half of the neurons respond to loud onsets and 10% of the cells to loud offsets.”</p><p>Histological data showing which other targets the AC neurons have is freely available online on the website of the Allen Brain Institute and of the mouse i-connectome project (www.mouseconnectome.org). We did not feel we can provide data of better quality during the time frame of the review process.</p><disp-quote content-type="editor-comment"><p>Also, in the realm of image analysis, the hierarchical clustering should be unpacked in a supplemental figure. How similar are neurons in each cluster? How much variance do the clusters explain etc.</p></disp-quote><p>In this revision we have unpacked the clustering of the 3 extended/new dataset by:</p><p>a) Allowing more overclustering in order to show more of the variability that appears inside the functional clusters we identified. We have also used more cluster labels, similar to our previous study on ramping sounds (Deneux et al., 2016).</p><p>b) we have provided response correlation matrices from which variability inside clusters can be visually estimated (see figure 1 and Figure 1—figure supplement 1)</p><p>c) we have measured the fraction of explained variance (about 50% in all cases) and provided the values in the result section.</p><disp-quote content-type="editor-comment"><p>It would have been more convincing to show data from mice where both V1-projecting and those that do not are imaged in the same mice (you already have the tools to do this). Imaging separate neurons in separate experiments (with and without specificity) is less compelling. It’s not absolutely necessary, but if possible, such an experiment (even for a limited number of example) could strengthen these results.</p></disp-quote><p>Originally, we had started our experiments with this approach, labeling V1 projecting neurons with TdTomato, while AAV1-syn-GCAMP6s was used to label broadly the neurons of AC. However, it turned out that GCAMP6s labelling was clearly weaker in the TdT labelled neurons than in the rest of the layer 5 population. Hence, it was not really possible to image them, because their fluorescence was covered by the fluorescence of their neighbors (neuropil signal). This effect was magnified by the slightly decreased spatial resolution and contrast of imaging data in Layer 5 obtained with the bulk AAV1-syn-GCAMP6 injection strategy that also tends to label L2/3 (thus increasing the contribution of out-of-focus light at large depths). Together these technical limitations prevented us from providing data in which cells from the same field of view are imaged and sorted according to their projection target. Note also that absence of retrograde labelling does not indicate absence of projection to V1.</p><disp-quote content-type="editor-comment"><p>2) The new aspects of the modulation described here relate to the inhibition in darkness vs excitation in light, and the supralinear responses between perceptually matching stimuli. Therefore, the inhibitory responses need to be understood well, especially given that the 'switch' in the model strongly hinges on this finding.</p><p>However, the inhibitory effects of sound in darkness are a bit enigmatic and not very well supported by the figures. The authors provide mean deconvolved traces, but these are difficult to digest in the context of these types of responses, as this assumes that there is high baseline activity in V1 in darkness. Either there is a general but very consistent small decrease in all neurons, or a decreased response in a few neurons that are highly active under darkness – but then why the low variance.</p></disp-quote><p>As we now show in Figure 3—figure supplement 1 panel B, the inhibition is consistent and wide spread across neurons. This explains the low variance.</p><disp-quote content-type="editor-comment"><p>For negative responses it is important to exclude the possibilities that technical issues have seeped in. For example, can the authors rule out changes due to vertical movements; and how much does the neuropil signal subtraction affect these responses? 0.7Fnp is a general and accepted rule for neuropil subtraction. However, neutral responses (i.e. no change) might be very sensitive to the 0.7x threshold (especially in densely labeled populations). Could the authors 'play' with these parameters to see how they affect the outcome (e.g. same analysis with and without NP correction or varying thresholds), and convincingly show at the level of individual neurons that their responses were indeed reduced?</p></disp-quote><p>As we now show in Figure 3—figure supplement 1 panel A, the inhibition is almost unchanged in the absence of neuropil correction as compared to -0.7Fnp. Thus, the neuropil correction does not introduce supplementary inhibition. Excessive neuropil signals correction (-1*Fnp) lowers the basal fluorescence and thereby increase ΔF/F values, but inhibition is preserved.</p><disp-quote content-type="editor-comment"><p>Altogether, it is essential to provide example images of groups of neurons (preferably time lapse images) and traces of individual neurons. They should also compare baseline neuronal activity of those neurons that are inhibited versus those who are not, and possibly perform movement (z-plane) correction in images in which they have neurons labeled in red.</p></disp-quote><p>We have done efficient z-plane correction using the green channel. We do not have simultaneous imaging of the red channel in any dataset (the red image was acquired before at a different wavelength), so we cannot use the red-labelled neurons to perform horizontal motion artefact correction.</p><p>We now provide in Figure 3—figure supplement 1 panel C, single trial raw traces and spontaneous activity for representative sample neurons (inhibited, excited by sounds). The inhibitory example shows that inhibition is visible in single trials although not systematically. Together with this inhibition we observe a decrease in the occurrence of occasional transients. This suggests that there is a basal level of activity and that inhibition is revealed as a modulation of this activity level. To validate this idea, we run simulations of GCAMP6s signals based on published parameters for transients resulting from a single spike and supposing basal Poisson firing (different rates were tested). We simulated an inhibition by removing spikes of the Poisson train in a 1s time bin. We show (Figure 3—figure supplement 1 panel D) that resulting calcium signals and their trial average are highly compatible with signals observed in vivo for firing rate as low as 1Hz. Hence, observed inhibition likely results from a consistent modulation of a low basal firing rate, compatible with published basal firing rate values in awake cortex (e.g. loose patch recordings indicate a distribution around 2-3Hz Hromadka, Zador et al., 2008).</p><p>This conclusion is stated in the Results section “The observed inhibition was seen consistently across cells and sound presentations, likely corresponding to a transient decrease in basal firing rates (Figure 3—figure supplement 1).”</p><disp-quote content-type="editor-comment"><p>3) The authors try and exclude the possibility that a difference in arousal state between the dark and light trials could explain the V1 sign switching. However, it is unclear that closing one eye could simply test the influence of arousal, as this would by itself represent yet a different state of arousal. It is not simply a matter of being in light or darkness, as there are many factors in an experimental setup that determine arousal.</p></disp-quote><p>We agree that this experiment does not fully address the question of arousal. We now moved the results in Figure 3—figure supplement 2, and together with a sentence that does not mention arousal: “Interestingly, hiding the contralateral eye was sufficient to obtain the same inhibition as in darkness (Figure 3—figure supplement 2), suggesting a possible role of direct unilateral visual inputs in the context dependence of auditory responses in V1”</p><disp-quote content-type="editor-comment"><p>4) In Figure 4, the authors inhibited AC to test whether auditory responses in V1 are caused by AC projections. Where are the statistical comparisons for the DREADD experiments (n=3)? The authors report that this showed a &quot;similar effect, but less robust&quot; as compared to the muscimol experiment. Whereas this trend might be true for the experiments under light, this remains inconclusive for the DREADD experiments in darkness since in one animal the average responses were drastically reduced. The authors should report the statistical comparison for the DREADD experiment and increase the n if they feel that this addition is necessary to support the conclusions.</p></disp-quote><p>The referee is right to point out that the number of mice used in the DREADD experiment is too low for accurate statistical analysis across animals. Yet the observations of DREADD effects are quite robust for the recorded population if one considers variability across sound repeats, as displayed in the standard error of the average response traces. We thus now provide in Figure 4C the statistics across n=20 independent repeats of the stimulus presentations for the entire population of neurons recorded. This shows high significance. For consistency, the variations across mice are shown in Figure 4—figure supplement 1, together with a statement that DREADD silencing is less robust across animals as expected from variations in targeting efficiency in the double viral injection approach, which likely touches only a subset of all neurons projecting to A1 (virus efficiency and tropism; also the CAV-Cre virus is injected in a single spot V1).</p><p>As the DREADD experiments essentially corroborates the muscimol experiments and are thus not crucial for our conclusions, we did not increase the n.</p><p>New text: “We also used targeted chemogenetic inhibition [...] This produced a decrease of sound responses which was consistent across sound repetitions for the imaged neural population (Figure 4C). However, probably because this strategy impacts an incomplete subset of all V1-projecting neurons, the effects were smaller and more variable across experiments (Figure 4—figure supplement 1). “</p><disp-quote content-type="editor-comment"><p>5) Why opt out of showing 'sound only' averages in Figure 6D? For unimodal stimuli was there any background stimulus in either modality (e.g. for sound only, dim light; and for light only, white noise)? If not, why is the sound-only inhibitory response in V1 (as seen in Figure 5) not reproduced in Figure 6?</p></disp-quote><p>The sound only responses in muscimol and saline (both in light and dark conditions) were recorded in the same experiment and are already shown in Figure 4. Adding them in Figure 6D would mean overcrowding the figure with 4 more curves in each plot (6 curves in total), this is the reason why we did not reproduce them.</p><disp-quote content-type="editor-comment"><p>6) The results of what they call context are the most interesting part of the paper. The claims of effect-specificity to loud onset sounds and looming stimuli are too strong. After all, they only tested a limited amount of stimuli, both auditory and visual. And even within this limited set, the effects are not binary.</p></disp-quote><p>We have toned down the conclusions throughout the paper to avoid conveying the wrong impression that the effects are just restricted to Loud onset sounds (see our response above).</p><disp-quote content-type="editor-comment"><p>7) The value of the minimal model of Figure 7 is unclear. Reproducing the empirical results with a model is a good starting point but eventually it has to provide something more (e.g. some hypotheses to test in the context of mechanism). Do they suggest specific biophysical mechanisms of the neurons are involved? This should be clarified.</p></disp-quote><p>After describing our model, we now propose a biophysical implementation for the two key aspects:</p><p>- For the modulation of a subpopulation of L1 interneurons:</p><p>“Biophysically, this mechanism requires a tonic inhibitory drive to the identified subset of L1 interneurons which could be provided by another inhibitory population. Interestingly, we observed that the subpopulation of L1 interneurons that are less responsive to sounds in light also have a smaller baseline fluorescence in light, an effect that is not observed for other L1 interneurons (Figure 7B). This could be a possible manifestation of the tonic drive hypothesized in our model.”</p><p>- For the low and high gain behavior of L2/3 neurons:</p><p>“While less conventional than a regular spike threshold, this mechanism could be biophysically implemented with non-linear dendritic processing. Apical dendrites of layer 2/3 pyramidal cells are known to produce large calcium spikes which boost somatic output when somatic activity coincides with excitatory inputs arriving in the apical tree (Larkum et al., 2007). As AC axon terminals are concentrated in layer 1 where they likely contact the apical dendrites of pyramidal cells, sound-driven boosting of visual responses could rely on this dendritic phenomenon.”</p><disp-quote content-type="editor-comment"><p>8) The authors' major conclusion is that (from the Abstract) &quot;a small number of layer 1 interneurons gates this cross-modal information flow&quot;. They then design a model to demonstrate how this might work through the application of distinct gain conditions and a non-linear threshold. However, another possibility (as the authors acknowledge in the Discussion section) is that the gating might occur in the auditory cortex such that the inputs to the L1 population are only active in the dark. Evidence that the A1-&gt;V1 population is insensitive to ambient light conditions would add significant support to the authors assumed model.</p></disp-quote><p>We have done new experiments, recording V1 projecting cells in auditory cortex in light and dark conditions. These experiments show that illumination context does not modulate activity of auditory cortex neurons. Therefore, we continue to believe that this contextual gating or modulation happens in V1.</p><p>The results are shown in Figure 3F and commented in the result section: “We then wondered whether the context-dependence was a property arising in the circuit of V1 or if it is due to a modulation of auditory inputs by the light context. We imaged sound responses in V1-projecting neurons of auditory cortex specifically labelled using the CAV-Cre retrograde virus approach (see Figure 1). We observed no response modulation between the dark and lit conditions (Figure 3F), indicating that context-dependent modulation arises in V1.”</p><disp-quote content-type="editor-comment"><p>9) The authors' model suggests that ambient light alters the excitability (and therefore gain) of L1 interneurons. This is a hypothesis that the authors could test by measuring F in baseline conditions (in the absence of auditory stimulation) in the light and dark. Evidence that the L1 neurons that are driven only in the dark have higher baseline F in the dark (while other less selective L1 interneurons do not show such strong modulation) would significantly strengthen the authors' argument.</p></disp-quote><p>We have done this analysis. Indeed, L1 interneurons that are responding less in the light display also a higher baseline in the dark, in line with our model. We now show this result in an inset of Figure 7 and describe it in the Results section:</p><p>“provided that they are close to activation threshold in the dark and that inhibition by visual inputs brings them well below threshold in lit condition, interneurons will be less active in the light than in the dark. […] Biophysically, this mechanism requires a tonic inhibitory drive to the identified subset of L1 interneurons which could be provided by another inhibitory population. Interestingly, we observed that the subpopulation of L1 interneurons that are less responsive to sounds in light also have a smaller baseline fluorescence in light, an effect that is not observed for other L1 interneurons (Figure 7B). This could be a possible manifestation of the tonic drive hypothesized in our model”</p><disp-quote content-type="editor-comment"><p>10) The authors' proposed model assumes a functionally homogeneous input to V1. However, while the majority of V1-&gt;A1 neurons prefer down-ramp sounds, there are still a significant number that respond to up-ramps. In fact, up-ramp sounds are sufficient to drive suppression in the dark, though unlike down-ramp sounds do not evoke either excitation or inhibition in the light (at least not on average as shown in Figure 3B). This suggests that down-ramp preferring neurons may only contact L1 interneurons and not provide direct excitation to L2/3. Thus, the authors should make it clear that there are anatomical specializations that might also support the observed gating.</p></disp-quote><p>This is a good point, which we added to the Discussion section. “As a result of these fine adjustments of feature distributions, layer 2/3 neurons in V1 are more impacted by abrupt sounds with loud onsets (e.g. down-ramps) than by sounds with softer onsets (e.g. up-ramps). This asymmetry is particularly striking for excitatory effects seen in light or with visual stimuli (Figures 3B and 6), but less pronounced for inhibitory effects in the dark (Figures 3B). It is thus possible that the projection targets also depend on encoded features. For example, L2/3 V1 pyramidal cells may receive projections almost exclusively from AC loud onset neurons while interneurons providing inhibition in the dark still receive a significant fraction of quiet onset and loud offset inputs.”</p></body></sub-article></article>