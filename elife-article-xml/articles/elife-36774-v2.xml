<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">36774</article-id><article-id pub-id-type="doi">10.7554/eLife.36774</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Constraints on neural redundancy</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-108668"><name><surname>Hennig</surname><given-names>Jay A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7982-8553</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-38261"><name><surname>Golub</surname><given-names>Matthew D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4508-0537</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-109451"><name><surname>Lund</surname><given-names>Peter J</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-109452"><name><surname>Sadtler</surname><given-names>Patrick T</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-109449"><name><surname>Oby</surname><given-names>Emily R</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-109450"><name><surname>Quick</surname><given-names>Kristin M</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-18623"><name><surname>Ryu</surname><given-names>Stephen I</given-names></name><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-109453"><name><surname>Tyler-Kabara</surname><given-names>Elizabeth C</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="aff" rid="aff10">10</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-109466"><name><surname>Batista</surname><given-names>Aaron P</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-38262"><name><surname>Yu</surname><given-names>Byron M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2252-6938</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-36482"><name><surname>Chase</surname><given-names>Steven M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4450-6313</contrib-id><email>schase@cmu.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Program in Neural Computation</institution><institution>Carnegie Mellon University</institution><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Center for the Neural Basis of Cognition</institution><institution>Carnegie Mellon University</institution><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Machine Learning Department</institution><institution>Carnegie Mellon University</institution><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Electrical and Computer Engineering</institution><institution>Carnegie Mellon University</institution><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of Biomedical Engineering</institution><institution>Carnegie Mellon University</institution><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Department of Bioengineering</institution><institution>University of Pittsburgh</institution><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution content-type="dept">Department of Neurosurgery</institution><institution>Palo Alto Medical Foundation</institution><addr-line><named-content content-type="city">California</named-content></addr-line><country>United States</country></aff><aff id="aff8"><label>8</label><institution content-type="dept">Department of Electrical Engineering</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">California</named-content></addr-line><country>United States</country></aff><aff id="aff9"><label>9</label><institution content-type="dept">Department of Physical Medicine and Rehabilitation</institution><institution>University of Pittsburgh</institution><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff10"><label>10</label><institution content-type="dept">Department of Neurological Surgery</institution><institution>University of Pittsburgh</institution><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Shea-Brown</surname><given-names>Eric</given-names></name><role>Reviewing Editor</role><aff><institution>University of Washington</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>15</day><month>08</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e36774</elocation-id><history><date date-type="received" iso-8601-date="2018-03-18"><day>18</day><month>03</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2018-08-06"><day>06</day><month>08</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Hennig et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Hennig et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-36774-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.36774.001</object-id><p>Millions of neurons drive the activity of hundreds of muscles, meaning many different neural population activity patterns could generate the same movement. Studies have suggested that these redundant (i.e. behaviorally equivalent) activity patterns may be beneficial for neural computation. However, it is unknown what constraints may limit the selection of different redundant activity patterns. We leveraged a brain-computer interface, allowing us to define precisely which neural activity patterns were redundant. Rhesus monkeys made cursor movements by modulating neural activity in primary motor cortex. We attempted to predict the observed distribution of redundant neural activity. Principles inspired by work on muscular redundancy did not accurately predict these distributions. Surprisingly, the distributions of redundant neural activity and task-relevant activity were coupled, which enabled accurate predictions of the distributions of redundant activity. This suggests limits on the extent to which redundancy may be exploited by the brain for computation.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.36774.002</object-id><title>eLife digest</title><p>When you swing a tennis racket, muscles in your arm contract in a specific sequence. For this to happen, millions of neurons in your brain and spinal cord must fire to make those muscles contract. If you swing the racket a second time, the same muscles in your arm will contract again. But the firing pattern of the underlying neurons will probably be different. This phenomenon, in which different patterns of neural activity generate the same outcome, is called neural redundancy.</p><p>Neural redundancy allows a set of neurons to perform multiple tasks at once. For example, the same neurons may drive an arm movement while simultaneously planning the next activity. But does performing a given task constrain how often different patterns of neural activity can be produced? If so, this would limit whether other tasks could be carried out at the same time. To address this, Hennig et al. trained macaque monkeys to use a brain-computer interface (BCI). This is a device that reads out electrical brain activity and converts it into signals that can be used to control another device. The key advantage of a BCI is that the redundant activity patterns are precisely known. The monkeys learned to use their brain activity, via the BCI, to move a cursor on a computer screen in different directions.</p><p>The results revealed that monkeys could only produce a limited number of different patterns of brain activity for a given BCI cursor movement. This suggests that the ability of a group of neurons to multitask is restricted. For example, if the same set of neurons is involved in both planning and performing movements, then an animal’s ability to plan a future movement will depend on the one it is currently performing.</p><p>BCIs can help patients who have suffered stroke or paralysis. They enable patients to use their brain activity to control a computer or even robotic limbs. Understanding how the brain controls BCIs will help us improve their performance and deepen our knowledge of how the brain plans and performs movements. This might include designing BCIs that allow users to multitask more effectively.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>neural redundancy</kwd><kwd>motor control</kwd><kwd>brain-computer interface</kwd><kwd>neural computation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NCS BCS1533672</award-id><principal-award-recipient><name><surname>Batista</surname><given-names>Aaron P</given-names></name><name><surname>Yu</surname><given-names>Byron M</given-names></name><name><surname>Chase</surname><given-names>Steven M</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 HD071686</award-id><principal-award-recipient><name><surname>Batista</surname><given-names>Aaron P</given-names></name><name><surname>Yu</surname><given-names>Byron M</given-names></name><name><surname>Chase</surname><given-names>Steven M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>Career award IOS1553252</award-id><principal-award-recipient><name><surname>Chase</surname><given-names>Steven M</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>CRCNS R01 NS105318</award-id><principal-award-recipient><name><surname>Batista</surname><given-names>Aaron P</given-names></name><name><surname>Yu</surname><given-names>Byron M</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100005191</institution-id><institution>Craig H. Neilsen Foundation</institution></institution-wrap></funding-source><award-id>280028</award-id><principal-award-recipient><name><surname>Batista</surname><given-names>Aaron P</given-names></name><name><surname>Yu</surname><given-names>Byron M</given-names></name><name><surname>Chase</surname><given-names>Steven M</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>364994</award-id><principal-award-recipient><name><surname>Yu</surname><given-names>Byron M</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004897</institution-id><institution>Pennsylvania Department of Health</institution></institution-wrap></funding-source><award-id>Research Formula Grant SAP 4100077048</award-id><principal-award-recipient><name><surname>Yu</surname><given-names>Byron M</given-names></name><name><surname>Chase</surname><given-names>Steven M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The distribution of redundant neural activity is coupled with task-relevant activity, which may limit the extent to which redundancy can be exploited by the brain for computation.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neural circuits relay information from one population of neurons to another. This relay involves successive stages of downstream neurons reading out the activity of upstream neurons. In many cases, the same activity in the downstream population can be produced by different population activity patterns in the upstream population, a phenomenon termed <italic>neural redundancy</italic>. Redundancy is ubiquitous in neural computation, from sensory input to motor output. For example, during a task where subjects need to discriminate the color of a stimulus while ignoring its orientation (<xref ref-type="bibr" rid="bib41">Mante et al., 2013</xref>), population activity patterns corresponding to the same color but different orientations are read out equivalently, and are therefore redundant. There is mounting evidence that redundancy in readouts may provide various computational benefits. For example, neural redundancy may allow us to prepare movements without executing them (<xref ref-type="bibr" rid="bib31">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Elsayed et al., 2016</xref>), enable stable computation despite unstable neural dynamics (<xref ref-type="bibr" rid="bib11">Driscoll et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">Druckmann and Chklovskii, 2012</xref>; <xref ref-type="bibr" rid="bib45">Murray et al., 2017</xref>) and allow the central nervous system to filter out unwanted noise (<xref ref-type="bibr" rid="bib44">Moreno-Bote et al., 2014</xref>).</p><p>To fully utilize the proposed benefits of neural redundancy, the population activity should be allowed to freely vary, as long as the readout of this activity remains consistent with task demands. This would allow the population activity to perform computations that are not reflected in the readout. However, a commonly held assumption is that neural activity might also be constrained by energetics: All things being equal, if two population activity patterns are read out equivalently, the brain should prefer the pattern that requires less energy to produce (<xref ref-type="bibr" rid="bib35">Laughlin et al., 1998</xref>; <xref ref-type="bibr" rid="bib4">Barlow, 1969</xref>; <xref ref-type="bibr" rid="bib40">Levy and Baxter, 1996</xref>). These two lines of reasoning raise the following questions: What principles guide the production of redundant neural activity patterns? Are there constraints on which redundant activity patterns can be produced? If so, this may limit the extent to which neural circuits can exploit the proposed computational benefits of redundancy.</p><p>Redundancy has been studied extensively in motor control (<xref ref-type="bibr" rid="bib33">Lashley, 1933</xref>; <xref ref-type="bibr" rid="bib5">Bernstein, 1967</xref>), albeit in terms of muscular redundancy rather than neural redundancy. During arm movements, different combinations of muscle activity can lead to the same arm kinematics, meaning these different muscle activity patterns are redundant. Previous work on this muscle redundancy problem has identified two principles guiding the selection of redundant muscle activity. First, because muscle contraction requires energy in the form of ATP, the selected muscle activity should require minimum energy relative to the other redundant options (<xref ref-type="bibr" rid="bib63">Thoroughman and Shadmehr, 1999</xref>; <xref ref-type="bibr" rid="bib30">Huang et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Fagg et al., 2002</xref>). Second, a minimal intervention strategy has been proposed in which subjects control only the aspects of muscle activity that influence the task outcome, and allow for variability in the aspects of muscle activity that do not influence the task outcome (<xref ref-type="bibr" rid="bib55">Scholz and Schöner, 1999</xref>; <xref ref-type="bibr" rid="bib64">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib66">Valero-Cuevas et al., 2009</xref>). To generate movements, the brain not only needs to deal with muscle redundancy, but also <italic>neural</italic> redundancy, which has been less studied.</p><p>One way in which neural redundancy can arise is when there are more elements (neurons or muscles) upstream than downstream. During arm movements, the activity of around thirty muscles in the arm and hand is controlled by tens of thousands of neurons in the spinal cord (<xref ref-type="bibr" rid="bib26">Gray, 1918</xref>; <xref ref-type="bibr" rid="bib18">Feinstein et al., 1955</xref>). Those neurons are in turn influenced by millions of neurons in the primary motor cortex and other motor areas (<xref ref-type="bibr" rid="bib15">Ettema et al., 1998</xref>; <xref ref-type="bibr" rid="bib38">Lemon, 2008</xref>). Thus, the neural control of arm movement is redundant (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), in that different population activity patterns can generate the same movement (<xref ref-type="bibr" rid="bib51">Rokni et al., 2007</xref>; <xref ref-type="bibr" rid="bib1">Ajemian et al., 2013</xref>). Can the principles of muscular redundancy inform our understanding of neural redundancy?</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.36774.003</object-id><label>Figure 1.</label><caption><title>Studying the selection of redundant neural activity.</title><p>(<bold>A</bold>) Millions of neurons in motor cortex drive tens of muscles to move our arms. Thus, different population activity patterns can be redundant, meaning they produce the same muscle activations and movement. (<bold>B</bold>) In a BCI, the mapping between neural activity and movement is defined by the experimenter. A subject modulates the spiking activity of tens of neurons (green rectangle) to control the 2D velocity (<inline-formula><mml:math id="inf1"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle></mml:math></inline-formula>) of a cursor on a screen. (<bold>C</bold>) Example of redundant neural activity in a simplified example where the activity of two neurons (horizontal and vertical axes) drives a 1D cursor velocity (left, L, or right, R). For each of the population activity patterns shown (green squares and circles), the component of the activity along the ‘Output-potent’ axis determines the cursor velocity (e.g. <inline-formula><mml:math id="inf2"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), while the position of this activity along the orthogonal axis (‘Output-null’ axis) has no effect on the cursor’s movement. Activity patterns on the same dotted line (e.g. the two dark green patterns) are redundant, because these patterns have the same output-potent activity and produce the same cursor velocity (e.g. <inline-formula><mml:math id="inf4"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>). (<bold>D</bold>) Example distributions of neural activity along the output-null dimension (corresponding to dotted lines in (<bold>C</bold>)). Each black trace depicts the density of output-null activity observed over the course of an experiment when the cursor velocity was <inline-formula><mml:math id="inf5"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (top) or <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (bottom). The output-null activities of the green symbols from (<bold>C</bold>) are marked for reference. In the actual experiments, there were two output-potent dimensions and eight output-null dimensions. Output-null activity has units of spikes/s, presented relative to the vector of mean activity for each neuron (‘baseline’).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig1-v2"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36774.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Summary of behavior during the 2D center-out BCI task.</title><p>Animals achieved stable control of a computer cursor under two BCI mappings in a 2D center-out task. (<bold>A</bold>) Cursor traces from 50 consecutive trials during three blocks in an example experiment (N2016726). Cursor traces are colored by the corresponding target on each trial. Cursor positions exceeding a cutoff distance from the workspace center are omitted from view. (<bold>B</bold>) Success rate during the same example experiment shown in (<bold>A</bold>), with colors indicating three blocks similar to those shown in (<bold>A</bold>): all trials under the first mapping (black), the first 50 trials under the second mapping (dark gray), and all analyzed trials under the second mapping (red). Success rates were smoothed using a 100-trial moving window. (<bold>C</bold>) Average success rate across all sessions for each of three animals, during the three blocks highlighted in (<bold>B</bold>). Error bars depict mean <inline-formula><mml:math id="inf7"><mml:mo>±</mml:mo></mml:math></inline-formula> SE. (<bold>D</bold>) Same conventions as (<bold>B</bold>), for target acquisition time. Acquisition times were computed during successful trials only. For each session, for analysis we identified a consecutive block of at least 100 trials that showed both substantial learning of the second mapping and consistent behavior. Trials with substantial learning were identified by thresholding the smoothed acquisition times shown here (see Materials and methods). Only correct trials within this block were analyzed. (<bold>E</bold>) Same conventions as (<bold>C</bold>), for acquisition time.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig1-figsupp1-v2"/></fig></fig-group><p>A common challenge in studying neural redundancy is that it is typically not known which neural activity patterns are redundant, because we do not know how downstream neurons or muscles read out information. In this study we overcome this problem by leveraging a brain-computer interface (BCI), in which the activity of dozens of neurons is read out as movements of a cursor on a computer screen (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) (<xref ref-type="bibr" rid="bib62">Taylor et al., 2002</xref>; <xref ref-type="bibr" rid="bib7">Carmena et al., 2003</xref>; <xref ref-type="bibr" rid="bib29">Hochberg et al., 2006</xref>; <xref ref-type="bibr" rid="bib20">Ganguly and Carmena, 2009</xref>; <xref ref-type="bibr" rid="bib21">Gilja et al., 2012</xref>; <xref ref-type="bibr" rid="bib28">Hauschild et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>). A key advantage of a BCI is that the readout of the population activity (termed the BCI mapping) is fully known and defined by the experimenter (<xref ref-type="bibr" rid="bib22">Golub et al., 2016</xref>). This allows us to determine precisely the redundant population activity patterns, which are those that move the cursor in exactly the same way. To illustrate this, consider a simplified example where the activity of two neurons controls a 1D cursor velocity (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). The two dark green activity patterns produce the same cursor movement (<inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), and the two light green patterns produce a different movement (<inline-formula><mml:math id="inf9"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>). We can decompose any population activity pattern into two orthogonal components: output-potent activity and output-null activity (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, black axes) (<xref ref-type="bibr" rid="bib31">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="bib37">Law et al., 2014</xref>). The output-potent component determines the cursor’s movement, whereas the output-null component has no effect on the cursor. Two population activity patterns are redundant if they have the same output-potent activity, but different output-null activity (e.g. the dark green square and circle on the '<inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>' dotted line in <xref ref-type="fig" rid="fig1">Figure 1C</xref>). The question we address here is, which redundant population activity patterns are preferred by the nervous system? To answer this, we assessed the distribution of output-null activity produced during each cursor movement (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), and compared it to what we would expect to observe under each of several candidate hypotheses for explaining neural redundancy.</p><p>We trained three Rhesus macaques to perform a brain-computer interface task in which they controlled the velocity of a cursor on a computer screen by volitionally modulating neural activity in primary motor cortex. To understand the principles guiding the selection of redundant neural activity, we compared the observed distributions of output-null activity to those predicted by three different hypotheses. The first two hypotheses we considered were inspired by studies of muscle redundancy. First, by analogy to minimum energy principles (<xref ref-type="bibr" rid="bib63">Thoroughman and Shadmehr, 1999</xref>; <xref ref-type="bibr" rid="bib30">Huang et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Fagg et al., 2002</xref>), neural activity may minimize unnecessary spiking (<xref ref-type="bibr" rid="bib4">Barlow, 1969</xref>; <xref ref-type="bibr" rid="bib40">Levy and Baxter, 1996</xref>). Second, by analogy to the minimal intervention strategy (<xref ref-type="bibr" rid="bib55">Scholz and Schöner, 1999</xref>; <xref ref-type="bibr" rid="bib64">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib66">Valero-Cuevas et al., 2009</xref>), output-null activity might be uncontrolled (i.e. output-potent activity is modified independently of output-null activity) because neural variability in this space has no effect on cursor movement. Third, we considered the possibility that the distribution of redundant activity may be coupled with the task-relevant activity, so that producing particular activity patterns in output-potent dimensions requires changing the distribution of activity in output-null dimensions.</p><p>We tested all hypotheses in terms of their ability to predict the distribution of output-null activity, given the output-potent activity. Hypotheses were tested within the space in which the population activity naturally resides, termed the <italic>intrinsic manifold</italic> (<xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>). The results of <xref ref-type="bibr" rid="bib53">Sadtler et al. (2014)</xref> indicate that neural activity cannot readily leave this manifold, and more recent results demonstrate that neural activity is further constrained by a <italic>neural repertoire</italic> within the intrinsic manifold (<xref ref-type="bibr" rid="bib23">Golub et al., 2018</xref>). However, a repertoire defines only a set of population activity patterns, and not how often different activity patterns within the repertoire are produced. Therefore, to understand the principles governing the selection among redundant population activity patterns, we focused on predicting the <italic>distribution</italic> of redundant population activity within the intrinsic manifold and neural repertoire.</p><p>We found strong evidence for the third hypothesis, that redundant activity is coupled with task-relevant activity. This indicates that neural redundancy is resolved differently than muscular redundancy. Furthermore, the output-null space should not be thought of as a space in which neural activity can freely vary to carry out computations without regard to the output-potent activity. Instead, the distribution of output-null activity is constrained by the corresponding output-potent activity. If the required output-potent activity is defined by the task demands, this can constrain how the output-null activity can vary, and correspondingly the computations that can be carried out in the output-null space.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To study the selection of redundant neural activity, we used a BCI based on 85–94 neural units recorded using a Utah array in the primary motor cortex in each of three Rhesus macaques. Animals modulated their neural activity to move a computer cursor in a 2D center-out task (see Materials and methods; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). At the beginning of each experiment, we identified the 10 dimensions of the population activity that described the largest activity modulations shared among the neural units, termed the <italic>intrinsic manifold</italic> (<xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>). A two-dimensional subspace of the 10-dimensional intrinsic manifold was mapped to horizontal and vertical cursor velocity and was therefore output-potent, while the eight orthogonal dimensions were output-null. Our goal was to predict the joint distribution of the observed neural activity in this eight-dimensional output-null space.</p><p>We tested several hypotheses for the selection of redundant neural activity using the following logic. First, we predicted the distributions of output-null activity expected under each hypothesis. All hypotheses’ predictions were consistent with the observed behavior (i.e. the output-potent activity), and we ensured that none of these predictions required unrealistic firing rates when combined with the output-potent activity. Next, we compared the predicted distributions to the observed distributions of output-null activity to determine which hypothesis provided the best match to the observed distributions. We built the observed distributions of output-null activity as follows: At each time step during the BCI task, we assigned the recorded population activity pattern to one of eight bins corresponding to the direction of cursor movement (<inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>45</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>0°, 45°, 90°, etc.) produced by that neural activity. We binned by the cursor movement because we are studying the population activity that is redundant for a given cursor movement direction. For each bin, we projected the corresponding population activity patterns onto the eight output-null dimensions of the intrinsic manifold. The black histograms in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, and <xref ref-type="fig" rid="fig4">Figure 4</xref> show the marginal distributions in the first three output-null dimensions (ordered by variance accounted for). The colored histograms in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, and <xref ref-type="fig" rid="fig4">Figure 4</xref> are the predicted output-null distributions built under each hypothesis, which we compared to the observed distributions. The ensuing three subsections describe each hypothesis, and compare how well the corresponding predicted distributions matched the observed distributions.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.36774.005</object-id><label>Figure 2.</label><caption><title>Minimal firing hypotheses.</title><p>(<bold>A</bold>) Minimal Firing hypothesis: Given a particular output-potent activity (i.e. activity is constrained to black dotted line), subject selects the activity pattern (orange square) that requires the fewest spikes (i.e. nearest the gray square). (<bold>B</bold>) Distribution of observed output-null activity (‘Data’, in black) and activity predicted by the Minimal Firing hypothesis (‘Predicted’, in orange), in the first output-null dimension for upwards cursor movements. For this visualization, we applied PCA to the observed output-null activity to display the dimensions ordered by the amount of shared variance, with only the first of those dimensions shown here. The range of activity (e.g. <inline-formula><mml:math id="inf12"><mml:mo>±</mml:mo></mml:math></inline-formula> 150 spikes/s) appears larger than that expected for a single neuron because the range tends to increase with the number of neural units contributing to that dimension. Session L20131218. (<bold>C</bold>) Distributions of observed and predicted output-null activity as in (<bold>B</bold>), for time steps when the cursor was moving in eight different directions (rows), in three (of eight) output-null dimensions explaining the most output-null variance (columns). (<bold>D</bold>) Minimal Deviation hypothesis: Given a particular output-potent activity, subject selects the activity pattern (red square) nearest a fixed population activity pattern chosen for each session by cross-validation (gray square). (<bold>E–F</bold>) Same conventions as in (<bold>B–C</bold>) for the Minimal Deviation hypothesis.</p><p><supplementary-material id="fig2sdata1"><object-id pub-id-type="doi">10.7554/eLife.36774.006</object-id><label>Figure 2—source data 1.</label><caption><title>Histograms of predictions and data, as depicted in <xref ref-type="fig" rid="fig2">Figure 2B–C</xref> and <xref ref-type="fig" rid="fig2">Figure 2E–F</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-36774-fig2-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig2-v2"/></fig><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.36774.007</object-id><label>Figure 3.</label><caption><title>Uncontrolled hypotheses.</title><p>(<bold>A</bold>) Uncontrolled-uniform hypothesis: Given a particular output-potent activity, subject selects any activity within the physiological range (dark green), sampled from a uniform distribution. (<bold>B–C</bold>) Distributions of output-null activity observed and predicted by the Uncontrolled-uniform hypothesis; same conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The predicted distributions appear mound-shaped rather than uniform because we applied PCA to display the dimensions of output-null activity with the most shared variance (see Materials and methods). The range of activity increases with the number of neural units. Session L20131218. (<bold>D</bold>) Uncontrolled-empirical hypothesis: Subject selects output-null activity from the distribution of all output-null activity produced at any time while subjects used a different BCI mapping. (<bold>E–F</bold>) Same conventions as in (<bold>B–C</bold>) for the Uncontrolled-empirical hypothesis.</p><p><supplementary-material id="fig3sdata1"><object-id pub-id-type="doi">10.7554/eLife.36774.008</object-id><label>Figure 3—source data 1.</label><caption><title>Histograms of predictions and data, as depicted in <xref ref-type="fig" rid="fig3">Figure 3B–C</xref> and <xref ref-type="fig" rid="fig3">Figure 3E–F</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-36774-fig3-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig3-v2"/></fig><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.36774.009</object-id><label>Figure 4.</label><caption><title>Task-transfer hypotheses.</title><p>(<bold>A</bold>) Persistent Strategy hypothesis: Given a particular output-potent activity, subject selects an activity pattern appropriate under a different mapping (light blue rectangle), and corrects its output-potent component (red arrows) so as to produce the desired output-potent value under the current mapping (darker blue rectangle). (<bold>B–C</bold>) Distributions of output-null activity observed and predicted by the Persistent Strategy hypothesis; same conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The range of activity increases with the number of neural units. Session L20131218. (<bold>D</bold>) Fixed Distribution hypothesis: Given a particular output-potent activity, subject selects from the output-null activity patterns that were observed concurrently with this output-potent activity while controlling a different mapping. Different patterns are selected with the same frequencies as they were under the previous mapping. (<bold>E–F</bold>) Same conventions as in (<bold>B–C</bold>) for the Fixed Distribution hypothesis.</p><p><supplementary-material id="fig4sdata1"><object-id pub-id-type="doi">10.7554/eLife.36774.010</object-id><label>Figure 4—source data 1.</label><caption><title>Histograms of predictions and data, as depicted in <xref ref-type="fig" rid="fig4">Figure 4B–C</xref> and <xref ref-type="fig" rid="fig4">Figure 4E–F</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-36774-fig4-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig4-v2"/></fig><p>During each experiment, animals controlled two different BCI mappings (i.e. the two mappings had different output-potent subspaces). The first mapping was an ‘intuitive’ one that required no learning for proficient control. The second mapping was a within-manifold perturbation (see Materials and methods). For the second mapping, we analyzed the trials after the behavioral performance reached asymptote. Each hypothesis predicted the distribution of output-null activity that the animal would produce under the second mapping. To form its prediction, a hypothesis could utilize the output-potent activity observed during the second mapping, as well as all neural activity recorded under control of the first mapping. This technique allowed us to avoid circularity in our results because we built the hypothesized distributions using the first behavioral context and evaluated those predictions in the second. Additionally, because animals learned to use the BCI mappings through trial and error, it is possible that the animals’ assumptions about the output-null dimensions do not align perfectly with the actual output-null dimensions of the BCI mapping. To control for this, we estimated the animal’s internal model of the BCI mapping (<xref ref-type="bibr" rid="bib24">Golub et al., 2015</xref>). The results in the main text are based on this internal model, and we show in supplemental figures that all results still hold when using the actual BCI mapping.</p><sec id="s2-1"><title>Minimal firing hypotheses do not accurately predict output-null activity</title><p>Previous work in motor control has found that subjects select muscle activations that minimize energy use, that is, subjects tend not to make movements with more stiffness or muscular co-contraction than necessary to complete the task (<xref ref-type="bibr" rid="bib63">Thoroughman and Shadmehr, 1999</xref>; <xref ref-type="bibr" rid="bib16">Fagg et al., 2002</xref>; <xref ref-type="bibr" rid="bib30">Huang et al., 2012</xref>). We tested whether an analogous principle might hold true at the level of neurons (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <italic>Minimal Firing</italic> hypothesis). Because spiking incurs a metabolic cost (<xref ref-type="bibr" rid="bib36">Laughlin, 2001</xref>; <xref ref-type="bibr" rid="bib35">Laughlin et al., 1998</xref>), we first considered the hypothesis that among all the population activity patterns that produce the same cursor movement, the subject will select the one requiring the fewest spikes (<xref ref-type="bibr" rid="bib4">Barlow, 1969</xref>; <xref ref-type="bibr" rid="bib59">Softky and Kammen, 1991</xref>; <xref ref-type="bibr" rid="bib40">Levy and Baxter, 1996</xref>).</p><p>To predict the distribution of output-null activity under this hypothesis, at each time step we found the population activity pattern that would produce the observed cursor movement with the fewest spikes across all recorded neurons (see Materials and methods). This means population activity will have minimal variability in output-null dimensions, because spiking in these dimensions does not affect cursor movement. In <xref ref-type="fig" rid="fig2">Figure 2A</xref>, the orange square depicts the activity pattern nearest zero spikes/s (gray square) among all activity patterns that would produce the same cursor movement (black dotted line). This would produce a delta distribution of output-null activity, where the delta would be located at the predicted value (orange square). To make this prediction more realistic, we incorporated Poisson spiking noise. In addition, for this hypothesis and those following, we ensured that all predictions were physiologically plausible (i.e. firing rates were between zero and the maximum rates observed in the experiment; see Materials and methods).</p><p>We constructed histograms of the output-null activity predicted by the Minimal Firing hypothesis by pooling over all time steps in which the cursor moved in a similar direction (e.g. 0°, 45°, etc.) (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, orange). We compared these predicted distributions to the observed distributions of output-null activity measured for that movement direction during the experiment (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, black). <xref ref-type="fig" rid="fig2">Figure 2C</xref> depicts these histograms for the same session across eight different cursor directions (rows), in three of the eight output-null dimensions (columns). For visualization, we applied principal components analysis (PCA) to display the output-null dimensions ordered by the amount of shared variance in the output-null activity. To assess how well the Minimal Firing hypothesis predicted the observed output-null activity, we computed the absolute error between the predicted and observed histograms. These errors were averaged across histograms for all eight cursor directions and eight output-null dimensions in a given session. We normalized the errors so that a perfect match between the observed and predicted histograms would result in an error of 0%, while complete mismatch between the predicted and observed histograms would yield an error of 100% (see Materials and methods). We found that the predictions of the Minimal Firing hypothesis differed from the observed activity by 73.2% <inline-formula><mml:math id="inf13"><mml:mo>±</mml:mo></mml:math></inline-formula>1.3% (mean <inline-formula><mml:math id="inf14"><mml:mo>±</mml:mo></mml:math></inline-formula> SE) across sessions.</p><p>One possible explanation as to why these predictions were so different from the observed activity is that minimal energy principles in the brain may not equate to minimal spiking. Perhaps a more relevant constraint is not how far the activity is away from zero firing, but rather how far the activity is from a different level of activity, such as the mean firing rate for each neuron. This alternative version of a minimal energy hypothesis (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, <italic>Minimal Deviation</italic> hypothesis) predicts that among all the population activity patterns that produce the same cursor movement, subjects select the one with the smallest deviation from some baseline population activity pattern. For each session, we identified the population activity pattern that would minimize the output-null prediction error across cursor directions in a cross-validated fashion (see Materials and methods) (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). This hypothesis yielded an average histogram error of 30.9% ±1.2% (mean ± SE) across sessions. While this represents a substantial improvement over the Minimal Firing hypothesis (paired t-test of histogram errors in each session, <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>), the predicted distributions of output-null activity still show clear discrepancies from the observed distributions (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). Thus, we sought a hypothesis that could better predict the observed distributions of output-null activity.</p></sec><sec id="s2-2"><title>Uncontrolled hypotheses do not accurately predict output-null activity</title><p>It has been shown that muscle activity exhibits more variability in output-null dimensions than in output-potent dimensions (<xref ref-type="bibr" rid="bib55">Scholz and Schöner, 1999</xref>; <xref ref-type="bibr" rid="bib64">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib66">Valero-Cuevas et al., 2009</xref>). An explanation of this variability asymmetry is the ‘minimal intervention’ principle (<xref ref-type="bibr" rid="bib64">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib66">Valero-Cuevas et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Diedrichsen et al., 2010</xref>), which states that while variability in output-potent dimensions should be corrected to ensure task success, variability in output-null dimensions can be left uncorrected because it does not lead to deficits in task performance. While this principle has been used to explain muscle activity, here we investigate whether it also explains neural activity. This hypothesis, that output-null activity will be ‘uncontrolled’ and have high variability, is in contrast to the minimal firing hypotheses, which predict that output-null activity will have low variability.</p><p>The idea that neural activity may be selected according to a minimal intervention principle does not, by itself, specify the form of the distribution in output-null dimensions. We therefore considered two specific forms of uncontrolled hypotheses. First, we supposed that if all values of output-null activity are equally likely, then output-null activity would have a uniform distribution with bounds determined by each neuron’s physiological range (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, <italic>Uncontrolled-uniform</italic>). We emphasize that the minimal intervention principle does not specify a candidate distribution, and so we consider this particular hypothesis as a limiting case, where output-null activity has maximum entropy within bounds on minimum and maximum activity. At each time step, we sampled the output-null activity from a uniform distribution within ranges observed experimentally (see Materials and methods). This procedure predicts that the output-null activity is selected independently of the current output-potent activity, reflecting the minimal intervention principle. However, note that the extent of the uniform distribution depends on the physiological range of each neuron, and so the predicted distributions of output-null activity vary slightly with the cursor direction (<xref ref-type="fig" rid="fig3">Figure 3B–C</xref>) (e.g. the length of the green bar in <xref ref-type="fig" rid="fig3">Figure 3A</xref> depends on the output-potent activity). As before, for visualization we ordered the eight output-null dimensions by the amount of shared variance explained in the recorded activity, and displayed the first three of these output-null dimensions (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Because these three dimensions were rotated along the dimensions of highest variance, the predicted histograms are mound-shaped rather than uniformly distributed (see Materials and methods). The predictions of the Uncontrolled-uniform hypothesis differed from the observed output-null activity by 56.6% ±1.1% (mean ± SE) across sessions.</p><p>In the second variant of this hypothesis, we considered a non-uniform distribution of output-null activity. If the natural variability of output-null activity is truly unmodified, then the distribution of activity observed in the same dimensions when a subject was controlling a different (previous) BCI mapping should have the same distribution under the current mapping (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, <italic>Uncontrolled-empirical</italic>). Thus, under this hypothesis we construct an empirical distribution of output-null activity, which we form by projecting all of the population activity that the subject produced under the <italic>previous</italic> mapping onto the output-null dimensions of the <italic>current</italic> BCI mapping (see Materials and methods). At each time step, we sampled from this empirical distribution of output-null activity independently of the output-potent activity, again reflecting the minimal intervention principle (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). We checked that combining the output-null and output-potent activity resulted in physiologically plausible population activity (see Materials and methods). If it did not, then we re-sampled a different output-null activity pattern until the combination resulted in physiologically plausible population activity. Due to this resampling, the predicted distributions of output-null activity vary slightly with the cursor direction (<xref ref-type="fig" rid="fig3">Figure 3E–F</xref>). The histograms of the predictions differed from the observed data by only 23.8% ±0.8% (mean ± SE) across sessions, which is the lowest error of all hypotheses considered so far. This suggests that previously observed population activity (in this case, recorded during use of a different BCI mapping) offers greater predictive power of the selection of output-null activity than a priori predictions such as those of the Minimal Firing, Minimal Deviation, and Uncontrolled-uniform hypotheses.</p></sec><sec id="s2-3"><title>Task-transfer hypotheses accurately predict output-null activity</title><p>Thus far, the hypothesis that best predicts the observed output-null activity is the one that uses previously observed activity to generate its predictions (Uncontrolled-empirical). This motivated us to consider more refined hypotheses that make use of this previously observed activity to generate predictions.</p><p>We first considered the hypothesis that in order to produce a desired movement, the subject selects neural activity as if he were still using the previous mapping, and corrects this activity only to ensure task success (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <italic>Persistent Strategy</italic>). Conceptually, when the subject wants to move the cursor in a particular direction using the current BCI mapping, he starts with the population activity patterns that he used to move the cursor in that direction under an earlier mapping (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, light blue shading). Because this activity will not move the cursor in the same way that it did under the previous mapping, this activity is modified along the output-potent dimensions of the current mapping (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, red arrows), reflecting the minimal intervention principle (<xref ref-type="bibr" rid="bib64">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib66">Valero-Cuevas et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Diedrichsen et al., 2010</xref>). This is similar to the Uncontrolled-empirical hypothesis in that we assume activity in output-null dimensions can be corrected independently of the activity in output-potent dimensions. However, instead of sampling from the entire distribution of previously observed output-null activity at each time step, here we only sample from the subset of this activity observed when subjects needed to move the cursor in the same direction as the current time step. The predictions of this hypothesis (<xref ref-type="fig" rid="fig4">Figure 4B–C</xref>) differed from the observed output-null activity by 17.4% ±0.7% (mean ± SE) across sessions.</p><p>The principle of minimal intervention posits that output-null activity can change independently from output-potent activity. Here we examine this assumption in detail. Previous work has found that the characteristic ways in which neurons covary (i.e. the dimensions of the intrinsic manifold) persist even under different BCI mappings, perhaps owing to underlying network constraints (<xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>). All hypotheses we consider here are evaluated within the intrinsic manifold, and thus respect these constraints on population variability. Because the dimensions of the intrinsic manifold capture the variability among the neurons, it is plausible that the activity along different dimensions of the intrinsic manifold can vary independently, consistent with the minimal intervention principle. By contrast, in the next hypothesis we consider the possibility that activity along different dimensions exhibit dependencies.</p><p>We considered the hypothesis that the distribution of activity in output-null dimensions would be predictably coupled with the activity in output-potent dimensions, even under a different BCI mapping when those dimensions were not necessarily potent and null. Under this hypothesis (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, <italic>Fixed Distribution</italic>), given the output-potent activity, the distribution of the corresponding output-null activity remains the same as it was under a different BCI mapping (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, blue frequency distribution), even if this activity was not output-null under the other mapping. This hypothesis predicts that neural activity patterns are ‘yoked’ across dimensions, such that producing particular activity in output-potent dimensions requires changing the distribution of activity in output-null dimensions. The histograms of output-null activity predicted by the Fixed Distribution hypothesis were a striking visual match to the recorded activity, and accurately predicted the dependence of these distributions on the cursor direction (<xref ref-type="fig" rid="fig4">Figure 4E–F</xref>). Overall, these predictions differed from the observed output-null activity by only 13.4% ±0.5% (mean ± SE) across sessions.</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.36774.011</object-id><label>Figure 5.</label><caption><title>Fixed Distribution hypothesis best predicts output-null activity.</title><p>Boxes depict the 25th, 50th, and 75th percentile of errors observed across sessions for all animals combined. Whiskers extend to cover approximately 99.3% of the data. Gray boxes depict the error floor across sessions (mean ± s.d.), estimated using half of the observed output-null activity to estimate the histogram, mean, and covariance of the other half (see Materials and methods). Asterisks depict a significant difference between errors of Fixed Distribution and other hypotheses for a one-sided Wilcoxon signed rank test at the <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula> (***) level. (<bold>A</bold>) Error in predicted histograms of output-null activity. For each session, histogram error was averaged across all output-null dimensions and cursor directions. Average histogram error floor was 6.7% ± 1.9% (mean ± s.d., also shown in gray). (<bold>B</bold>) Error in predicted mean of output-null activity. For each session, mean error was averaged across all cursor directions, where the mean is an 8D vector of the average activity in each output-null dimension. Average mean error floor was 6.9 ± 2.5 spikes/s (mean ± s.d., also shown in gray). (<bold>C</bold>) Error in predicted covariance of output-null activity. For each session, covariance error was averaged across all cursor directions. Average covariance error floor was 1.0 ± 0.3 (mean ± s.d., also shown in gray).</p><p><supplementary-material id="fig5sdata1"><object-id pub-id-type="doi">10.7554/eLife.36774.016</object-id><label>Figure 5—source data 1.</label><caption><title>Histogram, mean, and covariance errors of all hypotheses for all sessions, as depicted in <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-36774-fig5-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig5-v2"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36774.012</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Results for each animal.</title><p>Fixed Distribution hypothesis best predicts observed output-null activity for each animal. (<bold>A</bold>) Monkey J. (<bold>B</bold>) Monkey L. (<bold>C</bold>) Monkey N. Same conventions as <xref ref-type="fig" rid="fig5">Figure 5</xref>. Asterisks denote a significance level of <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> (*), <inline-formula><mml:math id="inf18"><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> (**), and <inline-formula><mml:math id="inf19"><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula> (***).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig5-figsupp1-v2"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36774.013</object-id><label>Figure 5—figure supplement 2.</label><caption><title>Results when predicting output-null activity during first mapping.</title><p>Predicting output-null activity produced during the first mapping using activity observed during the second mapping. Throughout this work, we predict the output-null activity recorded while subjects used the second BCI mapping, and hypotheses can use activity recorded during use of the first BCI mapping to make their predictions. However, most of the hypotheses do not depend on the order in which the two mappings were presented to the subjects. Here we predict the output-null component of the activity recorded during use of the first BCI mapping, and hypotheses can use activity recorded under the second BCI mapping to make their predictions. Same conventions as <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig5-figsupp2-v2"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36774.014</object-id><label>Figure 5—figure supplement 3.</label><caption><title>Results when not using animals’ internal model to define the output-null dimensions.</title><p>Predicting output-null activity without using animal’s internal model (IME) to define the output-null dimensions. Same conventions as <xref ref-type="fig" rid="fig5">Figure 5</xref>. Asterisks denote a significance level of <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> (*), <inline-formula><mml:math id="inf21"><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> (**), and <inline-formula><mml:math id="inf22"><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula> (***). ‘n.s.’ is not significant.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig5-figsupp3-v2"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36774.015</object-id><label>Figure 5—figure supplement 4.</label><caption><title>Identifying the animal’s internal model to define the output-null dimensions.</title><p>Identifying the animal’s internal model of the BCI mapping. (<bold>A</bold>) BCI cursor trajectory (black) for an example trial during the second mapping. At each time step, the subject takes its most recent visual feedback (where red whisker touches black trajectory) and propagates it forward in time (red whisker) using an internal model and neural activity produced in the recent past. This yields the subject’s internal estimate of the current cursor position (red open circle), which is different from the actual BCI cursor position (black open circle). At this time step, the cursor movement according to the internal model (red arrow) points more directly to the target (green circle) than the actual BCI cursor movement (black arrow). Each dot indicates a 45 ms time step. (<bold>B</bold>) Average absolute angular cursor error (in degrees) for each session based on the actual BCI cursor movements (analogous to black arrow in (<bold>A</bold>)). Angular errors during stable control of the second mapping were larger than the angular errors during control of the first mapping. (<bold>C</bold>) Average absolute angular cursor error (in degrees) for each session based on IME (analogous to red arrow in (<bold>A</bold>)). When viewed through animals’ internal estimates of the BCI, angular errors during control of the first and second mapping were similar.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig5-figsupp4-v2"/></fig></fig-group><p>The Fixed Distribution hypothesis yielded a lower histogram error than all other hypotheses across sessions from three different animals (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). In total, the Fixed Distribution hypothesis had the lowest histogram error in 41 of 42 sessions. The histogram error metric does not explicitly capture the degree to which hypotheses predicted the mean output-null activity, or any correlations that exist across output-null dimensions. We therefore assessed how well the predictions captured the mean and covariance of observed data in all output-null dimensions jointly (see Materials and methods). In agreement with our findings for histogram error, the mean (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) and covariance (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) of output-null activity was best predicted by the Fixed Distribution hypothesis, with an average mean error of 23.5 ± 1.4 spikes/s (mean ± SE) and an average covariance error of 1.4 ± 0.1 (mean ± SE in arbitrary units; see Materials and methods). These error metrics offer further evidence that the Fixed Distribution hypothesis provides a good match to the output-null distribution, as measured by the agreement between the first and second moments of the two distributions. Because these error metrics rely on a limited number of trials, they should not be compared relative to zero error. We estimated the smallest histogram, mean, and covariance errors achievable by any hypothesis, given the limited number of samples available to estimate the true output-null distributions (see Materials and methods, and gray regions in <xref ref-type="fig" rid="fig5">Figure 5</xref>). The errors of Fixed Distribution were exceedingly close to the lowest achievable error given the number of samples available (see Materials and methods). Next, we found that the Fixed Distribution hypothesis achieved the lowest prediction errors among all hypotheses when data for each monkey was considered individually (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We repeated our analyses to predict output-null activity produced during the first mapping using activity observed during the second mapping (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). We also predicted output-null activity using the actual BCI mapping rather than the animal’s internal model to define the output-null dimensions (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). Both analyses yielded results similar to those in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></sec><sec id="s2-4"><title>Predicting changes in neural variability when activity becomes output-null</title><p>So far we have shown that the Fixed Distribution hypothesis provides a better explanation for the structure of output-null activity than hypotheses incorporating constraints on firing rates or the minimal intervention principle. We next sought stronger evidence for the Fixed Distribution hypothesis by assessing our predictions in the particular dimensions of population activity where it is least likely to hold. Because cursor velocity is a two-dimensional quantity, all but two dimensions of population activity for each BCI mapping are output-null. Thus, given two different BCI mappings, most dimensions will be output-null under both mappings, and so most components of the population activity have no reason to change from one mapping to the other. Therefore, we assessed whether our results held in dimensions of population activity that were output-potent during the first mapping, but output-null during the second mapping (see Materials and methods). These are the dimensions in which one would expect to see the most changes in the population activity between the first and second mappings.</p><p>Our hypotheses make distinct predictions about how the variance of activity should change if a dimension is output-potent under the first mapping and becomes output-null under the second mapping. For example, according to the Minimal Firing and Minimal Deviation hypotheses, the variance of activity will collapse in dimensions that are output-null because unnecessary spiking is undesirable. Thus, if a dimension becomes output-null, variance in this space should exhibit a marked decrease. On the other hand, the Uncontrolled hypotheses predict that, when conditioned on the cursor movement, variance will expand when the activity is output-null. This occurs because variability in this dimension will no longer affect cursor movement, and would therefore no longer need to be suppressed. Finally, the Fixed Distribution hypothesis posits that the same distributions of output-null activity will be observed regardless of whether a dimension was previously output-potent or output-null, and so this hypothesis predicts that there will be little to no change in the variance of activity in a particular dimension under the two mappings.</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.36774.017</object-id><label>Figure 6.</label><caption><title>Variance of neural activity in dimensions that become output-null.</title><p>(<bold>A</bold>) Observed activity from a representative session in the 2D subspace in which activity was output-potent under the first mapping and output-null under the second mapping. Activity recorded during use of the first mapping (black points) was output-potent while activity recorded during use of the second mapping (gray points) was output-null. The covariances during the first and second mapping (black and gray ellipses, respectively) are depicted as the 95% contours of a Gaussian density fit to the activity. Session J20120403, for all time steps when the activity would have moved the cursor to the right under the second mapping. (<bold>B</bold>) Covariance ellipses for all sessions and eight different cursor movement angles. Same conventions as in (<bold>A</bold>). Ellipses shown in (<bold>A</bold>) indicated by gray box. (<bold>C</bold>) Change in variance of neural activity in the same subspace as in (<bold>A</bold>), for the activity observed (‘Data’) and predicted by each hypothesis. Height of bars depicts the average change in variance across sessions (mean ± 2 SE).</p><p><supplementary-material id="fig6sdata1"><object-id pub-id-type="doi">10.7554/eLife.36774.020</object-id><label>Figure 6—source data 1.</label><caption><title>Observed and predicted change in covariance across sessions, as depicted in <xref ref-type="fig" rid="fig6">Figure 6C</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-36774-fig6-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig6-v2"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36774.018</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Variance of neural activity did not change in the dimensions that became output-potent.</title><p>Variance of neural activity in dimensions that became output-potent. We repeated the analyses shown in <xref ref-type="fig" rid="fig6">Figure 6</xref> on the predictions of output-null activity produced during the first mapping using activity observed during the second mapping (shown in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). This analysis amounts to assessing the change in variance in dimensions that were output-null during the first mapping and output-potent during the second mapping. Same conventions as <xref ref-type="fig" rid="fig6">Figure 6B–C</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig6-figsupp1-v2"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36774.019</object-id><label>Figure 6—figure supplement 2.</label><caption><title>Decrease in variance was not accompanied by the mean activity moving toward predictions of minimal energy hypotheses.</title><p>Output-null activity was not closer to the mean predicted by Minimal Deviation than expected under Fixed Distribution. (<bold>A</bold>) Determining whether the observed mean output-null activity (‘Data’) was closer to the Minimal Deviation (‘MD’) prediction than expected under Fixed Distribution (‘FD’). Dots and ellipses indicate the mean and covariance of output-null activity observed (black), predicted by Fixed Distribution (blue), and predicted by Minimal Deviation (red), for all time steps from session J20160714 corresponding to the same cursor movement, in the first two of eight output-null dimensions. Similar to <xref ref-type="fig" rid="fig6">Figure 6</xref>, the observed covariance (black ellipse) is slightly smaller than that of Fixed Distribution (blue), suggesting the observed covariance is moving in the direction of the small covariance expected under Minimal Deviation (red). Do we see a similar trend in the mean activity, where the observed mean activity is closer to the Minimal Deviation mean than expected under Fixed Distribution? We can assess this by comparing the lengths of the dotted lines, which indicate the distances of the mean activity observed (black) and predicted by Fixed Distribution (blue) from the mean predicted by Minimal Deviation. (<bold>B</bold>) Distance of the observed and predicted output-null activity from the activity predicted by Minimal Deviation. Each dot indicates the average distance of the output-null activity observed (horizontal axis) and predicted by Fixed Distribution (vertical axis, ‘FD’) from the Minimal Deviation (‘MD’) in a session. For each cursor direction, the distance was computed as the <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> norm of the difference between the mean 8D output-null activity (predicted or observed) from the mean activity predicted by MD, similar to (<bold>A</bold>). Session distances were the average distances across all cursor directions. Most points lie below the diagonal, suggesting that the observed output-null activity was not closer to the MD predictions than expected under Fixed Distribution. Results were similar when comparing distances to the means predicted by Minimal Firing instead of Minimal Deviation.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-fig6-figsupp2-v2"/></fig></fig-group><p>We asked whether the variance of population activity decreased, increased, or remained the same in dimensions that changed from being output-potent to output-null (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Critically, we computed the variance of activity after first binning by the corresponding angle in the output-potent dimensions of the <italic>second</italic> mapping. This was done so that the neural activity in each bin would all result in similar cursor movements under the second mapping, and is identical to the procedure used previously to assess the errors of the hypotheses’ predictions. Notably, binning in this way means that each bin may contain activity corresponding to different cursor movements under the <italic>first</italic> mapping, and so one might expect that in each bin the activity recorded under the first mapping would be more heterogeneous than the activity recorded under the second mapping.</p><p>We observed that the variance of population activity recorded under the first and second mappings was remarkably similar in the dimensions that changed from output-potent to output-null, even though these activity patterns usually corresponded to different cursor movements under the two mappings (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Thus, the variance of activity did not change much when an output-potent dimension became output-null, in agreement with the predictions of the Fixed Distribution hypothesis. To quantify these observations, we computed the average change in variance in each session (see Materials and methods). Across sessions, we found that the variance of observed activity showed a small but significant decrease when it became output-null (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, ‘Data’) (t-test, <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>). This is in contrast to the predictions of the Minimal Firing and Minimal Deviation hypotheses, which predicted much larger decreases.</p><p>The observed change in variance lies closest to the predictions of the Fixed Distribution hypothesis. In fact, we observed that the Fixed Distribution hypothesis also predicted a slight decrease in variance in dimensions that became output-null (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, ‘Fixed Distribution’) (t-test, <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>). This slight predicted change in variance occurs because the distributions of activity in the output-potent dimensions of the second mapping are different under the first and second mappings. Because the Fixed Distribution hypothesis predicts a fixed conditional distribution of output-null activity given the output-potent activity, slightly different sets of output-potent activity will result in a slightly different distribution of the corresponding output-null activity.</p><p>These analyses show that, contrary to the predictions of the minimal firing and uncontrolled hypotheses, the variance of population activity did not change dramatically in dimensions that were output-potent under the first mapping and output-null under the second mapping. We also assessed whether the reverse was true—if the variance of activity changed in dimensions that began as output-null and became output-potent. To measure this, we repeated the above analyses after predicting output-null activity produced during the first mapping using the activity observed under the second mapping (as in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). We found that the activity showed little to no change in variance in these dimensions (t-test, <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>), in agreement with the predictions of Fixed Distribution (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><p>Importantly, the agreement between the observed output-null activity and the predictions of the Fixed Distribution hypothesis in these analyses indicates that our ability to accurately predict the distribution of output-null activity is not merely a result of most activity being output-null under both mappings. Instead, the distribution of output-null activity remains consistent with the Fixed Distribution hypothesis even in the output-null dimensions that were previously output-potent.</p><p>In <xref ref-type="fig" rid="fig6">Figure 6C</xref>, the observed output-null activity showed a larger decrease in variance than the predictions of the Fixed Distribution hypothesis, at least in the 2D subspace of output-null activity that was output-potent during the first mapping. This slight decrease in variance is in the direction of the predictions of Minimal Firing and Minimal Deviation. If this decrease in variance is to be explained by Minimal Firing or Minimal Deviation principles, we would expect that the observed <italic>mean</italic> output-null activity would also move in the direction of the predictions of Minimal Firing and Minimal Deviation, relative to what is predicted by Fixed Distribution. To see if this was the case, we first computed the distance of the observed mean output-null activity from the mean predicted by Minimal Deviation for each movement direction, and compared this to the distance of the mean output-null activity predicted by Fixed Distribution from the mean predictions of Minimal Deviation (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2A</xref>). We did not find evidence that the observed mean output-null activity was closer to the mean predicted by Minimal Deviation than was the mean predicted by Fixed Distribution (one-sided Wilcoxon signed rank test, <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>; see <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2B</xref> and Materials and methods). Repeating the analysis with Minimal Firing instead of Minimal Deviation yielded similar results (one-sided Wilcoxon signed rank test, <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>). Thus, while we observed a slight decrease in the <italic>variance</italic> of output-null activity in dimensions that changed from output-potent to output-null, we did not find any evidence that the <italic>mean</italic> output-null activity moved in the direction of the predictions of Minimal Firing or Minimal Deviation.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Recent work has suggested that neural redundancy may be exploited for various computations (<xref ref-type="bibr" rid="bib12">Druckmann and Chklovskii, 2012</xref>; <xref ref-type="bibr" rid="bib31">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="bib44">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Elsayed et al., 2016</xref>; <xref ref-type="bibr" rid="bib11">Driscoll et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Murray et al., 2017</xref>). However, if the activity in output-null dimensions is constrained by the output-potent activity, then this may limit the ability of output-null activity to perform computations without affecting the readout. Here, we studied neural redundancy in the primary motor cortex using a BCI, where it is known exactly which population activity patterns are redundant, meaning they produce an identical cursor movement. We generated predictions of the distributions of output-null neural activity for subjects performing a BCI cursor control task, and compared them to the distributions observed in our experiments. We found that hypotheses inspired by minimal firing and minimal intervention principles, drawn from theories of muscle coordination, did not accurately predict the observed output-null activity. Instead, we found that the distribution of output-null activity was well predicted by the activity in the two output-potent dimensions. This coupling between the output-potent and output-null activity implies that, when output-potent activity is used to satisfy task demands, there are constraints on the extent to which neural circuits can use redundant activity to perform additional computations.</p><p>Our results indicate that the way in which neural redundancy is resolved is different from how muscle redundancy is resolved. There have been several prevalent proposals for how muscle redundancy is resolved, including minimal energy, optimal feedback control (OFC), and habitual control. Models incorporating minimal energy principles have helped to explain observed gait (<xref ref-type="bibr" rid="bib42">McNeill Alexander and McNeill, 2002</xref>) and arm reaches (<xref ref-type="bibr" rid="bib63">Thoroughman and Shadmehr, 1999</xref>; <xref ref-type="bibr" rid="bib30">Huang et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Fagg et al., 2002</xref>; <xref ref-type="bibr" rid="bib17">Farshchiansadegh et al., 2016</xref>). By analogy, it has been proposed that the brain may prefer an ‘economy of impulses’ (<xref ref-type="bibr" rid="bib4">Barlow, 1969</xref>; <xref ref-type="bibr" rid="bib59">Softky and Kammen, 1991</xref>; <xref ref-type="bibr" rid="bib40">Levy and Baxter, 1996</xref>), resolving neural redundancy by minimizing the production of action potentials. However, we found that minimal energy principles in terms of firing rates do not play a dominant role in the selection of output-null neural activity. Given that metabolic activity can decrease without corresponding changes in firing rates (<xref ref-type="bibr" rid="bib48">Picard et al., 2013</xref>), the brain may implement minimal energy principles without influencing the way neural redundancy is resolved.</p><p>OFC posits that motor control signals are selected to minimize a cost function that depends on task requirements and other factors, such as effort or delayed reward. OFC models have been widely used to explain muscle activity during motor tasks (<xref ref-type="bibr" rid="bib65">Todorov, 2004</xref>; <xref ref-type="bibr" rid="bib56">Scott, 2004</xref>; <xref ref-type="bibr" rid="bib10">Diedrichsen et al., 2010</xref>). Our results for neural activity differ in two important respects from OFC predictions with standard cost functions involving task requirements and effort. First, those implementations of OFC predict that variability in task-irrelevant dimensions should be higher than variability in task-relevant dimensions, a concept often referred to as the ‘uncontrolled manifold’ (<xref ref-type="bibr" rid="bib55">Scholz and Schöner, 1999</xref>). We found that the variability of neural activity did not increase in dimensions that went from being task-relevant to task-irrelevant (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). Second, those implementations of OFC predict a ‘minimal intervention’ strategy, whereby activity in task-relevant dimensions is corrected independently of activity in task-irrelevant dimensions (<xref ref-type="bibr" rid="bib64">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib66">Valero-Cuevas et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Diedrichsen et al., 2010</xref>). Three of the hypotheses we tested incorporate this minimal intervention principle: Uncontrolled-uniform, Uncontrolled-empirical, and Persistent Strategy. None of these hypotheses predicted neural activity in task-irrelevant dimensions as accurately as did the Fixed Distribution hypothesis, which predicts that the distributions of task-relevant and task-irrelevant activity are yoked. Overall, our work does not rule out the possibility that OFC is appropriate for predicting neural activity. First, it may be possible to design a cost function such that OFC predictions are consistent with the findings presented here. Second, one could consider applying OFC with the control signal being the input to M1 (e.g. PMd activity), rather than the control signal being M1 activity (as we have done here) or muscle activity (where OFC has been traditionally applied). This could induce coupling between the output-potent and output-null dimensions of the M1 activity, and thereby yield predictions that are consistent with the findings presented here.</p><p>It has also been proposed that muscle recruitment is habitual rather than optimal, such that muscle recruitment under altered dynamics is a rescaled version of that under normal control (<xref ref-type="bibr" rid="bib9">de Rugy et al., 2012</xref>). The results for habitual control are similar to what we found for neural activity, in that (1) we could predict activity from previously observed activity, and (2) we observed a tight coupling of the distributions of task-relevant and task-irrelevant activity (in contrast to minimal intervention). However, the results for habitual control are different from our findings in that we found that subjects appear to use the same distribution of activity in each of two different BCI mappings, whereas different (overlapping) subsets of muscle activation patterns were used under different conditions in <xref ref-type="bibr" rid="bib9">de Rugy et al. (2012)</xref>.</p><p>Given how many dimensions of population activity there are (in this case, 10), it is somewhat surprising that conditioning on only the two output-potent dimensions could provide so much explanatory power for predicting the distribution in the remaining neural dimensions. This suggests that many of the dimensions of population activity are coupled, that is, changing the activity along some dimensions may also lead to changes along other dimensions, even though those dimensions are mutually orthogonal. During arm movement control, output dimensionality and presumably the neural dimensionality are larger than in our BCI setup. We speculate that during arm movements, many of the null dimensions will remain coupled with the potent dimensions, thereby yielding results similar to what we found here. Future work could examine whether animals can be trained to uncouple dimensions, as well as the effects of larger output-potent dimensionality on redundancy, by repeating our analyses with a higher-dimensional effector, such as a multiple degree-of-freedom robotic limb (e.g. <xref ref-type="bibr" rid="bib69">Wodlinger et al., 2015</xref>).</p><p>The results presented here are related to, and go beyond, those in <xref ref-type="bibr" rid="bib23">Golub et al. (2018)</xref>. Although the two studies analyzed data from the same experiments, they ask distinct questions. <xref ref-type="bibr" rid="bib23">Golub et al. (2018)</xref> focused on explaining the changes in population activity underlying behavioral learning. By contrast, in the present work we seek to determine the constraints on activity in the task-irrelevant (i.e. output-null) dimensions. In other words, while <xref ref-type="bibr" rid="bib23">Golub et al. (2018)</xref> focused on explaining the changes leading to <italic>behavioral</italic> learning, we focus here on the principles <italic>other than behavior</italic> that constrain population activity. As a result, all hypotheses we consider in the present work make predictions consistent with the observed behavior in the output-potent dimensions.</p><p><xref ref-type="bibr" rid="bib23">Golub et al. (2018)</xref> found that the amount of learning animals showed was consistent with a fixed neural repertoire of population activity patterns being reassociated to control the second BCI mapping. The repertoire of population activity refers to the set of population activity patterns that were observed, whereas here we focused on the <italic>distribution</italic>, which describes how often the animals produced different activity patterns. In other words, the finding of a fixed repertoire is a statement about the support of the distribution of population activity, whereas here we found that the distribution of population activity can be predicted in output-null dimensions, given the output-potent activity. Because many different distributions of neural activity can be constructed from a fixed repertoire, the present results represent a stronger constraint on population activity than that shown in <xref ref-type="bibr" rid="bib23">Golub et al. (2018)</xref>. Indeed, the majority of the hypotheses we tested were consistent with a fixed neural repertoire, and thus cannot be disambiguated based on our prior work. This is evidenced by the predicted distributions largely overlapping with the support of the actual data distributions (<xref ref-type="fig" rid="fig2">Figures 2</xref>–<xref ref-type="fig" rid="fig4">4</xref>). The two hypotheses that were not fully consistent with a fixed repertoire are the Minimal Firing and Uncontrolled-uniform hypotheses. However, in the context of predicting the distribution of activity in redundant dimensions, these hypotheses represent interesting cases worth considering (i.e. where population activity either obeys minimal firing constraints, or that the output-null activity is fully unstructured, respectively), and so we included these hypotheses to cover these possibilities.</p><p>It is interesting to consider the relationship between arm movements and BCI cursor movements (<xref ref-type="bibr" rid="bib46">Orsborn et al., 2014</xref>; <xref ref-type="bibr" rid="bib67">Vyas et al., 2018</xref>). If the dimensions responsible for moving the arm overlap with both the output-potent and output-null dimensions of the BCI, this might explain the coupling we observe between the output-potent and output-null dimensions. However, in these experiments, the animal’s arm was not moving during BCI control (see Extended Data Figure 5 in <xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>). Thus, the activity we study here resides within the arm’s output-null dimensions. This implies that in our recordings the arm’s output-potent dimensions do not overlap with either the output-potent or the output-null dimensions of the BCI, and so arm movements (or the lack thereof) are unlikely to explain the coupling we observed between the output-potent and output-null dimensions of the BCI. Overall, being unaware of extra output-potent dimensions would likely make the predictions of the Fixed Distribution hypothesis worse, not better. The reason for this is as follows. The Fixed Distribution hypothesis predicts that the distribution of activity in output-null dimensions depends upon the corresponding output-potent activity. Under this hypothesis, the more we know of the output-potent activity, the better we can predict the output-null distribution. If there is an output-potent dimension that we have not accounted for in our analyses, accounting for this dimension would likely improve our predictions. The fact that we were able to accurately predict the output-null distributions (13% histogram error on average, with the lowest possible error being 7%) without knowing all the potent dimensions is then evidence that these extra potent dimensions, if they exist, would not provide substantial additional predictive power.</p><p>In this work, we define a set of population activity patterns as redundant if they all result in the same readout in downstream areas. This definition of redundancy comes from early work on motor control (<xref ref-type="bibr" rid="bib5">Bernstein, 1967</xref>; <xref ref-type="bibr" rid="bib60">Sporns and Edelman, 1993</xref>), where it was noted that different motor signals can result in the same movement kinematics. This is related to but distinct from the information-theoretic definition of redundancy (<xref ref-type="bibr" rid="bib54">Schneidman et al., 2003</xref>; <xref ref-type="bibr" rid="bib34">Latham et al., 2005</xref>; <xref ref-type="bibr" rid="bib3">Averbeck et al., 2006</xref>). In the information-theoretic case, redundancy describes the extent to which correlations among neurons limit decoding accuracy for different stimuli. This is distinct from the type of redundancy studied here, defined as the existence of multiple population activity patterns corresponding to the same readout. For example, by the information-theoretic definition, a system may have no redundancy (e.g. the population activity allows one to perfectly decode the encoded variable), but there may still be multiple population activity patterns that refer to this same encoded variable.</p><p>We found that the distribution of output-null activity could be well predicted using activity recorded under a different BCI mapping. Two factors of our experimental design are particularly relevant when interpreting this result. First, we used a balanced center-out task design in which subjects made roughly equal numbers of movements in each direction. If we had, for example, required far more leftward than rightward movements, this would have altered the distribution of joint activity and skewed the estimates of output-null activity during the second mapping. Second, this study focused on short timescales, where we predicted output-null activity within one to two hours of subjects learning a new BCI mapping. On this timescale, the motor system must be able to rapidly learn a variety of different mappings between neural activity and behavior, and thus, a variety of different sets of redundant activity. An interesting avenue for further research would be to determine if the constraints we observe on neural redundancy remain over longer timescales. Given repeated practice with the same BCI mapping across days and weeks (<xref ref-type="bibr" rid="bib20">Ganguly and Carmena, 2009</xref>), it is possible that there are different and perhaps fewer constraints on neural redundancy than what we found here.</p><p>We have tested six specific hypotheses about how neural redundancy is resolved. These hypotheses cover a spectrum of how strongly the activity in output-null dimensions is constrained, with the minimal firing hypotheses being the most constrained, the minimal intervention hypotheses being the least constrained, and the Fixed Distribution hypothesis lying in between. Although the hypotheses we tested are not exhaustive, the best hypothesis (Fixed Distribution) yielded predictions of the distributions of output-null activity whose marginal histograms differed from the data by only 13% on average (<xref ref-type="fig" rid="fig4">Figure 4F</xref>), where we estimated the lowest error possible to be 7% on average. Further improvements to the prediction accuracy may be possible by incorporating additional constraints, such as dynamics (<xref ref-type="bibr" rid="bib57">Shenoy et al., 2013</xref>). It should be stressed that our focus here was on predicting the <italic>distribution</italic> of output-null activity. Future work can assess whether output-null activity can be predicted on a time-step-by-time-step basis.</p><p>The central premise of the null space concept is that some aspects of neural activity are read out by downstream areas (output-potent) while other aspects are not (output-null) (<xref ref-type="bibr" rid="bib31">Kaufman et al., 2014</xref>). This idea is related to the study of noise correlations, where it was recognized that activity fluctuations that lie outside of a stimulus encoding space (i.e. ‘stimulus-null’) are not detrimental to the stimulus information encoded by the neurons (<xref ref-type="bibr" rid="bib3">Averbeck et al., 2006</xref>; <xref ref-type="bibr" rid="bib44">Moreno-Bote et al., 2014</xref>). Studies have also shown that structuring neural activity in an appropriate null space can allow for multiplexing of different types of information (<xref ref-type="bibr" rid="bib41">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Raposo et al., 2014</xref>), as well as stable behavior (<xref ref-type="bibr" rid="bib39">Leonardo, 2005</xref>; <xref ref-type="bibr" rid="bib51">Rokni et al., 2007</xref>; <xref ref-type="bibr" rid="bib1">Ajemian et al., 2013</xref>) and stable working memory (<xref ref-type="bibr" rid="bib12">Druckmann and Chklovskii, 2012</xref>; <xref ref-type="bibr" rid="bib45">Murray et al., 2017</xref>) in the presence of time-varying neural activity. Additionally, the existence of output-null dimensions in the motor system may facilitate motor learning (<xref ref-type="bibr" rid="bib43">Moorman et al., 2017</xref>; <xref ref-type="bibr" rid="bib49">Ranganathan et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Singh et al., 2016</xref>) or allow for motor preparation (<xref ref-type="bibr" rid="bib31">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Elsayed et al., 2016</xref>) or novel feedback processing (<xref ref-type="bibr" rid="bib61">Stavisky et al., 2017</xref>) without causing overt movement. Our work suggests that there may be limits on the extent to which output-null activity might be leveraged for neural computation. The coupling we observe between the distributions of output-null and output-potent activity suggests that output-null activity is not modified independently of output-potent activity. This coupling may cause activity fluctuations in a stimulus-null space to influence the downstream readout, or limit one’s ability to plan the next movement without influencing the current movement. Moving forward, an important direction for understanding the computations performed by different brain areas is to find out which aspects of the neural activity are read out (<xref ref-type="bibr" rid="bib47">Pagan et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Kaufman et al., 2014</xref>) and to understand how the dependencies like those identified in this study impact the computations being performed.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Defining the mapping between neural activity and cursor movement</title><p>Experimental methods are described in detail in both <xref ref-type="bibr" rid="bib53">Sadtler et al. (2014)</xref> and <xref ref-type="bibr" rid="bib23">Golub et al. (2018)</xref>. Briefly, we recorded from the proximal arm region of primary motor cortex (M1) in three male Rhesus macaques using implanted 96-channel microelectrode arrays (Blackrock Microsystems). All animal care and handling procedures conformed to the NIH Guidelines for the Care And Use of Laboratory Animals and were approved by the University of Pittsburgh’s Institutional Animal Care and Use Committee. The population spiking activity in each non-overlapping 45 ms bin was computed as the number of threshold crossings on each channel. In each session, 85–94 neural units were recorded (25 sessions from monkey J, six sessions from monkey L, 11 sessions from monkey N). These sessions were analyzed previously in <xref ref-type="bibr" rid="bib23">Golub et al. (2018)</xref>. Data from monkeys J and L were first presented in <xref ref-type="bibr" rid="bib53">Sadtler et al. (2014)</xref>. The average firing rate of the neural units per session was 50 ± 8, 42 ± 4, and 55 ± 14 spikes/s (mean ± s.d.) for monkeys J, L, and N, respectively.</p><p>Each session began with a block of calibration trials. The calibration procedure for monkey J involved either passive observation of cursor movement, or closed-loop BCI cursor control using the previous day’s BCI mapping. For monkeys L and N, we used a closed-loop calibration procedure that gradually stepped from passive observation to closed-loop control, as described in <xref ref-type="bibr" rid="bib53">Sadtler et al. (2014)</xref>. We then applied factor analysis (FA) to the spike counts recorded during these calibration trials to identify the 10D linear subspace (i.e. the ‘intrinsic manifold’) that captured dominant patterns of co-modulation across neural units (<xref ref-type="bibr" rid="bib8">Churchland et al., 2010</xref>; <xref ref-type="bibr" rid="bib27">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>; <xref ref-type="bibr" rid="bib2">Athalye et al., 2017</xref>). We then estimated the factor activity, <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, as the posterior expectation given the observed spike counts, <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf31"><mml:mi>q</mml:mi></mml:math></inline-formula> is the number of neural units:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>Ψ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf33"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle></mml:math></inline-formula> are FA parameters estimated using the expectation-maximization algorithm, where <inline-formula><mml:math id="inf34"><mml:mi>Ψ</mml:mi></mml:math></inline-formula> is constrained to be a diagonal matrix. The factor activity, <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, can be interpreted as a weighted combination of the activity of different neural units. We refer to <inline-formula><mml:math id="inf36"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as a ‘population activity pattern.’</p><p>We next orthonormalized <inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> so that it had units of spike counts per time bin (<xref ref-type="bibr" rid="bib70">Yu et al., 2009</xref>), using the following approach. In our FA model, <inline-formula><mml:math id="inf38"><mml:mi>L</mml:mi></mml:math></inline-formula> defines a mapping from low-dimensional factor space to the higher-dimensional neural space. Because the columns of <inline-formula><mml:math id="inf39"><mml:mi>L</mml:mi></mml:math></inline-formula> are not orthonormal, the factor activity does not have the same units (spikes counts per time bin) as the neural activity. However, we can fix this by finding an orthonormal basis for the columns of <inline-formula><mml:math id="inf40"><mml:mi>L</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib70">Yu et al., 2009</xref>). To do this, we apply the singular value decomposition, yielding <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>S</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>U</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>×</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> have orthonormal columns and <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is diagonal. Then, we can write <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Because <inline-formula><mml:math id="inf46"><mml:mi>U</mml:mi></mml:math></inline-formula> has orthonormal columns, <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> has the same units (spike counts per time bin) as <inline-formula><mml:math id="inf48"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. For notational simplicity, we refer to <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as <inline-formula><mml:math id="inf50"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> throughout. The values in <inline-formula><mml:math id="inf51"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> appear larger than those expected for a single neuron because this value tends to grow with the total number of neural units.</p><p>Over the course of each experiment, animals used two different BCI mappings (see ‘Behavioral task’ below). Each BCI mapping translated the resulting moment-by-moment factor activity (<inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) into a 2D cursor velocity (<inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) using a Kalman filter:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>c</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For the first BCI mapping, <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> were computed from the Kalman filter parameters, estimated using the calibration trials. For the second BCI mapping, we changed the relationship between population activity and cursor movement by randomly permuting the elements of <inline-formula><mml:math id="inf57"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> before applying <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. This permutation procedure can be formulated so that <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> still applies to the second BCI mapping, but with an updated definition of <inline-formula><mml:math id="inf58"><mml:mi>B</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>).</p></sec><sec id="s4-2"><title>Behavioral task</title><p>Each animal performed an 8-target center-out task by modulating its M1 activity to control the velocity of a computer cursor. Each session involved two different BCI mappings. The first mapping was chosen to be intuitive for the animal to use. The animal used this first mapping for 200–400 trials, after which the mapping was changed abruptly to a second BCI mapping. The second mapping was initially difficult for the animal to use, and the animal was given 400–600 trials to learn to use the second mapping. Both mappings were chosen to be within the animal’s instrinic manifold, mappings that we found in previous work could be readily learned within one session (<xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>).</p><p>At the beginning of each trial, a cursor appeared in the center of the workspace, followed by the appearance of one of eight possible peripheral targets (chosen pseudorandomly). For the first 300 ms of the trial, the velocity of the cursor was fixed at zero. After this, the velocity of the cursor was controlled by the animal through the BCI mapping. If the animal acquired the peripheral target with the cursor within 7.5 s, he received a water reward, and the next trial began 200 ms after target acquisition. Otherwise, the trial ended, and the animal was given a 1.5 s time-out before the start of the next trial.</p></sec><sec id="s4-3"><title>Session and trial selection</title><p>The data analyzed in this study were part of a larger study involving learning two different types of BCI mapping changes: within-manifold perturbations (WMP) and outside-manifold perturbations (OMP) (<xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>). We found that animals learned WMPs better than OMPs. Because we need animals to show stable cursor control under both mappings, we only analyzed WMP sessions in this study. Among the WMP sessions, we further selected those in which the animal learned stable control of the second mapping (42 selected and 12 discarded). This was important because performance with the second mapping was generally not as good as with the first mapping (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), and we wanted to ensure that any potential results were not due to incomplete learning of the second mapping (see also ‘Internal model estimation’ below). We further sub-selected from each session only those trials which exhibited stable behavioral performance, using a metric defined below. This was done to ensure that we were analyzing trials for which animals used a consistent strategy for selecting activity patterns.</p><p>We included sessions in which there existed a block of at least 100 consecutive trials that showed both substantial learning of the second mapping and consistent behavior. To identify trials showing substantial learning, we computed the running mean of the target acquisition time (on correct trials only), smoothed with a 100-trial boxcar shifted one trial at a time. The smoothed acquisition time for a trial corresponded to the average acquisition time within a 100-trial window centered on that trial. We then normalized these values so that 1 corresponded to the largest acquisition time in the first 50 trials using the second mapping, and 0 corresponded to the smallest acquisition time in the subsequent trials using the second mapping. We defined trials showing substantial learning as those with normalized acquisition times below 0.5. Next, to identify trials with consistent behavior, we computed the running variance of the target acquisition time. This was computed by taking the variance of the smoothed acquisition time above in a 100-trial boxcar, shifted one trial at a time. We then normalized these variances so that 1 corresponded to the largest variance in the first half of trials using the second mapping, and 0 corresponded to the smallest variance in any trial using the second mapping. We defined trials showing stable behavior as those with normalized variance below 0.5. We then identified blocks of consecutive trials that passed both of these criteria, joining blocks if they were separated by no more than 10 trials. We then selected the longest such block of at least 100 trials for our analyses. If no such block of trials was found, we excluded that session from our analyses. This procedure resulted in the 42 sessions across three monkeys that we included in our analyses.</p><p>We analyzed only successful trials. To avoid analyzing time steps with potentially idiosyncratic cursor control, we also ignored portions of the trial when the cursor was closer than 50 mm or more than 125 mm away from the origin. We repeated our analyses without the latter exclusion and obtained quantitatively similar results.</p></sec><sec id="s4-4"><title>Internal model estimation</title><p>When an animal uses a BCI mapping, its internal conception of the BCI mapping can differ from the actual BCI mapping, even during proficient control (<xref ref-type="bibr" rid="bib24">Golub et al., 2015</xref>). As a result, the animal’s conception of output-potent versus output-null dimensions can be different from those defined by the actual BCI mapping. To control for this possibility, we evaluated our predictions based on the animal’s internal conception of the output-null dimensions, rather than the actual output-null dimensions of the BCI mapping. This is particularly important for the second mapping, but we also did this for the first mapping. We used a method (Internal Model Estimation, IME) that we developed previously for estimating the animal’s internal model of the BCI mapping (<xref ref-type="bibr" rid="bib24">Golub et al., 2015</xref>), with the exception that here we apply the model directly to the factor activity (<inline-formula><mml:math id="inf59"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) as opposed to the neural activity (<inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), as was done in <xref ref-type="bibr" rid="bib24">Golub et al. (2015)</xref>.</p><p>The main idea of the IME framework is that the animal generates neural activity consistent with aiming straight to the target through an internal model of the BCI mapping. Due to natural visual feedback delay, the animal cannot exactly know the current cursor position, and thus aims from an internal estimate of the current cursor position. The internal estimate of the cursor position is a feedforward prediction based on previously issued neural activity and the most recently available visual feedback. <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4A</xref> shows a single-trial BCI cursor trajectory (black), along with the animal’s internal belief (red ‘whisker’) about how cursor position (red dots) evolved from the cursor position known from the most recently available visual feedback. The final segments of the trajectories reflect the same neural activity, which produces the actual cursor velocity (black arrow) through the actual BCI mapping, or the animal’s intended cursor velocity (red arrow) through the animal’s internal model. The animal’s velocity command viewed through the internal model points closer toward the target than the actual movement of the BCI cursor, corresponding to a smaller angular error. Across sessions, the animals’ angular errors when using the second BCI mapping did not usually return to the original level of error that the animal achieved under the first mapping (<xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>) (<xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4B</xref>). However, when viewed through the animals’ internal models of the BCI mappings, angular errors during the second mapping were more similar to those observed during the first mapping (<xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4C</xref>). Thus, the internal model helps to control for possible incomplete learning of the second mapping.</p><p>We used IME to obtain the animal’s internal model of the BCI mapping (in the form of <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>c</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), which yielded a corresponding set of cursor velocities (<inline-formula><mml:math id="inf62"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), cursor-target angles (<inline-formula><mml:math id="inf63"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), and bases for the output-potent and output-null dimensions of each mapping (see <inline-formula><mml:math id="inf64"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf65"><mml:mi>R</mml:mi></mml:math></inline-formula> below) that we used in our offline analyses. The results reported in the main text are based on these quantities obtained from IME. When we analyzed the data without using IME (i.e. using the actual output-null dimensions of the BCI mapping), all of the results we report still held (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>).</p></sec><sec id="s4-5"><title>Defining output-null activity</title><p>In <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, the matrix <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>B</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> linearly projects a 10-dimensional input (factor activity) to a 2-dimensional output (cursor velocity). Thus, for any given cursor velocity (<inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) there are multiple values of factor activity (<inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) that would produce it. These multiple values of factor activity are all behaviorally equivalent, and we refer to their existence as ‘neural redundancy.’</p><p>Mathematically, it is useful to consider the null space, <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>N</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, and the row space, <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, of the matrix <inline-formula><mml:math id="inf71"><mml:mi>B</mml:mi></mml:math></inline-formula>. The critical property of <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>N</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is that for any element <inline-formula><mml:math id="inf73"><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo><mml:mo>⊆</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, we have <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf75"><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. In other words, any change in activity within the null space of <inline-formula><mml:math id="inf76"><mml:mi>B</mml:mi></mml:math></inline-formula> has no effect on the cursor movement produced. On the other hand, to achieve a particular cursor velocity (<inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), there is exactly one <inline-formula><mml:math id="inf78"><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> such that <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>B</mml:mi><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Thus, the activity in the row space of <inline-formula><mml:math id="inf80"><mml:mi>B</mml:mi></mml:math></inline-formula> uniquely determines the cursor movement. To find a basis for <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>N</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, we took a singular value decomposition of <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>S</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where the diagonal elements of <inline-formula><mml:math id="inf84"><mml:mi>S</mml:mi></mml:math></inline-formula> were ordered so that only the first two values were nonzero. Then, we let <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> be the first two columns of <inline-formula><mml:math id="inf86"><mml:mi>V</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>N</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> be the remaining eight columns. The columns of <inline-formula><mml:math id="inf88"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf89"><mml:mi>R</mml:mi></mml:math></inline-formula> are mutually orthonormal and together form an orthonormal basis for the 10-dimensional space of factor activity. This allowed us to decompose the factor activity <inline-formula><mml:math id="inf90"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> at each time step into two orthogonal components: (1) activity in the row space of <inline-formula><mml:math id="inf91"><mml:mi>B</mml:mi></mml:math></inline-formula> that affects the cursor velocity, which we call the <italic>output-potent</italic> activity (<inline-formula><mml:math id="inf92"><mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>); and (2) activity in the null space of <inline-formula><mml:math id="inf93"><mml:mi>B</mml:mi></mml:math></inline-formula> that does not affect the cursor movement, which we call the <italic>output-null</italic> activity (<inline-formula><mml:math id="inf94"><mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>8</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>where</mml:mtext><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that all behaviorally equivalent activity will have the same output-potent activity (<inline-formula><mml:math id="inf95"><mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>), but can differ in output-null dimensions. Thus, for time steps with similar cursor movements, the subject’s choice of 8D output-null activity (<inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>) describes how the subject selected activity from a set of behaviorally equivalent options. Because the cursor velocity (<inline-formula><mml:math id="inf97"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) at each time step is a combination of output-potent activity and the cursor velocity at the previous time step (see <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), output-potent activity can be thought of as driving a change in the cursor velocity. Note that in the depictions of hypotheses in <xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, and <xref ref-type="fig" rid="fig4">Figure 4</xref>, we used <inline-formula><mml:math id="inf98"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> instead of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> for clarity.</p></sec><sec id="s4-6"><title>Predicting output-null activity</title><p>Our goal for each experiment was to predict the distribution of observed output-null activity during the second mapping across time steps corresponding to a given cursor movement direction (defined as the angle of <inline-formula><mml:math id="inf99"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). In the context of the center-out task, we assumed that cursor movements in the same direction but with different speeds were still behaviorally equivalent to the animal. This is supported by previous work that found substantially more direction-related information than speed-related information in both single-unit and population activity in M1 (<xref ref-type="bibr" rid="bib25">Golub et al., 2014</xref>). For this reason we assessed the output-null distribution in bins of cursor movement direction rather than cursor velocity (i.e. direction <inline-formula><mml:math id="inf100"><mml:mo>×</mml:mo></mml:math></inline-formula> speed).</p><p>All hypotheses generated predictions of the distribution of output-null activity observed while animals used the second BCI mapping, unless otherwise noted. To generate predictions of the distributions of output-null activity, we made predictions of the output-null activity at each time step. This allowed us to ensure that our predictions were consistent with the cursor kinematics observed during the experiment. We then aggregated the predictions across all time steps during the experiment with a similar cursor movement direction. In all cases, the predicted output-null activity respected the intrinsic manifold (<xref ref-type="bibr" rid="bib53">Sadtler et al., 2014</xref>), because the output-null activity lies in an <inline-formula><mml:math id="inf101"><mml:mn>8</mml:mn></mml:math></inline-formula>-dimensional subspace of the <inline-formula><mml:math id="inf102"><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>-dimensional intrinsic manifold.</p><p>To generate a prediction of the output-null activity for a particular time step (<inline-formula><mml:math id="inf103"><mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>), each hypothesis had access to three sources of information recorded during the experiments. First, all hypotheses used the observed output-potent activity (<inline-formula><mml:math id="inf104"><mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>), in order to ensure that every prediction was physiologically plausible (see below). Second, all hypotheses except for the Minimal Firing hypothesis utilized factor activity recorded during use of the first BCI mapping to form their predictions of output-null activity. Finally, the Persistent Strategy hypothesis also utilized the current position of the cursor relative to the target, defined as the cursor-target angle (<inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>).</p><p>We ensured that all predictions of output-null activity (<inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">z</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>) corresponded to physiologically plausible neural activity (<inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">u</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). By ‘physiologically plausible’ we mean that the neural activity was non-negative, and no greater than the maximum number of spikes (per 45 ms time step) observed for that neural unit during trials using the first BCI mapping (<inline-formula><mml:math id="inf108"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). To enforce the constraint, we either incorporated the constraint <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">u</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> directly in the optimization problem (Minimal Firing hypothesis), or rejected predictions of neural activity that fell outside of the constraint (all other hypotheses). In the latter case, we combined the predicted output-null activity with the observed output-potent activity at that time step to form the predicted factor activity (<inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">z</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). We then converted this value to neural activity using the FA generative model:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">u</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">z</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>If this neural activity was not physiologically plausible, we attempted to generate a new prediction of <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">z</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> according to the hypothesis. This was possible because all hypotheses incorporated some form of sampling to generate their predictions. If this procedure failed even after 100 attempts to generate a physiologically plausible prediction, we skipped making a prediction for that time step. This happened for less than 1% of all time steps.</p><sec id="s4-6-1"><title>Minimal firing hypotheses</title><p>According to the Minimal Firing hypothesis, generating spikes incurs a metabolic cost. Thus, the subject should select the population activity pattern that involves the fewest spikes among all patterns that generate the desired cursor movement. Predictions for this hypothesis were generated as follows. For each time step, we find the spiking activity closest to zero firing that produces the observed cursor velocity:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">u</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:mrow/><mml:munder><mml:mo form="prefix">min</mml:mo><mml:mi>u</mml:mi></mml:munder><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:msubsup><mml:mo stretchy="false">∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext> subject</mml:mtext><mml:mspace width="thinmathspace"/><mml:mtext>to</mml:mtext><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mtext>and</mml:mtext><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Above, <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> refers to the factor activity corresponding to <inline-formula><mml:math id="inf113"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:math></inline-formula>, as in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. Because <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is a linear function of <inline-formula><mml:math id="inf115"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:math></inline-formula>, the above minimization is a convex problem. <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the maximum activity level observed for each neuron, as described above. We solved for <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> at each time step <inline-formula><mml:math id="inf118"><mml:mi>t</mml:mi></mml:math></inline-formula> using Matlab’s quadprog. All trends in results were the same if the <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> norm in the optimization problem was changed to an <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> norm. After solving the above minimization, we incorporated variability in spike generation by sampling from a Poisson: <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>∼</mml:mo><mml:mtext>Poisson</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We repeated this last step if necessary until <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> was physiologically plausible. Finally, we converted the prediction to factor activity, so that the resulting prediction of <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">z</mml:mi><mml:mo mathvariant="bold">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> was <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">z</mml:mi><mml:mo mathvariant="bold">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msubsup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>We chose to incorporate Poisson variability into the predictions of the Minimal Firing (above) and Minimal Deviation hypotheses (below), rather than the Gaussian noise assumed by our FA model. The observed spike counts are discrete, whereas adding Gaussian noise would make the spike counts predicted by these hypotheses continuous. For this reason, to ensure a fair comparison we used Poisson variability, which will ensure the predictions remain discrete even after adding variability.</p><p>For the Minimal Deviation hypothesis, we generalized the Minimal Firing hypothesis so that instead of predicting the spiking activity nearest zero spikes/s, we predicted factor activity closest to some unknown activity level <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Solving this problem in the 10-dimensional factor space for the optimal value of <inline-formula><mml:math id="inf126"><mml:mi>η</mml:mi></mml:math></inline-formula> yields lower prediction error than doing so in the <inline-formula><mml:math id="inf127"><mml:mi>q</mml:mi></mml:math></inline-formula>-dimensional neural space because we ultimately evaluate the hypotheses’ predictions in factor space. After choosing <inline-formula><mml:math id="inf128"><mml:mi>η</mml:mi></mml:math></inline-formula> (see below), the predicted factor activity was obtained by solving the following optimization problem:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo form="prefix">min</mml:mo><mml:mi>z</mml:mi></mml:munder><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mo stretchy="false">∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext> subject</mml:mtext><mml:mspace width="thinmathspace"/><mml:mtext>to</mml:mtext><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The above problem is known as a ‘minimum norm’ problem, and it turns out that the resulting solution’s output-null activity, <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, is a constant, for all <inline-formula><mml:math id="inf130"><mml:mi>t</mml:mi></mml:math></inline-formula>:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mi>η</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Because of the simple form of this solution, it was possible to choose the best value of <inline-formula><mml:math id="inf131"><mml:mi>η</mml:mi></mml:math></inline-formula> for each session by minimizing the resulting output-null prediction error across cursor directions (see ‘Error in mean’ below). This value is:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi>η</mml:mi><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>8</mml:mn></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf132"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the average output-null activity in the <inline-formula><mml:math id="inf133"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> cursor direction bin, which we estimate using activity recorded during the first BCI mapping. This ensures that the data used to evaluate the predictions were not used to obtain <inline-formula><mml:math id="inf134"><mml:mi>η</mml:mi></mml:math></inline-formula>. Finally, we incorporated spiking variability just as we did for the Minimal Firing hypothesis. To do this, we first converted the above prediction (<inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) to neural activity using the FA generative model (<inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>). We then incorporated Poisson variability as described above, repeating the procedure until the resulting prediction was physiologically plausible, where the prediction of <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> was <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>∼</mml:mo><mml:mtext>Poisson</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-6-2"><title>Uncontrolled-uniform hypothesis</title><p>According to the uncontrolled manifold concept (<xref ref-type="bibr" rid="bib55">Scholz and Schöner, 1999</xref>), variability in output-null dimensions will be higher than that in output-potent dimensions. One explanation of this idea is the minimal intervention principle (<xref ref-type="bibr" rid="bib64">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib66">Valero-Cuevas et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Diedrichsen et al., 2010</xref>), which states that the variability in output-potent dimensions is controlled independently of the output-null activity, with the output-null activity being unmodified. While this principle specifies that output-null activity is independent of output-potent activity, it does not specify what the distribution of output-null activity actually is. Thus, we considered two hypotheses about this distribution. First, we supposed that the output-null activity would be uniformly distributed within bounds determined by the physiological range of population activity. This hypothesis thus predicts that activity in output-null dimensions has maximal entropy within the physiological range. For each <inline-formula><mml:math id="inf140"><mml:mi>t</mml:mi></mml:math></inline-formula>, we sampled:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>∼</mml:mo><mml:mtext>Uniform</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Above, <inline-formula><mml:math id="inf141"><mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf142"><mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> set the range on the minimum and maximum possible output-null activity. These bounds were set using population activity recorded during use of the first BCI mapping. We then resampled if necessary until our predictions generated physiologically plausible spiking activity when combined with the output-potent activity.</p><p>Note that in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, and <xref ref-type="fig" rid="fig4">Figure 4</xref> we applied PCA to the observed output-null activity to depict the three output-null dimensions with the most shared variance in the observed activity. Because of this, our visualizations of the distributions predicted by the Uncontrolled-uniform hypothesis in <xref ref-type="fig" rid="fig3">Figure 3</xref> appear mound-shaped rather than uniform. To understand this, suppose we sample from a uniform distribution over a rectangle in 2D. If we rotate this rectangle slightly, and visualize the distribution of points along the x-axis, the distribution will be mound-shaped. Similarly, the Uncontrolled-uniform hypothesis samples from a uniform distribution in the <inline-formula><mml:math id="inf143"><mml:mn>8</mml:mn></mml:math></inline-formula>-dimensional output-null space, where the bounds of the rectangle are determined by bf <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf145"><mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> above. Applying PCA rotates this activity, such that the density along the PC dimensions appears mound-shaped.</p></sec><sec id="s4-6-3"><title>Uncontrolled-empirical hypothesis</title><p>Next, we considered a different hypothesis about the distribution of output-null activity under the minimal intervention principle. Rather than assuming output-null activity is uniformly distributed, we obtained an empirical distribution using population activity observed under the first mapping. To produce predictions of output-null activity during the second mapping, for each time step during the second mapping we sampled randomly from the population activity observed under the first mapping, and assessed the projection of that activity in the null space of the second mapping.</p><p>Concretely, let <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> be the set of all time steps under the first mapping, and <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> be the set of all time steps under the second mapping. Our prediction for each <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is obtained by randomly sampling with replacement:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">∀</mml:mi><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In other words, at each time step using the second mapping, we randomly select factor activity observed during the first mapping (<inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), and project it into the null space of the second mapping (<inline-formula><mml:math id="inf150"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). We then resampled if necessary until our predictions generated physiologically plausible spiking activity when combined with the output-potent activity.</p></sec><sec id="s4-6-4"><title>Persistent Strategy hypothesis</title><p>An extension of the Uncontrolled-empirical hypothesis is motivated by the idea that the subject may select activity under one mapping by modifying the activity he used under the first mapping. For a given cursor-target angle, if the subject selects the same population activity under the second mapping as under the first mapping, that activity may not move the cursor towards the target under the second mapping. To correct the cursor movement, he modifies this activity according to the minimal intervention principle (<xref ref-type="bibr" rid="bib64">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib66">Valero-Cuevas et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Diedrichsen et al., 2010</xref>), correcting activity only along output-potent dimensions of the current mapping. Concretely, for each <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, we sampled with replacement:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">∀</mml:mi><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mtext> such</mml:mtext><mml:mspace width="thinmathspace"/><mml:mtext>that</mml:mtext><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>±</mml:mo><mml:msup><mml:mn>22.5</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf152"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the cursor-to-target angle at time <inline-formula><mml:math id="inf153"><mml:mi>t</mml:mi></mml:math></inline-formula>. As before, we resampled if necessary until our predictions generated physiologically plausible spiking activity when combined with the output-potent activity. This hypothesis is identical to the Uncontrolled-empirical hypothesis, except that at each time step we sampled only from time steps during the first mapping that had a similar cursor-target angle (i.e. within a 45<bold>°</bold> wedge around <inline-formula><mml:math id="inf154"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). We found no consistent improvements in varying the constraints on the cursor-target angle (i.e. using values other than 22.5<bold>°</bold> in <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>), or when using the output-potent angle rather than the cursor-target angle.</p></sec><sec id="s4-6-5"><title>Fixed Distribution hypothesis</title><p>According to the Fixed Distribution hypothesis, the activity in output-null dimensions is tightly coupled to the activity in output-potent dimensions, even under different BCI mappings when these dimensions are not necessarily still null and potent. This is in contrast to the three previous hypotheses (Uncontrolled-uniform, Uncontrolled-empirical, Persistent Strategy), which all incorporated a minimal intervention principle, whereby output-null activity can be modified independently of the output-potent activity, within the physiological limits on the firing rate of each unit.</p><p>Under the Fixed Distribution hypothesis, we predict that the distribution of output-null activity given the output-potent activity will be the same distribution as it was under the previous mapping. To implement this hypothesis, for each time step during the second mapping, we predict that the subject selects whichever activity pattern he produced under the previous mapping that would best match the current output-potent activity. Specifically, given the output-potent activity produced during the second mapping (<inline-formula><mml:math id="inf155"><mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>), we found the time step during the first mapping (<inline-formula><mml:math id="inf156"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) where the factor activity <inline-formula><mml:math id="inf157"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> would have come closest to producing that output-potent activity using the second mapping. Our prediction for output-null activity was then the output-null component of <inline-formula><mml:math id="inf158"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> through the second mapping (<inline-formula><mml:math id="inf159"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). Mathematically, for each <inline-formula><mml:math id="inf160"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> our prediction was:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext> where</mml:mtext><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo form="prefix">min</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>∥</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo stretchy="false">∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We observed that these predictions all satisfied physiological constraints, which suggests that the values of <inline-formula><mml:math id="inf161"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> selected at each time step each produced output-potent activity sufficiently close to <inline-formula><mml:math id="inf162"><mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p><p>This above implementation is also equivalent to the following: At each time step we identified the <inline-formula><mml:math id="inf163"><mml:mi>K</mml:mi></mml:math></inline-formula> previously observed population activity patterns that would produce output-potent activity closest to the current output-potent activity under the second mapping. We then selected one of these patterns at random, and used the output-null activity of that pattern as our prediction at that time step. In our above implementation, <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. We found that using other values of <inline-formula><mml:math id="inf165"><mml:mi>K</mml:mi></mml:math></inline-formula> (e.g. <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula>) yielded similar results.</p></sec></sec><sec id="s4-7"><title>Evaluating predictions</title><p>For each session, we evaluated the predicted output-null distributions of the above hypotheses in terms of how well they matched the observed output-null distributions for all time steps with similar cursor movements. To do this, we first grouped time steps by their corresponding cursor velocity into eight non-overlapping bins of cursor movement directions (<inline-formula><mml:math id="inf168"><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>±</mml:mo><mml:msup><mml:mrow><mml:mn>22.5</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>45</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>±</mml:mo><mml:msup><mml:mrow><mml:mn>22.5</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>315</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>±</mml:mo><mml:msup><mml:mrow><mml:mn>22.5</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>). We then evaluated the accuracy of the predictions for each cursor movement direction.</p><p>For consistency, all predictions were evaluated in terms of factor activity. The Minimal Firing hypothesis generated its predictions in terms of neural activity, and we converted these predictions to factor activity using <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>.</p><sec id="s4-7-1"><title>Histogram overlap</title><p>We compared the predicted and observed distributions of output-null activity in each dimension in terms of the average overlap of their histograms. For each session, we selected a single bin size for all histograms using cross-validation (<xref ref-type="bibr" rid="bib52">Rudemo, 1982</xref>). Then, for each cursor direction and output-null dimension, we computed the error between the observed (<inline-formula><mml:math id="inf169"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>) and predicted (<inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) histograms. Let <inline-formula><mml:math id="inf171"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> be the normalized frequency in the <inline-formula><mml:math id="inf172"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> bin, so that <inline-formula><mml:math id="inf173"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and similarly for <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then the histogram error was computed as follows:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Above, <inline-formula><mml:math id="inf175"><mml:mrow><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula> is included so that <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> if the two histograms are completely non-overlapping. <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> if the two histograms are identical. This error was then averaged across all cursor directions and output-null dimensions. We multiplied this value by 100 to yield the average histogram error percentages reported in the main text.</p><p>For the visualizations in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, and <xref ref-type="fig" rid="fig4">Figure 4</xref>, we displayed the marginal histograms in the three output-null dimensions with highest variance in the observed output-null activity, as found by PCA. For all error calculations we considered all eight output-null dimensions without applying PCA.</p></sec><sec id="s4-7-2"><title>Error in mean</title><p>We assessed how well our predictions matched the observed mean output-null activity for each cursor movement direction. For all time steps in the same cursor movement direction bin, let <inline-formula><mml:math id="inf178"><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>n</mml:mi></mml:mstyle></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> be the vector of the mean observed output-null activity, and <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> the mean output-null activity predicted by a particular hypothesis. These are both vectors, and so we computed the distance between them using the <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> norm:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=∥</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mo stretchy="false">∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>For each hypothesis, we computed the error in mean in each cursor movement direction bin, and took the average of these values as the error in mean for each session.</p></sec><sec id="s4-7-3"><title>Error in covariance</title><p>We next assessed how well our predictions matched the observed covariance of output-null activity for each cursor movement direction. Let <inline-formula><mml:math id="inf181"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> be the covariance of the observed and predicted output-null activity, respectively. There are a variety of methods for comparing covariance matrices, such as comparing their trace or determinant. We chose a metric invariant to affine transformations (e.g. scaling, translations, rotations) of the coordinate system (<xref ref-type="bibr" rid="bib13">Dryden et al., 2009</xref>). Because the amount of variance in the recorded population activity might vary from session to session, this property of affine invariance helps ensure we can reasonably compare our covariance errors across sessions.</p><p>Let <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> be the <inline-formula><mml:math id="inf184"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> generalized eigenvalue of <inline-formula><mml:math id="inf185"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (i.e. a value <inline-formula><mml:math id="inf187"><mml:mi>λ</mml:mi></mml:math></inline-formula> such that <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix" movablelimits="true">det</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Then following <xref ref-type="bibr" rid="bib32">Lang (1999)</xref> and <xref ref-type="bibr" rid="bib19">Förstner and Moonen (2003)</xref>, we computed the distance between these two matrices as:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mtext> </mml:mtext><mml:msup><mml:mi>log</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>If <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, then <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. For each hypothesis, we computed the error in covariance in each cursor movement direction bin, and took the average of these values as the error in covariance for each session.</p></sec><sec id="s4-7-4"><title>Error floor</title><p>To estimate the smallest errors achievable by any hypothesis (the ‘error floor’), given a limited number of samples to estimate the true output-null distributions, we performed the following analysis. For each session, we randomly split the data during the second mapping in half, and measured the histogram, mean, and covariance errors when using the output-null activity from one half to predict the distribution of the output-null activity during the other half. We repeated this process 100 times per session, and took the averages of the resulting errors as our estimates of the error floors for that session.</p></sec></sec><sec id="s4-8"><title>Activity that became output-null in the second mapping</title><p>We sought to assess whether the variance of population activity changed in dimensions that became output-null under the second mapping. To do this, we identified the subspace of activity that was output-potent under the first mapping, but output-null under the second mapping.</p><p>As before, let the columns of <inline-formula><mml:math id="inf191"><mml:mi>N</mml:mi></mml:math></inline-formula> be a basis for the null space of the second mapping. Now let the columns of <inline-formula><mml:math id="inf192"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> be a basis for the row space of the first mapping. Then the space spanned by the columns of <inline-formula><mml:math id="inf193"><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>)</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> describes the activity that would move the cursor during the first mapping but would not move the cursor during the second mapping. Let <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>S</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> be an orthonormal basis for <inline-formula><mml:math id="inf195"><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>)</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, which we obtained by performing a singular value decomposition. Now let <inline-formula><mml:math id="inf196"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> be a matrix of <inline-formula><mml:math id="inf197"><mml:mi>n</mml:mi></mml:math></inline-formula> factor activity patterns. To measure the amount of variance of <inline-formula><mml:math id="inf198"><mml:mi>Z</mml:mi></mml:math></inline-formula> in the subspace spanned by the columns of <inline-formula><mml:math id="inf199"><mml:mi>S</mml:mi></mml:math></inline-formula>, we computed <inline-formula><mml:math id="inf200"><mml:mrow><mml:mtext>Trace</mml:mtext><mml:mo>(</mml:mo><mml:mtext>Cov</mml:mtext><mml:mo>(</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mi>S</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>To assess how the variance of activity changes when it becomes irrelevant to cursor control, we grouped the time steps based on the cursor movement angle under the <italic>second</italic> mapping, for activity recorded under both the first and second mappings. First conditioning on the movement angle under the second mapping is consistent with our earlier analyses, when comparing the predicted and observed output-null distributions. To compute the cursor movement angle through the second mapping for activity recorded under the first mapping, we used the terms of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> not involving the cursor velocity at the previous time step (i.e. we computed <inline-formula><mml:math id="inf201"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>c</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula>). For consistency, we recomputed the cursor movement angle for activity recorded under the second mapping in the same way.</p><p>Let <inline-formula><mml:math id="inf202"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf203"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> be the factor activity in the same cursor movement angle bin recorded during the first and second mappings, respectively. We then computed the ratio of variance <inline-formula><mml:math id="inf204"><mml:mi>R</mml:mi></mml:math></inline-formula> as follows:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Trace</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Cov</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>Trace</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Cov</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The sign of <inline-formula><mml:math id="inf205"><mml:mi>R</mml:mi></mml:math></inline-formula> specifies whether the variance of activity increased (<inline-formula><mml:math id="inf206"><mml:mrow><mml:mi>R</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) or decreased (<inline-formula><mml:math id="inf207"><mml:mrow><mml:mi>R</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) when that activity became irrelevant to cursor control under the second mapping. We took the average of this ratio across all cursor movement direction bins to compute a ratio for each session.</p><p>To compute this ratio for the predictions of our hypotheses, as in <xref ref-type="fig" rid="fig6">Figure 6C</xref>, we substituted <inline-formula><mml:math id="inf208"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> with the predictions of our hypotheses, i.e. by combining their predicted output-null activity with the observed output-potent activity under the second mapping.</p><p>We also repeated the above analyses on our predictions of output-null activity produced during the first mapping using the activity observed under the second mapping, as shown in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. This was done by swapping the roles of the first and second mappings in the above analysis description.</p></sec><sec id="s4-9"><title>Distances of mean output-null activity from Minimal Firing and Minimal Deviation</title><p>For each cursor direction on each session, we computed the distance from the mean observed output-null activity to the mean predicted by the Minimal Deviation hypothesis, where the distance was computed as the <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> norm between the two 8D mean vectors. We then compared this distance to the distance between the mean predicted by Fixed Distribution and the mean predicted by Minimal Deviation (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). If the latter distance was consistently smaller than the former, this would be evidence that the observed mean output-null activity had moved towards the predictions of Minimal Deviation, relative to what was predicted by Fixed Distribution. We did not find evidence that this was the case (one-sided Wilcoxon signed rank test, <inline-formula><mml:math id="inf210"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>), suggesting that the mean observed output-null activity was not closer to Minimal Deviation than expected under Fixed Distribution. We repeated the same analysis using the mean predicted by Minimal Firing instead of Minimal Deviation, and reached the same results (one-sided Wilcoxon signed rank test, <inline-formula><mml:math id="inf211"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgments</title><p>The authors would like to thank Wilsaan Joiner and Doug Ruff for their feedback on the manuscript. This work was supported by NIH R01 HD071686 (APB, BMY, and SMC), NSF NCS BCS1533672 (SMC, BMY, and APB), NSF CAREER award IOS1553252 (SMC), NIH CRCNS R01 NS105318 (BMY and APB), Craig H Neilsen Foundation 280028 (BMY, SMC, and APB), Simons Foundation 364994 (BMY), and Pennsylvania Department of Health Research Formula Grant SAP 4100077048 under the Commonwealth Universal Research Enhancement program (SMC and BMY).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Writing—review and editing, Performed the animal experiments</p></fn><fn fn-type="con" id="con5"><p>Writing—review and editing, Performed the animal surgeries, Performed the animal experiments</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Writing—review and editing</p></fn><fn fn-type="con" id="con7"><p>Writing—review and editing, Performed the animal surgeries</p></fn><fn fn-type="con" id="con8"><p>Writing—review and editing, Performed the animal surgeries</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Supervision, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Supervision, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con11"><p>Conceptualization, Supervision, Writing—original draft, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All animal handling procedures were approved by the University of Pittsburgh Institutional Animal Care and Use Committee (protocol #15096685) in accordance with NIH guidelines. All surgery was performed under general anesthesia and strictly sterile conditions, and every effort was made to minimize suffering.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><object-id pub-id-type="doi">10.7554/eLife.36774.021</object-id><label>Supplementary file 1.</label><caption><title>Table of statistical tests.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-36774-supp1-v2.xlsx"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.36774.022</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-36774-transrepform-v2.pdf"/></supplementary-material><sec id="s8" sec-type="data-availability"><title>Data availability</title><p>Source data files have been provided for Figures 2-6. Code for analysis has been made available at <ext-link ext-link-type="uri" xlink:href="https://github.com/mobeets/neural-redundancy-elife2018">https://github.com/mobeets/neural-redundancy-elife2018</ext-link>, with an MIT open source license (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/neural-redundancy-elife2018">https://github.com/elifesciences-publications/neural-redundancy-elife2018</ext-link>).</p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ajemian</surname> <given-names>R</given-names></name><name><surname>D'Ausilio</surname> <given-names>A</given-names></name><name><surname>Moorman</surname> <given-names>H</given-names></name><name><surname>Bizzi</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A theory for how sensorimotor skills are learned and retained in noisy and nonstationary neural circuits</article-title><source>PNAS</source><volume>110</volume><fpage>E5078</fpage><lpage>E5087</lpage><pub-id pub-id-type="doi">10.1073/pnas.1320116110</pub-id><pub-id pub-id-type="pmid">24324147</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Athalye</surname> <given-names>VR</given-names></name><name><surname>Ganguly</surname> <given-names>K</given-names></name><name><surname>Costa</surname> <given-names>RM</given-names></name><name><surname>Carmena</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Emergence of coordinated neural dynamics underlies neuroprosthetic learning and skillful control</article-title><source>Neuron</source><volume>93</volume><fpage>955</fpage><lpage>970</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.016</pub-id><pub-id pub-id-type="pmid">28190641</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname> <given-names>BB</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural correlations, population coding and computation</article-title><source>Nature Reviews Neuroscience</source><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn1888</pub-id><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1969">1969</year><chapter-title>Trigger features, adaptation and economy of impulses</chapter-title><source>Information Processing in the Nervous System</source><publisher-loc>New York</publisher-loc><publisher-name>Springer Science &amp; Business Media</publisher-name><fpage>209</fpage><lpage>230</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bernstein</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1967">1967</year><chapter-title>The problem of interrelation between coordination and localization</chapter-title><source>The Coordination and Regulation of Movements</source><publisher-loc>New York</publisher-loc><publisher-name>Pergamon</publisher-name><fpage>15</fpage><lpage>59</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bjorck</surname> <given-names>A</given-names></name><name><surname>Golub</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Numerical methods for computing angles between linear subspaces</article-title><source>Mathematics of Computation</source><volume>27</volume><fpage>579</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.2307/2005662</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carmena</surname> <given-names>JM</given-names></name><name><surname>Lebedev</surname> <given-names>MA</given-names></name><name><surname>Crist</surname> <given-names>RE</given-names></name><name><surname>O'Doherty</surname> <given-names>JE</given-names></name><name><surname>Santucci</surname> <given-names>DM</given-names></name><name><surname>Dimitrov</surname> <given-names>DF</given-names></name><name><surname>Patil</surname> <given-names>PG</given-names></name><name><surname>Henriquez</surname> <given-names>CS</given-names></name><name><surname>Nicolelis</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Learning to control a brain-machine interface for reaching and grasping by primates</article-title><source>PLoS Biology</source><volume>1</volume><elocation-id>e42</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0000042</pub-id><pub-id pub-id-type="pmid">14624244</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Sugrue</surname> <given-names>LP</given-names></name><name><surname>Cohen</surname> <given-names>MR</given-names></name><name><surname>Corrado</surname> <given-names>GS</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name><name><surname>Clark</surname> <given-names>AM</given-names></name><name><surname>Hosseini</surname> <given-names>P</given-names></name><name><surname>Scott</surname> <given-names>BB</given-names></name><name><surname>Bradley</surname> <given-names>DC</given-names></name><name><surname>Smith</surname> <given-names>MA</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name><name><surname>Armstrong</surname> <given-names>KM</given-names></name><name><surname>Moore</surname> <given-names>T</given-names></name><name><surname>Chang</surname> <given-names>SW</given-names></name><name><surname>Snyder</surname> <given-names>LH</given-names></name><name><surname>Lisberger</surname> <given-names>SG</given-names></name><name><surname>Priebe</surname> <given-names>NJ</given-names></name><name><surname>Finn</surname> <given-names>IM</given-names></name><name><surname>Ferster</surname> <given-names>D</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Santhanam</surname> <given-names>G</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Stimulus onset quenches neural variability: a widespread cortical phenomenon</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>369</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1038/nn.2501</pub-id><pub-id pub-id-type="pmid">20173745</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Rugy</surname> <given-names>A</given-names></name><name><surname>Loeb</surname> <given-names>GE</given-names></name><name><surname>Carroll</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Muscle coordination is habitual rather than optimal</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>7384</fpage><lpage>7391</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5792-11.2012</pub-id><pub-id pub-id-type="pmid">22623684</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diedrichsen</surname> <given-names>J</given-names></name><name><surname>Shadmehr</surname> <given-names>R</given-names></name><name><surname>Ivry</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The coordination of movement: optimal feedback control and beyond</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>31</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.11.004</pub-id><pub-id pub-id-type="pmid">20005767</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driscoll</surname> <given-names>LN</given-names></name><name><surname>Pettit</surname> <given-names>NL</given-names></name><name><surname>Minderer</surname> <given-names>M</given-names></name><name><surname>Chettih</surname> <given-names>SN</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic reorganization of neuronal activity patterns in parietal cortex</article-title><source>Cell</source><volume>170</volume><fpage>986</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.07.021</pub-id><pub-id pub-id-type="pmid">28823559</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Druckmann</surname> <given-names>S</given-names></name><name><surname>Chklovskii</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuronal circuits underlying persistent representations despite time varying activity</article-title><source>Current Biology</source><volume>22</volume><fpage>2095</fpage><lpage>2103</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.08.058</pub-id><pub-id pub-id-type="pmid">23084992</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dryden</surname> <given-names>IL</given-names></name><name><surname>Koloydenko</surname> <given-names>A</given-names></name><name><surname>Zhou</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Non-Euclidean statistics for covariance matrices, with applications to diffusion tensor imaging</article-title><source>The Annals of Applied Statistics</source><volume>3</volume><fpage>1102</fpage><lpage>1123</lpage><pub-id pub-id-type="doi">10.1214/09-AOAS249</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsayed</surname> <given-names>GF</given-names></name><name><surname>Lara</surname> <given-names>AH</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reorganization between preparatory and movement population responses in motor cortex</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13239</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13239</pub-id><pub-id pub-id-type="pmid">27807345</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ettema</surname> <given-names>GJC</given-names></name><name><surname>Styles</surname> <given-names>G</given-names></name><name><surname>Kippers</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The moment arms of 23 muscle segments of the upper limb with varying elbow and forearm positions: Implications for motor control</article-title><source>Human Movement Science</source><volume>17</volume><fpage>201</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/S0167-9457(97)00030-4</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fagg</surname> <given-names>AH</given-names></name><name><surname>Shah</surname> <given-names>A</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A computational model of muscle recruitment for wrist movements</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>3348</fpage><lpage>3358</lpage><pub-id pub-id-type="doi">10.1152/jn.00621.2002</pub-id><pub-id pub-id-type="pmid">12466451</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farshchiansadegh</surname> <given-names>A</given-names></name><name><surname>Melendez-Calderon</surname> <given-names>A</given-names></name><name><surname>Ranganathan</surname> <given-names>R</given-names></name><name><surname>Murphey</surname> <given-names>TD</given-names></name><name><surname>Mussa-Ivaldi</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Sensory agreement guides kinetic energy optimization of arm movements during object manipulation</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004861</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004861</pub-id><pub-id pub-id-type="pmid">27035587</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinstein</surname> <given-names>B</given-names></name><name><surname>Lindegård</surname> <given-names>B</given-names></name><name><surname>Nyman</surname> <given-names>E</given-names></name><name><surname>Wohlfart</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1955">1955</year><article-title>Morphologic studies of motor units in normal human muscles</article-title><source>Cells Tissues Organs</source><volume>23</volume><fpage>127</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1159/000140989</pub-id><pub-id pub-id-type="pmid">14349537</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Förstner</surname> <given-names>W</given-names></name><name><surname>Moonen</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2003">2003</year><chapter-title>A metric for covariance matrices</chapter-title><source>Geodesy-the Challenge of the 3rd Millennium Berlin</source><publisher-loc>Heidelberg</publisher-loc><publisher-name>Springer-Verlag</publisher-name><fpage>299</fpage><lpage>309</lpage></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguly</surname> <given-names>K</given-names></name><name><surname>Carmena</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Emergence of a stable cortical map for neuroprosthetic control</article-title><source>PLoS Biology</source><volume>7</volume><elocation-id>e1000153</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000153</pub-id><pub-id pub-id-type="pmid">19621062</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilja</surname> <given-names>V</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Chestek</surname> <given-names>CA</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Fan</surname> <given-names>JM</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Kao</surname> <given-names>JC</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A high-performance neural prosthesis enabled by control algorithm design</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1752</fpage><lpage>1757</lpage><pub-id pub-id-type="doi">10.1038/nn.3265</pub-id><pub-id pub-id-type="pmid">23160043</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golub</surname> <given-names>MD</given-names></name><name><surname>Chase</surname> <given-names>SM</given-names></name><name><surname>Batista</surname> <given-names>AP</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Bm</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain-computer interfaces for dissecting cognitive processes underlying sensorimotor control</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>53</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.12.005</pub-id><pub-id pub-id-type="pmid">26796293</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golub</surname> <given-names>MD</given-names></name><name><surname>Sadtler</surname> <given-names>PT</given-names></name><name><surname>Oby</surname> <given-names>ER</given-names></name><name><surname>Quick</surname> <given-names>KM</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name><name><surname>Batista</surname> <given-names>AP</given-names></name><name><surname>Chase</surname> <given-names>SM</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning by neural reassociation</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>607</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0095-3</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golub</surname> <given-names>MD</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Chase</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Internal models for interpreting neural population activity during sensorimotor control</article-title><source>eLife</source><volume>4</volume><elocation-id>10015</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.10015</pub-id><pub-id pub-id-type="pmid">26646183</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golub</surname> <given-names>MD</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Schwartz</surname> <given-names>AB</given-names></name><name><surname>Chase</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Motor cortical control of movement speed with implications for brain-machine interface control</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>411</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1152/jn.00391.2013</pub-id><pub-id pub-id-type="pmid">24717350</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gray</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1918">1918</year><source>Anatomy of the Human Body</source><publisher-loc>Philadelphia</publisher-loc><publisher-name>Lea &amp; Febiger</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title><source>Nature</source><volume>484</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1038/nature10918</pub-id><pub-id pub-id-type="pmid">22419153</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauschild</surname> <given-names>M</given-names></name><name><surname>Mulliken</surname> <given-names>GH</given-names></name><name><surname>Fineman</surname> <given-names>I</given-names></name><name><surname>Loeb</surname> <given-names>GE</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cognitive signals for brain-machine interfaces in posterior parietal cortex include continuous 3D trajectory commands</article-title><source>PNAS</source><volume>109</volume><fpage>17075</fpage><lpage>17080</lpage><pub-id pub-id-type="doi">10.1073/pnas.1215092109</pub-id><pub-id pub-id-type="pmid">23027946</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Serruya</surname> <given-names>MD</given-names></name><name><surname>Friehs</surname> <given-names>GM</given-names></name><name><surname>Mukand</surname> <given-names>JA</given-names></name><name><surname>Saleh</surname> <given-names>M</given-names></name><name><surname>Caplan</surname> <given-names>AH</given-names></name><name><surname>Branner</surname> <given-names>A</given-names></name><name><surname>Chen</surname> <given-names>D</given-names></name><name><surname>Penn</surname> <given-names>RD</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neuronal ensemble control of prosthetic devices by a human with tetraplegia</article-title><source>Nature</source><volume>442</volume><fpage>164</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1038/nature04970</pub-id><pub-id pub-id-type="pmid">16838014</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>HJ</given-names></name><name><surname>Kram</surname> <given-names>R</given-names></name><name><surname>Ahmed</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reduction of metabolic cost during motor learning of arm reaching dynamics</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>2182</fpage><lpage>2190</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4003-11.2012</pub-id><pub-id pub-id-type="pmid">22323730</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cortical activity in the null space: permitting preparation without movement</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>440</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1038/nn.3643</pub-id><pub-id pub-id-type="pmid">24487233</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lang</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1999">1999</year><chapter-title>An Example of Seminegative Curvature</chapter-title><source>Fundamentals of Differential Geometry</source><publisher-loc>New York</publisher-loc><publisher-name>Springer Science &amp; Business Media</publisher-name><fpage>322</fpage><lpage>326</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lashley</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="1933">1933</year><article-title>Integrative functions of the cerebral cortex</article-title><source>Physiological Reviews</source><volume>13</volume><fpage>1</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1152/physrev.1933.13.1.1</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Nirenberg</surname> <given-names>S</given-names></name><name><surname>Synergy</surname> <given-names>NS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Synergy, redundancy, and independence in population codes, revisited</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>5195</fpage><lpage>5206</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5319-04.2005</pub-id><pub-id pub-id-type="pmid">15917459</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname> <given-names>SB</given-names></name><name><surname>de Ruyter van Steveninck</surname> <given-names>RR</given-names></name><name><surname>Anderson</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The metabolic cost of neural information</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>36</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1038/236</pub-id><pub-id pub-id-type="pmid">10195106</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Energy as a constraint on the coding and processing of sensory information</article-title><source>Current Opinion in Neurobiology</source><volume>11</volume><fpage>475</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(00)00237-3</pub-id><pub-id pub-id-type="pmid">11502395</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Law</surname> <given-names>AJ</given-names></name><name><surname>Rivlis</surname> <given-names>G</given-names></name><name><surname>Schieber</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rapid acquisition of novel interface control by small ensembles of arbitrarily selected primary motor cortex neurons</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>1528</fpage><lpage>1548</lpage><pub-id pub-id-type="doi">10.1152/jn.00373.2013</pub-id><pub-id pub-id-type="pmid">24920030</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lemon</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Descending pathways in motor control</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>195</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.060407.125547</pub-id><pub-id pub-id-type="pmid">18558853</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonardo</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Degenerate coding in neural systems</article-title><source>Journal of Comparative Physiology A</source><volume>191</volume><fpage>995</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1007/s00359-005-0026-0</pub-id><pub-id pub-id-type="pmid">16252121</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname> <given-names>WB</given-names></name><name><surname>Baxter</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Energy efficient neural codes</article-title><source>Neural Computation</source><volume>8</volume><fpage>531</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1162/neco.1996.8.3.531</pub-id><pub-id pub-id-type="pmid">8868566</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname> <given-names>V</given-names></name><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNeill Alexander</surname> <given-names>R</given-names></name><name><surname>McNeill</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Energetics and optimization of human walking and running: the 2000 Raymond Pearl memorial lecture</article-title><source>American Journal of Human Biology</source><volume>14</volume><fpage>641</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1002/ajhb.10067</pub-id><pub-id pub-id-type="pmid">12203818</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moorman</surname> <given-names>HG</given-names></name><name><surname>Gowda</surname> <given-names>S</given-names></name><name><surname>Carmena</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Control of redundant kinematic degrees of freedom in a closed-loop brain-machine interface</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>25</volume><fpage>750</fpage><lpage>760</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2016.2593696</pub-id><pub-id pub-id-type="pmid">27455526</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname> <given-names>R</given-names></name><name><surname>Beck</surname> <given-names>J</given-names></name><name><surname>Kanitscheider</surname> <given-names>I</given-names></name><name><surname>Pitkow</surname> <given-names>X</given-names></name><name><surname>Latham</surname> <given-names>P</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Information-limiting correlations</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1410</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1038/nn.3807</pub-id><pub-id pub-id-type="pmid">25195105</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>JD</given-names></name><name><surname>Bernacchia</surname> <given-names>A</given-names></name><name><surname>Roy</surname> <given-names>NA</given-names></name><name><surname>Constantinidis</surname> <given-names>C</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex</article-title><source>PNAS</source><volume>114</volume><fpage>394</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1073/pnas.1619449114</pub-id><pub-id pub-id-type="pmid">28028221</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orsborn</surname> <given-names>AL</given-names></name><name><surname>Moorman</surname> <given-names>HG</given-names></name><name><surname>Overduin</surname> <given-names>SA</given-names></name><name><surname>Shanechi</surname> <given-names>MM</given-names></name><name><surname>Dimitrov</surname> <given-names>DF</given-names></name><name><surname>Carmena</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Closed-loop decoder adaptation shapes neural plasticity for skillful neuroprosthetic control</article-title><source>Neuron</source><volume>82</volume><fpage>1380</fpage><lpage>1393</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.048</pub-id><pub-id pub-id-type="pmid">24945777</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagan</surname> <given-names>M</given-names></name><name><surname>Urban</surname> <given-names>LS</given-names></name><name><surname>Wohl</surname> <given-names>MP</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Signals in inferotemporal and perirhinal cortex suggest an untangling of visual target information</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1132</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1038/nn.3433</pub-id><pub-id pub-id-type="pmid">23792943</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picard</surname> <given-names>N</given-names></name><name><surname>Matsuzaka</surname> <given-names>Y</given-names></name><name><surname>Strick</surname> <given-names>PL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Extended practice of a motor skill is associated with reduced metabolic activity in M1</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1340</fpage><lpage>1347</lpage><pub-id pub-id-type="doi">10.1038/nn.3477</pub-id><pub-id pub-id-type="pmid">23912947</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranganathan</surname> <given-names>R</given-names></name><name><surname>Adewuyi</surname> <given-names>A</given-names></name><name><surname>Mussa-Ivaldi</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Learning to be lazy: exploiting redundancy in a novel task to minimize movement-related effort</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>2754</fpage><lpage>2760</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1553-12.2013</pub-id><pub-id pub-id-type="pmid">23407935</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname> <given-names>D</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A category-free neural population supports evolving demands during decision-making</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1784</fpage><lpage>1792</lpage><pub-id pub-id-type="doi">10.1038/nn.3865</pub-id><pub-id pub-id-type="pmid">25383902</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rokni</surname> <given-names>U</given-names></name><name><surname>Richardson</surname> <given-names>AG</given-names></name><name><surname>Bizzi</surname> <given-names>E</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Motor learning with unstable neural representations</article-title><source>Neuron</source><volume>54</volume><fpage>653</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.04.030</pub-id><pub-id pub-id-type="pmid">17521576</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudemo</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Empirical choice of histograms and kernel density estimators</article-title><source>Scandinavian Journal of Statistics</source><volume>9</volume><fpage>65</fpage><lpage>78</lpage></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadtler</surname> <given-names>PT</given-names></name><name><surname>Quick</surname> <given-names>KM</given-names></name><name><surname>Golub</surname> <given-names>MD</given-names></name><name><surname>Chase</surname> <given-names>SM</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Batista</surname> <given-names>AP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural constraints on learning</article-title><source>Nature</source><volume>512</volume><fpage>423</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1038/nature13665</pub-id><pub-id pub-id-type="pmid">25164754</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname> <given-names>E</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Berry</surname> <given-names>MJ</given-names></name><name><surname>Synergy</surname> <given-names>BMJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Synergy, redundancy, and independence in population codes</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>11539</fpage><lpage>11553</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-37-11539.2003</pub-id><pub-id pub-id-type="pmid">14684857</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholz</surname> <given-names>JP</given-names></name><name><surname>Schöner</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The uncontrolled manifold concept: identifying control variables for a functional task</article-title><source>Experimental Brain Research</source><volume>126</volume><fpage>289</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1007/s002210050738</pub-id><pub-id pub-id-type="pmid">10382616</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname> <given-names>SH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Optimal feedback control and the neural basis of volitional motor control</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>532</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1038/nrn1427</pub-id><pub-id pub-id-type="pmid">15208695</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical control of arm movements: a dynamical systems perspective</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>337</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150509</pub-id><pub-id pub-id-type="pmid">23725001</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname> <given-names>P</given-names></name><name><surname>Jana</surname> <given-names>S</given-names></name><name><surname>Ghosal</surname> <given-names>A</given-names></name><name><surname>Murthy</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Exploration of joint redundancy but not task space variability facilitates supervised motor learning</article-title><source>PNAS</source><volume>113</volume><fpage>14414</fpage><lpage>14419</lpage><pub-id pub-id-type="doi">10.1073/pnas.1613383113</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Softky</surname> <given-names>WR</given-names></name><name><surname>Kammen</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Correlations in high dimensional or asymmetric data sets: Hebbian neuronal processing</article-title><source>Neural Networks</source><volume>4</volume><fpage>337</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/0893-6080(91)90070-L</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname> <given-names>O</given-names></name><name><surname>Edelman</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Solving Bernstein's problem: a proposal for the development of coordinated movement by selection</article-title><source>Child Development</source><volume>64</volume><fpage>960</fpage><lpage>981</lpage><pub-id pub-id-type="doi">10.2307/1131321</pub-id><pub-id pub-id-type="pmid">8404271</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stavisky</surname> <given-names>SD</given-names></name><name><surname>Kao</surname> <given-names>JC</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Motor cortical visuomotor feedback activity is initially isolated from downstream targets in output-null neural state space dimensions</article-title><source>Neuron</source><volume>95</volume><fpage>195</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.023</pub-id><pub-id pub-id-type="pmid">28625485</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname> <given-names>DM</given-names></name><name><surname>Tillery</surname> <given-names>SI</given-names></name><name><surname>Schwartz</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Direct cortical control of 3D neuroprosthetic devices</article-title><source>Science</source><volume>296</volume><fpage>1829</fpage><lpage>1832</lpage><pub-id pub-id-type="doi">10.1126/science.1070291</pub-id><pub-id pub-id-type="pmid">12052948</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thoroughman</surname> <given-names>KA</given-names></name><name><surname>Shadmehr</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Electromyographic correlates of learning an internal model of reaching movements</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>8573</fpage><lpage>8588</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-19-08573.1999</pub-id><pub-id pub-id-type="pmid">10493757</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorov</surname> <given-names>E</given-names></name><name><surname>Jordan</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Optimal feedback control as a theory of motor coordination</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>1226</fpage><lpage>1235</lpage><pub-id pub-id-type="doi">10.1038/nn963</pub-id><pub-id pub-id-type="pmid">12404008</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorov</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Optimality principles in sensorimotor control</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>907</fpage><lpage>915</lpage><pub-id pub-id-type="doi">10.1038/nn1309</pub-id><pub-id pub-id-type="pmid">15332089</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valero-Cuevas</surname> <given-names>FJ</given-names></name><name><surname>Venkadesan</surname> <given-names>M</given-names></name><name><surname>Todorov</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Structured variability of muscle activations supports the minimal intervention principle of motor control</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>59</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1152/jn.90324.2008</pub-id><pub-id pub-id-type="pmid">19369362</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vyas</surname> <given-names>S</given-names></name><name><surname>Even-Chen</surname> <given-names>N</given-names></name><name><surname>Stavisky</surname> <given-names>SD</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural population dynamics underlying motor learning transfer</article-title><source>Neuron</source><volume>97</volume><fpage>1177</fpage><lpage>1186</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.040</pub-id><pub-id pub-id-type="pmid">29456026</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williamson</surname> <given-names>RC</given-names></name><name><surname>Cowley</surname> <given-names>BR</given-names></name><name><surname>Litwin-Kumar</surname> <given-names>A</given-names></name><name><surname>Doiron</surname> <given-names>B</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Smith</surname> <given-names>MA</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Byron</surname> <given-names>MY</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Scaling properties of dimensionality reduction for neural populations and network models</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005141</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005141</pub-id><pub-id pub-id-type="pmid">27926936</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wodlinger</surname> <given-names>B</given-names></name><name><surname>Downey</surname> <given-names>JE</given-names></name><name><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name><name><surname>Schwartz</surname> <given-names>AB</given-names></name><name><surname>Boninger</surname> <given-names>ML</given-names></name><name><surname>Collinger</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Ten-dimensional anthropomorphic arm control in a human brain-machine interface: difficulties, solutions, and limitations</article-title><source>Journal of Neural Engineering</source><volume>12</volume><elocation-id>016011</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/12/1/016011</pub-id><pub-id pub-id-type="pmid">25514320</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Santhanam</surname> <given-names>G</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gaussian-Process factor analysis for Low-Dimensional Single-Trial analysis of neural population activity</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>614</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1152/jn.90941.2008</pub-id><pub-id pub-id-type="pmid">19357332</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1 </title><sec id="s7" sec-type="appendix"><boxed-text><object-id pub-id-type="doi">10.7554/eLife.36774.023</object-id><p> To understand how our results might change if we recorded from more neural units, we assessed the dimensionality and shared variance of population activity with a varying number of units (<xref ref-type="bibr" rid="bib68">Williamson et al., 2016</xref>) (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). For each session, we fit factor analysis (FA) models (as defined in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) to subsets of varying numbers of units and identified the number of factors needed to maximize the cross-validated data likelihood. This resulted in estimates of the model parameters <inline-formula><mml:math id="inf212"><mml:mi>L</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf213"><mml:mi>Ψ</mml:mi></mml:math></inline-formula>. As in <xref ref-type="bibr" rid="bib68">Williamson et al. (2016)</xref>, dimensionality was defined as the number of eigenvectors of <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> needed to explain 95% of the shared variance. Concretely, if the eigenvalues of <inline-formula><mml:math id="inf215"><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are <inline-formula><mml:math id="inf216"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, then <inline-formula><mml:math id="inf217"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the smallest <inline-formula><mml:math id="inf218"><mml:mi>J</mml:mi></mml:math></inline-formula> such that <inline-formula><mml:math id="inf219"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:msubsup></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math></inline-formula>. Note that the absolute dimensionality depends on the method (% shared variance threshold) and criterion (threshold = 95%) used for assessing dimensionality. This is the same method used in <xref ref-type="bibr" rid="bib68">Williamson et al. (2016)</xref>, but differs slightly from the method used in <xref ref-type="bibr" rid="bib53">Sadtler et al. (2014)</xref>. We found that the dimensionality of the population activity increased with the number of units (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>). </p><p> As in <xref ref-type="bibr" rid="bib68">Williamson et al. (2016)</xref>, we computed the percentage of each neural unit’s activity variance that was shared with other recorded units (% shared variance). We calculated the average percent shared variance across neurons as follows: <disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>Percent</mml:mtext><mml:mspace width="thinmathspace"/><mml:mtext>shared</mml:mtext><mml:mspace width="thinmathspace"/><mml:mtext>variance</mml:mtext><mml:mspace width="thinmathspace"/><mml:mtext>for</mml:mtext><mml:mspace width="thinmathspace"/><mml:mtext>neuron</mml:mtext><mml:mspace width="thinmathspace"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mfrac><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf220"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the row of <inline-formula><mml:math id="inf221"><mml:mi>L</mml:mi></mml:math></inline-formula> corresponding to unit <inline-formula><mml:math id="inf222"><mml:mi>k</mml:mi></mml:math></inline-formula>. We found that the % shared variance initially increased with the number of units, then reached an asymptote, such that the % shared variance was similar with 30 and 85 units (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>). The results in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A–B</xref> imply that the top <inline-formula><mml:math id="inf223"><mml:mo>∼</mml:mo></mml:math></inline-formula>10 dimensions explain nearly all of the shared variance, and that additional dimensions identified by recording from more units explain only a small amount of additional shared variance. Thus, recording from more units beyond the <inline-formula><mml:math id="inf224"><mml:mo>∼</mml:mo></mml:math></inline-formula>85 units that we recorded in these experiments is not likely to reveal additional dimensions with substantial shared variance. We next measured the principal angles between modes identified using 30 units with those identified using 85 units (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C</xref>) (<xref ref-type="bibr" rid="bib6">Bjorck and Golub, 1973</xref>). Modes were defined as the eigenvectors of the shared covariance matrices corresponding to units from the 30-unit set (i.e. the eigenvectors of <inline-formula><mml:math id="inf225"><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf226"><mml:mi>L</mml:mi></mml:math></inline-formula> includes only the rows corresponding to the same 30 units). To restrict the analysis to the number of modes used to estimate the intrinsic manifold, only the ten modes explaining the most shared variance were included in the principal angle calculations. The small principal angles between modes identified using 30 and 85 units indicate that the dominant modes remained largely unchanged when using more units, in agreement with <xref ref-type="bibr" rid="bib68">Williamson et al. (2016)</xref>. These modes define the intrinsic manifold, the space within which we perform all of our analyses in the current work. Thus, recording from more units beyond the <inline-formula><mml:math id="inf227"><mml:mo>∼</mml:mo></mml:math></inline-formula>85 units that we recorded in these experiments is not likely to substantially change the results reported in this work.</p><fig id="app1fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.36774.024</object-id><label>Appendix 1—figure 1.</label><caption><title>Recording from more units is likely to reveal an intrinsic manifold similar to that identified in this study.</title><p>(<bold>A</bold>) We assessed the dimensionality (<inline-formula><mml:math id="inf228"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) of population activity after applying factor analysis to varying numbers of units from each session. Dimensionality is defined as the number of factors needed to explain 95% of the shared variance. Dimensionality increased with the number of units. Error bars depict mean <inline-formula><mml:math id="inf229"><mml:mo>±</mml:mo></mml:math></inline-formula> SE, across sessions. (<bold>B</bold>) We also computed the percentage of each neural unit’s activity variance that was shared with other recorded units (% shared variance). The % shared variance is based on the same factor analysis models identified in (<bold>A</bold>). The % shared variance initially increased with the number of units, then reached an asymptote, such that the % shared variance was similar with 30 and 85 units. Error bars depict mean <inline-formula><mml:math id="inf230"><mml:mo>±</mml:mo></mml:math></inline-formula> SE, across sessions. (<bold>C</bold>) We next measured the principal angles between the modes identified by factor analysis using 30 units with those identified using 85 units. Modes are defined as the eigenvectors of the shared covariance matrices corresponding to units from the 30-unit set. The small principal angles between modes identified using 30 and 85 units indicate that the dominant modes remained largely unchanged when using more units. Gray points represent principal angles between random 30-dimensional vectors. Error bars for black points depict mean <inline-formula><mml:math id="inf231"><mml:mo>±</mml:mo></mml:math></inline-formula> SE, across sessions, while error bars for gray points depict mean <inline-formula><mml:math id="inf232"><mml:mo>±</mml:mo></mml:math></inline-formula> s.d.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-36774-app1-fig1-v2"/></fig></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.36774.026</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Shea-Brown</surname><given-names>Eric</given-names></name><role>Reviewing Editor</role><aff><institution>University of Washington</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Constraints on neural redundancy&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Eric Shea-Brown as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Sabine Kastner as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Guillaume Hennequin (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>In this paper, Hennig et al. investigate the structure of output-null activity as recorded during BCI tasks in which these output-null directions are under their experimental control. They go systematically through a series of hypotheses regarding the marginal distributions of population activity along the few relevant output-null directions (contained within a subspace that already accounts for most of the variance in the population activity). These hypotheses are both very clearly articulated (e.g. organised in 3 categories) and their quantitative assessment is thorough. The results argue against hypotheses that null-space neural variability is either uncontrolled or determined by energy constraints (the minimal firing and minimal deviation hypotheses, which are well explained and clearly linked). Rather the work shows that activity in potent and null dimensions is surprisingly coupled, which constrains activity in the redundant dimensions, at least over the hundreds of trials following a switch in the BCI decoder. They accomplish this by exploiting an experimental setup and dataset, previously studied in the context of other relearning questions, where monkeys are first trained to perform a cursor &quot;reach&quot; task and then are trained to perform the same task with a different mapping of activity to response.</p><p>The dataset and analyses that the paper uses are highly interesting, and the questions they address about adaptation and function of high-dimensional neural activity are very timely. Manifold perturbations and learning over them via BCI decoders is a setup where clear hypotheses on neural variability and covariability can directly be formulated and tested. The analysis has a high level of complexity but a set of statistical controls are carried out in the supplementary material, as well as in, e.g., Figure 6, to show that the analysis itself is robust. The metrics of histogram error and error floors are well motivated and explained. The writing is very clear and well explained. The hypotheses tested seemed appropriate and were well justified. Despite some concerns about over-interpretation and one about novelty below, these features lend confidence in the authors' results.</p><p>Essential revisions:</p><p>Several concerns arose about over-interpretation and novelty, as well as data that should be included. These need to be addressed, and are as follows:</p><p>A1) Golub et al. (2018), by many of the same authors and using a similar dataset (or perhaps exactly the same dataset; it isn't clear but should be) – is a closely related paper, in which the authors conclude that monkeys continue to produce a fixed set of neural activity patterns when BCI mappings change. In other words, Golub et al., 2018 argues that the same fixed set of activity patterns are reassociated with different BCI outputs. What is the impact of this published result, and the allied covariance analysis in Golub et al., on the a priori viability of the different hypotheses that the authors test in the present paper? For example, it seems consistent with the present fixed distribution hypothesis, but perhaps not with first version of the minimal firing hypothesis? The authors should do a much better job of discussing this and clarifying what the present paper adds – both throughout the paper and in the Discussion – rather than only giving a one sentence mention of Golub 2018 as in the current Discussion.</p><p>A2) The authors give no justification for this statement: &quot;To fully utilize the proposed benefits of neural redundancy, there should be as few constraints as possible on which population activity patterns can be produced while leaving the readout unaffected.&quot;</p><p>Related to this: The authors state that &quot;the output-null space should not be thought of as a space in which neural activity can freely vary to carry out computations that are not reflected in output-potent activity. Instead, the output-potent component of the population activity pattern determines the distribution of the corresponding output-null activity.&quot; Do the results support this conclusion? The results affirm that the covariance structure of neurons within the manifold is persistent across their experimental tasks. But does this mean that the null activity is not used for computation or preparation?</p><p>A3) The paper is based on a center-out BCI task. However, there were no figures or data included on the behavioral performance. After reading the methods, I saw a paper referenced. It would be nice to include a figure on behavioral performance in the current manuscript, especially given that a third monkey appears to have been added following the previously published paper. It is helpful to the reader to have the behavioral data present in the current paper rather than having to go back to the previous work.</p><p>The following points require clarification or rewriting in the main text and/or addition to Discussion:</p><p>B1) &quot;Minimal intervention principle&quot; (MIP, in this review) is used to describe a set of alternatives that – for us at least – do not really embody this notion. Here, the authors refer to MIP as the neural activity not moving along directions that do not matter for cursor movement, i.e. output-null directions. This view might stem from the authors considering the cursor to be the &quot;plant&quot;, and M1 being the input to the plant. Another, perhaps more brain-centric view of the BCI control problem is to view both M1 and the cursor as the plant under control. After all, the brain needs to control the activity of M1 as part of controlling the cursor. Under this perspective, optimal control with MIP prescribes driving M1 with such minimal inputs as to adequately steer the activity in the directions that matter (output-potent) while being tolerant of fluctuations in output-null directions. Typically, though, such control inputs will generate activity trajectories that visit both subspaces in a coordinated way. For example, it might be that, due to the internal dynamics of M1, the most efficient way of moving its activity along an output-potent direction is to, in fact, make a pronounced excursion along an output-null direction. We are by no means rejecting hypotheses 3-5 as being irrelevant, but we think that MIP might in fact have more in common with hypothesis 6 (&quot;fixed strategy&quot;) in that optimal control of a few activity directions is likely to cause correlated activity across potent- and null-directions due to the intrinsic dynamics of the circuit.</p><p>B2) Concerning the last hypothesis (fixed distribution), we believe it could be tested using any relevant distribution of activity, not only the one recorded under the &quot;previous mapping&quot;; it seems that all that matters is that you have a &quot;reference distribution&quot; that predicts some degree of coupling/correlations between the two sets of potent/null directions that define the current mapping. This, for example, could be the activity used to define the &quot;intrinsic manifold&quot;. Have the authors considered testing this last hypothesis using such prior data? If it explains the structure of output-null activity equally well, this could add to the argument that output-null activity is mainly constrained by the dynamics of the circuit under control. The authors should comment on this, explain how this would fits into the results in the paper as a whole.</p><p>Related to this, it seems possible that the motor cortex activity could drive muscle movements, in addition to the BCI cursor. In this case, some of the output null dimensions could actually be related to output potent dimensions, just not for cursor movement. In this case, the so-called redundant or output null dimensions might not actually be redundant. Rather, they would be serving a different purpose, which could explain possible structure in their activity distributions. In this sense, I would not consider these redundant dimensions. Rather, the activity could be driving movements in a higher than 2D space if one considers both the BCI output and muscle movements. It would be helpful to see some description of how the authors have ruled out this possibility or why they have considered it to be a non-significant issue.</p><p>B3) Are the authors confident that, following a switch in the decoder, the animal has enough time with the new decoder for any expected null space changes to occur? The trial count into the hundreds does seem substantial, so this is not a big concern, though the authors might wish to comment on how long the animal had with the new decoder (especially whether this was over one or multiple sessions/days), in relationship to timescales of plasticity and expected changes in activity, in the Discussion.</p><p>B4) The BCI case is quite nice for studying this type of question. However, the dimensionality of the neural population controlling the BCI output is rather limited compared to perhaps a natural case of an entire motor cortical region capable of controlling a set of muscles. Dimensionality seems like it should play a significant role in considerations of redundancy, both on the neural end and the output end. It would be helpful to see more analysis or discussion of how neural and behavioral dimensionality, and perhaps most significantly the difference between the two, could play a role in interpretation and analysis of redundancy. For example, intuitively it seems possible that redundancy might be less constrained if 10,000 electrodes were used for BCI rather than 96. Is this true?</p><p>B5) The authors limit their analysis to the in-manifold perturbations of their experiment. The dataset has also out-of-manifold perturbations. Some comments on what is to be expected here seem worth mentioning in the Discussion.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.36774.027</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Several concerns arose about over-interpretation and novelty, as well as data that should be included. These need to be addressed, and are as follows:</p><p>A1) Golub et al. (2018), by many of the same authors and using a similar dataset (or perhaps exactly the same dataset; it isn't clear but should be) – is a closely related paper, in which the authors conclude that monkeys continue to produce a fixed set of neural activity patterns when BCI mappings change. In other words, Golub et al., 2018 argues that the same fixed set of activity patterns are reassociated with different BCI outputs. What is the impact of this published result, and the allied covariance analysis in Golub et al., on the a priori viability of the different hypotheses that the authors test in the present paper? For example, it seems consistent with the present fixed distribution hypothesis, but perhaps not with first version of the minimal firing hypothesis? The authors should do a much better job of discussing this and clarifying what the present paper adds – both throughout the paper and in the Discussion – rather than only giving a one sentence mention of Golub 2018 as in the current Discussion.</p></disp-quote><p>We agree with the suggestion entirely. As the reviewers describe, Golub et al. (2018) found evidence that the amount of learning in these BCI experiments was consistent with a fixed “neural repertoire”, whereby neural activity patterns were reassociated (“Reassociation”) to control the second BCI mapping. In the next few paragraphs we discuss the ways in which the results in the present work go beyond the results of Golub 2018, as well as how the hypotheses we test are consistent with Golub et al.’s finding of a fixed neural repertoire.</p><p>The analyses in the present work were developed in parallel with those of Golub 2018, and while we analyzed data from the same experiments, the two papers ask distinct questions. In Golub 2018, the focus is on the amount of learning observed in these experiments, and how changes in the structure of population activity enabled that learning. By contrast, in the present work we seek to determine the constraints on activity in the task-irrelevant (i.e. output-null) dimensions. In other words, while Golub 2018 focused on explaining the changes leading to <italic>behavioral</italic> learning, we focus here on the principles <italic>other than behavior</italic> that constrain population activity. As a result, all hypotheses we consider in the present work make predictions consistent with the observed amount of learning in the output-potent dimensions.</p><p>Golub 2018 does not rule out or disambiguate among most of the hypotheses we test here. The <italic>repertoire</italic> of population activity describes the set of population activity patterns that were observed, while the <italic>distribution</italic> describes the frequencies with which these different patterns occurred. In other words, the repertoire describes the “support” of the distribution. The majority of the hypotheses we test are consistent with a fixed neural repertoire of population activity, as observed in Golub 2018. This is evidenced by the plots of output-null distributions during an example session (Figures 2-4), where the predicted distributions largely overlap with the support of the actual data distributions. The two hypotheses that, as it turns out, are not fully consistent with a fixed repertoire are the Minimal Firing and Uncontrolled-uniform hypotheses. However, in the context of predicting the distribution of activity in redundant dimensions, these hypotheses represent interesting cases that readers are likely to consider (i.e. where neural activity either obeys minimal firing constraints, or that the null activity is fully unstructured, respectively), and so we believe they are hypotheses worth including as useful points of reference.</p><p>The finding of a Fixed Distribution in the present work goes beyond the predictions of a fixed repertoire in the following ways. First, while the Reassociation hypothesis of Golub 2018 predicts a fixed repertoire of activity, it does not directly describe its distribution. This is an important distinction because many different distributions of neural activity can be constructed from a fixed repertoire. Here we find that the distribution of population activity in output-null dimensions can be predicted from the output-potent activity. This was a surprising discovery, even in light of Golub 2018. Second, Golub 2018 do find that the covariance of activity in the potent dimensions of the BCI mappings is conserved during control of the second mapping (Figure 5A in Golub 2018), similar to what we find in Figure 6C of the present manuscript. However, the present results go beyond this, as we find that not only is the <italic>covariance</italic> preserved in these 2D subspaces of neural activity, but in fact the <italic>distribution</italic> of activity in the 8D output-null dimensions is preserved. This is a stronger constraint than what Golub 2018 identified. This finding is important because there are many ways in which the covariance of activity in a 2D subspace could be preserved, including cases where the distribution in the 8D null space is different; knowing that in fact the distribution in the 8D null space is preserved suggests that output-null activity is much more constrained by the output-potent activity than one could have concluded from this previous work.</p><p>To summarize, the results of the present work are that the distribution of output-null activity is predicted by the activity in only the two potent dimensions. This indicates that the output-null activity is coupled with the output-potent activity, implying that the number of independently controllable degrees of freedom that a neural population can express may be even lower than the intrinsic dimensionality of the population activity. We also found that principles inspired by the study of muscular redundancy, namely minimal firing and minimal intervention, were not the best predictors for the selection of redundant neural activity. These results cannot be (or are not easily) deduced from the results of Golub 2018.</p><p>We have added the following paragraphs to the Discussion relating the present results with those reported in Golub 2018:</p><p>“The results presented here are related to, and go beyond, those in Golub et al. (2018). […] However, in the context of predicting the distribution of activity in redundant dimensions, these hypotheses represent interesting cases worth considering (i.e. where population activity either obeys minimal firing constraints, or that the output-null activity is fully unstructured, respectively), and so we included these hypotheses to cover these possibilities.”</p><p>We have also clarified in the Introduction of the manuscript that the results of Golub 2018 do not imply the results in the present work:</p><p>“We tested all hypotheses in terms of their ability to predict the distribution of output-null activity, given the output-potent activity. […] Therefore, to understand the principles governing the selection among redundant population activity patterns, we focused on predicting the <italic>distribution</italic> of redundant population activity within the intrinsic manifold and neural repertoire.”</p><p>We have also updated the Materials and methods to clarify that we analyzed data from the same experiments as in Golub et al. (2018).</p><p>“Experimental methods are described in detail in both Sadtler et al. (2014) and Golub et al. (2018)…In each session, 85-94 neural units were recorded (25 sessions from monkey J, 6 sessions from monkey L, 11 sessions from monkey N). These sessions were analyzed previously in Golub et al. (2018). Data from monkeys J and L were first presented in Sadtler et al. (2014).”</p><disp-quote content-type="editor-comment"><p>A2) The authors give no justification for this statement: &quot;To fully utilize the proposed benefits of neural redundancy, there should be as few constraints as possible on which population activity patterns can be produced while leaving the readout unaffected.&quot;</p><p>Related to this: The authors state that &quot;the output-null space should not be thought of as a space in which neural activity can freely vary to carry out computations that are not reflected in output-potent activity. Instead, the output-potent component of the population activity pattern determines the distribution of the corresponding output-null activity.&quot; Do the results support this conclusion? The results affirm that the covariance structure of neurons within the manifold is persistent across their experimental tasks. But does this mean that the null activity is not used for computation or preparation?</p></disp-quote><p>These are all great points. To clarify what we meant by the statement in the Introduction, one proposed advantage of having redundancy in population activity is that it would allow the brain to perform computations without affecting readout. If there are constraints on this redundant activity (e.g., if task requirements on activity in the readout dimensions limit the redundant activity), this may limit the extent to which this activity is able to perform additional computations. We have rewritten the sentence in question so that this point is more clearly stated:</p><p>“To fully utilize the proposed benefits of neural redundancy, the population activity should be allowed to freely vary, as long as the readout of this activity remains consistent with task demands. This would allow the population activity to perform computations that are not reflected in the readout.”</p><p>We have clarified in the revised manuscript that our results do not imply that output-null activity is not used for computation (see below). Rather, we find that the output-potent activity likely constrains the extent to which output-null activity can be used for computation. Our work does not contradict previous findings (Kaufman et al., 2014; Elsayed et al., 2016) that computations can be carried out in the null space. Rather, we wish to prevent an overinterpretation of Kaufman and Elsayed, whereby readers might imagine that null space activity is completely available for computation. Our work suggests that the output-null distributions observed in these previous studies were limited or constrained by the different requirements for potent space activity during the two epochs in these studies (e.g., zero or constant potent activity during preparation, and non-zero or time-varying potent activity during execution).</p><p>In the other quoted sentence mentioned by the reviewers, we realized that the word “determines” was too strong and inadvertently implied that null space activity is not used for computation/preparation. Rather, we found that the distribution of null space activity was predicted by knowing the activity in potent/readout dimensions, indicating that output-potent and output-null activity do not vary independently. If the output-potent activity needs to be a certain value due to task demands, this can constrain how the output-null activity can vary, which likely constrains the computations that can be carried out in the output-null space. We have changed the last two sentences of the Introduction to clarify that our results suggest neural activity does not carry out null space computations <italic>independently</italic> of the output-potent activity, and we no longer use the word “determines”:</p><p>“Furthermore, the output-null space should not be thought of as a space in which neural activity can freely vary to carry out computations without regard to the output-potent activity. […] If the required output-potent activity is defined by the task demands, this can constrain how the output-null activity can vary, and correspondingly the computations that can be carried out in the output-null space.”</p><disp-quote content-type="editor-comment"><p>A3) The paper is based on a center-out BCI task. However, there were no figures or data included on the behavioral performance. After reading the methods, I saw a paper referenced. It would be nice to include a figure on behavioral performance in the current manuscript, especially given that a third monkey appears to have been added following the previously published paper. It is helpful to the reader to have the behavioral data present in the current paper rather than having to go back to the previous work.</p></disp-quote><p>We have added a supplementary figure including a summary of behavior for the included experiments (Figure 1—figure supplement 1). We have also updated the Materials and methods to clarify that the experimental data is the same as in Golub et al. (2018), and that indeed we now include data from a third monkey that was not included in Sadtler et al. (2014) (see end of response to A1 above).</p><disp-quote content-type="editor-comment"><p>The following points require clarification or rewriting in the main text and/or addition to Discussion:</p><p>B1) &quot;Minimal intervention principle&quot; (MIP, in this review) is used to describe a set of alternatives that – for us at least – do not really embody this notion. […] We are by no means rejecting hypotheses 3-5 as being irrelevant, but we think that MIP might in fact have more in common with hypothesis 6 (&quot;fixed strategy&quot;) in that optimal control of a few activity directions is likely to cause correlated activity across potent- and null-directions due to the intrinsic dynamics of the circuit.</p></disp-quote><p>We agree with the reviewers that another version of the minimal intervention principle (MIP) may apply <italic>upstream</italic> of M1 (so that both the cursor and M1 are the plant), and that from this perspective, one might expect that this would induce coupling between the output-null and output-potent dimensions of the BCI, and thus be entirely consistent with our findings of a Fixed Distribution. This exact hypothesis would be difficult to test directly with our current data (because we are not recording from any upstream areas), but could in the future be investigated by recording simultaneously from areas likely to drive M1 (e.g., PMd).</p><p>As the reviewers point out, there are many ways of applying MIP to neural activity, and in our case we interpret the cursor as the plant and M1 as the input to the plant. The fact that MIP, as we implement it in our hypotheses, did not accurately predict the output-null distribution does not rule out the possibility of some other implementation of MIP being relevant to describing neural activity. In our Discussion paragraph on optimal feedback control (OFC), we have clarified that our current application of OFC/MIP does not rule out the possibility that other versions of OFC/MIP are relevant to predicting neural activity:</p><p>“Overall, our work does not rule out the possibility that OFC is appropriate for predicting neural activity. […] This could induce coupling between the output-potent and output-null dimensions of the M1 activity, and thereby yield predictions that are consistent with the findings presented here.”</p><p>In this work we refer to the minimal intervention principle as the idea that activity in task-relevant (output-potent) dimensions can be corrected independently of activity in task-irrelevant (output-null) dimensions (Todorov et al., 2002; Valero-Cuevas et al., 2009; Diedrichsen et al., 2010). Typically, this idea has been applied to muscle activity, whereas in the present study we apply MIP to M1 activity. We have clarified our definition of MIP for neural activity in the Results:</p><p>“An explanation of this variability asymmetry is the “minimal intervention” principle (Todorov et al., 2002; Valero-Cuevas et al., 2009; Diedrichsen et al., 2010), which states that while variability in output-potent dimensions should be corrected to ensure task success, variability in output-null dimensions can be left uncorrected because it does not lead to deficits in task performance. While this principle has been used to explain muscle activity, here we investigate whether it also explains neural activity.”</p><disp-quote content-type="editor-comment"><p>B2) Concerning the last hypothesis (fixed distribution), we believe it could be tested using any relevant distribution of activity, not only the one recorded under the &quot;previous mapping&quot;; it seems that all that matters is that you have a &quot;reference distribution&quot; that predicts some degree of coupling/correlations between the two sets of potent/null directions that define the current mapping. This, for example, could be the activity used to define the &quot;intrinsic manifold&quot;. Have the authors considered testing this last hypothesis using such prior data? If it explains the structure of output-null activity equally well, this could add to the argument that output-null activity is mainly constrained by the dynamics of the circuit under control. The authors should comment on this, explain how this would fits into the results in the paper as a whole.</p></disp-quote><p>Indeed, the neural activity that is used to define the intrinsic manifold could be used as a “reference distribution” to predict the output-null activity expected during control of the first or second BCI mappings. However, this data is usually a small number of trials, and so would not provide a large enough sample of the neural activity to predict the output-null distribution expected during control of the BCI mapping. Having a sufficiently large number of samples is necessary because all but two of our hypotheses (Minimal Firing and Minimal Deviation) must use this activity to form predictions of the distributions of output-null activity expected for each direction of cursor movement.</p><p>In support of the idea that all you need is a “reference distribution” of neural activity, we predicted output-null activity in the opposite order to what was shown in the main text, i.e., we predicted output-null activity during the first (“Intuitive”) mapping using the activity observed during the second (“Perturbed”) mapping (Figure 5—figure supplement 2). The results were similar to those in the main text.</p><disp-quote content-type="editor-comment"><p>Related to this, it seems possible that the motor cortex activity could drive muscle movements, in addition to the BCI cursor. In this case, some of the output null dimensions could actually be related to output potent dimensions, just not for cursor movement. In this case, the so-called redundant or output null dimensions might not actually be redundant. Rather, they would be serving a different purpose, which could explain possible structure in their activity distributions. In this sense, I would not consider these redundant dimensions. Rather, the activity could be driving movements in a higher than 2D space if one considers both the BCI output and muscle movements. It would be helpful to see some description of how the authors have ruled out this possibility or why they have considered it to be a non-significant issue.</p></disp-quote><p>This is an interesting point to consider. In general, we agree with the reviewers that, if the dimensions responsible for moving the arm overlap with both the output-potent and output-null dimensions of the BCI, this might explain the coupling we observe between the output-potent and output-null dimensions. However, in these experiments, the animal’s arm was not moving during BCI control (see Extended Data Figure 5 in Sadtler et al., 2014). Thus, the activity we study here resides within the arm’s output-null dimensions. This implies that in our recordings the arm’s output-potent dimensions do not overlap with either the output-potent or the output-null dimensions of the BCI, and so this is unlikely to explain the coupling we observed between the output-potent and output-null dimensions of the BCI.</p><p>Overall, being unaware of extra output-potent dimensions would likely make the predictions of the Fixed Distribution hypothesis worse, not better. The reason for this is as follows. The Fixed Distribution hypothesis predicts that the distribution of activity in output-null dimensions depends upon the corresponding output-potent activity. Under this hypothesis, without knowing all of the output-potent activity, we cannot fully describe the corresponding output-null distribution. So if there is an output-potent dimension that we have not accounted for in our analyses (e.g., the dimension that helps keep the arm still), we would likely improve our predictions if we did account for this dimension. The fact that we predict so accurately (13% histogram error on average, with the lowest possible error being 7%) without knowing all the potent dimensions is then evidence that these extra output-potent dimensions, if they exist, would likely not provide substantial additional predictive power. We have updated the Discussion to address this possibility:</p><p>“It is interesting to consider the relationship between arm movements and BCI cursor movements (Orsborn et al., 2014; Vyas et al., 2018). […] The fact that we were able to accurately predict the output-null distributions (13% histogram error on average, with the lowest possible error being 7%) without knowing all the potent dimensions is then evidence that these extra potent dimensions, if they exist, would not provide substantial additional predictive power.”</p><disp-quote content-type="editor-comment"><p>B3) Are the authors confident that, following a switch in the decoder, the animal has enough time with the new decoder for any expected null space changes to occur? The trial count into the hundreds does seem substantial, so this is not a big concern, though the authors might wish to comment on how long the animal had with the new decoder (especially whether this was over one or multiple sessions/days), in relationship to timescales of plasticity and expected changes in activity, in the Discussion.</p></disp-quote><p>We agree that this is an important point to consider, which we bring out in Discussion</p><p>“Second, this study focused on short timescales, where we predicted output-null activity within one to two hours of subjects learning a new BCI mapping. […] Given repeated practice with the same BCI mapping across days and weeks (Ganguly et al., 2009), it is possible that there are different or fewer constraints on neural redundancy than what we found here.”</p><p>Performance with the second mapping was generally not as good as with the first mapping (Figure 1—figure supplement 1; see response to A3 above). To ensure that any potential results were not due to incomplete learning of the second mapping, we did two things. First, we restricted our analyses to the trials where monkeys had established stable performance with the new BCI mapping, as described in Materials and methods:</p><p>“We further selected those [sessions] in which the animal learned stable control of the second mapping. […] This was done to ensure that we were analyzing trials for which animals used a consistent strategy for selecting activity patterns.”</p><p>Second, we considered the possibility that animals may have incorrectly estimated which neural dimensions were output-potent in the BCI mapping and which were output-null. To control for this, we estimated which dimensions the animals believed to be output-null (Golub et al., 2015), and performed our analyses in the main text relative to these dimensions. We also repeated our analyses using the true output-null dimensions of the BCI and found similar results (Figure 5—figure supplement 3), suggesting that animals’ mis-estimates of the BCI mapping did not affect our analyses. We describe this approach in Results:</p><p>“Additionally, because animals learned to use the BCI mappings through trial and error, it is possible that the animals' assumptions about the output-null dimensions do not align perfectly with the actual output-null dimensions of the BCI mapping. […] The results in the main text are based on this internal model, and we show in supplemental figures that all results still hold when using the actual BCI mapping.”</p><disp-quote content-type="editor-comment"><p>B4) The BCI case is quite nice for studying this type of question. However, the dimensionality of the neural population controlling the BCI output is rather limited compared to perhaps a natural case of an entire motor cortical region capable of controlling a set of muscles. Dimensionality seems like it should play a significant role in considerations of redundancy, both on the neural end and the output end. It would be helpful to see more analysis or discussion of how neural and behavioral dimensionality, and perhaps most significantly the difference between the two, could play a role in interpretation and analysis of redundancy. For example, intuitively it seems possible that redundancy might be less constrained if 10,000 electrodes were used for BCI rather than 96. Is this true?</p></disp-quote><p>These are all great points. First, we address the impact of neural and output dimensionality on our results. In our setup, the dimensionality of the population activity is 10 (for the ~90 neural channels in our recordings), and the output (BCI) dimensionality is two. Given how many more dimensions of population activity there are than output dimensions, it came as a surprise to us that conditioning on only two neural dimensions (as we do in the present study) could provide so much explanatory power for predicting the distribution in the remaining neural dimensions. This suggests that many of the dimensions of population activity are coupled, i.e., changing the activity along some dimensions may also lead to changes along other dimensions. During arm movement control, output dimensionality and presumably the neural dimensionality are larger than in our setup. We speculate that during arm movements, many of the null dimensions will remain coupled with the potent dimensions, thereby yielding results similar to what we found in the present study. Of course, as the reviewers appreciate, the current barrier to testing these hypotheses using arm movements is the difficulty in identifying the output-potent dimensions. Until then, future work could examine the effects of larger output dimensionality on redundancy by repeating our analyses with a higher-dimensional BCI effector, such as a multiple degree of freedom robotic limb (e.g., Wodlinger et al., 2015). We have added the following to Discussion to address this point:</p><p>“Given how many dimensions of population activity there are (in this case, 10), it is somewhat surprising that conditioning on only the two output-potent dimensions could provide so much explanatory power for predicting the distribution in the remaining neural dimensions. […] Future work could examine whether animals can be trained to uncouple dimensions, as well as the effects of larger output-potent dimensionality on redundancy, by repeating our analyses with a higher-dimensional effector, such as a multiple degree-of-freedom robotic limb (e.g., Wodlinger et al., 2014).”</p><p>As for the impact of recording from 10,000 electrodes rather than 96, this is an interesting question. Because we analyzed neural activity in the low-d space identified using factor analysis (the 10D “intrinsic manifold” of Sadtler et al., 2014), we believe that our results would be qualitatively similar if we recorded from more units. The reason comes from Williamson et al. (2016), who used data from V1 recordings as well as network models to study how dimensionality and shared variance change as you record from more units. Using V1 data, they found that as the number of units recorded from increased, estimates of neural dimensionality also increased, but the same dominant (i.e. high covariance) modes that were identified with 20 units were also identified with 80 units. They then observed the same trend when using network simulations, where the number of simulated units varied from 20 units to 500 units. We repeated the same analyses using the recordings in the present study (varying the number of neural units from 2 to 85) and found similar results as the V1 data of the Williamson et al. study (Appendix—Figure 1). So even though recording from more neural units reveals more neural dimensions (Appendix—Figure 1A), these additional dimensions explain a very small proportion of the overall shared variance (Appendix—Figure 1 plateau in panel B). Thus, the effective dimensionality remained unchanged when going from 30 to 85 units. Furthermore, we found that these dimensions with high shared variance (i.e. the intrinsic manifold) were similar when increasing number of recorded units (Appendix—Figure 1C). Because all analyses in this work were carried out within the intrinsic manifold, we believe that our results would likely continue to hold with an even larger number of recorded units. We have added this analysis to Materials and methods, including a new Appendix—Figure 1:</p><p>“To understand how our results might change if we recorded from more neural units, we assessed the dimensionality and shared variance of population activity with a varying number of units (Williamson et al., 2016) (Appendix—Figure 1). […] Thus, recording from more units beyond the ~85 units that we recorded in these experiments is not likely to substantially change the results reported in this work.”</p><disp-quote content-type="editor-comment"><p>B5) The authors limit their analysis to the in-manifold perturbations of their experiment. The dataset has also out-of-manifold perturbations. Some comments on what is to be expected here seem worth mentioning in the Discussion.</p></disp-quote><p>For these analyses we need monkeys to show proficient cursor control using the two mappings presented in each session. However, as shown in Sadtler 2014, monkeys were less able to learn to control OMPs (outside-manifold perturbations) than WMPs (within-manifold perturbations) within a day. For OMPs, we would expect Fixed Distribution to do well for trivial reasons: If the monkey has not learned to control the cursor under the new mapping, the neural activity would likely not have changed much from what it was during the previous mapping. Thus, we decided to focus on WMPs in the present work.</p><p>We have clarified this point in the Materials and methods:</p><p>“The data analyzed in this study was part of a larger study involving learning two different types of BCI mapping changes: within-manifold perturbations (WMP) and outside-manifold perturbations (OMP) (Sadtler et al., 2014). […] Among the WMP sessions, we further selected those in which the animal learned stable control of the second mapping (42 selected and 12 discarded).”</p></body></sub-article></article>