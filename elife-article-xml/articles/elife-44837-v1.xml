<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">44837</article-id><article-id pub-id-type="doi">10.7554/eLife.44837</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Modulation of tonotopic ventral medial geniculate body is behaviorally relevant for speech recognition</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-128450"><name><surname>Mihai</surname><given-names>Paul Glad</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5715-6442</contrib-id><email>glad@posteo.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131271"><name><surname>Moerel</surname><given-names>Michelle</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131272"><name><surname>de Martino</surname><given-names>Federico</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131273"><name><surname>Trampel</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131274"><name><surname>Kiebel</surname><given-names>Stefan</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-131275"><name><surname>von Kriegstein</surname><given-names>Katharina</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7989-5860</contrib-id><email>katharina.von_kriegstein@tu-dresden.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Chair of Cognitive and Clinical Neuroscience, Faculty of Psychology</institution><institution>Technische Universität Dresden</institution><addr-line><named-content content-type="city">Dresden</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience</institution><institution>Maastricht University</institution><addr-line><named-content content-type="city">Maastricht</named-content></addr-line><country>Netherlands</country></aff><aff id="aff4"><label>4</label><institution>Maastricht Brain Imaging Center (MBIC)</institution><addr-line><named-content content-type="city">Maastricht</named-content></addr-line><country>Netherlands</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Maastricht Centre for Systems Biology (MaCSBio)</institution><institution>Maastricht University</institution><addr-line><named-content content-type="city">Maastricht</named-content></addr-line><country>Netherlands</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Center for Magnetic Resonance Research</institution><institution>University of Minnesota</institution><addr-line><named-content content-type="city">Minneapolis</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role>Reviewing Editor</role><aff><institution>Newcastle University</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>27</day><month>08</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e44837</elocation-id><history><date date-type="received" iso-8601-date="2019-01-09"><day>09</day><month>01</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-07-19"><day>19</day><month>07</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Mihai et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Mihai et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-44837-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.44837.001</object-id><p>Sensory thalami are central sensory pathway stations for information processing. Their role for human cognition and perception, however, remains unclear. Recent evidence suggests an involvement of the sensory thalami in speech recognition. In particular, the auditory thalamus (medial geniculate body, MGB) response is modulated by speech recognition tasks and the amount of this task-dependent modulation is associated with speech recognition abilities. Here, we tested the specific hypothesis that this behaviorally relevant modulation is present in the MGB subsection that corresponds to the primary auditory pathway (i.e., the ventral MGB [vMGB]). We used ultra-high field 7T fMRI to identify the vMGB, and found a significant positive correlation between the amount of task-dependent modulation and the speech recognition performance across participants within left vMGB, but not within the other MGB subsections. These results imply that modulation of thalamic driving input to the auditory cortex facilitates speech recognition.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>ventral MGB</kwd><kwd>thalamus</kwd><kwd>speech</kwd><kwd>top-down modulation</kwd><kwd>7T fMRI</kwd><kwd>auditory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max-Planck-Gesellschaft</institution></institution-wrap></funding-source><award-id>Research Group Grant and open-access funding</award-id><principal-award-recipient><name><surname>Mihai</surname><given-names>Paul Glad</given-names></name><name><surname>von Kriegstein</surname><given-names>Katharina</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>SENSOCOM (647051)</award-id><principal-award-recipient><name><surname>Mihai</surname><given-names>Paul Glad</given-names></name><name><surname>von Kriegstein</surname><given-names>Katharina</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max-Planck-Gesellschaft</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Trampel</surname><given-names>Robert</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>VIDI grant 864-13-012</award-id><principal-award-recipient><name><surname>de Martino</surname><given-names>Federico</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>VENI grant 451-15-012</award-id><principal-award-recipient><name><surname>Moerel</surname><given-names>Michelle</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Ultra-high field neuroimaging dissects the ventral medial geniculate body (vMGB) of the primary auditory pathway from other MGB subregions and reveals that vMGB top-down modulation is relevant for speech recognition.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Human communication relies on fast and accurate decoding of speech—the most important tool available to us for exchanging information. Understanding the neural decoding mechanisms for speech recognition is important for understanding human brain function (<xref ref-type="bibr" rid="bib92">Rauschecker and Scott, 2009</xref>), but also for understanding communication disorders such as developmental dyslexia (<xref ref-type="bibr" rid="bib41">Galaburda et al., 1994</xref>; <xref ref-type="bibr" rid="bib83">Müller-Axt et al., 2017</xref>). Since the early findings of <xref ref-type="bibr" rid="bib117">Wernicke (1874)</xref> neuroscientific models of speech recognition have mainly focused on cerebral cortex mechanisms (<xref ref-type="bibr" rid="bib55">Hickok and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib38">Friederici and Gierhan, 2013</xref>). Yet, more recently it has been suggested that a full understanding of speech recognition mechanisms might need to take the subcortical sensory pathways—particularly the sensory thalami—into account (<xref ref-type="bibr" rid="bib112">von Kriegstein et al., 2008a</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Díaz et al., 2018</xref>; <xref ref-type="bibr" rid="bib17">Chandrasekaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib18">Chandrasekaran et al., 2012</xref>).</p><p>The classic view, that the sensory thalamus is a passive relay station has been by-and-large abandoned over the last two decades. First of all, it is well known that there are strong corticofugal projections to the sensory thalamus (<xref ref-type="bibr" rid="bib102">Sherman and Guillery, 2006</xref>; <xref ref-type="bibr" rid="bib122">Winer and Prieto, 2001</xref>; <xref ref-type="bibr" rid="bib70">Lee and Sherman, 2012</xref>; <xref ref-type="bibr" rid="bib71">Lee and Winer, 2011</xref>). Furthermore, experimental evidence in humans and other mammals in the visual as well as the auditory modality has shown that sensory thalamus responses are modulated by attention (<xref ref-type="bibr" rid="bib95">Saalmann and Kastner, 2011</xref>), percept (<xref ref-type="bibr" rid="bib53">Haynes et al., 2005</xref>), context (<xref ref-type="bibr" rid="bib7">Antunes and Malmierca, 2011</xref>; <xref ref-type="bibr" rid="bib76">McAlonan et al., 2008</xref>; <xref ref-type="bibr" rid="bib84">O'Connor et al., 2002</xref>), and task (<xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>; <xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>; <xref ref-type="bibr" rid="bib29">Díaz et al., 2018</xref>). Based on these findings, the sensory thalamus has become accepted as a structure that is modulated by cognitive demands and is more involved in active information regulation (<xref ref-type="bibr" rid="bib95">Saalmann and Kastner, 2011</xref>; <xref ref-type="bibr" rid="bib53">Haynes et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">Antunes and Malmierca, 2011</xref>; <xref ref-type="bibr" rid="bib76">McAlonan et al., 2008</xref>; <xref ref-type="bibr" rid="bib84">O'Connor et al., 2002</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>; <xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>), for a different take see <xref ref-type="bibr" rid="bib16">Camarillo et al. (2012)</xref>.</p><p>In the case of speech, previous studies showed a task-dependent modulation in the auditory sensory thalamus for auditory speech recognition, (MGB; <xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>) as well as a task-dependent modulation in the visual sensory thalamus for visual speech recognition (LGN; <xref ref-type="bibr" rid="bib29">Díaz et al., 2018</xref>). In the MGB there were two findings: First, bilateral MGB showed significantly higher responses to an auditory speech recognition task than to control tasks, independent of attentional load or task-difficulty (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>). Second, the performance level in the auditory speech recognition task was significantly correlated with the task-dependent modulation in the MGB of the left hemisphere across participants (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>).</p><p>Following the Bayesian brain hypothesis, (<xref ref-type="bibr" rid="bib67">Knill and Pouget, 2004</xref>; <xref ref-type="bibr" rid="bib40">Friston and Kiebel, 2009</xref>; <xref ref-type="bibr" rid="bib39">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib65">Kiebel et al., 2008</xref>) and based on findings in non-human animals (<xref ref-type="bibr" rid="bib68">Krupa et al., 1999</xref>; <xref ref-type="bibr" rid="bib104">Sillito et al., 1994</xref>; <xref ref-type="bibr" rid="bib116">Wang et al., 2019</xref>), one possible explanation for the MGB task-dependent modulation for speech is that cerebral cortex areas tune the sensory thalamus depending on behavioral demand, and that this tuning is particularly relevant for fast-varying and predictable stimuli such as speech (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>). This view entails that the task-dependent modulation occurs already in those parts of the MGB that drive the cerebral cortex representations (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>)—the so-called first-order sensory thalamus (<xref ref-type="bibr" rid="bib101">Sherman and Guillery, 1998</xref>).</p><p>The MGB consists of three divisions. Only the ventral MGB (vMGB) can be considered first-order sensory thalamus (<xref ref-type="bibr" rid="bib74">Malmierca et al., 2015</xref> [review], <xref ref-type="bibr" rid="bib121">Winer et al., 2005</xref> [review]), as vMGB receives driving inputs from sources that relay information from the sensory periphery and projects this information to the cerebral cortex (<xref ref-type="bibr" rid="bib101">Sherman and Guillery, 1998</xref>). Ventral MGB also receives modulatory input from cerebral cortex (<xref ref-type="bibr" rid="bib101">Sherman and Guillery, 1998</xref>). In contrast, the other two MGB divisions, the dorsal (dMGB) and medial MGB (mMGB), do not show major projections to primary auditory cortices (<xref ref-type="bibr" rid="bib111">Vasquez-Lopez et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Anderson et al., 2007</xref>; <xref ref-type="bibr" rid="bib26">de la Mothe et al., 2006</xref>), and are not considered to be part of the first order (i.e., lemniscal) auditory pathway (<xref ref-type="bibr" rid="bib2">Anderson et al., 2007</xref>; <xref ref-type="bibr" rid="bib3">Anderson et al., 2009</xref>; <xref ref-type="bibr" rid="bib15">Calford, 1983</xref>; <xref ref-type="bibr" rid="bib21">Cruikshank et al., 2001</xref>; <xref ref-type="bibr" rid="bib45">Gonzalez-Lima and Cada, 1994</xref>; <xref ref-type="bibr" rid="bib50">Hackett et al., 1998</xref>; <xref ref-type="bibr" rid="bib81">Morest, 1964</xref>; <xref ref-type="bibr" rid="bib120">Winer et al., 1999</xref>); although see <xref ref-type="bibr" rid="bib4">Anderson and Linden (2011)</xref>.</p><p>The goal of the present study was to test whether the behaviorally relevant task-dependent modulation for speech is located in the first-order auditory thalamus; that is, the vMGB (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>). Localization of the behaviorally relevant task-dependent modulation for speech to the vMGB would provide a crucial step forward in understanding sensory thalamus function for human cognition in vivo, as it would imply that the stimulus representation in the auditory sensory pathway is modulated when humans recognize speech.</p><p>Due to the relatively small size of human MGB (ca. 5 × 4 × 5 mm, <xref ref-type="bibr" rid="bib119">Winer, 1984</xref>) and the spatial limitations of the non-invasive imaging techniques used in previous studies (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>), it was so far not possible to differentiate in which of the three major MGB divisions there is behaviorally relevant task-dependent modulation for speech. Here we, therefore, used ultra-high field functional magnetic resonance imaging (fMRI) at 7 Tesla, enabling high spatial resolution measurements (<xref ref-type="bibr" rid="bib31">Duyn, 2012</xref>). The vMGB has a strong tonotopic organization (<xref ref-type="bibr" rid="bib15">Calford, 1983</xref>; <xref ref-type="bibr" rid="bib93">Rodrigues-Dagaeff et al., 1989</xref>; <xref ref-type="bibr" rid="bib2">Anderson et al., 2007</xref>) while the other two MGB subsections have only a weak tonotopic organization (i.e., broadly tuned neurons; <xref ref-type="bibr" rid="bib4">Anderson and Linden, 2011</xref>; <xref ref-type="bibr" rid="bib15">Calford, 1983</xref>; <xref ref-type="bibr" rid="bib11">Bartlett and Wang, 2011</xref>; <xref ref-type="bibr" rid="bib93">Rodrigues-Dagaeff et al., 1989</xref>; <xref ref-type="bibr" rid="bib85">Ohga et al., 2018</xref>). We planned to distinguish the vMGB based on its tonotopic organization as well as its topographic (i.e., ventral) location.</p><p>We employed three fMRI paradigms—an MGB localizer, a tonotopy localizer, and the speech experiment. In the MGB localizer and the tonotopy localizer (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), participants listened to natural sounds (human voices, animal cries, tool sounds) (<xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>). The MGB localizer served to identify the left and right MGB. The tonotopic maps resulting from the tonotopy localizer were used to localize the left and right vMGB. These served as regions of interest for hypothesis testing in the speech experiment. In the speech experiment (<xref ref-type="fig" rid="fig1">Figure 1B and C</xref>), participants listened to blocks of auditory syllables (e.g.,/aba/), and performed either a speech or a speaker task. In the speech task, participants reported via button press whether the current syllable was different from the previous one (1-back task). In the speaker task, participants reported via button press whether the current speaker was different from the previous one.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.44837.002</object-id><label>Figure 1.</label><caption><title>MRI sequence acquisition and experimental design.</title><p>(<bold>A</bold>) MRI sequence acquisition of MGB and tonotopy localizer. Stimuli (‘sound’) were presented in silence periods between scan acquisitions and jittered with 2, 3, or 4 TRs. TR: repetition time of volume acquisition. (<bold>B</bold>) MRI sequence acquisition of the speech experiment. Each green or magenta rectangle of a block symbolizes a syllable presentation. Blocks had an average length of 17 s. Task instructions (‘speech’, ‘speaker’) were presented for 2 s before each block. MRI data were acquired continuously (‘scan acquisition’) with a TR of 1600 ms. (<bold>C</bold>) Design and trial structure of speech experiment. In the speech task, listeners performed a one-back syllable task. They pressed a button whenever there was a change in syllable in contrast to the immediately preceding one, independent of speaker change. The speaker task used exactly the same stimulus material and trial structure. The task was, however, to press a button when there was a change in speaker identity in contrast to the immediately preceding one, independent of syllable change. Syllables differed either in vowels or in consonants within one block of trials. An initial task instruction screen informed participants about which task to perform.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig1-v1.tif"/></fig><p>In previous studies we found the task-dependent modulation for speech (i.e., higher response in the speech in contrast to a control task on the same stimulus material) in both the left and right MGB and a correlation of the task-dependent modulation with speech recognition performance only in the left MGB (<xref ref-type="bibr" rid="bib112">von Kriegstein et al., 2008a</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>). Since our aim of the present paper was to test whether the behaviourally-relevant task dependent modulation of MGB is present in the vMGB, we tested two hypotheses. We hypothesized (i) a higher response to the speech than to the control (speaker) task in the tonotopically organized left and right vMGB, and (ii) a positive correlation between speech recognition performance and the task-dependent modulation for speech in the tonotopically organized left vMGB. Within our design these hypotheses could be addressed by (i) the main effect of task (speech task vs speaker task) in bilateral vMGB and by (ii) a correlation between the contrast speech task vs speaker task with speech recognition performance across participants in left vMGB.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Tonotopy localizer – replication of tonotopy in MGB</title><p>First, we replicated the MGB tonotopy reported previously by <xref ref-type="bibr" rid="bib79">Moerel et al. (2015)</xref> with a larger participant sample. Participants listened to natural sounds (human voices, animal cries, tool sounds) in a fast event-related scheme during silent gaps of the clustered imaging technique (<xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>) (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Using a model that mimics peripheral sound processing (<xref ref-type="bibr" rid="bib19">Chi et al., 2005</xref>), each sound was represented as a spectrogram. The resulting spectrograms were averaged over time and divided into ten equal bandwidths in octaves. Onsets for each bin were convolved with the hemodynamic response function and entered into the general linear model. Each voxel within each participant’s left and right MGB localizer mask was labeled according to the frequency bin to which it responded strongest, that is which had the highest parameter estimate (<xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>). Thus, voxels would have values from 1 to 10 corresponding to the frequency bin that they best represented. This resulted in a map of frequency distributions from low to high frequencies in the left and right MGB for each participant.</p><p>Similar as in <xref ref-type="bibr" rid="bib79">Moerel et al. (2015)</xref>, we found two tonotopic gradients within the MGB in the group analysis. On visual inspection, one high frequency region in the middle of the MGB was flanked by gradually lower frequency components dorsally and ventrally (<xref ref-type="fig" rid="fig2">Figure 2A and B</xref>). The two gradients run dorso-lateral to ventro-medial.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.44837.003</object-id><label>Figure 2.</label><caption><title>Visualization of the average tonotopy across participants (n = 28) found in the MGB using the tonotopic localizer.</title><p>The half-brain image at the top shows the cut through the brain with a red line denoting the −45° oblique plane used in the visualizations in panels A-B. (<bold>A</bold>) Three dimensional representation of the tonotopy in the left and right MGB with two low-high frequency gradients. (<bold>B</bold>) Same as in A with a different orientation. Crosshairs denote orientation (A: anterior, P: posterior, L: left, R: right, S: superior, I: inferior).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig2-v1.tif"/></fig><p>The regions of low and high frequency preference could be observed in the sagittal view. To quantify the tonotopic gradient direction, we calculated gradient angles in ten slices of the left and right tonotopic map in sagittal orientation. Histograms of gradient angles in 5° steps were calculated for each slice. The histograms of the gradients were then averaged first over slices per participant, followed by an average over participants. The analysis of the mean gradient distributions across individuals (<xref ref-type="fig" rid="fig3">Figure 3</xref>, black line with standard error of the mean in gray) for the left MGB had maxima at 130° and 300° (dashed red lines, <xref ref-type="fig" rid="fig3">Figure 3</xref>). In the right MGB the mean across individual distributions had maxima at 130° and 310°.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.44837.004</object-id><label>Figure 3.</label><caption><title>Distribution of gradients in a sagittal plane for ten slices averaged over participants (n = 28).</title><p>The mean number of angle counts in 5° steps (black line with standard error of the mean in gray, numbers indicate counts) for the left MGB have maxima at 130° and 300° (red dashed lines). For the right MGB the maximum gradients are at 130° and 310° (red dashed lines). We interpreted these as two gradients in each MGB: one from anterior-ventral to the center (130°) and the other from the center to anterior-dorsal-lateral (300°, 310°). The two outer images display a slice of the mean tonotopic map in the left and right MGB in sagittal view (S: superior, I: inferior, P: posterior, A: anterior).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44837.005</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Mean distribution of gradients for the inferior colliculus (IC) in coronal view for three slices averaged over participants (n = 28).</title><p>The mean number of angle counts in 5° steps (black line; numbers indicate counts) for the left IC cumulatively point towards 65°. For the right IC the general gradient direction was 285°. The two outer images display one slice of the mean tonotopic map across participants in the left and right IC in coronal view (S: superior, I: inferior, L: left, R: right). Individual tonotopies showed high variability (results not shown). The mean tonotopy revealed a gradient from low frequencies in lateral locations to high frequencies in medial locations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig3-figsupp1-v1.tif"/></fig></fig-group><p>For completeness we ran the same tonotopy analysis also on the inferior colliculi (IC). This analysis (n = 28) revealed a single gradient in the IC similarly to previous reports in the macaque (n = 3) (<xref ref-type="bibr" rid="bib12">Baumann et al., 2011</xref>) and human (n = 6; n = 5) (<xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">De Martino et al., 2013</xref>) (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></sec><sec id="s2-2"><title>Tonotopy localizer—Localisation of vMGB</title><p>We used the high frequency components in the middle of the MGB as a reference to subdivide the MGB volume into two regions per hemisphere (<xref ref-type="fig" rid="fig4">Figure 4A and B</xref>). For the left MGB, gradient one was located ventrally and slightly medial compared to gradient 2, which was situated more anterior, dorsal, and lateral. For the right MGB we found similar locations: gradient one was more ventral and medial compared to gradient 2. The center of mass (COM) and the volume for each region is summarized in <xref ref-type="table" rid="table1">Table 1</xref>. Based on the tonotopy and its ventral location (<xref ref-type="bibr" rid="bib80">Morel et al., 1997</xref>; <xref ref-type="bibr" rid="bib11">Bartlett and Wang, 2011</xref>) we considered gradient one to represent the vMGB (<xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>).</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.44837.006</object-id><label>Figure 4.</label><caption><title>Visualization of the tonotopic gradients found in the MGB based on the tonotopic localizer (see <xref ref-type="fig" rid="fig2">Figure 2</xref>).</title><p>(<bold>A</bold>) Three dimensional rendering of the two tonotopic gradients (yellow: ventro-medial gradient 1, interpreted as vMGB, cyan: dorso-lateral gradient 2) in the left and right MGB. (<bold>B</bold>) Same as in A with a different orientation. Orientation is the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>; crosshairs denote orientation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig4-v1.tif"/></fig><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.44837.007</object-id><label>Table 1.</label><caption><title>Center of mass (COM) and volume of each MGB mask used in the analysis.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Mask</th><th valign="top">COM (MNI coordinates mm)</th><th valign="top">Volume (mm³)</th></tr></thead><tbody><tr><td valign="top">Left Gradient 1 (ventro-medial)</td><td valign="top">(−12.7,–26.9, −6.3)</td><td valign="top">37.38</td></tr><tr><td valign="top">Left Gradient 2 (dorso-lateral)</td><td valign="top">(−14.8,–25.9, −5.4)</td><td valign="top">77.38</td></tr><tr><td valign="top">Right Gradient 1 (ventro-medial)</td><td valign="top">(12.7,–27.6, −4.4)</td><td valign="top">45.00</td></tr><tr><td valign="top">Right Gradient 2 (dorso-lateral)</td><td valign="top">(14.7,–25.8, −4.3)</td><td valign="top">67.38</td></tr></tbody></table></table-wrap></sec><sec id="s2-3"><title>Speech experiment</title><sec id="s2-3-1"><title>Behavioral results</title><p>Participants scored a mean hit rate in the speech task of 0.872 with 97% highest posterior density (HPD) interval [0.828, 0.915], and a mean hit rate in the speaker task of 0.760 with a 97% HPD interval [0.706, 0.810] (<xref ref-type="fig" rid="fig5">Figure 5</xref>; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). The mean hit-rate was 0.112 higher in the speech task than in the speaker task with 97% HPD interval [0.760, 0.150].</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.44837.008</object-id><label>Figure 5.</label><caption><title>Mean proportion of correct button presses for the speech and speaker task behavioral scores, as well as the difference between the speech and speaker task (n = 33).</title><p>Mean speech task: 0.872 with 97% HPD [0.828, 0.915], mean speaker task: 0.760 with 97% HPD [0.706, 0.800], mean speech vs speaker task: 0.112 with 97% HPD [0.760, 0.150]. Raw data provided in the Source Data File.</p><p> <supplementary-material id="fig5sdata1"><object-id pub-id-type="doi">10.7554/eLife.44837.011</object-id><label>Figure 5—source data 1.</label><caption><title>Raw behavioral correct button presses and total correct answers expected.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-44837-fig5-data1-v1.tds"/></supplementary-material> </p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44837.009</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Mean proportion of correct button presses for each condition.</title><p>The red dots and black numbers indicate estimated means, and the gray dots are individual participant responses averaged over the experiment. The violin plots summarize the distribution of data points. Raw data are provided in the Source Data File.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44837.010</object-id><label>Figure 5—figure supplement 2.</label><caption><title>Results from the reaction time analysis.</title><p>(<bold>A</bold>) Reaction times per condition summarized as a mean (black line) and 95% highest posterior density interval (gray area). The blue lines indicate averaged individual reaction times per condition. (<bold>B</bold>) The speech task reaction time is on average 78 ms quicker than the speaker task reaction time indicating that there is no speed-accuracy trade-off.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig5-figsupp2-v1.tif"/></fig></fig-group><p>An analysis of reaction times showed that there was no speed-accuracy trade-off for the two tasks (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p></sec><sec id="s2-3-2"><title>fMRI results</title><p>Using the fMRI data of the speech experiment we tested our two hypotheses. The first hypothesis was that within the ventral tonotopic gradient (i.e., vMGB) there is a task-dependent modulation (i.e., higher responses for the speech than the speaker task). Unexpectedly, there was no evidence for higher BOLD response in the speech task in comparison to the speaker task (speech vs speaker contrast) in vMGB.</p><p>Our second hypothesis was that there is a positive correlation between speech recognition performance and the amount of task-dependent modulation for speech (i.e., speech vs speaker contrast) in the left vMGB across participants. As expected, there was a significant correlation between the speech vs speaker contrast and mean percent correct speech recognition scores across participants in the left vMGB [MNI coordinate:<inline-formula><mml:math id="inf1"><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mn>11</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mo>-</mml:mo><mml:mn>28</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mo>-</mml:mo><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>; SVC for vMGB <inline-formula><mml:math id="inf2"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn></mml:math></inline-formula> FWE, <inline-formula><mml:math id="inf3"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>2.97</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.46</mml:mn></mml:math></inline-formula> using T to r transform from <xref ref-type="bibr" rid="bib36">Fisher (1915)</xref>; parameter estimate (β) and 90% CI 0.82 [0.36, 1.27]; <xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig7">7</xref>]. The correlation coefficient r=0.46 (R²=0.21) is considered to represent a large effect (<xref ref-type="bibr" rid="bib20">Cohen, 1988</xref> p. 80), explaining 21% of the variance of either variable when linearly associated with the variance in the other.</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.44837.012</object-id><label>Figure 6.</label><caption><title>Overlap between MGB divisions and the behaviourally relevant task-dependent modulation.</title><p>(<bold>A</bold>) The mean structural image across participants (n = 33) in MNI space. The red squares denote the approximate location of the left MGB and encompass the zoomed in view in B. (<bold>B</bold>) Overlap of correlation between the speech vs speaker contrast and the mean percent correct in the speech task (hot color code) across participants within the left vMGB (yellow). The tonotopic gradient two is shown in cyan. Panels correspond to sagittal, coronal, and axial slices (P: posterior, A: anterior, S: superior, I: inferior, L: left, R: right). Crosshairs point to the significant voxel using SVC in the vMGB mask (MNI coordinate −11,–28, −5).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44837.013</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Example of the orientation and volume covered by the 28 slices of the functional MRI measurements.</title><p>From left to right: left lateral sagittal view, medial sagittal view, and coronal view (left hemisphere is on the left).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44837.014</object-id><label>Figure 6—figure supplement 2.</label><caption><title>An example of an echo planar image in sagittal, axial and coronal view, as recorded in the speech experiment.</title><p>Wrapping artifacts were avoided by using appropriate phase over-sampling (33% larger FoV in phase-encoding direction A-P; A: anterior, P: posterior, L: left, R: right, S: superior, I: inferior).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig6-figsupp2-v1.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44837.015</object-id><label>Figure 6—figure supplement 3.</label><caption><title>Example workflow for the functional MRI analysis of the speech experiment as coded in nipype depicting different processing stages (preprocessing, Register to MNI, First level design and estimation, overlay output for quick inspection).</title><p>Each bubble represents a node with a descriptive name and the software package or utility in parentheses (io: input/output utility). Arrows indicate directional connections between nodes. The infosource utility takes the subject names and passes them on to the nodes that require this information (datasource, selectfiles, bbreg, fssource, gestubinforuns). Datasource and selectfiles select the files needed for the analysis. The functional runs are first realigned and unwarped (RealignUnwarp). The mean realigned image together with the structural image are used in the bbreg step to perform the boundary based registration. Convert2itk (c3: c3d converts 3D images between common file formats, https://sourceforge.net/p/c3d/git/ci/master/tree/doc/c3d.md) converts the rotation/translation matrix from bbreg to ITK format in order to be able to use ANTs to apply the coregistration transformations of the functional to structural runs in the warpbbreg step. The coregistered runs are then smoothed (smooth_native). Outliers are computed in the art (rapidart) step. Model generation is implemented in modelspec, where smoothed images, regressors of interest (onsets, durations convolved with HRF from getsubinforuns), and regressors of no interest (realignment parameters, outliers, physiological regressors, button presses) are gathered. These are modeled in the level1design step in SPM, estimated in level1estimate, and contrasts calculated in the contrast step. The two ANTs registrations to MNI (antsRegister_init and antsRegister_mask) are computed in parallel. Contrast warping to MNI space is computed in the warpcon step, and individual structural images are warped to MNI space in the warpstruc step. Using FSL statistical maps are overlayed in overlaystats and slicestats for quick visual inspection. The sinker collects desired files written in the temporary directory and organizes them according to subject name and computational step for easy access. Data processing workflows for MGB and tonotopic localizer follow the one for the speech experiment. Transformation matrices to MNI space were taken from the speech experiment computation, as data from all experiments need to be in the same space.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig6-figsupp3-v1.tif"/></fig></fig-group><fig-group><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.44837.016</object-id><label>Figure 7.</label><caption><title>Task-dependent modulation of left vMGB correlates with proportion correct responses in the speech task over participants (n = 33): the better the behavioral score in the speech task, the stronger the BOLD response difference between speech and speaker task in the left vMGB (maximum statistic at MNI coordinate [−11,–28, −5].</title><p>The line represents the best fit with 97% bootstrapped confidence interval (gray shaded region).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44837.017</object-id><label>Figure 7—figure supplement 1.</label><caption><title>Correlation of parameter estimates (Speech vs. Speaker) for the significant voxel in the vMGB in the speech vs. speaker task with percent correct behavioral score in the speech task.</title><p>The cyan squares denote those participants who scored below 50% in the reading speed and comprehension test. A lower reading speed and comprehension score in these typically developed participants did not imply a lower vMGB response.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig7-figsupp1-v1.tif"/></fig></fig-group><p>To check whether potential outliers were driving the correlation, we excluded those data points that were two standard deviations away from the parameter estimate mean. One data point was outside this threshold. The re-calculated correlation was very similar to the one with all data points (<inline-formula><mml:math id="inf4"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>2.92</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.038</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf6"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.46</mml:mn></mml:math></inline-formula>), indicating that the correlation was robust to outlier removal.</p></sec><sec id="s2-3-3"><title>Meta-analysis of the main effect of task (speech vs speaker contrast)</title><p>We performed a random effects meta-analysis to test whether the (non-significant) effect of the main effect of task in the present study (i.e., speech vs speaker task contrast) was different from other studies that have reported a significant task-dependent MGB modulation for speech. We included five studies in the meta-analysis that each contained a speech task vs control task contrast: two experiments from <xref ref-type="bibr" rid="bib113">von Kriegstein et al. (2008b)</xref>, the data from the control participants of <xref ref-type="bibr" rid="bib28">Díaz et al. (2012)</xref>, the result of a recent study (<xref ref-type="bibr" rid="bib78">Mihai et al., 2019</xref>), and the result of the current study. The meta-analysis yielded an overall large effect size of d = 0.85 [0.06, 1.65], p=0.036 (<xref ref-type="fig" rid="fig8">Figure 8</xref>). The analysis showed that four of the five experiments had a positive medium to large effect and only the current study had a very small insignificant negative effect. The confidence intervals from the current study also do not overlap with those of the other studies. In terms of equivalence testing (<xref ref-type="bibr" rid="bib99">Schuirmann, 1987</xref>), this means that the result of the speech task vs control task contrast of the current study is different from the other studies compared here. We detail potential reasons for this difference in the task-dependent modulation between the studies in the discussion.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.44837.018</object-id><label>Figure 8.</label><caption><title>Meta-analysis of five experiments that investigated the difference between a speech and a non-speech control task.</title><p>Experiment 1 of <xref ref-type="bibr" rid="bib113">von Kriegstein et al. (2008b)</xref> tested a speech task vs loudness task contrast (n = 16). All other experiments included a speech task vs speaker task contrast (i.e., Experiment 2 of <xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref> (n = 17), the control participants of <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref> (n = 14), the result of a recent experiment (<xref ref-type="bibr" rid="bib78">Mihai et al., 2019</xref>) (n = 17) as well as the current study (n = 33)). The meta-analysis yielded an overall large effect size of d = 0.85 [0.06, 1.65], p=0.036. The area of the squares denoting the effect size is directly proportional to the weighting of the particular study when computing the meta-analytic overall score.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44837-fig8-v1.tif"/></fig></sec><sec id="s2-3-4"><title>Exploratory analyses</title><p><italic>Specificity of the behaviorally relevant task-dependent modulation for speech.</italic> In exploratory control analyses we checked whether we could test for a specificity of the correlation between the task-dependent modulation for speech (i.e., the speech task vs speaker task contrast) and the speech recognition behavior across participants. This was, however, not possible. Performance in the speech and the speaker tasks was significantly correlated (<italic>r</italic> = 0.77, p&lt;0.001). Accordingly, there was no difference in the correlation between the contrast speech vs speaker task and the speech recognition scores and the correlation of the contrast speech vs speaker with the speaker recognition score (z = −0.717, p=0.474). The results indicated that although there is a behaviorally relevant task-dependent modulation in the vMGB for speech, we currently do not know whether it is specific to speech recognition abilities.</p><p><italic>Specificity of the behaviorally relevant task-dependent modulation to the vMGB.</italic> In an exploratory analysis we checked whether the correlation between the task-dependent modulation for speech and the speech recognition behavior across participants was specific to the vMGB in contrast to other MGB subsection. There was no correlation (speech vs speaker task correlated with speech recognition performance across participants) in the left MGB–gradient 2 (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). The comparison of the two correlations (i.e., for left vMGB and left MGB-gradient 2) showed that they are significantly different (z = 4.466, p&lt;0.0001). Thus, this is a first indication that the behaviorally relevant task-dependent modulation for speech is specific to the left vMGB.</p><p><italic>Exploration of other subcortical areas.</italic> In exploratory analyses we tested the main effect of task (speech vs speaker contrast) and the correlation of the speech vs speaker contrast with behavioral performance in the speech task in several areas outside the vMGB, for which we did not have an a priori hypothesis. There were no significant effects in any other regions (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Using ultra-high field fMRI we showed that it is the left auditory first-order sensory thalamus – the left ventral subdivision of the MGB (vMGB) – that is modulated during speech recognition. The vMGB is the primary sensory pathway nucleus of the auditory thalamus and transmits input to the cerebral cortex (<xref ref-type="bibr" rid="bib119">Winer, 1984</xref>; <xref ref-type="bibr" rid="bib2">Anderson et al., 2007</xref>; <xref ref-type="bibr" rid="bib10">Bartlett et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Bordi and LeDoux, 1994</xref>; <xref ref-type="bibr" rid="bib15">Calford, 1983</xref>; <xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>). The present results imply that, when decoding speech, higher order cortical areas modify representations of the sensory input in the primary sensory thalamus and that such modification is relevant for speech recognition abilities. These results are a further indication that speech recognition might only be fully understood if dynamic cortico-thalamic interactions are taken into account (<xref ref-type="bibr" rid="bib66">Klostermann et al., 2013</xref>; <xref ref-type="bibr" rid="bib112">von Kriegstein et al., 2008a</xref>). The results are based on the test of two equally weighted hypotheses: a main effect and a correlation. Although we found no significant main effect in the vMGB (nor in any other subregion of the MGB), the correlation between the task-dependent modulation and behavioral performance was, as hypothesized, significant within the left vMGB.</p><p>We localized the vMGB based on its tonotopic organization and location relative to other MGB divisions. The tonotopic organization of the vMGB has been observed in many species with the use of invasive techniques (<xref ref-type="bibr" rid="bib119">Winer, 1984</xref>; <xref ref-type="bibr" rid="bib2">Anderson et al., 2007</xref>; <xref ref-type="bibr" rid="bib10">Bartlett et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Bordi and LeDoux, 1994</xref>; <xref ref-type="bibr" rid="bib15">Calford, 1983</xref>) and non-invasively in six human participants using ultra-high field fMRI (<xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>). Similar to <xref ref-type="bibr" rid="bib79">Moerel et al. (2015)</xref> we here also identified two tonotopic gradients. <xref ref-type="bibr" rid="bib79">Moerel et al. (2015)</xref> attributed the ventral gradient to the ventral MGB, and the other gradient cautiously to the tonotopically organized lateral posterior thalamic nucleus (Pol), which is part of the non-lemniscal system (<xref ref-type="bibr" rid="bib62">Jones, 1985</xref>). The Pol is also tonotopically organized with sharp tuning curves similar to the vMGB (<xref ref-type="bibr" rid="bib61">Imig and Morel, 1985</xref>). Gradient two in our study is, however, larger than gradient 1. Thus, gradient two might also represent a composite of several nuclei that are in close proximity to the MGB (<xref ref-type="bibr" rid="bib11">Bartlett and Wang, 2011</xref>) such as the Pol and potentially the suprageniculate, which has a preference for high frequencies (<xref ref-type="bibr" rid="bib14">Bordi and LeDoux, 1994</xref>) (for a detailed thalamic atlas see <xref ref-type="bibr" rid="bib80">Morel et al., 1997</xref>). Furthermore, the weak tonotopy of the dMGB or mMGB might also contribute to gradient 2. Another interpretation of the two tonotopic gradients is that the vMGB in humans might include two tonotopic maps, that is that frequency gradient 1 and 2 are part of the vMGB. Two tonotopic gradients have been found in the rat vMGB (<xref ref-type="bibr" rid="bib103">Shiramatsu et al., 2016</xref>), but not consistently in other species (<xref ref-type="bibr" rid="bib51">Hackett et al., 2011</xref>; <xref ref-type="bibr" rid="bib59">Horie et al., 2013</xref>; <xref ref-type="bibr" rid="bib110">Tsukano et al., 2017</xref>). The volume of the two gradients, however, speaks against the possibility of two tonotopic maps in human vMGB. That is, the two gradients make up already ca. 100 mm³ and reported whole MGB volumes based on characterization in post-mortem human brains are between ca. 40–120 mm³ (<xref ref-type="bibr" rid="bib91">Rademacher et al., 2002</xref>; <xref ref-type="bibr" rid="bib82">Moro et al., 2015</xref>). Thus, while gradient one can be clearly attributed to the vMGB, due to its tonotopic gradient and ventral location, the nature of the second frequency gradient remains an open question.</p><p>Based on previous findings (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>), we expected significant responses for the speech vs speaker task contrast in vMGB. The lack of a significant main effect of task (speech vs speaker) in the vMGB was surprising, and the meta-analysis showed that the null-finding was indeed different from categorical task-effects (speech vs loudness tasks and speech vs speaker tasks) observed in other experiments in participants with typical development (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>; <xref ref-type="bibr" rid="bib78">Mihai et al., 2019</xref>).</p><p>There are three potential explanations for the difference in the results for the main effect of task between the present and previous studies. First, the speaker task was more difficult to perform (indicated by the lower behavioral score during the speaker vs speech task and subjective reports of the participants), which may have led to higher BOLD responses for the more difficult task. However, this explanation is unlikely as previous studies with matched performance across tasks (<xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>; <xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref> experiment 2) and also studies where the control task was more difficult than the speech task (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref> experiment 1) have found a task-dependent MGB modulation for the speech task.</p><p>Second, we employed a liberal threshold in choosing participants based on their reading speed and comprehension scores (lower fourth of the mean and higher, i.e., 26–100%). Participants who scored lower on this test might also show a lower task-dependent modulation of the MGB (see <xref ref-type="bibr" rid="bib29">Díaz et al., 2018</xref>). However, we find this explanation unlikely as those participants with lower reading score showed a broad (low to high) BOLD-response spectrum (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>), and in the previous study (<xref ref-type="bibr" rid="bib29">Díaz et al., 2018</xref>) a correlation between the MGB task-dependent modulation and reading speed and comprehension scores has been found only in participants with developmental dyslexia, but not in typically developed controls.</p><p>A third explanation is that we used unmanipulated natural voices from different speakers. In the previous studies different speaker voices were synthesized from one original voice to differ only in two key voice-identity parameters, that is the acoustic effect of the vocal tract length and the fundamental frequency (f0) (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>; <xref ref-type="bibr" rid="bib42">Gaudrain et al., 2009</xref>). Vocal tract length and f0 are relatively stable acoustic cues that do not vary greatly over time in contrast to the highly dynamic cues (e.g., formant transitions, voice onset times, stops, <xref ref-type="bibr" rid="bib64">Kent et al., 1992</xref>) that are most important for signaling phonemes and are used for speech recognition. However, dynamic cues, such as pitch periodicity, segmental timings, and prosody can also be used for speaker identification (<xref ref-type="bibr" rid="bib13">Benesty et al., 2007</xref>). In the present experiment, which included natural voices, participants might have also used fast changing cues for speaker identity recognition, particularly because the task was difficult. Since dynamic cues are essential for speech recognition, using dynamic cues in a speaker task would render the two tasks less different. Thus, MGB modulation might also have played a role in performing the speaker task.</p><p>The localization of the correlation between the speech vs speaker contrast and performance in the speech task to the vMGB confirmed our hypothesis that it is the first-order thalamic nucleus (vMGB) that is involved in speech recognition. The correlation indicated that those participants on the lower side of the task-dependent modulation spectrum, as given by the speech vs speaker contrast, have lower proportion of hits in the speech task and those participants on the higher side of the task-dependent modulation spectrum, show higher proportion of hits in the speech task and are thus better at speech recognition. The variability of proportion of hits across participants was between 85.2 and 93.9% which translates to an odds ratio of 2.66 (i.e., those participants with higher task-dependent modulation are ~2.5 times more likely to have more hits compared to misses in the speech task). This variability is meaningful for a speech recognition task; for example, it has been shown that the hearing impaired perform 5–10% lower on speech recognition compared to normal hearing participants (<xref ref-type="bibr" rid="bib88">Panouillères and Möttönen, 2018</xref>; <xref ref-type="bibr" rid="bib46">Gordon-Salant and Fitzgibbons, 1993</xref>).</p><p>In the present study, the results cannot be explained by differences in stimulus input in the two conditions, as the same stimuli were heard in both tasks. We, however, cannot exclude with the present data set that, in general, participants who are better task-performers have a higher task-dependent modulation of the vMGB for speech. It is therefore still an open question whether the task-dependent modulation is specific to speech recognition behavior. A previous study in the visual modality, gave first evidence that the task-dependent modulation of the visual thalamus (LGN) for visual speech is specific for predictable dynamic information such as visual speech in contrast to unpredictable dynamic information (<xref ref-type="bibr" rid="bib29">Díaz et al., 2018</xref>). However a similar study in the auditory modality is so far missing.</p><p>What kind of mechanism could be represented by the correlation between task-dependent modulation of the vMGB and speech recognition performance? Experimental and theoretical accounts of brain function emphasize the importance of an anatomical cortical and subcortical hierarchy that is organized according to the timescale of complex stimuli in the natural environment (<xref ref-type="bibr" rid="bib24">Davis and Johnsrude, 2007</xref>; <xref ref-type="bibr" rid="bib44">Giraud et al., 2000</xref>; <xref ref-type="bibr" rid="bib65">Kiebel et al., 2008</xref>; <xref ref-type="bibr" rid="bib114">Wang et al., 2008</xref>). In brief, it is assumed that levels closer to the sensory input encode faster dynamics of the stimulus than levels further away from the sensory input. In accordance with this view, the MGB (as well as the visual ﬁrst-order thalamus [LGN]; <xref ref-type="bibr" rid="bib56">Hicks et al., 1983</xref>) is tuned to high frequencies of temporal modulation (ca. 16 Hz in human MGB; <xref ref-type="bibr" rid="bib44">Giraud et al., 2000</xref>) in relation to their associated primary sensory cortical areas (<xref ref-type="bibr" rid="bib44">Giraud et al., 2000</xref>; <xref ref-type="bibr" rid="bib114">Wang et al., 2008</xref>; <xref ref-type="bibr" rid="bib37">Foster et al., 1985</xref>). For humans, the optimized encoding of relatively fast dynamics; for example at the phoneme level, is critical for speech recognition and communication (<xref ref-type="bibr" rid="bib100">Shannon et al., 1995</xref>; <xref ref-type="bibr" rid="bib107">Tallal et al., 1996</xref>; <xref ref-type="bibr" rid="bib108">Tallal and Piercy, 1975</xref>). Many important speech components like formant transitions, voice onset times, or stops are on very fast time scales of 100 ms or less (<xref ref-type="bibr" rid="bib54">Hayward, 2000</xref>). Additionally, the sound envelope described by relatively fast temporal modulations (1–10 Hz in quiet environments, 10–50 Hz in noisy environments) is important for speech recognition (<xref ref-type="bibr" rid="bib32">Elliott and Theunissen, 2009</xref>; ; <xref ref-type="bibr" rid="bib100">Shannon et al., 1995</xref>). The Bayesian brain hypothesis proposes that the brain uses internal dynamic models of its environment to predict the trajectory of the sensory input (<xref ref-type="bibr" rid="bib67">Knill and Pouget, 2004</xref>; <xref ref-type="bibr" rid="bib40">Friston and Kiebel, 2009</xref>; <xref ref-type="bibr" rid="bib39">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib65">Kiebel et al., 2008</xref>). In accordance with this hypothesis, we have previously suggested that slower dynamics encoded by auditory cortical areas (<xref ref-type="bibr" rid="bib44">Giraud et al., 2000</xref>; <xref ref-type="bibr" rid="bib114">Wang et al., 2008</xref>) provide predictions about input arriving at lower levels of the temporal-anatomic hierarchy (<xref ref-type="bibr" rid="bib65">Kiebel et al., 2008</xref>; <xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>). In this view, these dynamic predictions modulate the response properties of the ﬁrst-order sensory thalamus to optimize the early stages of speech recognition. For example, in non-human animals cortico-thalamic projections outnumber thalamo-cortical projections (reviewed in <xref ref-type="bibr" rid="bib86">Ojima and Rouiller, 2011</xref>), and alter the response properties of thalamic neurons (<xref ref-type="bibr" rid="bib6">Andolina et al., 2007</xref>; <xref ref-type="bibr" rid="bib22">Cudeiro and Sillito, 2006</xref>; <xref ref-type="bibr" rid="bib33">Ergenzinger et al., 1998</xref>; <xref ref-type="bibr" rid="bib43">Ghazanfar and Nicolelis, 2001</xref>; <xref ref-type="bibr" rid="bib105">Sillito et al., 2006</xref> [review]; <xref ref-type="bibr" rid="bib115">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib104">Sillito et al., 1994</xref>; <xref ref-type="bibr" rid="bib68">Krupa et al., 1999</xref>). In speech processing such a mechanism might be especially useful as the signal includes both rapid dynamics, and is predictable (e.g., due to co-articulation or learned statistical regularities in words <xref ref-type="bibr" rid="bib96">Saffran, 2003</xref>). We suggest that higher-level regularities yield the predictions for lower-level details of the stimulus. While, for example, words are represented at the level of the cerebral cortex (<xref ref-type="bibr" rid="bib60">Huth et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Davis and Johnsrude, 2003</xref>; <xref ref-type="bibr" rid="bib90">Price et al., 2005</xref>), the predictions about the most likely components of the word are percolating down the hierarchy. In this view, sensory thalamus structures do not represent word level information but they are tuned by the cerebral cortex areas to expect the detailed trajectory of the speech signal that is most likely given the expectations generated at the cerebral cortex level. Speech needs to be computed online often under suboptimal listening conditions. Building up accurate predictions within an internal generative model about fast sensory dynamics would result in more efficient processing when the perceptual system is confronted with taxing conditions such as fast stimulus presentation rates or background noise. We speculate that the correlation between speech task performance and task-dependent vMGB modulation might be a result of feedback from cerebral cortex areas. Feedback may emanate directly from auditory primary or association cortices, or indirectly via other structures such as the reticular nucleus with its inhibitory connections to the MGB (<xref ref-type="bibr" rid="bib94">Rouiller and de Ribaupierre, 1985</xref>). Feedback cortico-thalamic projections from layer six in A1 to the vMGB, but also from association cortices such as the motion sensitive part of the planum temporale (<xref ref-type="bibr" rid="bib109">Tschentscher et al., 2019</xref>), may modulate information ascending through the lemniscal pathway, rather than convey information to the ventral division (<xref ref-type="bibr" rid="bib69">Lee, 2013</xref>; <xref ref-type="bibr" rid="bib73">Llano and Sherman, 2008</xref>).</p><p>Although most of speech and language research focuses on cerebral cortex structures, investigating subcortical sensory contributions to speech perception is paramount to the development of a mechanistic understanding of how the human brain accomplishes speech recognition. The present study brings us a decisive step further in this direction by suggesting that it is the <italic>primary</italic> sensory auditory thalamus that shows a behaviorally relevant task-dependent modulation for speech recognition.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>The Ethics committee of the Medical Faculty, University of Leipzig, Germany approved the study (protocol number 273/14-ff). We recruited 33 participants using the database of the Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany. The participants were right handed (as assessed by the Edinburgh Handedness Inventory; <xref ref-type="bibr" rid="bib87">Oldfield, 1971</xref>), native German-speakers, had a mean age and standard deviation (SD) of 24.9 ± 2.5 years, and included 23 females. Participants provided written informed consent. None of the participants reported a history of psychiatric or neurological disorders, hearing difficulties, or current use of psychoactive medications. Normal hearing abilities were confirmed with pure tone audiometry (250 Hz to 8000 Hz) with a threshold equal to and below 25 dB (Madsen Micromate 304, GN Otometrics, Denmark). To exclude possible undiagnosed dyslexics, we tested the participant’s reading speed and reading comprehension using the German LGVT: 6–12 test (<xref ref-type="bibr" rid="bib98">Schneider et al., 2007</xref>). The cut-off for both reading scores were set to those levels mentioned in the test instructions as the ‘lower average and above’ performance range (i.e., 26–100% of the calculated population distribution). None of the participants performed below the cut-off performance (mean and standard deviation: 69.9 ± 19.5%, lowest mean score: 36%). Furthermore, none of the participants exhibited a clinically relevant number of traits associated with autism spectrum disorder as assessed by the Autism Spectrum Quotient (AQ; mean and standard deviation: 16.2 ± 4.8; cutoff: 32–50; <xref ref-type="bibr" rid="bib9">Baron-Cohen et al., 2001</xref>). We tested AQ as autism can be associated with difficulties in speech-in-noise perception (<xref ref-type="bibr" rid="bib1">Alcántara et al., 2004</xref>; <xref ref-type="bibr" rid="bib49">Groen et al., 2009</xref>) and has overlapping symptoms with dyslexia (<xref ref-type="bibr" rid="bib118">White et al., 2006</xref>). Participants received monetary compensation for participating in the study.</p></sec><sec id="s4-2"><title>Experiments</title><p>We performed three different functional MRI measurements: the speech experiment (n = 33), a MGB localizer (n = 33), and a tonotopy localizer (n = 28, 18 females, age 24.8 ± 5.0 years). Each experiment was performed once.</p></sec><sec id="s4-3"><title>Stimuli</title><sec id="s4-3-1"><title>MGB and Tonotopy localizer</title><p>The stimuli for the MGB localizer and the tonotopy localizer consisted of 84 and 56 natural sounds, respectively, sampled at 16 kHz at 32 bit, and included samples of human speech, animal cries and tool sounds (these were the same as described in <xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>). The stimuli had a duration of 1000 ms, were ramped with 10 ms linear slopes, and had equalized root-mean-square levels.</p></sec><sec id="s4-3-2"><title>Speech experiment</title><p>The speech experiment stimuli consisted of 448 vowel-consonant-vowel (VCV) syllables with an average duration and SD of 803 ± 105 ms. These were spoken by three female and three male speakers (mean age and SD 27.7 ± 3.3 years) unfamiliar to the participants, and were recorded with a video camera (Canon Legria HFS10, Canon, Japan) and a Røde NTG-1 Microphone (Røde Microphones, Silverwater, NSW, Australia) connected to a pre-amplifier (TubeMP Project Series, Applied Research and Technology, Rochester, NY, USA) in a sound-attenuated room. The sampling rate was 48 kHz at 16 bit. Auditory stimuli were cut and flanked by Hamming windows of 15 ms at the beginning and end, converted to mono, and root-mean-square equalized using Python 3.6 (Python Software Foundation, <ext-link ext-link-type="uri" xlink:href="http://www.python.org">www.python.org</ext-link>).</p></sec></sec><sec id="s4-4"><title>Procedure</title><sec id="s4-4-1"><title>MGB and Tonotopy localizer</title><p>For the MGB localizer and the tonotopy localizer, participants listened to natural sounds (human voices, animal cries, tool sounds; <xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>). The MGB localizer consisted of one run where 84 natural sound stimuli were presented in random order and had a duration of 12:50 min. The tonotopy localizer consisted of six runs where 56 of the 84 natural sound stimuli from the MGB localizer were presented. The sounds were randomly chosen before the first run and the same 56 sounds were played in each run. Each run had a duration of 8:58 min. To ensure listener engagement, in both localizers the participants performed a 1-back task and pushed a button when two consecutive sounds were the same. This happened on average 5% of the time. Additionally, 5% of the trials contained no sound (null events). Within each run, sounds were randomly jittered at an interval of 2, 3, or 4 repetition times (TR) and presented in the middle of the silent gap of 1200 ms (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The MGB localizer was used as an independent functional identifier for the left and right MGB. The resulting masks were then used to constrain the analyses of the tonotopy localizer to these regions of interest. In turn, the tonotopic regions of the MGB were used as masks in the speech experiment (see section Functional MRI Data Analysis).</p></sec><sec id="s4-4-2"><title>Speech experiment</title><p>In the speech experiment (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) participants listened to blocks of auditory VCV syllables, and were asked to perform two types of tasks: a speech task and a speaker task. In the speech task, participants reported via button press whether the current syllable was different from the previous one (1-back task). In the speaker task, participants reported via button press whether the current speaker was different from the previous one. Speakers within a block were either all male or all female. This was necessary to avoid that participants performed a gender discrimination task on some trials and a speaker identity task on other trials. Task instructions were presented for two seconds prior to each block and consisted of white written words on a black background (German words ‘Silbe’ for syllable, and ‘Person’ for person). After the instruction, the block of syllables started (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Each block contained 14 stimuli. Each stimulus presentation was followed by 400 ms of silence. Within one block both syllables and speakers changed six or seven times. The average length of a block and SD was 17.0 ± 0.9 s. Counterbalancing of the stimulus material for the two tasks was achieved by presenting each block twice: once with the instruction to perform the speech task and once with the instruction to perform the speaker task. Besides the factor ‘task’, the experiment included another factor. That is, blocks had either only vowel or only consonant changes. While this factor is included in the analysis, it is irrelevant for addressing the current research question.</p><p>The experiment was divided into five runs with a duration of 8:30 min per run. Each of the four condition blocks (speech vowel change, speaker vowel change, speech consonant change, speaker consonant change) were presented six times in pseudo-randomized order. The last stimulus presentation in the run was followed by 30 s of no stimulation. Participants were allowed to rest for one minute between runs. To familiarize participants with speakers’ voices and to ensure they understood the task, they performed two initial training runs outside the MRI-scanner: one run for speaker familiarization, and one for experiment familiarization (detailed below).</p><p>The experiments were programmed and presented using Presentation (v17.1, NeuroBehavioral Systems, Berkley, CA, USA) in Windows XP and delivered through an MrConfon amplifier and earbuds linked to the transducers via air tubes (manufactured 2008, MrConfon GmbH, Magdeburg, Germany).</p></sec></sec><sec id="s4-5"><title>Participant training</title><p>The participant training consisted of a speaker familiarization and an experiment familiarization.</p><p>The speaker familiarization consisted of a presentation of the speakers and a test phase. In the presentation phase, the speakers were presented in six blocks, each containing nine pseudo-randomly chosen stimuli. Participants heard a pseudo-random choice of the same stimuli used in the experiment. Each block contained one speaker-identity only. Participants were alerted to the onset of a new speaker-identity block with the phrase ‘Andere/r Sprecher/in’ (German for ‘Another Speaker’). Participants listened to the voices with the instruction to memorize the speaker’s voice. In the following test phase participants were presented with four blocks of nine trials that each contained syllable pairs spoken by the same or different speaker from the ones they were trained on. Participants were asked to indicate whether the speakers of the two syllables were the same by pressing keypad buttons ‘1’ for yes and ‘2’ for no. Participants received visual feedback for correct (green flashing German word for correct: ‘Richtig’) and incorrect (red flashing German word for incorrect: ‘Falsch’) answers. If participants scored below 85%, they repeated the speaker familiarization training.</p><p>The experiment familiarization consisted of one 8:30 min long run of the fMRI speech experiment (<xref ref-type="fig" rid="fig1">Figure 1B/C</xref>). Stimuli were randomly chosen from the same stimulus material used in the experiment. If participants scored below 85% across tasks in the experiment familiarization run they repeated the experiment familiarization training.</p><p>The training (speaker and experiment familiarization) took place within the same testing-session as the pre-tests (audiometry, reading comprehension, and AQ questionnaire), and was repeated half an hour prior to the fMRI experiment, which took place at a later date.</p></sec><sec id="s4-6"><title>Data acquisition and processing</title><p>MRI data were acquired using a Siemens Magnetom 7 T scanner (Siemens Healthineers, Erlangen, Germany) with a Nova 32-channel head coil (Nova Medical, Wilmington MA, USA). Functional MRI data were acquired using echo planar imaging (EPI) sequences. We used a field of view (FoV) of 132 × 132 mm and partial coverage with 28 slices. This volume was oriented obliquely such that the slices encompassed the inferior colliculi (IC), the MGB and the superior temporal gyrus, running in parallel to the latter (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><p>The MGB and tonotopy localizers had the following acquisition parameters: TR = 2800 ms (acquisition time TA = 1600 ms, silent gap: 1200 ms), TE = 22 ms, flip angle 65°, GRAPPA (<xref ref-type="bibr" rid="bib48">Griswold et al., 2002</xref>) with acceleration factor 2, 33% phase oversampling, matrix size 120 × 120, FoV 132 mm x 132 mm, phase partial Fourier 6/8, voxel size 1.1 mm x 1.1 mm x 1.1 mm, interleaved acquisition, anterior to posterior phase-encode direction. We employed a clustered EPI technique allowing for stimulus presentations in quiet in a fast-event related design. Stimuli were presented during the silent gap. For the MGB localizer, we acquired one run of 275 volumes (13:06 min). For the tonotopy, localizer we acquired 187 volumes (9:01 min) per run with a total of six runs.</p><p>For the speech experiment, acquisition parameters were the same as for the localizers, with the exception of a shorter TR (1600 ms) due to continuous scanning (i.e., no silent gap), 320 volumes per run, and total length of acquisition per run of 8:30 min. Five runs were recorded for each participant. The acquisition parameters were similar to the protocol described by <xref ref-type="bibr" rid="bib79">Moerel et al. (2015)</xref>, with the exception of a longer echo time and phase oversampling which eschewed front-back wrapping artifacts. An example EPI is shown in <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>. The difference in echo time between our sequence and the one in <xref ref-type="bibr" rid="bib79">Moerel et al. (2015)</xref> may have resulted in a lower signal-to-noise ratio in subcortical structures. However, as the MGB has a T2* value of ~33 ms the different echo times of 19 ms (our sequence) and 22 ms (<xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>) had little to no effect on the applied general linear model (<xref ref-type="bibr" rid="bib25">de Hollander et al., 2017</xref>).</p><p>During functional MRI data acquisition we also acquired physiological values (heart rate, and respiration rate) using a BIOPAC MP150 system (BIOPAC Systems Inc, Goleta, CA, USA). Structural images were recorded using an MP2RAGE (<xref ref-type="bibr" rid="bib75">Marques et al., 2010</xref>) T1 protocol: 700 µm isotropic resolution, TE = 2.45 ms, TR = 5000 ms, TI1 = 900 ms, TI2 = 2750 ms, flip angle 1 = 5°, flip angle 2 = 3°, FoV 224 mm ×224 mm, GRAPPA acceleration factor 2, duration 10:57 min.</p></sec><sec id="s4-7"><title>Behavioral data analysis</title><p>Button presses were modeled using a binomial logistic regression which predicts the probability of correct button presses based on four independent variables (speech task, vowel change; speech task, consonant change; speaker task, vowel change; speaker, task consonant change) in a Bayesian framework (<xref ref-type="bibr" rid="bib77">Mcelreath, 2015</xref>).</p><p>To pool over participants and runs we modeled the correlation between intercepts and slopes. For the model implementation and data analysis, we used PyMC3 (<xref ref-type="bibr" rid="bib97">Salvatier et al., 2016</xref>) using a No-U-Turn Sampler (<xref ref-type="bibr" rid="bib58">Hoffman and Gelman, 2011</xref>) with three parallel chains. Per chain we had 20,000 samples with 5000 of these as warm-up. Only the latter 7500 were used for posterior mean and highest posterior density (HPD) interval estimates. The difference in percent correct button presses between the speech and speaker task was calculated using the posterior densities averaged over consonant and vowel changes. The resulting distribution was averaged and the 97% HPD was calculated. If the posterior probability distribution does not strongly overlap zero, then there was a detectable difference between conditions (<xref ref-type="bibr" rid="bib77">Mcelreath, 2015</xref>).</p><p>The predictors included in the behavioral data model were: task (1 = speech, 0 = speaker), and syllable change (1 = vowel, 0 = consonant). We also included the two way interaction of task and syllable change. Because data were collected across participants and runs it is reasonable to include random effects for both of these in the logistic model, albeit we expected little to no difference in performance between runs. The model had the following likelihood and linear models:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ2"><mml:math id="m2"><mml:mtext>logit</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext>, for </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo> <mml:mi/><mml:mi>I</mml:mi><mml:mo>;</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mo>…</mml:mo><mml:mo>,</mml:mo> <mml:mi/><mml:mi>J</mml:mi></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>participant</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>run</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ4"><mml:math id="m4"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mtext>participant</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mtext>run</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ5"><mml:math id="m5"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mtext>run</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ6"><mml:math id="m6"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mtext>participant</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mtext>run</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ7"><mml:math id="m7"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mtext>participant</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mtext>participant</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mtext>participant</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>∼</mml:mo><mml:mtext>MVNormal</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>α</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ8"><mml:math id="m8"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>∼</mml:mo><mml:mtext>MVNormal</mml:mtext><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mfenced separators="|"><mml:mrow><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>α</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ9"><mml:math id="m9"><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ10"><mml:math id="m10"><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">u</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">u</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ11"><mml:math id="m11"><mml:mi>α</mml:mi><mml:mo>∼</mml:mo><mml:mtext>Normal</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mn>0,5</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ12"><mml:math id="m12"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>Normal</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mn>0,5</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ13"><mml:math id="m13"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>Normal</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mn>0,5</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ14"><mml:math id="m14"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>Normal</mml:mtext><mml:mo>(</mml:mo><mml:mn>0,5</mml:mn><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula id="equ15"><mml:math id="m15"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mtext>participant</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>∼</mml:mo><mml:mtext>HalfCauchy</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ16"><mml:math id="m16"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>HalfCauchy</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ17"><mml:math id="m17"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>HalfCauchy</mml:mtext><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula id="equ18"><mml:math id="m18"><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>L</mml:mi><mml:mi>K</mml:mi><mml:mi>J</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ19"><mml:math id="m19"><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">u</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>L</mml:mi><mml:mi>K</mml:mi><mml:mi>J</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula>where <italic>I</italic> is the number of subjects and <italic>J</italic> is the number of runs. The model is compartmentalized into sub-models for the intercept and each slope. <italic>A<sub>i,j</sub></italic> is the sub-model for the intercept for observations <italic>i,j</italic>. Similarly, B<sub>S,i,j</sub>, B<sub>V,i,j</sub> and B<sub>SV,i,j</sub> are the sub-models for the speech-speaker slope, vowel-consonant slope and the interaction slope; <inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the covariance matrices, <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the priors for the correlation matrices between the intercepts and slopes modeled as a LKJ probability density for participants and runs (<xref ref-type="bibr" rid="bib72">Lewandowski et al., 2009</xref>). Informative priors for the intercept (<italic>α</italic>) and additional coefficients (e.g., <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), random effects for subject and run (<inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>,<inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), and multivariate priors for subjects and runs identify the model by constraining the position of <italic>p<sub>i,j</sub></italic> to reasonable values.</p><p>To check for correlations in the performance between tasks, we calculated the Pearson’s product moment (<xref ref-type="bibr" rid="bib20">Cohen, 1988</xref> p. 75) on the proportion of hits in the speech and speaker task using Python 3.6.</p><p>We analyzed reaction times using a linear model in a Bayesian framework. Reaction times were mean centered and the priors on reaction times were modeled as T-distributions. We calculated the mean difference in reaction times between the speech and the speaker task to check for a speed-accuracy trade-off. The model is described below:<disp-formula id="equ20"><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ21"><mml:math id="m21"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext>, for </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo> <mml:mi/><mml:mi>I</mml:mi><mml:mo>;</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mo>…</mml:mo><mml:mo>,</mml:mo> <mml:mi/><mml:mi>J</mml:mi></mml:math></disp-formula><disp-formula id="equ22"><mml:math id="m22"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>participant</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>run</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ23"><mml:math id="m23"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mtext>participant</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mtext>run</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ24"><mml:math id="m24"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mtext>run</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ25"><mml:math id="m25"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mtext>participant</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mtext>run</mml:mtext><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ26"><mml:math id="m26"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mtext>participant</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mtext>participant</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mtext>participant</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>∼</mml:mo><mml:mtext>MVNormal</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>α</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ27"><mml:math id="m27"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>∼</mml:mo><mml:mtext>MVNormal</mml:mtext><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mfenced separators="|"><mml:mrow><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>α</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ28"><mml:math id="m28"><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ29"><mml:math id="m29"><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">u</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">u</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ30"><mml:math id="m30"><mml:mi>α</mml:mi><mml:mo>∼</mml:mo><mml:mi>T</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>3,0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ31"><mml:math id="m31"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>T</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>3,0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ32"><mml:math id="m32"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>T</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>3,0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ33"><mml:math id="m33"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>T</mml:mi><mml:mo>(</mml:mo><mml:mn>3,0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula id="equ34"><mml:math id="m34"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mtext>participant</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mtext>run</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>∼</mml:mo><mml:mtext>HalfCauchy</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ35"><mml:math id="m35"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>HalfCauchy</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ36"><mml:math id="m36"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>HalfCauchy</mml:mtext><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula id="equ37"><mml:math id="m37"><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>L</mml:mi><mml:mi>K</mml:mi><mml:mi>J</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ38"><mml:math id="m38"><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">u</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>L</mml:mi><mml:mi>K</mml:mi><mml:mi>J</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula>where <italic>I</italic> is the number of subjects and <italic>J</italic> is the number of runs. The model is compartmentalized into sub-models for the intercept and each slope. <italic>A<sub>i,j</sub></italic> is the sub-model for the intercept for observations <italic>i,j</italic>. Similarly, B<sub>S,i,j</sub>, B<sub>V,i,j</sub> and B<sub>SV,i,j</sub> are the sub-models for the speech-speaker slope, vowel-consonant slope and the interaction slope; <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the covariance matrices, <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the priors for the correlation matrices between the intercepts and slopes modeled as a LKJ probability density for participants and runs (<xref ref-type="bibr" rid="bib72">Lewandowski et al., 2009</xref>). Informative priors for the intercept (<italic>α</italic>) and additional coefficients (e.g., <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), random effects for subject and run (<inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>,<inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), and multivariate priors for subjects and runs identify the model by constraining the position of <italic>p<sub>i,j</sub></italic> to reasonable values.</p></sec><sec id="s4-8"><title>Functional MRI data analysis</title><sec id="s4-8-1"><title>Preprocessing of MRI data</title><p>The partial coverage by the 28 slices and the lack of a whole brain EPI measurement resulted in coregistration difficulties of functional and structural data. As a workaround, the origin (participant space coordinate [0, 0, 0]) of all EPI and MP2RAGE images were manually set to the anterior commissure using SPM 12. Furthermore, to deal with the noise surrounding the head in MP2RAGE images, these were first segmented using SPM’s new segment function (SPM 12, version 12.6906, Wellcome Trust Centre for Human Neuroimaging, UCL, UK, <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>) running on Matlab 8.6 (The Mathworks Inc, Natick, MA, USA). The resulting gray and white matter segmentations were summed and binarized to remove voxels that contain air, scalp, skull and cerebrospinal fluid from structural images using the ImCalc function of SPM.</p><p>A template of all participants was created with ANTs (<xref ref-type="bibr" rid="bib8">Avants et al., 2009</xref>) using the participants’ MP2RAGE images, which was then registered to the MNI space using the same software package and the MNI152 (0.5 mm)³ voxel size template provided by FSL 5.0.8 (<xref ref-type="bibr" rid="bib106">Smith et al., 2004</xref>). All MP2RAGE images were preprocessed with Freesurfer (<xref ref-type="bibr" rid="bib34">Fischl et al., 2002</xref>; <xref ref-type="bibr" rid="bib35">Fischl et al., 2004</xref>; <xref ref-type="bibr" rid="bib52">Han and Fischl, 2007</xref>) using the recon-all command to obtain boundaries between gray and white matter, which were later used in the functional to structural registration step.</p><p>The rest of the analysis was coded in nipype (<xref ref-type="bibr" rid="bib47">Gorgolewski et al., 2011</xref>). A graphical overview of the nipype pipeline can be found in <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>. Head motion and susceptibility distortion by movement interaction of functional runs were corrected using the Realign and Unwarp method (<xref ref-type="bibr" rid="bib5">Andersson et al., 2001</xref>) in SPM 12 after which outlier volumes were detected using ArtifactDetect (composite threshold of translation and rotation: 1; intensity Z-threshold: 3; global threshold: 8; <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/artifact_detect/">https://www.nitrc.org/projects/artifact_detect/</ext-link>). Coregistration matrices for realigned functional runs per participant were computed based on each participant’s structural image using Freesurfer’s BBregister function (register mean EPI image to T1, option ‘—init-header’ was specified in order to preserve the origin of the manual alignment of structural and functional data). Warping using coregistration matrices (after conversion to ITK coordinate system) and resampling to 1 mm isovoxel was performed using ANTs. Before model creation we smoothed the data in SPM12 using a 1 mm kernel at full-width half-maximum.</p></sec></sec><sec id="s4-9"><title>Physiological data</title><p>Physiological data (heart rate and respiration rate) were processed by the PhysIO Toolbox (<xref ref-type="bibr" rid="bib63">Kasper et al., 2017</xref>) to obtain Fourier expansions of each, in order to enter these into the design matrix (see statistical analyses sections below).</p></sec><sec id="s4-10"><title>Statistical analysis of the speech experiment</title><p>Models were set up in SPM using the native space data for each participant. The design matrix included three cardiac and four respiratory regressors, six realignment parameters, and a variable number of outlier regressors from the ArtifactDetect step, depending on how many outliers were found in each run. These regressors of no interest were also used in the models of the other two experiments (MGB and tonotopy localizer). Since participants provided a response only for the target stimulus changes and not for each stimulus presentation, we modeled these to eschew a potential sensory-motor confound as 0.5 for hit, ‒0.5 for miss and 0.0 for everything else. If more than one syllable presentation took place within one volume acquisition, the values within this volume were averaged. The speech experiment had a total of five modeled conditions, which were convolved with the hemodynamic response function (HRF): speech task/vowel change, speech task/consonant change, speaker task/vowel change, speaker task/consonant change, and task instruction. Parameter estimates were computed for the contrast speech vs speaker at the first level using restricted maximum likelihood (REML) as implemented in SPM 12.</p><p>After estimation, the contrasts were registered to the MNI structural template of all participants using a two-step registration in ANTs (see also <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>). First, a quick registration was performed on the whole head using rigid, affine and diffeomorphic (using Symmetric Normalization: SyN) transformations and the mutual information similarity metric. Second, the high quality registration was confined to a rectangular prism mask encompassing the left and right MGB, and IC only. This step used affine and SyN transformations and mean squares and neighborhood cross correlation similarity measures, respectively. We performed the registration to MNI space for all experiments by linearly interpolating the contrast images using the composite transforms from the high quality registration.</p><p>We used a random effects (RFX) analysis to compute the speech vs speaker contrast across participants to test our first hypothesis that the MGB response is modulated by this contrast. To do this, we took the first level contrasts across participants and entered them into an RFX model to be estimated using REML. Based on the results of previous experiments (<xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>; <xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>), we expected a result for the categorical speech vs speaker contrast in the left and right MGB. Our second hypothesis was that the proportion of correct button presses in the speech task correlates with the responses elicited by the speech vs speaker contrast over participants in the left MGB only (<xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>). We thus computed the RFX correlation between the speech vs speaker contrast and the proportion of correct button presses in the speech task across participants. This was implemented using the behavioral percent correct scores for the speech task as a covariate of interest for each participant in the SPM RFX model. We used an equivalent procedure to test for correlation between the speech vs speaker task contrast and the proportion of correct button presses in the speaker task across participants. To formally test for a difference between these two correlations we performed a comparison using a freely available online tool (comparingcorrelations.org) (<xref ref-type="bibr" rid="bib30">Diedenhofen and Musch, 2015</xref>). This tool computes the z-score and p-value of the difference between two correlations based on <xref ref-type="bibr" rid="bib57">Hittner et al. (2003)</xref>.</p></sec><sec id="s4-11"><title>Meta-analysis of the main effect of task (speech vs speaker task contrast)</title><p>The lack of statistical significance for the speech vs speaker contrast raised the question whether the overall effect is different from the ones reported previously (<xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>; <xref ref-type="bibr" rid="bib113">von Kriegstein et al., 2008b</xref>). We performed a random effects meta-analysis to test whether the lack of task-dependent modulation in the present study was different from other studies that have reported a task-dependent modulation of the MGB. We included five studies in the meta-analysis that included a speech task vs control task contrast: two experiments from <xref ref-type="bibr" rid="bib113">von Kriegstein et al. (2008b)</xref>, the data from the control participants of <xref ref-type="bibr" rid="bib28">Díaz et al. (2012)</xref>, the result of a recent study (<xref ref-type="bibr" rid="bib78">Mihai et al., 2019</xref>), and the result of the current study. Effect sizes and standard errors were entered into a random effects model that was estimated with maximum likelihood using JASP 0.9 (jasp-stats.org).</p></sec><sec id="s4-12"><title>Statistical analysis of the MGB localizer</title><p>For the MGB localizer we used a stick function convolved with the HRF to model each presented sound. Null events were not modeled, as well as repeated sounds, to avoid a sensory-motor confound through the button-press. The data were modeled according to <xref ref-type="bibr" rid="bib89">Perrachione and Ghosh (2013)</xref> where repetition (TR = 2.8 s) and acquisition times (TA = 1.6 s) were modeled separately. The contrast <italic>Sound vs Silence</italic> was computed for each participant. The inference across participants was modeled using the first level contrasts in a second-level RFX analysis for the group. Significant voxels (see Section Masks below) in the left and right MGB found in the RFX analysis for the contrast sound vs silence were used as a mask for the tonotopy localizer.</p></sec><sec id="s4-13"><title>Statistical analysis of the tonotopy localizer</title><p>For the tonotopy localizer we followed a similar approach as <xref ref-type="bibr" rid="bib79">Moerel et al. (2015)</xref>. The sounds were first processed through the NSL toolbox (<xref ref-type="bibr" rid="bib19">Chi et al., 2005</xref>) which mimics the spectral transformation of sounds passing through the cochlea to the midbrain. This frequency representation includes a bank of 128 overlapping bandpass filters equally spaced on a log frequency axis (180-7040 Hz; range 5.3 octaves). The resulting spectrograms were averaged over time. To reduce overfitting we divided the tonotopic axis into 12 equal bandwidths in octaves and averaged the model’s output within these regions. The MrConfon headphones guarantee a linear frequency response up to 4 kHz, thus only the first 10 bins were used in the analysis, which resulted in 10 frequency bins for each sound file. The frequency model consisted of a vector of values corresponding to the frequency representations per sound. Since each sound had a frequency representation the final model is a matrix <inline-formula><mml:math id="inf21"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mi>S</mml:mi> <mml:mi/><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mo>]</mml:mo></mml:math></inline-formula>, where <inline-formula><mml:math id="inf22"><mml:mi>S</mml:mi></mml:math></inline-formula> is the number of sounds and <inline-formula><mml:math id="inf23"><mml:mi>F</mml:mi></mml:math></inline-formula> the number of features per sound. The predictors were z-scored across bins since low frequencies have more energy and would thus be more strongly represented compared to high frequencies (<xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>). The matrix was convolved with the hemodynamic response function and its components (i.e., the 10 frequency bins) were used as regressors of interest in the design matrix of SPM. In addition, we included the same regressors of no-interest as in the design matrix for the speech experiment (i.e., six respiratory regressors, six realignment parameters, and a variable number of outlier regressors from the ArtifactDetect step, depending on how many outliers were found). Parameter estimates were calculated for each frequency bin at the first level in native space.</p></sec><sec id="s4-14"><title>Masks</title><sec id="s4-14-1"><title>MGB localizer</title><p>We created masks using all voxels from the second level MGB localizer analysis for the contrast sound vs silence (family-wise error [FWE] corrected <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) constrained within a <inline-formula><mml:math id="inf25"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula> mm sphere centered at the voxel with the statistical maximum in the left and right MGB. We chose such a stringent p-value due to the strong effect and the multitude of above threshold voxels found within and around the left and right MGB. This procedure excluded all voxels which were clearly too far away from the structural boundaries of the MGB as seen in the MP2RAGE MNI template, yet still within the cluster, to be considered part of the MGB. These masks were inverse transformed per participant from MNI space to participant space using ANTs. Above threshold voxels (uncorrected <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) within the transformed masks were extracted, for each participant, from the MGB localizer <italic>Sound vs Silence</italic> contrast. These masks were then used to define each participant’s tonotopy with the tonotopy localizer.</p></sec><sec id="s4-14-2"><title>Tonotopy localizer</title><p>Each voxel within each participant’s left and right MGB and IC localizer mask was labeled according to the frequency bin to which it responded strongest, that is which had the highest parameter estimate (<xref ref-type="bibr" rid="bib79">Moerel et al., 2015</xref>). Thus, voxels would have values from 1 to 10 corresponding to the frequency bin that they best represented. This resulted in a map of frequency distributions from low to high frequencies in the left and right MGB for each participant. To create masks at the group level, these tonotopic maps were registered to MNI space using ANTs and averaged across participants.</p><p>To evaluate the tonotopic representations in the MGB and IC in a similar way as <xref ref-type="bibr" rid="bib79">Moerel et al. (2015)</xref>, we visually inspected the direction which showed the strongest tonotopy. This was a dorsal-lateral to ventral-medial gradient that was most visible in a sagittal view. We thus rotated and resliced the individual maps around the z-axis by <inline-formula><mml:math id="inf27"><mml:mn>90</mml:mn><mml:mo>°</mml:mo></mml:math></inline-formula>, which placed the sagittal view in the x-y plane. In this plane we calculated gradient directions in 10 adjacent slices, ensuring a representative coverage of the tonotopic pattern. A cut at <inline-formula><mml:math id="inf28"><mml:mn>90</mml:mn><mml:mo>°</mml:mo></mml:math></inline-formula> captured both low and high frequency areas. Histograms in 5° steps were calculated for each slice. The histograms of the gradients were then averaged first over slices per participant, followed by an average over participants. Based on the atlas by <xref ref-type="bibr" rid="bib80">Morel et al. (1997)</xref> and findings of MGB subdivisions in awake primates (<xref ref-type="bibr" rid="bib11">Bartlett and Wang, 2011</xref>) we parcellated the resulting frequency gradients as distinct regions. Voxels that represented the highest frequency were chosen as the boundary within each slice. Voxels above this boundary corresponded to one region, and those below this boundary to the other region. The regions were drawn in each slice using ITKSnap (v. 3.6.0; <xref ref-type="bibr" rid="bib123">Yushkevich et al., 2006</xref>). Volume size and center of mass (COM) for each gradient are listed in <xref ref-type="table" rid="table1">Table 1</xref>.</p><p>For the IC, we created functional masks of IC responses from the MGB localizer experiment (Sound vs Silence, uncorrected <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and constrained these to the anatomical volumes of the IC. Frequency distribution maps were calculated per participant in MNI space and averaged. Gradient directions were calculated from the mean tonotopy maps in three different slices. Histograms were then averaged and plotted. As the IC was not part of the main objective of this manuscript, we report it in the <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</p><p>Unthresholded t-maps of contrasts of interest, tonotopy maps of the MGB and IC, as well as vMGB masks are available on neurovault (<ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/4785/">https://neurovault.org/collections/4785/</ext-link>).</p></sec></sec><sec id="s4-15"><title>Significance testing</title><p>We used small volume corrections (SVC) to test for significant voxels for the contrast speech vs speaker task as well as the correlation of speech vs speaker task with the behavioral proportion correct scores in the speech task (significance defined as <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> FWE corrected for the region of interest). We tested bilaterally using the vMGB masks described above for the first hypothesis (main effect of task) and left vMGB for the second hypothesis (correlation between speech recognition performance and main effect of task) motivated by findings in previous studies (<xref ref-type="bibr" rid="bib112">von Kriegstein et al., 2008a</xref>; <xref ref-type="bibr" rid="bib28">Díaz et al., 2012</xref>).</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: The Ethics committee of the Medical Faculty, University of Leipzig, Germany, approved the study (protocol number 273/14-ff). Participants provided written informed consent to participate in the study and publish the material in a scientific journal.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><object-id pub-id-type="doi">10.7554/eLife.44837.020</object-id><label>Supplementary file 1.</label><caption><title>Montreal neurological institute (MNI) coordinates, p-values, T-values and parameter estimates (β) and 90% confidence intervals (CI) for voxels within regions for which we did not have an a-priori hypothesis.</title><p>Family-wise error corrected p-values were calculated using small volume correction for the voxels within the masks.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-44837-supp1-v1.docx"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.44837.021</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-44837-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>As participants did not give consent for their functional MRI data to be released publicly within the General Data Protection Regulation 2016/679 of the EU, these data can be made available on request to the corresponding author. Behavioral data are found in Figure 5-source data 1. Unthresholded maps have been uploaded to neurovault.org for the contrasts speech vs speaker task and the correlation speech vs speaker task with proportion of hits in the speech task (Figure 6). Additionally, the tonotopic maps for left and right MGB and IC are also provided (Figures 2, 3, and Figure 3-figure supplement 1), as well as the left and right vMGB mask (Figure 6). Files can be found at: <ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/4785/">https://neurovault.org/collections/4785/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Paul</surname><given-names>Glad Mihai</given-names></name><name><surname>Michelle</surname><given-names>Moerel</given-names></name><name><surname>Federico</surname><given-names>de Martino</given-names></name><name><surname>Robert</surname><given-names>Trampel</given-names></name><name><surname>Stefan</surname><given-names>Kiebel</given-names></name><name><surname>Katharina</surname><given-names>von Kriegstein</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Modulation of tonotopic ventral MGB is behaviorally relevant for speech recognition</data-title><source>NeuroVault</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://neurovault.org/collections/4785/">4785</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alcántara</surname> <given-names>JI</given-names></name><name><surname>Weisblatt</surname> <given-names>EJ</given-names></name><name><surname>Moore</surname> <given-names>BC</given-names></name><name><surname>Bolton</surname> <given-names>PF</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Speech-in-noise perception in high-functioning individuals with autism or Asperger's syndrome</article-title><source>Journal of Child Psychology and Psychiatry</source><volume>45</volume><fpage>1107</fpage><lpage>1114</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7610.2004.t01-1-00303.x</pub-id><pub-id pub-id-type="pmid">15257667</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>LA</given-names></name><name><surname>Wallace</surname> <given-names>MN</given-names></name><name><surname>Palmer</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Identification of subdivisions in the medial geniculate body of the guinea pig</article-title><source>Hearing Research</source><volume>228</volume><fpage>156</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2007.02.005</pub-id><pub-id pub-id-type="pmid">17399924</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>LA</given-names></name><name><surname>Christianson</surname> <given-names>GB</given-names></name><name><surname>Linden</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Stimulus-specific adaptation occurs in the auditory thalamus</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>7359</fpage><lpage>7363</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0793-09.2009</pub-id><pub-id pub-id-type="pmid">19494157</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>LA</given-names></name><name><surname>Linden</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Physiological differences between histologically defined subdivisions in the mouse auditory thalamus</article-title><source>Hearing Research</source><volume>274</volume><fpage>48</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2010.12.016</pub-id><pub-id pub-id-type="pmid">21185928</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname> <given-names>JL</given-names></name><name><surname>Hutton</surname> <given-names>C</given-names></name><name><surname>Ashburner</surname> <given-names>J</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Modeling geometric deformations in EPI time series</article-title><source>NeuroImage</source><volume>13</volume><fpage>903</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0746</pub-id><pub-id pub-id-type="pmid">11304086</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andolina</surname> <given-names>IM</given-names></name><name><surname>Jones</surname> <given-names>HE</given-names></name><name><surname>Wang</surname> <given-names>W</given-names></name><name><surname>Sillito</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Corticothalamic feedback enhances stimulus response precision in the visual system</article-title><source>PNAS</source><volume>104</volume><fpage>1685</fpage><lpage>1690</lpage><pub-id pub-id-type="doi">10.1073/pnas.0609318104</pub-id><pub-id pub-id-type="pmid">17237220</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antunes</surname> <given-names>FM</given-names></name><name><surname>Malmierca</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Effect of auditory cortex deactivation on stimulus-specific adaptation in the medial geniculate body</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>17306</fpage><lpage>17316</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1915-11.2011</pub-id><pub-id pub-id-type="pmid">22114297</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname> <given-names>BB</given-names></name><name><surname>Tustison</surname> <given-names>N</given-names></name><name><surname>Song</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Advanced normalization tools (ANTS)</article-title><source>The Insight Journal</source><volume>2</volume><fpage>1</fpage><lpage>35</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baron-Cohen</surname> <given-names>S</given-names></name><name><surname>Wheelwright</surname> <given-names>S</given-names></name><name><surname>Skinner</surname> <given-names>R</given-names></name><name><surname>Martin</surname> <given-names>J</given-names></name><name><surname>Clubley</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The autism-spectrum quotient (AQ): evidence from asperger syndrome/high-functioning autism, males and females, scientists and mathematicians</article-title><source>Journal of Autism and Developmental Disorders</source><volume>31</volume><fpage>5</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1023/A:1005653411471</pub-id><pub-id pub-id-type="pmid">11439754</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartlett</surname> <given-names>EL</given-names></name><name><surname>Sadagopan</surname> <given-names>S</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Fine frequency tuning in monkey auditory cortex and thalamus</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>849</fpage><lpage>859</lpage><pub-id pub-id-type="doi">10.1152/jn.00559.2010</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartlett</surname> <given-names>EL</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Correlation of neural response properties with auditory thalamus subdivisions in the awake marmoset</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>2647</fpage><lpage>2667</lpage><pub-id pub-id-type="doi">10.1152/jn.00238.2010</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumann</surname> <given-names>S</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Sun</surname> <given-names>L</given-names></name><name><surname>Petkov</surname> <given-names>CI</given-names></name><name><surname>Thiele</surname> <given-names>A</given-names></name><name><surname>Rees</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Orthogonal representation of sound dimensions in the primate midbrain</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>423</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1038/nn.2771</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Benesty</surname> <given-names>J</given-names></name><name><surname>Sondhi</surname> <given-names>MM</given-names></name><name><surname>Huang</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Springer Handbook of Speech Processing</source><publisher-loc>Berlin Heidelberg</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bordi</surname> <given-names>F</given-names></name><name><surname>LeDoux</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Response properties of single units in areas of rat auditory thalamus that project to the amygdala</article-title><source>Experimental Brain Research</source><volume>98</volume><fpage>261</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1007/BF00228414</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calford</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The parcellation of the medial geniculate body of the cat defined by the auditory response properties of single units</article-title><source>The Journal of Neuroscience</source><volume>3</volume><fpage>2350</fpage><lpage>2364</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.03-11-02350.1983</pub-id><pub-id pub-id-type="pmid">6631485</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camarillo</surname> <given-names>L</given-names></name><name><surname>Luna</surname> <given-names>R</given-names></name><name><surname>Nácher</surname> <given-names>V</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Coding perceptual discrimination in the somatosensory thalamus</article-title><source>PNAS</source><volume>109</volume><fpage>21093</fpage><lpage>21098</lpage><pub-id pub-id-type="doi">10.1073/pnas.1219636110</pub-id><pub-id pub-id-type="pmid">23213243</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname> <given-names>B</given-names></name><name><surname>Hornickel</surname> <given-names>J</given-names></name><name><surname>Skoe</surname> <given-names>E</given-names></name><name><surname>Nicol</surname> <given-names>T</given-names></name><name><surname>Kraus</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Context-Dependent encoding in the human auditory brainstem relates to hearing speech in noise: implications for developmental dyslexia</article-title><source>Neuron</source><volume>64</volume><fpage>311</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.10.006</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname> <given-names>B</given-names></name><name><surname>Kraus</surname> <given-names>N</given-names></name><name><surname>Wong</surname> <given-names>PC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Human inferior colliculus activity relates to individual differences in spoken language learning</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>1325</fpage><lpage>1336</lpage><pub-id pub-id-type="doi">10.1152/jn.00923.2011</pub-id><pub-id pub-id-type="pmid">22131377</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname> <given-names>T</given-names></name><name><surname>Ru</surname> <given-names>P</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title><source>The Journal of the Acoustical Society of America</source><volume>118</volume><fpage>887</fpage><lpage>906</lpage><pub-id pub-id-type="doi">10.1121/1.1945807</pub-id><pub-id pub-id-type="pmid">16158645</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1988">1988</year><source>Statistical Power Analysis for the Behavioral Sciences</source><publisher-name>Lawrence Erlbaum Associates</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cruikshank</surname> <given-names>SJ</given-names></name><name><surname>Killackey</surname> <given-names>HP</given-names></name><name><surname>Metherate</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Parvalbumin and calbindin are differentially distributed within primary and secondary subregions of the mouse auditory forebrain</article-title><source>Neuroscience</source><volume>105</volume><fpage>553</fpage><lpage>569</lpage><pub-id pub-id-type="doi">10.1016/S0306-4522(01)00226-3</pub-id><pub-id pub-id-type="pmid">11516823</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cudeiro</surname> <given-names>J</given-names></name><name><surname>Sillito</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Looking back: corticothalamic feedback and early visual processing</article-title><source>Trends in Neurosciences</source><volume>29</volume><fpage>298</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2006.05.002</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname> <given-names>MH</given-names></name><name><surname>Johnsrude</surname> <given-names>IS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Hierarchical processing in spoken language comprehension</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>3423</fpage><lpage>3431</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-08-03423.2003</pub-id><pub-id pub-id-type="pmid">12716950</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname> <given-names>MH</given-names></name><name><surname>Johnsrude</surname> <given-names>IS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hearing speech sounds: top-down influences on the interface between audition and speech perception</article-title><source>Hearing Research</source><volume>229</volume><fpage>132</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2007.01.014</pub-id><pub-id pub-id-type="pmid">17317056</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Hollander</surname> <given-names>G</given-names></name><name><surname>Keuken</surname> <given-names>MC</given-names></name><name><surname>van der Zwaag</surname> <given-names>W</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Trampel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Comparing functional MRI protocols for small, iron-rich basal ganglia nuclei such as the subthalamic nucleus at 7 T and 3 T</article-title><source>Human Brain Mapping</source><volume>38</volume><fpage>3226</fpage><lpage>3248</lpage><pub-id pub-id-type="doi">10.1002/hbm.23586</pub-id><pub-id pub-id-type="pmid">28345164</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de la Mothe</surname> <given-names>LA</given-names></name><name><surname>Blumell</surname> <given-names>S</given-names></name><name><surname>Kajikawa</surname> <given-names>Y</given-names></name><name><surname>Hackett</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Thalamic connections of the auditory cortex in marmoset monkeys: core and medial belt regions</article-title><source>The Journal of Comparative Neurology</source><volume>496</volume><fpage>72</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1002/cne.20924</pub-id><pub-id pub-id-type="pmid">16528728</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spatial organization of frequency preference and selectivity in the human inferior colliculus</article-title><source>Nature Communications</source><volume>4</volume><elocation-id>1386</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms2379</pub-id><pub-id pub-id-type="pmid">23340426</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Díaz</surname> <given-names>B</given-names></name><name><surname>Hintz</surname> <given-names>F</given-names></name><name><surname>Kiebel</surname> <given-names>SJ</given-names></name><name><surname>von Kriegstein</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Dysfunction of the auditory thalamus in developmental dyslexia</article-title><source>PNAS</source><volume>109</volume><fpage>13841</fpage><lpage>13846</lpage><pub-id pub-id-type="doi">10.1073/pnas.1119828109</pub-id><pub-id pub-id-type="pmid">22869724</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Díaz</surname> <given-names>B</given-names></name><name><surname>Blank</surname> <given-names>H</given-names></name><name><surname>von Kriegstein</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Task-dependent modulation of the visual sensory thalamus assists visual-speech recognition</article-title><source>NeuroImage</source><volume>178</volume><fpage>721</fpage><lpage>734</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.05.032</pub-id><pub-id pub-id-type="pmid">29772380</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diedenhofen</surname> <given-names>B</given-names></name><name><surname>Musch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cocor: a comprehensive solution for the statistical comparison of correlations</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0121945</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0121945</pub-id><pub-id pub-id-type="pmid">25835001</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duyn</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The future of ultra-high field MRI and fMRI for study of the human brain</article-title><source>NeuroImage</source><volume>62</volume><fpage>1241</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.065</pub-id><pub-id pub-id-type="pmid">22063093</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elliott</surname> <given-names>TM</given-names></name><name><surname>Theunissen</surname> <given-names>FE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The modulation transfer function for speech intelligibility</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000302</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000302</pub-id><pub-id pub-id-type="pmid">19266016</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ergenzinger</surname> <given-names>ER</given-names></name><name><surname>Glasier</surname> <given-names>MM</given-names></name><name><surname>Hahm</surname> <given-names>JO</given-names></name><name><surname>Pons</surname> <given-names>TP</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortically induced thalamic plasticity in the primate somatosensory system</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>226</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1038/673</pub-id><pub-id pub-id-type="pmid">10195147</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Salat</surname> <given-names>DH</given-names></name><name><surname>Busa</surname> <given-names>E</given-names></name><name><surname>Albert</surname> <given-names>M</given-names></name><name><surname>Dieterich</surname> <given-names>M</given-names></name><name><surname>Haselgrove</surname> <given-names>C</given-names></name><name><surname>van der Kouwe</surname> <given-names>A</given-names></name><name><surname>Killiany</surname> <given-names>R</given-names></name><name><surname>Kennedy</surname> <given-names>D</given-names></name><name><surname>Klaveness</surname> <given-names>S</given-names></name><name><surname>Montillo</surname> <given-names>A</given-names></name><name><surname>Makris</surname> <given-names>N</given-names></name><name><surname>Rosen</surname> <given-names>B</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Whole brain segmentation: automated labeling of neuroanatomical structures in the human brain</article-title><source>Neuron</source><volume>33</volume><fpage>341</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00569-X</pub-id><pub-id pub-id-type="pmid">11832223</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Salat</surname> <given-names>DH</given-names></name><name><surname>van der Kouwe</surname> <given-names>AJ</given-names></name><name><surname>Makris</surname> <given-names>N</given-names></name><name><surname>Ségonne</surname> <given-names>F</given-names></name><name><surname>Quinn</surname> <given-names>BT</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Sequence-independent segmentation of magnetic resonance images</article-title><source>NeuroImage</source><volume>23 Suppl 1</volume><fpage>S69</fpage><lpage>S84</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.016</pub-id><pub-id pub-id-type="pmid">15501102</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1915">1915</year><article-title>Frequency distribution of the values of the correlation coefficient in samples from an indefinitely large population</article-title><source>Biometrika</source><volume>10</volume><fpage>507</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.2307/2331838</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>KH</given-names></name><name><surname>Gaska</surname> <given-names>JP</given-names></name><name><surname>Nagler</surname> <given-names>M</given-names></name><name><surname>Pollen</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatial and temporal frequency selectivity of neurones in visual cortical areas V1 and V2 of the macaque monkey</article-title><source>The Journal of Physiology</source><volume>365</volume><fpage>331</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1985.sp015776</pub-id><pub-id pub-id-type="pmid">4032318</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname> <given-names>AD</given-names></name><name><surname>Gierhan</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The language network</article-title><source>Current Opinion in Neurobiology</source><volume>23</volume><fpage>250</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.10.002</pub-id><pub-id pub-id-type="pmid">23146876</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id><pub-id pub-id-type="pmid">15937014</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name><name><surname>Kiebel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Predictive coding under the free-energy principle</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>364</volume><fpage>1211</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0300</pub-id><pub-id pub-id-type="pmid">19528002</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galaburda</surname> <given-names>AM</given-names></name><name><surname>Menard</surname> <given-names>MT</given-names></name><name><surname>Rosen</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Evidence for aberrant auditory anatomy in developmental dyslexia</article-title><source>PNAS</source><volume>91</volume><fpage>8010</fpage><lpage>8013</lpage><pub-id pub-id-type="doi">10.1073/pnas.91.17.8010</pub-id><pub-id pub-id-type="pmid">8058748</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaudrain</surname> <given-names>E</given-names></name><name><surname>Ban</surname> <given-names>VS</given-names></name><name><surname>Patterson</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The role of glottal pulse rate and vocal tract length in the perception of speaker identity</article-title><source>Interspeech</source><volume>2009</volume><fpage>148</fpage><lpage>151</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name><name><surname>Nicolelis</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Feature article: the structure and function of dynamic cortical and thalamic receptive fields</article-title><source>Cerebral Cortex</source><volume>11</volume><fpage>183</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1093/cercor/11.3.183</pub-id><pub-id pub-id-type="pmid">11230091</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname> <given-names>AL</given-names></name><name><surname>Lorenzi</surname> <given-names>C</given-names></name><name><surname>Ashburner</surname> <given-names>J</given-names></name><name><surname>Wable</surname> <given-names>J</given-names></name><name><surname>Johnsrude</surname> <given-names>I</given-names></name><name><surname>Frackowiak</surname> <given-names>R</given-names></name><name><surname>Kleinschmidt</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Representation of the temporal envelope of sounds in the human brain</article-title><source>Journal of Neurophysiology</source><volume>84</volume><fpage>1588</fpage><lpage>1598</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.84.3.1588</pub-id><pub-id pub-id-type="pmid">10980029</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez-Lima</surname> <given-names>F</given-names></name><name><surname>Cada</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Cytochrome oxidase activity in the auditory system of the mouse: a qualitative and quantitative histochemical study</article-title><source>Neuroscience</source><volume>63</volume><fpage>559</fpage><lpage>578</lpage><pub-id pub-id-type="doi">10.1016/0306-4522(94)90550-9</pub-id><pub-id pub-id-type="pmid">7891865</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon-Salant</surname> <given-names>S</given-names></name><name><surname>Fitzgibbons</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Temporal Factors and Speech Recognition Performance in Young and Elderly Listeners</article-title><source>Journal of Speech, Language, and Hearing Research</source><volume>36</volume><fpage>1276</fpage><lpage>1285</lpage><pub-id pub-id-type="doi">10.1044/jshr.3606.1276</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname> <given-names>K</given-names></name><name><surname>Burns</surname> <given-names>CD</given-names></name><name><surname>Madison</surname> <given-names>C</given-names></name><name><surname>Clark</surname> <given-names>D</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Waskom</surname> <given-names>ML</given-names></name><name><surname>Ghosh</surname> <given-names>SS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in Python</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id><pub-id pub-id-type="pmid">21897815</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griswold</surname> <given-names>MA</given-names></name><name><surname>Jakob</surname> <given-names>PM</given-names></name><name><surname>Heidemann</surname> <given-names>RM</given-names></name><name><surname>Nittka</surname> <given-names>M</given-names></name><name><surname>Jellus</surname> <given-names>V</given-names></name><name><surname>Wang</surname> <given-names>J</given-names></name><name><surname>Kiefer</surname> <given-names>B</given-names></name><name><surname>Haase</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Generalized autocalibrating partially parallel acquisitions (GRAPPA)</article-title><source>Magnetic Resonance in Medicine</source><volume>47</volume><fpage>1202</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1002/mrm.10171</pub-id><pub-id pub-id-type="pmid">12111967</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname> <given-names>WB</given-names></name><name><surname>van Orsouw</surname> <given-names>L</given-names></name><name><surname>Huurne</surname> <given-names>Nter</given-names></name><name><surname>Swinkels</surname> <given-names>S</given-names></name><name><surname>van der Gaag</surname> <given-names>R-J</given-names></name><name><surname>Buitelaar</surname> <given-names>JK</given-names></name><name><surname>Zwiers</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Intact Spectral but Abnormal Temporal Processing of Auditory Stimuli in Autism</article-title><source>Journal of Autism and Developmental Disorders</source><volume>39</volume><fpage>742</fpage><lpage>750</lpage><pub-id pub-id-type="doi">10.1007/s10803-008-0682-3</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hackett</surname> <given-names>TA</given-names></name><name><surname>Stepniewska</surname> <given-names>I</given-names></name><name><surname>Kaas</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Thalamocortical connections of the parabelt auditory cortex in macaque monkeys</article-title><source>The Journal of Comparative Neurology</source><volume>400</volume><fpage>271</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19981019)400:2&lt;271::AID-CNE8&gt;3.0.CO;2-6</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hackett</surname> <given-names>TA</given-names></name><name><surname>Barkat</surname> <given-names>TR</given-names></name><name><surname>O'Brien</surname> <given-names>BM</given-names></name><name><surname>Hensch</surname> <given-names>TK</given-names></name><name><surname>Polley</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Linking topography to tonotopy in the mouse auditory thalamocortical circuit</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>2983</fpage><lpage>2995</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5333-10.2011</pub-id><pub-id pub-id-type="pmid">21414920</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname> <given-names>X</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Atlas renormalization for improved brain MR image segmentation across scanner platforms</article-title><source>IEEE Transactions on Medical Imaging</source><volume>26</volume><fpage>479</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1109/TMI.2007.893282</pub-id><pub-id pub-id-type="pmid">17427735</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname> <given-names>JD</given-names></name><name><surname>Deichmann</surname> <given-names>R</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Eye-specific effects of binocular rivalry in the human lateral geniculate nucleus</article-title><source>Nature</source><volume>438</volume><fpage>496</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1038/nature04169</pub-id><pub-id pub-id-type="pmid">16244649</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hayward</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Experimental Phonetics, Harlow, Eng</source><publisher-loc>New York</publisher-loc><publisher-name>Longman</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The cortical organization of speech processing</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1038/nrn2113</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hicks</surname> <given-names>TP</given-names></name><name><surname>Lee</surname> <given-names>BB</given-names></name><name><surname>Vidyasagar</surname> <given-names>TR</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The responses of cells in macaque lateral geniculate nucleus to sinusoidal gratings</article-title><source>The Journal of Physiology</source><volume>337</volume><fpage>183</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1983.sp014619</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hittner</surname> <given-names>JB</given-names></name><name><surname>May</surname> <given-names>K</given-names></name><name><surname>Silver</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A monte carlo evaluation of tests for comparing dependent correlations</article-title><source>The Journal of General Psychology</source><volume>130</volume><fpage>149</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1080/00221300309601282</pub-id><pub-id pub-id-type="pmid">12773018</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hoffman</surname> <given-names>MD</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The No-U-Turn sampler: adaptively setting path lengths in hamiltonian monte carlo</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1111.4246">https://arxiv.org/abs/1111.4246</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horie</surname> <given-names>M</given-names></name><name><surname>Tsukano</surname> <given-names>H</given-names></name><name><surname>Hishida</surname> <given-names>R</given-names></name><name><surname>Takebayashi</surname> <given-names>H</given-names></name><name><surname>Shibuki</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dual compartments of the ventral division of the medial geniculate body projecting to the core region of the auditory cortex in C57BL/6 mice</article-title><source>Neuroscience Research</source><volume>76</volume><fpage>207</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2013.05.004</pub-id><pub-id pub-id-type="pmid">23692741</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname> <given-names>AG</given-names></name><name><surname>de Heer</surname> <given-names>WA</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>Theunissen</surname> <given-names>FE</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title><source>Nature</source><volume>532</volume><fpage>453</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1038/nature17637</pub-id><pub-id pub-id-type="pmid">27121839</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imig</surname> <given-names>TJ</given-names></name><name><surname>Morel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Tonotopic organization in ventral nucleus of medial geniculate body in the cat</article-title><source>Journal of Neurophysiology</source><volume>53</volume><fpage>309</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1152/jn.1985.53.1.309</pub-id><pub-id pub-id-type="pmid">3973661</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>EG</given-names></name></person-group><year iso-8601-date="1985">1985</year><source>The Thalamus</source><publisher-loc>New York</publisher-loc><publisher-name>Plenum Press</publisher-name></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kasper</surname> <given-names>L</given-names></name><name><surname>Bollmann</surname> <given-names>S</given-names></name><name><surname>Diaconescu</surname> <given-names>AO</given-names></name><name><surname>Hutton</surname> <given-names>C</given-names></name><name><surname>Heinzle</surname> <given-names>J</given-names></name><name><surname>Iglesias</surname> <given-names>S</given-names></name><name><surname>Hauser</surname> <given-names>TU</given-names></name><name><surname>Sebold</surname> <given-names>M</given-names></name><name><surname>Manjaly</surname> <given-names>ZM</given-names></name><name><surname>Pruessmann</surname> <given-names>KP</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The PhysIO toolbox for modeling physiological noise in fMRI data</article-title><source>Journal of Neuroscience Methods</source><volume>276</volume><fpage>56</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.10.019</pub-id><pub-id pub-id-type="pmid">27832957</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kent</surname> <given-names>RD</given-names></name><name><surname>Read</surname> <given-names>C</given-names></name><name><surname>Kent</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>The Acoustic Analysis of Speech</source><publisher-loc>San Diego</publisher-loc><publisher-name>Singular Publishing Group</publisher-name></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiebel</surname> <given-names>SJ</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A hierarchy of time-scales and the brain</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000209</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000209</pub-id><pub-id pub-id-type="pmid">19008936</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klostermann</surname> <given-names>F</given-names></name><name><surname>Krugel</surname> <given-names>LK</given-names></name><name><surname>Ehlen</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Functional roles of the thalamus for language capacities</article-title><source>Frontiers in Systems Neuroscience</source><volume>7</volume><elocation-id>32</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2013.00032</pub-id><pub-id pub-id-type="pmid">23882191</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname> <given-names>DC</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title><source>Trends in Neurosciences</source><volume>27</volume><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krupa</surname> <given-names>DJ</given-names></name><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name><name><surname>Nicolelis</surname> <given-names>MAL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Immediate thalamic sensory plasticity depends on corticothalamic feedback</article-title><source>PNAS</source><volume>96</volume><fpage>8200</fpage><lpage>8205</lpage><pub-id pub-id-type="doi">10.1073/pnas.96.14.8200</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Thalamic and cortical pathways supporting auditory processing</article-title><source>Brain and Language</source><volume>126</volume><fpage>22</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2012.05.004</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>CC</given-names></name><name><surname>Sherman</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Intrinsic modulators of auditory thalamocortical transmission</article-title><source>Hearing Research</source><volume>287</volume><fpage>43</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2012.04.001</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>CC</given-names></name><name><surname>Winer</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Convergence of thalamic and cortical pathways in cat auditory cortex</article-title><source>Hearing Research</source><volume>274</volume><fpage>85</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2010.05.008</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewandowski</surname> <given-names>D</given-names></name><name><surname>Kurowicka</surname> <given-names>D</given-names></name><name><surname>Joe</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating random correlation matrices based on vines and extended onion method</article-title><source>Journal of Multivariate Analysis</source><volume>100</volume><fpage>1989</fpage><lpage>2001</lpage><pub-id pub-id-type="doi">10.1016/j.jmva.2009.04.008</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Llano</surname> <given-names>DA</given-names></name><name><surname>Sherman</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Evidence for nonreciprocal organization of the mouse auditory thalamocortical-corticothalamic projection systems</article-title><source>The Journal of Comparative Neurology</source><volume>507</volume><fpage>1209</fpage><lpage>1227</lpage><pub-id pub-id-type="doi">10.1002/cne.21602</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malmierca</surname> <given-names>MS</given-names></name><name><surname>Anderson</surname> <given-names>LA</given-names></name><name><surname>Antunes</surname> <given-names>FM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The cortical modulation of stimulus-specific adaptation in the auditory midbrain and thalamus: a potential neuronal correlate for predictive coding</article-title><source>Frontiers in Systems Neuroscience</source><volume>9</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2015.00019</pub-id><pub-id pub-id-type="pmid">25805974</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marques</surname> <given-names>JP</given-names></name><name><surname>Kober</surname> <given-names>T</given-names></name><name><surname>Krueger</surname> <given-names>G</given-names></name><name><surname>van der Zwaag</surname> <given-names>W</given-names></name><name><surname>Van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Gruetter</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>MP2RAGE, a self bias-field corrected sequence for improved segmentation and T1-mapping at high field</article-title><source>NeuroImage</source><volume>49</volume><fpage>1271</fpage><lpage>1281</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.10.002</pub-id><pub-id pub-id-type="pmid">19819338</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAlonan</surname> <given-names>K</given-names></name><name><surname>Cavanaugh</surname> <given-names>J</given-names></name><name><surname>Wurtz</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Guarding the gateway to cortex with attention in visual thalamus</article-title><source>Nature</source><volume>456</volume><fpage>391</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.1038/nature07382</pub-id><pub-id pub-id-type="pmid">18849967</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Mcelreath</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Statistical Rethinking: A Bayesian Course with Examples in R and Stan [Online]</data-title></element-citation></ref><ref id="bib78"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mihai</surname> <given-names>PG</given-names></name><name><surname>Tschentscher</surname> <given-names>N</given-names></name><name><surname>Kriegstein</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The role of the ventral MGB in speech in noise comprehension</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/646570</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Uğurbil</surname> <given-names>K</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Processing of frequency and location in human subcortical auditory structures</article-title><source>Scientific Reports</source><volume>5</volume><pub-id pub-id-type="doi">10.1038/srep17048</pub-id><pub-id pub-id-type="pmid">26597173</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morel</surname> <given-names>A</given-names></name><name><surname>Magnin</surname> <given-names>M</given-names></name><name><surname>Jeanmonod</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Multiarchitectonic and stereotactic atlas of the human thalamus</article-title><source>The Journal of Comparative Neurology</source><volume>387</volume><fpage>588</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19971103)387:4&lt;588::AID-CNE8&gt;3.0.CO;2-Z</pub-id><pub-id pub-id-type="pmid">9373015</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morest</surname> <given-names>DK</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>The neuronal architecture of the medial geniculate body of the cat</article-title><source>Journal of Anatomy</source><volume>98</volume><fpage>611</fpage><lpage>630</lpage><pub-id pub-id-type="pmid">14229992</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moro</surname> <given-names>SS</given-names></name><name><surname>Kelly</surname> <given-names>KR</given-names></name><name><surname>McKetton</surname> <given-names>L</given-names></name><name><surname>Gallie</surname> <given-names>BL</given-names></name><name><surname>Steeves</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Evidence of multisensory plasticity: asymmetrical medial geniculate body in people with one eye</article-title><source>NeuroImage: Clinical</source><volume>9</volume><fpage>513</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1016/j.nicl.2015.09.016</pub-id><pub-id pub-id-type="pmid">26594632</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller-Axt</surname> <given-names>C</given-names></name><name><surname>Anwander</surname> <given-names>A</given-names></name><name><surname>von Kriegstein</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Altered structural connectivity of the left visual thalamus in developmental dyslexia</article-title><source>Current Biology</source><volume>27</volume><fpage>3692</fpage><lpage>3698</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.10.034</pub-id><pub-id pub-id-type="pmid">29153326</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Connor</surname> <given-names>DH</given-names></name><name><surname>Fukui</surname> <given-names>MM</given-names></name><name><surname>Pinsk</surname> <given-names>MA</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Attention modulates responses in the human lateral geniculate nucleus</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>1203</fpage><lpage>1209</lpage><pub-id pub-id-type="doi">10.1038/nn957</pub-id><pub-id pub-id-type="pmid">12379861</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohga</surname> <given-names>S</given-names></name><name><surname>Tsukano</surname> <given-names>H</given-names></name><name><surname>Horie</surname> <given-names>M</given-names></name><name><surname>Terashima</surname> <given-names>H</given-names></name><name><surname>Nishio</surname> <given-names>N</given-names></name><name><surname>Kubota</surname> <given-names>Y</given-names></name><name><surname>Takahashi</surname> <given-names>K</given-names></name><name><surname>Hishida</surname> <given-names>R</given-names></name><name><surname>Takebayashi</surname> <given-names>H</given-names></name><name><surname>Shibuki</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Direct relay pathways from lemniscal auditory thalamus to secondary auditory field in mice</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>4424</fpage><lpage>4439</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy234</pub-id><pub-id pub-id-type="pmid">30272122</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ojima</surname> <given-names>H</given-names></name><name><surname>Rouiller</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Auditory Cortical Projections to the Medial Geniculate Body</chapter-title><person-group person-group-type="editor"><name><surname>Winer</surname> <given-names>J. A</given-names></name><name><surname>Schreiner</surname> <given-names>C. E</given-names></name></person-group><source>The Auditory Cortex</source><publisher-loc>Boston, MA</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldfield</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The assessment and analysis of handedness: the edinburgh inventory</article-title><source>Neuropsychologia</source><volume>9</volume><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id><pub-id pub-id-type="pmid">5146491</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panouillères</surname> <given-names>MTN</given-names></name><name><surname>Möttönen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Decline of auditory-motor speech processing in older adults with hearing loss</article-title><source>Neurobiology of Aging</source><volume>72</volume><fpage>89</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/j.neurobiolaging.2018.07.013</pub-id><pub-id pub-id-type="pmid">30240945</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrachione</surname> <given-names>TK</given-names></name><name><surname>Ghosh</surname> <given-names>SS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Optimized design and analysis of Sparse-Sampling fMRI experiments</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>55</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00055</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname> <given-names>C</given-names></name><name><surname>Thierry</surname> <given-names>G</given-names></name><name><surname>Griffiths</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Speech-specific auditory processing: where is it?</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>271</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.03.009</pub-id><pub-id pub-id-type="pmid">15925805</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademacher</surname> <given-names>J</given-names></name><name><surname>Bürgel</surname> <given-names>U</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Stereotaxic localization, intersubject variability, and interhemispheric differences of the human auditory thalamocortical system</article-title><source>NeuroImage</source><volume>17</volume><fpage>142</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1178</pub-id><pub-id pub-id-type="pmid">12482073</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname> <given-names>JP</given-names></name><name><surname>Scott</surname> <given-names>SK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>718</fpage><lpage>724</lpage><pub-id pub-id-type="doi">10.1038/nn.2331</pub-id><pub-id pub-id-type="pmid">19471271</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodrigues-Dagaeff</surname> <given-names>C</given-names></name><name><surname>Simm</surname> <given-names>G</given-names></name><name><surname>De Ribaupierre</surname> <given-names>Y</given-names></name><name><surname>Villa</surname> <given-names>A</given-names></name><name><surname>De Ribaupierre</surname> <given-names>F</given-names></name><name><surname>Rouiller</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Functional organization of the ventral division of the medial geniculate body of the cat: evidence for a rostro-caudal gradient of response properties and cortical projections</article-title><source>Hearing Research</source><volume>39</volume><fpage>103</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1016/0378-5955(89)90085-3</pub-id><pub-id pub-id-type="pmid">2737959</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouiller</surname> <given-names>EM</given-names></name><name><surname>de Ribaupierre</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Origin of afferents to physiologically defined regions of the medial geniculate body of the cat: ventral and dorsal divisions</article-title><source>Hearing Research</source><volume>19</volume><fpage>97</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1016/0378-5955(85)90114-5</pub-id><pub-id pub-id-type="pmid">4055537</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saalmann</surname> <given-names>YB</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cognitive and perceptual functions of the visual thalamus</article-title><source>Neuron</source><volume>71</volume><fpage>209</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.027</pub-id><pub-id pub-id-type="pmid">21791281</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Statistical language learning: mechanisms and constraints</article-title><source>Current Directions in Psychological Science</source><volume>12</volume><fpage>110</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1111/1467-8721.01243</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salvatier</surname> <given-names>J</given-names></name><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Fonnesbeck</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Probabilistic programming in Python using PyMC3</article-title><source>PeerJ Computer Science</source><volume>2</volume><elocation-id>e55</elocation-id><pub-id pub-id-type="doi">10.7717/peerj-cs.55</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schneider</surname> <given-names>W</given-names></name><name><surname>Ennemoser</surname> <given-names>M</given-names></name><name><surname>SCHLAGMÜLLER</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Lesegeschwindigkeits- Und -Verständnistest Für Die Klassen 6 - 12: LGVT 6 - 12</source><publisher-name>Hogrefe</publisher-name></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuirmann</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability</article-title><source>Journal of Pharmacokinetics and Biopharmaceutics</source><volume>15</volume><fpage>657</fpage><lpage>680</lpage><pub-id pub-id-type="doi">10.1007/BF01068419</pub-id><pub-id pub-id-type="pmid">3450848</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname> <given-names>RV</given-names></name><name><surname>Zeng</surname> <given-names>FG</given-names></name><name><surname>Kamath</surname> <given-names>V</given-names></name><name><surname>Wygonski</surname> <given-names>J</given-names></name><name><surname>Ekelid</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Speech recognition with primarily temporal cues</article-title><source>Science</source><volume>270</volume><fpage>303</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1126/science.270.5234.303</pub-id><pub-id pub-id-type="pmid">7569981</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherman</surname> <given-names>SM</given-names></name><name><surname>Guillery</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>On the actions that one nerve cell can have on another: distinguishing &quot;drivers&quot; from &quot;modulators&quot;</article-title><source>PNAS</source><volume>95</volume><fpage>7121</fpage><lpage>7126</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.12.7121</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sherman</surname> <given-names>SM</given-names></name><name><surname>Guillery</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Exploring the Thalamus and Its Role in Cortical Function</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shiramatsu</surname> <given-names>TI</given-names></name><name><surname>Takahashi</surname> <given-names>K</given-names></name><name><surname>Noda</surname> <given-names>T</given-names></name><name><surname>Kanzaki</surname> <given-names>R</given-names></name><name><surname>Nakahara</surname> <given-names>H</given-names></name><name><surname>Takahashi</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Microelectrode mapping of tonotopic, laminar, and field-specific organization of thalamo-cortical pathway in rat</article-title><source>Neuroscience</source><volume>332</volume><fpage>38</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2016.06.024</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sillito</surname> <given-names>AM</given-names></name><name><surname>Jones</surname> <given-names>HE</given-names></name><name><surname>Gerstein</surname> <given-names>GL</given-names></name><name><surname>West</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Feature-linked synchronization of thalamic relay cell firing induced by feedback from the visual cortex</article-title><source>Nature</source><volume>369</volume><fpage>479</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1038/369479a0</pub-id><pub-id pub-id-type="pmid">8202137</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sillito</surname> <given-names>AM</given-names></name><name><surname>Cudeiro</surname> <given-names>J</given-names></name><name><surname>Jones</surname> <given-names>HE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Always returning: feedback and sensory processing in visual cortex and thalamus</article-title><source>Trends in Neurosciences</source><volume>29</volume><fpage>307</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2006.05.001</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Johansen-Berg</surname> <given-names>H</given-names></name><name><surname>Bannister</surname> <given-names>PR</given-names></name><name><surname>De Luca</surname> <given-names>M</given-names></name><name><surname>Drobnjak</surname> <given-names>I</given-names></name><name><surname>Flitney</surname> <given-names>DE</given-names></name><name><surname>Niazy</surname> <given-names>RK</given-names></name><name><surname>Saunders</surname> <given-names>J</given-names></name><name><surname>Vickers</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>De Stefano</surname> <given-names>N</given-names></name><name><surname>Brady</surname> <given-names>JM</given-names></name><name><surname>Matthews</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><volume>23 Suppl 1</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tallal</surname> <given-names>P</given-names></name><name><surname>Miller</surname> <given-names>SL</given-names></name><name><surname>Bedi</surname> <given-names>G</given-names></name><name><surname>Byma</surname> <given-names>G</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Nagarajan</surname> <given-names>SS</given-names></name><name><surname>Schreiner</surname> <given-names>C</given-names></name><name><surname>Jenkins</surname> <given-names>WM</given-names></name><name><surname>Merzenich</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Language comprehension in language-learning impaired children improved with acoustically modified speech</article-title><source>Science</source><volume>271</volume><fpage>81</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1126/science.271.5245.81</pub-id><pub-id pub-id-type="pmid">8539604</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tallal</surname> <given-names>P</given-names></name><name><surname>Piercy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Developmental aphasia: the perception of brief vowels and extended stop consonants</article-title><source>Neuropsychologia</source><volume>13</volume><fpage>69</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(75)90049-4</pub-id><pub-id pub-id-type="pmid">1109463</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tschentscher</surname> <given-names>N</given-names></name><name><surname>Ruisinger</surname> <given-names>A</given-names></name><name><surname>Blank</surname> <given-names>H</given-names></name><name><surname>Díaz</surname> <given-names>B</given-names></name><name><surname>von Kriegstein</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reduced structural connectivity between left auditory thalamus and the motion-sensitive planum temporale in developmental dyslexia</article-title><source>The Journal of Neuroscience</source><volume>39</volume><elocation-id>1435-18</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1435-18.2018</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsukano</surname> <given-names>H</given-names></name><name><surname>Horie</surname> <given-names>M</given-names></name><name><surname>Ohga</surname> <given-names>S</given-names></name><name><surname>Takahashi</surname> <given-names>K</given-names></name><name><surname>Kubota</surname> <given-names>Y</given-names></name><name><surname>Hishida</surname> <given-names>R</given-names></name><name><surname>Takebayashi</surname> <given-names>H</given-names></name><name><surname>Shibuki</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reconsidering tonotopic maps in the auditory cortex and lemniscal auditory thalamus in mice</article-title><source>Frontiers in Neural Circuits</source><volume>11</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2017.00014</pub-id><pub-id pub-id-type="pmid">28293178</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasquez-Lopez</surname> <given-names>SA</given-names></name><name><surname>Weissenberger</surname> <given-names>Y</given-names></name><name><surname>Lohse</surname> <given-names>M</given-names></name><name><surname>Keating</surname> <given-names>P</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name><name><surname>Dahmen</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Thalamic input to auditory cortex is locally heterogeneous but globally tonotopic</article-title><source>eLife</source><volume>6</volume><elocation-id>e25141</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.25141</pub-id><pub-id pub-id-type="pmid">28891466</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Kriegstein</surname> <given-names>K</given-names></name><name><surname>Dogan</surname> <given-names>O</given-names></name><name><surname>Grüter</surname> <given-names>M</given-names></name><name><surname>Giraud</surname> <given-names>AL</given-names></name><name><surname>Kell</surname> <given-names>CA</given-names></name><name><surname>Grüter</surname> <given-names>T</given-names></name><name><surname>Kleinschmidt</surname> <given-names>A</given-names></name><name><surname>Kiebel</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Simulation of talking faces in the human brain improves auditory speech recognition</article-title><source>PNAS</source><volume>105</volume><fpage>6747</fpage><lpage>6752</lpage><pub-id pub-id-type="doi">10.1073/pnas.0710826105</pub-id><pub-id pub-id-type="pmid">18436648</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Kriegstein</surname> <given-names>K</given-names></name><name><surname>Patterson</surname> <given-names>RD</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>Task-dependent modulation of medial geniculate body is behaviorally relevant for speech recognition</article-title><source>Current Biology</source><volume>18</volume><fpage>1855</fpage><lpage>1859</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.10.052</pub-id><pub-id pub-id-type="pmid">19062286</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Lu</surname> <given-names>T</given-names></name><name><surname>Bendor</surname> <given-names>D</given-names></name><name><surname>Bartlett</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural coding of temporal information in auditory thalamus and cortex</article-title><source>Neuroscience</source><volume>154</volume><fpage>294</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2008.03.065</pub-id><pub-id pub-id-type="pmid">18555164</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>W</given-names></name><name><surname>Andolina</surname> <given-names>IM</given-names></name><name><surname>Lu</surname> <given-names>Y</given-names></name><name><surname>Jones</surname> <given-names>HE</given-names></name><name><surname>Sillito</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Focal gain control of thalamic visual receptive fields by layer 6 corticothalamic feedback</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>267</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw376</pub-id><pub-id pub-id-type="pmid">27988493</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Zhang</surname> <given-names>J</given-names></name><name><surname>Zou</surname> <given-names>J</given-names></name><name><surname>Luo</surname> <given-names>H</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Prior knowledge guides speech segregation in human auditory cortex</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>1561</fpage><lpage>1571</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy052</pub-id><pub-id pub-id-type="pmid">29788144</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wernicke</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1874">1874</year><source>Der Aphasische Symptomencomplex: Eine Psychologische Studie Auf Anatomischer Basis</source><publisher-name>Cohn</publisher-name></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname> <given-names>S</given-names></name><name><surname>Frith</surname> <given-names>U</given-names></name><name><surname>Milne</surname> <given-names>E</given-names></name><name><surname>Rosen</surname> <given-names>S</given-names></name><name><surname>Swettenham</surname> <given-names>J</given-names></name><name><surname>Ramus</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A double dissociation between sensorimotor impairments and reading disability: A comparison of autistic and dyslexic children</article-title><source>Cognitive Neuropsychology</source><volume>23</volume><fpage>748</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1080/02643290500438607</pub-id><pub-id pub-id-type="pmid">21049352</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winer</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>The human medial geniculate body</article-title><source>Hearing Research</source><volume>15</volume><fpage>225</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/0378-5955(84)90031-5</pub-id><pub-id pub-id-type="pmid">6501112</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winer</surname> <given-names>JA</given-names></name><name><surname>Kelly</surname> <given-names>JB</given-names></name><name><surname>Larue</surname> <given-names>DT</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neural architecture of the rat medial geniculate body</article-title><source>Hearing Research</source><volume>130</volume><fpage>19</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(98)00216-0</pub-id><pub-id pub-id-type="pmid">10320097</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winer</surname> <given-names>JA</given-names></name><name><surname>Miller</surname> <given-names>LM</given-names></name><name><surname>Lee</surname> <given-names>CC</given-names></name><name><surname>Schreiner</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Auditory thalamocortical transformation: structure and function</article-title><source>Trends in Neurosciences</source><volume>28</volume><fpage>255</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2005.03.009</pub-id><pub-id pub-id-type="pmid">15866200</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winer</surname> <given-names>JA</given-names></name><name><surname>Prieto</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Layer V in cat primary auditory cortex (AI): cellular architecture and identification of projection neurons</article-title><source>The Journal of Comparative Neurology</source><volume>434</volume><fpage>379</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1002/cne.1183</pub-id><pub-id pub-id-type="pmid">11343289</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yushkevich</surname> <given-names>PA</given-names></name><name><surname>Piven</surname> <given-names>J</given-names></name><name><surname>Hazlett</surname> <given-names>HC</given-names></name><name><surname>Smith</surname> <given-names>RG</given-names></name><name><surname>Ho</surname> <given-names>S</given-names></name><name><surname>Gee</surname> <given-names>JC</given-names></name><name><surname>Gerig</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability</article-title><source>NeuroImage</source><volume>31</volume><fpage>1116</fpage><lpage>1128</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.015</pub-id><pub-id pub-id-type="pmid">16545965</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.44837.024</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role>Reviewing Editor</role><aff><institution>Newcastle University</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role>Reviewer</role><aff><institution>Newcastle University</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Davis</surname><given-names>Matthew H</given-names></name><role>Reviewer</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Modulation of tonotopic ventral MGB is behaviorally relevant for speech recognition&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Timothy D Griffiths as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Andrew King as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Matthew H Davis (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>The work is well executed in seeking changes for a controlled contrast between a speech and non-speech task in vMGB, could fit with a predictive coding account, which would be interesting and novel in applying predictive coding accounts to subcortical structures. The main issue is that more robust support for the conclusion that the MGB is involved in processing speech is required: the reviewers were concerned about the absence of a straightforward main effect for the speech minus non-speech task, the weak association between accuracy scores in the speech task and MGB modulation, and the interpretation of the latter which was clearly not based on an a prior hypothesis. We felt further analyses to achieve this were possible within two months hence the decision to ask for a major revision.</p><p>The individual reviews are appended below, but the major changes required are summarised here by the Reviewing Editor:</p><p>1) Further analyses to combine the present null finding for the main effect of task with previous results as suggested by reviewer 3.</p><p>2) Further analysis to make the correlation between performance and activation in MGB more robust is also required.</p><p><italic>Reviewer #1:</italic> </p><p>I think the work is well executed and very interesting in seeking changes for a nicely controlled contrast between a speech and non-speech task in vMGB, which I agree could fit with predictive coding account, and which would be interesting in applying predictive coding accounts to subcortical structures which I think is novel in auditory work.</p><p>I had some questions for the authors:</p><p>1) I did not actually think the tonotopy data were required but I guess they do make the case they are examining a lemniscal area. I would be interested in whether the authors feel the results are consistent with those in macaque (PMID 21378972). The gradient running anteromedially looks similar to me and I would not expect much difference (in contrast to the speech).</p><p>2) The existence of a significant speech minus non-speech contrast might have fitted a predictive coding account and was clearly predicted by the authors. I found the left vMGB correlation between the [speech vs speaker] contrast and speech recognition performance interesting but rather more nuanced in interpretation and I think that slightly reduces the impact of the work.</p><p><italic>Reviewer #2:</italic> </p><p>The study examines the extent to which task-related modulation (speech&gt;speaker focus using the same set of stimuli) is present within the ventral medial geniculate body (MGB), the section that from animal models and previous work in humans is known to be tonotopically organized. To achieve this goal, the authors run several different conditions at ultra-high field (7T): a tonotopy localizer that replicates a prior study Moerel et al., 2015, in a significant sample size (28 participants!) establishing tonotopic gradients within the MGB, and a behavioral task involving participants making judgments (one back) regarding change in speech content (consonant/vowel) or speaker. The results show clear tonotopic gradients-based on animal models and anatomical considerations of vMGB were identified. This is a direct replication of Moerel et al., 2015. The second part of the experiment involved examining task-related modulation of the identified vMGB, with the hypothesis that speech change&gt;speaker change will evoke the left MGB (consistent with previous experiments from the group: von Kriegstein et al., 2008, Diaz et al., 2012). This prediction was invalidated by the results that show no task-related modulation (speech&gt;speaker) in the vMGB. A potential confound here is that the speaker change condition was statistically harder (speaker judgments were within sex) than the speech change. This behavioral result differed from prior experiments from the same group. An additional correlation analysis was performed indicating a weak association between accuracy scores in the speech task and MGB modulation (greater task modulation relates to better speech performance). There is little variability however in the speech task performance (is the range from 84% to 95% biologically-relevant?).</p><p>Major comments: There are less than a handful of studies examining the human auditory thalamus at ultra-high fields-a severe paucity that limits our understanding of thalamic function in processing behaviorally-relevant signals in humans. Within this context, the goals of the present study are laudable. The study convincingly shows tonotopic representation in the vMGB, providing a large-scale replication of Moerel et al., 2015. I'm not at all convinced about the other main thrust of the study related to task modulation/speech recognition. The results show no statistically significant difference in vMGB activity as a function of task. The correlation analyses yield a weak association, but the nature of this association is unclear. What does it mean that modulation in the left vMGB correlates with the speech recognition scores when the task performance in speech vary very little, and there are substantial accuracy differences between the speech and the speaker task? The authors provide three potential explanations for their lack of replication of prior work and seem to suggest that the most likely explanation is that MGB modulation might also have played a role in performing the speaker task. I agree, but this also suggests a critical flaw with the study design which a priori defined task modulation on the basis of a difference in the speech&gt;speaker contrast.</p><p><italic>Reviewer #3:</italic> </p><p>This paper describes a high resolution fMRI study to establish whether a tonotopic, auditory region of the thalamus (ventral MGB) corresponds to the thalamus region previously shown to have activity modulated during performance of a speech perception task (compared to a well matched speaker identification task), and to correlate with individual differences in speech perception performance. The study appears well constructed and uses an impressive set of state-of-the-art methods including ultra-high field 7T fMRI, sub-mm voxels and sparse imaging, tonotopic modelling to specify frequency preferences and between voxel analysis to identify tonotopic gradients, etc.</p><p>However, despite these many technical strengths, I didn't feel that the present manuscript led to a substantial advance in my understanding of the contributions made by the auditory thalamus to speech perception. In part this is due to a surprising failure to replicate several previous findings of additional activation for a speech identification than a speaker identification task (from three similar conventional 3T fMRI paradigms reported previously by the same group). Both for this result, and the cross-subject correlation of activation with performance that is replicated in the present study, I felt that additional statistical analysis would allow the authors to strengthen their interpretation of their findings. At present, too many of the conclusions of the study appear to be speculation in the absence of statistical evidence. I'll expand on this below:</p><p>1) Speech &gt; speaker task activation in vMGB – the authors remark that their failure to observe this effect is: &quot;surprising, as this categorical task-effect was observed in three previous experiments in participants with typical development (von Kriegstein et al., 2008, Díaz et al., 2012).&quot; Then in the Discussion section they offer three potential factors that might mediate this surprising outcome.</p><p>However, all of this discussion is building on a null finding. The absence of evidence for a speech task effect is not evidence of absence. Further analysis is necessary to show which of two interpretations is more likely: (1) that this is an underpowered or merely unlucky study, or (2) a true change from the expected positive to a null (or negative?) effect. Positive evidence of a change in the magnitude of the effect could come from a between-experiment comparison (using activation vs rest as a dependent measure to ensure cross-experiment findings are comparable despite differences in field strength, experimental design, etc.). Or they could use a Bayesian or meta-analytic method to determine whether the expected, opposite or null hypothesis is more likely for the current experiment when combined with a prior from previous studies. Without these analyses, though, the three paragraphs of speculation regarding possible causes of a change in experimental outcome are irrelevant. No compelling statistical evidence for a change has been provided.</p><p>2) Correlation with individual differences in speech perception performance – despite the lack of a categorical task effect, the authors use their correlation with behaviour as evidence that: &quot;confirmed our hypothesis that the left first-order thalamic nucleus – vMGB – is involved in speech recognition.&quot; Later in the same paragraph they go on to argue that: &quot;The results can be explained neither by differences in stimulus input in the two conditions, as the same stimuli were heard in both tasks, nor by a correlation with general better task performance, as there was no correlation with the speaker task.&quot;</p><p>Here again, they are arguing for a difference between a positive finding (correlation) and a null finding (no correlation) based on the difference between a p-value less than 0.05 and another above 0.05. This is fallacious. They should statistically compare the two correlations to confirm that there is a difference between the speech and speaker findings in order to confirm that some task-specific, auditory aspect of speech perception, rather than generic attentional, executive or motoric factors (that can also be associated with performance) are linked to these regions of the auditory thalamus.</p><p>Summary: These two stringent criticisms challenge the authors to do more with the data that they have collected. Nonetheless, I think that the work that these authors have done to localise the tonotopic regions of the auditory thalamus has considerable merit (though it is too far outside my main interest for me to assess this contribution in detail). Yet, I still think further statistical evidence is required for them to associate this thalamus region with a single, specifically-auditory aspect of speech perception (and not speaker perception or generic task abilities). I'd like to see the authors do this additional work in order to more convincingly establish the ventral MGB as part of the neural circuitry for speech perception.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.44837.025</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>The work is well executed in seeking changes for a controlled contrast between a speech and non-speech task in vMGB, could fit with a predictive coding account, which would be interesting and novel in applying predictive coding accounts to subcortical structures. The main issue is that more robust support for the conclusion that the MGB is involved in processing speech is required: the reviewers were concerned about the absence of a straightforward main effect for the speech minus non-speech task, the weak association between accuracy scores in the speech task and MGB modulation, and the interpretation of the latter which was clearly not based on an a prior hypothesis. We felt further analyses to achieve this were possible within 2 months hence the decision to ask for a major revision.</p></disp-quote><p>We thank the reviewers for the constructive and very helpful comments. Here, we want to emphasize that both the main effect of task as well as the correlation of the task-dependent modulation with speech recognition behavior were equally weighted a priori hypotheses based on prior findings (von Kriegstein et al., 2008; Diaz et al., 2012) (see detailed reply below). In addition, we were surprised that the association between accuracy scores in the speech tasks and MGB modulation was considered weak. According to Cohen (1988; p.88) the effect would be considered large. We have detailed this our response to reviewer #2’s comments.</p><disp-quote content-type="editor-comment"><p>The individual reviews are appended below, but the major changes required are summarised here by the Reviewing Editor:</p><p>1) Further analyses to combine the present null finding for the main effect of task with previous results as suggested by reviewer 3.</p></disp-quote><p>We thank the reviewers for this great suggestion. We performed meta-analyses for the main effect that included results from three previous experiments (Díaz et al., 2012; von Kriegstein et al., 2008) and results from a recent study (Mihai et al., 2019). The results showed a positive large meta-analytic effect over five studies for the main effect of speech vs non-speech task. Four studies with overlapping confidence intervals showed a positive main effect of task and only the present study falls out of this scheme. Given this result and the large sample size of the present study, it is highly likely that there is a real difference in the main effect of task between the present and the other four studies. We have integrated the new analyses in the manuscript.</p><disp-quote content-type="editor-comment"><p>2) Further analysis to make the correlation between performance and activation in MGB more robust is also required.</p></disp-quote><p>We thank the reviewers for this constructive comment. We performed two additional analyses. In the first analysis, we excluded one data point that exceeded two standard deviations from the mean and recalculated the correlation of the speech vs speaker contrast with the behavioral performance in the speech task across participants. The correlation resulted in very similar values (T=2.92, p=0.038, r=0.47) as the correlation with the outlier included. Thus the correlation is robust to outlier removal and we have now also included it in the manuscript. Second, we revisited the control correlation and found that there is a difference in correlations (correlation 1: [speech vs speaker] correlated with proportion of hits in speech task; correlation 2: [speaker vs speech] correlated with proportion of hits in the speaker task). We however, also found a positive correlation ([speech vs speaker] correlated with proportion of hits in the speaker task). This result can be explained by a correlation in task-performance across the speech and speaker task. Thus, we cannot claim that the vMGB task-dependent modulation (speech vs speaker) is specific to the speech task performance and we have removed instances in the manuscript that might imply such a specificity claim.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>I think the work is well executed and very interesting in seeking changes for a nicely controlled contrast between a speech and non-speech task in vMGB, which I agree could fit with predictive coding account, and which would be interesting in applying predictive coding accounts to subcortical structures which I think is novel in auditory work.</p><p>I had some questions for the authors</p><p>1) I did not actually think the tonotopy data was required but I guess it does make the case they are examining a lemniscal area. I would be interested in whether the authors feel the results are consistent with those in macaque (PMID 21378972). The gradient running anteromedially looks similar to me and I would not expect much difference (in contrast to the speech).</p></disp-quote><p>We thank the reviewer for this suggestion. Also in response to reviewer 2, we now computed the tonotopy for the inferior colliculus (Figure 3—figure supplement 1). We included the following sentence in the manuscript:</p><p>“For completeness we ran the same tonotopy analysis also on the inferior colliculi (IC). This analysis (n=28) revealed a single gradient in the IC similarly to previous reports in the macaque (n=3) (Baumann et al., 2011) and human (n=6; n=5) (De Martino et al., 2013; Moerel et al., 2015) (Figure 3—figure supplement 1).”</p><disp-quote content-type="editor-comment"><p>2) The existence of a significant speech minus non-speech contrast might have fitted a predictive coding account and was clearly predicted by the authors. I found the left vMGB correlation between the [speech vs speaker] contrast and speech recognition performance interesting but rather more nuanced in interpretation and I think that slightly reduces the impact of the work.</p></disp-quote><p>We thank the reviewer for this important comment. We hypothesized from the beginning two points, both of which were based on results from previous studies (Díaz et al., 2012; von Kriegstein et al., 2008): (1) a task-dependent modulation exemplified through the speech – speaker contrast, and (2) a positive correlation between the task-dependent modulation and the speech recognition performance. We weighted the importance of these hypotheses equally. We do not think that the main effect is more valuable than the correlation, because the correlation indicates that the task-dependent modulation is behaviorally relevant – a finding that is expected under a predictive-coding account. Important for the present aim of the paper is that the correlation is present in vMGB – something that previous reports on similar correlations (von Kriegstein et al., 2008) were not able to show based on the lack of spatial resolution. Whether the same can be said from a main effect is unclear from the present study. We have now added explanatory sentences in the Introduction and Discussion. In addition, to avoid misunderstanding, we have separated our result section into one that directly describes the results in relation to our two a-priori hypotheses and one section that contains exploratory analyses.</p><p>In the Introduction:</p><p>“Since our aim of the present paper was to test whether the behaviourally-task dependent modulation of MGB is present in the vMGB, we tested two hypotheses. We hypothesized (i) a higher response to the speech than to the control (speaker) task in the tonotopically organized left and right vMGB, and (ii) a positive correlation between speech recognition performance and the task-dependent modulation for speech in the tonotopically organized left vMGB. Within our design these hypotheses could be addressed by (i) the main effect of task (speech task vs speaker task) in bilateral vMGB and by (ii) a correlation between the contrast speech task vs speaker task with speech recognition performance across participants in left vMGB.”</p><p>In the Discussion:</p><p>“The results are based on the test of two equally weighted hypotheses: a main effect and a correlation. Although we found no significant main effect in the vMGB (nor in any other subregion of the MGB), the correlation between the task-dependent modulation and behavioral performance was, as hypothesized, significant within the left vMGB.”</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…] Major comments: There are less than a handful of studies examining the human auditory thalamus at ultra-high fields-a severe paucity that limits our understanding of thalamic function in processing behaviorally-relevant signals in humans. Within this context, the goals of the present study are laudable. The study convincingly shows tonotopic representation in the vMGB, providing a large-scale replication of Moerel et al., 2015. I'm not at all convinced about the other main thrust of the study related to task modulation/speech recognition. The results show no statistically significant difference in vMGB activity as a function of task. The correlation analyses yield a weak association, but the nature of this association is unclear. What does it mean that modulation in the left vMGB correlates with the speech recognition scores when the task performance in speech vary very little, and there are substantial accuracy differences between the speech and the speaker task? The authors provide three potential explanation for their lack of replication of prior work and seem to suggest that the most likely explanation is that MGB modulation might also have played a role in performing the speaker task…I agree, but this also suggests a critical flaw with the study design which a priori defined task modulation on the basis of a difference in the speech&gt;speaker contrast.</p></disp-quote><p>We thank the reviewer for the positive and constructive comments. We address them in three points:</p><p>a) effect size of the correlation</p><p>The reviewer finds that the correlation analyses show a weak association between task-dependent modulation and speech recognition performance. However, according to Cohen the effect size of the correlation is not weak, but large (Cohen, 1988, p. 80). Cohen describes correlation coefficients (Pearson’s r) in terms of effect sizes and R² values, which are interpreted as the amount of variance explained by the effect. A correlation coefficient of r = 0.46, which Cohen, 1988, deems a large effect in relative terms, results in an R²=0.21. We included the following sentence in the manuscript:</p><p>“The correlation coefficient r=0.46 (R²=0.21) is considered a large effect (Cohen, 1988, p. 80) by explaining 21% of the variance of either variable when linearly associated with the variance in the other.”</p><p>b) meaning of the correlation</p><p>The reviewer asked about the meaning of the correlation (given the difference of 10% between performance on the easier speech task and the more difficult speaker task). The correlation between the task-dependent modulation of the left vMGB and the speech recognition performance across participants means that those participants that displayed a larger task-dependent modulation of left vMGB responses, also achieved a higher amount of hits in the speech task. This indicates that the task-dependent modulation of the left vMGB is behaviourally relevant.</p><p>The reviewer asks whether the task performance between 85% and 95% is biologically relevant. We are convinced that this range is relevant. As an example, a 10% difference in task performance when listening to 1000 syllables would result in 100 more syllables correctly identified (correct hits) for those that were better at the task compared to those that were worse at it. We consider this a relevant figure, particularly for such an important aspect of human communication as speech recognition. Previous publications show similar behavioral ranges for speech recognition (for example (Harris et al., 2009; Panouillères and Möttönen, 2018)) where even a 5-10% difference in speech recognition differentiates the hearing impaired from the normal hearing participants. We have now added a clarification in the manuscript:</p><p>“The correlation indicated that those participants on the lower side of the task-dependent modulation spectrum, as given by the speech vs speaker contrast, have a lower proportion of hits in the speech task and those participants on the higher side of the task-dependent modulation spectrum, show a higher proportion of hits in the speech task and are thus better at speech recognition. […] However a similar study in the auditory modality is so far missing.”</p><p>c) potential flaw in the study design</p><p>We do not consider the use of natural speaker voices a potential flaw in the study design since a large influence of dynamic cues in the speaker task was unexpected. We rather consider the null-finding of the present study for the main effect as a potential to initiate further research. Reviewer 3 prompted us to formally assess via a meta-analysis whether the lack of main effect is really a difference between studies or just a merely unlucky result. The results of the meta-analyses imply that indeed there is a difference in the main effect between studies (see response to reviewer 3). Whether this difference in studies is really due to participants using dynamic speaker cues cannot be answered with the present data, but is a testable hypothesis for future studies.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>[…] 1) Speech &gt; speaker task activation in vMGB – the authors remark that their failure to observe this effect is: &quot;surprising, as this categorical task-effect was observed in three previous experiments in participants with typical development (von Kriegstein et al., 2008, Díaz et al., 2012).&quot; Then in the Discussion section they offer three potential factors that might mediate this surprising outcome.</p><p>However, all of this discussion is building on a null finding. The absence of evidence for a speech task effect is not evidence of absence. Further analysis is necessary to show which of two interpretations is more likely: (1) that this is an underpowered or merely unlucky study, or (2) a true change from the expected positive to a null (or negative?) effect. Positive evidence of a change in the magnitude of the effect could come from a between-experiment comparison (using activation vs rest as a dependent measure to ensure cross-experiment findings are comparable despite differences in field strength, experimental design, etc.). Or they could use a Bayesian or meta-analytic method to determine whether the expected, opposite or null hypothesis is more likely for the current experiment when combined with a prior from previous studies. Without these analyses, though, the three paragraphs of speculation regarding possible causes of a change in experimental outcome are irrelevant. No compelling statistical evidence for a change has been provided.</p></disp-quote><p>We thank the reviewer for this important and helpful comment. We have now conducted meta-analyses (for details see below), and found a net large positive effect size for the task-dependent modulation in MGB over experiments (Figure 8 and Figure 8—figure supplement 1). The sample size of the current experiment is approximately twice as large (n=33) as the other studies included in the meta-analysis (n = [17, 16, 14, 17]), and we hence are convinced that the study was adequately powered. Four studies with overlapping confidence intervals show a positive main effect of task and only the present study falls out of this scheme. We therefore conclude that the difference between the four other studies and the present one in task-dependent modulation is real.</p><p>We have now included the analysis in the Materials and methods section:</p><p>“Meta-analysis on the main effect of task (speech vs speaker task contrast)</p><p>The lack of statistical significance for the speech vs speaker contrast raised the question whether the overall effect is different from the ones reported previously (Diaz et al., 2012; von Kriegstein et al., 2008). […] Effect sizes and standard errors were entered into a random effects model which was estimated with maximum likelihood using JASP 0.9 (jasp-stats.org).”</p><p>In the Results section we added:</p><p>“We performed a random effects meta-analysis to test whether the (non-significant) effect of the main effect of task in the present study (i.e., speech vs speaker task contrast) was different from other studies that have reported a significant task-dependent MGB modulation for speech. […] We detail potential reasons for this difference in the task-dependent modulation between the studies in the discussion.”</p><p>In the Discussion section we added:</p><p>“The lack of a significant main effect of task (speech vs speaker) in the vMGB was surprising, and the meta-analyses showed that the null-finding was indeed different from categorical task-effects (speech vs loudness tasks and speech vs speaker tasks) observed in other experiments in participants with typical development (Díaz et al., 2012; Mihai et al., 2019; von Kriegstein et al., 2008).”</p><disp-quote content-type="editor-comment"><p>2) Correlation with individual differences in speech perception performance – despite the lack of a categorical task effect, the authors use their correlation with behaviour as evidence that: &quot;confirmed our hypothesis that the left first-order thalamic nucleus – vMGB – is involved in speech recognition.&quot; Later in the same paragraph they go on to argue that: &quot;The results can be explained neither by differences in stimulus input in the two conditions, as the same stimuli were heard in both tasks, nor by a correlation with general better task performance, as there was no correlation with the speaker task.&quot;</p><p>Here again, they are arguing for a difference between a positive finding (correlation) and a null finding (no correlation) based on the difference between a p-value less than 0.05 and another above 0.05. This is fallacious. They should statistically compare the two correlations to confirm that there is a difference between the speech and speaker findings in order to confirm that some task-specific, auditory aspect of speech perception, rather than generic attentional, executive or motoric factors (that can also be associated with performance) are linked to these regions of the auditory thalamus.</p></disp-quote><p>We thank the reviewer for prompting us to reevaluate the control correlation with the speaker task and to compute differences between the correlations. We found a mistake in the previous control correlation analysis, which has now been corrected. The new results do not permit to claim that the behaviorally relevant task-dependent modulation is specific to the speech task and we have toned down our claims on this. Unfortunately, it is impossible to test the specificity for the task-dependent modulation for speech with the present data set. For the visual modality (and the lateral geniculate body, LGN) we have previously found first evidence for the specificity of task-dependent modulation for speech (Diaz et al., 2018). However, such an experiment still needs to be done for the auditory modality. For the current paper, the main goal was to test whether the correlation between task-dependent modulation and speech-task performance is present in vMGB – something that previous reports on similar correlations (von Kriegstein et al., 2008) were not able to show.</p><p>We now write on:</p><p>“Specificity of the behaviorally relevant task-dependent modulation for speech. In exploratory control analyses we checked whether we could test for a specificity of the correlation between the task-dependent modulation for speech (i.e., the speech task vs speaker task contrast) and the speech recognition behavior across participants. […] The results indicated that although there is a behaviorally relevant task-dependent modulation in the vMGB for speech, we currently do not know whether it is specific to speech recognition abilities.”</p><p>Discussion:</p><p>“In the present study, the results cannot be explained by differences in stimulus input in the two conditions, as the same stimuli were heard in both tasks. […] However a similar study in the auditory modality is so far missing.”</p><p>Additionally, since a lack of behaviourally relevant task-dependent modulation of the other MGB subsection could also be due to just thresholding we now included an exploratory analysis to confirm the specificity of vMGB involvement:</p><p>“Specificity of the behaviorally relevant task-dependent modulation to the vMGB. […]Thus this is a first indication that the behaviourally relevant task-dependent modulation for speech is specific to the left vMGB.”</p><disp-quote content-type="editor-comment"><p>Summary: These two stringent criticisms challenge the authors to do more with the data that they have collected. Nonetheless, I think that the work that these authors have done to localise the tonotopic regions of the auditory thalamus has considerable merit (though it is too far outside my main interest for me to assess this contribution in detail). Yet, I still think further statistical evidence is required for them to associate this thalamus region with a single, specifically-auditory aspects of speech perception (and not speaker perception or generic task abilities). I'd like to see the authors do this additional work in order to more convincingly establish the ventral MGB as part of the neural circuitry for speech perception.</p></disp-quote><p>We thank the reviewer for the encouraging remarks and helpful criticisms and believe that addressing them has made our paper stronger. The additional analyses supported our main conclusions and provided the following key points:</p><p>· The meta-analyses showed an overall positive effect for the Speech – control task contrast across studies in the MGB,</p><p>· the effect of the speech – control task contrast in the current study was different from the other experiments testing the same (or a similar) contrast,</p><p>· the correlation between task-dependent modulation and the speech recognition performance across participants in the vMGB was robust against outliers and was present only in vMGB but not in other MGB subsections</p><p>· the difference between the two brain-behavior correlations (task-dependent modulation correlated with speech behavioral score and task-dependent modulation correlated with speaker behavioral score) was not significant and we have therefore toned down our claims on speech specificity.</p><p>References</p><p>Baumann S, Griffiths TD, Sun L, Petkov CI, Thiele A, Rees A. 2011. Orthogonal representation of sound dimensions in the primate midbrain. <italic>Nature Neuroscience</italic> 14:423–425. doi:10.1038/nn.2771</p><p>Cohen J. 1988. Statistical Power Analysis for the Behavioral Sciences, 2nd ed. Lawrence Erlbaum Associates. doi:10.1016/C2013-0-10517-X</p><p>Davis MH, Johnsrude IS. 2003. Hierarchical Processing in Spoken Language Comprehension. <italic>The Journal of Neuroscience</italic> 23:3423–3431. doi:10.1523/JNEUROSCI.23-08-03423.2003</p><p>De Martino F, Moerel M, van de Moortele P-F, Ugurbil K, Goebel R, Yacoub E, Formisano E. 2013. Spatial organization of frequency preference and selectivity in the human inferior colliculus. <italic>Nature Communications</italic> 4:1386. doi:10.1038/ncomms2379</p><p>Díaz B, Hintz F, Kiebel SJ, Kriegstein K von. 2012. Dysfunction of the auditory thalamus in developmental dyslexia. <italic>PNAS</italic> 109:13841–13846. doi:10.1073/pnas.1119828109</p><p>Gordon-Salant S, Fitzgibbons PJ. 1993. Temporal Factors and Speech Recognition Performance in Young and Elderly Listeners. <italic>Journal of Speech, Language, and Hearing Research</italic> 36:1276–1285. doi:10.1044/jshr.3606.1276</p><p>Harris KC, Dubno JR, Keren NI, Ahlstrom JB, Eckert MA. 2009. Speech Recognition in Younger and Older Adults: A Dependency on Low-Level Auditory Cortex. <italic>J Neurosci</italic> 29:6078–6087. doi:10.1523/JNEUROSCI.0412-09.2009</p><p>Huth AG, de Heer WA, Griffiths TL, Theunissen FE, Gallant JL. 2016. Natural speech reveals the semantic maps that tile human cerebral cortex. <italic>Nature</italic> 532:453–458. doi:10.1038/nature17637</p><p>Mihai PG, Tschentscher N, Kriegstein K von. 2019. The role of the ventral MGB in speech in noise comprehension. <italic>bioRxiv</italic> 646570. doi:10.1101/646570</p><p>Moerel M, De Martino F, Uğurbil K, Yacoub E, Formisano E. 2015. Processing of frequency and location in human subcortical auditory structures. <italic>Sci Rep</italic> 5. doi:10.1038/srep17048</p><p>Panouillères MTN, Möttönen R. 2018. Decline of auditory-motor speech processing in older adults with hearing loss. <italic>Neurobiology of Aging</italic> 72:89–97. doi:10.1016/j.neurobiolaging.2018.07.013</p><p>Price C, Thierry G, Griffiths T. 2005. Speech-specific auditory processing: where is it? <italic>Trends in Cognitive Sciences</italic> 9:271–276. doi:10.1016/j.tics.2005.03.009</p><p>Schuirmann DJ. 1987. A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability. <italic>Journal of pharmacokinetics and biopharmaceutics</italic> 15:657–680.</p><p>von Kriegstein K, Patterson RD, Griffiths TD. 2008. Task-Dependent Modulation of Medial Geniculate Body Is Behaviorally Relevant for Speech Recognition. <italic>Current Biology</italic> 18:1855–1859. doi:10.1016/j.cub.2008.10.052</p></body></sub-article></article>