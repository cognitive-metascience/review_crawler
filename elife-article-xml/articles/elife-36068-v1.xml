<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">36068</article-id><article-id pub-id-type="doi">10.7554/eLife.36068</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Content-specific activity in frontoparietal and default-mode networks during prior-guided visual perception</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-108329"><name><surname>González-García</surname><given-names>Carlos</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6627-5777</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-108333"><name><surname>Flounders</surname><given-names>Matthew W</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-108334"><name><surname>Chang</surname><given-names>Raymond</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-108335"><name><surname>Baria</surname><given-names>Alexis T</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-71932"><name><surname>He</surname><given-names>Biyu J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1549-1351</contrib-id><email>biyu.jade.he@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">National Institute of Neurological Disorders and Stroke</institution><institution>National Institutes of Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Experimental Psychology</institution><institution>Ghent University</institution><addr-line><named-content content-type="city">Ghent</named-content></addr-line><country>Belgium</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Neuroscience Institute</institution><institution>New York University Langone Medical Center</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Departments of Neurology</institution><institution>New York University Langone Medical Center</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Departments of Neuroscience and Physiology</institution><institution>New York University Langone Medical Center</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Departments of Radiology</institution><institution>New York University Langone Medical Center</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Marder</surname><given-names>Eve</given-names></name><role>Senior Editor</role><aff><institution>Brandeis University</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Culham</surname><given-names>Jody C</given-names></name><role>Reviewing Editor</role><aff><institution>University of Western Ontario</institution><country>Canada</country></aff></contrib></contrib-group><author-notes><fn fn-type="equal" id="fn1"><p><sup>#</sup> These authors contributed equally.</p></fn><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>31</day><month>07</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e36068</elocation-id><history><date date-type="received" iso-8601-date="2018-02-19"><day>19</day><month>02</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2018-07-10"><day>10</day><month>07</month><year>2018</year></date></history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-36068-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.36068.001</object-id><p>How prior knowledge shapes perceptual processing across the human brain, particularly in the frontoparietal (FPN) and default-mode (DMN) networks, remains unknown. Using ultra-high-field (7T) functional magnetic resonance imaging (fMRI), we elucidated the effects that the acquisition of prior knowledge has on perceptual processing across the brain. We observed that prior knowledge significantly impacted neural representations in the FPN and DMN, rendering responses to individual visual images more distinct from each other, and more similar to the image-specific prior. In addition, neural representations were structured in a hierarchy that remained stable across perceptual conditions, with early visual areas and DMN anchored at the two extremes. Two large-scale cortical gradients occur along this hierarchy: first, dimensionality of the neural representational space increased along the hierarchy; second, prior’s impact on neural representations was greater in higher-order areas. These results reveal extensive and graded influences of prior knowledge on perceptual processing across the brain.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual perception</kwd><kwd>prior experience</kwd><kwd>default-mode network</kwd><kwd>frontoparietal network</kwd><kwd>7T fMRI</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007027</institution-id><institution>Leon Levy Foundation</institution></institution-wrap></funding-source><award-id>Leon Levy Neuroscience Fellowship</award-id><principal-award-recipient><name><surname>He</surname><given-names>Biyu J</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001207</institution-id><institution>Esther A. and Joseph Klingenstein Fund</institution></institution-wrap></funding-source><award-id>Klingenstein-Simons Neuroscience Fellowship</award-id><principal-award-recipient><name><surname>He</surname><given-names>Biyu J</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010629</institution-id><institution>Fulbright Association</institution></institution-wrap></funding-source><award-id>The Fulbright Program</award-id><principal-award-recipient><name><surname>González-García</surname><given-names>Carlos</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>Intramural Research Program</award-id><principal-award-recipient><name><surname>He</surname><given-names>Biyu J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Prior experience alters content-specific neural representations of visual input in frontoparietal and default-mode networks.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Prior experiences can have an enormous impact on perception. For example, a harmless rope on the trail may be perceived as danger and trigger a jump in fright if one has had a recent encounter with a snake. Although it is well established that perception arises as a consequence of a continuous interaction between incoming sensory input and internal priors (<xref ref-type="bibr" rid="bib4">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="bib39">Mumford, 1992</xref>; <xref ref-type="bibr" rid="bib61">Yuille and Kersten, 2006</xref>; <xref ref-type="bibr" rid="bib1">Albright, 2012</xref>; <xref ref-type="bibr" rid="bib53">Summerfield and de Lange, 2014</xref>; <xref ref-type="bibr" rid="bib56">Trapp and Bar, 2015</xref>), the neural mechanisms underlying this process remain unclear. In particular, while extensive research has focused on how prior experience and knowledge shape neural processing in early sensory areas (e.g., [<xref ref-type="bibr" rid="bib29">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Schlack and Albright, 2007</xref>; <xref ref-type="bibr" rid="bib2">Alink et al., 2010</xref>; <xref ref-type="bibr" rid="bib24">Hsieh et al., 2010</xref>]), whether higher-order frontoparietal regions contain content-specific neural representations that are involved in prior-guided perceptual processing remains unknown.</p><p>Here, we test whether content-specific neural activity exists during prior-guided visual processing in the default-mode network (DMN) (<xref ref-type="bibr" rid="bib43">Raichle et al., 2001</xref>) and frontoparietal network (FPN) (<xref ref-type="bibr" rid="bib14">Dosenbach et al., 2008</xref>). Classic theories suggest that the DMN is involved in internally oriented processes (<xref ref-type="bibr" rid="bib7">Buckner et al., 2008</xref>), such as self-related processing (<xref ref-type="bibr" rid="bib27">Kelley et al., 2002</xref>), spontaneous/task irrelevant thought (<xref ref-type="bibr" rid="bib37">Mason et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Christoff et al., 2009</xref>), episodic and semantic memory (<xref ref-type="bibr" rid="bib46">Shapira-Lichter et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Binder et al., 2009</xref>; <xref ref-type="bibr" rid="bib45">Sestieri et al., 2011</xref>). This idea is challenged by recent studies showing DMN activation in externally oriented tasks, including executive control and working memory tasks (<xref ref-type="bibr" rid="bib11">Crittenden et al., 2015</xref>; <xref ref-type="bibr" rid="bib31">Konishi et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Vatansever et al., 2015</xref>). These results have inspired a recent framework suggesting that the DMN, being positioned farthest away from uni-modal sensory and motor areas in both functional connectivity and anatomical space, allows online information processing to be guided by stored representations (<xref ref-type="bibr" rid="bib36">Margulies et al., 2016</xref>). However, to date evidence for content-specific activity in the DMN during externally-oriented tasks is still lacking, which would be crucial for establishing this network’s role in specific computations linking online processing with stored representations. We reasoned that prior-guided sensory processing, which requires using previously acquired priors (broadly defined) to guide online perceptual processing, is an ideal context for investigating this possibility. In particular, the role of DMN in prior-guided visual perception remains controversial (<xref ref-type="bibr" rid="bib13">Dolan et al., 1997</xref>; <xref ref-type="bibr" rid="bib20">Gorlin et al., 2012</xref>).</p><p>Whether the FPN is involved in content-specific perceptual processing is also unclear. One framework suggests that this network only encodes task/response-relevant information (<xref ref-type="bibr" rid="bib16">Erez and Duncan, 2015</xref>; <xref ref-type="bibr" rid="bib6">Bracci et al., 2017</xref>), such as learnt arbitrary categories of visual objects that are relevant to performance (<xref ref-type="bibr" rid="bib17">Freedman et al., 2001</xref>) or the distinction between a target and a non-target (<xref ref-type="bibr" rid="bib16">Erez and Duncan, 2015</xref>). On the other hand, neural activity reflecting perceptual content has been observed in both the lateral prefrontal cortex (LPFC)(<xref ref-type="bibr" rid="bib60">Wang et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Panagiotaropoulos et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">Mendoza-Halliday and Martinez-Trujillo, 2017</xref>) and posterior-parietal cortex (PPC)(<xref ref-type="bibr" rid="bib26">Jeong and Xu, 2016</xref>; <xref ref-type="bibr" rid="bib30">Konen and Kastner, 2008</xref>; <xref ref-type="bibr" rid="bib18">Freud et al., 2016</xref>) components of this network, even when task demand is held constant or was irrelevant to the perceptual content (e.g., [<xref ref-type="bibr" rid="bib41">Panagiotaropoulos et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">Mendoza-Halliday and Martinez-Trujillo, 2017</xref>; <xref ref-type="bibr" rid="bib30">Konen and Kastner, 2008</xref>]). Currently, whether the LPFC contributes to visual perceptual processing is a heavily debated topic (<xref ref-type="bibr" rid="bib12">Dehaene et al., 2017</xref>; <xref ref-type="bibr" rid="bib28">Koch et al., 2016</xref>). Inspired by the observation that in anatomical and functional connectivity space, the FPN is situated in between the DMN and sensory areas (<xref ref-type="bibr" rid="bib36">Margulies et al., 2016</xref>), we reasoned that the FPN may exhibit an intermediate pattern of effects compared to the DMN and sensory areas during prior-guided perceptual processing.</p><p>In the laboratory, Mooney images provide a well-controlled paradigm for studying how prior experience shapes perceptual processing. Mooney images are black-and-white degraded images that are difficult to recognize at first. Yet, once the subject is exposed to the original, non-degraded grayscale image (a process called ‘disambiguation’), their recognition of the corresponding Mooney image becomes effortless, and this effect can last for days to months, even a lifetime (<xref ref-type="bibr" rid="bib1">Albright, 2012</xref>; <xref ref-type="bibr" rid="bib35">Ludmer et al., 2011</xref>). This phenomenon demonstrates that experience can shape perception in a remarkably fast and robust manner.</p><p>The Mooney images paradigm thus allows comparing neural processing of a repeatedly presented, physically identical image that leads to distinct perceptual outcomes depending on whether a prior due to earlier experience is present in the brain. Recent studies have shown that after disambiguation, neural activity patterns elicited by Mooney images in early and category-selective visual regions become more distinct between different images and more similar to those elicited by their matching grayscale counterparts (<xref ref-type="bibr" rid="bib24">Hsieh et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Gorlin et al., 2012</xref>; <xref ref-type="bibr" rid="bib58">van Loon et al., 2016</xref>). However, how prior experience impacts neural processing in higher-order frontoparietal cortices remains largely unknown. In this study, we investigated neural processing of Mooney images before and after disambiguation across the brain, from retinotopic and category-selective visual areas to FPN and DMN, using ultra-high-field (7T) fMRI and multivariate pattern analyses. We hypothesized that the acquisition of perceptual priors would alter neural representations and enhance content-specific information in DMN and FPN regions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Task paradigm and behavioral results</title><p>Nineteen subjects were shown 33 Mooney images, 17 of which contained animals, and 16 contained manmade objects. Each Mooney image was presented six times before its corresponding gray-scale image was shown to the subject, and six times after. Following each Mooney image presentation, subjects responded to the question ‘Can you recognize and name the object in the image?’ using a button press (‘subjective recognition’). Each fMRI run included three distinct gray-scale images, their corresponding post-disambiguation Mooney images, and three new Mooney images shown pre-disambiguation (their corresponding gray-scale images would be shown in the next run), with randomized order within each stage such that a gray-scale image was rarely followed immediately by its corresponding Mooney image (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; for details, see Materials and methods, <italic>Task paradigm</italic>). To ensure that subjects’ self-reported recognition matched the true content of the Mooney images, at the end of each run, Mooney images presented during that run were shown again and participants were asked to verbally report what they saw in the image and were allowed to answer ‘unknown’. This resulted in a verbal test for each Mooney image once before disambiguation and once after disambiguation (‘verbal identification’). Verbal responses were scored as correct or incorrect using a pre-determined list of acceptable responses for each image.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.36068.002</object-id><label>Figure 1.</label><caption><title>Paradigm and behavioral results.</title><p>(<bold>A</bold>) Task design, and flow of events at trial, block, and fMRI run level. Subjects viewed gray-scale and Mooney images and were instructed to respond to the question ‘Can you recognize and name the object in the image?”. Each block included three gray-scale images, three money images corresponding to these gray-scale images (i.e., post-disambiguation Mooney images), and three Mooney images unrelated to the gray-scale images (i.e. pre-disambiguation Mooney images, as their corresponding gray-scale images would be presented in the following block). 33 unique images were used and each was presented six times before and six times after disambiguation (see Materials and methods for details). (<bold>B</bold>) Left: Percentage of ‘recognized’ answers across all Mooney image presentations before and after disambiguation. These two percentages significantly differed from each other (p=3.4e-13). Right: Percentage of correctly identified Mooney images before and after disambiguation. These two percentages significantly differed from each other (p=1.7e-15). (<bold>C</bold>) Recognition rate for Mooney images sorted by presentation number, for the pre- and post-disambiguation period, respectively. A repeated-measures ANOVA revealed significant effects of the condition (p=3.4e-13), the presentation number (p=0.002), and the interaction of the two factors (p=0.001). (<bold>D</bold>) Distribution of recognition rate across 33 Mooney images pre- (left) and post- (right) disambiguation. Dashed boxes depict the cut-offs used to classify an image as recognized or not-recognized. All error bars denote s.e.m. across subjects.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig1-v1"/></fig><p>Disambiguation by viewing the gray-scale images had a substantial effect on participants’ subjective recognition responses, with significantly higher rate of recognition for Mooney images presented post-disambiguation (86 ± 1%; mean ± s.d. across subjects) compared to the same images presented before disambiguation (43 ± 12%; <italic>t</italic><sub>1,18</sub> = 18.6, p=3.4e-13, Cohen’s d = 4.2; <xref ref-type="fig" rid="fig1">Figure 1B</xref>, left). A similar pattern of results was observed using the verbal identification responses. Mooney images were correctly identified significantly more often after disambiguation (86 ± 0.8%) than before (34 ± 12%; <italic>t</italic><sub>1,18</sub> = 25.3, p=1.7e-15, Cohen’s d = 5.8; <xref ref-type="fig" rid="fig1">Figure 1B</xref>, right).</p><p>A two-way ANOVA on recognition rate with presentation number (<xref ref-type="bibr" rid="bib4">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="bib39">Mumford, 1992</xref>; <xref ref-type="bibr" rid="bib61">Yuille and Kersten, 2006</xref>; <xref ref-type="bibr" rid="bib1">Albright, 2012</xref>; <xref ref-type="bibr" rid="bib53">Summerfield and de Lange, 2014</xref>; <xref ref-type="bibr" rid="bib56">Trapp and Bar, 2015</xref>) and disambiguation stage (pre- vs. post-) as factors revealed that disambiguation had a dramatic effect on recognition rate (<italic>F</italic><sub>1,18</sub> = 345.6, p=3.4e-13, η<sup>2</sup><sub>p</sub> =.95, while repetition also improved recognition (<italic>F</italic><sub>5,90</sub> = 8.1, p=0.002, η<sup>2</sup><sub>p</sub> = 0.3) (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). The interaction between these two factors was significant (<italic>F</italic><sub>5,90</sub> = 7.5, p=0.001, η<sup>2</sup><sub>p</sub> = 0.29). A post-hoc analysis (Bonferroni-corrected) suggested that the repetition effect was driven by pre-disambiguation images, with recognition rate increasing gradually across presentations before disambiguation (p&lt;0.05), but not after disambiguation (p=1). For instance, for pre-disambiguation images, the recognition rate was significantly higher in the sixth presentation (47 ± 14%, mean ± s.d. across subjects) compared to the first presentation (36 ± 13%; paired t-test across subjects, <italic>t</italic><sub>1,18</sub> = 4.2, p=0.007, Cohen’s d = 0.97), suggesting that participants did not stop attending after being unable to recognize Mooney images at first, and actively tried to recognize these in subsequent presentations.</p><p>Based on the bimodal distribution of subjective recognition rates across images (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), we established cut-offs to select ‘pre-(disambiguation) not-recognized’ images as those recognized two or fewer times before disambiguation, ‘pre-(disambiguation) recognized’ images as those recognized at least 4 out of 6 times before disambiguation, and ‘post-(disambiguation) recognized’ images as those recognized four or more times after disambiguation. Accuracy of verbal identification responses in these three groups were 8.7 ± 5.8%, 70.6 ± 16.0%, 93.6 ± 4.5% (mean ± s.d. across subjects), respectively. Due to the low number of images, the category ‘post-(disambiguation) not-recognized’ was not included in further analyses. Results described below using subjective recognition reports based on these cut-offs are very similar to those obtained using correct vs. incorrect verbal identification responses.</p></sec><sec id="s2-2"><title>Disambiguation leads to widespread changes in neural activity</title><p>We first investigated which brain regions encode the <italic>status</italic> of a Mooney image (pre- vs. post- disambiguation) in their overall activation magnitudes or voxel-wise activity patterns, using general linear model (GLM) and multivariate pattern analysis (MVPA), respectively. For this analysis we selected, for each subject, the set of Mooney images that were both unrecognized in the pre-disambiguation stage and recognized in the post-disambiguation stage. First, consistent with earlier studies (<xref ref-type="bibr" rid="bib13">Dolan et al., 1997</xref>; <xref ref-type="bibr" rid="bib20">Gorlin et al., 2012</xref>), we found that disambiguated Mooney images elicited significantly higher activity in regions of the DMN, including the posterior cingulate cortex (PCC), lateral parietal cortices (Par Lat), and the medial prefrontal cortex (MPFC) than the same images presented pre-disambiguation (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, p&lt;0.05, FWE-corrected; results using verbal identification responses are similar and presented in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). A more detailed analysis revealed that this effect resulted from a reduced deactivation in the post-disambiguation period (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, p&lt;0.05, FWE-corrected). Using a searchlight decoding analysis across the whole brain, we found that information about the status of a Mooney image was contained in the voxel-wise activity pattern in an extensive set of brain regions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, p&lt;0.01, corrected by cluster-based permutation test; results using verbal identification responses are similar and presented in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), comprising mainly the FPN (<xref ref-type="bibr" rid="bib14">Dosenbach et al., 2008</xref>), but also including the anterior cingulate cortex (ACC), bilateral anterior insulae, fusiform gyri (FG), and components of the DMN.</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.36068.003</object-id><label>Figure 2.</label><caption><title>Disambiguation-induced changes in neural activity magnitude and pattern.</title><p>(<bold>A</bold>) GLM contrast related to the disambiguation effect. For each subject, the set of Mooney images that were not recognized in the pre-disambiguation period and recognized in the post-disambiguation period are used. Thus, the GLM contrast is between an identical set of images that elicit distinct perceptual outcomes. Warm colors show regions with significantly higher activity magnitudes after than before disambiguation (p&lt;0.05, FWE-corrected). (<bold>B</bold>) Activation and deactivation maps for each condition separately (p&lt;0.05, FWE-corrected). Top row: Activation/deactivation map corresponding to pre-disambiguation, not recognized Mooney images, as compared to baseline. Bottom row: post-disambiguation, recognized Mooney images. (<bold>C</bold>) Searchlight decoding of Mooney image status (pre-disambiguation not-recognized vs. post-disambiguation recognized). For each subject, the same set of Mooney images are included in both conditions. Results are shown at p&lt;0.01, corrected level (cluster-based permutation test).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig2-v1"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36068.004</object-id><label>Figure 2—figure supplement 1.</label><caption><title>GLM results of the <italic>post-disambiguation identified &gt;pre disambiguation not-identified</italic> contrast, based on the verbal identification responses.</title><p>For each subject, the same set of Mooney images are included in both conditions. Results were corrected for multiple comparisons using a cluster-defining threshold of z &gt; 3.1, cluster size ≥17 voxels, corresponding to p&lt;0.05, FWE-corrected (for details see Materials and methods, <italic>GLM analysis</italic>).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig2-figsupp1-v1"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36068.005</object-id><label>Figure 2—figure supplement 2.</label><caption><title>Searchlight decoding of <italic>pre-disambiguation not-identified</italic> vs <italic>post-disambiguation identified</italic> Mooney images, selected based on verbal identification responses.</title><p>For each subject, the same set of images are included in both conditions. Results were corrected for multiple comparisons using a cluster-based permutation test and thresholded at a p&lt;0.01, corrected level (for details see Materials and methods, <italic>MVPA</italic>).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig2-figsupp2-v1"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36068.006</object-id><label>Figure 2—figure supplement 3.</label><caption><title>Regions of interest (ROIs) used in RSA.</title><p>(<bold>A</bold>) The FPN included the following ROIs: right frontal (14427 voxels, vx), left frontal (13383 vx), right parietal (3381 vx) and left parietal (3949 vx). The DMN included: MPFC (1135 vx), PCC (2145 vx), right lateral parietal (1469 vx) and left lateral parietal (1361 vx). Category-selective regions included the right (4572 vx) and left (4176 vx) fusiform gyrus (FG), and the right (744 vx) and left (1195 vx) lateral occipital complex (LOC). FPN ROIs were defined at the population level first and then registered back to each subject’s native space. FG ROIs were extracted from a structural atlas and registered back to each subject’s native space as well. DMN and LOC ROIs were individually defined based on each subject’s estimate of relevant GLM contrasts (see Materials and methods, <italic>ROI definition</italic>). (<bold>B</bold>) Example of single-subject early visual ROIs (right hemisphere shown). Early visual ROIs included right V1 (577 vx), left V1 (550 vx), right V2 (637 vx), left V2 (564 vx), right V3 (540 vx), left V3 (477 vx), right V4 (256 vx) and left V4 (198 vx). A retinotopic functional localizer was performed to delineate these ROIs in each subject’s native space. All voxel (vx) counts reported in parentheses were calculated in each subject’s native space, and then averaged across subjects.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig2-figsupp3-v1"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36068.007</object-id><label>Figure 2—figure supplement 4.</label><caption><title>The definition of FPN ROIs from image status decoding results (for details, see Materials and methods, <italic>ROI definition</italic>).</title><p>(<bold>A</bold>) Searchlight decoding of Mooney image status, reproduced from <xref ref-type="fig" rid="fig2">Figure 2C</xref> on inflated cortical surface. (<bold>B</bold>) The map from A is overlaid with the FPN and DMN ROIs used in this study. These ROIs are also shown in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>. (<bold>C</bold>) FPN and DMN ROIs reported in <xref ref-type="bibr" rid="bib42">Power et al. (2011)</xref>.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig2-figsupp4-v1"/></fig></fig-group><p>There are several alternative interpretations of the above findings. The disambiguation-related activation contrast found in DMN regions could reflect perceptual priors present following disambiguation, as previously suggested (<xref ref-type="bibr" rid="bib13">Dolan et al., 1997</xref>). Alternatively, these regions may lack encoding of image-specific information, and the changes in their activity magnitudes may reflect nonspecific effects such as increased arousal (<xref ref-type="bibr" rid="bib20">Gorlin et al., 2012</xref>) or decreased task difficulty (<xref ref-type="bibr" rid="bib49">Singh and Fawcett, 2008</xref>) in the post-disambiguation period. Similarly, regions showing significant decoding of the disambiguation effect may respond to these nonspecific effects or, alternatively, contain content-specific activity related to the perceptual processing of the images. To adjudicate between these alternatives and determine whether DMN and FPN contain content-specific neural representations, we extracted four regions of interests (ROIs) from the GLM result shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, including MPFC, PCC, and the left and right lateral parietal cortices (Par Lat), and four ROIs that represent key components of the FPN (bilateral frontal and parietal cortices) from the decoding result shown in <xref ref-type="fig" rid="fig2">Figure 2C</xref>. For each ROI, we used representational similarity analysis (RSA) to probe the neural representation format of Mooney images before and after disambiguation. We further defined retinotopic visual ROIs (from V1 to V4), lateral occipital complex (LOC) and fusiform gyrus (FG) for each subject using separate localizers to serve as a comparison to frontoparietal areas (for ROI definitions and replication with independent FPN and DMN ROIs, see Materials and methods, <italic>ROI definition</italic>, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, and <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>).</p></sec><sec id="s2-3"><title>Mooney images are encoded more distinctly after disambiguation throughout the cortical hierarchy</title><p>RSA sheds light onto the neural representational format by quantifying similarities or differences between neural representations of different images, or of the same image in different conditions (<xref ref-type="bibr" rid="bib32">Kriegeskorte et al., 2008</xref>). For each subject and ROI, we obtained a representational dissimilarity matrix (RDM) that contained the correlational distance (1 – Pearson’s r) between every pair of images (<xref ref-type="fig" rid="fig3">Figure 3A</xref>; the RDM is a symmetrical matrix). Thirty-three Mooney images shown in the pre- and post-disambiguation period, and their grayscale counterparts, were arranged along the x and y axis of the matrix (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Correlational distance was then computed between every pair of images that were either in the same condition (green triangles in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, top-right panel), or in different conditions (yellow squares in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, bottom-right panel).</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.36068.008</object-id><label>Figure 3.</label><caption><title>Neural representation format of individual images in visual regions.</title><p>(<bold>A</bold>) Analysis schematic. Dissimilarity (1 – Pearson’s r) between the neural response patterns to pairs of images was computed to construct the representational dissimilarity matrix (RDM) for each ROI. Two statistical analyses were performed: <italic>First</italic> (top-right panel, ‘within-condition’), mean dissimilarity across image pairs was calculated for each condition (green triangles), and compared between conditions (brackets). Cyan bracket highlights the main effect of interest (disambiguation). <italic>Second</italic> (bottom-right panel, ‘between-condition’), the mean of between-condition diagonals (green lines) was compared. For ease of interpretation, this analysis was carried out on the representational similarity matrix (RSM). Each element in the diagonal represents the neural similarity between the same Mooney image presented in different stages (Pre-Post), or between a Mooney image and its corresponding gray-scale image (Pre-Gray and Post-Gray). (<bold>B</bold>) Group-average RDMs for V1, V2, V3, LOC and FG ROIs in the right hemisphere. Black lines delimit boundaries of each condition. Within each condition, natural (‘nat’) and man-made (‘man’) images are grouped together. (<bold>C</bold>) 2-D MDS plots corresponding to the RDMs in B. Pre-disambiguation, post-disambiguation, and gray-scale images are shown as blue, yellow-green, and pink-red dots, respectively. (<bold>D</bold>) Mean within-condition representational dissimilarity between different images for each ROI, corresponding to the ‘within-condition’ analysis depicted in A. (<bold>E</bold>) Mean between-condition similarity for the same or corresponding images for each ROI, corresponding to the ‘between-condition’ analysis depicted in A. In D and E, asterisks denote significant differences (p&lt;0.05, Wilcoxon signed-rank test, FDR-corrected), and error bars denote s.e.m. across subjects. Results from V4 are shown in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>. Interactive 3-dimensional MDS plots corresponding to first-order RDMs for each ROI can be found at: <ext-link ext-link-type="uri" xlink:href="https://gonzalezgarcia.github.io/mds.html">https://gonzalezgarcia.github.io/mds.html</ext-link>.</p><p><supplementary-material id="fig3sdata1"><object-id pub-id-type="doi">10.7554/eLife.36068.011</object-id><label>Figure 3—source data 1.</label><caption><title>RDM for each ROI in each subject. Includes source code to perform statistical analysis and produce <xref ref-type="fig" rid="fig3">Figure 3 and 4</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-36068-fig3-data1-v1.zip"/></supplementary-material> </p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig3-v1"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36068.009</object-id><label>Figure 3—figure supplement 1.</label><caption><title>RSA results for left hemisphere visual regions.</title><p>Same as <xref ref-type="fig" rid="fig3">Figure 3B–E</xref>, but for left hemisphere ROIs.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig3-figsupp1-v1"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36068.010</object-id><label>Figure 3—figure supplement 2.</label><caption><title>RSA results for the right and left V4 (left and middle column, respectively), and the corresponding ROI size control analysis (right column).</title><p>The size of V4 ROIs (right: 256 vx; left: 198 vx) was much smaller than V1-V3 ROIs. Right column: We merged the left and right V4 ROIs for each subject, which yielded ROIs of approximately 500 voxels (mean = 455 vx, s.e.m = 39.1 vx). Again, this analysis yielded very similar results to the original analysis. Red dashed lines in the right column of panels C and D correspond to the original results from right V4 shown in the left column.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig3-figsupp2-v1"/></fig></fig-group><p>Strikingly, disambiguation increased representational dissimilarity between individual images in all ROIs investigated, including FPN and DMN. This can be seen from the gradient of color difference between the Pre-Pre square and the Post-Post square in the RDM (<xref ref-type="fig" rid="fig3">Figures 3B</xref> and <xref ref-type="fig" rid="fig4">4A</xref>, which include mid-line and right hemisphere ROIs; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for similar results from the left hemisphere ROIs, and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> for similar results from V4). The similarity structure of activation patterns for different images can be visualized by applying multi-dimensional scaling (MDS) to the RDM (see <xref ref-type="fig" rid="fig3">Figure 3C and 4B</xref> for 2-D MDS plots; SI Result for interactive 3-D MDS plots). In the MDS plot, each dot represents one image shown in a particular condition, and distances between dots preserve the correlational distance in the RDM as much as possible. As can be seen from the MDS plots, post-disambiguation Mooney images are represented more distinctly from each other (shown as greater distances among yellow and green dots) than their pre-disambiguation counterparts (blue dots), which are more clustered. This effect was most pronounced in FPN and DMN regions. Similarly, in all ROIs, grayscale images are represented relatively distinctly from each other (as shown by the large distances among red-pink dots).</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.36068.012</object-id><label>Figure 4.</label><caption><title>Neural representation format of individual images in frontoparietal regions.</title><p>RSA results for FPN and DMN ROIs from the right hemisphere. Format is the same as in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Interactive 3-dimensional MDS plots corresponding to first-order RDMs for each ROI can be found at: <ext-link ext-link-type="uri" xlink:href="https://gonzalezgarcia.github.io/mds.html">https://gonzalezgarcia.github.io/mds.html</ext-link>.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig4-v1"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36068.013</object-id><label>Figure 4—figure supplement 1.</label><caption><title>RSA results for left hemisphere FPN and DMN regions.</title><p>Same as <xref ref-type="fig" rid="fig4">Figure 4</xref>, but for left hemisphere FPN and DMN ROIs, including left parietal and left frontal ROIs from FPN, and left lateral parietal ROI from DMN.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig4-figsupp1-v1"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36068.014</object-id><label>Figure 4—figure supplement 2.</label><caption><title>DMN results of the ROI size control analysis.</title><p>To assess whether ROI size affected dissimilarity, a control analysis was performed in a subset of regions, including DMN and V4 (V4 results presented in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). We thresholded individual subject’s activation contrast maps (as in <xref ref-type="fig" rid="fig2">Figure 2A</xref>) to obtain DMN ROIs of approximately 500 voxels each (mean = 503 vx, s.e.m. = 2.1 vx), which resembled ROI sizes for early visual regions (e.g. 550 voxels for left V1; all ROI sizes are reported in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> legend). Using much smaller DMN ROIs, results very similar to the original analysis (shown here as red dashed line in panels C and D) were found.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig4-figsupp2-v1"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36068.015</object-id><label>Figure 4—figure supplement 3.</label><caption><title>Image category (natural vs.manmade) information.</title><p>(<bold>A</bold>) Analysis schematic. For details, see Materials and methods, <italic>RSA</italic>. (<bold>B–D</bold>) Category information in each ROI under each perceptual condition. Bars represent the difference in mean dissimilarity between within-category (Nat-Nat; Man-Man) image pairs and between-category (Nat-Man) image pairs. Error bars denote s.e.m. across subjects. Asterisks denote significantly higher dissimilarity for between-category than within-category elements of the RDM (p&lt;0.05, FDR-corrected, Wilcoxon signed-rank test across subjects).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig4-figsupp3-v1"/></fig></fig-group><p>To statistically assess this effect, we averaged the correlational distance across image pairs within each condition and compared the mean dissimilarity between conditions (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, top-right panel). Representational dissimilarity between individual Mooney images significantly increased after disambiguation in all ROIs investigated across both hemispheres, except the left V2 and left V4 (<xref ref-type="fig" rid="fig3">Figure 3D and 4C</xref>, cyan brackets, all p&lt;0.05, FDR-corrected, assessed by paired Wilcoxon signed-rank test across subjects; and see panel C of <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>), suggesting that Mooney images are represented more distinctly after disambiguation throughout the cortical hierarchy. In addition, representational dissimilarity between different images within each condition (shown as the height of bars in <xref ref-type="fig" rid="fig3">Figure 3D and 4C</xref>) gradually increased from early visual areas to category-selective visual regions, to FPN and DMN. A control analysis ruled out the possibility that this result was affected by different sizes of the ROIs, as equating ROI sizes yielded similar results (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p><p>Next, we assessed the amount of image category (natural vs. manmade) information in each ROI under each perceptual condition. This analysis (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>) showed that for grayscale images, category information is significant in category-selective visual regions (bilateral LOC and FG), as well as parietal regions of the FPN and DMN (including bilateral parietal cortices of FPN, right Par Lat and PCC of DMN) (all p&lt;0.05, FDR-corrected). For pre-disambiguation Mooney images, no significant category information was found in any ROI; however, for post-disambiguation Mooney images, significant category information was present in the right LOC and bilateral FG only (p&lt;0.05, FDR-corrected).</p><p>Together, the above results suggest that FPN and DMN regions represent individual Mooney images more distinctly after disambiguation, and that this representation is more linked to individual image identity than image category.</p></sec><sec id="s2-4"><title>Disambiguation shifts neural representations toward the prior throughout the cortical hierarchy</title><p>We then examined how neural representation of a given Mooney image is altered by disambiguation, by comparing the neural activity pattern elicited by a Mooney image before or after disambiguation to that elicited by its matching gray-scale image. For intuitiveness, here we used representational similarity matrices (RSM), which correspond to 1 – RDM. The representational <italic>similarity</italic> between neural activity patterns elicited by a Mooney image and its matching gray-scale image constitutes an element along the diagonal of the ‘Pre-Gray’ square and the ‘Post-Gray’ square of the RSM (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, bottom-right panel). We thus averaged the elements along each diagonal and compared their mean (red asterisk in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, bottom-right).</p><p>In all ROIs investigated across both hemispheres, from early and category-selective visual areas to FPN and DMN regions, the representational similarity between a post-disambiguation Mooney image and its matching gray-scale image is greater than that between a pre-disambiguation Mooney image and its matching gray-scale image (<xref ref-type="fig" rid="fig3">Figure 3E and 4D</xref>, red brackets; all p&lt;0.05, FDR-corrected, Wilcoxon signed-rank tests across subjects; and see panel D of <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Thus, disambiguation shifts neural representations of Mooney images significantly toward their priors throughout the cortical hierarchy.</p><p>To further examine the extent of this shift, we wondered whether the impact of priors could outweigh the influence of sensory input. In other words: Is a post-disambiguation Mooney image represented more similarly to the identical image presented in the pre-disambiguation period, or to the matching grayscale image, which, albeit physically different, elicits a similar perceptual recognition outcome (e.g., ‘It’s a crab!”)? These alternatives can be adjudicated by comparing the diagonals of the ‘Pre-Post’ and ‘Post-Gray’ squares in the RSM (green asterisk in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, bottom-right panel). Interestingly, in all ROIs except V1, post-disambiguation Mooney images were represented more similarly to their grayscale counterparts than to the same Mooney images shown before disambiguation (<xref ref-type="fig" rid="fig3">Figure 3E and 4D</xref>, green brackets; all p&lt;0.05, FDR-corrected, Wilcoxon signed-rank tests; and see panel D of <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). A similar trend is present in V1, albeit not significant (right V1: p=0.06; left V1: p=0.07). This result suggests that across the cortical hierarchy, from V2 to FPN and DMN, content-specific neural activity patterns are shaped more strongly by prior and perceptual outcome than the immediate sensory input.</p><p>Since Gray-scale and post-disambiguation images are always presented later than Pre-disambiguation images, a potential confound to the results in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref> could be repetition suppression. To control for this, we repeated the above RSA analysis using two different sets of images: (1) images that were unrecognized in the pre-disambiguation stage and recognized in the post-disambiguation stage (‘Disambiguation set’); and (2) images that were recognized in both the pre- and post-disambiguation stages (‘Repetition set’). In both cases, for each participant, the same set of Mooney images were used in the Pre and Post conditions. Since repetition manipulation is identical between these two sets of images, this analysis allowed us to test whether disambiguation had an effect on neural activity above and beyond that of repetition. Using a repeated-measures ANOVA, we found a significant condition (pre- vs. post-disambiguation) × set (‘disambiguation’ vs. ‘repetition’) interaction for all neural effects of interest: higher between-image dissimilarity for post- than pre-disambiguation images (cyan brackets in <xref ref-type="fig" rid="fig3">Figure 3 and 4</xref>, F<sub>1,14</sub> = 4.6, p=0.05); higher within-image similarity for post-Gray than pre-Gray comparison (red brackets, F<sub>1,14</sub> =14.9, p=0.002); higher within-image similarity for post-Gray than pre-post comparison (green brackets; F<sub>1,14</sub> = 6.3, p=0.02). Thus, this analysis revealed consistently larger neural effects in the Disambiguation set compare to the Recognition set, demonstrating that disambiguation sculpted neural representations above and beyond the effects expected by mere repetition.</p></sec><sec id="s2-5"><title>Neural representation format across regions follow a principal gradient of macroscale cortical organization</title><p>The above results reveal extensive influences of priors on neural representations across the cortical hierarchy, and qualitatively similar effects of disambiguation across ROIs. But is there any systematic variation in neural representation format across brain regions? To address this, we carried out a second-order RSA (<xref ref-type="bibr" rid="bib32">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib21">Guntupalli et al., 2016</xref>). Second-order RSA calculates pairwise distance (1 - Spearman correlation) between the first-order RDMs of each ROI (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left, see Materials and methods for details). Each element of the resulting second-order RDM (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) describes how similar two ROIs are in terms of their neural representation format. From <xref ref-type="fig" rid="fig5">Figure 5B</xref>, it can be seen that neural representational format is relatively similar between retinotopic visual areas, among LOC and fusiform regions, and across regions within the FPN or DMN, but relatively dissimilar between networks.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.36068.016</object-id><label>Figure 5.</label><caption><title>Relationship between neural representation format in different ROIs.</title><p>(<bold>A</bold>) Analysis schematic for second-order RSA across perceptual conditions (left) and within each condition (right). The RDMs from different ROIs were averaged across subjects. Then, a dissimilarity value (1 – Spearman rho) was obtained for each pair of RDMs. (<bold>B</bold>) Across-condition second-order RDM, depicting the dissimilarities between first-order RDMs of different ROIs. (<bold>C</bold>) MDS plot corresponding to the second-order RDM show in B. (<bold>D</bold>) Second-order RDMs and corresponding MDS plots for the pre-disambiguation (left), gray-scale (middle), and post-disambiguation (right) conditions.</p><p><supplementary-material id="fig5sdata1"><object-id pub-id-type="doi">10.7554/eLife.36068.017</object-id><label>Figure 5—source data 1.</label><caption><title>RDM for each ROI in each subject. Includes source code to perform second-order RSA and reproduce <xref ref-type="fig" rid="fig5">Figure 5</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-36068-fig5-data1-v1.zip"/></supplementary-material> </p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig5-v1"/></fig><p>Like first-order RSA (<xref ref-type="fig" rid="fig3">Figure 3 and 4</xref>), the second-order RDM can be visualized using MDS to reveal relationships amongst ROIs in their neural representation format. The 2-D MDS solution of the second-order RDM reveals an organization in accordance with known anatomical and functional networks (<xref ref-type="fig" rid="fig5">Figure 5C</xref>; r<sup>2</sup> = 0.92, stress = 0.06, that is, the 2-D MDS solution explains 92% of variance in the high-dimensional RDM and has a high goodness-of-fit). This suggests that, consistent with the above impression, regions within a network have similar neural representation format. Moreover, regions are located along a gradient consistent with the visual hierarchy (Dimension 1), which ranges from V1 – V4, to LOC and FG, then frontoparietal regions. In addition, this hierarchy mirrors a principal gradient of macroscale cortical organization recently found based on resting-state functional connectivity and cortical anatomy, with sensory areas and DMN situated at the two extremes of the gradient, and FPN being intermediate (<xref ref-type="bibr" rid="bib36">Margulies et al., 2016</xref>).</p><p>To assess whether the hierarchy of neural representation format is similar across perceptual conditions, we conducted a second-order RSA for each perceptual condition separately (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, right). The 2-D MDS solutions for each within-condition second-order RDM are shown in <xref ref-type="fig" rid="fig5">Figure 5D</xref>, bottom, and all have high goodness-of-fit (pre-disambiguation: r<sup>2</sup> = 0.91, stress = 0.07; Gray-scale: r<sup>2</sup> = 0.93, stress = 0.06; post-disambiguation: r<sup>2</sup> = 0.90, stress = 0.09). Comparing the MDS solutions across perceptual conditions suggested that, indeed, there is a stable overall hierarchy of representation format across ROIs, despite changes in neural representation within each ROI due to disambiguation (as shown in <xref ref-type="fig" rid="fig3">Figure 3 and 4</xref>). An interesting, qualitative variation on this theme is that frontal regions of the FPN rise higher up in the hierarchy after disambiguation, such that the principal gradient (Dimension 1) separates frontal from parietal regions (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, right panel), instead of DMN from FPN (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, left and middle).</p></sec><sec id="s2-6"><title>Dimensionality of neural representation increases along the cortical hierarchy and following disambiguation</title><p>What might contribute to the stable, large-scale hierarchy of neural representation format? To shed light on this question, we estimated the dimensionality of the neural representational space for each ROI in each perceptual condition (i.e., the dimensionality of the within-condition 33 × 33 RDM shown in <xref ref-type="fig" rid="fig3">Figure 3 and 4</xref>; for details see Materials and methods, <italic>RSA</italic>), to test the hypothesis that more dimensions are needed to capture the complexity of the representational space in higher order regions. The group-averaged dimensionality for each ROI is shown in <xref ref-type="fig" rid="fig6">Figure 6A–C</xref>. A large-scale gradient can be seen in every perceptual condition: Dimensionality of neural representational space is relatively low in early visual areas and LOC, but substantially higher in FG, FPN and DMN regions.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.36068.018</object-id><label>Figure 6.</label><caption><title>Dimensionality of neural representation space across ROIs and perceptual conditions.</title><p>The dimensionality of neural representation (estimated by the number of dimensions needed in the MDS solution to achieve r<sup>2</sup> &gt;0.9) for each ROI, in the pre-disambiguation (<bold>A</bold>), post-disambiguation (<bold>B</bold>), and gray-scale (<bold>C</bold>) condition, respectively. Each bar represents the mean dimensionality averaged across subjects for each ROI. (<bold>D</bold>) Group-averaged dimensionality for each network and condition. Error bars denote s.e.m. across subjects. Both the network (p=3.1e-16) and the condition (p=2e-6) factors significantly impacted dimensionality, while the interaction was not significant (p=0.29).</p><p><supplementary-material id="fig6sdata1"><object-id pub-id-type="doi">10.7554/eLife.36068.019</object-id><label>Figure 6—source data 1.</label><caption><title>RDM for each ROI in each subject. Includes source code to perform dimensionality analysis and reproduce <xref ref-type="fig" rid="fig6">Figure 6</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-36068-fig6-data1-v1.zip"/></supplementary-material> </p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig6-v1"/></fig><p>To quantitatively compare the dimensionality of neural representational space across networks and perceptual conditions, we averaged dimensionality across ROIs for each network (while keeping LOC and FG separate), and conducted a two-way repeated-measures ANOVA (factors: network and perceptual condition; dependent variable: dimensionality; see <xref ref-type="fig" rid="fig6">Figure 6D</xref>). Both main effects were highly significant (network: F<sub>56,4</sub> = 41.96, p=3.1e-16, η<sup>2</sup><sub>p</sub> = 0.75; condition: F<sub>28,2</sub> = 22.27, p=2e-6, η<sup>2</sup><sub>p</sub> = 0.61), while the interaction was not significant (p=0.29, η<sup>2</sup><sub>p</sub> = 0.08). Post-hoc tests (Bonferroni-corrected) revealed that compared to early visual areas (2.9 ± 0.3; mean ±s.d. across subjects), dimensionality was significantly higher in FG (4.6 ± 0.9), FPN (4.6 ± 1) and DMN (4.3 ± 0.7; all <italic>ps</italic> &lt; 0.001), but not LOC (3.1 ± 0.6; p&gt;0.3). On the other hand, dimensionality did not differ significantly between FG and FPN or DMN (all <italic>ps</italic> &gt; 0.08). Comparing across perceptual conditions, dimensionality was significantly higher for both post-disambiguation Mooney images (4 ± 1.1) and grayscale images (4.1 ± 1.1) than pre-disambiguation Mooney images (3.7 ± 0.7; both p&lt;0.001), while the difference between post-disambiguation Mooney images and grayscale images was not significant (p&gt;0.16).</p><p>Thus, these results reveal a stable hierarchy in the dimensionality of neural representational space: lowest in early visual areas, rising slightly in LOC, and highest in FG and frontoparietal areas. Moreover, in all these regions, dimensionality of neural representational space for Mooney images increased significantly following disambiguation, reaching roughly the same level as for grayscale images.</p></sec><sec id="s2-7"><title>Significant preservation of neural code following disambiguation in visual and parietal areas</title><p>Lastly, to quantitatively compare the impact of prior on neural representations across brain regions, we asked the following question for each region: How much of the neural code for pre-disambiguation Mooney images is preserved following disambiguation? To this end, we computed a Preservation Index (PI) by comparing the diagonal of the Pre-Post square of each ROI’s RSM to the off-diagonal values (<xref ref-type="fig" rid="fig7">Figure 7A</xref>; for details see Materials and methods). The PI quantifies the similarity between the neural representation of a Mooney image in the pre- and post- disambiguation periods above and beyond its similarity to other images (i.e., is post-Image-A represented more similarly to pre-Image-A than to pre-Image-B?), and thus can be interpreted as cross-condition decoding of individual image identity (<xref ref-type="bibr" rid="bib33">Lee et al., 2012</xref>). Therefore, a high PI suggests stronger preservation (as in ‘better cross-decoding’) of neural code before vs. after disambiguation, and a PI close to 0 suggests alteration of neural code (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, right). For each subject, only Mooney images that were not-recognized in the pre-disambiguation period and recognized in the post-disambiguation period were used in this analysis (an additional analysis based on correct vs. incorrect verbal identification reports yielded very similar results).</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.36068.020</object-id><label>Figure 7.</label><caption><title>Significant preservation of neural code is found in visual and FPN regions.</title><p>(<bold>A</bold>) Analysis schematic. For each ROI in each subject, a representational similarity matrix (RSM) is constructed using the set of Mooney images that are not recognized in the pre-disambiguation period (PRE-NR) and recognized in the post-disambiguation period (POST-R). The Pre-Post square was extracted from this RSM, and the r-values were Fisher-z-transformed. Then, the difference between diagonal and off-diagonal elements was calculated for each subject, termed ‘Preservation Index’ (PI). A significant positive PI means that a post-disambiguation Mooney image is represented more similarly to the same image than other images shown in the pre-disambiguation period. (<bold>B</bold>) Group-averaged PI values for each ROI. Error bars denote s.e.m. across subjects. Asterisks denote significant PI values (p&lt;0.05, FDR-corrected, one-sample t-test across subjects).</p><p><supplementary-material id="fig7sdata1"><object-id pub-id-type="doi">10.7554/eLife.36068.021</object-id><label>Figure 7—source data 1.</label><caption><title>RSM (as shown in <xref ref-type="fig" rid="fig7">Figure 7A</xref>, left) for each ROI in each subject. Includes source code to perform analysis and reproduce <xref ref-type="fig" rid="fig7">Figure 7</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-36068-fig7-data1-v1.zip"/></supplementary-material> </p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36068-fig7-v1"/></fig><p>This analysis revealed a large-scale trend of decreasing PI towards higher-order brain regions (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). The neural code of Mooney images was significantly preserved following disambiguation in early visual areas, LOC, FG, and the FPN (all p&lt;0.05, FDR-corrected, one-sample t-tests across subjects). However, the preservation index did not differ significantly from zero in DMN regions (all p&gt;0.05). To assess differences in PI across networks, we performed a repeated-measures ANOVA with network (Visual, LOC, FG, FPN, and DMN) being the independent factor and PI being the dependent variable. This analysis revealed a significant effect of Network (F<sub>56,4</sub> = 41.86, p=3.27e-16, η<sup>2</sup><sub>p</sub> = 0.75). Post-hoc pairwise comparisons between individual networks were all significant (all ps &lt; 0.015, Bonferroni corrected), except between Visual and LOC (p=0.81), or between FPN and DMN (p=1).</p><p>Further inspection of the data suggested differential PI in frontal and parietal regions of the FPN. To statistically assess this difference, we performed an additional repeated-measures ANOVA with only three levels for the Network factor (parietal regions of the FPN, frontal regions of the FPN, and the DMN). This analysis yielded again a significant effect of Network (F<sub>36,2</sub> = 6.86, p=0.003, η<sup>2</sup><sub>p</sub> = 0.28). Post-hoc comparisons (Bonferroni-corrected) revealed that PI was higher in FPN-parietal regions than FPN-frontal regions (p=0.001) or the DMN (p=0.02). In contrast, the difference between FPN-frontal regions and the DMN was not significant (p=1). While, by itself, these findings are consistent with the interpretation that higher-order regions such as the DMN and lateral prefrontal cortex may be unresponsive to the visual images, our earlier results rule out this possibility: neural representations in the DMN and frontal FPN regions are significantly impacted by disambiguation, becoming more distinct between individual images and shifting significantly toward the priors (<xref ref-type="fig" rid="fig4">Figure 4</xref>), with increased dimensionality in the neural representational space (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Hence, neural representation is most profoundly altered by disambiguation in higher-order brain regions including the lateral prefrontal cortex and the DMN.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In summary, we observed extensive influences of prior experience on perceptual processing across the brain: Following the encoding of perceptual priors via disambiguation, Mooney images were represented more distinctly from each other, and more similarly to the prior-inducing grayscale image, throughout the cortical hierarchy – from early visual and category-selective areas to FPN and DMN. These results reveal, unambiguously, content-specific neural representations during prior-guided visual processing in the DMN and FPN. Interestingly, despite the prior’s pervasive influence on neural activity, which was more pronounced in higher-order brain regions, the neural representation format across brain regions followed a stable macroscale hierarchy that mirrors one recently uncovered in resting-state functional connectivity, with visual areas and DMN situated at the two extremes, and FPN being intermediate (<xref ref-type="bibr" rid="bib36">Margulies et al., 2016</xref>). Along this hierarchy, the dimensionality (i.e., complexity) of neural representational space increases towards higher-order brain areas, and rises in all regions after the establishment of perceptual priors.</p><p>Previous studies have shown that the DMN is active in tasks that rely on information retrieved from memory, such as spontaneous thoughts, autobiographical and semantic memory, planning about the future, and executive control tasks requiring long-term or working memory (e.g., [<xref ref-type="bibr" rid="bib7">Buckner et al., 2008</xref>; <xref ref-type="bibr" rid="bib9">Christoff et al., 2009</xref>; <xref ref-type="bibr" rid="bib46">Shapira-Lichter et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Konishi et al., 2015</xref>; <xref ref-type="bibr" rid="bib52">Spreng and Grady, 2010</xref>; <xref ref-type="bibr" rid="bib51">Spreng et al., 2014</xref>]). Yet, whether the DMN may be involved in representing prior information that impacts <italic>perceptual</italic> processing has heretofore remained unknown. Our results herein reveal content-specific neural activity in the DMN during prior-guided visual processing for the first time. Specifically, our results demonstrated that: (<italic>i</italic>) image-specific information increases following disambiguation in DMN regions; (<italic>ii</italic>) neural representations of disambiguated Mooney images shift significantly toward their priors; (<italic>iii</italic>) individual images are represented by DMN regions in a higher-dimensional space following disambiguation. Importantly, the enhanced image-level information in DMN regions following disambiguation could not be attributed to an overall higher level of signal, since the activity is actually closer to baseline than that elicited by pre-disambiguation Mooney images. These results suggest that, even in an externally oriented perceptual task, DMN regions cannot merely encode nonspecific effects such as arousal (<xref ref-type="bibr" rid="bib20">Gorlin et al., 2012</xref>) or task difficulty (<xref ref-type="bibr" rid="bib49">Singh and Fawcett, 2008</xref>) as previously postulated. Instead, they are actively involved in the processing of these visual images, as their representation of a given visual stimulus was altered by the presence of a perceptual prior and shifted significantly toward that prior. The profound changes in DMN representations following fast, automatic disambiguation uncovered herein resonate with a previous finding showing that slow, deliberate perceptual training alters resting-state functional connectivity between DMN regions and the visual cortex (<xref ref-type="bibr" rid="bib34">Lewis et al., 2009</xref>). Our results are also consistent with a recent study showing greater DMN activation to objects than color patches during a working memory task (<xref ref-type="bibr" rid="bib40">Murphy et al., 2018</xref>), which, speculatively, may reflect stronger priors for the former learnt through past experiences, although differential stimulus complexity might provide another account for this previous finding.</p><p>Consistent with our prior hypothesis, the FPN exhibited an intermediate pattern compared to DMN and visual areas in this task. Although all these regions exhibited enhanced image-level information, a higher-dimensional neural representational space, and activity patterns that shifted towards the prior-inducing images following disambiguation, using a second-order RSA, we established that the FPN’s neural representational format was indeed intermediate between that of visual areas and DMN. In addition, the extent to which priors altered neural representations in the FPN was also intermediate of that in the visual areas and DMN (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Because all disambiguated Mooney images required the same response (counterbalanced button presses to indicate ‘recognized’), these findings in the FPN do not fit easily with a previous suggestion that the FPN only encode the distinction between targets and non-targets (<xref ref-type="bibr" rid="bib16">Erez and Duncan, 2015</xref>). Instead, they point to content-specific representations relevant to <italic>perceptual</italic> processing in the FPN. Interestingly, although the global hierarchy of neural representation format across cortical areas remained stable across perceptual conditions, following disambiguation frontal areas of the FPN moved up the hierarchy, suggesting that they may have a special role in utilizing priors to guide perceptual processing. These results amplify limited existing evidence for frontal areas’ involvement in prior-guided visual perception (<xref ref-type="bibr" rid="bib60">Wang et al., 2013</xref>; <xref ref-type="bibr" rid="bib3">Bar et al., 2006</xref>; <xref ref-type="bibr" rid="bib25">Imamoglu et al., 2012</xref>; <xref ref-type="bibr" rid="bib54">Summerfield et al., 2006</xref>).</p><p>Outside the DMN and FPN, our findings support earlier reports of increased image-level information in early and category-selective visual regions following disambiguation, as well as a shift of neural representation in visual areas toward the prior-inducing image (<xref ref-type="bibr" rid="bib24">Hsieh et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Gorlin et al., 2012</xref>; <xref ref-type="bibr" rid="bib58">van Loon et al., 2016</xref>). An unresolved issue in these previous studies was whether image-specific information increases in V1 following disambiguation (<xref ref-type="bibr" rid="bib20">Gorlin et al., 2012</xref>; <xref ref-type="bibr" rid="bib58">van Loon et al., 2016</xref>); our results suggest that this is indeed the case, in line with the finding in (<xref ref-type="bibr" rid="bib58">van Loon et al., 2016</xref>). In addition, an intriguing, novel finding in visual areas in the current study concerns the difference in neural representation between the LOC and the FG. Although both are high-level, category-selective visual areas, and both are consistently placed between early visual areas (V1 – V4) and FPN in the macroscale cortical hierarchy of neural representational format (<xref ref-type="fig" rid="fig5">Figure 5</xref>), several findings converge to show a clear hierarchy between them, with the FG being a higher-order region than LOC: First, FG is consistently placed higher up than LOC along the cortical hierarchy across perceptual conditions (<xref ref-type="fig" rid="fig5">Figure 5</xref>); second, FG consistently exhibits higher dimensionality in its neural representational space than LOC across perceptual conditions; last, disambiguation alters neural representation more strongly in FG than LOC. Both higher dimensionality and larger impact of prior are features associated with higher-order brain areas (<xref ref-type="fig" rid="fig6">Figure 6 and 7</xref>). These results revealing a hierarchy between LOC and FG are compatible with the FG being located more anteriorly than the LOC (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>) and previous report of a representational gradient between these two regions (<xref ref-type="bibr" rid="bib22">Haushofer et al., 2008</xref>).</p><p>The Mooney image disambiguation effect is a dramatic example of one-shot learning, where a single exposure to an event leaves a long-term memory in the brain that influences future perception of a different, but related stimulus. While the poster child of one-shot memory is episodic memory, the perceptual prior acquired during Mooney image disambiguation differs from episodic memory by being non-self-referential and potentially non-declarative (<xref ref-type="bibr" rid="bib35">Ludmer et al., 2011</xref>; <xref ref-type="bibr" rid="bib8">Chang et al., 2016</xref>). The presence of this perceptual prior is probed by whether the subject recognizes the corresponding Mooney image, not by whether the subject recalls having seen either the Mooney or Gray-scale image earlier (the episodic memory of the event itself). Supporting this distinction, a previous study found that activity in mid-level visual regions, MPFC, and amygdala, but not hippocampus, during gray-scale image viewing predicted successful disambiguation of Mooney images (<xref ref-type="bibr" rid="bib35">Ludmer et al., 2011</xref>). In accordance, we did not find a significant increase in image-specific information following disambiguation in anatomically defined hippocampal ROIs. Compared to episodic memory, the neural mechanisms underlying experience-induced perceptual priors have been much less studied. Understanding how a single experience may leave a prior in the brain that alters future perception addresses a fundamental scientific question with important clinical implications. For instance, if prior experiences overwhelm sensory input, hallucinations may follow, as happens in post-traumatic stress disorder (<xref ref-type="bibr" rid="bib10">Clancy et al., 2017</xref>). Similarly, psychosis-prone individuals have a larger disambiguation effect than control subjects in the Mooney image paradigm, consistent with the idea that internal priors play an especially strong role in shaping perception during psychosis (<xref ref-type="bibr" rid="bib55">Teufel et al., 2015</xref>).</p><p>In conclusion, we observed that prior experience impacts visual perceptual processing throughout the cortical hierarchy, from occipitotemporal visual regions to frontoparietal and default-mode networks. In addition, we observed a macroscale hierarchy of neural representation format, with increased dimensionality in neural representational space and more profound influences of perceptual prior on neural representations in higher-order frontoparietal regions. These results reveal content-specific neural representations in frontoparietal and default-mode networks involved in prior-guided visual perception.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>Twenty-three healthy volunteers participated in the study. All participants were right-handed and neurologically healthy, with normal or corrected-to-normal vision. The experiment was approved by the Institutional Review Board of the National Institute of Neurological Disorders and Stroke. All subjects provided written informed consent. Four subjects were excluded due to excessive movements in the scanner, leaving 19 subjects for the analyses reported herein (age range = 19–32; mean age = 24.6; 11 females).</p></sec><sec id="s4-2"><title>Visual stimuli</title><p>The construction and selection of Mooney images and corresponding gray-scale images were described in detail in a previous study (<xref ref-type="bibr" rid="bib8">Chang et al., 2016</xref>). Briefly, Mooney and gray-scale images were generated from gray-scale photographs of real-world man-made objects and animals selected from the Caltech (<ext-link ext-link-type="uri" xlink:href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html">http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html</ext-link>) and Pascal VOC (<ext-link ext-link-type="uri" xlink:href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html</ext-link>) databases. First, gray-scale images were constructed by cropping gray-scale photographs with a single man-made object or animal in a natural setting to 500 × 500 pixels and applying a box filter. Mooney images were subsequently generated by thresholding the gray-scale image. Threshold level and filter size were initially set at the median intensity of each image and 10 × 10 pixels, respectively. Each parameter was then titrated so that the Mooney image was difficult to recognize without first seeing the corresponding gray-scale image. Out of an original set of 252 images, 33 (16 were man-made objects, the rest animals – unbeknownst to the subjects) were chosen for the experiment via an initial screening procedure, which was performed by six additional subjects recruited separately from the main experiment. Images with the largest disambiguation effect – assessed by changes in difficulty rating before vs. after disambiguation – were chosen for this study. Images were projected onto a screen located at the back of the scanner and subtended approximately 11.9 × 11.9 degrees of visual angle.</p></sec><sec id="s4-3"><title>Task paradigm</title><p>Each trial started with a red fixation cross presented in the center of the screen for 2 s, and then a Mooney image or a gray-scale image presented for 4 s. The fixation cross was visible during image presentation, and subjects were instructed to maintain fixation throughout. A 2 s blank period appeared next, followed by a brighter fixation cross (lasting 2 s) that prompted participants to respond (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Participants were instructed to respond to the question ‘Can you recognize and name the object in the image?’ with an fMRI-compatible button box using their right thumb. Trials were grouped into blocks, using a design similar to previous studies (<xref ref-type="bibr" rid="bib20">Gorlin et al., 2012</xref>; <xref ref-type="bibr" rid="bib8">Chang et al., 2016</xref>). Each block contained fifteen trials: three gray-scale images followed by six Mooney-images and a shuffled repetition of the same six Mooney-images. Three of the Mooney images had been presented in the previous run and corresponded to the gray-scale images in the same block (these are post-disambiguation Mooney images). The other three Mooney-images were novel and did not match the gray-scale images (pre-disambiguation); their corresponding gray-scale images would be presented in the following run. Each fMRI run included three blocks of the same images; within each block, image order was shuffled but maintained the same block structure (gray-scale followed by Mooney images). After each run, a verbal test was conducted between fMRI runs. During the verbal test, the six Mooney images from the previous run were presented one by one for 4 s each, and participants were instructed to verbally report the identity of the image. Each participant completed 12 runs. In order for all Mooney images to be presented pre- and post-disambiguation, the first and last runs were ‘half runs’. The first run contained only three novel Mooney images (pre-disambiguation). The last run consisted of 3 gray-scale images and their corresponding post-disambiguation Mooney images. The total duration of the task was ~90 min. The order of Mooney images presentation was randomized across participants.</p></sec><sec id="s4-4"><title>Data acquisition and preprocessing</title><p>Imaging was performed on a Siemens 7T MRI scanner equipped with a 32-channel head coil (Nova Medical, Wilmington, MA, USA). T1-weighted anatomical images were obtained using a magnetization-prepared rapid-acquisition gradient echo (MP-RAGE) sequence (sagittal orientation, 1 × 1×1 mm resolution). Additionally, a proton-density (PD) sequence was used to obtain PD-weighted images also with 1 × 1 × 1 mm resolution, to help correct for field inhomogeneity in the MP-RAGE images (54). Functional images were obtained using a single-shot echo planar imaging (EPI) sequence (TR = 2000 ms, TE = 25 ms, flip angle = 50°, 52 oblique slices, slice thickness = 2 mm, spacing = 0 mm, in-plane resolution = 1.8×1.8 mm, FOV = 192 mm, acceleration factor/GRAPPA = 3). The functional data were later resampled to 2 mm isotropic voxels. Respiration and cardiac data were collected using a breathing belt and a pulse oximeter, respectively. The breathing belt was wrapped around the upper abdomen, and the pulse oximeter was placed around the left index finger. Physiological data were collected simultaneously with fMRI data using the AcqKnowledge software (Biopac Systems, Inc.).</p><p>For anatomical data preprocessing, MP-RAGE and PD images were first skull-stripped. Then, the PD image was smoothed using a 2 mm full-width at half maximum (FWHM) kernel. Afterwards, the MP-RAGE image was divided by the smoothed PD image to correct for field inhomogeneity.</p><p>Functional data preprocessing started with the removal of physiological (respiration- and cardiac-related) noise using the RETROICOR method (<xref ref-type="bibr" rid="bib19">Glover et al., 2000</xref>). The next preprocessing steps were performed using the FSL package (<ext-link ext-link-type="uri" xlink:href="http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL">http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL</ext-link>). These included: ICA cleaning to remove components corresponding to physiological or movement-related noise,&gt;0.007 Hz high-pass filtering (corresponding to a temporal period of 150 s) to remove low-frequency drifts, rigid-body transformation to correct for head motion within and across runs, slice timing correction to compensate for systematic differences in the time of slice acquisition, and spatial smoothing with a 3 mm FWHM Gaussian kernel. Last, images were registered to the atlas in two steps. First, functional images were registered to the individual’s anatomical image via global rescale (7 degrees of freedom) transformations. Affine (12 degrees of freedom) transformations were then used to register the resulting functional image to a 2 × 2 × 2 mm MNI atlas. Registration to MNI space was performed on voxel-wise GLM and searchlight MVPA results to obtain population-level whole-brain maps. RSA analyses were conducted in individual subject’s functional data space and results were pooled across subjects.</p></sec><sec id="s4-5"><title>Lateral occipital complex (LOC) functional localizer</title><p>133 images of objects (including fruits, vegetables, and man-made objects) were extracted from the Bank of Standardized Stimuli (BOSS - <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/bosstimuli">https://sites.google.com/site/bosstimuli</ext-link>). From each object image, a corresponding image of phase-shuffled noise was created. Each trial consisted of a brief blank period of 100 ms, followed by an image or shuffled noise, presented for 700 ms. Each block contained 16 trials, and subjects were instructed to maintain fixation on a dimmed red cross at the centre of the screen throughout the block. Within each block, four trials consisted of repetitions of the previous item. Subjects were instructed to press a button when the same image of object or shuffled noise appeared twice in a row, which occurred randomly within the block. The single localizer run (lasting ~3 min) contained 10 blocks, 5 of images and 5 of shuffled noise, each of which started with a warning cross lasting 4 s. The LOC functional localizer data were analyzed using a GLM, which included regressors for image and shuffled-noise events, as well as an additional regressor that modelled the warning cross at the beginning of each block. A significance map of the Image &gt; Noise contrast was obtained for each subject.</p></sec><sec id="s4-6"><title>Retinotopy functional localizer</title><p>Subjects were shown a circular checkerboard of 100% contrast and 21° diameter through a bar aperture that progressed through the screen to cover the entire visual field. One sweep included 18 steps, one every TR (2 s), taking a total of 36 s. During the single retinotopy scan, eight sweeps were performed, accounting for four different orientations (left, right, bottom, up) and two directions. The specific pattern of sweeps was as follows: left-right, bottom right-top left, top-down, bottom left-top right, right-left, top left-bottom right, bottom-top, and top right-bottom left. Participants were instructed to stare at a fixation cross in the center of the screen and press a button when it changed from green to red. These changes occurred in a semi-random fashion, with approximately two changes per sweep. The total duration of the retinotopy scan was ~5 min.</p><p>Retinotopy data preprocessing was performed using custom-written AFNI code. Preprocessing included motion correction to the first volume in the run, detrending and removal of linear trends (such as scanner drift) using linear least-squares, and spatial smoothing (5 mm FWHM kernel). Population receptive fields (pRF) (<xref ref-type="bibr" rid="bib15">Dumoulin and Wandell, 2008</xref>) analysis was performed using an AFNI-based implementation (<xref ref-type="bibr" rid="bib47">Silson et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Silson et al., 2016</xref>). Briefly, for each voxel, two algorithms found the best fit (in terms of location in the field of view, and size of the receptive field) between the predicted and observed time-series, by minimizing the least-squares error between these two. The output of the model included X and Y location, sigma (size) and R<sup>2</sup> estimate of the best fitting model. In order to delineate the different visual regions, polar angle and eccentricity components were created from X and Y data. These components were projected onto 3-D hemispherical surface reconstruction of individual subject’s gray and white matter boundaries (obtained using the recon-all command in Freesurfer, <ext-link ext-link-type="uri" xlink:href="http://freesurfer.net/">http://freesurfer.net/</ext-link>). Following the criteria described in (<xref ref-type="bibr" rid="bib48">Silson et al., 2016</xref>), we defined the following field maps in left and right hemispheres of each subject: V1, V2d, V2v, V3d, V3v and V4. Dorsal and ventral portions of V2 and V3 were merged into one single ROI for each hemisphere, respectively.</p></sec><sec id="s4-7"><title>General Lineal Model (GLM) analysis</title><p>A GLM was used to assess changes in activation magnitude across conditions. At the individual subject level, a model was constructed including regressors for pre-disambiguation not-recognized, pre-disambiguation recognized, post-disambiguation recognized Mooney images, and gray-scale images. Two different versions of the model were created, one based on subjective recognition responses and the other based on verbal identification responses. In the former, an image was defined as not-recognized if the subject responded ‘yes’ in two or fewer out of the 6 presentations of that image; it was defined as recognized if the subject responded ‘yes’ in four or more presentations of the image (see <xref ref-type="fig" rid="fig1">Figure 1D</xref>). Importantly, the same set of images are included in the ‘pre-disambiguation not-recognized’ and ‘post-disambiguation recognized’ conditions.</p><p>All regressors were convolved with a hemodynamic response function (HRF) following a Gamma shape (half-width of 3 s and lag of 6 s). In addition, temporal derivatives of each regressor were also included in the model. At the population level, parameter estimates of each regressor were entered into a mixed-effects analysis to obtain group-level estimates (FLAME1 estimation method). To correct for multiple comparisons, a cluster-defining threshold of p&lt;0.001 (z &gt; 3.1) and minimum cluster size of 17 voxels was applied (corresponding to p&lt;0.05, FWE-corrected).</p></sec><sec id="s4-8"><title>Multivariate pattern analysis (MVPA)</title><p>MVPA was performed using The Decoding Toolbox (<xref ref-type="bibr" rid="bib23">Hebart et al., 2014</xref>) and custom-written codes in MATLAB. First, in order to obtain a beta estimate for each individual image and subject, a new GLM was fit to the preprocessed fMRI data in the native space. For each subject, each regressor included all presentations of a given image in a given condition.</p><p>To decode the status of the image (pre-disambiguation not-recognized from post- disambiguation recognized), a searchlight analysis across the whole brain was conducted using 6-voxel radius spheres and following an n-fold cross-validation scheme. As in the GLM analysis, the same set of images were included in both conditions. In each fold, all samples but two (one from each class) were used to train the classifier (linear support vector machine (SVM); cost parameter = 1) which was then tested on the remaining two samples. The accuracy value was averaged across folds and assigned to the center voxel of each sphere. To assess significance at the population level, a non-parametric permutation-based approach was used, with 5000 shuffles. A threshold-free cluster enhancement algorithm (<xref ref-type="bibr" rid="bib50">Smith and Nichols, 2009</xref>) was then used to find significant clusters (p&lt;0.01, FWE-corrected) on the resulting map.</p></sec><sec id="s4-9"><title>Region-of-interest (ROI) definition</title><p>Individually defined ROIs of DMN [medial prefrontal cortex (MPFC), posterior cingulate cortex (PCC), left and right lateral parietal cortex (Par Lat)] were extracted from each subject’s estimate of the post-recognized &gt; pre not-recognized GLM contrast in the native space (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The same approach was applied to the LOC localizer data to obtain LOC ROIs for each subject, using the ‘Image &gt; Shuffled Noise’ contrast. To define FPN ROIs, the corrected, population-level statistical map of the pre-not-recognized vs. post-recognized decoding analysis (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) was used to define frontal and parietal clusters, bilaterally. Frontal clusters of the decoding results roughly matched the frontal areas of FPN reported in previous studies (<xref ref-type="bibr" rid="bib42">Power et al., 2011</xref>) (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4C</xref>) and were used to define our FPN frontal ROIs (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4B</xref>, red). In contrast, parietal clusters of the decoding result encompassed the parietal regions of both FPN and DMN (compare panels A and C in <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). Thus, we selected those voxels of our decoding map overlapping with previously reported parietal regions of the FPN (<xref ref-type="bibr" rid="bib42">Power et al., 2011</xref>) to define our FPN parietal ROIs (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4B</xref>, red, also see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). These population-level ROIs were then registered back to the native space of each subject to obtain individual ROIs. Additionally, we obtained left and right fusiform gyrus ROIs by extracting the template regions from the Harvard-Oxford Cortical Structural Atlas (<ext-link ext-link-type="uri" xlink:href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Atlases">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Atlases</ext-link>) and registering these back to the native space of each subject. Last, V1-V4 were defined in each subject’s native space based on the retinotopic localizer described earlier.</p><p>Finally, since DMN and FPN ROIs were defined using the same data set, albeit by independent analyses (GLM and MVPA, respectively) from the ROI-based RSA analyses, to exclude any concern about potential influences of double dipping, we repeated the RSA analyses using FPN and DMN ROIs defined from an independent resting-state data set (<xref ref-type="bibr" rid="bib42">Power et al., 2011</xref>). The results obtained using these ROIs replicated those reported in <xref ref-type="fig" rid="fig4">Figures 4</xref>–<xref ref-type="fig" rid="fig7">7</xref>.</p></sec><sec id="s4-10"><title>Representational similarity analysis (RSA)</title><p>We performed ROI-based RSA on the activity patterns of individual images to assess the impact of disambiguation on their neural representation. These activity patterns were derived from GLM’s beta weights as described above in the section MVPA. For each subject and ROI, we calculated the representational distance (1 – Pearson’s r) using activity pattern across voxels between pairs of images from all conditions (33 pre-disambiguation, 33 post-disambiguation and 33 gray-scale), obtaining a 99 × 99 symmetrical representational dissimilarity matrix (RDM) (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The diagonal elements of the RDM have value 0 and are not used in further analyses. In order to conduct group-level statistics, all images from all subjects were used and this analysis thus did not depend on subjective recognition or verbal identification responses.</p><p>To facilitate visualization, the RDM was projected to a 2-dimensional plot using non-metric multidimensional scaling (MDS; criterion = stress1), where each dot corresponds to the neural representation of one image, and distances between dots preserve the original representational distances in the RDM as much as possible.</p><p>Based on each ROI’s RDM, two tests were conducted. First, we assessed whether individual images are represented more distinctly in one perceptual condition than another, by comparing the mean of the lower triangle of within-category squares of the RDM (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, top-right; for example, comparing the mean representational distance between pre-disambiguation Mooney images to the mean distance between post-disambiguation images). Second, we investigated the representational <italic>similarity</italic> of an identical or related image presented in different conditions (e.g., comparing the representational similarity between the same image presented pre and post disambiguation to that between a post-disambiguation Mooney image and its corresponding gray-scale image), taking the diagonals of between-condition squares of the representational similarity matrix (RSM, where each element includes the Pearson’s r-value between two images; <xref ref-type="fig" rid="fig3">Figure 3A</xref>, bottom-right). In both cases, differences between conditions were assessed by paired Wilcoxon signed-rank tests across subjects, and results were FDR-corrected for multiple comparisons.</p><p>To quantify the amount of category information in each ROI under each perceptual condition, for each subject, we first extracted the within-condition (Pre-Pre; Post-Post; Gray-Gray) portions of the RDM (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3A</xref>). Within each condition, images are sorted based on category: Natural (Nat) and Manmade (Man). We then computed the mean dissimilarity across image pairs where both images are of the same category but are non-identical (i.e., off-diagonal elements of the Nat-Nat square and Man-Man square), as well as the mean dissimilarity across image pairs where the two images are of different categories (i.e., elements in the Nat-Man square). The difference between the two mean dissimilarity scores was computed for each subject, and subjected to a group-level Wilcoxon signed-rank test against 0. A significant difference, with lower within-category dissimilarity than between-category dissimilarity, suggests significant category-level information.</p><p>Finally, to estimate the dimensionality of neural representational space for each ROI in each perceptual condition, we performed MDS using progressively increasing numbers (from 2 to 10) of dimensions. For each MDS solution, we assessed its goodness-of-fit using r<sup>2</sup>, which indexes the proportion of variance in the original RDM explained by the MDS. To obtain r<sup>2</sup>, we first computed the pairwise Euclidean distances of the coordinates resulting from the MDS, and then calculated the squared correlation between this new RDM (corresponding to the N-dimensional MDS solution) with the original RDM. For each ROI and perceptual condition, we then determined the smallest number of dimensions needed for the MDS to achieve a high goodness-of-fit (set at a threshold of r<sup>2</sup> &gt; 0.90).</p></sec><sec id="s4-11"><title>Second-order RSA</title><p>To perform second-order RSA (<xref ref-type="bibr" rid="bib32">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib21">Guntupalli et al., 2016</xref>), we calculated the distance (using 1 - Spearman rho) of group-averaged RDM between every pair of ROIs (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). As in first-order RSA, we applied MDS to obtain a 2-D plot of the representational distances amongst ROIs (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), such that distance between any two ROIs in this plot is inversely proportional to how similarly they represent the set of images used in this experiment. The goodness-of-fit of the 2-D MDS solution was assessed using two metrics: r<sup>2</sup>, which quantifies the fraction of variance in the original high-dimensional RDM captured by the 2-D MDS solution; and stress, where a stress &lt; 0.1 indicates high goodness-of-fit. This analysis was first calculated across perceptual conditions (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left), that is, using the entire group-averaged RDMs. The analysis was then repeated for each condition separately, using the within-condition square of the group-averaged RDMs (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, right).</p></sec><sec id="s4-12"><title>Preservation Index (PI) for investigating changes in neural code due to disambiguation</title><p>To avoid potential confounds (see Results), this analysis was performed in new RDMs computed exclusively with pre-not-recognized and post-recognized images. Based on the representational similarity matrix (RSM) from each ROI, we used the following formula to compute a preservation index by comparing the diagonal and off-diagonal elements in the pre-post square of the matrix (<xref ref-type="fig" rid="fig7">Figure 7A</xref>):</p><p>Preservation index = mean(z(diagonal similarity)) – mean(z(off-diagonal similarity)), where z denotes Fisher’s z-transform, which transforms Pearson’s r-values into a normal distribution. We then performed a one-sample t-test against 0 on the PI across subjects for each ROI, and the results were FDR-corrected for multiple comparisons. A significant positive PI suggests that the neural representation of a post-disambiguation Mooney image is significantly more similar to the same image than to other images presented in the pre-disambiguation period.</p><p>An additional analysis was carried out using a normalized PI metric as follows:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">z</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">z</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">z</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">z</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>A one-sample Wilcoxon signed-rank test (because the ratio of two normally distributed variables no longer follows a normal distribution) on the normalized PI across subjects yielded an identical pattern of results as the above analysis.</p></sec><sec id="s4-13"><title>Source data and code</title><p>Data and Matlab code used for analyses and producing results in the main figures are available as Source Data and Source Code with this article.</p></sec></sec></body><back><ack id="ack"> <title>Acknowledgements</title> <p>This research was supported by the Intramural Research Program of the National Institutes of Health/National Institute of Neurological Disorders and Stroke, New York University Langone Medical Center, Leon Levy Foundation and Klingenstein-Simons Fellowship (to BJH). CGG was supported by the Department of State Fulbright program. We thank Edward Silson for help with implementing population receptive field retinotopic mapping, Souheil Inati and Vinai Roopchansingh for help with implementing scanner pulse sequence, and Catie Chang for help with using RETROICOR.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Methodology</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Validation, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Twenty-three healthy volunteers participated in the study. The experiment was approved by the Institutional Review Board of the National Institute of Neurological Disorders and Stroke. All subjects provided written informed consent.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.36068.022</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-36068-transrepform-v1.docx"/></supplementary-material><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files. Source data files have been provided for Figures 3–7.</p><p>The following previously published datasets were used:</p><p><related-object content-type="generated-dataset" id="dataset1" source-id="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html" source-id-type="uri"><collab collab-type="author">Fei-Fei L</collab><collab collab-type="author">Fergus R</collab><collab collab-type="author">Perona P</collab><year>2007</year><source>Caltech 101 dataset</source><ext-link ext-link-type="uri" xlink:href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html">http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html</ext-link><comment>Publicly available</comment></related-object></p><p><related-object content-type="generated-dataset" id="dataset2" source-id="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html" source-id-type="uri"><collab collab-type="author">Everingham M</collab><collab collab-type="author">Van Gool L</collab><collab collab-type="author">Williams CK</collab><collab collab-type="author">Winn J</collab><collab collab-type="author">Zisserman A</collab><year>2010</year><source>Pascal VOC database</source><ext-link ext-link-type="uri" xlink:href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html</ext-link><comment>Publicly available</comment></related-object></p></sec></sec><ref-list> <title>References</title> <ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albright</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>On the perception of probable things: neural substrates of associative memory, imagery, and perception</article-title><source>Neuron</source><volume>74</volume><fpage>227</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.001</pub-id><pub-id pub-id-type="pmid">22542178</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Schwiedrzik</surname> <given-names>CM</given-names></name><name><surname>Kohler</surname> <given-names>A</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name><name><surname>Muckli</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Stimulus predictability reduces responses in primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>2960</fpage><lpage>2966</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3730-10.2010</pub-id><pub-id pub-id-type="pmid">20181593</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name><name><surname>Kassam</surname> <given-names>KS</given-names></name><name><surname>Ghuman</surname> <given-names>AS</given-names></name><name><surname>Boshyan</surname> <given-names>J</given-names></name><name><surname>Schmid</surname> <given-names>AM</given-names></name><name><surname>Schmidt</surname> <given-names>AM</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Hämäläinen</surname> <given-names>MS</given-names></name><name><surname>Marinkovic</surname> <given-names>K</given-names></name><name><surname>Schacter</surname> <given-names>DL</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Top-down facilitation of visual recognition</article-title><source>PNAS</source><volume>103</volume><fpage>449</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507062103</pub-id><pub-id pub-id-type="pmid">16407167</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname> <given-names>AM</given-names></name><name><surname>Usrey</surname> <given-names>WM</given-names></name><name><surname>Adams</surname> <given-names>RA</given-names></name><name><surname>Mangun</surname> <given-names>GR</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Canonical microcircuits for predictive coding</article-title><source>Neuron</source><volume>76</volume><fpage>695</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id><pub-id pub-id-type="pmid">23177956</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname> <given-names>JR</given-names></name><name><surname>Desai</surname> <given-names>RH</given-names></name><name><surname>Graves</surname> <given-names>WW</given-names></name><name><surname>Conant</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Where is the semantic system? A critical review and meta-analysis of 120 functional neuroimaging studies</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>2767</fpage><lpage>2796</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp055</pub-id><pub-id pub-id-type="pmid">19329570</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Daniels</surname> <given-names>N</given-names></name><name><surname>Op de Beeck</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task context overrules object- and Category-Related representational content in the human parietal cortex</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>310</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw419</pub-id><pub-id pub-id-type="pmid">28108492</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckner</surname> <given-names>RL</given-names></name><name><surname>Andrews-Hanna</surname> <given-names>JR</given-names></name><name><surname>Schacter</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The brain's default network: anatomy, function, and relevance to disease</article-title><source>Annals of the New York Academy of Sciences</source><volume>1124</volume><fpage>1</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1196/annals.1440.011</pub-id><pub-id pub-id-type="pmid">18400922</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>R</given-names></name><name><surname>Baria</surname> <given-names>AT</given-names></name><name><surname>Flounders</surname> <given-names>MW</given-names></name><name><surname>He</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Unconsciously elicited perceptual prior</article-title><source>Neuroscience of Consciousness</source><volume>2016</volume><elocation-id>niw008</elocation-id><pub-id pub-id-type="doi">10.1093/nc/niw008</pub-id><pub-id pub-id-type="pmid">27595010</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christoff</surname> <given-names>K</given-names></name><name><surname>Gordon</surname> <given-names>AM</given-names></name><name><surname>Smallwood</surname> <given-names>J</given-names></name><name><surname>Smith</surname> <given-names>R</given-names></name><name><surname>Schooler</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Experience sampling during fMRI reveals default network and executive system contributions to mind wandering</article-title><source>PNAS</source><volume>106</volume><fpage>8719</fpage><lpage>8724</lpage><pub-id pub-id-type="doi">10.1073/pnas.0900234106</pub-id><pub-id pub-id-type="pmid">19433790</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clancy</surname> <given-names>K</given-names></name><name><surname>Ding</surname> <given-names>M</given-names></name><name><surname>Bernat</surname> <given-names>E</given-names></name><name><surname>Schmidt</surname> <given-names>NB</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Restless 'rest': intrinsic sensory hyperactivity and disinhibition in post-traumatic stress disorder</article-title><source>Brain</source><volume>140</volume><fpage>2041</fpage><lpage>2050</lpage><pub-id pub-id-type="doi">10.1093/brain/awx116</pub-id><pub-id pub-id-type="pmid">28582479</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crittenden</surname> <given-names>BM</given-names></name><name><surname>Mitchell</surname> <given-names>DJ</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Recruitment of the default mode network during a demanding act of executive control</article-title><source>eLife</source><volume>4</volume><elocation-id>e06481</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06481</pub-id><pub-id pub-id-type="pmid">25866927</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name><name><surname>Kouider</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>What is consciousness, and could machines have it?</article-title><source>Science</source><volume>358</volume><fpage>486</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1126/science.aan8871</pub-id><pub-id pub-id-type="pmid">29074769</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Fink</surname> <given-names>GR</given-names></name><name><surname>Rolls</surname> <given-names>E</given-names></name><name><surname>Booth</surname> <given-names>M</given-names></name><name><surname>Holmes</surname> <given-names>A</given-names></name><name><surname>Frackowiak</surname> <given-names>RS</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>How the brain learns to see objects and faces in an impoverished context</article-title><source>Nature</source><volume>389</volume><fpage>596</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1038/39309</pub-id><pub-id pub-id-type="pmid">9335498</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dosenbach</surname> <given-names>NU</given-names></name><name><surname>Fair</surname> <given-names>DA</given-names></name><name><surname>Cohen</surname> <given-names>AL</given-names></name><name><surname>Schlaggar</surname> <given-names>BL</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A dual-networks architecture of top-down control</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>99</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.01.001</pub-id><pub-id pub-id-type="pmid">18262825</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Population receptive field estimates in human visual cortex</article-title><source>NeuroImage</source><volume>39</volume><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id><pub-id pub-id-type="pmid">17977024</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erez</surname> <given-names>Y</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Discrimination of visual categories based on behavioral relevance in widespread regions of frontoparietal cortex</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>12383</fpage><lpage>12393</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1134-15.2015</pub-id><pub-id pub-id-type="pmid">26354907</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname> <given-names>DJ</given-names></name><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Categorical representation of visual stimuli in the primate prefrontal cortex</article-title><source>Science</source><volume>291</volume><fpage>312</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1126/science.291.5502.312</pub-id><pub-id pub-id-type="pmid">11209083</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freud</surname> <given-names>E</given-names></name><name><surname>Plaut</surname> <given-names>DC</given-names></name><name><surname>Behrmann</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>'What' Is Happening in the Dorsal Visual Pathway</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>773</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.08.003</pub-id><pub-id pub-id-type="pmid">27615805</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glover</surname> <given-names>GH</given-names></name><name><surname>Li</surname> <given-names>TQ</given-names></name><name><surname>Ress</surname> <given-names>D</given-names></name><name><surname>Tq</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Image-based method for retrospective correction of physiological motion effects in fMRI: RETROICOR</article-title><source>Magnetic Resonance in Medicine</source><volume>44</volume><fpage>162</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1002/1522-2594(200007)44:1&lt;162::AID-MRM23&gt;3.0.CO;2-E</pub-id><pub-id pub-id-type="pmid">10893535</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorlin</surname> <given-names>S</given-names></name><name><surname>Meng</surname> <given-names>M</given-names></name><name><surname>Sharma</surname> <given-names>J</given-names></name><name><surname>Sugihara</surname> <given-names>H</given-names></name><name><surname>Sur</surname> <given-names>M</given-names></name><name><surname>Sinha</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imaging prior information in the brain</article-title><source>PNAS</source><volume>109</volume><fpage>7935</fpage><lpage>7940</lpage><pub-id pub-id-type="doi">10.1073/pnas.1111224109</pub-id><pub-id pub-id-type="pmid">22538820</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guntupalli</surname> <given-names>JS</given-names></name><name><surname>Hanke</surname> <given-names>M</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Ramadge</surname> <given-names>PJ</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A model of representational spaces in human cortex</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>2919</fpage><lpage>2934</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw068</pub-id><pub-id pub-id-type="pmid">26980615</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haushofer</surname> <given-names>J</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multivariate patterns in object-selective cortex dissociate perceptual and physical shape similarity</article-title><source>PLoS Biology</source><volume>6</volume><elocation-id>e187</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060187</pub-id><pub-id pub-id-type="pmid">18666833</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Görgen</surname> <given-names>K</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00088</pub-id><pub-id pub-id-type="pmid">25610393</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsieh</surname> <given-names>PJ</given-names></name><name><surname>Vul</surname> <given-names>E</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Recognition alters the spatial pattern of FMRI activation in early retinotopic cortex</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>1501</fpage><lpage>1507</lpage><pub-id pub-id-type="doi">10.1152/jn.00812.2009</pub-id><pub-id pub-id-type="pmid">20071627</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imamoglu</surname> <given-names>F</given-names></name><name><surname>Kahnt</surname> <given-names>T</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Changes in functional connectivity support conscious object recognition</article-title><source>NeuroImage</source><volume>63</volume><fpage>1909</fpage><lpage>1917</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.07.056</pub-id><pub-id pub-id-type="pmid">22877578</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname> <given-names>SK</given-names></name><name><surname>Xu</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Behaviorally relevant abstract object identity representation in the human parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1607</fpage><lpage>1619</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1016-15.2016</pub-id><pub-id pub-id-type="pmid">26843642</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelley</surname> <given-names>WM</given-names></name><name><surname>Macrae</surname> <given-names>CN</given-names></name><name><surname>Wyland</surname> <given-names>CL</given-names></name><name><surname>Caglar</surname> <given-names>S</given-names></name><name><surname>Inati</surname> <given-names>S</given-names></name><name><surname>Heatherton</surname> <given-names>TF</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Finding the self? An event-related fMRI study</article-title><source>Journal of Cognitive Neuroscience</source><volume>14</volume><fpage>785</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1162/08989290260138672</pub-id><pub-id pub-id-type="pmid">12167262</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Massimini</surname> <given-names>M</given-names></name><name><surname>Boly</surname> <given-names>M</given-names></name><name><surname>Tononi</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural correlates of consciousness: progress and problems</article-title><source>Nature Reviews Neuroscience</source><volume>17</volume><fpage>307</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.22</pub-id><pub-id pub-id-type="pmid">27094080</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konen</surname> <given-names>CS</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Two hierarchically organized neural systems for object information in human visual cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>224</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1038/nn2036</pub-id><pub-id pub-id-type="pmid">18193041</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konishi</surname> <given-names>M</given-names></name><name><surname>McLaren</surname> <given-names>DG</given-names></name><name><surname>Engen</surname> <given-names>H</given-names></name><name><surname>Smallwood</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Shaped by the past: the default mode network supports cognition that is independent of immediate perceptual input</article-title><source>PloS One</source><volume>10</volume><elocation-id>e0132209</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0132209</pub-id><pub-id pub-id-type="pmid">26125559</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>SH</given-names></name><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Disentangling visual imagery and perception of real-world objects</article-title><source>NeuroImage</source><volume>59</volume><fpage>4064</fpage><lpage>4073</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.055</pub-id><pub-id pub-id-type="pmid">22040738</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname> <given-names>CM</given-names></name><name><surname>Baldassarre</surname> <given-names>A</given-names></name><name><surname>Committeri</surname> <given-names>G</given-names></name><name><surname>Romani</surname> <given-names>GL</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Learning sculpts the spontaneous activity of the resting human brain</article-title><source>PNAS</source><volume>106</volume><fpage>17558</fpage><lpage>17563</lpage><pub-id pub-id-type="doi">10.1073/pnas.0902455106</pub-id><pub-id pub-id-type="pmid">19805061</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludmer</surname> <given-names>R</given-names></name><name><surname>Dudai</surname> <given-names>Y</given-names></name><name><surname>Rubin</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Uncovering camouflage: amygdala activation predicts long-term memory of induced perceptual insight</article-title><source>Neuron</source><volume>69</volume><fpage>1002</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.013</pub-id><pub-id pub-id-type="pmid">21382558</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margulies</surname> <given-names>DS</given-names></name><name><surname>Ghosh</surname> <given-names>SS</given-names></name><name><surname>Goulas</surname> <given-names>A</given-names></name><name><surname>Falkiewicz</surname> <given-names>M</given-names></name><name><surname>Huntenburg</surname> <given-names>JM</given-names></name><name><surname>Langs</surname> <given-names>G</given-names></name><name><surname>Bezgin</surname> <given-names>G</given-names></name><name><surname>Eickhoff</surname> <given-names>SB</given-names></name><name><surname>Castellanos</surname> <given-names>FX</given-names></name><name><surname>Petrides</surname> <given-names>M</given-names></name><name><surname>Jefferies</surname> <given-names>E</given-names></name><name><surname>Smallwood</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Situating the default-mode network along a principal gradient of macroscale cortical organization</article-title><source>PNAS</source><volume>113</volume><fpage>12574</fpage><lpage>12579</lpage><pub-id pub-id-type="doi">10.1073/pnas.1608282113</pub-id><pub-id pub-id-type="pmid">27791099</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mason</surname> <given-names>MF</given-names></name><name><surname>Norton</surname> <given-names>MI</given-names></name><name><surname>Van Horn</surname> <given-names>JD</given-names></name><name><surname>Wegner</surname> <given-names>DM</given-names></name><name><surname>Grafton</surname> <given-names>ST</given-names></name><name><surname>Macrae</surname> <given-names>CN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Wandering minds: the default network and stimulus-independent thought</article-title><source>Science</source><volume>315</volume><fpage>393</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1126/science.1131295</pub-id><pub-id pub-id-type="pmid">17234951</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mendoza-Halliday</surname> <given-names>D</given-names></name><name><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuronal population coding of perceived and memorized visual features in the lateral prefrontal cortex</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15471</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15471</pub-id><pub-id pub-id-type="pmid">28569756</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mumford</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>On the computational architecture of the neocortex. II. the role of cortico-cortical loops</article-title><source>Biological Cybernetics</source><volume>66</volume><fpage>241</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1007/BF00198477</pub-id><pub-id pub-id-type="pmid">1540675</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>C</given-names></name><name><surname>Jefferies</surname> <given-names>E</given-names></name><name><surname>Rueschemeyer</surname> <given-names>SA</given-names></name><name><surname>Sormaz</surname> <given-names>M</given-names></name><name><surname>Wang</surname> <given-names>HT</given-names></name><name><surname>Margulies</surname> <given-names>DS</given-names></name><name><surname>Smallwood</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distant from input: evidence of regions within the default mode network supporting perceptually-decoupled and conceptually-guided cognition</article-title><source>NeuroImage</source><volume>171</volume><fpage>393</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.017</pub-id><pub-id pub-id-type="pmid">29339310</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panagiotaropoulos</surname> <given-names>TI</given-names></name><name><surname>Deco</surname> <given-names>G</given-names></name><name><surname>Kapoor</surname> <given-names>V</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuronal discharges and gamma oscillations explicitly reflect visual consciousness in the lateral prefrontal cortex</article-title><source>Neuron</source><volume>74</volume><fpage>924</fpage><lpage>935</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.013</pub-id><pub-id pub-id-type="pmid">22681695</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname> <given-names>JD</given-names></name><name><surname>Cohen</surname> <given-names>AL</given-names></name><name><surname>Nelson</surname> <given-names>SM</given-names></name><name><surname>Wig</surname> <given-names>GS</given-names></name><name><surname>Barnes</surname> <given-names>KA</given-names></name><name><surname>Church</surname> <given-names>JA</given-names></name><name><surname>Vogel</surname> <given-names>AC</given-names></name><name><surname>Laumann</surname> <given-names>TO</given-names></name><name><surname>Miezin</surname> <given-names>FM</given-names></name><name><surname>Schlaggar</surname> <given-names>BL</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional network organization of the human brain</article-title><source>Neuron</source><volume>72</volume><fpage>665</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.09.006</pub-id><pub-id pub-id-type="pmid">22099467</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raichle</surname> <given-names>ME</given-names></name><name><surname>MacLeod</surname> <given-names>AM</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Powers</surname> <given-names>WJ</given-names></name><name><surname>Gusnard</surname> <given-names>DA</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A default mode of brain function</article-title><source>PNAS</source><volume>98</volume><fpage>676</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1073/pnas.98.2.676</pub-id><pub-id pub-id-type="pmid">11209064</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlack</surname> <given-names>A</given-names></name><name><surname>Albright</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Remembering visual motion: neural correlates of associative plasticity and motion recall in cortical area MT</article-title><source>Neuron</source><volume>53</volume><fpage>881</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.02.028</pub-id><pub-id pub-id-type="pmid">17359922</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sestieri</surname> <given-names>C</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Romani</surname> <given-names>GL</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Episodic memory retrieval, parietal cortex, and the default mode network: functional and topographic analyses</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>4407</fpage><lpage>4420</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3335-10.2011</pub-id><pub-id pub-id-type="pmid">21430142</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapira-Lichter</surname> <given-names>I</given-names></name><name><surname>Oren</surname> <given-names>N</given-names></name><name><surname>Jacob</surname> <given-names>Y</given-names></name><name><surname>Gruberger</surname> <given-names>M</given-names></name><name><surname>Hendler</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Portraying the unique contribution of the default mode network to internally driven mnemonic processes</article-title><source>PNAS</source><volume>110</volume><fpage>4950</fpage><lpage>4955</lpage><pub-id pub-id-type="doi">10.1073/pnas.1209888110</pub-id><pub-id pub-id-type="pmid">23479650</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silson</surname> <given-names>EH</given-names></name><name><surname>Chan</surname> <given-names>AW</given-names></name><name><surname>Reynolds</surname> <given-names>RC</given-names></name><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A retinotopic basis for the division of High-Level scene processing between lateral and ventral human occipitotemporal cortex</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>11921</fpage><lpage>11935</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0137-15.2015</pub-id><pub-id pub-id-type="pmid">26311774</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silson</surname> <given-names>EH</given-names></name><name><surname>Steel</surname> <given-names>AD</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Scene-Selectivity and retinotopy in medial parietal cortex</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>412</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00412</pub-id><pub-id pub-id-type="pmid">27588001</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname> <given-names>KD</given-names></name><name><surname>Fawcett</surname> <given-names>IP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Transient and linearly graded deactivation of the human default-mode network by a visual detection task</article-title><source>NeuroImage</source><volume>41</volume><fpage>100</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.01.051</pub-id><pub-id pub-id-type="pmid">18375149</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spreng</surname> <given-names>RN</given-names></name><name><surname>DuPre</surname> <given-names>E</given-names></name><name><surname>Selarka</surname> <given-names>D</given-names></name><name><surname>Garcia</surname> <given-names>J</given-names></name><name><surname>Gojkovic</surname> <given-names>S</given-names></name><name><surname>Mildner</surname> <given-names>J</given-names></name><name><surname>Luh</surname> <given-names>WM</given-names></name><name><surname>Turner</surname> <given-names>GR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Goal-congruent default network activity facilitates cognitive control</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>14108</fpage><lpage>14114</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2815-14.2014</pub-id><pub-id pub-id-type="pmid">25319706</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spreng</surname> <given-names>RN</given-names></name><name><surname>Grady</surname> <given-names>CL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Patterns of brain activity supporting autobiographical memory, prospection, and theory of mind, and their relationship to the default mode network</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>1112</fpage><lpage>1123</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21282</pub-id><pub-id pub-id-type="pmid">19580387</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>C</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Expectation in perceptual decision making: neural and computational mechanisms</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>745</fpage><lpage>756</lpage><pub-id pub-id-type="doi">10.1038/nrn3838</pub-id><pub-id pub-id-type="pmid">25315388</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>C</given-names></name><name><surname>Egner</surname> <given-names>T</given-names></name><name><surname>Greene</surname> <given-names>M</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name><name><surname>Mangels</surname> <given-names>J</given-names></name><name><surname>Hirsch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Predictive codes for forthcoming perception in the frontal cortex</article-title><source>Science</source><volume>314</volume><fpage>1311</fpage><lpage>1314</lpage><pub-id pub-id-type="doi">10.1126/science.1132028</pub-id><pub-id pub-id-type="pmid">17124325</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teufel</surname> <given-names>C</given-names></name><name><surname>Subramaniam</surname> <given-names>N</given-names></name><name><surname>Dobler</surname> <given-names>V</given-names></name><name><surname>Perez</surname> <given-names>J</given-names></name><name><surname>Finnemann</surname> <given-names>J</given-names></name><name><surname>Mehta</surname> <given-names>PR</given-names></name><name><surname>Goodyer</surname> <given-names>IM</given-names></name><name><surname>Fletcher</surname> <given-names>PC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Shift toward prior knowledge confers a perceptual advantage in early psychosis and psychosis-prone healthy individuals</article-title><source>PNAS</source><volume>112</volume><fpage>13401</fpage><lpage>13406</lpage><pub-id pub-id-type="doi">10.1073/pnas.1503916112</pub-id><pub-id pub-id-type="pmid">26460044</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trapp</surname> <given-names>S</given-names></name><name><surname>Bar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prediction, context, and competition in visual recognition</article-title><source>Annals of the New York Academy of Sciences</source><volume>1339</volume><fpage>190</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1111/nyas.12680</pub-id><pub-id pub-id-type="pmid">25728836</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Auerbach</surname> <given-names>EJ</given-names></name><name><surname>Olman</surname> <given-names>C</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Uğurbil</surname> <given-names>K</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>T1 weighted brain images at 7 tesla unbiased for proton density, T2* contrast and RF coil receive B1 sensitivity with simultaneous vessel visualization</article-title><source>NeuroImage</source><volume>46</volume><fpage>432</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.02.009</pub-id><pub-id pub-id-type="pmid">19233292</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Loon</surname> <given-names>AM</given-names></name><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name><name><surname>van der Velde</surname> <given-names>B</given-names></name><name><surname>Lirk</surname> <given-names>PB</given-names></name><name><surname>Vulink</surname> <given-names>NC</given-names></name><name><surname>Hollmann</surname> <given-names>MW</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>NMDA receptor antagonist ketamine distorts object recognition by reducing feedback to early visual cortex</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>1986</fpage><lpage>1996</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv018</pub-id><pub-id pub-id-type="pmid">25662715</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vatansever</surname> <given-names>D</given-names></name><name><surname>Menon</surname> <given-names>DK</given-names></name><name><surname>Manktelow</surname> <given-names>AE</given-names></name><name><surname>Sahakian</surname> <given-names>BJ</given-names></name><name><surname>Stamatakis</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Default mode dynamics for global functional integration</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>15254</fpage><lpage>15262</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2135-15.2015</pub-id><pub-id pub-id-type="pmid">26586814</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>M</given-names></name><name><surname>Arteaga</surname> <given-names>D</given-names></name><name><surname>He</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain mechanisms for simple perception and bistable perception</article-title><source>PNAS</source><volume>110</volume><fpage>E3350</fpage><lpage>E3359</lpage><pub-id pub-id-type="doi">10.1073/pnas.1221945110</pub-id><pub-id pub-id-type="pmid">23942129</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuille</surname> <given-names>A</given-names></name><name><surname>Kersten</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Vision as Bayesian inference: analysis by synthesis?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>301</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.05.002</pub-id><pub-id pub-id-type="pmid">16784882</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.36068.029</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Culham</surname><given-names>Jody C</given-names></name><role>Reviewing Editor</role><aff><institution>University of Western Ontario</institution><country>Canada</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Content-specific activity in frontoparietal and default-mode networks during prior-guided visual perception&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Jody C Culham as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by David Van Essen as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Jonathan Smallwood (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The reviewers agreed that your paper was an elegant and well-conducted study showing that disambiguation of Mooney images makes their neural representations more similar to the respective unambiguous images and less similar to one another. These results are observed not only in visual regions but also the frontoparietal and default mode networks and suggest richer perceptual representations than may have been expected. A particular strength of the experiment is the very clever method employed which enables the differentiation between meaningful and non meaningful stimuli in a manner that is not confounded with differences in stimulus properties. A fascinating aspect of the results is that they show regions in transmodal cortex, including the default mode network, play an important role in the processing of meaning from the external information. Moreover, while other studies have examined the transitions from analyzing perceptual features in visual areas to semantic features in higher areas, the absence of any particular semantic task other than naming suggests that fronto-parietal and default mode networks access semantics by default.</p><p>Essential revisions:</p><p>Two substantive comments were raised by the reviewers:</p><p>1) Two reviewers raised concerns about the inability to differentiate disambiguation from repetition suppression. This concern will need to be addressed by additional analysis or perhaps stronger argumentation.</p><p>One reviewer stated:</p><p>I have one substantive concern. The paradigm relies on showing participants a set of Mooney images before and after disambiguation and examining representational similarity. However, given the phenomenon of repetition suppression (or adaptation or priming), it is difficult to discern the degree to which representational changes are due to disambiguation vs. mere repetition. The ideal solution would have been to include control images that were presented the same number of times but never disambiguated. In the absence of that, perhaps other analyses could be done to show that effects between Pre and Post are not observed between early Pre and late Pre or between early Post and late Post.</p><p>Another reviewer stated:</p><p>A major concern is that at least the results reported in Figures 3 and 4 are explainable by neural adaptation rather than the transition from uninterpreted to interpreted. The most clear case is the greater dissimilarity of Post and Gray images compared to Pre images (Figures 3C and 4C). Adaptation between two images will reduce activity of the neural populations they have in common and leave the contrastive neural populations that do not commonly represent them. The Post Mooney images receive adaptation from the contrasting Post images in their run, the same contrasting images in the previous run (which were Pre images at the time), and presumably quite a bit of adaptation from the contrasting Gray images as well. The relevant Pre images from the previous run had only received adaptation from the contrasting Pre images that shared the same run. Thus the non-contrastive neurons representing the Post images should have been much more adapted than the non-contrastive neurons representing the Pre images, predicting greater dissimilarity among the Post images than the Pre images (Figures 3D and 4D). It is likely that the same could be said for the Gray images. To the degree that similar contrastive neurons are preserved in the Gray and Post images, it seems at least plausible that the same adaptation effects could make the Gray and Post images more similar to each other than they are the Pre images (Figures 3E and 4E). Regarding whether these same effects would increase dimensionality of the MDS (Figure 6), I don't know. It would certainly &quot;sparsen&quot; the patterns, which would seem to suggest the opposite reduction of dimensionality rather than the increase that was observed, but I think that is speculative when talking about such big complicated patterns.</p><p>2) One reviewer raised concerns about non-independence issues in the selection of FPN and DMN ROIs. The strongest rebuttal here would be a demonstration that the results hold with a fully independent definition of these ROIs (e.g., based on anatomy, Neurosynth, etc.) At minimum, the issue must be discussed and defended.</p><p>One possible concern is the rather piecemeal methods of selecting the FPN and DMN ROIs. The DMN was selected from a GLM contrast map (Post recognized vs. Pre recognized) and the FPN was selected from a whole brain SVM contrast between the same conditions. Because they didn't localize the networks with an independent resting state localizer, I'm assuming that these ROIs fell out of various explorations of the data. As far as I can tell, only one analysis in the paper is directly contaminated by double dipping. Figure 4D shows that similarity between Pre and Post Mooney images is less than similarity between Post and Gray scale. Similarity between Pre and Post (third bar) is artificially decreased because the FPN was selected using the SVM contrasting Pre and Post, selecting the regions where these conditions are least similar. The GLM contrast used to localize the DMN might also decrease the similarity between Pre and Post, but I'm not as sure about that one – maybe an RSA maven could help me out. It would at least be good to discuss these issues in the paper: 1) why were different contrasts required to locate the DMN and FPN and 2) acknowledge double dipping in Figure 4D. I think the paper would be strengthened by repeating the analyses using anatomical definitions of DMN and FPN.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Content-specific activity in frontoparietal and default-mode networks during prior-guided visual perception&quot; for consideration by <italic>eLife</italic>. Your resubmission has been reviewed by the Reviewing Editor, and the evaluation has been overseen by Eve Marder as the Senior Editor.</p><p>Although the reply to reviewers satisfactorily addressed the concerns raised by the reviewers, track changes showed no changes based on the major concerns. The two major concerns should be addressed directly in the manuscript. This is particularly true for the first comment – if two reviewers raised the same point, it could very well be a concern of other readers. The responses in the text do not have to be extensive as in the reply (but at least a brief statement addressing the crux of the concerns should appear in the main manuscript).</p><p>Once this correction has been made, the manuscript can proceed promptly to publication.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.36068.030</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Two substantive comments were raised by the reviewers:</p><p>1) Two reviewers raised concerns about the inability to differentiate disambiguation from repetition suppression. This concern will need to be addressed by additional analysis or perhaps stronger argumentation.</p><p>One reviewer stated:</p><p>I have one substantive concern. The paradigm relies on showing participants a set of Mooney images before and after disambiguation and examining representational similarity. However, given the phenomenon of repetition suppression (or adaptation or priming), it is difficult to discern the degree to which representational changes are due to disambiguation vs. mere repetition. The ideal solution would have been to include control images that were presented the same number of times but never disambiguated. In the absence of that, perhaps other analyses could be done to show that effects between Pre and Post are not observed between early Pre and late Pre or between early Post and late Post.</p><p>Another reviewer stated:</p><p>A major concern is that at least the results reported in Figures 3 and 4 are explainable by neural adaptation rather than the transition from uninterpreted to interpreted. The most clear case is the greater dissimilarity of Post and Gray images compared to Pre images (Figures 3C and 4C). Adaptation between two images will reduce activity of the neural populations they have in common and leave the contrastive neural populations that do not commonly represent them. The Post Mooney images receive adaptation from the contrasting Post images in their run, the same contrasting images in the previous run (which were Pre images at the time), and presumably quite a bit of adaptation from the contrasting Gray images as well. The relevant Pre images from the previous run had only received adaptation from the contrasting Pre images that shared the same run. Thus the non-contrastive neurons representing the Post images should have been much more adapted than the non-contrastive neurons representing the Pre images, predicting greater dissimilarity among the Post images than the Pre images (Figures 3D and 4D). It is likely that the same could be said for the Gray images. To the degree that similar contrastive neurons are preserved in the Gray and Post images, it seems at least plausible that the same adaptation effects could make the Gray and Post images more similar to each other than they are the Pre images (Figures 3E and 4E). Regarding whether these same effects would increase dimensionality of the MDS (Figure 6), I don't know. It would certainly &quot;sparsen&quot; the patterns, which would seem to suggest the opposite reduction of dimensionality rather than the increase that was observed, but I think that is speculative when talking about such big complicated patterns.</p></disp-quote><p>We thank the reviewers for raising this important point. In order to control for the effect of repetition, we repeated the 1<sup>st</sup>-order RSA analysis while considering the subjects’ recognition status for each Mooney image in each presentation stage. First, for each subject, we selected the set of Mooney images that were unrecognized in the pre-disambiguation stage and yet recognized in the post-disambiguation stage (“Disambiguation set”), similar to the disambiguation effect assessed in the GLM and MVPA analyses shown in Figure 2. Second, for each subject, we selected the set of Mooney images that were <italic>recognized</italic> in both pre- and post-disambiguation stages (“Repetition set”). Importantly, in both cases, for each participant, the same set of Mooney images were used for Pre and Post conditions (group-mean and s.d. for the number of distinct Mooney images included in each set: Disambiguation set, 14 ± 3; Repetition set, 14 ± 5). Thus, the repetition set includes identical Mooney images that were recognized in both pre- and post-disambiguation stages, whereas the disambiguation set includes identical Mooney images that underwent a salient change in recognition status before vs. after disambiguation. <italic>Since both sets of images underwent identical sequence of repetition, this allowed us to test whether the change in recognition status had an effect above and beyond that of repetition on the neural activity</italic>.</p><p>To this end, we performed the following repeated-measures ANOVAs with three factors: Set (image-set type: Disambiguation vs. Repetition), Condition (Pre- vs. Post-disambiguation presentation stage), and Network (Visual, LOC, FG, FPN, and DMN, as in Figure 6D). The dependent variables were measures from 1<sup>st</sup>-order RSA as assessed in Figure 3D-E and Figure 4C-D. Before being entered in the ANOVA, r values of the RDMs were transformed to z-values, using the Fisher’s transform.</p><p>First, we compared the increase in between-image dissimilarity from Pre to Post stage (cyan brackets in Figures 3 and 4) in both data sets. This ANOVA revealed a clear effect of Condition (F<sub>1,14</sub> = 24.3, <italic>p</italic> &lt; 0.001), and, crucially, a Set x Condition interaction (F<sub>1,14</sub> = 4.6, <italic>p</italic> = 0.05). Post-hoc comparisons revealed that the Pre-to-Post increase in dissimilarity was stronger in the Disambiguation Set (F<sub>1</sub> = 24.9), compared to the Recognition set (F<sub>1</sub> = 10.7).</p><p>Second, when comparing the increase in similarity from Pre-Gray to Post-Gray (red brackets in Figures 3 and 4), the same interaction was found (Set x Condition, F<sub>1,14</sub> =14.9, <italic>p</italic> = 0.002). Again, post-hoc comparisons revealed the increase was larger in the Disambiguation set (F<sub>1</sub> = 96.3) than in the Repetition set (F<sub>1</sub> = 18.9).</p><p>Third, when comparing the difference in similarity between Pre-Post and Post-Gray (green brackets in Figures 3 and 4), we found the same interaction effect (Set x Condition, F<sub>1,14</sub> = 6.3, <italic>p</italic> = 0.02). As in the previous analyses, the difference between Pre-Post and Post-Gray was larger in the Disambiguation set (F<sub>1</sub> = 71.2) than in the Repetition set (F<sub>1</sub> = 25.6).</p><p>Together, these analyses reveal that, as suggested by the reviewers, repetition has an effect on neural activity. <italic>However, they demonstrate that disambiguation (i.e., change in recognition status of Mooney images due to viewing the corresponding Gray-scale images) influences neural representations above and beyond effects expected by mere repetition.</italic></p><p>Last, the first reviewer mentioned the interesting idea of including control images that were presented the same number of times but never disambiguated. Due to the long duration of our experiment, which was already pushing the limit of subjects’ tolerability in the 7T scanner (~2 hr), we could not include control images in this experiment. However, we would like to mention that in a separate magnetoencephalography (MEG) experiment from our lab (Flounders et al., in prep), using a similar paradigm and the same 33 Mooney images as studied here, plus control Mooney images that were presented in the same manner but shown with non-matching gray-scale images (and thus never disambiguated), we found a similar pattern of disambiguation-related effects on both behavior and neural activity in the regular images, which did not extend to control images.</p><disp-quote content-type="editor-comment"><p>2) One reviewer raised concerns about non-independence issues in the selection of FPN and DMN ROIs. The strongest rebuttal here would be a demonstration that the results hold with a fully independent definition of these ROIs (e.g., based on anatomy, Neurosynth, etc.) At minimum, the issue must be discussed and defended.</p><p>One possible concern is the rather piecemeal methods of selecting the FPN and DMN ROIs. The DMN was selected from a GLM contrast map (Post recognized vs. Pre recognized) and the FPN was selected from a whole brain SVM contrast between the same conditions. Because they didn't localize the networks with an independent resting state localizer, I'm assuming that these ROIs fell out of various explorations of the data. As far as I can tell, only one analysis in the paper is directly contaminated by double dipping. Figure 4D shows that similarity between Pre and Post Mooney images is less than similarity between Post and Gray scale. Similarity between Pre and Post (third bar) is artificially decreased because the FPN was selected using the SVM contrasting pre and post, selecting the regions where these conditions are least similar. The GLM contrast used to localize the DMN might also decrease the similarity between Pre and Post, but I'm not as sure about that one – maybe an RSA maven could help me out. It would at least be good to discuss these issues in the paper: 1) why were different contrasts required to locate the DMN and FPN 2) acknowledge double dipping in Figure 4D. I think the paper would be strengthened by repeating the analyses using anatomical definitions of DMN and FPN.</p></disp-quote><p>We have now repeated the analyses shown in Figure 4 using FPN and DMN ROIs defined in an independent resting-state data set (Power et al., 2011; as shown in Figure2—figure supplement 4D). The new results are presented in <xref ref-type="fig" rid="respfig1">Author response image 1</xref> and are nearly identical to the original results in Figure 4. This figure includes right-hemisphere and mid-line ROIs; results from left-hemisphere ROIs were also nearly identical to the original results shown in Figure4—figure supplement 1. Although not requested by the reviewer, we have also double checked the results presented in Figures 5, 6, 7 using these independently defined FPN and DMN ROIs, all of which were very similar to the original results.</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.36068.025</object-id><label>Author response image 1.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-36068-resp-fig1-v1"/></fig><p>Given the above results, and since both FPN and DMN are functionally heterogeneous networks (e.g., Andrews-Hanna et al., 2010), we have opted to retain the original figures in the manuscript using ROIs defined from independent analyses on the same data set, which should extract regions that are more specifically involved in this task.</p><p>To answer the reviewer’s question about why different contrasts were used to extract DMN and FPN ROIs: as we mention in the text, changes in the overall activation magnitude between pre- and post-disambiguation Mooney images were previously reported in DMN regions (Dolan et al., 1997; Gorlin et al., 2012), and thus this finding was expected. By contrast, MVPA probes voxel-wise activation pattern – information that is complementary to the overall activation magnitude. Using MVPA, we found significant decoding of presentation stage (pre- vs. post-disambiguation) in regions of the FPN – a novel finding of the present study. Neither of these two analyses investigated information encoding specific to individual images, which was the subject of the following RSA analyses.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Although the reply to reviewers satisfactorily addressed the concerns raised by the reviewers, track changes showed no changes based on the major concerns. The two major concerns should be addressed directly in the manuscript. This is particularly true for the first comment – if two reviewers raised the same point, it could very well be a concern of other readers. The responses in the text do not have to be extensive as in the reply (but at least a brief statement addressing the crux of the concerns should appear in the main manuscript.</p><p>Once this correction has been made, the manuscript can proceed promptly to publication.</p></disp-quote><p>We have now addressed both major concerns in the main text of the manuscript – major concern #1: subsection “Disambiguation shifts neural representations toward the prior throughout the cortical hierarchy”; major concern #2: subsections “Disambiguation leads to widespread changes in neural activity” and “Region-of-interest (ROI) definition”.</p></body></sub-article></article>