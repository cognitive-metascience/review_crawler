<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">54687</article-id><article-id pub-id-type="doi">10.7554/eLife.54687</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Efficient recognition of facial expressions does not require motor simulation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-169287"><name><surname>Vannuscorps</surname><given-names>Gilles</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5686-7349</contrib-id><email>gilles.vannuscorps@uclouvain.be</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-173888"><name><surname>Andres</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-173889"><name><surname>Caramazza</surname><given-names>Alfonso</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, Harvard University</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Institute of Neuroscience, Université catholique de Louvain</institution><addr-line><named-content content-type="city">Ottignies-Louvain-la-Neuve</named-content></addr-line><country>Belgium</country></aff><aff id="aff3"><label>3</label><institution>Psychological Sciences Research Institute, Université catholique de Louvain</institution><addr-line><named-content content-type="city">Ottignies-Louvain-la-Neuve</named-content></addr-line><country>Belgium</country></aff><aff id="aff4"><label>4</label><institution>Center for Mind/Brain Sciences, Università degli Studi di Trento</institution><addr-line><named-content content-type="city">Mattarello</named-content></addr-line><country>Italy</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Senior Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>04</day><month>05</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e54687</elocation-id><history><date date-type="received" iso-8601-date="2019-12-22"><day>22</day><month>12</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-05-03"><day>03</day><month>05</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Vannuscorps et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Vannuscorps et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-54687-v2.pdf"/><abstract><p>What mechanisms underlie facial expression recognition? A popular hypothesis holds that efficient facial expression recognition cannot be achieved by visual analysis alone but additionally requires a mechanism of motor simulation — an unconscious, covert imitation of the observed facial postures and movements. Here, we first discuss why this hypothesis does not necessarily follow from extant empirical evidence. Next, we report experimental evidence against the central premise of this view: we demonstrate that individuals can achieve normotypical efficient facial expression recognition despite a congenital absence of relevant facial motor representations and, therefore, unaided by motor simulation. This underscores the need to reconsider the role of motor simulation in facial expression recognition.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>emotion</kwd><kwd>facial expression</kwd><kwd>visual recognition</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007229</institution-id><institution>Harvard University</institution></institution-wrap></funding-source><award-id>Mind, Brain and Behavior Interfaculty Initiative</award-id><principal-award-recipient><name><surname>Caramazza</surname><given-names>Alfonso</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100012943</institution-id><institution>Institut national de la recherche scientifique</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Andres</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>It is possible to account for efficient facial expression recognition without having to invoke a mechanism of motor simulation, even in very sensitive and challenging tasks.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Stereotyped facial movements – facial expressions – reveal people’s happiness, surprise, fear, anger, disgust, and sadness in a way that can be easily interpreted by their congeners (<xref ref-type="bibr" rid="bib27">Ekman, 1982</xref>). The ability to express and recognize facial expressions is crucial for social interactions (<xref ref-type="bibr" rid="bib28">Elfenbein et al., 2007</xref>). A fundamental issue addressed here concerns the nature of the mechanisms underlying facial expression recognition: what types of information are used in recognizing a facial expression in everyday life?</p><p>On one view, the efficient recognition of facial expressions relies on computations occurring in the visuo-perceptual system, supported by perceptual processes and information extrapolated from perceptual learning (<xref ref-type="bibr" rid="bib9">Bruce and Young, 1986</xref>; <xref ref-type="bibr" rid="bib24">Du et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">Huelle et al., 2014</xref>). Three types of representations are necessary and sufficient for efficient visual recognition of facial expression: (1) a series of progressively more abstract visuo-perceptual representations of the postures and movements of the observed face; (2) stored structural descriptions of the features characterizing known facial expressions; and (3) the conceptual/semantic representations that characterize the facial expressions (e.g., <xref ref-type="bibr" rid="bib9">Bruce and Young, 1986</xref>). Of course, viewing facial expressions may bring about other cognitive, affective and motor processes, involving episodic memories, empathy or imitation, but these other types of processes and representations are not necessary for efficient recognition of facial expressions.</p><p>In the last 20 years, an alternative view suggesting that efficient (i.e., fast and accurate) facial expression recognition cannot be achieved by visual analysis alone but requires a process of motor simulation – an unconscious, covert imitation of the observed facial postures or movements – has gained considerable prominence (<xref ref-type="bibr" rid="bib32">Goldman and Sripada, 2005</xref>; <xref ref-type="bibr" rid="bib38">Ipser and Cook, 2016</xref>; <xref ref-type="bibr" rid="bib50">Montgomery and Haxby, 2008</xref>; <xref ref-type="bibr" rid="bib53">Niedenthal et al., 2010</xref>; <xref ref-type="bibr" rid="bib58">Pitcher et al., 2008</xref>; <xref ref-type="bibr" rid="bib57">Paracampo et al., 2017</xref>). This ‘motor’ view has become increasingly influential in neuroscience, philosophy, neurology and psychiatry where it is suggested that it opens new clinical perspectives for the diagnosis, understanding and rehabilitation of clinical populations presenting with facial expression recognition deficits such as Parkinson’s disease (<xref ref-type="bibr" rid="bib62">Ricciardi et al., 2017</xref>), autism (<xref ref-type="bibr" rid="bib18">Dapretto et al., 2006</xref>; <xref ref-type="bibr" rid="bib33">Gordon et al., 2014</xref>), and schizophrenia (<xref ref-type="bibr" rid="bib71">Torregrossa et al., 2019</xref>).</p><p>Three main types of evidence are typically cited in support of the motor theories. First, neuroimaging studies and studies combining transcranial magnetic stimulation (TMS) of the motor cortex and electromyographic recording of facial muscles have reported that viewing a facial expression activates parts of the motor system, which are also involved in executing that facial expression (<xref ref-type="bibr" rid="bib12">Carr et al., 2003</xref>; <xref ref-type="bibr" rid="bib20">Dimberg, 1982</xref>; <xref ref-type="bibr" rid="bib21">Dimberg et al., 2000</xref>; <xref ref-type="bibr" rid="bib36">Hess and Blairy, 2001</xref>; <xref ref-type="bibr" rid="bib43">Leslie et al., 2004</xref>; <xref ref-type="bibr" rid="bib50">Montgomery and Haxby, 2008</xref>). Second, performance in tasks involving facial expression recognition can be modulated by experimental manipulations of the normal state of the observers’ motor system, such as concurrent motor tasks or transcranial magnetic stimulation (TMS) over the motor cortex (<xref ref-type="bibr" rid="bib38">Ipser and Cook, 2016</xref>; <xref ref-type="bibr" rid="bib46">Maringer et al., 2011</xref>; <xref ref-type="bibr" rid="bib54">Oberman et al., 2007</xref>; <xref ref-type="bibr" rid="bib57">Paracampo et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Ponari et al., 2012</xref>; <xref ref-type="bibr" rid="bib66">Rychlowska et al., 2014</xref>; <xref ref-type="bibr" rid="bib83">Wood et al., 2016</xref>). Third, studies of individuals suffering from congenital or acquired facial expression production disorders have been reported to have difficulties in recognizing facial expressions. Several studies have reported, for instance, that individuals with Moebius Syndrome, an extremely rare congenital nonprogressive condition (<xref ref-type="bibr" rid="bib81">Verzijl et al., 2003</xref>) resulting in facial paralysis (usually complete and bilateral), scored below average in facial expression recognition experiments (<xref ref-type="bibr" rid="bib8">Bate et al., 2013</xref>; <xref ref-type="bibr" rid="bib11">Calder et al., 2000</xref>; <xref ref-type="bibr" rid="bib30">Giannini et al., 1984</xref>; <xref ref-type="bibr" rid="bib52">Nicolini et al., 2019</xref>). Co-occurrence of facial expression production and recognition disorders have also been reported in Parkinson’s (<xref ref-type="bibr" rid="bib62">Ricciardi et al., 2017</xref>) and in Huntington’s disease (<xref ref-type="bibr" rid="bib72">Trinkler et al., 2013</xref>).</p><p>However, these various findings are open to alternative explanations. The activation of the motor system during the observation of facial expressions could result from a mere reaction (e.g., emotional contagion or a way to signal empathy) of the observer to others’ facial expression <italic>after</italic> it has been recognized as an instance of a given emotion (<xref ref-type="bibr" rid="bib35">Hess et al., 2014</xref>). It may also result from visuo-motor transformations serving other purposes than facial expression recognition such as working memory encoding (<xref ref-type="bibr" rid="bib78">Vannuscorps and Caramazza, 2016a</xref>) or mimicry in order to foster affiliative goals (<xref ref-type="bibr" rid="bib29">Fischer and Hess, 2017</xref>).</p><p>The demonstration that interfering with the observer’s motor system may influence facial expression recognition performance in some tasks clearly shows that the motor and visual systems are functionally connected. However, the conclusion that these findings demonstrate that the motor system is necessary for efficient facial expression recognition faces two main problems. First, the results do not imply that the motor and the visual systems are functionally connected for the purpose of facial expression recognition. TMS applied to an area may have distant effects on other areas to which it projects (<xref ref-type="bibr" rid="bib56">Papeo et al., 2015</xref>; <xref ref-type="bibr" rid="bib65">Ruff et al., 2009</xref>; <xref ref-type="bibr" rid="bib67">Siebner et al., 2009</xref>; <xref ref-type="bibr" rid="bib68">Siebner and Rothwell, 2003</xref>). The motor cortex projects to the visual cortex involved in the perception of biological motion in order to support the control of one’s body movements (<xref ref-type="bibr" rid="bib82">Wolpert et al., 2003</xref>) and the visual cortex involved in the perception of body parts shows increased activity when moving these body parts even in the absence of visual feedback (<xref ref-type="bibr" rid="bib5">Astafiev et al., 2004</xref>; <xref ref-type="bibr" rid="bib22">Dinstein et al., 2007</xref>; <xref ref-type="bibr" rid="bib55">Orlov et al., 2010</xref>). Thus, TMS applied to the motor system may have functional effects upon facial expression recognition by modulating the visuo-perceptual system to which it is naturally connected to support the control of one’s movements. Second, and more importantly, evidence that information in one system (e.g., the motor system) may influence computations in another system (e.g., the visual system) is not evidence that the former is necessary for the latter to function efficiently. For instance, the finding that visual information about lip movements influences auditory speech perception does not imply that auditory speech perception requires the visual system and has never been presented as a proof of this (<xref ref-type="bibr" rid="bib47">McGurk and MacDonald, 1976</xref>). Likewise, results showing that auditory information affects the efficiency of facial expression recognition have been interpreted as evidence that available auditory or context information can influence facial expression recognition, but not that the auditory system is critical for efficient facial expression recognition (<xref ref-type="bibr" rid="bib15">Collignon et al., 2008</xref>). Thus, although the effects of TMS and behavioral motor interference provide interesting information about perceptual and motor interactions, they may very likely originate from outside the set of cognitive and neural mechanisms that are necessary for efficient facial expression recognition. Thus, the behavioral, fMRI and TMS results reported above are not critical to discriminate between the perceptual and motor view because both views can equally account for the finding that interfering with the motor system can modulate the efficiency with which facial expressions are recognized.</p><p>Equally indeterminate is the evidential value of the reported co-occurrence of deficits of facial expression production and recognition. Such co-occurrence indicates that a relationship exists between production and perception abilities, but on their own such instances do not imply that there is a causal relationship between them. The interpretation of these associations is all the more difficult given that most patients with Parkinson’s and Huntington’s disease have widespread brain lesions and suffer from cognitive disorders like visuo-perceptual or executive function disorders (<xref ref-type="bibr" rid="bib62">Ricciardi et al., 2017</xref>; <xref ref-type="bibr" rid="bib72">Trinkler et al., 2013</xref>), which may be independently responsible for poor performance in facial expression recognition. In line with this possibility, in a study of 108 patients with focal brain lesions, <xref ref-type="bibr" rid="bib1">Adolphs et al., 2000</xref> found no association between the ability to recognize facial expressions and motor lesions/impairment. Likewise, Moebius syndrome typically impacts not only the individuals’ sensorimotor system, but also their visual, perceptual, cognitive, and social abilities (<xref ref-type="bibr" rid="bib8">Bate et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Carta et al., 2011</xref>; <xref ref-type="bibr" rid="bib31">Gillberg and Steffenburg, 1989</xref>; <xref ref-type="bibr" rid="bib39">Johansson et al., 2007</xref>). Therefore, it is not clear whether the facial expression recognition deficit observed in these cases was the direct consequence of the production disorder or, instead, the result of other impaired but functionally separate processes. In a study by <xref ref-type="bibr" rid="bib8">Bate et al., 2013</xref>, for instance, five out of six participants with Moebius Syndrome were impaired in at least one of three facial expression recognition tasks. It may be tempting to conclude that this finding supports motor theories. However, the same five individuals were also impaired in their ability to recognize facial identity and/or in tests assessing low-level vision and object recognition indicating that the facial expression recognition impairment cannot unambiguously be associated with the facial paralysis.</p><p>In favor of the view that recognition does not require motor simulation, there are reports of individuals with Moebius Syndrome (IMS) who achieve a normal level of performance in facial expression recognition despite their congenital facial paralysis (<xref ref-type="bibr" rid="bib8">Bate et al., 2013</xref>; <xref ref-type="bibr" rid="bib11">Calder et al., 2000</xref>; <xref ref-type="bibr" rid="bib63">Rives Bogart and Matsumoto, 2010</xref>). For instance, in contrast to the five other MS participants from Bate et al.’s study (2103) cited above, one IMS (the only one who obtained normal scores in facial identity recognition, object recognition and low-level vision tests, MB4) performed very close to the average level of performance of the controls in three facial expression recognition tests (the Ekman 60 faces test, the Emotion Hexagon test, and the Reading the Mind in the Eyes test) despite having facial movements restricted to slight puckering of the mouth bilaterally. In <xref ref-type="bibr" rid="bib63">Rives Bogart and Matsumoto, 2010</xref> study, the performance of 37 IMS participants was comparable to that of 37 control participants in a task in which they viewed pictures of one of seven emotions (anger, contempt, disgust, fear, happiness, sadness, and surprise) and were asked to select the corresponding emotion from a list of the alternatives.</p><p>Although these findings undermine the hypothesis that facial expression recognition requires motor simulation, it has been suggested that a mild or subtle deficit in facial expression recognition may have gone undetected in these studies because they relied mostly on untimed picture labelling tasks (<xref ref-type="bibr" rid="bib19">De Stefani et al., 2019</xref>; <xref ref-type="bibr" rid="bib74">Van Rysewyk, 2011</xref>). In addition, these previous findings left open the possibility that motor simulation may contribute to facial expression recognition efficiency when the tasks are more challenging, such as when facial expressions are more complex, must be interpreted quickly, only partial information is available, or when the task requires subtle intra-category discriminations such as discriminating fake versus genuine smiles (<xref ref-type="bibr" rid="bib53">Niedenthal et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">Paracampo et al., 2017</xref>). The research reported here was designed to explore this large remaining hypothesis space for a role of motor simulation in facial expression recognition: is it <italic>possible</italic> to achieve efficient facial expression recognition without motor simulation in the type of sensitive and challenging tasks that have been cited by proponents of the motor theories as examples of tasks in which motor simulation is needed to support facial expression recognition?</p><p>We studied 11 individuals with Moebius Syndrome using an experimental procedure designed to overcome the sensitivity and interpretative issues that have been raised for previous reports of intact facial expression recognition abilities in IMS:</p><list list-type="order"><list-item><p>Given the heterogeneity of the clinical expression of Moebius Syndrome, especially in terms of associated visuo-perceptual symptoms (<xref ref-type="bibr" rid="bib8">Bate et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Carta et al., 2011</xref>), and the specific prediction of the motor simulation hypothesis tested in this study – viz., that none of the individuals with congenital facial paralysis should be as efficient as the controls in facial recognition – we conducted analyses focused on the performance of each IMS.</p></list-item> <list-item><p>To ensure the sensitivity of the facial expression recognition measures and the specificity of the assessment with respect to the predictions of the motor theories, we selected five challenging facial expression tasks that have been shown to be sensitive to even subtle facial expression recognition difficulties and/or that have been explicitly cited by proponents of the motor theories as examples of tasks in which motor simulation should support facial expression recognition (see Materials and Procedures for detail).</p></list-item> <list-item><p>To minimize the likelihood of false negatives, that is, the risk to conclude erroneously that an IMS achieves a ‘normotypical level’ of efficiency in a facial expression recognition experiment, we compared the performance of each IMS to that of 25 typically developed highly educated young adults and selected a high alpha level (p<italic>&gt;</italic>0.2), which set the threshold for ‘typically efficient’ performance in a given experiment to scores above 0.85 standard deviations below the mean of the controls <italic>after</italic> control participants with an abnormally low score were dismissed (i.e., a score below two standard deviations from the mean of the controls).</p></list-item> <list-item><p>To gain more power to detect even subtle degradations of facial expression recognition skills than in simple IMS versus controls comparison, we also examined each participant’s performance in two experiments assessing facial identity recognition abilities and in one experiment assessing the ability to recognize emotions from emotional speech. This allowed us to assess the impact of the inability to engage in motor simulation on facial expression recognition skills in a within-subject design, by comparing the IMS’s ability to recognize facial expressions versus facial identity and emotional speech recognition, in comparison to the controls’ performance for these tasks.</p></list-item> <list-item><p>To investigate the possibility that the IMS who achieve typically efficient performance in facial expression recognition could do so by completing the task through different means or somewhat differently than the control participants, we included planned qualitative analyses of the performance of the IMS who achieved quantitatively normotypical performance profiles.</p></list-item></list><p>Using the outlined experimental procedures, we established three criteria for concluding that an individual achieves ‘normotypically efficient facial expression recognition’: (1) s/he scores above 0.85 standard deviations below the mean of the controls in <italic>all</italic> the facial expression recognition tasks, (2) s/he does not perform significantly worse in these tasks than in other face-related visual tasks (face identity) and emotion recognition tasks (vocal emotion recognition), (3) and, s/he performs these tasks in a way that is qualitatively similar to the controls. If motor simulation were necessary for efficient facial expression recognition, then, none of the tested IMS should meet these criteria.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>The results of all participants are displayed in <xref ref-type="fig" rid="fig1">Figure 1</xref>. We conducted six series of analyses. We first conducted analyses to verify the sensitivity of the facial expression recognition tasks and the representativity of our sample of control participants. To this end, we first performed a series of one-sample <italic>t</italic>-tests to verify that the controls’ performance was significantly above chance and a series of Shapiro-Wilk tests to verify that the controls’ data were distributed normally in all the facial expression recognition experiments. The results of these analyses showed that the controls’ data were indeed significantly above chance (all <italic>t</italic>-tests &gt; 8) and normally distributed in all five facial expression experiments (all five Ws &gt; 0.930, all ps &gt; 0.13). Then, we compared the results of our control participants to those of comparable samples available in the literature. The aim of these analyses was to verify that our control sample was representative. Previous data were available for Experiments 3–5. One-sample <italic>t</italic>-tests indicated that our control participants’ average performance (M = 28.6; SD = 2.4) in Experiment four was significantly better than that reported in the only other study reporting the results of the same (French) version of the task (M = 24.8; SD = 3.8; <italic>t</italic> (24)=7.9; <xref ref-type="bibr" rid="bib61">Prevost et al., 2014</xref>). Our control group also performed slightly better (M = 63.04, SD = 10.22) than the average performance of the three comparable age groups reported in a previous study using the task of Experiment 3 (M = 62.6, SD = 9.6; <italic>t</italic> (24)&lt;0.2; <xref ref-type="bibr" rid="bib40">Kessels et al., 2014</xref>). Our control sample performed lower than the performance expected from <xref ref-type="bibr" rid="bib57">Paracampo et al., 2017</xref> norming study of the Amusement from Smiles experiment (66% in our study vs 75%). However, our experiment included only half the number of trials of the original experiment (<xref ref-type="bibr" rid="bib57">Paracampo et al., 2017</xref>) offering less perceptual learning opportunity to our participants (<xref ref-type="bibr" rid="bib37">Huelle et al., 2014</xref>). Taken together, the outcomes of these analyses thus provided confidence in the sensitivity of the tasks and in the representativity of the controls’ data.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Results of Experiments 1–8 by individual participant.</title><p>In black: control participants; in light grey: mean of the controls; in green and yellow: IMS 8 and 10 who performed Experiments 1–5 with normotypical efficiency; in red: the nine other IMS. A small circle (°) indicates IMS participants with a ‘normotypical’ score (at least above 0.85 standard deviation below the controls’ mean performance) after control participants with an abnormally low score (below 2 SD from the other control participants) was/were discarded (indicated by an *). An asterisk (*) also indicates IMS participants with a score below two standard deviations from the mean of the controls.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54687-fig1-v2.tif"/></fig><p>In a second step, we turned to our main question and investigated whether at least one IMS performed at a ‘normotypical’ level of performance (at least above 0.85 standard deviation below the controls’ mean performance) in the five facial expression experiments. To obtain a stringent test of performance equality, we first excluded from the control sample participants who performed below two standard deviations from the controls’ mean (one participant in Experiment one and one participant in Experiment 2, indicated by an * in <xref ref-type="fig" rid="fig1">Figure 1</xref>). Then, a series of <xref ref-type="bibr" rid="bib17">Crawford and Howell, 1998</xref> modified t-tests were used to test whether each IMS’s performance in the various facial expression recognition experiments was or was not ‘normotypical’ (indicated by an ° in <xref ref-type="fig" rid="fig1">Figure 1</xref>). These analyses indicated that IMS 8 and 10, displayed in green and yellow in <xref ref-type="fig" rid="fig1">Figure 1</xref>, performed the five experiments with normotypical efficiency.</p><p>Third, we focused on the two IMS who met the criteria for normotypical performance and assessed whether any discrepancy between their performance and that of the controls was larger on facial expression recognition (Experiments 1–5) than on facial identity (Experiments 6–7) and speech emotion recognition (Experiment 8). The aim of these analyses was to seek evidence that despite their normotypical performance on facial recognition tasks, IMS 8 and 10 may nevertheless be comparatively less good in facial expression recognition than in other tasks not assumed to rely on motor simulation. To this end, we computed Crawford and Garthwaite's Bayesian Standardized Difference Test (BSDT) (<xref ref-type="bibr" rid="bib16">Crawford and Garthwaite, 2007</xref>). The BSDT allows computing an estimate of the percentage of the control population exhibiting a more extreme discrepancy between two tasks than a given individual. We performed 15 BSDTs for each IMS (comparison of three control tasks and five facial expression recognition tasks of interest). All the comparisons were either clearly not significant (3/30 comparisons, all BSDTs &gt; 0.6) or indicated a comparatively better performance in facial expression recognition tasks than in the control tasks (27/30 comparisons). Thus, there was no evidence that IMS 8 and 10 performed facial expression recognition tasks less efficiently than facial identity recognition or emotional speech recognition tasks.</p><p>Fourth, we conducted a qualitative analysis of the performance of these two IMS (IMS 8, 10) in the facial expression recognition experiments. The aim of these analyses was to seek evidence that despite their quantitative normotypical performance, these IMS might have performed the task somewhat differently than the controls; for instance, that they used different facial diagnostic features. Any such processing differences would likely result in different patterns of behavioral responses to expressions and, in particular, in different patterns or errors. To explore this possibility, we computed response matrices between the six displayed facial expressions (in rows) and the six response alternatives (happiness, surprise, anger, sadness, fear, and disgust) for Experiments 1–3 separately for the control participants and for IMS 8 and 10 (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Then, to examine the similarity between the controls’ and the IMS’s matrices, we vectorized the matrices and correlated them with each other. The resulting correlation coefficients were very high both when all responses were considered (all three Pearson’s Rs (36)&gt;0.95; all <italic>p</italic>s &lt; 0.001) and when only errors were considered (all three Pearson’s Rs (30)&gt;0.63; all <italic>p</italic>s &lt; 0.001), indicating that the groups consistently confused the same set of alternatives. Note that the confusion matrices of the other IMS were also strongly correlated to that of the control participants (all three Pearson’s Rs (36)&gt;0.95; all <italic>p</italic>s &lt; 0.001; see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Confusion matrices.</title><p>(<bold>A, B, C</bold>) Distribution of control participants’ (left) and IMS 8 and 10’s (right) percentage of trials in which they chose each of the six response alternatives when faced with the six displayed facial expressions in Experiment 1 (<bold>A</bold>), 2 (<bold>B</bold>), and 3 (<bold>C</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54687-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Confusion matrices.</title><p>(<bold>A, B, C</bold>) Distribution of control participants’ (left), IMS 8 and 10’s (middle) and the other IMS’s (right) percentage of trials in which they chose each of the six response alternatives when faced with the six displayed facial expressions in Experiment 1 (<bold>A</bold>), 2 (<bold>B</bold>), and 3 (<bold>C</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54687-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Analysis of participants' action units when imitating facial expressions.</title><p>IMS 8, 10 and 5 control participants were asked to imitate pictures of an actor’s face expressing one of six facial expressions (anger, disgust, fear, happiness, sadness, and surprise). Their imitation was video-recorded and, then, analyzed offline with OpenFace 2.1.0, an open source deep learning facial recognition system allowing automatic detection of action unit (AU) presence and intensity (<xref ref-type="bibr" rid="bib4">Amos et al., 2016</xref>; <xref ref-type="bibr" rid="bib6">Baltrusaitis et al., 2016</xref>). For each participant and facial expression, we first computed the average intensity of 12 facial action units relevant for facial expressions (AU01, 02, 04, 05, 06, 07, 09, 12, 15, 20, 23 and 26). Then, to obtain a measure of the similarity between the different facial expressions executed by each participant, we correlated the intensity of the 12 facial action units observed for the different facial expressions to each other. This allowed obtaining an objective measure of the similarity of the intensities of the different relevant action units when the IMS executed the different facial expressions. The results indicate that the intensity of the action units during the execution of the different facial expressions were highly correlated in the three IMS and much more correlated than in typical control participants. This supports the claim that although IMS 8 and 10 could execute some subtle facial movements, these movements were largely unspecific.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54687-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>IMS 8 and 10's facial expression execution.</title><p>(<bold>A</bold>) Facial expression naming task. Control participants’ (six men, nine women, mean age = 26) were presented sequentially with a picture model from the Karolinska Directed Emotional Faces set (KDEF; Actress AF01, front view; <xref ref-type="bibr" rid="bib45">Lundqvist et al., 1998</xref>) and video-clips of the IMS 8 and 10 (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B-D</xref>) executing the six basic facial expressions (in rows) and were asked to choose the corresponding label (in column) among six alternatives (anger, disgust, fear, happiness, sadness, and surprise). Control participants categorized accurately the facial expressions of the picture model but erred most of the time when asked to categorize the facial expressions of the IMS. Under the (minimal) assumption that a facial expression is recognized accurately if (1) it is more often correctly than incorrectly labelled and (2) that its corresponding label is provided more often for that expression than for other ones, only the facial expression of anger in IMS8 (recognized by 36% of the control participants) has been recognized accurately by the controls. B. Facial expression sorting task. Control participants (five men, nine women) were presented simultaneously with the six facial expressions executed by a model (Actress AF01, front view from the KDEF; <xref ref-type="bibr" rid="bib45">Lundqvist et al., 1998</xref>), by IMS8 or by IMS10 and were asked to associated the six pictures to their corresponding labels (anger, disgust, fear, happiness, sadness, and surprise). Each label could be used only once. Control participants categorized accurately the facial expressions of the picture model but erred most of the time when asked to categorize the facial expressions of the two IMS. Under the assumption that a facial expression is recognized accurately if (1) it is more often correctly than incorrectly labelled and (2) that its corresponding label is provided more often for that expression than for other ones, only the facial expressions of happiness in IMS8 (100% of the controls) and disgust, fear and surprise in IMS 10 (recognized by 43%, 64 and 50 of the control participants) have been recognized accurately by the controls.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54687-fig2-figsupp3-v2.tif"/></fig></fig-group><p>Fifth, we conducted additional tests and analyses of IMS 8 and 10’s facial movement abilities to explore the possibility that, despite their severe facial paralysis, they could nevertheless imitate some aspects of the facial expressions tested in Experiments 1–5, which would provide them with sufficient information to support facial expression recognition by simulation. In an additional orofacial motor examination that included attempted imitation of the six basic facial expressions, IMS 8 and 10 could execute a few (very) small facial movements. IMS eight was able to execute a mild combined backward/upward movement of the right angle of the mouth, a slight backward movement of the left angle of the mouth, a slight contraction of the mentalis and some slight movements of the upper eye lids. IMS10 was only able to execute a slight bilateral movement of the angles of the mouth backwards and downwards and some up and down movements of the superior eye lids. Importantly, however, these movements covered only a small part of the facial movements that would be required to imitate the different facial expressions of emotions (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>) and they were largely the same when they attempted to imitate the different facial expressions (sadness, happiness, anger, surprise, disgust, fear, see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Control participants were unable to recognize video-clips showing the two IMS attempting to imitate the six same facial expressions (see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). In sum, the facial movements that the IMS were able to execute are small in amplitude, very limited in types, and do not discriminate different facial expressions.</p><p>Sixth, we investigated whether the discrepant results between IMS 8 and 10 and the other IMS, who did not meet the criteria for ‘normotypical’ performance (i.e., they performed below 0.85 standard deviation from the controls’ mean performance in at least one experiment) could be because IMS 8 and 10 were affected by milder facial paralysis. This was clearly not the case: IMS 10 was among the participants with the most severe facial paralysis (see <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Summary of the IMS participants’ facial movements.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>IMS</th><th>Inf. lip</th><th>Sup. lip</th><th>Nose</th><th>Eyebrows</th><th>Forehead</th><th>R. cheek</th><th>L. cheek</th><th>Sup. R. eyelid</th><th>Inf. R. eyelid</th><th>Sup. L. eyelid</th><th>Inf. L. eyelid</th></tr></thead><tbody><tr><td>IMS1</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td></tr><tr><td>IMS2</td><td>Slight</td><td>None</td><td>None</td><td>None</td><td>None</td><td>Slight</td><td>Slight</td><td>Slight</td><td>Slight</td><td>Slight</td><td>Slight</td></tr><tr><td>IMS3</td><td>Slight</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>Slight</td><td>None</td><td>Slight</td><td>None</td></tr><tr><td>IMS4</td><td>Slight</td><td>None</td><td>None</td><td>None</td><td>None</td><td>Slight</td><td>Slight</td><td>None</td><td>None</td><td>None</td><td>None</td></tr><tr><td>IMS5</td><td>Slight</td><td>Slight</td><td>None</td><td>None</td><td>None</td><td>Slight</td><td>Slight</td><td>Slight</td><td>None</td><td>Slight</td><td>None</td></tr><tr><td>IMS6</td><td>Mild</td><td>None</td><td>None</td><td>None</td><td>None</td><td>Mild</td><td>Mild</td><td>Slight</td><td>None</td><td>Slight</td><td>None</td></tr><tr><td>IMS7</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>Mild</td><td>None</td><td>Mild</td><td>Slight</td><td>None</td><td>None</td></tr><tr><td>IMS8</td><td>Slight</td><td>None</td><td>None</td><td>None</td><td>None</td><td>Mild</td><td>Slight</td><td>Slight</td><td>Slight</td><td>Slight</td><td>Slight</td></tr><tr><td>IMS9</td><td>Mild</td><td>None</td><td>None</td><td>None</td><td>None</td><td>Mild</td><td>Slight</td><td>Slight</td><td>Slight</td><td>None</td><td>None</td></tr><tr><td>IMS10</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>Slight</td><td>Slight</td><td>None</td><td>None</td><td>None</td><td>None</td></tr><tr><td>IMS11</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td><td>Slight</td><td>None</td><td>None</td><td>None</td><td>None</td><td>None</td></tr></tbody></table></table-wrap><p>Instructively, and by contrast, the performance of the IMS who performed more poorly than controls in Experiments 1–3 was strongly and significantly correlated with their respective performance on the mid-level perception screening test (three <italic>r</italic>s &gt; 0.5, <italic>p</italic>s &lt; 0.05), and 7/9 of the IMS participants (IMS1, 2, 4, 5, 6, 7, 9) who failed to meet the criteria for normotypical efficient facial expression recognition obtained also equally weak or weaker performance in the facial identity and/or the emotional speech recognition task (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). Thus, the IMS’s performance variability seems to be at least partly due to associated visual and/or cognitive disorders, rather than to their facial paralysis. To further explore this possibility, we first performed a series of one-way ANCOVAs with Group as a fixed effect and participants’ performance in Experiments 6–8 as covariates. Participants performing below two absolute deviations from the median of their group were discarded in order to satisfy ANCOVA normality assumptions (<xref ref-type="bibr" rid="bib44">Leys et al., 2013</xref>). In line with the possibility that the IMS’s performance variability is likely due to associated visual and/or cognitive disorders rather than to their facial paralysis, these analyses failed to reveal any significant effect of Group in Experiments 1, 2, 4 and 5 (all Fs &lt;1; all <italic>p</italic>s &gt; 0.37). The same analysis could not be performed on the results of Experiment three due to the low number of IMS who took part to this experiment. Then, to test more directly the possibility that at least some IMS may have some facial expression recognition impairment that cannot be explained by a visual or cognitive deficit, we used <xref ref-type="bibr" rid="bib16">Crawford and Garthwaite, 2007</xref> Bayesian Standardized Difference Test (BSDT) to test whether the performance of IMS1-7, 9 and 10 in at least some facial expression experiments may be comparatively less good than in the three control tasks. Only IMS1 performed significantly less well in Experiment five than in the three control experiments (Experiments 6–8). All other comparisons indicated either better performance in facial expression recognition experiments than in at least one of the control experiments (55% of the comparisons) or slightly but non-significantly weaker performance (45% of comparisons, all BSDTs &gt; 0.1).</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>According to popular motor theories of facial expression recognition, efficient recognition of others’ facial expressions cannot be achieved by visual analysis of the movements alone but requires additional unconscious covert imitation – motor simulation – of the observed movements. In this study, we tested a prediction drawn from these theories, namely, that any individual who has no motor representations of facial expressions due to congenital facial paralysis should be less efficient than typically developed individuals in recognizing facial expressions. We did this by assessing the performance of eleven individuals born with Moebius Syndrome, an extremely rare congenital nonprogressive condition resulting in severe or complete bilateral facial paralysis often accompanied by visual and mid-level perceptual deficits, in five challenging facial expression experiments. We compared their performances to that of 25 young and highly educated control participants. As in previous studies (<xref ref-type="bibr" rid="bib8">Bate et al., 2013</xref>; <xref ref-type="bibr" rid="bib11">Calder et al., 2000</xref>; <xref ref-type="bibr" rid="bib30">Giannini et al., 1984</xref>; <xref ref-type="bibr" rid="bib52">Nicolini et al., 2019</xref>), several IMS failed to recognize facial expressions as efficiently as control participants in one (IMS 1, 3, 5, 7) or several (IMS 2, 6, 9, 11) experiments (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). Although these difficulties have to be interpreted with caution because the performance of these IMS was compared to that of highly educated young adults who performed Experiments 3 and 4 at a higher level of performance than previously published norms, this finding suggests that some individuals with Moebius Syndrome have difficulty in recognizing facial expressions in at least some tasks. Such association of deficits is interesting but is difficult to interpret because of the co-occurrence of motor, visual and perceptual deficits in the IMS we tested (<xref ref-type="table" rid="table2">Table 2</xref>) makes it difficult to establish unambiguously the (possibly multiple) origins of these difficulties. Nevertheless, and more interestingly, we also found that two of the IMS (IMS 8 and 10) fulfilled the criteria for ‘normotypical efficient facial expression recognition’: (1) they both scored above 0.85 standard deviations below the mean of the controls in all five facial expression recognition experiments; (2) they performed as well in these tasks as in other face-related visual tasks (face identity) and emotion recognition tasks (vocal emotion recognition); and (3) they performed these tasks in a way that is qualitatively similar to the controls. These two IMS had severe facial paralysis. They could not execute most of the facial movements that would be required to imitate the different facial expressions of emotions (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). In addition, they executed largely the same inappropriate movements when they attempted to imitate the different facial expressions (sadness, happiness, anger, surprise, disgust, fear, see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplements 2</xref>–<xref ref-type="fig" rid="fig2s3">3</xref>). These dissociations challenge the central premise of the models of facial expression recognition centered on facial mimicry: they constitute existence proof that it is <italic>possible</italic> to account for efficient facial expression recognition without having to invoke a mechanism of ‘motor simulation’, even in very sensitive and challenging tasks requiring, for instance, the identification of a facial expression in 150 milliseconds, only from information available in the eye region, when the stimuli are morphs composed of a mixture of emotions, and when the task involves discriminating fake versus genuine smiles.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Information regarding IMS participants’ visual and visuo-perceptual abilities.</title></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Vision</th><th>Reported best corrected acuity</th><th>Strabismus</th><th>Eye movements</th><th>Mid-level perception* <break/>(modified <italic>t</italic>-test)<sup>2</sup></th></tr></thead><tbody><tr><td>IMS1</td><td>Hypermetropy, astigmatism</td><td>Mild vision loss (7/10)</td><td>Slight</td><td>H: Absent; V: Reduced</td><td>0.9</td></tr><tr><td>IMS2</td><td>Hypermetropy, astigmatism</td><td>Normal vision</td><td>Slight</td><td>H: Absent; V: Reduced</td><td>−3.3</td></tr><tr><td>IMS3</td><td>Myopia</td><td>Normal vision</td><td>None</td><td>H: Absent; V: Reduced</td><td>−0.3</td></tr><tr><td>IMS4</td><td>Hypermetropy, astigmatism</td><td>Mild vision loss (5/10)</td><td>None</td><td>H: Absent; V: Typical</td><td>−2.9</td></tr><tr><td>IMS5</td><td>Hypermetropy, astigmatism</td><td>Mild vision loss (8/10)</td><td>Slight</td><td>H: Very limited; V: Typical</td><td>−0.3</td></tr><tr><td>IMS6</td><td>Hypermetropy, astigmatism</td><td>Normal vision</td><td>Slight</td><td>H: Typical; V: Typical</td><td>0.1</td></tr><tr><td>IMS7</td><td>Hypermetropy, astigmatism</td><td>Moderate vision loss of the left eye (2/10)</td><td>None</td><td>H: Typical; V: Typical</td><td>0.5</td></tr><tr><td>IMS8</td><td>Normal</td><td>Normal vision</td><td>None</td><td>H: Typical; V: Typical</td><td>0.5</td></tr><tr><td>IMS9</td><td>Normal</td><td>Normal vision</td><td>Slight</td><td>H: Absent; V: Absent</td><td>−3.3</td></tr><tr><td>IMS10</td><td>Myopia</td><td>Mild vision loss (8/10)</td><td>None</td><td>H: Absent; V: Typical</td><td>−0.3</td></tr><tr><td>IMS11</td><td>Myopia, astigmatism</td><td>Mild vision loss: 6/10 right eye; 5/10 left eye</td><td>Slight</td><td>H: Reduced; V: Typical</td><td>−4.2</td></tr></tbody></table><table-wrap-foot><fn><p>* Leuven Perceptual Organization Screening Test, L-POST (<xref ref-type="bibr" rid="bib70">Torfs et al., 2014</xref>). <sup>2</sup>(<xref ref-type="bibr" rid="bib17">Crawford and Howell, 1998</xref>).</p></fn></table-wrap-foot></table-wrap><p>This conclusion is in line with previous reports of MS participants who achieved normal performance in facial expression recognition despite their congenital facial paralysis (<xref ref-type="bibr" rid="bib8">Bate et al., 2013</xref>; <xref ref-type="bibr" rid="bib11">Calder et al., 2000</xref>; <xref ref-type="bibr" rid="bib63">Rives Bogart and Matsumoto, 2010</xref>). However, the interpretation of those findings was limited by the putative relative insensitivity of the facial expression recognition tasks used in those studies (<xref ref-type="bibr" rid="bib74">Van Rysewyk, 2011</xref>; <xref ref-type="bibr" rid="bib19">De Stefani et al., 2019</xref>). The findings reported herein go beyond this previous evidence and demonstrate that motor simulation contributes neither to the ease of facial expression recognition nor to its robustness in particularly challenging tasks.</p><p>Our findings are also in line with the results of previous behavioral studies showing that individuals congenitally deprived of hand motor representations nonetheless perceive and comprehend hand actions, which they cannot covertly imitate, as accurately, as fast, with the same biases, and very similar brain networks as typically developed participants (<xref ref-type="bibr" rid="bib77">Vannuscorps and Caramazza, 2015</xref>; <xref ref-type="bibr" rid="bib79">Vannuscorps and Caramazza, 2016b</xref>; <xref ref-type="bibr" rid="bib80">Vannuscorps and Caramazza, 2016c</xref>; <xref ref-type="bibr" rid="bib76">Vannuscorps et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Vannuscorps et al., 2012</xref>). As such, beyond contributing to the question of the nature of the mechanisms involved in facial expression recognition, our findings are also relevant for theories of the types of representations that support action perception and recognition in general. According to motor simulation theories of action recognition, the recognition of others’ actions cannot be achieved by visual analysis of the movements alone but requires unconscious covert imitation – motor simulation – of the observed movements (<xref ref-type="bibr" rid="bib64">Rizzolatti and Sinigaglia, 2010</xref>). Our results challenge this premise.</p><p>In sum, our finding constitutes existence proof that the visuo-perceptual system can support efficient facial expression recognition unaided by motor simulation. Of course, this does not <italic>imply</italic> that motor simulation does not support facial expression in typically developed participants. Correlated sensorimotor experience may be necessary for the development of motor contributions to action perception (e.g., <xref ref-type="bibr" rid="bib14">Catmur and Heyes, 2019</xref>) and IMS 8 and 10 may have developed an atypically efficient visual system to compensate for their congenital paralysis, for instance. However, the observation that these IMS performed the facial recognition tasks in a qualitatively similar manner to the controls makes this unlikely. In any event, and more importantly, in the current absence of compelling direct evidence that motor simulation does underlie facial expression recognition in typically developed participants, our results at the very least emphasize the need for a shift in the burden of proof regarding this question. If there were to be unambiguous evidence that efficient visual perception and interpretation of facial expressions requires the involvement of one’s motor system this would revolutionize our understanding of how the Mind/Brain is organized and our results would have to be re-interpreted as useful evidence about the range of computational neural plasticity that is possible in a system that typically relies on motor simulation; but so far this claim is neither justified by current experimental evidence nor necessary to account for typically efficient facial expression recognition. At this juncture, facial expression recognition is thus better explained by the perceptual-cognitive view according to which it is supported by visuo-perceptual, structural and conceptual, but not sensorimotor, representations.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>We tested eleven individuals with severe to complete facial paralysis in the context of the Moebius Syndrome (IMS participants; eight females; all right-handed; various education levels; mean age ± SD: 27.7 ± 9.25 years) and compared their performance to that of 25 typically developed highly educated young adults (15 females; three left-handed; all college students or graduates without any history of psychiatric or neurological disorder; Mean age ± SD: 28.6 ± 6.5 years). All IMS were able to speak intelligibly and to convey emotions in their speech. Information regarding all participants’ neurological and psychiatric history and about the IMS’s medical, surgical and therapeutic history associated with the syndrome was obtained through a series of questionnaires (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Information about all participants’ visual and mid-level perceptual skills were obtained through questionnaires and a perceptual screening test (<xref ref-type="table" rid="table2">Table 2</xref>). Information about the IMS’s facial motor abilities was obtained through a facial movements examination (see supplemental methods for detail; <xref ref-type="table" rid="table1">Table 1</xref>).</p></sec><sec id="s4-2"><title>Materials and procedures</title><p>The experimental investigations were carried out from October 2015 to April 2019 in sessions lasting between 60 and 90 min. The study was approved by the biomedical ethics committee of the Cliniques Universitaires Saint-Luc, Brussels, Belgium and all participants gave written informed consent prior to the study.</p><p>Participants performed five facial expression recognition tasks, two facial identity recognition tasks, and an emotional speech recognition task. These experiments were controlled by the online <ext-link ext-link-type="uri" xlink:href="https://www.testable.org/">testable.org</ext-link> interface (<ext-link ext-link-type="uri" xlink:href="http://www.testable.org">http://www.testable.org</ext-link>), which allows precise spatiotemporal control of online experiments. Control participants were tested on the 15.6-inch anti-glare screen (set at 1366 × 768 pixels and 60 Hz) of a Dell Latitude E5530 laptop operated by Windows 10. During the experiment, they sat comfortably at a distance of about 60 cm of the screen. The IMS were tested remotely under supervision of the experimenter through a Visio conference system (Skype). They sat at about 60 cm from their own computer screen. All procedures were the same for all participants. At the beginning of each experiment, the participant was instructed to set the browsing window of the computer to full screen, minimize possible distractions (e.g., TV, phone, etc.) and position themselves at arm’s length from the monitor for the duration of the experiment. Next, a calibration procedure ascertaining homogeneous presentation size and time on all computer screens took place.</p><p>Experiments 1 and 2 tested whether motor simulation supports efficient recognition of briefly-presented facial expressions. In each of the 60 trials of Experiments 1 and 2, participants viewed a picture of an actor’s face expressing one of six facial expressions (anger, disgust, fear, happiness, sadness, and surprise; <xref ref-type="bibr" rid="bib45">Lundqvist et al., 1998</xref>) presented for either 150 ms (Experiment 1) or 1 s (Experiment 2) between two pictures of the same actor displaying a neutral expression (presented for 1 s each). Participants were asked to carefully observe the target picture and to associate it with its corresponding label, presented among five alternatives.</p><p>Experiments 3 and 4 tested whether motor simulation enhances stimulus identification under adverse perceptual conditions, such as when the emotion must be inferred from minimal information or when it is subtle and ambiguous. In Experiment 3, participants viewed 96 video clips depicting an actor’s face expressing one of the six basic emotions at one of four different levels of intensity (40%, 60%, 80%, and 100%; <xref ref-type="bibr" rid="bib40">Kessels et al., 2014</xref>; <xref ref-type="bibr" rid="bib49">Montagne et al., 2007</xref>) and had to associate it with its corresponding label among six alternatives. This experimental paradigm has been shown to be sensitive to subtle emotion recognition disorders in clinical conditions such as post-traumatic stress disorder (<xref ref-type="bibr" rid="bib59">Poljac et al., 2011</xref>), amygdalectomy (<xref ref-type="bibr" rid="bib3">Ammerlaan et al., 2008</xref>) and social phobia (<xref ref-type="bibr" rid="bib48">Montagne et al., 2006</xref>).</p><p>In Experiment 4, we tested the ability to recognize more complex mental states (e.g., skeptical, insisting, suspicious) from information restricted to the actor’s eye region. On each of the 36 trials of this experiment (the ‘Reading the Mind in the Eyes’ test Revised version; <xref ref-type="bibr" rid="bib7">Baron-Cohen et al., 2001</xref>), participants were asked to associate a picture depicting the eye-region of someone’s face to the verbal label that best described that person’s emotion or mental state (e.g., skeptical, insisting, suspicious) among four subtly different alternatives. Performance in this experiment has been previously shown to be sensitive to both subtle experimental modulations of the observers’ motor system and slight emotion recognition disorders in clinical conditions: It is affected by botulinum toxin (Botox) injections on the forehead (<xref ref-type="bibr" rid="bib51">Neal and Chartrand, 2011</xref>), oxytocin administration (<xref ref-type="bibr" rid="bib23">Domes et al., 2007</xref>), and hampered in persons with autism (<xref ref-type="bibr" rid="bib7">Baron-Cohen et al., 2001</xref>), schizophrenia (<xref ref-type="bibr" rid="bib41">Kettle et al., 2008</xref>), anorexia nervosa (<xref ref-type="bibr" rid="bib34">Harrison et al., 2010</xref>), depression (<xref ref-type="bibr" rid="bib69">Szanto et al., 2012</xref>), Parkinson’s disease (<xref ref-type="bibr" rid="bib73">Tsuruya et al., 2011</xref>) and Huntington’s disease (<xref ref-type="bibr" rid="bib2">Allain et al., 2011</xref>).</p><p>Experiment five tested the hypothesis that motor simulation is particularly important for the ability to discriminate fake versus genuine smiles (e.g., <xref ref-type="bibr" rid="bib57">Paracampo et al., 2017</xref>). In each of the 32 trials of this experiment, participants viewed a video clip showing an actor who was either spontaneously smiling out of amusement (genuine smile) or producing a forced (fake) smile and had to use subtle morphological and dynamic features of the smile in order to discriminate genuine from fake smiles (<xref ref-type="bibr" rid="bib27">Ekman, 1982</xref>; <xref ref-type="bibr" rid="bib42">Krumhuber and Manstead, 2009</xref>; <xref ref-type="bibr" rid="bib57">Paracampo et al., 2017</xref>). Performance in the Amusement from Smiles task is influenced by simple procedures such as wearing a mouth guard (<xref ref-type="bibr" rid="bib66">Rychlowska et al., 2014</xref>), TMS in the face sensorimotor cortex (<xref ref-type="bibr" rid="bib57">Paracampo et al., 2017</xref>), and vowel articulation (<xref ref-type="bibr" rid="bib38">Ipser and Cook, 2016</xref>).</p><p>Experiments 6 and 7 tested participants’ facial identity recognition skills. Experiment six used the Cambridge Face Memory Test (CFMT; <xref ref-type="bibr" rid="bib26">Duchaine and Nakayama, 2006</xref>), which has two parts. In the first phase, participants were asked to observe, memorize, and recognize six persons’ faces. For each person, they were presented with three pictures of the person’s face displayed from three different viewpoints for 3 s each and they were then asked to recognize this face among two foils in three successive forced-choice trials. In the second phase, participants were first presented with a frontal view of these six faces for 20 s, and then asked to recognize any of these six faces from a set of 30 three-alternative forced-choice trials containing new images of faces studied in the first phase. Performance was measured by counting the number of trials, out of 48, in which the learned face was chosen correctly. In each of the 8 trials of Experiment 7 – the Cambridge Face Perception Test (CFPT; <xref ref-type="bibr" rid="bib25">Duchaine et al., 2007</xref>) – participants were presented with a target face and, below it, six pictures morphed to contain different proportions of the target face (28%, 40%, 52%, 64%, 76% and 88%) and were asked to sort them from most like to least like the target face. For each trial, the ﬁnal order was scored by summing the deviations from the correct order (e.g., if a face is three places away from its correct place, it is given a score of 3).</p><p>Experiment eight tested participants’ ability to recognize emotions from emotional speech. In each of the 60 trials of this experiment, participants were asked to listen to a ~ 2 s long audio-clip in which a female or a male speaker simulated one of seven emotions (anger, anxiety/fear, boredom, disgust, happiness, sadness, neutral) while producing a sentence which could be applied to any emotion in a language that none of the IMS or control participants declared to understand (in German; from <xref ref-type="bibr" rid="bib10">Burkhardt et al., 2005</xref>). Participants were asked to listen carefully to the audio-clips and, then, to pick its corresponding label presented among the six other alternatives. We counted each participant’s number of correct responses.</p><p>The participants with Moebius Syndrome additionally participated to a motor, which included three main parts: (1) They were asked to imitate facial movements involving specific muscles on the territory of the facial nerve: the buccinator (pressing cheeks on the side of the teeth and pull back the angle of the mouth), zigomaticus major (large smile), risorius (draw the angle of the mouth straight backwards), levator anguli oris (pulling the angle of the mouth upward), depressor anguli oris (pulling the angle of the mouth downward), levator labii superioris (raise and protrude the upper lips), depressor labii inferioris (pull the lower lip and angle of the mouth down), the mentalis (protrude the lower lip and make a pouting face), the frontalis (elevate the eyebrows), the corrigator supercilia and depressor supercilia (frown), orbicularis oculi (close the eyes gently, close the eyes tightly), and the procerus and nasalis (wrinkle the nose) muscles. (2) In order to test the trigeminal and hypoglossal nerves, we asked them to produce various facial other movements upon verbal command: opening and closing the jaws, moving the tongue from left to right, up and down, back and forth, and articulating repeatedly the syllable/pa/for 5–6 s. (3) In order to document their ability to execute facial expressions, they were asked to imitate as well as possible six facial expressions (happiness, anger, fear, disgust, sadness and surprise) based on a picture example from the Karolinska Directed Emotional Faces set (Actress AF01, front view; Lundqvist, D., Flykt, A., and Öhman, A., 1998). Their performance during these tasks was recorded and analyzed offline. The results of these analyses are presented on <xref ref-type="table" rid="table1">Table 1</xref>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was supported by the Mind, Brain and Behavior Interfaculty Initiative provostial funds. MA is a research associate at the Fonds National de la Recherche Scientifique (FRS-FNRS, Belgium).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Writing - original draft, Project administration</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was approved by the local Ethical committee at UCLouvain (Registration # B403201629166). Written informed consents were obtained from all participants prior to the study, and after the nature and possible consequences of the studies were explained.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Information regarding IMS participants’ demographic, neurological, psychiatric, medical and surgical/therapeutic history.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-54687-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Facial action units corresponding to the facial expression of the six basic emotions and their presence/absence in the repertoire of the IMS 8 and 10.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-54687-supp2-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-54687-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data and stimulus materials are publicly available and can be accessed on the Open Science Framework platform (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/8T4FV">https://doi.org/10.17605/OSF.IO/8T4FV</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Vannuscorps</surname><given-names>G</given-names></name><name><surname>Andres</surname><given-names>M</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Data from: Efficient recognition of facial expressions does not require motor simulation</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/OSF.IO/8T4FV</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adolphs</surname> <given-names>R</given-names></name><name><surname>Damasio</surname> <given-names>H</given-names></name><name><surname>Tranel</surname> <given-names>D</given-names></name><name><surname>Cooper</surname> <given-names>G</given-names></name><name><surname>Damasio</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A role for somatosensory cortices in the visual recognition of emotion as revealed by three-dimensional lesion mapping</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>2683</fpage><lpage>2690</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-07-02683.2000</pub-id><pub-id pub-id-type="pmid">10729349</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allain</surname> <given-names>P</given-names></name><name><surname>Havet-Thomassin</surname> <given-names>V</given-names></name><name><surname>Verny</surname> <given-names>C</given-names></name><name><surname>Gohier</surname> <given-names>B</given-names></name><name><surname>Lancelot</surname> <given-names>C</given-names></name><name><surname>Besnard</surname> <given-names>J</given-names></name><name><surname>Fasotti</surname> <given-names>L</given-names></name><name><surname>Le Gall</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Evidence for deficits on different components of theory of mind in Huntington's disease</article-title><source>Neuropsychology</source><volume>25</volume><fpage>741</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.1037/a0024408</pub-id><pub-id pub-id-type="pmid">21728429</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ammerlaan</surname> <given-names>EJ</given-names></name><name><surname>Hendriks</surname> <given-names>MP</given-names></name><name><surname>Colon</surname> <given-names>AJ</given-names></name><name><surname>Kessels</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Emotion perception and interpersonal behavior in epilepsy patients after unilateral amygdalohippocampectomy</article-title><source>Acta Neurobiologiae Experimentalis</source><volume>68</volume><fpage>214</fpage><lpage>218</lpage><pub-id pub-id-type="pmid">18511957</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Amos</surname> <given-names>B</given-names></name><name><surname>Ludwiczuk</surname> <given-names>B</given-names></name><name><surname>Satyanarayanan</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>OpenFace: A General-Purpose Face Recognition Library with Mobile Applications</data-title><publisher-name>CMU-CS-16-118, CMU School of Computer Science</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astafiev</surname> <given-names>SV</given-names></name><name><surname>Stanley</surname> <given-names>CM</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Extrastriate body area in human occipital cortex responds to the performance of motor actions</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>542</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nn1241</pub-id><pub-id pub-id-type="pmid">15107859</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Baltrusaitis</surname> <given-names>T</given-names></name><name><surname>Robinson</surname> <given-names>P</given-names></name><name><surname>Morency</surname> <given-names>LP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>OpenFace: an open source facial behavior analysis toolkit</article-title><conf-name>2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016. Institute of Electrical and Electronics Engineers Inc</conf-name><pub-id pub-id-type="doi">10.1109/WACV.2016.7477553</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baron-Cohen</surname> <given-names>S</given-names></name><name><surname>Wheelwright</surname> <given-names>S</given-names></name><name><surname>Hill</surname> <given-names>J</given-names></name><name><surname>Raste</surname> <given-names>Y</given-names></name><name><surname>Plumb</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The &quot;Reading the Mind in the Eyes&quot; Test revised version: a study with normal adults, and adults with Asperger syndrome or high-functioning autism</article-title><source>Journal of Child Psychology and Psychiatry</source><volume>42</volume><fpage>241</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1111/1469-7610.00715</pub-id><pub-id pub-id-type="pmid">11280420</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bate</surname> <given-names>S</given-names></name><name><surname>Cook</surname> <given-names>SJ</given-names></name><name><surname>Mole</surname> <given-names>J</given-names></name><name><surname>Cole</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>First report of generalized face processing difficulties in möbius sequence</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e62656</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0062656</pub-id><pub-id pub-id-type="pmid">23638131</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname> <given-names>V</given-names></name><name><surname>Young</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Understanding face recognition</article-title><source>British Journal of Psychology</source><volume>77 ( Pt 3</volume><fpage>305</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8295.1986.tb02199.x</pub-id><pub-id pub-id-type="pmid">3756376</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Burkhardt</surname> <given-names>F</given-names></name><name><surname>Paeschke</surname> <given-names>A</given-names></name><name><surname>Rolfes</surname> <given-names>M</given-names></name><name><surname>Sendlmeier</surname> <given-names>W</given-names></name><name><surname>Weiss</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A database of German emotional speech</article-title><conf-name>9th European Conference on Speech Communication and Technology</conf-name><fpage>1517</fpage><lpage>1520</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calder</surname> <given-names>AJ</given-names></name><name><surname>Keane</surname> <given-names>J</given-names></name><name><surname>Cole</surname> <given-names>J</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name><name><surname>Young</surname> <given-names>AW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Facial expression recognition by people with mobius syndrome</article-title><source>Cognitive Neuropsychology</source><volume>17</volume><fpage>73</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1080/026432900380490</pub-id><pub-id pub-id-type="pmid">20945172</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname> <given-names>L</given-names></name><name><surname>Iacoboni</surname> <given-names>M</given-names></name><name><surname>Dubeau</surname> <given-names>MC</given-names></name><name><surname>Mazziotta</surname> <given-names>JC</given-names></name><name><surname>Lenzi</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neural mechanisms of empathy in humans: a relay from neural systems for imitation to limbic Areas</article-title><source>PNAS</source><volume>100</volume><fpage>5497</fpage><lpage>5502</lpage><pub-id pub-id-type="doi">10.1073/pnas.0935845100</pub-id><pub-id pub-id-type="pmid">12682281</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carta</surname> <given-names>A</given-names></name><name><surname>Mora</surname> <given-names>P</given-names></name><name><surname>Neri</surname> <given-names>A</given-names></name><name><surname>Favilla</surname> <given-names>S</given-names></name><name><surname>Sadun</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Ophthalmologic and systemic features in möbius syndrome an italian case series</article-title><source>Ophthalmology</source><volume>118</volume><fpage>1518</fpage><lpage>1523</lpage><pub-id pub-id-type="doi">10.1016/j.ophtha.2011.01.023</pub-id><pub-id pub-id-type="pmid">21459449</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catmur</surname> <given-names>C</given-names></name><name><surname>Heyes</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mirroring ‘meaningful’ actions: Sensorimotor learning modulates imitation of goal-directed actions</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>72</volume><fpage>322</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1080/17470218.2017.1344257</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collignon</surname> <given-names>O</given-names></name><name><surname>Girard</surname> <given-names>S</given-names></name><name><surname>Gosselin</surname> <given-names>F</given-names></name><name><surname>Roy</surname> <given-names>S</given-names></name><name><surname>Saint-Amour</surname> <given-names>D</given-names></name><name><surname>Lassonde</surname> <given-names>M</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Audio-visual integration of emotion expression</article-title><source>Brain Research</source><volume>1242</volume><fpage>126</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2008.04.023</pub-id><pub-id pub-id-type="pmid">18495094</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crawford</surname> <given-names>JR</given-names></name><name><surname>Garthwaite</surname> <given-names>PH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Comparison of a single case to a control or normative sample in neuropsychology: development of a bayesian approach</article-title><source>Cognitive Neuropsychology</source><volume>24</volume><fpage>343</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1080/02643290701290146</pub-id><pub-id pub-id-type="pmid">18416496</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crawford</surname> <given-names>JR</given-names></name><name><surname>Howell</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Comparing an individual's Test Score Against Norms Derived from Small Samples</article-title><source>The Clinical Neuropsychologist</source><volume>12</volume><fpage>482</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1076/clin.12.4.482.7241</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dapretto</surname> <given-names>M</given-names></name><name><surname>Davies</surname> <given-names>MS</given-names></name><name><surname>Pfeifer</surname> <given-names>JH</given-names></name><name><surname>Scott</surname> <given-names>AA</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name><name><surname>Bookheimer</surname> <given-names>SY</given-names></name><name><surname>Iacoboni</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Understanding emotions in others: mirror neuron dysfunction in children with autism spectrum disorders</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>28</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1038/nn1611</pub-id><pub-id pub-id-type="pmid">16327784</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Stefani</surname> <given-names>E</given-names></name><name><surname>Nicolini</surname> <given-names>Y</given-names></name><name><surname>Belluardo</surname> <given-names>M</given-names></name><name><surname>Ferrari</surname> <given-names>PF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Congenital facial palsy and emotion processing: the case of moebius syndrome</article-title><source>Genes, Brain and Behavior</source><volume>18</volume><elocation-id>e12548</elocation-id><pub-id pub-id-type="doi">10.1111/gbb.12548</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dimberg</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Facial reactions to facial expressions</article-title><source>Psychophysiology</source><volume>19</volume><fpage>643</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1982.tb02516.x</pub-id><pub-id pub-id-type="pmid">7178381</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dimberg</surname> <given-names>U</given-names></name><name><surname>Thunberg</surname> <given-names>M</given-names></name><name><surname>Elmehed</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Unconscious facial reactions to emotional facial expressions</article-title><source>Psychological Science</source><volume>11</volume><fpage>86</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00221</pub-id><pub-id pub-id-type="pmid">11228851</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dinstein</surname> <given-names>I</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Rubin</surname> <given-names>N</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Brain Areas selective for both observed and executed movements</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>1415</fpage><lpage>1427</lpage><pub-id pub-id-type="doi">10.1152/jn.00238.2007</pub-id><pub-id pub-id-type="pmid">17596409</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Domes</surname> <given-names>G</given-names></name><name><surname>Heinrichs</surname> <given-names>M</given-names></name><name><surname>Michel</surname> <given-names>A</given-names></name><name><surname>Berger</surname> <given-names>C</given-names></name><name><surname>Herpertz</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Oxytocin improves &quot;mind-reading&quot; in humans</article-title><source>Biological Psychiatry</source><volume>61</volume><fpage>731</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2006.07.015</pub-id><pub-id pub-id-type="pmid">17137561</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname> <given-names>Y</given-names></name><name><surname>Zhang</surname> <given-names>F</given-names></name><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Bi</surname> <given-names>T</given-names></name><name><surname>Qiu</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual learning of facial expressions</article-title><source>Vision Research</source><volume>128</volume><fpage>19</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2016.08.005</pub-id><pub-id pub-id-type="pmid">27664348</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duchaine</surname> <given-names>B</given-names></name><name><surname>Germine</surname> <given-names>L</given-names></name><name><surname>Nakayama</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Family resemblance: ten family members with prosopagnosia and within-class object Agnosia</article-title><source>Cognitive Neuropsychology</source><volume>24</volume><fpage>419</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1080/02643290701380491</pub-id><pub-id pub-id-type="pmid">18416499</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duchaine</surname> <given-names>B</given-names></name><name><surname>Nakayama</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The cambridge face memory test: results for neurologically intact individuals and an investigation of its validity using inverted face stimuli and prosopagnosic participants</article-title><source>Neuropsychologia</source><volume>44</volume><fpage>576</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.07.001</pub-id><pub-id pub-id-type="pmid">16169565</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ekman</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Emotion in the Human Face</source><publisher-loc>New York</publisher-loc><publisher-name>Cambridge Cambridgeshire</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elfenbein</surname> <given-names>HA</given-names></name><name><surname>Foo</surname> <given-names>MD</given-names></name><name><surname>White</surname> <given-names>J</given-names></name><name><surname>Tan</surname> <given-names>HH</given-names></name><name><surname>Aik</surname> <given-names>VC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reading your counterpart: the benefit of emotion recognition accuracy for effectiveness in negotiation</article-title><source>Journal of Nonverbal Behavior</source><volume>31</volume><fpage>205</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1007/s10919-007-0033-7</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname> <given-names>A</given-names></name><name><surname>Hess</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mimicking emotions</article-title><source>Current Opinion in Psychology</source><volume>17</volume><fpage>151</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.copsyc.2017.07.008</pub-id><pub-id pub-id-type="pmid">28950963</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giannini</surname> <given-names>AJ</given-names></name><name><surname>Tamulonis</surname> <given-names>D</given-names></name><name><surname>Giannini</surname> <given-names>MC</given-names></name><name><surname>Loiselle</surname> <given-names>RH</given-names></name><name><surname>Spirtos</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Defective response to social cues in möbius' syndrome</article-title><source>The Journal of Nervous and Mental Disease</source><volume>172</volume><fpage>174</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1097/00005053-198403000-00008</pub-id><pub-id pub-id-type="pmid">6699632</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillberg</surname> <given-names>C</given-names></name><name><surname>Steffenburg</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Autistic behaviour in moebius syndrome</article-title><source>Acta Paediatrica</source><volume>78</volume><fpage>314</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1111/j.1651-2227.1989.tb11076.x</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman</surname> <given-names>AI</given-names></name><name><surname>Sripada</surname> <given-names>CS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Simulationist models of face-based emotion recognition</article-title><source>Cognition</source><volume>94</volume><fpage>193</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2004.01.005</pub-id><pub-id pub-id-type="pmid">15617671</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname> <given-names>I</given-names></name><name><surname>Pierce</surname> <given-names>MD</given-names></name><name><surname>Bartlett</surname> <given-names>MS</given-names></name><name><surname>Tanaka</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Training facial expression production in children on the autism spectrum</article-title><source>Journal of Autism and Developmental Disorders</source><volume>44</volume><fpage>2486</fpage><lpage>2498</lpage><pub-id pub-id-type="doi">10.1007/s10803-014-2118-6</pub-id><pub-id pub-id-type="pmid">24777287</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname> <given-names>A</given-names></name><name><surname>Tchanturia</surname> <given-names>K</given-names></name><name><surname>Treasure</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attentional Bias, emotion recognition, and emotion regulation in anorexia: state or trait?</article-title><source>Biological Psychiatry</source><volume>68</volume><fpage>755</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2010.04.037</pub-id><pub-id pub-id-type="pmid">20591417</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hess</surname> <given-names>U</given-names></name><name><surname>Houde</surname> <given-names>S</given-names></name><name><surname>Fischer</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Do we mimic what we see or what we know?</chapter-title><person-group person-group-type="editor"><name><surname>von Scheve</surname> <given-names>C</given-names></name><name><surname>Salmela</surname> <given-names>M</given-names></name></person-group><source>Collective Emotions</source><publisher-loc>Oxford</publisher-loc><publisher-name>Oxford University Press</publisher-name><fpage>94</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1093/acprof:oso/9780199659180.001.0001</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hess</surname> <given-names>U</given-names></name><name><surname>Blairy</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Facial mimicry and emotional contagion to dynamic emotional facial expressions and their influence on decoding accuracy</article-title><source>International Journal of Psychophysiology</source><volume>40</volume><fpage>129</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/S0167-8760(00)00161-6</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huelle</surname> <given-names>JO</given-names></name><name><surname>Sack</surname> <given-names>B</given-names></name><name><surname>Broer</surname> <given-names>K</given-names></name><name><surname>Komlewa</surname> <given-names>I</given-names></name><name><surname>Anders</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Unsupervised learning of facial emotion decoding skills</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>77</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00077</pub-id><pub-id pub-id-type="pmid">24578686</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ipser</surname> <given-names>A</given-names></name><name><surname>Cook</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inducing a concurrent motor load reduces categorization precision for facial expressions</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>42</volume><fpage>706</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1037/xhp0000177</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johansson</surname> <given-names>M</given-names></name><name><surname>Wentz</surname> <given-names>E</given-names></name><name><surname>Fernell</surname> <given-names>E</given-names></name><name><surname>Strömland</surname> <given-names>K</given-names></name><name><surname>Miller</surname> <given-names>MT</given-names></name><name><surname>Gillberg</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Autistic spectrum disorders in möbius sequence: a comprehensive study of 25 individuals</article-title><source>Developmental Medicine &amp; Child Neurology</source><volume>43</volume><fpage>338</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8749.2001.tb00214.x</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kessels</surname> <given-names>RP</given-names></name><name><surname>Montagne</surname> <given-names>B</given-names></name><name><surname>Hendriks</surname> <given-names>AW</given-names></name><name><surname>Perrett</surname> <given-names>DI</given-names></name><name><surname>de Haan</surname> <given-names>EH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Assessment of perception of morphed facial expressions using the emotion recognition task: normative data from healthy participants aged 8-75</article-title><source>Journal of Neuropsychology</source><volume>8</volume><fpage>75</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1111/jnp.12009</pub-id><pub-id pub-id-type="pmid">23409767</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kettle</surname> <given-names>JW</given-names></name><name><surname>O'Brien-Simpson</surname> <given-names>L</given-names></name><name><surname>Allen</surname> <given-names>NB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Impaired theory of mind in first-episode schizophrenia: comparison with community, university and depressed controls</article-title><source>Schizophrenia Research</source><volume>99</volume><fpage>96</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1016/j.schres.2007.11.011</pub-id><pub-id pub-id-type="pmid">18155447</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krumhuber</surname> <given-names>EG</given-names></name><name><surname>Manstead</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Can duchenne smiles be feigned? new evidence on felt and false smiles</article-title><source>Emotion</source><volume>9</volume><fpage>807</fpage><lpage>820</lpage><pub-id pub-id-type="doi">10.1037/a0017844</pub-id><pub-id pub-id-type="pmid">20001124</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leslie</surname> <given-names>KR</given-names></name><name><surname>Johnson-Frey</surname> <given-names>SH</given-names></name><name><surname>Grafton</surname> <given-names>ST</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Functional imaging of face and hand imitation: towards a motor theory of empathy</article-title><source>NeuroImage</source><volume>21</volume><fpage>601</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.09.038</pub-id><pub-id pub-id-type="pmid">14980562</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leys</surname> <given-names>C</given-names></name><name><surname>Ley</surname> <given-names>C</given-names></name><name><surname>Klein</surname> <given-names>O</given-names></name><name><surname>Bernard</surname> <given-names>P</given-names></name><name><surname>Licata</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median</article-title><source>Journal of Experimental Social Psychology</source><volume>49</volume><fpage>764</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1016/j.jesp.2013.03.013</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Lundqvist</surname> <given-names>D</given-names></name><name><surname>Flykt</surname> <given-names>A</given-names></name><name><surname>Öhman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><data-title><italic>The Karolinska Directed Emotional Faces—KDEF [CD-ROM]</italic></data-title><publisher-loc>Stockholm, Sweden</publisher-loc><publisher-name>Department of Clinical Neuroscience, Psychology Section, Karolinska Institutet</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maringer</surname> <given-names>M</given-names></name><name><surname>Krumhuber</surname> <given-names>EG</given-names></name><name><surname>Fischer</surname> <given-names>AH</given-names></name><name><surname>Niedenthal</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Beyond smile dynamics: mimicry and beliefs in judgments of smiles</article-title><source>Emotion</source><volume>11</volume><fpage>181</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1037/a0022596</pub-id><pub-id pub-id-type="pmid">21401238</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGurk</surname> <given-names>H</given-names></name><name><surname>MacDonald</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Hearing lips and seeing voices</article-title><source>Nature</source><volume>264</volume><fpage>746</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1038/264746a0</pub-id><pub-id pub-id-type="pmid">1012311</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montagne</surname> <given-names>B</given-names></name><name><surname>Schutters</surname> <given-names>S</given-names></name><name><surname>Westenberg</surname> <given-names>HG</given-names></name><name><surname>van Honk</surname> <given-names>J</given-names></name><name><surname>Kessels</surname> <given-names>RP</given-names></name><name><surname>de Haan</surname> <given-names>EH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reduced sensitivity in the recognition of anger and disgust in social anxiety disorder</article-title><source>Cognitive Neuropsychiatry</source><volume>11</volume><fpage>389</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1080/13546800444000254</pub-id><pub-id pub-id-type="pmid">17354077</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montagne</surname> <given-names>B</given-names></name><name><surname>Kessels</surname> <given-names>RP</given-names></name><name><surname>De Haan</surname> <given-names>EH</given-names></name><name><surname>Perrett</surname> <given-names>DI</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The emotion recognition task: a paradigm to measure the perception of facial emotional expressions at different intensities</article-title><source>Perceptual and Motor Skills</source><volume>104</volume><fpage>589</fpage><lpage>598</lpage><pub-id pub-id-type="doi">10.2466/pms.104.2.589-598</pub-id><pub-id pub-id-type="pmid">17566449</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montgomery</surname> <given-names>KJ</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mirror neuron system differentially activated by facial expressions and social hand gestures: a functional magnetic resonance imaging study</article-title><source>Journal of Cognitive Neuroscience</source><volume>20</volume><fpage>1866</fpage><lpage>1877</lpage><pub-id pub-id-type="doi">10.1162/jocn.2008.20127</pub-id><pub-id pub-id-type="pmid">18370602</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neal</surname> <given-names>DT</given-names></name><name><surname>Chartrand</surname> <given-names>TL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Embodied emotion perception: amplifying and dampening facial feedback modulates emotion perception accuracy</article-title><source>Social Psychological and Personality Science</source><volume>2</volume><fpage>673</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1177/1948550611406138</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicolini</surname> <given-names>Y</given-names></name><name><surname>Manini</surname> <given-names>B</given-names></name><name><surname>De Stefani</surname> <given-names>E</given-names></name><name><surname>Coudé</surname> <given-names>G</given-names></name><name><surname>Cardone</surname> <given-names>D</given-names></name><name><surname>Barbot</surname> <given-names>A</given-names></name><name><surname>Bertolini</surname> <given-names>C</given-names></name><name><surname>Zannoni</surname> <given-names>C</given-names></name><name><surname>Belluardo</surname> <given-names>M</given-names></name><name><surname>Zangrandi</surname> <given-names>A</given-names></name><name><surname>Bianchi</surname> <given-names>B</given-names></name><name><surname>Merla</surname> <given-names>A</given-names></name><name><surname>Ferrari</surname> <given-names>PF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Autonomic responses to emotional stimuli in children affected by facial palsy: the case of moebius syndrome</article-title><source>Neural Plasticity</source><volume>2019</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1155/2019/7253768</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niedenthal</surname> <given-names>PM</given-names></name><name><surname>Mermillod</surname> <given-names>M</given-names></name><name><surname>Maringer</surname> <given-names>M</given-names></name><name><surname>Hess</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The simulation of smiles (SIMS) model: embodied simulation and the meaning of facial expression</article-title><source>Behavioral and Brain Sciences</source><volume>33</volume><fpage>417</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1017/S0140525X10000865</pub-id><pub-id pub-id-type="pmid">21211115</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberman</surname> <given-names>LM</given-names></name><name><surname>Winkielman</surname> <given-names>P</given-names></name><name><surname>Ramachandran</surname> <given-names>VS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Face to face: Blocking facial mimicry can selectively impair recognition of emotional expressions</article-title><source>Social Neuroscience</source><volume>2</volume><fpage>167</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1080/17470910701391943</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orlov</surname> <given-names>T</given-names></name><name><surname>Makin</surname> <given-names>TR</given-names></name><name><surname>Zohary</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Topographic representation of the human body in the occipitotemporal cortex</article-title><source>Neuron</source><volume>68</volume><fpage>586</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.032</pub-id><pub-id pub-id-type="pmid">21040856</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papeo</surname> <given-names>L</given-names></name><name><surname>Lingnau</surname> <given-names>A</given-names></name><name><surname>Agosta</surname> <given-names>S</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name><name><surname>Battelli</surname> <given-names>L</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The origin of word-related motor activity</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>1668</fpage><lpage>1675</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht423</pub-id><pub-id pub-id-type="pmid">24421174</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paracampo</surname> <given-names>R</given-names></name><name><surname>Tidoni</surname> <given-names>E</given-names></name><name><surname>Borgomaneri</surname> <given-names>S</given-names></name><name><surname>di Pellegrino</surname> <given-names>G</given-names></name><name><surname>Avenanti</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sensorimotor network crucial for inferring amusement from smiles</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>5116</fpage><lpage>5129</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw294</pub-id><pub-id pub-id-type="pmid">27660050</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname> <given-names>D</given-names></name><name><surname>Garrido</surname> <given-names>L</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name><name><surname>Duchaine</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Transcranial magnetic stimulation disrupts the perception and embodiment of facial expressions</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>8929</fpage><lpage>8933</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1450-08.2008</pub-id><pub-id pub-id-type="pmid">18768686</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poljac</surname> <given-names>E</given-names></name><name><surname>Montagne</surname> <given-names>B</given-names></name><name><surname>de Haan</surname> <given-names>EH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reduced recognition of fear and sadness in post-traumatic stress disorder</article-title><source>Cortex</source><volume>47</volume><fpage>974</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2010.10.002</pub-id><pub-id pub-id-type="pmid">21075363</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponari</surname> <given-names>M</given-names></name><name><surname>Conson</surname> <given-names>M</given-names></name><name><surname>D'Amico</surname> <given-names>NP</given-names></name><name><surname>Grossi</surname> <given-names>D</given-names></name><name><surname>Trojano</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mapping correspondence between facial mimicry and emotion recognition in healthy subjects</article-title><source>Emotion</source><volume>12</volume><fpage>1398</fpage><lpage>1403</lpage><pub-id pub-id-type="doi">10.1037/a0028588</pub-id><pub-id pub-id-type="pmid">22642357</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prevost</surname> <given-names>M</given-names></name><name><surname>Carrier</surname> <given-names>ME</given-names></name><name><surname>Chowne</surname> <given-names>G</given-names></name><name><surname>Zelkowitz</surname> <given-names>P</given-names></name><name><surname>Joseph</surname> <given-names>L</given-names></name><name><surname>Gold</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The reading the mind in the eyes test: validation of a french version and exploration of cultural variations in a multi-ethnic city</article-title><source>Cognitive Neuropsychiatry</source><volume>19</volume><fpage>189</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1080/13546805.2013.823859</pub-id><pub-id pub-id-type="pmid">23937473</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricciardi</surname> <given-names>L</given-names></name><name><surname>Visco-Comandini</surname> <given-names>F</given-names></name><name><surname>Erro</surname> <given-names>R</given-names></name><name><surname>Morgante</surname> <given-names>F</given-names></name><name><surname>Bologna</surname> <given-names>M</given-names></name><name><surname>Fasano</surname> <given-names>A</given-names></name><name><surname>Ricciardi</surname> <given-names>D</given-names></name><name><surname>Edwards</surname> <given-names>MJ</given-names></name><name><surname>Kilner</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Facial emotion recognition and expression in Parkinson’s Disease: An Emotional Mirror Mechanism?</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0169110</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives Bogart</surname> <given-names>K</given-names></name><name><surname>Matsumoto</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Facial mimicry is not necessary to recognize emotion: facial expression recognition by people with moebius syndrome</article-title><source>Social Neuroscience</source><volume>5</volume><fpage>241</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1080/17470910903395692</pub-id><pub-id pub-id-type="pmid">19882440</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname> <given-names>G</given-names></name><name><surname>Sinigaglia</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The functional role of the parieto-frontal mirror circuit: interpretations and misinterpretations</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>264</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1038/nrn2805</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruff</surname> <given-names>CC</given-names></name><name><surname>Driver</surname> <given-names>J</given-names></name><name><surname>Bestmann</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Combining TMS and fMRI: from 'virtual lesions' to functional-network accounts of cognition</article-title><source>Cortex</source><volume>45</volume><fpage>1043</fpage><lpage>1049</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2008.10.012</pub-id><pub-id pub-id-type="pmid">19166996</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rychlowska</surname> <given-names>M</given-names></name><name><surname>Cañadas</surname> <given-names>E</given-names></name><name><surname>Wood</surname> <given-names>A</given-names></name><name><surname>Krumhuber</surname> <given-names>EG</given-names></name><name><surname>Fischer</surname> <given-names>A</given-names></name><name><surname>Niedenthal</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Blocking mimicry makes true and false smiles look the same</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e90876</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0090876</pub-id><pub-id pub-id-type="pmid">24670316</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siebner</surname> <given-names>HR</given-names></name><name><surname>Hartwigsen</surname> <given-names>G</given-names></name><name><surname>Kassuba</surname> <given-names>T</given-names></name><name><surname>Rothwell</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How does transcranial magnetic stimulation modify neuronal activity in the brain? implications for studies of cognition</article-title><source>Cortex</source><volume>45</volume><fpage>1035</fpage><lpage>1042</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2009.02.007</pub-id><pub-id pub-id-type="pmid">19371866</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siebner</surname> <given-names>HR</given-names></name><name><surname>Rothwell</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Transcranial magnetic stimulation: new insights into representational cortical plasticity</article-title><source>Experimental Brain Research</source><volume>148</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1007/s00221-002-1234-2</pub-id><pub-id pub-id-type="pmid">12478392</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szanto</surname> <given-names>K</given-names></name><name><surname>Dombrovski</surname> <given-names>AY</given-names></name><name><surname>Sahakian</surname> <given-names>BJ</given-names></name><name><surname>Mulsant</surname> <given-names>BH</given-names></name><name><surname>Houck</surname> <given-names>PR</given-names></name><name><surname>Reynolds</surname> <given-names>CF</given-names></name><name><surname>Clark</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Social emotion recognition, social functioning, and attempted suicide in late-life depression</article-title><source>The American Journal of Geriatric Psychiatry</source><volume>20</volume><fpage>257</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1097/JGP.0b013e31820eea0c</pub-id><pub-id pub-id-type="pmid">22354116</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torfs</surname> <given-names>K</given-names></name><name><surname>Vancleef</surname> <given-names>K</given-names></name><name><surname>Lafosse</surname> <given-names>C</given-names></name><name><surname>Wagemans</surname> <given-names>J</given-names></name><name><surname>de-Wit</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The Leuven perceptual organization screening test (L-POST), an online test to assess mid-level visual perception</article-title><source>Behavior Research Methods</source><volume>46</volume><fpage>472</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.3758/s13428-013-0382-6</pub-id><pub-id pub-id-type="pmid">24190065</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torregrossa</surname> <given-names>LJ</given-names></name><name><surname>Bian</surname> <given-names>D</given-names></name><name><surname>Wade</surname> <given-names>J</given-names></name><name><surname>Adery</surname> <given-names>LH</given-names></name><name><surname>Ichinose</surname> <given-names>M</given-names></name><name><surname>Nichols</surname> <given-names>H</given-names></name><name><surname>Bekele</surname> <given-names>E</given-names></name><name><surname>Sarkar</surname> <given-names>N</given-names></name><name><surname>Park</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Decoupling of spontaneous facial mimicry from emotion recognition in schizophrenia</article-title><source>Psychiatry Research</source><volume>275</volume><fpage>169</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.psychres.2019.03.035</pub-id><pub-id pub-id-type="pmid">30921747</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trinkler</surname> <given-names>I</given-names></name><name><surname>Cleret de Langavant</surname> <given-names>L</given-names></name><name><surname>Bachoud-Lévi</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Joint recognition-expression impairment of facial emotions in Huntington's disease despite intact understanding of feelings</article-title><source>Cortex</source><volume>49</volume><fpage>549</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2011.12.003</pub-id><pub-id pub-id-type="pmid">22244587</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsuruya</surname> <given-names>N</given-names></name><name><surname>Kobayakawa</surname> <given-names>M</given-names></name><name><surname>Kawamura</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Is &quot;reading mind in the eyes&quot; impaired in Parkinson's disease?</article-title><source>Parkinsonism &amp; Related Disorders</source><volume>17</volume><fpage>246</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1016/j.parkreldis.2010.09.001</pub-id><pub-id pub-id-type="pmid">20889365</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van Rysewyk</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Beyond faces: The relevance of Moebius syndrome to emotion recognition and empathy</chapter-title><person-group person-group-type="editor"><name><surname>Freitas-Magalhaes</surname> <given-names>A</given-names></name></person-group><source>Emotional Expression: The Brain and Face</source><publisher-name>University of Fernando Press</publisher-name><fpage>75</fpage><lpage>97</lpage></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vannuscorps</surname> <given-names>G</given-names></name><name><surname>Pillon</surname> <given-names>A</given-names></name><name><surname>Andres</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Effect of biomechanical constraints in the hand laterality judgment task: where does it come from?</article-title><source>Frontiers in Human Neuroscience</source><volume>6</volume><elocation-id>299</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00299</pub-id><pub-id pub-id-type="pmid">23125830</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vannuscorps</surname> <given-names>G</given-names></name><name><surname>F Wurm</surname> <given-names>M</given-names></name><name><surname>Striem-Amit</surname> <given-names>E</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Large-Scale organization of the hand action observation network in individuals born without hands</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>3434</fpage><lpage>3444</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy212</pub-id><pub-id pub-id-type="pmid">30169751</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vannuscorps</surname> <given-names>G</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Typical biomechanical Bias in the perception of congenitally absent hands</article-title><source>Cortex</source><volume>67</volume><fpage>147</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.02.015</pub-id><pub-id pub-id-type="pmid">25824630</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vannuscorps</surname> <given-names>G</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Impaired short-term memory for hand postures in individuals born without hands</article-title><source>Cortex</source><volume>83</volume><fpage>136</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2016.07.019</pub-id><pub-id pub-id-type="pmid">27533132</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vannuscorps</surname> <given-names>G</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Typical action perception and interpretation without motor simulation</article-title><source>PNAS</source><volume>113</volume><fpage>86</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1073/pnas.1516978112</pub-id><pub-id pub-id-type="pmid">26699468</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vannuscorps</surname> <given-names>G</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016c</year><article-title>The origin of the biomechanical Bias in apparent body movement perception</article-title><source>Neuropsychologia</source><volume>89</volume><fpage>281</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.05.029</pub-id><pub-id pub-id-type="pmid">27238946</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verzijl</surname> <given-names>HT</given-names></name><name><surname>van der Zwaag</surname> <given-names>B</given-names></name><name><surname>Cruysberg</surname> <given-names>JR</given-names></name><name><surname>Padberg</surname> <given-names>GW</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Möbius syndrome redefined: a syndrome of rhombencephalic maldevelopment</article-title><source>Neurology</source><volume>61</volume><fpage>327</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1212/01.wnl.0000076484.91275.cd</pub-id><pub-id pub-id-type="pmid">12913192</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A unifying computational framework for motor control and social interaction</article-title><source>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</source><volume>358</volume><fpage>593</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1098/rstb.2002.1238</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname> <given-names>A</given-names></name><name><surname>Lupyan</surname> <given-names>G</given-names></name><name><surname>Sherrin</surname> <given-names>S</given-names></name><name><surname>Niedenthal</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Altering sensorimotor feedback disrupts visual discrimination of facial expressions</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>1150</fpage><lpage>1156</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0974-5</pub-id><pub-id pub-id-type="pmid">26542827</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54687.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Wood</surname><given-names>Adrienne</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>An influential theory of face perception builds on the general concept of embodied cognition, with the idea that this perceptual process requires the engagement of more motoric brain regions, those involved in the generation of facial expressions. By this view, recognition might entail the operation of simulation, where the perceiver simulates the action that might best match the sensory input to identify the affective intent. As a test of this hypothesis, the current study turns to an atypical population, individuals with Moebius Syndrome, a condition marked by congenital facial paralysis. The results show that some of these individuals exhibit normal performance on facial expression recognition, despite an absence of facial motor representations. As such, the results present an intriguing counter-example to the claim that facial expression recognition requires referent to motoric representations.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Efficient recognition of facial expressions does not require motor simulation&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen Richard Ivry as the Senior Editor and Reviewing Editor. The following individual involved in review of your submission has agreed to reveal their identity: Adrienne Wood.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission. As you will see, there are no serious issues for revision. It is mostly about the theoretical framing and the weight to be given individual cases-issues we have covered to some extent in our initial email correspondence. They did come up again in review and I've consolidated things so you have the feedback from some new &quot;samples.&quot; But I don't expect they will require much revision on your part.</p><p>Summary:</p><p>The study aims to assess whether individuals who suffer from Moebius syndrome (IMS) can achieve normotypical efficient facial expression recognition despite the congenital absence of relevant facial motor representations (a necessity hypothesis). The manuscript is well written. The research design is well thought out and uses a very high standard for the assessment of normotypical performance. The results show that some of the IMS patients perform well within the normal range on the emotional facial recognition tests even though they are unable to produce such expressions. As such, they present examples arguing against the necessity hypothesis, challenging simulation-based models of facial recognition. The study obviously suffers from a small sample, but this is a rare population.</p><p>Major Comments for Revision:</p><p>1) Theoretical framing:</p><p>a) Concern that the presentation of the motor theories of action perception are a straw man. Question the strong claim that the &quot;standard belief&quot; in the field is that sensorimotor activity is necessary for action perception.</p><p>b) More recent accounts of mimicry and the closely-related idea of sensorimotor simulation emphasize how motor-perceptual mappings could emerge through associative learning. If, over the course of development, an infant is regularly experiencing the correlation between their own facial movements and the visual perception of others' facial expressions (because adults are engaging in extensive mimicry), then they could learn to recruit this cross-modal association in the service of efficient interpretation of others' facial expressions. But if the infant never experiences that correlation-for instance, if their facial muscles are impaired-then that association won't be made and won't be drawn on to process the meaning of others' facial expressions. We agree that it is unlikely that perception of actions would require motor activity; humans can learn to recognize the differences between thousands of bird species without sensorimotor simulation, so if necessary, they could rely on purely visual activity to recognize the differences between others' facial movements. A fairer Introduction and Discussion would acknowledge these more nuanced sensorimotor perspectives, which are compatible with the present findings (see de Klerk et al., 2018; Catmur and Heyes, 2019; Koban et al., 2019).</p><p>c) The &quot;necessity hypothesis&quot; is ascribed to articles that do not make such claims (Wood et al., 2016; Goldman and Sripada, 2005). Wood et al., 2016, argue simply that sensorimotor activity can support facial expression processing, saying, &quot;whether chronic facial paralysis disrupts emotion recognition or compensatory perceptual strategies can eventually develop is not fully clear&quot;. Similarly, Goldman and Sripada, 2005 write about Moebius patients, &quot;…given the long-standing nature of their impairments, these subjects' normal performance may reflect the operation of a compensatory strategy.&quot; So at least these two papers are not properly represented in the present manuscript.</p><p>2) Weight to be given to individual cases, especially when they are the exception, not the rule:</p><p>Does the presence of two individuals with Moebius Syndrome performing within normal bounds contradict contemporary sensorimotor perspectives? And even here, the pattern is more of a trend. As shown in Figure 1, patients 8 and 10 are the top two IMS performers in three of the five expression recognition tasks.</p><p>3) Selectivity of impairment:</p><p>a) The IMS group perform less well overall than the controls in the facial emotion recognition. They also tend to perform less well on other tasks as well including emotion recognition from speech.</p><p>b) There is a high bar here for rejecting the necessity hypothesis, using a larger alpha (any p &lt;.20 would be taken as evidence that the Moebius patients are different from the healthy controls), discarding the low-performing healthy controls, and including the non-facial expression tasks. Moebius syndrome can have other symptoms besides facial motor impairment-importantly, visual impairment-that might account for deficits in perception tasks. Indeed, there is a correlation between the patients' mid-level visual screening test performance and their performance on the facial expression tasks, suggesting any group differences might be due to visual, not motor, disruptions.</p><p>4) Additional analyses:</p><p>Consider a between-group comparison of performance on the expression recognition tasks, since that's the analysis most people would expect. This could even include performance on the facial identity tasks as a covariate to control for &quot;general&quot; performance on such tasks.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Efficient recognition of facial expressions does not require motor simulation&quot; for consideration by <italic>eLife</italic>. I felt it was only important to engage one of the original reviewers, Adrienne Wood, to assist me in evaluating the revision.</p><p>We think the paper is essentially there but Adrienne has one substantive request and I find it reasonable. To quote her:</p><p>I appreciate the authors' thorough responses to our comments on their manuscript. I continue to think that in an effort to carefully define and then reject the motor simulation hypothesis, they have shifted the burden of proof too far in the other direction. As we agreed originally, I do think the case study nature of their approach is ok. If they can find any IMS with normal expression recognition, then it brings into question the strong motor simulation position that motor functioning is “necessary” for expression recognition on challenging tasks. But I think they go too far when they reject even running the between-group statistical test (reviewer point #4) that might create a more nuanced picture. Perhaps IMS often suffer expression recognition impairment (even controlling for more general visual impairment), but not always. The authors write, &quot;we would prefer to limit the statistical analysis presented in the paper to only those meaningful for the question at hand: is it possible to achieve efficient facial expression recognition without motor simulation.&quot; There is no rule that secondary questions cannot be answered in a Results section; they could identify those analyses as &quot;post-hoc&quot; if they are concerned about fishing.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54687.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Major Comments for Revision:</p><p>1) Theoretical framing:</p><p>a) Concern that the presentation of the motor theories of action perception are a straw man. Question the strong claim that the &quot;standard belief&quot; in the field is that sensorimotor activity is necessary for action perception.</p></disp-quote><p>The prediction that we tested is not simply whether motor simulation is “necessary for action perception”. The experiments reported in this study were specifically designed to test the more fine-grained hypothesis that motor simulation is necessary for <italic>efficient</italic> (i.e., fast and accurate) facial expression recognition, in particular when the tasks are challenging, such as when facial expressions must be interpreted after only a short exposure (Experiment 1 and 2), when they are more complex or ambiguous (Experiment 3 and 4) or when the task requires subtle intra-category discriminations such as discriminating fake versus genuine smiles, like it is predicted by numerous authors (see below). There is no doubt that this hypothesis is currently widespread in the field. To quote only some of the most recent and influential articles, for instance:</p><p>Niedenthal et al., 2010, propose that “Embodied simulation supports the recognition and access to meaning of facial expressions”. They further clarify their position and explain that although “recognition tasks on prototypic expressions can be accomplished by perceptual analysis alone, without motor mimicry (….) in some cases, however, mimicry does facilitate recognition”. They go on providing three examples of tasks in which motor simulation may contribute to facial expression recognition: “Niedenthal et al., 2001, for example, observed effects of mimicry when participants had to detect the boundary of facial expression between happiness and sadness. In a more recent study, Stel and van Knippenberg, 2008 found that blocking mimicry affected the speed, but not the accuracy, of categorizing facial expressions as positive or negative. These ﬁndings point to the possibility that simulation does become important in recognition tasks when they require ﬁne distinctions in smile meaning, such as the processing of different smile types.” These three hypotheses were specifically tested in our Experiments 3, 1 and 2, and 5, respectively.</p><p>Wood et al., 2016’s paper “Sensorimotor Simulation Contributes to Facial Expression Recognition”, proposes that “When we observe a facial expression of emotion, we often mimic it. This automatic mimicry reflects underlying sensorimotor simulation that supports accurate emotion recognition.” In the paragraph titled “Simulation and the Recognition of Facial Expression”, they note that “Substantial research evidence suggests that sensorimotor simulation contributes to accurate and efficient recognition of the specific emotion, valence, intensity and intentionality conveyed by facial expressions.” And, they go on with “Factors that disrupt somatosensory feedback from, or motor output to, the facial muscles are expected to affect emotion recognition.” and continue noting that motor simulation may be particularly useful precisely for the types of tasks that we presented to our participants: “Sensorimotor simulation may be especially useful for emotion recognition when the perceived expression is subtle or ambiguous, or the judgment to be made particularly challenging. Judging the authenticity of a smile, for instance, is a complex task that relies on detecting subtle differences in the morphological specificities and temporal dynamics of the smile. (…). Disrupting mimicry impairs the ability of observers to distinguish between videos of people producing spontaneous and intentional smiles” They end the paragraph with “to summarize the current evidence, automatic sensorimotor simulation plays a functional role not only in the recognition of actions, but also in the processing of emotional expressions”.</p><p>Paracampo et al., 2016’s paper “sensorimotor network crucial for inferring amusement from smiles” argues that “ motor and somatosensory circuits for controlling and sensing facial movements are causally essential for inferring amusement from another’s smile”, once again a prediction that applies precisely to the ability that we tested in one of our experiments (Experiment 5). Note that, in fact, we used precisely the same stimuli as Paracampo and colleagues.</p><p>Ipser and Cook, 2016, conclude their article by emphasizing that their findings “are consistent with models proposing that the motor system makes <italic>a causal contribution</italic> to the perception and interpretation of facial expressions”.</p><p>We think the paper is clear about the hypotheses that we tested. When presenting the motor simulation theories of facial expression recognition, for instance, we wrote that they assume that “<italic>efficient</italic> (i.e., fast and accurate) facial expression recognition cannot be achieved by visual analysis <italic>alone</italic> but requires a process of motor simulation”. At a later point we are even more specific and wrote “previous findings left open the possibility that motor simulation may contribute to facial expression recognition efficiency when the tasks are more challenging, such as when facial expressions are more complex, must be interpreted quickly, only partial information is available, or when the task requires subtle intra-category discriminations such as discriminating fake versus genuine smiles. The research reported here was designed to explore this large remaining hypothesis space for a role of motor simulation in facial expression recognition: is it possible to achieve efficient facial expression recognition without motor simulation in the type of sensitive and challenging tasks that have been cited by proponents of the motor theories as examples of tasks in which motor simulation is needed to support facial expression recognition?”.</p><disp-quote content-type="editor-comment"><p>b) More recent accounts of mimicry and the closely-related idea of sensorimotor simulation emphasize how motor-perceptual mappings could emerge through associative learning. If, over the course of development, an infant is regularly experiencing the correlation between their own facial movements and the visual perception of others' facial expressions (because adults are engaging in extensive mimicry), then they could learn to recruit this cross-modal association in the service of efficient interpretation of others' facial expressions. But if the infant never experiences that correlation-for instance, if their facial muscles are impaired-then that association won't be made and won't be drawn on to process the meaning of others' facial expressions. We agree that it is unlikely that perception of actions would require motor activity; humans can learn to recognize the differences between thousands of bird species without sensorimotor simulation, so if necessary, they could rely on purely visual activity to recognize the differences between others' facial movements. A fairer Introduction and Discussion would acknowledge these more nuanced sensorimotor perspectives, which are compatible with the present findings (see de Klerk et al., 2018; Catmur and Heyes, 2019; Koban et al., 2019).</p></disp-quote><p>In this paper, we tested whether congenital facial paralysis hampers facial expression recognition efficiency in challenging tasks, as is predicted by various theories of facial expression recognition based on motor simulation (e.g., the generate-and-test model, the reverse simulation model, Niedenthal and coworkers’ simulation of smiles hypothesis). The reviewer asks us to acknowledge the possibility that motor simulation could be necessary for efficient facial expression recognition (would be “drawn on to process the meaning of others' facial expressions”) <italic>only</italic> in people in whom associative learning has formed connections between sensory and motor representations of actions.</p><p>We followed this suggestion, we now write “In sum, our finding constitutes existence proof that the visuo-perceptual system can support efficient facial expression recognition unaided by motor simulation. Of course, this does not imply that motor simulation does not support facial expression in typically developed participants. For example, correlated sensorimotor experience may be necessary for the development of motor contributions to action perception (e.g., Catmur and Heyes, 2019) and IMS 8 and 10 may have developed an atypically efficient visual system to compensate for their congenital paralysis.”</p><p>Importantly, however, there is currently no evidence that the IMS would perform the task differently and no empirical evidence that motor simulation is indeed necessary for individuals who have benefitted from correlated visuo-motor experience. Hence, our results at the very least emphasize the need for a shift in the burden of proof regarding this question.</p><disp-quote content-type="editor-comment"><p>c) The &quot;necessity hypothesis&quot; is ascribed to articles that do not make such claims (Wood et al., 2016; Goldman and Sripada, 2005). Wood et al., 2016, argue simply that sensorimotor activity can support facial expression processing, saying, &quot;whether chronic facial paralysis disrupts emotion recognition or compensatory perceptual strategies can eventually develop is not fully clear&quot;. Similarly, Goldman and Sripada, 2005, write about Moebius patients, &quot;…given the long-standing nature of their impairments, these subjects' normal performance may reflect the operation of a compensatory strategy.&quot; So at least these two papers are not properly represented in the present manuscript.</p></disp-quote><p>We cite these two papers in the Introduction : “an alternative view suggesting that efficient (i.e., fast and accurate) facial expression recognition cannot be achieved by visual analysis alone but requires a process of motor simulation – an unconscious, covert imitation of the observed facial postures or movements – has gained considerable prominence”.</p><p>We decided to cite Goldman and Sripada, 2005, on the ground that they discuss two potential simulationist models (the generate-and-test model on p202 and the reverse simulation model on p203, see Figure 2 on P. 204) and emphasize (p. 209) that these two models predict “reduced recognition” of facial expressions in case of a decrease of motor resources (e.g., dual task paradigm). The prediction that “reduced recognition” should result from a decrease of motor resources follows necessarily from the claim that motor simulation is necessary for efficient facial expression recognition.</p><p>Even though Wood et al., 2016, wrote “whether chronic facial paralysis disrupts emotion recognition or compensatory perceptual strategies can eventually develop is not fully clear”, we decided to cite that paper on the ground that, on the first page of the article, they write : “People’s recognition and understanding of others’ facial expressions is compromised by experimental (e.g., mechanical blocking) and clinical (e.g., facial paralysis and long-term pacifier use) disruption to sensorimotor processing in the face. Emotion perception involves automatic activation of pre and primary motor and somatosensory cortices and the inhibition of activity in sensorimotor networks reduces performance on subtle or challenging emotion recognition tasks. Sensorimotor simulation flexibly supports not only conceptual processing of facial expression but also, through cross-modal influences on visual processing, the building of a complete percept of the expression”. The interfering, automatic and constitutive nature of the role assigned to motor simulation by the authors in their article led us to assume that they view motor simulation as necessary for efficient recognition of facial expression.</p><p>Admittedly, both Goldman and Sripada, 2005, and Wood et al., 2016, mention that “compensatory strategies” may develop in individuals with facial paralysis. However, as this claim is not supported by any evidence (or explanation of what that compensatory strategy could be or why it would not develop in typically developed people) it seems to serve no other purpose than insulating the motor simulation hypothesis from the implications of the performance of these individuals. Hence, while the authors refer to “compensatory strategies” to discard incompatible evidence from patients with facial paralysis, they nevertheless cite evidence that “People’s recognition and understanding of others’ facial expressions is compromised by experimental (e.g., mechanical blocking) and clinical (e.g., facial paralysis and long-term pacifier use) disruption to sensorimotor processing in the face” as evidence in favor of the same theory.</p><p>In the revised version of the manuscript we nevertheless decided to follow the reviewer’s suggestion and delete the reference to Wood et al., 2016, and we took the liberty of replacing it with a more recent paper with a less ambiguous stand on the role of motor simulation in the recognition of motor actions (Paracampo et al., 2017).</p><disp-quote content-type="editor-comment"><p>2) Weight to be given to individual cases, especially when they are the exception, not the rule:</p><p>Does the presence of two individuals with Moebius Syndrome performing within normal bounds contradict contemporary sensorimotor perspectives? And even here, the pattern is more of a trend. As shown in Figure 1, patients 8 and 10 are the top two IMS performers in three of the five expression recognition tasks.</p></disp-quote><p>We identified two separate issues in this comment.</p><p>The first concerns whether “the presence of two individuals with Moebius Syndrome performing within normal bounds contradict contemporary sensorimotor perspectives?”. It depends what the reviewer means by “contradict”. As written in the final paragraph of the Discussion, we propose that this finding constitutes existence proof that the visuo-perceptual system can support efficient facial expression recognition unaided by motor simulation. However, this observation does not necessarily <italic>imply</italic> that motor simulation does not support facial expression in typically developed participants. Nevertheless, in the absence of convincing evidence that motor simulation is used in the normotypical population, this finding emphasizes the importance of a shift in the burden of proof regarding the role of motor simulation for facial expression recognition.</p><p>The second concerns the weight to be given to individual cases vs the group (or the other individuals), especially when the cases are the exception, not the rule. We suggest that the issue should be considered on a case-by-case basis: one should ask which result constitutes the most valid constrain for the theories tested and what is the most plausible account of the discrepancy between the cases and the rest of the group.</p><p>Concluding that facial paralysis hampers facial expression recognition based on the result of the 9 IMS who perform among the weak participants (<italic>p &lt; 0.2)</italic> in at least one experiment would require that (1) their weak performance can be related specifically to their facial paralysis and (2) that a plausible account of the performance of the two best IMS compatible with the simulation thesis can be formulated. In the paper, however, we found no empirical support for either (1) or (2). In contrast to (1), we report that the weak performance of the 9 IMS in facial expressions tasks is likely not due to their facial paralysis but rather, to their associated visual and/or cognitive disorder (Results final paragraph). In contrast to (2), we found that the performance of IMS 8 and 10 cannot be explained by them being extraordinarily efficient individuals across-the-board (In comparison to control participants, these two individuals were as efficient in facial expression recognition experiments than in other experiments, see Results paragraph three) or by them being less severely paralyzed (Results paragraph six).</p><p>In sum, in the specific case of our study, the data from the 9 IMS provides no support for the motor simulation theory. In contrast, the results of IMS8 and 10 provide unambiguous evidence that efficient facial expression recognition can be achieved in the absence of facial movements.</p><disp-quote content-type="editor-comment"><p>3) Selectivity of impairment:</p><p>a) The IMS group perform less well overall than the controls in the facial emotion recognition. They also tend to perform less well on other tasks as well including emotion recognition from speech.</p></disp-quote><p>This is correct. As written: “Moebius syndrome typically impacts not only the individuals’ sensorimotor system, but also their visual, perceptual, cognitive, and social abilities.” Therefore, a weaker than normal average performance across-the-board is expected.</p><disp-quote content-type="editor-comment"><p>b) There is a high bar here for rejecting the necessity hypothesis, using a larger alpha (any p &lt;.20 would be taken as evidence that the Moebius patients are different from the healthy controls), discarding the low-performing healthy controls, and including the non-facial expression tasks. Moebius syndrome can have other symptoms besides facial motor impairment-importantly, visual impairment-that might account for deficits in perception tasks. Indeed, there is a correlation between the patients' mid-level visual screening test performance and their performance on the facial expression tasks, suggesting any group differences might be due to visual, not motor, disruptions.</p></disp-quote><p>We thank the reviewer for this positive assessment of our approach. Setting such as high bar allowed us to minimize the likelihood of false negatives (i.e., the risk to conclude erroneously that an IMS achieves a “normotypical level” of efficiency) and provided more power to detect even subtle degradations of facial expression recognition skills.</p><disp-quote content-type="editor-comment"><p>4) Additional analyses:</p><p>Consider a between-group comparison of performance on the expression recognition tasks, since that's the analysis most people would expect. This could even include performance on the facial identity tasks as a covariate to control for &quot;general&quot; performance on such tasks.</p></disp-quote><p>We thank the reviewer for this suggestion but we would prefer to limit the statistical analysis presented in the paper to only those meaningful for the question at hand: is it possible to achieve efficient facial expression recognition without motor simulation. We are already clear about this in the paper (Introduction section).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>We think the paper is essentially there but Adrienne has one substantive request and I find it reasonable. To quote her:</p><p>I appreciate the authors' thorough responses to our comments on their manuscript. I continue to think that in an effort to carefully define and then reject the motor simulation hypothesis, they have shifted the burden of proof too far in the other direction. As we agreed originally, I do think the case study nature of their approach is ok. If they can find any IMS with normal expression recognition, then it brings into question the strong motor simulation position that motor functioning is “necessary” for expression recognition on challenging tasks. But I think they go too far when they reject even running the between-group statistical test (reviewer point #4) that might create a more nuanced picture. Perhaps IMS often suffer expression recognition impairment (even controlling for more general visual impairment), but not always. The authors write, &quot;we would prefer to limit the statistical analysis presented in the paper to only those meaningful for the question at hand: is it possible to achieve efficient facial expression recognition without motor simulation.&quot; There is no rule that secondary questions cannot be answered in a Results section; they could identify those analyses as &quot;post-hoc&quot; if they are concerned about fishing.</p></disp-quote><p>We decided to follow the reviewer and the Editor’s suggestion to include between-group statistical tests in the result section of the manuscript devoted to discussing whether the IMS’s performance variability is most likely due to associated visual and/or cognitive disorders or to their facial paralysis. We have now added:</p><p>“To further explore this possibility, we first performed a series of one-way ANCOVAs with Group as a fixed effect and participants’ performance in Experiments 6-8 as covariates. Participants performing below 2 absolute deviations from the median of their group were discarded in order to satisfy ANCOVA normality assumptions (Leys, Ley, Klein, Bernard and Licata, 2013). In line with the possibility that the IMS’s performance variability is likely due to associated visual and/or cognitive disorders rather than to their facial paralysis, these analyses failed to reveal any significant effect of Group in Experiments 1, 2, 4 and 5 (all Fs &lt; 1; all <italic>p</italic>s &gt; 0.37). The same analysis could not be performed on the results of Experiment 3 due to the low number of IMS who took part to this experiment.”</p><p>In addition, in order to address more directly the possibility raised by the reviewer that “Perhaps IMS often suffer expression recognition impairment (even controlling for more general visual impairment), but not always”, we now report the results of a new set of single case analyses allowing to test this possibility:</p><p>“Then, to test more directly the possibility that at least some IMS may have some facial expression recognition impairment that cannot be explained by a visual or cognitive deficit, we used Crawford and Garthwaite’s, 2007, Bayesian Standardized Difference Test (BSDT) to test whether the performance of IMS1-7, 9 and 10 in at least some facial expression experiments may be comparatively less good than in the three control tasks. Only IMS1 performed significantly less well in Experiment 5 than in the three control experiments (Experiments 6-8). All other comparisons indicated either better performance in facial expression recognition experiments than in at least one of the control experiments (55% of the comparisons) or slightly but non-significantly weaker performance (45% of comparisons, all BSDTs &gt; 0.1).”</p><p>In sum, these two additional series of analyses corroborate the conclusion that “the IMS’s performance variability seems to be at least partly due to associated visual and/or cognitive disorders, rather than to their facial paralysis”.</p></body></sub-article></article>