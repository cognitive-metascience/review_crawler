<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">44516</article-id><article-id pub-id-type="doi">10.7554/eLife.44516</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Integrating prediction errors at two time scales permits rapid recalibration of speech sound categories</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-127697"><name><surname>Olasagasti</surname><given-names>Itsaso</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5172-5373</contrib-id><email>itsaso.olasagasti@gmail.com</email><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23745"><name><surname>Giraud</surname><given-names>Anne-Lise</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution>Department of Basic Neuroscience, University of Geneva</institution><addr-line><named-content content-type="city">Geneva</named-content></addr-line><country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>30</day><month>03</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e44516</elocation-id><history><date date-type="received" iso-8601-date="2019-01-17"><day>17</day><month>01</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-03-17"><day>17</day><month>03</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Olasagasti and Giraud</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Olasagasti and Giraud</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-44516-v2.pdf"/><abstract><p>Speech perception presumably arises from internal models of how specific sensory features are associated with speech sounds. These features change constantly (e.g. different speakers, articulation modes etc.), and listeners need to recalibrate their internal models by appropriately weighing new versus old evidence. Models of speech recalibration classically ignore this volatility. The effect of volatility in tasks where sensory cues were associated with arbitrary experimenter-defined categories were well described by models that continuously adapt the learning rate while keeping a single representation of the category. Using neurocomputational modelling we show that recalibration of <italic>natural</italic> speech sound categories is better described by representing the latter at different time scales. We illustrate our proposal by modeling fast recalibration of speech sounds after experiencing the McGurk effect. We propose that working representations of speech categories are driven both by their current environment and their long-term memory representations.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>People can distinguish words or syllables even though they may sound different with every speaker. This striking ability reflects the fact that our brain is continually modifying the way we recognise and interpret the spoken word based on what we have heard before, by comparing past experience with the most recent one to update expectations. This phenomenon also occurs in the McGurk effect: an auditory illusion in which someone hears one syllable but sees a person saying another syllable and ends up perceiving a third distinct sound.</p><p>Abstract models, which provide a functional rather than a mechanistic description of what the brain does, can test how humans use expectations and prior knowledge to interpret the information delivered by the senses at any given moment. Olasagasti and Giraud have now built an abstract model of how brains recalibrate perception of natural speech sounds. By fitting the model with existing experimental data using the McGurk effect, the results suggest that, rather than using a single sound representation that is adjusted with each sensory experience, the brain recalibrates sounds at two different timescales.</p><p>Over and above slow “procedural” learning, the findings show that there is also rapid recalibration of how different sounds are interpreted. This working representation of speech enables adaptation to changing or noisy environments and illustrates that the process is far more dynamic and flexible than previously thought.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>adaptation</kwd><kwd>speech</kwd><kwd>systems modeling</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>320030B_182855</award-id><principal-award-recipient><name><surname>Giraud</surname><given-names>Anne-Lise</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Keeping flexible adaptable representations of speech categories at different time scales allows the brain to maintain stable perception in the face of varying speech sound characteristics.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The way the brain processes sensory information to represent the perceived world is flexible and varies depending on changes in the stimulus landscape. Neurocognitive adaptation to varying stimuli can be driven by an explicit external feedback signal, but might also take place with simple passive exposure to a changing stimulus environment via implicit statistical learning (<xref ref-type="bibr" rid="bib57">Saffran et al., 1996</xref>; <xref ref-type="bibr" rid="bib21">Gilbert et al., 2001</xref>; <xref ref-type="bibr" rid="bib4">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Schwiedrzik et al., 2014</xref>). In the domain of speech perception, neural representations of sound categories are susceptible to stimulus-driven recalibration. Typically, the perception of unclear or ambiguous speech stimuli that have previously been disambiguated by context (<xref ref-type="bibr" rid="bib46">McQueen et al., 2006</xref>; <xref ref-type="bibr" rid="bib10">Clarke and Luce, 2005</xref>) or by a concurrent visual stimulus (<xref ref-type="bibr" rid="bib7">Bertelson et al., 2003</xref>; <xref ref-type="bibr" rid="bib67">Vroomen et al., 2007</xref>) is biased by the disambiguating percept. Even simple exposure to novel statistics e.g., a variation in the spread of sensory features characteristic of stop consonants, quickly results in a modified slope in psychometric functions, and changes the way listeners classify stimuli (<xref ref-type="bibr" rid="bib11">Clayards et al., 2008</xref>). Interestingly, acoustic representations are also modified after altered auditory feedback during production (<xref ref-type="bibr" rid="bib49">Nasir and Ostry, 2009</xref>; <xref ref-type="bibr" rid="bib33">Lametti et al., 2014</xref>; <xref ref-type="bibr" rid="bib54">Patri et al., 2018</xref>). These observations illustrate that speech sound categories remain largely plastic in adulthood.</p><p>Using two-alternative forced choice tasks, studies have shown that changes in acoustic speech categories can be induced by input from the visual modality (e.g. <xref ref-type="bibr" rid="bib7">Bertelson et al., 2003</xref>). Reciprocally, acoustic information can also disambiguate lipreading (<xref ref-type="bibr" rid="bib2">Baart and Vroomen, 2010</xref>), resulting in measurable categorization aftereffects. While such effects can be observed after repeated exposure to the adapting stimuli, recalibration can also occur very rapidly, with effects being observable after a single exposure (<xref ref-type="bibr" rid="bib67">Vroomen et al., 2007</xref>). This fast and dynamic process has been modelled as incremental Bayesian updating (<xref ref-type="bibr" rid="bib28">Kleinschmidt and Jaeger, 2015</xref>) where internal perceptual categories track the stimulus statistics. According to this model, when listeners are confronted with altered versions of known speech categories, the perceived category representation is updated to become more consistent with the actual features of the stimulus. The resulting recalibration weighs all evidence equally, disregarding their recency. The model hence successfully describes perceptual changes observed when listeners are confronted with repeated presentations of a single modified version of a speech sound. However, it cannot appropriately deal with intrinsically changing environments, in which sensory cues quickly become obsolete. Real moment to moment physical changes in the environment are referred to as ‘volatility’ to distinguish them from the trial-by-trial response variability observed in a fixed environment. Inference in volatile environments has been studied mostly in relation to decision making tasks in which participants can use an explicit feedback to keep track of the varying statistics of arbitrary cue-reward associations (e.g. <xref ref-type="bibr" rid="bib5">Behrens et al., 2007</xref>) or arbitrarily defined categories (e.g. <xref ref-type="bibr" rid="bib62">Summerfield et al., 2011</xref>). These studies suggest that humans are able to adjust their learning rate to the volatility in the stimulus set, with faster learning rates (implying a stronger devaluation of recent past evidence) in more volatile environments. This has led to normative models focussing on the online estimation of volatility, in which task-relevant features are represented at a single variable time scale (<xref ref-type="bibr" rid="bib5">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib43">Mathys et al., 2011</xref>).</p><p>The notion of variable learning rates likely also applies to speech processing. However, we propose that when recalibrating natural speech categories, a normative model should additionally take into account that these categories may themselves change at different time scales. For example, transient acoustic changes within a given speech sound category, such as those coming from a new speaker, must not interfere with the long-term representation of that category that should be invariant for example to speakers. We therefore hypothesize that speech sound categories could be represented by more than a single varying timescale.</p><p>Although speech category recalibration has not been systematically studied in variable environments, Lüttke and collaborators (<xref ref-type="bibr" rid="bib37">Lüttke et al., 2016a</xref>; <xref ref-type="bibr" rid="bib39">Lüttke et al., 2018</xref>) found evidence for recalibration in an experiment that included audio-visual McGurk stimuli shown without an explicit adapting condition. The first study involved six different vowel/consonant/vowel stimuli presented in random order, and recalibration was observed even when acoustic stimuli were not ambiguous (e.g. the /aba/ sound in the McGurk trials). The McGurk effect (the fact that an acoustic stimulus /aba/is mostly perceived as an illusory/ada/ when presented with the video of a speaker producing /aga/) was powerful enough to yield observable adaptive effects across consecutive trials. Specifically, the probability of an acoustic/aba/ to be categorized as/ada/, was higher when the trial was preceded by an audio-visual McGurk fusion. Recalibration effects do not generalize to all phonetic contrasts/categories (<xref ref-type="bibr" rid="bib56">Reinisch et al., 2014</xref>). After participants had recalibrated acoustic sounds in the /aba /- /ada/continuum (on the basis of acoustic formant transitions), recalibration was neither present for /ibi /- /idi/ (cued by burst and frication), nor for /ama/- /ana/or to /ubu/- /udu/continua (both cued by formant transitions). Likewise, in a word recognition task, participants were able to keep different F0/VOT (fundamental frequency/voice onset time) correlation statistics for different places of articulation (<xref ref-type="bibr" rid="bib23">Idemaru and Holt, 2014</xref>). Based on this failure to generalize, we hypothesized that very short-term changes can modify the internal mapping between sensory features and sublexical speech categories rather than phonemic categories.</p><p>To test this hypothesis, we simulated Lüttke et al.’s experiment (<xref ref-type="bibr" rid="bib37">Lüttke et al., 2016a</xref>), using an audiovisual integration model based on hierarchical Bayesian inference. The model was a version of a previous model of the McGurk effect (<xref ref-type="bibr" rid="bib51">Olasagasti et al., 2015</xref>) that further included an adaptation mechanism using residual prediction errors to update internal representations associated with the perceived category. The model divides the process in two steps: perceptual inference and internal model recalibration. During perceptual inference the model takes the sensory input and infers a perceived speech category, by choosing the category that minimizes sensory prediction error. However, ‘residual’ prediction errors might remain following perceptual inference. This is typically the case after McGurk fusion; since the best explanation for the multisensory input, ‘ada’, is neither the audio /aba/ nor the visual /aga/, ‘residual’ prediction errors remain in both acoustic and visual modalities.</p><p>The best match to the experimental results described above (<xref ref-type="bibr" rid="bib37">Lüttke et al., 2016a</xref>) was obtained when we considered that adaptation included two different time scales, resulting in 1) a transient effect leading to recalibration towards the most recently presented stimulus features, decaying towards 2) a longer-lasting representation corresponding to a mapping between category and stimulus features determined within a longer time span.</p><p>Overall, these findings are consistent with theories that posit that the brain continuously recalibrates generative (forward) models to maintain self-consistency (e.g., <xref ref-type="bibr" rid="bib17">Friston et al., 2010</xref>), and offers a neuro-computationally plausible implementation to resolve cognitive conflicts, which can sometimes appear as irrational behaviors, such as in post-choice re-evaluation of alternatives (<xref ref-type="bibr" rid="bib13">Coppin et al., 2010</xref>; <xref ref-type="bibr" rid="bib25">Izuma et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Colosio et al., 2017</xref>; <xref ref-type="bibr" rid="bib53">Otten et al., 2017</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Simulation of the perceptual decision process</title><p>In a re-assessment of an existing dataset (<xref ref-type="bibr" rid="bib37">Lüttke et al., 2016a</xref>) selected participants with high percentage of fused percepts for McGurk stimuli. When presented with acoustic /aba/ together with a video of a speaker articulating /aga/ the most frequent percept was /ada/; we refer to these as fused McGurk trials. These listeners were combining information from acoustic (A) and visual (V) modalities since the same /aba/ acoustic token was correctly categorized as /aba/ when presented alone. After a fused McGurk trial, participants showed recalibration. They classified acoustic only /aba/ stimuli as ‘ada’ more frequently (29% ‘ada’ percepts) than when the acoustic only /aba/was presented after any other stimulus (16% ‘ada’ percepts).</p><p>Our goal was to compare generative models that interpret sensory input and continuously recalibrate themselves to best match the incoming input. Unlike many other studies of speech recalibration, the stimulus generating recalibration in Lüttke et al. (the McGurk stimulus) was not presented alone, but as part of a set of six stimuli that were presented in random order, thus making transient effects detectable. The assessment involved three acoustic only stimuli with sounds corresponding to /aba/, /ada/ or/aga/; and three audiovisual stimuli – congruent /aba/, congruent /ada/, and the McGurk stimulus (acoustic /aba/with video of /aga/).</p><p>We simulated Lüttke et al.’s experiment by using a generative model relating the three possible speech categories (/aba/, /ada/ and /aga/) to the sensory input. We characterized sensory input with a visual feature, the amplitude of lip closure during the transition between the two vowels (s<sub>V</sub>); and an acoustic feature, the amplitude of the 2<sup>nd</sup> formant transition (s<sub>A</sub>). We use ‘A’ to refer to quantities related to the acoustic feature and ‘V’ to quantities related to the visual feature.</p><p>The internal generative model that characterizes the participant, described in detail in the methods section, generates sensory inputs (s<sub>A</sub> and s<sub>V</sub>) for the congruent versions of each of the three possible categories (k = /aba/, /ada/, /aga/). The model has a representation for each congruent category based on a Gaussian distribution in a two-dimensional feature space, itself a product of two univariate Gaussian distributions centered at (θ<sub>k,V</sub> θ<sub>k,A</sub>) and with standard deviations (σ<sub>k,V</sub>, σ<sub>k,A</sub>) for tokens k = {/aba/,/ada/,/aga/} (<xref ref-type="fig" rid="fig1">Figure 1</xref>, right panel). The model assumes that given a speech token k, 2<sup>nd</sup> formant amplitude C<sub>A</sub> and lip closure amplitude C<sub>V</sub> for each individual trial are chosen from the corresponding Gaussian distribution (<xref ref-type="fig" rid="fig1">Figure 1</xref>, right panel). Once values for C<sub>A</sub> and C<sub>V</sub> have been determined, sensory lip closure and 2<sup>nd</sup> formant transitions are obtained by adding sensory noise (parameterized by σ<sub>V</sub> and σ<sub>A</sub>), to obtain the sensory input: s<sub>A</sub> and s<sub>V</sub>. During inference, the model is inverted and provides the posterior probability of a token given the noisy sensory input p(k|s<sub>V</sub>,s<sub>A</sub>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematics of the generative model.</title><p>In a given trial, a speech token ‘k’ determines the amplitudes of degree of lip closure (C<sub>V</sub>) and magnitude of second formant deflection (C<sub>A</sub>) by sampling from the appropriate Gaussian distribution. The distributions corresponding to each speech token ‘k’ are represented in the two-dimensional feature space on the right panel. The model also includes sensory noise to account for how these features appear at the sensory periphery (s<sub>V</sub> and s<sub>A</sub>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44516-fig1-v2.tif"/></fig><p>In both acoustic and audio-visual trials the listener was asked to report the perceived acoustic stimulus in a three-alternative forced choice task. To simulate the listener’s choice, we calculated the probability of the acoustic token given the stimulus. In our notation, this probability is expressed by p(k<sub>A</sub>|s<sub>V</sub>,s<sub>A</sub>) for audiovisual stimuli and by p(k<sub>A</sub>|s<sub>A</sub>) for unimodal acoustic stimuli (see Materials and Methods for details). The percept at a single trial was determined by choosing the category that maximizes the posterior.</p><p>The model qualitatively reproduces the average performance across participants in the task. We chose parameters that elicit a very high rate of McGurk percepts (<xref ref-type="fig" rid="fig2">Figure 2</xref>, middle panel of the bottom row) and assumed that listeners were always integrating the two sensory streams.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Model’s overall performance.</title><p>Simulation of the <xref ref-type="bibr" rid="bib37">Lüttke et al. (2016a)</xref> experiment. Model classification across all trials. Each subpanel shows the percentage of /aba/, /ada/ and /aga/ percepts corresponding to each of the six conditions (Ab: acoustic only /aba/; Ad: acoustic only /ada/; Ag: acoustic only /aga/; VbAb: congruent audiovisual /aba/; VgAb incongruent McGurk stimuli with visual /aga/ and acoustic /aba/; VgAg: congruent /aga/). Congruent and acoustic only stimuli are categorized with a high degree of accuracy and McGurk trials are consistently fused, that is, perceived as /ada/. We reproduce the experimental paradigm consisting of six types of stimuli presented in pseudo-random order; three non-ambiguous acoustic only tokens:/aba/, /ada/ and /aga/, and three audiovisual stimuli: congruent /aba/, incongruent visual /aga/with acoustic /aba/(McGurk stimuli), and congruent /aga/.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44516-fig2-v2.tif"/></fig></sec><sec id="s2-2"><title>Simulation of the recalibration process</title><p>After the model has made the perceptual decision, it is recalibrated. After each trial, we changed the generative model’s location parameters associated with the perceived category (θ<sub>k,V</sub> θ<sub>k,A</sub>), which represent the categories through their expected sensory feature values in each modality. This was done for both the acoustic and visual parameters after an audio-visual trial, and for the acoustic parameter after an acoustic trial. We assumed that this happens for every trial as part of a monitoring process that assesses how well the internal model matches sensory inputs. The changes are thus driven by residual sensory prediction error, the difference between the expected and observed values for the modulation amplitudes in each modality.</p><p>When listeners consistently reported the fused percept ’ada’ when confronted with a video of /aga/ and the sound of /aba/, the presence of the visual stream modified the acoustic percept from ’aba’ to ‘ada’. Given that the acoustic input did not correspond to the one that was most expected from the perceived token, there was a systematic residual sensory prediction error. Since this residual prediction error was used as a signal to drive the model’s adaptation, the /ada/ representation moved towards the McGurk stimulus parameters after a fused percept (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Internal model adaptation.</title><p>(<bold>A</bold>) Speech tokens are represented in a multimodal feature space here represented by two main dimensions. Each ellipse stands for the internal representation of each congruent category (‘aba’ in blue, ‘ada’ in red, ‘aga’ in yellow). The red squares show the location of the audiovisual stimuli in the 2D feature space. They represent congruent /aba/(top left), congruent /aga/(bottom right), and McGurk stimuli (bottom left). When McGurk stimuli are repeatedly perceived as /ada/, the /ada/ representation (in solid red) is modified in such a way that it ‘moves’ (dashed red) towards the presented McGurk stimulus (visual /aga/ with acoustic /aba/) and therefore should affect the processing of subsequent sensory input. The right panel illustrates how the acoustic representation for /ada/ has shifted towards that of /aba/. (<bold>B</bold>) The effects of the shift in the internal representation on the categorization of the purely acoustic /aba/(Ab), /ada/(Ad) and /aga/(Ag) sounds. Each panel shows the percentage of /aba/, /ada/ and /aga/ percepts for the ‘control’ representations (solid lines) and the representations with the recalibrated /ada/(dashed line). As in <xref ref-type="bibr" rid="bib37">Lüttke et al. (2016a)</xref>, the biggest effect is observed when categorizing the /aba/ sounds.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44516-fig3-v2.tif"/></fig><p>In the model, the residual prediction error occurs in the transformation from token identity to predicted modulation of the acoustic feature (C<sub>A</sub>). Sensory evidence drives estimated C<sub>A</sub> towards the experimentally presented value: /aba/ for McGurk stimuli. Thus, when the percept is /ada/, there is a mismatch between the top-down prediction as determined by the top-down component p(C<sub>A</sub>|k) that drives C<sub>A</sub> towards θ<sub>/ada/,A</sub>, and the actual value determined by the bottom-up component. This is evident in the following expression<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>with the first term reflecting the prior expectation for category ‘k’ and the second reflecting the sensory evidence.</p><p>The expression can be rewritten to make the prediction error explicit.<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This highlights how the listener’s estimate of the modulation comes from combining the prediction from the category (first term) and the weighted residual prediction error (in the second term).</p><p>To minimize residual prediction error we consider that the participant recalibrates its generative model, which changes θ<sub>k,V</sub> and θ<sub>k,A</sub> towards s<sub>V</sub> and s<sub>A</sub>. If the stimuli are chosen with parameters ‘adapted’ to the listener, as we do, s<sub>V</sub> ~ θ<sub>stim,V</sub> and s<sub>A</sub> ~ θ<sub>stim,A</sub>.</p><p>To drive recalibration we considered three different update rules; one derived from the Bayesian model used by <xref ref-type="bibr" rid="bib28">Kleinschmidt and Jaeger (2015)</xref>, which assumes a stable environment and two empirically motivated rules. As a control we also simulated the experiment with no parameter updates. For each recalibration model and parameter value set, we run the experiment 100 times, therefore simulating 100 different listeners that share the same perceptual model. Each of the 100 simulated listeners was presented with a different random presentation of the six stimulus types; each presented 69 times (as in the original paper). This gives a total of 414 trials per listener.</p><p>To compare with the results from Lüttke et al., who considered 27 participants, we randomly sampled groups of 27 from the 100 simulated listeners to obtain an empirical sampling distribution for the quantities of interest. We focus on the ‘McGurk contrast’: proportion of /aba/ sounds reported as ‘ada’ 29% when preceded by a fused McGurk trial versus 16% when preceded by other stimuli. We will also consider the ‘/ada/ contrast’: the difference in the proportion of purely acoustic /aba/ categorized as ‘ada’ when the preceding trial was a correctly categorized /ada/ sound (17%) versus other stimuli (15%) (percentages correspond to the values reported in <xref ref-type="bibr" rid="bib37">Lüttke et al., 2016a</xref>).</p><p>Although we did not perform an exhaustive parameter search, we did repeat the simulations for 20 different values of perceptual model parameters and we also varied recalibration parameter values for each update rule (details in Methods).</p><p>Below, for each recalibration model, we report results based on the parameter values that led to the best fit to the ‘McGurk contrast’.</p></sec><sec id="s2-3"><title>Model without recalibration</title><p>As a control we simulated the experiment with no recalibration. For the simulation with the closest fit to the McGurk contrast, the percentage of acoustic /aba/ categorized as ‘ada’ was 14% after fused McGurk stimuli and 15% after the control stimuli (Wilcoxon signed-rank test p=0.6, <xref ref-type="fig" rid="fig4">Figure 4A</xref>). The 95% CI for the difference between trials preceded or not by a fused McGurk stimulus was [−6.48, 5.25]%. For the /ada/ contrast, the percentage of acoustic /aba/ categorized as ‘ada’ was 14% after correctly identified /ada/ sounds and 19% after the control stimuli (Wilcoxon signed-rank test p=0.5). The 95% CI for the difference between the two conditions was [−8.24, 4.21]%.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Cumulative and transient update rules.</title><p>‘ada’ percepts in response to acoustic /aba/ stimulation. We show two contrasts. The McGurk contrast compares the percentage of ‘ada’ responses when acoustic /aba/ is preceded by control stimuli (acoustic /aba/ and /aga/, congruent /aba/ and /aga/) versus by fused McGurk trials. The /ada/ contrast refers to acoustic /aba/ preceded by control stimuli (acoustic /aba/ and /aga/) versus acoustic trials correctly perceived as ‘ada’. The four panels show the simulation results for the parameters that led to the closest fit to the McGurk contrast reported by <xref ref-type="bibr" rid="bib37">Lüttke et al. (2016a)</xref> for four different update rules as indicated in the insets: (<bold>A</bold>) control, no update, (<bold>B</bold>) the standard Bayesian updates, (<bold>C</bold>) the constant delta rule, and (<bold>D</bold>) Decay, the update rule that assumes that recalibration occurs at two time scales. Both the standard Bayes (<bold>B</bold>) and the constant delta rule (<bold>C</bold>) lead to changes in internal representations that are reflected in the overall increase in ‘ada’ percepts (with respect to the control, no update model on panel <bold>A</bold>) however, it did not translate into significant effects specific to the next trial. The model assuming two time scales does reproduce the effect of a fused McGurk on the next trial (McGurk contrast). Only the McGurk contrast for the two time scale recalibration model (<bold>D</bold>, left) was significant (**p=0.0003). All other p values were greater than 0.05, except (*, p=0.03, <bold>C</bold> right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44516-fig4-v2.tif"/></fig></sec><sec id="s2-4"><title>Bayesian updating</title><p>We first considered the same updating principle as in <xref ref-type="bibr" rid="bib28">Kleinschmidt and Jaeger (2015)</xref> to model changes in speech sound categorization after exposure to adapting stimuli. After each trial the generative model updates the parameters by considering their probability given the sensory input and the categorization p(θ|k s<sub>V</sub> s<sub>A</sub>), leading to sequential Bayesian updating (<xref ref-type="disp-formula" rid="equ18">Equation 6</xref> in the methods section). The closest fit to the McGurk contrast was obtained for simulations with κ<sub>k,f,0</sub> = 1 and ν<sub>k,f,0</sub> = 1. The percentage of /ada/ responses to acoustic only /aba/ stimuli was 23% after fused McGurk stimuli and 22% after control stimuli. This difference was not significant (Wilcoxon signed-rank test p=0.7, <xref ref-type="fig" rid="fig4">Figure 4B</xref>). The 95% confidence interval for the difference in medians between the two conditions was [−6.14, 7.97]%, thus failing to reproduce the effect of interest. This might be due to the fact that Bayesian update rules have the form of a delta rule with a decreasing learning rate. As a consequence, the magnitude of changes in the categories diminishes as the experiment progresses and all stimuli end up being related to the same internal model. The updates did lead to observable effects; there was an overall increase of /ada/ responses to acoustic /aba/(24% vs. the 14% of the control experiment without parameter updates). The resulting changes in model parameters are expected to induce an after-effect, that is, the point of subjective equivalence in an /aba /- /ada/ acoustic continuum should be shifted in the direction of /aba/.</p></sec><sec id="s2-5"><title>Constant delta rule</title><p>The Bayesian update rule used above assumes that the parameters are constant in time and that therefore all samples have equal value, whether they are old or recent. This is equivalent to a delta rule with a learning rate tending to zero. We therefore considered a rule with a constant learning rate, which allows for updates of similar magnitude over the whole experiment. The model’s expected modulation for the perceived category was recalibrated according to:<disp-formula id="equ3"><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0.2</mml:mn><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">k</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>f</italic> indexes the feature (<italic>f = A</italic>, acoustic feature; <italic>f = V</italic>, visual feature). As <xref ref-type="fig" rid="fig4">Figure 4C</xref> shows, the percentage of acoustic /aba/ categorized as ‘ada’ was not significantly higher when the preceding trial was a fused McGurk trial compared with any other stimulus (27% vs. 24%, Wilcoxon signed-rank test, p = 0.08 <xref ref-type="fig" rid="fig4">Figure 4C</xref>; median difference at 95% CI [−0.14 12]%). Note that in this case too, the proportion of /ada/ responses for acoustic /aba/ inputs is increased compared with simulations run without any adaptation (<xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><p>Although the learning rate is constant, which means that recalibration magnitude does not necessarily decrease during the experiment, recalibration does not decay across trials. As a result, for trials between consecutive /ada/ percepts, stimuli experience a similar /ada/ category and the simulations do not lead to a significant difference in the classification of acoustic /aba/ whether preceded or not by a fused McGurk trial.</p></sec><sec id="s2-6"><title>Hierarchical updates with intrinsic decay</title><p>We also tested an alternative update rule that was expected to better reflect how changes occur in the environment. We considered that they might occur hierarchically, with just two levels in a first approximation, corresponding to keeping ‘running averages’ over different time scales, enabling sensitivity to fast changes without erasing longer-lasting trends.</p><p>We considered two sets of hierarchically related variables associated with a single category: θ<sub>k,f</sub> (fast) and μ<sub>k,f</sub> (slow). The faster decaying one, θ<sub>k,f</sub>, is driven by both sensory prediction error and the more slowly changing variable, μ<sub>k,f</sub> (more details can be found in Materials and Methods). This slowly changing and decaying variable, μ<sub>k,f</sub>, keeps a representation based on a longer term ‘average’ over sensory evidence. In the limit, μ<sub>k,f</sub> is constant; and this is what we consider here for illustrative purposes. Thus the update rules include an instant change in the fast variable due to the sensory prediction error in the perceived category plus a decay term toward the slower variable for every category. The instant change corresponds to the traditional update after an observation; the decay reflects the transient character of this update.</p><p>The results in <xref ref-type="fig" rid="fig4">Figure 4D</xref> were run with the following parameters:<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0.4</mml:mn><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">k</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0.14</mml:mn><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ6"><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula>where subscript f indexes the feature (f = A for acoustic, f = V for visual). All categories decay toward the corresponding long-term stable values (μ<sub>k,A</sub>, μ<sub>k,V</sub>) in the inter-trial interval. By comparing the decay contribution Δθ<sub>k,f</sub> = 0.14 (μ<sub>k,f</sub> - θ<sub>k,f</sub>) with the update expression for a quantity with decay time constant τ in an interval Δt (here Δt = 5 s, the interval between consecutive trials) Δθ<sub>k,f</sub> = Δt/τ (μ<sub>k,f</sub> - θ<sub>k,f</sub>), we can derive a rough estimate for the decay time constant τ ~5/0.14 s ~ 35 s.</p><p>The percentage of acoustic /aba/categorized as ‘ada’ after control trials was 18% vs. 29% after fused McGurk (Wilcoxon ranked-signed test, p=0.0004), the median difference being 95% CI: [5.8, 18.0]%. Therefore two effects can be observed; the overall increase in acoustic /aba/ categorized as /ada/ and the rapid recalibration effect reflected in the specific increase observed when acoustic /aba/ was preceded by a fused McGurk trial.</p></sec><sec id="s2-7"><title>Update rule comparison</title><p>We have modelled recalibration as the continuous updating of the model parameters that represent each of the speech categories used to guide perceptual decisions, in particular the expected values of sensory features associated with each category θ<sub>k,V</sub>, θ<sub>k,A</sub> (<italic>k</italic> indexing the category). With this approach, the ideal adapter Bayesian account turned out to be incompatible with the experimental findings, due to the erroneous underlying assumption of a stable environment. Because the model assumes that all the sensory observations are derived from exactly the same non-changing distributions, past observations do not lose validity with time. As a result, the location estimate corresponds to the running average of the feature values in the stimuli that have been associated with each category in the course of the experiment. As the occurrence of a perceived category increases, the size of recalibration decreases, until categorization differences across successive trials are no longer observable (<xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Category parameters across an experiment.</title><p>θ<sub>/aba/,A</sub>, θ<sub>/ada/,A</sub> and θ<sub>/aga/,A</sub> after each of the 414 trials (69 repetitions of 6 different stimuli) in a sample simulated experimental run. (<bold>A</bold>) For the standard Bayesian model, category parameter updates become smaller as the experiment progresses. (<bold>B</bold>) For the constant delta rule updates of similar size occur throughout the experiment but are constant across trials. (<bold>C</bold>) Updates for the hierarchical delta rule with decay don’t drift but decay to a long-term less volatile component.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44516-fig5-v2.tif"/></fig><p>The ‘delta rule’ and the ‘hierarchical update with decay’ both involve a constant learning rate implying that the parameter changes following each perceptual decision do not decrease as the experiment progresses (<xref ref-type="fig" rid="fig5">Figure 5B–C</xref>). Although both models were able to qualitatively reproduce the main result, namely that the rate of acoustic /aba/ categorized as ‘ada’ was higher immediately after a fused McGurk trial, the delta rule without decay did not provide a good fit. The ‘hierarchical update with decay’ provided the best explanation for the experimental results. Specifically, its advantage over the ‘delta rule’ is that the update decays across trials after a perceptual decision towards a less volatile representation of the category, providing an effective empirical prior (<xref ref-type="fig" rid="fig5">Figure 5C</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Speech sound categories are constantly revised as a function of the most recently presented stimuli (<xref ref-type="bibr" rid="bib58">Samuel and Kraljic, 2009</xref>; <xref ref-type="bibr" rid="bib28">Kleinschmidt and Jaeger, 2015</xref>; <xref ref-type="bibr" rid="bib22">Heald et al., 2017</xref>). The proposed model provides a possible account for fast and transient changes in speech sound categories when confronted to a volatile sensory environment, involving the constant recalibration of internal models of speech.</p><p>The model was motivated by experimental results showing that /aba/ sounds were more frequently mis-categorized as ‘ada’ when they were preceded by a fused McGurk (<xref ref-type="bibr" rid="bib37">Lüttke et al., 2016a</xref>). Since the reported effect was distinct from other well-documented across-trial dependency effects, such as perceptual priming or selective adaptation (<xref ref-type="bibr" rid="bib22">Heald et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Gabay and Holt, 2018</xref>), we sought to model it considering only changes in perceived speech categories without external feedback. Like previous approaches, ours builds on the idea that the brain achieves perception by inverting a generative model (e.g., <xref ref-type="bibr" rid="bib55">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib29">Knill and Pouget, 2004</xref>; <xref ref-type="bibr" rid="bib15">Friston, 2005</xref>) and by continuously monitoring its performance to adapt it to changing stimulus landscapes. One way the brain can alter its internal models without external feedback is by using the perceptual outcome as a teaching signal or <italic>ground truth</italic> (<xref ref-type="bibr" rid="bib40">Luu and Stocker, 2018</xref>) to induce model recalibration, such that the outcome better explains the sensory features that produced it.</p><p>Such auto-recalibration of speech sound categories has been described within an ‘ideal adapter’ framework (<xref ref-type="bibr" rid="bib28">Kleinschmidt and Jaeger, 2015</xref>). In this framework, perceptual categories are subject to trial-by-trial changes well described by a Bayesian approach that implicitly assumes sound categories to be stable within an experimental session, and hence uses update/learning rules giving equal weight to recent and past evidence. While this makes sense in a stable environment where information remains equally relevant independent of its recency and the adaptation rule can describe the adaptation dynamics in sublexical speech categories in experiments with blocks of repeated stimuli (<xref ref-type="bibr" rid="bib7">Bertelson et al., 2003</xref>; <xref ref-type="bibr" rid="bib67">Vroomen et al., 2007</xref>), it failed here to account for the specific transient effect in <xref ref-type="bibr" rid="bib38">Lüttke et al. (2016b)</xref>.</p><p>By comparing neurocomputational models with and without decay in their update rules, we show that recalibration occurs at least at two levels, characterized by two different decaying timescales: a fast decaying/short-term process (estimated to operate on the order of tens of seconds) that weighs recent versus immediate past sensory evidence, and a slow decaying/long-term process that keeps a longer trace of past sensory evidence to stabilize invariant representations. Such a dual process is critical because speech sounds have both variable (e.g. speakers) and stable components (e.g. phonemic categories).</p><p>The model was able to successfully reproduce transient speech category recalibration using empirically motivated update equations that include two hierarchically related parameters with different learning and decaying timescales. The fast changing variable is driven by current sensory prediction errors, and the more slowly changing one also driven by prediction errors but with a slower learning rate and hence acting as a longer-span buffer (<xref ref-type="fig" rid="fig5">Figure 5</xref>; <xref ref-type="disp-formula" rid="equ20">Equation (8)</xref>). As a result, the parameter controlling the model expectations decays toward the longer-term parameter after a transient change driven by the current sensory prediction error. After a fused McGurk trial, the acoustic representation of /ada/ is shifted towards that of acoustic/aba/(<xref ref-type="fig" rid="fig3">Figure 3A</xref>), which increases the chances of an acoustic /aba/ to be mis-categorized as ‘ada’ (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Since the recalibration decays over time, the mis-categorization is most prominent for the acoustic /aba/ immediately following the McGurk fusion. On the other hand, because the decay drifts towards a more slowly changing ‘version’ of the /ada/ category, the model can also accommodate a more persistent accumulated adaptation as experimentally observed (<xref ref-type="bibr" rid="bib7">Bertelson et al., 2003</xref>; <xref ref-type="bibr" rid="bib67">Vroomen et al., 2007</xref>).</p><p>The model, which encompasses both the perceptual decision and the recalibration processes, is also consistent with activity observed in auditory cortex (<xref ref-type="bibr" rid="bib37">Lüttke et al., 2016a</xref>). When the acoustic-only /aba/ was presented after a fused McGurk, fMRI activity in auditory cortex is more frequently classified as ‘ada’ by a learning algorithm trained to distinguish correctly identified acoustic /aba/, /ada/ and /aga/. The intermediate level in our perceptual model corresponds to the representation of acoustic features in auditory cortex and visual features in extrastriate visual cortex. In our model the amplitude of the 2<sup>nd</sup> formant transition (the acoustic feature that is most important for the aba/ada contrast) results from a combination of bottom-up sensory information and top-down predictions (<xref ref-type="disp-formula" rid="equ1 equ2">Equations 1, 2</xref>). Since there are more /aba/ sounds perceived as ‘ada’ after a fused McGurk, the top-down predictions during the perceptual process are dominated by the /ada/ category, that is, our generative model predicts that activity in auditory cortex should be closer to /ada/ for acoustic /aba/ following fused McGurk trials, as shown by the fMRI data.</p><p>In summary, during audiovisual speech integration, and in McGurk stimuli in particular, the brain tends to find the most parsimonious account of the input, merging the acoustic and visual sensory streams even at the expense of residual prediction errors at brain areas that encode unisensory stimulus features, represented in the model by the acoustic 2<sup>nd</sup> formant and lip amplitude modulation. Different participants may value differently this parsimony/accuracy trade-off. Those who consistently fuse the two streams presumably recalibrate their category representations (e.g. /ada/ after fused McGurk) thereby reducing residual prediction errors at the feature level. That is, we suggest that recalibration does not happen primarily at the areas encoding the stimulus features, but at higher order areas that encode sub-lexical speech categories, in a process that updates categories at least with two time scales.</p><sec id="s3-1"><title>Non-Bayesian models of speech category learning and recalibration</title><p>As our interest lies at the computational level, we have not tested other, non-Bayesian models of speech category learning and recalibration (reviewed in <xref ref-type="bibr" rid="bib22">Heald et al., 2017</xref>). Some existing models involve processes that are similar to ours; for example, non-Bayesian abstract models of new speech category learning use Gaussian distributions with parameters that are updated with delta rules similar to that of <xref ref-type="disp-formula" rid="equ19">Equation 7</xref> (<xref ref-type="bibr" rid="bib64">Vallabha et al., 2007a</xref>; <xref ref-type="bibr" rid="bib45">McMurray et al., 2009</xref>). On the other hand, connectionist models of speech category learning posit a first layer of units with a topographic representation of the sound feature space and a second layer representing individual speech categories. Learning or recalibration is modelled by changing the connection weights between the two layers. In this way, <xref ref-type="bibr" rid="bib47">Mirman et al. (2006)</xref>, modelled the recalibration of established prelexical categories that arises when an ambiguous sound is disambiguated by the lexical context as in the Ganong effect – a sound between /g/ and /k/ that tends to be classified as /g/ when preceding ‘ift’ or as /k/ when preceding ‘iss’ (<xref ref-type="bibr" rid="bib19">Ganong, 1980</xref>). This effect shares some similarities with the McGurk effect, although the latter is stronger as it changes the perception of a non-ambiguous sound. The benefit of having two timescales is also illustrated by a connectionist model of the acquisition of non-native speech sound categories in the presence of well-established native ones (<xref ref-type="bibr" rid="bib65">Vallabha and McClelland, 2007b</xref>). Interference between new and existing categories was avoided by positing a fast learning pathway applied to the novel categories, and a slower learning pathway to the native ones. Finally, connectionist models can also reproduce short-term effects such as perceptual bias and habituation (<xref ref-type="bibr" rid="bib34">Lancia and Winter, 2013</xref>). These examples suggest that a connectionist model could provide a physiologically plausible instantiation of our abstract Bayesian model as long as one incorporates two pathways with two different timescales or a single pathway that uses metastable synapses (<xref ref-type="bibr" rid="bib6">Benna and Fusi, 2016</xref>).</p></sec><sec id="s3-2"><title>Advantages of a dual time-scale representation</title><p>Parallel learning systems working at different temporal scales have previously been proposed in relation to speech; one able to produce fast mappings and heavily relying on working memory, while the other relies on procedural learning structures that eventually results in effortless, implicit, associations (<xref ref-type="bibr" rid="bib48">Myers and Mesite, 2014</xref>; <xref ref-type="bibr" rid="bib69">Zeithamova et al., 2008</xref>; <xref ref-type="bibr" rid="bib41">Maddox and Chandrasekaran, 2014</xref>). Our proposal can theoretically be motivated on similar grounds. We argue that the brain implements at least two representations of natural categories; one more flexible than the other. The more flexible one might be used to achieve the agent’s current goal, while the more stable and less precise representation keeps general knowledge about sound categories. We propose the term ‘working’ representation for the more flexible sound category representation, to distinguish it from its more stable ‘episodic’ form.</p><p>Behaviourally, this can be advantageous when specific instances of a unique category, for example from a single speaker, have less associated uncertainty than the overall prior distribution across all possible instances across speakers. The ‘working representation’ corresponds to an ‘intermediate’ representation that has lower uncertainty and therefore makes the sensory integration process more precise, leading to more confident perceptual decisions at the single trial level. This strategy allows the agent to use a precise ‘working’ category that can quickly change from trial to trial.</p><p>In this view, the agent needs to infer the distribution (mean and covariance) that defines the working representation, and combine sensory evidence with the prior, that is, with the corresponding long-term ‘episodic’ representation. Based on these Bayesian principles we wrote the hierarchical recalibration rule (<xref ref-type="disp-formula" rid="equ20">Equation 8</xref>), which appears whenever there are three quantities informing the current estimate of a variable (under Gaussian conditions). In our model, the current expected value for the working representation is informed by the value derived from the observation in the previous and current trials, and from the episodic representation as schematized in the right panel of <xref ref-type="fig" rid="fig6">Figure 6</xref>. Bayesian inference assuming known volatilities for the two levels in <xref ref-type="fig" rid="fig6">Figure 6</xref>, and under the mean-field approximation, can be calculated analytically resulting in update equations that take the form written at the bottom of the diagram. The coefficients, which determine what is referred to as ‘learning rates’ in the reinforcement learning literature, are functions of the parameters of the model related to the different sources of uncertainty: volatilities at both hierarchical levels, sensory noise and the width of the episodic representation. Our proposed update equation therefore assumes that agents have already estimated the volatilities at the two levels (as in <xref ref-type="bibr" rid="bib5">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Nassar et al., 2010</xref>; <xref ref-type="bibr" rid="bib44">Mathys et al., 2014</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Statistical models underlying the two classes of update rules used in the paper.</title><p>Here Θ stands for the model parameters that determine the speech categories used by the perceptual model (See <xref ref-type="fig" rid="fig3">Figure 3</xref>) and ‘k’ for the trial index. On the standard Bayesian approach (left), model parameters are considered constant in time leading to update rules that give the same weight to all prediction errors, which in turn leads to a ‘learning rate’ α<sub>k</sub> that becomes smaller with the number of trials. On the right, we show a hierarchical Markov model implementation that would lead to the kind of update rules that we introduced empirically to accommodate the rapid recalibration effect. This alternative view implicitly assumes that model parameters can change in time and therefore lead to update rules with learning rates (α, β and λ) that under certain assumptions, settle to non-zero constant values.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44516-fig6-v2.tif"/></fig><p>Finally, from the optimal agent’s perspective, the internal model used for a given trial is the predictive working representation built from 1) updated representations after the last observation and 2) their expected change in the intervening time. The latter component denotes the uncertainty associated with the representations, for example more volatile representations becoming more uncertain more quickly. This last point is important as it means that, across trials, increased uncertainty associated with the previous estimate of the working representation implies more reliance on the long-term representation. In other words, across trials, the expected sound feature modulation encoded by the working representation ‘decays’ back towards that of the long-term representation. This reflects a form of ‘optimal forgetting’, that is, the expected loss of relevance of a past observation for the current trial.</p></sec><sec id="s3-3"><title>Functional neuroanatomy of transient category shifts</title><p>Whether the two time scales that were needed to explain simultaneous tracking of long and short-term category representations are hierarchically organized or implemented in parallel, and what brain regions or mechanisms might be implicated is an open question.</p><p>There are at least three scenarios that could support recalibration at different time scales. First, a hierarchy of time scales might exist at the single synapse level. Hierarchically related variables with increasing time scales (dynamics described by equations as in <xref ref-type="fig" rid="fig6">Figure 6B</xref>) have been used in a modelling study to increase the capacity of memory systems and improve the stability of synaptic modifications (<xref ref-type="bibr" rid="bib6">Benna and Fusi, 2016</xref>), and a model of synapses with a cascade of metastable states with increasing stability was able to learn more flexibly under uncertainty (<xref ref-type="bibr" rid="bib24">Iigaya, 2016</xref>). In the latter model, the intrinsic decay of synaptic modifications was faster for the more labile memory states, i.e. those that are more sensitive to new evidence. In contrast, the deeper, more stable memory states, showed slower decay, which overall nicely concurs with our proposal. It is thereforepossible that the representations are encoded at synapses with the transient recalibration corresponding to synaptic modification at the more labile states and the long-term component residing in the less labile states.</p><p>A second option involves prefrontal cortex working in tandem with other brain regions. In perceptual classification tasks under volatile conditions, prefrontal cortex can flexibly combine alternative strategies, such as optimal Bayesian-like learning in stable environments and a working memory model in volatile environments (<xref ref-type="bibr" rid="bib62">Summerfield et al., 2011</xref>). In our setting, to guide the speech classification process, it could conceivably combine a ‘working’ short time scale representation of the speech categories with a long-term ‘episodic’ representation, which might reside in different brain networks. Several fronto-parietal regions have indeed been implicated in controlling the effect of sensory and choice history on perceptual decisions: ‘Sensory evidence, choice and outcome’ could be decoded from ventrolateral prefrontal cortex and predicted choice biases (<xref ref-type="bibr" rid="bib63">Tsunada et al., 2019</xref>). Neuronal responses in fronto-parietal circuits could provide a basis for flexible timescales (<xref ref-type="bibr" rid="bib60">Scott et al., 2017</xref>), as dissociated effects of working memory and past sensory history have been found to involve the prefrontal cortex and posterior parietal cortices respectively (<xref ref-type="bibr" rid="bib1">Akrami et al., 2018</xref>). The observed sensitivity to sensory choice history and sensory evidence is consistent with our model, which uses internal category representations to interpret sensory evidence, with category representations being recalibrated based on choice (i.e., the perceived category).</p><p>Finally, the hierarchical nature of perception and action (<xref ref-type="bibr" rid="bib27">Kiebel et al., 2008</xref>; <xref ref-type="bibr" rid="bib16">Friston, 2008</xref>) might be paralleled in the brain, by hierarchical processing in prefrontal cortex (<xref ref-type="bibr" rid="bib3">Badre, 2008</xref>; <xref ref-type="bibr" rid="bib31">Koechlin and Jubault, 2006</xref>; <xref ref-type="bibr" rid="bib61">Summerfield et al., 2006</xref>) and sensory areas (<xref ref-type="bibr" rid="bib14">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="bib9">Chevillet et al., 2011</xref>). It is hence conceivable that the relation between the two timescales is hierarchical with higher-level representations becoming increasingly abstract and time insensitive. This could happen, for example, if the brain used representations at a speaker level that are drawn from more general representations of speech categories at the population level. Recalibration might work at every level of the temporal hierarchy, with higher levels integrating update information within increasingly longer time windows (longer timescales), making them less and less sensitive to new observations.</p></sec><sec id="s3-4"><title>Revised ‘ideal adapter’</title><p>While the ‘ideal adapter’ account focused on cumulative recalibration (<xref ref-type="bibr" rid="bib28">Kleinschmidt and Jaeger, 2015</xref>), our results suggest that shorter-lived effects are also behaviorally relevant. The ideal adapter could be formalized as a simple incremental optimal Bayesian inference in a non-volatile environment (<xref ref-type="fig" rid="fig6">Figure 6</xref>, left panel), whereas our update rule could be cast in a normative framework that explicitly accounts for environmental volatility. A hierarchical model with constant volatility at two levels (<xref ref-type="fig" rid="fig6">Figure 6</xref>, right panel) could lead to hierarchical update equations (<xref ref-type="bibr" rid="bib68">Wilson et al., 2013</xref>) that can be approximated by constant learning rates (with higher learning rates - faster ‘forgetting’- being related to stronger volatility). The right panel of <xref ref-type="fig" rid="fig6">Figure 6</xref> assumes that the higher level (μ) has lower volatility than the intermediate level (θ), hence combining volatility with hierarchy. This combination departs from models used to explain decision making in changing environments (<xref ref-type="bibr" rid="bib5">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib62">Summerfield et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Mathys et al., 2014</xref>), which are not hierarchical, and focus on the nontrivial task of inferring the environment volatility. These studies show that human participants adapt their learning rates to the changing volatility, which could be modelled without keeping representations across several time scales. In these tasks, participants need to keep track of short-lived changes in arbitrary cue-reward associations or in arbitrarily defined sensory categories (<xref ref-type="bibr" rid="bib62">Summerfield et al., 2011</xref>), whereas we model overlearned and behaviourally relevant categories, which also requires to maintain long-term estimates as empirical priors.</p></sec><sec id="s3-5"><title>Relevance to speech and language pathologies</title><p>Our modelling results are relevant to continuous speech processing, in particular to account for auditory processing anomalies in dyslexia. Evidence from a two-tone frequency discrimination task suggests that participants’ choices are driven not only by the tones presented at a given trial, but also by the recent history of tone frequencies in the experiment, with recent tones having more weight than earlier ones (<xref ref-type="bibr" rid="bib26">Jaffe-Dax et al., 2017</xref>). It turns out that, when compared with controls, subjects with dyslexia show a decreased reliance on temporally distant tones, suggesting a shorter time constant (<xref ref-type="bibr" rid="bib26">Jaffe-Dax et al., 2017</xref>). Translating this result to the current model, we could hypothesize that in dyslexia the long-term component (μ in <xref ref-type="fig" rid="fig6">Figure 6B</xref>) has either a shorter time span, or is coupled to the lower representation with a lower weight. In both cases, we would expect a deficit in building long-term stable speech category representations since they would be overly driven by the current context. ASD individuals on the other hand, are optimally biased by long-term tones, but do not show the bias by short-term tones of neurotypical participants (<xref ref-type="bibr" rid="bib36">Lieder et al., 2019</xref>), which suggests a faster decay or an absence of the short-term component in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. This would predict a failure of ASD individuals to show the specific effect after McGurk trials in the experiment simulated here.</p></sec><sec id="s3-6"><title>Conclusion</title><p>We present a revised ‘ideal adapter’ model for speech sound recalibration that has both transient and cumulative components organized hierarchically. This new model provides evidence for a hierarchy of processes in the recalibration of speech categories, and highlights that after experiencing the McGurk effect, it is not the acoustic features related to the sensory input that are modified, but higher-level syllabic representations. The model implies that the activity changes in sensory cortices are not locally generated but reflect the interaction of bottom-up peripheral sensory inputs and top-down expectations from regions where categorical perception takes place. Considering natural speech processing as the inversion of a continuously monitored and recalibrated internal model can unveil the potential operations and strategies that listeners use when they are confronted with the acoustic volatility associated with speech categories, which by their nature have both rapidly changing (e.g. speaker specific) and slowly changing (e.g. speaker general) components. Such a model can be implemented by a hierarchy of empirical priors that are subject to changes at different time scales. Although developed in the context of speech processing, our proposal may also apply to other cognitive domains requiring perhaps more nested timescales, such as action planning (<xref ref-type="bibr" rid="bib3">Badre, 2008</xref>; <xref ref-type="bibr" rid="bib32">Koechlin and Summerfield, 2007</xref>; <xref ref-type="bibr" rid="bib30">Koechlin et al., 2003</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Generative model</title><p>The goal of inference is to establish which is the speech token that gave rise to the incoming sensory input. We restrict ourselves to three possible tokens: /aba/, /ada/ and /aga/ (as in <xref ref-type="bibr" rid="bib37">Lüttke et al., 2016a</xref>). Although several acoustic and visual features can distinguish between them, we choose to model the 2<sup>nd</sup> formant transition, which is minimal for /aba/ but increases for /ada/ and /aga/, and the degree of lip closure, which is maximal for /aba/ and less prominent for /ada/ and /aga/(lip closure /aba/&gt;/ada/&gt;/aga/). This choice is based on the fact that what distinguishes between the three speech sounds is the place of articulation. Acoustically the 2<sup>nd</sup> formant transition is an important cue for place of articulation, particularly within the ‘a’ vowel context (<xref ref-type="bibr" rid="bib35">Liberman et al., 1957</xref>); visually it is the degree of lip aperture at the time of the vocal cavity occlusion depending on its location (complete lip closure for the bilabial (/aba/), and decreasing lip closure for the alveolar (/ada/) and velar (/aga/) (<xref ref-type="bibr" rid="bib8">Campbell, 2008</xref>; <xref ref-type="bibr" rid="bib66">Varnet et al., 2013</xref>).</p><p>The generative model has three levels; the higher level encodes the speech token, the speech token in turn determines the expected values for the audiovisual cues, as represented in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. The model includes the three possible tokens, each determining the expected distribution of its associated audiovisual features. We also introduce sensory noise to account for sensory variability. The parameters, location and spread of features associated with each token, as well as the parameter associated with the level of sensory noise, define an individual listener’s internal model. That is, the listener models both the variability due to different articulations of the same speech category as well as the variability due to noise in the sensory system.</p><p>We use ‘k’ as the speech token index k = {/aba/,/ada/,/aga/}, ‘f’ as feature index f = {V,A}, where ‘V’ stands for the visual feature (lip aperture) and ‘A’ for the acoustic feature (2<sup>nd</sup> formant transition). The idea is that the amplitudes of the lip aperture and 2<sup>nd</sup> formant modulations vary according to the identity of the speech token (‘k’). ‘C<sub>V</sub>’ or ‘C<sub>A</sub>’ denote hidden states associated with these amplitudes. Finally, ‘s<sub>V</sub>’ or ‘s<sub>A</sub>’ stand for the actual features in the audiovisual sensory input. The internal generative model considers ‘s<sub>V</sub>’ and ‘s<sub>A</sub>’ the versions of ‘C<sub>V</sub>’ or ‘C<sub>A</sub>’ corrupted by sensory noise (σ<sub>V</sub>, σ<sub>A</sub>).</p><p>There are two sources of variability, one related to sensory noise (σ<sub>V</sub>, σ<sub>A</sub>) and the variability of modulation amplitudes across different articulations of the same speech token (σ<sub>k,V</sub>, σ<sub>k,A</sub>), k = {/aba/,/ada/,/aga/}.</p><p>The hierarchical generative model is defined by the following relations:<disp-formula id="equ7"><label>(3)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>∝</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ8"><label>(4)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>∝</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>While the above defines the generative (top-down model) p(s<sub>f</sub>|C<sub>f</sub>,k), our interest lies in its inversion p(k|s<sub>A</sub>,s<sub>V</sub>), where s<sub>A</sub>,s<sub>V</sub> represents the sensory input in a single trial.</p><p>From the inversion of the model defined by the relations above one obtains:<disp-formula id="equ9"><label>(5)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>∝</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>which results from marginalizing over ‘C<sub>V</sub>’ and ‘C<sub>A</sub>’-the intermediate stages that encode the visual and acoustic features, explicitly:<disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="thinmathspace"/><mml:mo>∝</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ11"><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mo>∫</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>∫</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mfrac><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>≡</mml:mo><mml:mfrac><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>≡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We assume that initial variance and prior probabilities are equal across categories (p(k)=1/3).</p><p>Alternatively, marginalization over ‘k’ gives the probabilities over the hidden variables ‘C<sub>V</sub>’ and ‘C<sub>A</sub>’, which we associate with encoding of stimulus features (lip aperture, 2<sup>nd</sup> formant transition) in visual and auditory cortex respectively.<disp-formula id="equ12"><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mspace width="thinmathspace"/><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>≡</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>≡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This shows explicitly how internal estimates of sensory features (lip aperture ‘C<sub>V</sub>’ and 2<sup>nd</sup> formant ‘C<sub>A</sub>’) are driven by bottom up sensory evidence (s<sub>V</sub>, s<sub>A</sub>) and top-down expectations related with each category ‘k’ contributing according to its internal expectations (θ<sub>k,V</sub> θ<sub>k,A</sub>). When there is strong evidence for a given category ‘k’, the sum can be approximated by a single Gaussian centered at a compromise between θ<sub>k,V</sub> and s<sub>V</sub> for the visual feature and between θ<sub>k,A</sub> and s<sub>A</sub> for the acoustic feature.</p><p>In principle the model could also be made to perform causal inference (<xref ref-type="bibr" rid="bib42">Magnotti and Beauchamp, 2017</xref>), that is, decide whether the two sensory streams belong to the same source and therefore should be integrated, or whether the two sensory streams do not belong to the same source, in which case the participant should ignore the visual stream. Since Lüttke et al. explicitly selected the participants that consistently reported /ada/ for the McGurk stimulus, these subjects were fusing the two streams. We hence assume that integration is happening at every audio-visual trial.</p><p>The model’s percept corresponds to the category that maximizes the posterior distribution: p(k|s<sub>A</sub>, s<sub>V</sub>).</p></sec><sec id="s4-2"><title>Recalibration model</title><p>The previous section presented how the model does inference in a single trial. We now turn to how the model updates the parameters that encode the internal representation of the three speech categories. This happens after every trial, thus simulating an internal model that continuously tries to minimize the difference between its predictions and the actual observations; we assume that in this process, in which the model tries to make itself more consistent with the input just received, it will only update the category corresponding to its choice. We will present three updating rules. The normative incremental Bayesian update model used by <xref ref-type="bibr" rid="bib28">Kleinschmidt and Jaeger (2015)</xref>, and the empirically motivated constant delta rule and hierarchical delta rule with intrinsic decay.</p></sec><sec id="s4-3"><title>Bayesian updating</title><p>The internal representation of the speech categories is determined by six location parameters (θ<sub>k,V</sub> θ<sub>k,A</sub>) and six width parameters (σ<sub>k,V</sub>, σ<sub>k,A</sub>). We follow <xref ref-type="bibr" rid="bib20">Gelman et al. (2003)</xref> and define the following prior distributions for the internal model parameters (θ<sub>k,V</sub>, σ<sub>k,V</sub>; θ<sub>k,A</sub>, σ<sub>k,A</sub>). For each of the six (θ, σ) pairs (2 sensory features × 3 categories) the prior is written as:<disp-formula id="equ13"><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>∝</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>As above, f refers to the sensory feature, either V or A, and k to the speech category, either /aba/, /ada/ or /aga/.</p><p>After a new trial with sensory input (s<sub>V</sub>, s<sub>A</sub>) the updated prior has the following parameters:<disp-formula id="equ14"><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">←</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">←</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ16"><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">←</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ17"><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">←</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>After each trial the inference process described in the previous section determines the percept from the posterior probability p(k|s<sub>V</sub> s<sub>A</sub>). Only the feature parameters of the representation corresponding to the percept are subsequently updated.</p><p>We use the values that maximize the posterior over the parameters given the input and the current estimated category ‘k’ to determine the point estimates that will define the updated model parameters for the next trial (<xref ref-type="disp-formula" rid="equ18">Equation 6</xref>). The updates for the location and spread parameters then take the form:<disp-formula id="equ18"><label>(6)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where ‘k’ is the perceived category, n(k) the number of times the category has been perceived, f = V,A designates the sensory feature and ν<sub>k,f,0</sub> and κ<sub>k,f,0</sub> are parameters from the prior distribution. The larger ν<sub>k,f,0</sub> and κ<sub>k,f,0</sub> are, the more k ‘perceptions’ it takes for the parameter values of category ‘k’ to plateau but also the smaller the updates after each trial.</p></sec><sec id="s4-4"><title>Constant delta rule</title><p>The above update equations implicitly assume that the environment is stable and therefore updated parameters keep information from all previous trials. This is the result of the generative model, which did not include a model for environmental parameter changes. Introducing expectations about environmental changes led us to consider rules with constant learning rates. We restrict ourselves here to updates for the six location parameters (θ<sub>k,V</sub> θ<sub>k,A</sub>) of <xref ref-type="disp-formula" rid="equ18">Equation 6</xref>.</p><p>We first considered a constant delta rule scaled by the evidence in favor of the selected category p(k|s<sub>V</sub>, s<sub>A</sub>),<disp-formula id="equ19"><label>(7)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>As in the Bayesian case, updates accumulate without decay between trials. The main difference is that θ<sub>k,f</sub> is driven more strongly by recent evidence than by past evidence, implicitly acknowledging the presence of volatility.</p></sec><sec id="s4-5"><title>Hierarchical delta rule with decay</title><p>Finally we consider updates that decay with time. We reasoned that the decay should be towards parameter estimates that are more stable, which we denote by μ<sub>k,f</sub>. We propose a hierarchical relation, with updates in μ<sub>k,f</sub> being driven by θ<sub>k,f</sub>, while updates in θ<sub>k,f</sub> are driven by sensory evidence. At each trial all categories (k’) decay toward their long-term estimates and only the perceived category (k) updates both μ<sub>k,f</sub> and θ<sub>k,f</sub>:<disp-formula id="equ20"><label>(8)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The first equation reflects the decay while the last two equations apply to the perceived category ‘k’. ‘D’, ‘R<sub>1</sub>’ and ‘R<sub>2</sub>’ are constant parameters. R<sub>2</sub> was set to zero since we do not expect the long-term component to change significantly within the experimental session.</p></sec><sec id="s4-6"><title>Model simulations</title><p>We simulate the experimental paradigm in <xref ref-type="bibr" rid="bib37">Lüttke et al. (2016a)</xref>, in which human participants were asked about what they heard when presented with auditory syllables or auditory syllables accompanied with a video of the corresponding speaker’s lip movements. There were six stimulus types: three acoustic only stimuli: /aba/, /ada/ and /aga/ and three audiovisual stimuli, congruent /aba/, congruent /ada/ and McGurk stimuli, that is, acoustic /aba/ accompanied by the video of a speaker articulating /aga/. Each stimulus type was presented 69 times to each participant. In the original experiment three different realizations of each of the six types were used. In our simulations we use a single realization per stimulus that is corrupted by sensory noise.</p><p>As described above, our model proposes that syllables are encoded in terms of the expected amplitudes and variances of audiovisual features. The expected amplitudes were taken from the mean values across 10 productions from a single male speaker (<xref ref-type="bibr" rid="bib51">Olasagasti et al., 2015</xref>), the amplitudes were then normalized by dividing by the highest value for each feature resulting in the values: (θ<sub>/aba/,A</sub> = 0.1, θ<sub>/ada/,A</sub> = 0.4, θ<sub>/aga/,A</sub> = 1), (θ<sub>/aba/,V</sub> = 1, θ<sub>/ada/,V</sub> = 0.6, θ<sub>/aga/,V</sub> = 0.37).</p><p>For the other parameters defining the perceptual model, variances and sensory noise levels, we explored 20 different combinations. Five possible values for the pair (σ<sub>V</sub> σ<sub>A</sub>): (0.1, 0.1), (0.12, 0.12), (0.12, 0.15), (0.15, 0.12), (0.15, 0.15). For each, we used four possible (σ<sub>k,A</sub> σ<sub>k,V</sub>) pairs: (0.1, 0.1), (0.1, 0.2), (0.2, 0.1) and (0.2, 0.2). Parameters outside this range typically led to categorization accuracy worse than that from the participants in <xref ref-type="bibr" rid="bib37">Lüttke et al. (2016a)</xref>.</p><p>For each of the 20 parameter sets defining the perceptual model, we tested a set of values for the parameters that define each recalibration model. Standard Bayes has two free parameters per category and feature: κ<sub>k,f,0</sub> and ν<sub>k,f,0</sub> (<xref ref-type="disp-formula" rid="equ18">Equation 6</xref>). We tested the same values for all categories and features and therefore we drop the ‘k’ (category) and ‘f’ (feature) subscripts; κ<sub>k,f,0</sub> = κ<sub>0</sub>, ν<sub>k,f,0</sub> = ν<sub>0</sub>. (κ<sub>0</sub>, ν<sub>0</sub>) = (1, 5, 10) ⊗ (1, 5, 10) (where ⊗ denotes the tensor product).</p><p>For the constant delta rule, there is a single parameter per category and feature; we use the same for all categories but tested different values for different features. The learning rates for the visual feature (R<sub>V</sub>) and acoustic feature R<sub>A</sub> tested were (<xref ref-type="disp-formula" rid="equ19">Equation 7</xref>) (R<sub>V</sub>, R<sub>A</sub>) = {(0.05, 0.1) ⊗ (0.02, 0.04, 0.06, 0.08), (0.1, 0.2) ⊗ (0.1, 0.2, 0.3, 0.4, 0.5, 0.6), (0.4, 0.5, 0. 6) ⊗ (0.4, 0.5, 0. 6), (0.7, 0.8) ⊗ (0.7, 0.8)}. In this case, we tested different values for visual and acoustic to increase the chances of the delta rule to reproduce the experimental results.</p><p>The hierarchical delta rule with decay has three parameters per category and feature (<xref ref-type="disp-formula" rid="equ20">Equation 8</xref>). For all the simulation we set R<sub>2,V</sub> and R<sub>2,A</sub> = 0. For the other two parameters D and R<sub>1</sub> (common across categories and features) we tested 35 pairs: (R<sub>1</sub>, D) = (0.05, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2) ⊗ (0.05, 0.1, 0.2, 0.3, 0.4).</p><p>Although we did not perform an exhaustive exploration of parameter space, the ranges tested were determined by the informal observation that parameters outside the ranges tested led to excessive recalibration—too many /aba/s categorized as ‘ada’ overall.</p><p>The stimuli presented to the modelled participant corresponded to the expected acoustic and visual features of their internal model. Thus if the internal model for /aba/ is centered at θ<sub>/aba/,A</sub> for the acoustic feature and at θ<sub>/aba/,V</sub> for the visual feature, those are the amplitudes chosen for the input stimuli. In other words, stimuli were tailored to the modelled participant. It is worth emphasizing that the modelled agent does not have a fused ‘McGurk’ category; their model only includes congruent expectations.</p><p>The six stimuli were defined by:</p><list list-type="bullet"><list-item><p>Acoustic /aba/: (C<sub>V</sub> C<sub>A</sub>) = ( ∅, θ<sub>/aba/,A</sub>)</p></list-item><list-item><p>Acoustic /ada/: (C<sub>V</sub> C<sub>A</sub>) = ( ∅, θ<sub>/ada/,A</sub>)</p></list-item><list-item><p>Acoustic /aga/: (C<sub>V</sub> C<sub>A</sub>) = ( ∅, θ<sub>/aga/,A</sub>)</p></list-item><list-item><p>Congruent /aba/: (C<sub>V</sub> C<sub>A</sub>) = (θ<sub>/aba/,V</sub> θ<sub>/aba/,A</sub>)</p></list-item><list-item><p>McGurk: (C<sub>V</sub> C<sub>A</sub>) = (θ<sub>/aga/,V</sub> θ<sub>/aba/,A</sub>)</p></list-item><list-item><p>Congruent /aga/: (C<sub>V</sub> C<sub>A</sub>) = (θ<sub>/aga/,V</sub> θ<sub>/aga/,A</sub>)</p></list-item></list><p>Even if the underlying parameters for a given stimulus type were the same for every trial, sensory noise created variability. The input to the model was the pair s<sub>V</sub>, s<sub>A </sub> defined by:<disp-formula id="equ21"><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where η<sub>V</sub> and η<sub>A</sub> are sampled from independent Gaussian distributions with zero mean and unit variance. That is, s<sub>V</sub> s<sub>A</sub> are noisy versions of the true amplitude modulations in the visual and auditory modality.</p><p>The six stimulus types were presented to the model in random order with 69 repetitions for each. At the end of the presentation, the model chose a percept based on the posterior distribution over syllable identity ‘k’ given the stimulus. The perceived syllable was then recalibrated by updating its defining parameters (either both mean and variance or mean alone depending on the specific update rule).</p><p>In the model recalibration step, sometimes θ<sub>/ada/,A</sub> became smaller than θ<sub>/aba/,A</sub>. This happened mostly for the constant delta rule as we increased the learning rate parameter, which also lead to increases in the McGurk contrast. Despite this modification, the observed McGurk contrast for the constant delta rule was not statistically significant. θ<sub>/ada/,A</sub> becoming smaller than θ<sub>/aba/,A</sub> constitutes a reversal of the initial relation between these parameters; empirically one finds that 2<sup>nd</sup> formant modulation is larger for /ada/ than for /aba/ (θ<sub>/ada/,A</sub> &gt; θ<sub>/aba/,A</sub>). We included a line in our code that made sure that this did not occur. If after recalibration θ<sub>/ada/,A</sub> was smaller than θ<sub>/aba/,A</sub>, the two were interchanged. This can be interpreted as a prior that incorporates information about the relations between categories. If reversals were accepted, subsequent acoustic /aba/ would be systematically classified as /ada/.</p></sec><sec id="s4-7"><title>Update rule evaluation</title><p>To evaluate the performance of the models, we used the following data from the original experiment. 1) The McGurk contrast, defined by two values: p<sub>ada,Mc</sub> the proportion of acoustic only /aba/ categorized as ‘ada’ when preceded by a fused McGurk; (29%) or by other stimuli p<sub>ada,oth</sub>, acoustic /aba/ and /aga/ and congruent /aba/ and /aga/, (16%). 2) Overall performance: the proportion of the most frequent category for each of the six stimulus types: 80% of ‘aba’ percepts for acoustic only /aba/, 83% of ‘ada’ percepts for acoustic only /ada/, 98% of ‘aga’ percepts for acoustic only /aga/; 98% of ‘aba’ percepts for congruent audiovisual /aba/, 87% of ‘ada’ percepts for incongruent McGurk (acoustic /aba/ and visual /aga/), and 98% of ‘aga’ percepts for congruent /aga/. We will represent these values as the six entries of the vector c<sub>stim</sub>.</p><p>While the original experiment had 27 participants, we run the experiment 100 times. By drawing 6000 random samples of 27 from the 100 runs, we estimated appropriate sampling distributions. For the quantities of interest listed in the previous paragraph we calculated the medians over the 6000 samples.</p><p>In a first step, for each update rule, we selected the model with the parameters that lead to the minimum mean squared error for the McGurk contrast 2∆<sup>2</sup><sub>Mc</sub> = (p<sub>ada,Mc</sub> - 29)<sup>2</sup> +(p<sub>ada,oth</sub> - 16)<sup>2</sup>. The coefficients in the update rules appearing in the Results section correspond to those that lead to the minimum ∆<sub>Mc</sub> for each update rule.</p><p>In a second step, we concentrated on the model with the best parameters for each update rule. The 6000 random samples of size 27 were used to build a sampling distribution for ∆<sub>Mc</sub> and a measure of overall performance also based on a mean squared error: 6∆<sup>2</sup><sub>overall</sub> = (c<sub>Ab</sub> - 80)<sup>2</sup> +(c<sub>Ad</sub> - 83)<sup>2</sup> + (c<sub>Ag</sub> - 98)<sup>2</sup> + (c<sub>VbAb</sub> - 98)<sup>2</sup> + (c<sub>McGurk</sub> - 87)<sup>2</sup> + (c<sub>VgAg</sub> - 98)<sup>2</sup>. Additionaly we calculated the 95% confidence intervals for the size of the McGurk contrast (the difference p<sub>ada,Mc</sub> - p<sub>ada,oth</sub>). We choose as representative for the 6000 samples, the sample with the median value for 6∆<sup>2</sup><sub>overall</sub>+2∆<sup>2</sup><sub>Mc</sub>.</p><p>For each of the 6000 random samples we tested whether the McGurk contrast paired values p<sub>ada,Mc</sub> and p<sub>ada,oth</sub> were significantly different (Wilcoxon signed rank test). In the results section we report the 95% confidence intervals for the difference p<sub>ada,Mc</sub> - p<sub>ada,oth</sub>, as well as the Wilcoxon signed rank test for the sample with the median value for 6∆<sup>2</sup><sub>overall</sub>+2∆<sup>2</sup><sub>Mc</sub>, (the representative sample from the 6000).</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref>,includes the control or ‘/ada/ contrast’ for the representative sample. This contrast is defined by the proportion of acoustic only /aba/ sounds categorized as ‘ada’ when preceded by an acoustic only /ada/ correctly categorized as ‘ada’ or by other stimuli (acoustic only /aba/ or /aga/).</p><p>All simulations and statistical tests were performed using custom scripts written in MATLAB (Release R2014b, The MathWorks, Inc, Natick, Massachusetts, United States). The original MATLAB scripts used to run the simulations are available online (<ext-link ext-link-type="uri" xlink:href="https://gitlab.unige.ch/Miren.Olasagasti/recalibration-of-speech-categories">https://gitlab.unige.ch/Miren.Olasagasti/recalibration-of-speech-categories</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/recalibration-of-speech-categories">https://github.com/elifesciences-publications/recalibration-of-speech-categories</ext-link>; <xref ref-type="bibr" rid="bib52">Olasagasti, 2020</xref>).</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation</p></fn><fn fn-type="con" id="con2"><p>Conceptualization</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-44516-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The original MATLAB scripts used to run the simulations are available online (<ext-link ext-link-type="uri" xlink:href="https://gitlab.unige.ch/Miren.Olasagasti/recalibration-of-speech-categories">https://gitlab.unige.ch/Miren.Olasagasti/recalibration-of-speech-categories</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/recalibration-of-speech-categories">https://github.com/elifesciences-publications/recalibration-of-speech-categories</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akrami</surname> <given-names>A</given-names></name><name><surname>Kopec</surname> <given-names>CD</given-names></name><name><surname>Diamond</surname> <given-names>ME</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Posterior parietal cortex represents sensory history and mediates its effects on behaviour</article-title><source>Nature</source><volume>554</volume><fpage>368</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/nature25510</pub-id><pub-id pub-id-type="pmid">29414944</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baart</surname> <given-names>M</given-names></name><name><surname>Vroomen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Do you see what you are hearing? Cross-modal effects of speech sounds on lipreading</article-title><source>Neuroscience Letters</source><volume>471</volume><fpage>100</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2010.01.019</pub-id><pub-id pub-id-type="pmid">20080146</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cognitive control, hierarchy, and the rostro-caudal organization of the frontal lobes</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>193</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.02.004</pub-id><pub-id pub-id-type="pmid">18403252</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barascud</surname> <given-names>N</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns</article-title><source>PNAS</source><volume>113</volume><fpage>E616</fpage><lpage>E625</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508523113</pub-id><pub-id pub-id-type="pmid">26787854</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Walton</surname> <given-names>ME</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1038/nn1954</pub-id><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benna</surname> <given-names>MK</given-names></name><name><surname>Fusi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational principles of synaptic memory consolidation</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1697</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1038/nn.4401</pub-id><pub-id pub-id-type="pmid">27694992</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertelson</surname> <given-names>P</given-names></name><name><surname>Vroomen</surname> <given-names>J</given-names></name><name><surname>De Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Visual recalibration of auditory speech identification: a McGurk aftereffect</article-title><source>Psychological Science</source><volume>14</volume><fpage>592</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1046/j.0956-7976.2003.psci_1470.x</pub-id><pub-id pub-id-type="pmid">14629691</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The processing of audio-visual speech: empirical and neural bases</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>363</volume><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2155</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chevillet</surname> <given-names>M</given-names></name><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional correlates of the anterolateral processing hierarchy in human auditory cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>9345</fpage><lpage>9352</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1448-11.2011</pub-id><pub-id pub-id-type="pmid">21697384</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Clarke</surname> <given-names>CM</given-names></name><name><surname>Luce</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Perceptual adaptation to speaker characteristics: VOT boundaries in stop voicing categorization</article-title><conf-name>Proceedings of the ISCA Workshop on Plasticity in Speech Perception</conf-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clayards</surname> <given-names>M</given-names></name><name><surname>Tanenhaus</surname> <given-names>MK</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name><name><surname>Jacobs</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Perception of speech reflects optimal use of probabilistic speech cues</article-title><source>Cognition</source><volume>108</volume><fpage>804</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2008.04.004</pub-id><pub-id pub-id-type="pmid">18582855</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colosio</surname> <given-names>M</given-names></name><name><surname>Shestakova</surname> <given-names>A</given-names></name><name><surname>Nikulin</surname> <given-names>VV</given-names></name><name><surname>Blagovechtchenski</surname> <given-names>E</given-names></name><name><surname>Klucharev</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural mechanisms of cognitive dissonance (Revised): An EEG study</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>5074</fpage><lpage>5083</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3209-16.2017</pub-id><pub-id pub-id-type="pmid">28438968</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coppin</surname> <given-names>G</given-names></name><name><surname>Delplanque</surname> <given-names>S</given-names></name><name><surname>Cayeux</surname> <given-names>I</given-names></name><name><surname>Porcherot</surname> <given-names>C</given-names></name><name><surname>Sander</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>I'm no longer torn after choice: how explicit choices implicitly shape preferences of odors</article-title><source>Psychological Science</source><volume>21</volume><fpage>489</fpage><lpage>493</lpage><pub-id pub-id-type="doi">10.1177/0956797610364115</pub-id><pub-id pub-id-type="pmid">20424088</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname> <given-names>DJ</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id><pub-id pub-id-type="pmid">15937014</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Hierarchical models in the brain</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000211</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000211</pub-id><pub-id pub-id-type="pmid">18989391</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Kilner</surname> <given-names>J</given-names></name><name><surname>Kiebel</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Action and behavior: a free-energy formulation</article-title><source>Biological Cybernetics</source><volume>102</volume><fpage>227</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1007/s00422-010-0364-z</pub-id><pub-id pub-id-type="pmid">20148260</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabay</surname> <given-names>Y</given-names></name><name><surname>Holt</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Short-term adaptation to sound statistics is unimpaired in developmental dyslexia</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0198146</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0198146</pub-id><pub-id pub-id-type="pmid">29879142</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganong</surname> <given-names>WF</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Phonetic categorization in auditory word perception</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>6</volume><fpage>110</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.6.1.110</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Carlin</surname> <given-names>J</given-names></name><name><surname>Stern</surname> <given-names>H</given-names></name><name><surname>Rubin</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Bayesian Data Analysis (Chapman &amp; Hall/CRC Texts in Statistical Science</source><publisher-name>CRC Press</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname> <given-names>CD</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name><name><surname>Crist</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The neural basis of perceptual learning</article-title><source>Neuron</source><volume>31</volume><fpage>681</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(01)00424-X</pub-id><pub-id pub-id-type="pmid">11567610</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heald</surname> <given-names>SLM</given-names></name><name><surname>Van Hedger</surname> <given-names>SC</given-names></name><name><surname>Nusbaum</surname> <given-names>HC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Perceptual plasticity for auditory object recognition</article-title><source>Frontiers in Psychology</source><volume>8</volume><elocation-id>781</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2017.00781</pub-id><pub-id pub-id-type="pmid">28588524</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Idemaru</surname> <given-names>K</given-names></name><name><surname>Holt</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Specificity of dimension-based statistical learning in word recognition</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>40</volume><fpage>1009</fpage><lpage>1021</lpage><pub-id pub-id-type="doi">10.1037/a0035269</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iigaya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Adaptive learning and decision-making under uncertainty by metaplastic synapses guided by a surprise detection system</article-title><source>eLife</source><volume>5</volume><elocation-id>e18073</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.18073</pub-id><pub-id pub-id-type="pmid">27504806</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izuma</surname> <given-names>K</given-names></name><name><surname>Matsumoto</surname> <given-names>M</given-names></name><name><surname>Murayama</surname> <given-names>K</given-names></name><name><surname>Samejima</surname> <given-names>K</given-names></name><name><surname>Sadato</surname> <given-names>N</given-names></name><name><surname>Matsumoto</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural correlates of cognitive dissonance and choice-induced preference change</article-title><source>PNAS</source><volume>107</volume><fpage>22014</fpage><lpage>22019</lpage><pub-id pub-id-type="doi">10.1073/pnas.1011879108</pub-id><pub-id pub-id-type="pmid">21135218</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaffe-Dax</surname> <given-names>S</given-names></name><name><surname>Frenkel</surname> <given-names>O</given-names></name><name><surname>Ahissar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dyslexics' faster decay of implicit memory for sounds and words is manifested in their shorter neural adaptation</article-title><source>eLife</source><volume>6</volume><elocation-id>e20557</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.20557</pub-id><pub-id pub-id-type="pmid">28115055</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiebel</surname> <given-names>SJ</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A hierarchy of time-scales and the brain</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000209</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000209</pub-id><pub-id pub-id-type="pmid">19008936</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleinschmidt</surname> <given-names>DF</given-names></name><name><surname>Jaeger</surname> <given-names>TF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Robust speech perception: recognize the familiar, generalize to the similar, and adapt to the novel</article-title><source>Psychological Review</source><volume>122</volume><fpage>148</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1037/a0038695</pub-id><pub-id pub-id-type="pmid">25844873</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname> <given-names>DC</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The bayesian brain: the role of uncertainty in neural coding and computation</article-title><source>Trends in Neurosciences</source><volume>27</volume><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id><pub-id pub-id-type="pmid">15541511</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname> <given-names>E</given-names></name><name><surname>Ody</surname> <given-names>C</given-names></name><name><surname>Kouneiher</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The architecture of cognitive control in the human prefrontal cortex</article-title><source>Science</source><volume>302</volume><fpage>1181</fpage><lpage>1185</lpage><pub-id pub-id-type="doi">10.1126/science.1088545</pub-id><pub-id pub-id-type="pmid">14615530</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname> <given-names>E</given-names></name><name><surname>Jubault</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Broca's area and the hierarchical organization of human behavior</article-title><source>Neuron</source><volume>50</volume><fpage>963</fpage><lpage>974</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.05.017</pub-id><pub-id pub-id-type="pmid">16772176</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname> <given-names>E</given-names></name><name><surname>Summerfield</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>An information theoretical approach to prefrontal executive function</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>229</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.04.005</pub-id><pub-id pub-id-type="pmid">17475536</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lametti</surname> <given-names>DR</given-names></name><name><surname>Rochet-Capellan</surname> <given-names>A</given-names></name><name><surname>Neufeld</surname> <given-names>E</given-names></name><name><surname>Shiller</surname> <given-names>DM</given-names></name><name><surname>Ostry</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Plasticity in the human speech motor system drives changes in speech perception</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>10339</fpage><lpage>10346</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0108-14.2014</pub-id><pub-id pub-id-type="pmid">25080594</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lancia</surname> <given-names>L</given-names></name><name><surname>Winter</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The interaction between competition, learning, and habituation dynamics in speech perception</article-title><source>Laboratory Phonology</source><volume>4</volume><fpage>221</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.1515/lp-2013-0009</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname> <given-names>AM</given-names></name><name><surname>Harris</surname> <given-names>KS</given-names></name><name><surname>Hoffman</surname> <given-names>HS</given-names></name><name><surname>Griffith</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="1957">1957</year><article-title>The discrimination of speech sounds within and across phoneme boundaries</article-title><source>Journal of Experimental Psychology</source><volume>54</volume><fpage>358</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1037/h0044417</pub-id><pub-id pub-id-type="pmid">13481283</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieder</surname> <given-names>I</given-names></name><name><surname>Adam</surname> <given-names>V</given-names></name><name><surname>Frenkel</surname> <given-names>O</given-names></name><name><surname>Jaffe-Dax</surname> <given-names>S</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Ahissar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Perceptual Bias reveals slow-updating in autism and fast-forgetting in dyslexia</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>256</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0308-9</pub-id><pub-id pub-id-type="pmid">30643299</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lüttke</surname> <given-names>CS</given-names></name><name><surname>Ekman</surname> <given-names>M</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>McGurk illusion recalibrates subsequent auditory perception</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>32891</elocation-id><pub-id pub-id-type="doi">10.1038/srep32891</pub-id><pub-id pub-id-type="pmid">27611960</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lüttke</surname> <given-names>CS</given-names></name><name><surname>Ekman</surname> <given-names>M</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Preference for audiovisual speech congruency in superior temporal cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00874</pub-id><pub-id pub-id-type="pmid">26351991</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lüttke</surname> <given-names>CS</given-names></name><name><surname>Pérez-Bellido</surname> <given-names>A</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rapid recalibration of speech perception after experiencing the McGurk illusion</article-title><source>Royal Society Open Science</source><volume>5</volume><elocation-id>170909</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.170909</pub-id><pub-id pub-id-type="pmid">29657743</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luu</surname> <given-names>L</given-names></name><name><surname>Stocker</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Post-decision biases reveal a self-consistency principle in perceptual inference</article-title><source>eLife</source><volume>7</volume><elocation-id>e33334</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.33334</pub-id><pub-id pub-id-type="pmid">29785928</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddox</surname> <given-names>WT</given-names></name><name><surname>Chandrasekaran</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Tests of a dual-system model of speech category learning</article-title><source>Bilingualism: Language and Cognition</source><volume>17</volume><fpage>709</fpage><lpage>728</lpage><pub-id pub-id-type="doi">10.1017/S1366728913000783</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnotti</surname> <given-names>JF</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A causal inference model explains perception of the McGurk effect and other incongruent audiovisual speech</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005229</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005229</pub-id><pub-id pub-id-type="pmid">28207734</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathys</surname> <given-names>C</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A bayesian foundation for individual learning under uncertainty</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><elocation-id>39</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2011.00039</pub-id><pub-id pub-id-type="pmid">21629826</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathys</surname> <given-names>CD</given-names></name><name><surname>Lomakina</surname> <given-names>EI</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Iglesias</surname> <given-names>S</given-names></name><name><surname>Brodersen</surname> <given-names>KH</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Uncertainty in perception and the hierarchical gaussian filter</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>825</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00825</pub-id><pub-id pub-id-type="pmid">25477800</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McMurray</surname> <given-names>B</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name><name><surname>Toscano</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Statistical learning of phonetic categories: insights from a computational approach</article-title><source>Developmental Science</source><volume>12</volume><fpage>369</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1111/j.1467-7687.2009.00822.x</pub-id><pub-id pub-id-type="pmid">19371359</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McQueen</surname> <given-names>JM</given-names></name><name><surname>Cutler</surname> <given-names>A</given-names></name><name><surname>Norris</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Phonological abstraction in the mental lexicon</article-title><source>Cognitive Science</source><volume>30</volume><fpage>1113</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog0000_79</pub-id><pub-id pub-id-type="pmid">21702849</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirman</surname> <given-names>D</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>Holt</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An interactive hebbian account of lexically guided tuning of speech perception</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>13</volume><fpage>958</fpage><lpage>965</lpage><pub-id pub-id-type="doi">10.3758/BF03213909</pub-id><pub-id pub-id-type="pmid">17484419</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname> <given-names>EB</given-names></name><name><surname>Mesite</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural systems underlying perceptual adjustment to Non-Standard speech tokens</article-title><source>Journal of Memory and Language</source><volume>76</volume><fpage>80</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2014.06.007</pub-id><pub-id pub-id-type="pmid">25092949</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasir</surname> <given-names>SM</given-names></name><name><surname>Ostry</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Auditory plasticity and speech motor learning</article-title><source>PNAS</source><volume>106</volume><fpage>20470</fpage><lpage>20475</lpage><pub-id pub-id-type="doi">10.1073/pnas.0907032106</pub-id><pub-id pub-id-type="pmid">19884506</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Heasly</surname> <given-names>B</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An approximately bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>12366</fpage><lpage>12378</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0822-10.2010</pub-id><pub-id pub-id-type="pmid">20844132</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olasagasti</surname> <given-names>I</given-names></name><name><surname>Bouton</surname> <given-names>S</given-names></name><name><surname>Giraud</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prediction across sensory modalities: a neurocomputational model of the McGurk effect</article-title><source>Cortex</source><volume>68</volume><fpage>61</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.04.008</pub-id><pub-id pub-id-type="pmid">26009260</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Olasagasti</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Recalibration of speech categories</data-title><source>GitLab</source><version designator="1c4410b0">1c4410b0</version><ext-link ext-link-type="uri" xlink:href="https://gitlab.unige.ch/Miren.Olasagasti/recalibration-of-speech-categories">https://gitlab.unige.ch/Miren.Olasagasti/recalibration-of-speech-categories</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otten</surname> <given-names>M</given-names></name><name><surname>Seth</surname> <given-names>AK</given-names></name><name><surname>Pinto</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A social bayesian brain: how social knowledge can shape visual perception</article-title><source>Brain and Cognition</source><volume>112</volume><fpage>69</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2016.05.002</pub-id><pub-id pub-id-type="pmid">27221986</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patri</surname> <given-names>JF</given-names></name><name><surname>Perrier</surname> <given-names>P</given-names></name><name><surname>Schwartz</surname> <given-names>JL</given-names></name><name><surname>Diard</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What drives the perceptual change resulting from speech motor adaptation? evaluation of hypotheses in a bayesian modeling framework</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1005942</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005942</pub-id><pub-id pub-id-type="pmid">29357357</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>RP</given-names></name><name><surname>Ballard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinisch</surname> <given-names>E</given-names></name><name><surname>Wozny</surname> <given-names>DR</given-names></name><name><surname>Mitterer</surname> <given-names>H</given-names></name><name><surname>Holt</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phonetic category recalibration: what are the categories?</article-title><source>Journal of Phonetics</source><volume>45</volume><fpage>91</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.wocn.2014.04.002</pub-id><pub-id pub-id-type="pmid">24932053</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname> <given-names>JR</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name><name><surname>Newport</surname> <given-names>EL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Statistical learning by 8-month-old infants</article-title><source>Science</source><volume>274</volume><fpage>1926</fpage><lpage>1928</lpage><pub-id pub-id-type="doi">10.1126/science.274.5294.1926</pub-id><pub-id pub-id-type="pmid">8943209</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samuel</surname> <given-names>AG</given-names></name><name><surname>Kraljic</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Perceptual learning for speech</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>71</volume><fpage>1207</fpage><lpage>1218</lpage><pub-id pub-id-type="doi">10.3758/APP.71.6.1207</pub-id><pub-id pub-id-type="pmid">19633336</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwiedrzik</surname> <given-names>CM</given-names></name><name><surname>Ruff</surname> <given-names>CC</given-names></name><name><surname>Lazar</surname> <given-names>A</given-names></name><name><surname>Leitner</surname> <given-names>FC</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name><name><surname>Melloni</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Untangling perceptual memory: hysteresis and adaptation map into separate cortical networks</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>1152</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs396</pub-id><pub-id pub-id-type="pmid">23236204</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname> <given-names>BB</given-names></name><name><surname>Constantinople</surname> <given-names>CM</given-names></name><name><surname>Akrami</surname> <given-names>A</given-names></name><name><surname>Hanks</surname> <given-names>TD</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fronto-parietal cortical circuits encode accumulated evidence with a diversity of timescales</article-title><source>Neuron</source><volume>95</volume><fpage>385</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.013</pub-id><pub-id pub-id-type="pmid">28669543</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>C</given-names></name><name><surname>Egner</surname> <given-names>T</given-names></name><name><surname>Greene</surname> <given-names>M</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name><name><surname>Mangels</surname> <given-names>J</given-names></name><name><surname>Hirsch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Predictive codes for forthcoming perception in the frontal cortex</article-title><source>Science</source><volume>314</volume><fpage>1311</fpage><lpage>1314</lpage><pub-id pub-id-type="doi">10.1126/science.1132028</pub-id><pub-id pub-id-type="pmid">17124325</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>C</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Perceptual classification in a rapidly changing environment</article-title><source>Neuron</source><volume>71</volume><fpage>725</fpage><lpage>736</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.022</pub-id><pub-id pub-id-type="pmid">21867887</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsunada</surname> <given-names>J</given-names></name><name><surname>Cohen</surname> <given-names>Y</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Post-decision processing in primate prefrontal cortex influences subsequent choices on an auditory decision-making task</article-title><source>eLife</source><volume>8</volume><elocation-id>e46770</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46770</pub-id><pub-id pub-id-type="pmid">31169495</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallabha</surname> <given-names>GK</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>Pons</surname> <given-names>F</given-names></name><name><surname>Werker</surname> <given-names>JF</given-names></name><name><surname>Amano</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007a</year><article-title>Unsupervised learning of vowel categories from infant-directed speech</article-title><source>PNAS</source><volume>104</volume><fpage>13273</fpage><lpage>13278</lpage><pub-id pub-id-type="doi">10.1073/pnas.0705369104</pub-id><pub-id pub-id-type="pmid">17664424</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallabha</surname> <given-names>GK</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2007">2007b</year><article-title>Success and failure of new speech category learning in adulthood: consequences of learned hebbian attractors in topographic maps</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>7</volume><fpage>53</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.3758/CABN.7.1.53</pub-id><pub-id pub-id-type="pmid">17598735</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varnet</surname> <given-names>L</given-names></name><name><surname>Knoblauch</surname> <given-names>K</given-names></name><name><surname>Meunier</surname> <given-names>F</given-names></name><name><surname>Hoen</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Using auditory classification images for the identification of fine acoustic cues used in speech perception</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>865</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00865</pub-id><pub-id pub-id-type="pmid">24379774</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vroomen</surname> <given-names>J</given-names></name><name><surname>van Linden</surname> <given-names>S</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name><name><surname>Bertelson</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual recalibration and selective adaptation in auditory-visual speech perception: contrasting build-up courses</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>572</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.01.031</pub-id><pub-id pub-id-type="pmid">16530233</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A mixture of delta-rules approximation to bayesian inference in change-point problems</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003150</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003150</pub-id><pub-id pub-id-type="pmid">23935472</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeithamova</surname> <given-names>D</given-names></name><name><surname>Maddox</surname> <given-names>WT</given-names></name><name><surname>Schnyer</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dissociable prototype learning systems: evidence from brain imaging and behavior</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>13194</fpage><lpage>13201</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2915-08.2008</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.44516.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>In the real world, the acoustic environment changes continuously -- background noise can be loud or quiet, and even at a particular volume the spectrum can change dramatically (both on relatively short timescales). This work addresses the question of how speech perception adapts to such changes. The authors propose a model that is successful in explaining empirical data, including very recent data describing perceptual adaptation at multiple time scales. This should make an important contribution to our understanding of multi-scale speech processing.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Integrating prediction errors at two time scales permits rapid recalibration of speech sound categories&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>Olasagasti and Giraud constructed probabilistic models to study speech recalibration -- how speech perception adapts to changes in the acoustic environment of the perceiver. Unlike previous work, the current work addresses a volatile environment. The resulting model is successful in explaining empirical data, including very recent data describing perceptual adaptation at multiple time scales; in particular, an updating rule at two timescales outperformed models with an updating rule at a single scale. The work is important, the paper is well-written and the results will be of great value to the field of speech perception, and likely also to the field of perception in general.</p><p>Essential revisions:</p><p>1) The authors highlighted that updating rules at multiple timescales are beneficial, but it is probably the process implemented at each timescale that matters. I would like to ask the authors to further illustrate what is the nature of cognitive processes at each timescale, besides highlighting “two timescales”.</p><p>2) The sentence in the Abstract –“sound categories are represented at different time scales” – is not clear. Is it the information about sound category represented at different timescales? I would like the authors to clarify. The probabilistic models here represent a decision/inference process in my opinion, which is inconsistent with this claim. The frame-by-frame procedure of combining the visual/audio cues in the models are unrelated to the experimental evidence that humans can tolerate a large temporal lag between audio and visual cues of speech. I would like the authors to discuss the difference between the experimental evidence and their model procedures. If possible, could the authors jitter the lag between audio and visual cues to check the model performance. Could it be possible that the large timescale biased the model estimate in the beginning of each trial, even before perceptual information comes in. If possible, could the authors illustrate dynamics of the two- timescale model estimates as in Figure 2A.</p><p>3) The way in which the model is evaluated is not clear. Please be more specific about it – describe the relevant dataset, the evaluation of the model against the dataset and its comparison with other models in this respect.</p><p>4) Please elaborate on the principle of assigning different time scales to different levels in a hierarchical Bayesian inference framework; specifically, please describe how were specific time scales selected for specific Bayesian levels.</p><p>5) The advantages to speech perception are evident from this work, but the potential theoretical and computational consequences (difficulties, limitations and advantages) are not clear enough. Please devote a Discussion section to the broader aspect of evaluating time scales of adaptation.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.44516.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The authors highlighted that updating rules at multiple timescales are beneficial, but it is probably the process implemented at each timescale that matters. I would like to ask the authors to further illustrate what is the nature of cognitive processes at each timescale, besides highlighting “two timescales”.</p></disp-quote><p>In our approach, we have framed the recalibration process within the predictive/Bayesian brain framework that uses internal models to guide perception, in which the latter are continuously monitored and recalibrated such that they best explain sensory experience.</p><p>We use the term recalibration for the process by which the agent fine-tunes its model of the world, here representations of speech categories. This process should be considered as an advanced stage of learning that takes place after a category has been learned. It is known that, even at this point, categories are still relatively flexible.</p><p>It is important to note that learning depends on at least two systems. Fast changes are believed to rely on dopamine-dependent subcortical pathways that are dominant in the initial stages of learning a new category. As a category gets consolidated it is increasingly expressed through a cortical pathway that changes more slowly but also encodes information over long time spans.</p><p>Our shorter time scale could be related to the fast changing subcortical pathway and the longer time scale representation to that implemented through the consolidated cortical pathway.</p><p>When encountering a new speaker, the listener might use these two parallel processes with the less flexible cortical system encoding knowledge and expectations based on the entire past experience of the listener, and the more flexible system encoding changes over a short time scale.</p><p>The extent to which these two pathways are involved might be partly under cognitive control. The performance monitoring system might estimate volatility and flexibly determine how to weigh recent vs past experience.</p><p>As we elaborate in response to point 5 below, we propose that the “working” category representation meeting current goals is continuously being updated based on observations. From this perspective, the short time constant could be related to change detection and the long time constant to long-term consolidation of categories.</p><disp-quote content-type="editor-comment"><p>2) The sentence in the Abstract –“sound categories are represented at different time scales” – is not clear. Is it the information about sound category represented at different timescales? I would like the authors to clarify. The probabilistic models here represent a decision/inference process in my opinion, which is inconsistent with this claim. The frame-by-frame procedure of combining the visual/audio cues in the models are unrelated to the experimental evidence that humans can tolerate a large temporal lag between audio and visual cues of speech. I would like the authors to discuss the difference between the experimental evidence and their model procedures. If possible, could the authors jitter the lag between audio and visual cues to check the model performance. Could it be possible that the large timescale biased the model estimate in the beginning of each trial, even before perceptual information comes in. If possible, could the authors illustrate dynamics of the two- timescale model estimates as in Figure 2A.</p></disp-quote><p>We have modified the Abstract to clarify several points, including those raised here.</p><p>Our model consists of two steps, an inference/ perceptual decision step characterized by the frame-by-frame procedure illustrated in Figure 2A of the original submission, and a model recalibration step, taking place after the perceptual inference and decision step, in which the model is being updated. The internal model used in the decision/inference step is characterized by what we refer to as internal representations of the speech sound categories. Each category “representation” corresponds to a mapping between speech sound category and the expected stimulus features associated with that category; in this sense the categories “predict” sensory features. Our claim is that there are at least two such mappings with different timescales. They both determine the current belief of the agent. The mapping with the shorter timescale is the one used to derive the sensory feature estimates and the classification of the stimulus as “aba”, “ada” or “aga” at every trial (denoted with θ<sub>k,f</sub> k: category, f: sensory feature). Once the stimulus has been categorized, both representations/mappings (θ<sub>k,f</sub> and μ<sub>k,f</sub>) (information about sound category) are in principle updated. The difference is that the short time scale one (θ<sub>k,f</sub>) is driven more strongly by the estimated sensory features and also forgets them faster. The higher level representation with a longer timescale (μ<sub>k,f</sub>) does not change as much and also forgets less quickly and can therefore provide an empirical prior that works as an anchor for the representation at the lower-level with the shorter timescale. In our simulations we made the simplification that the recalibration of (μ<sub>k,f</sub>) was not observable in the course of the experiment.</p><p>Therefore, the two timescales do not refer to time constants that influence the inference process itself, and therefore do not relate to the issue of the temporal lag that humans can tolerate. The frame-by-frame inference process could work with any lag provided we explicitly include a lag variable and a prior on lag durations as in Magnotti et al., 2013 (Magnotti JF, Ma WJ, Beauchamp MS. Causal inference of asynchronous audiovisual speech. Front Psychol [Internet]. 2013 Jan [cited 2013 Dec 9];4:798). Moreover, lags could not be a factor in the current work because the experiment that we modelled did not vary lags.</p><p>Regarding the issue of whether the longer timescale biases the estimate at the beginning of the trial. As stressed in the previous paragraph, the two time-scales refer to how the parameters of the model are updated after the stimulus presentation, and keep information about past observations and categorizations. During the frame-by-frame process corresponding to the perceptual decision stage, the parameters are kept constant. However, they do “bias” the sensory feature estimates throughout the trial, in the sense that the estimates are linear combinations of the actual sensory features and the expected features from the category representations.</p><disp-quote content-type="editor-comment"><p>3) The way in which the model is evaluated is not clear. Please be more specific about it – describe the relevant dataset, the evaluation of the model against the dataset and its comparison with other models in this respect.</p></disp-quote><p>For this revision we have run more extensive simulations. In order to evaluate the models we used the average data and qualitative descriptions reported in (Lüttke 2015, 2016). This is now described in Materials and methods. Specifically, we used goodness of fit measures based on the McGurk contrast: proportion of acoustic aba stimuli miscategorised as “ada” when preceded by control stimuli (16%) or by fused McGurk stimuli (29%). We also used the reported overall performance of the participants: percentages of correctly identified acoustic “aba” (80%), correctly identified acoustic “ada” (83%), correctly identified acoustic “aga” (98%), correctly identified congruent audiovisual “aba” (98%), correctly identified congruent audiovisual “aga” (98%) and percentage of fused McGurk stimuli (87%).</p><p>In this round of simulations we varied parameters that define the perceptual model and parameters that defining the update rules. For each update method we chose the set of parameters that gave the best goodness of fit. We then compared the methods by comparing their 95% CI for the difference in the main McGurk contrast.</p><disp-quote content-type="editor-comment"><p>4) Please elaborate on the principle of assigning different time scales to different levels in a hierarchical Bayesian inference framework; specifically, please describe how were specific time scales selected for specific Bayesian levels.</p></disp-quote><p>In the setup represented in the right panel of Figure 6, time-scale is an emergent concept. As we elaborate in our answer to the next point, the learning rates and time scales are related to the coefficients in the update rules, which are in turn functions of the expected “volatility” at each level. The motivation to assign different expected volatilities lies on the fact that the causes for a change in how a speech category sounds in a given utterance by a given speaker can have different origins varying with time. For example, it could change because the speaker changes –a fast change- or because of a slower drift impeding on speech categories (e.g. deafness), which would not be measurable within an experimental session.</p><p>In our simulations we chose zero volatility – infinite time scale – for the longer-term representation. The timescale of the more volatile representation is a free parameter of the model.</p><p>In the present revision we have varied the assigned values indirectly by running simulations using different values for the coefficients of the update rule.</p><disp-quote content-type="editor-comment"><p>5) The advantages to speech perception are evident from this work, but the potential theoretical and computational consequences (difficulties, limitations and advantages) are not clear enough. Please devote a Discussion section to the broader aspect of evaluating time scales of adaptation.</p></disp-quote><p>Our model implies that the brain somehow incorporates at least two representations of natural categories, a flexible and volatile one, e.g. used for the agent’s current goals, and a more stable one that keeps general and less accurate knowledge about sound categories. We propose the term “working representation” for the more flexible sound category representation, to distinguish it from its more stable “episodic representation”. Eventually, the episodic representation itself could also have several subcomponents (not modelled here), which relates to the issue raised by the reviewers about the evaluation of the timescales of adaptation.</p><p>Behaviourally, this can be advantageous when specific instances of a unique category, for example from a single speaker, have less variance and therefore less associated uncertainty than the overall prior distribution over all possible instances. Having an “intermediate level” representation with lower uncertainty makes the sensory integration process more precise, leading to more confident perceptual decisions at the single trial level. In the case of speech, this strategy allows the agent to change its working category to a more precise one, while not forgetting the underlying distribution from which the current instance has been drawn.</p><p>If this is indeed what agents do, they need to infer the distribution (mean and variance) that defines the working representation, and according to Bayesian principles they should combine sensory evidence with the prior (in this case, the memory representation). Based on these Bayesian principles we wrote the update rule in Equation 8, that appears whenever there are three quantities informing the current estimate of a variable (under Gaussian conditions). In our model, the current expected value for the working representation is informed by the value derived from the observation in the previous and current trials, and from the episodic representation -- Figure 6, right diagram. Bayesian inference assuming known volatilities for the two levels in Figure 6, and under the mean-field approximation, can be calculated analytically resulting in update equations at the bottom. The coefficients, which determine what is referred to as “learning rates” in the reinforcement learning literature, are functions of the parameters of the model related to the different sources of uncertainty: the constant volatilities at both levels, sensory noise and the width of the memory representation.</p><p>Our proposed update equation therefore assumes that agents have estimated the volatilities. The trial-by-trial behavioural data of individual participants in Lüttke et al., 2016, if available, would help estimate the effective learning rates.</p><p>Finally, from the optimal agent’s perspective, the working representation used for a given trial is the predictive working representation built from 1) updated representations after the last observation and 2) their expected change in the intervening time. The latter component denotes the uncertainty associated with the representations, e.g. more volatile representations becoming more uncertain more quickly. This last point is important as it means that, across trials, increased uncertainty associated with the previous estimate of the working representation implies more reliance on the long-term representation. In other words, across trials, the working representation “decays” back towards the long-term representation. This reflects a form of “optimal forgetting” corresponding to the expected loss of relevance of a past observation to the current trial.</p><p>Many questions remain to be answered. Notably, is the fast time scale adapting to the current environment under “cognitive” control, or is it a default component in a system with a hierarchy of fixed timescales that can be flexibly combined?</p><p>Our work originated from our interest in speech perception, however, we think that it could apply to other categorization/abstraction processes that involve natural categories such as odors, colors or faces.</p><p>We have included some of these issues in the Discussion, particularly in the paragraph “Advantages of a dual time-scale representation.”.</p></body></sub-article></article>