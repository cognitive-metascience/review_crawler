<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">48182</article-id><article-id pub-id-type="doi">10.7554/eLife.48182</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Short Report</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A neural mechanism for contextualizing fragmented inputs during naturalistic vision</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-80489"><name><surname>Kaiser</surname><given-names>Daniel</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9007-3160</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-143230"><name><surname>Turini</surname><given-names>Jacopo</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-143231"><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Psychology</institution>, <institution>University of York</institution>, <addr-line><named-content content-type="city">York</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff2"><institution content-type="dept">Department of Education and Psychology</institution>, <institution>Freie Universität Berlin</institution>, <addr-line><named-content content-type="city">Berlin</named-content></addr-line>, <country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-19516"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing editor</role><aff><institution>Peking University</institution>, <country>China</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>danielkaiser.net@gmail.com</email> (DK);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>09</day><month>10</month><year>2019</year></pub-date><volume>8</volume><elocation-id>e48182</elocation-id><history><date date-type="received"><day>03</day><month>05</month><year>2019</year></date><date date-type="accepted"><day>08</day><month>10</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Kaiser et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Kaiser et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-48182-v1.pdf"/><abstract><p>With every glimpse of our eyes, we sample only a small and incomplete fragment of the visual world, which needs to be contextualized and integrated into a coherent scene representation. Here we show that the visual system achieves this contextualization by exploiting spatial schemata, that is our knowledge about the composition of natural scenes. We measured fMRI and EEG responses to incomplete scene fragments and used representational similarity analysis to reconstruct their cortical representations in space and time. We observed a sorting of representations according to the fragments' place within the scene schema, which occurred during perceptual analysis in the occipital place area and within the first 200ms of vision. This schema-based coding operates flexibly across visual features (as measured by a deep neural network model) and different types of environments (indoor and outdoor scenes). This flexibility highlights the mechanism's ability to efficiently organize incoming information under dynamic real-world conditions.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>KA4683/2-1</award-id><principal-award-recipient><name><surname>Kaiser</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>CI241/1-1</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>CI241/3-1</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>ERC-2018-StG 803370</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants provided informed written consent. All procedures were approved by the ethical committee of the Department of Education and Psychology at Freie Universität Berlin (reference 140/2017) and were in accordance with the Declaration of Helsinki.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Data are publicly available on OSF (http://doi.org/10.17605/OSF.IO/H3G6V), as indicated in the Materials and Methods section of the manuscript.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Kaiser D</collab><collab>Turini J</collab><collab>Cichy RM</collab></person-group><year iso-8601-date="2019">2019</year><source>A neural mechanism for contextualizing fragmented information during naturalistic vision</source><ext-link ext-link-type="uri" xlink:href="http://doi.org/10.17605/OSF.IO/H3G6V">http://doi.org/10.17605/OSF.IO/H3G6V</ext-link><comment>Open Science Framwork, 10.17605/OSF.IO/H3G6V</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-48182-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>