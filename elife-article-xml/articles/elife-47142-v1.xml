<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">47142</article-id><article-id pub-id-type="doi">10.7554/eLife.47142</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The nature of the animacy organization in human ventral temporal cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-137886"><name><surname>Thorat</surname><given-names>Sushrut</given-names></name><email>s.thorat@donders.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-137887"><name><surname>Proklova</surname><given-names>Daria</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-76860"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4026-7303</contrib-id><email>m.peelen@donders.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Donders Institute for Brain, Cognition and Behaviour</institution><institution>Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Brain and Mind Institute</institution><institution>University of Western Ontario</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serre</surname><given-names>Thomas</given-names></name><role>Reviewing Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>09</day><month>09</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e47142</elocation-id><history><date date-type="received" iso-8601-date="2019-03-26"><day>26</day><month>03</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-07-17"><day>17</day><month>07</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Thorat et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Thorat et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-47142-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.47142.001</object-id><p>The principles underlying the animacy organization of the ventral temporal cortex (VTC) remain hotly debated, with recent evidence pointing to an animacy continuum rather than a dichotomy. What drives this continuum? According to the visual categorization hypothesis, the continuum reflects the degree to which animals contain animal-diagnostic features. By contrast, the agency hypothesis posits that the continuum reflects the degree to which animals are perceived as (social) agents. Here, we tested both hypotheses with a stimulus set in which visual categorizability and agency were dissociated based on representations in convolutional neural networks and behavioral experiments. Using fMRI, we found that visual categorizability and agency explained independent components of the animacy continuum in VTC. Modeled together, they fully explained the animacy continuum. Finally, clusters explained by visual categorizability were localized posterior to clusters explained by agency. These results show that multiple organizing principles, including agency, underlie the animacy continuum in VTC.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>object representation</kwd><kwd>visual cortex</kwd><kwd>object categorization</kwd><kwd>neuroimaging</kwd><kwd>animacy organization</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>725970</award-id><principal-award-recipient><name><surname>Peelen</surname><given-names>Marius V</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009890</institution-id><institution>Autonomous Province of Trento</institution></institution-wrap></funding-source><award-id>ATTEND</award-id><principal-award-recipient><name><surname>Peelen</surname><given-names>Marius V</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The animacy organization in human ventral visual cortex is driven by both the presence of animal-diagnostic visual features and the psychological property of agency.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>One of the main goals of visual cognitive neuroscience is to understand the principles that govern the organization of object representations in high-level visual cortex. There is broad consensus that the first principle of organization in ventral temporal cortex (VTC) reflects the distinction between animate and inanimate objects. These categories form distinct representational clusters (<xref ref-type="bibr" rid="bib20">Kriegeskorte et al., 2008</xref>) and activate anatomically distinct regions of VTC (<xref ref-type="bibr" rid="bib16">Grill-Spector and Weiner, 2014</xref>; <xref ref-type="bibr" rid="bib9">Chao et al., 1999</xref>).</p><p>According to the visual categorization hypothesis, this animate-inanimate organization supports the efficient readout of superordinate category information, allowing for the rapid visual categorization of objects as being animate or inanimate (<xref ref-type="bibr" rid="bib16">Grill-Spector and Weiner, 2014</xref>). The ability to rapidly detect animals may have constituted an evolutionary advantage (<xref ref-type="bibr" rid="bib6">Caramazza and Shelton, 1998</xref>; <xref ref-type="bibr" rid="bib29">New et al., 2007</xref>).</p><p>However, recent work has shown that the animacy organization reflects a continuum rather than a dichotomy, with VTC showing a gradation from objects and insects to birds and mammals (<xref ref-type="bibr" rid="bib11">Connolly et al., 2012</xref>; <xref ref-type="bibr" rid="bib36">Sha et al., 2015</xref>). This continuum was interpreted as evidence that VTC reflects the psychological property of animacy, or agency, in line with earlier work showing animate-like VTC responses to simple shapes whose movements imply agency (<xref ref-type="bibr" rid="bib8">Castelli et al., 2002</xref>; <xref ref-type="bibr" rid="bib26">Martin and Weisberg, 2003</xref>; <xref ref-type="bibr" rid="bib15">Gobbini et al., 2007</xref>). According to this agency hypothesis, the animacy organization reflects the degree to which animals share psychological characteristics with humans, such as the ability to perform goal-directed actions and experiencing thoughts and feelings.</p><p>Importantly, however, the finding of an animacy continuum can also be explained under the visual categorization hypothesis. This is because some animals (such as cats) are easier to visually categorize as animate than others (such as stingrays). This visual categorizability is closely related to visual typicality – an animal's perceptual similarity to other animals (<xref ref-type="bibr" rid="bib27">Mohan and Arun, 2012</xref>). Indeed, recent work showed that the visual categorizability of animals (as measured by reaction times) correlates with the representational distance of those animals from the decision boundary of an animate-inanimate classifier trained on VTC activity patterns (<xref ref-type="bibr" rid="bib7">Carlson et al., 2014</xref>). The finding of an animacy continuum is thus fully in line with the visual categorization hypothesis.</p><p>The difficulty in distinguishing between the visual categorization and agency hypotheses lies in the fact that animals’ visual categorizability and agency are correlated. For example, four-legged mammals are relatively fast to categorize as animate and are also judged to be psychologically relatively similar to humans. Nevertheless, visual categorizability and agency are distinct properties that can be experimentally dissociated. For example, a dolphin and a trout differ strongly in perceived agency (dolphin &gt; trout) but not necessarily in visual categorizability. In the present fMRI study, we disentangled visual categorizability and agency to assess their ability to explain the animacy continuum in VTC. This was achieved by selecting, out of a larger set, 12 animals for which visual categorizability and agency were orthogonal to each other across the set.</p><p>We find that visual categorizability and agency independently contribute to the animacy continuum in VTC as a whole. A model that combines these two factors fully explains the animacy continuum. In further analyses, we localize the independent contributions of visual categorizability and agency to distinct regions in posterior and anterior VTC, respectively. These results provide evidence that multiple organizing principles, including agency, underlie the animacy continuum and that these principles express in different parts of visual cortex.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Disentangling visual categorizability and agency</title><p>To create a stimulus set in which visual categorizability and agency are dissociated, we selected 12 animals from a total of 40 animals. Visual categorizability was quantified in two ways, using convolutional neural networks (CNNs) and human behavior, to ensure a comprehensive measure of visual categorizability. Agency was measured using a rating experiment in which participants indicated the degree to which an animal can think and feel. Familiarity with the objects was also assessed and controlled for in the final stimulus set used in the fMRI experiment.</p><sec id="s2-1-1"><title>Agency and familiarity</title><p>Agency and familiarity measures were obtained through ratings (N = 16), in which participants indicated the thoughtfulness of, feelings of, and familiarity with the 40 animals (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The correlation between the thoughtfulness and feelings ratings (<italic>τ</italic> = 0.70) was at the noise ceiling of both those ratings (<italic>τ<sub>thought</sub></italic> = 0.69, <italic>τ</italic><sub><italic>feel</italic></sub> = 0.70). We therefore averaged the thoughtfulness and feelings ratings and considered the averaged rating a measure of agency.</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.47142.002</object-id><label>Figure 1.</label><caption><title>Obtaining the models to describe animacy in the ventral temporal cortex.</title><p>(<bold>A</bold>) Trials from the ratings experiment are shown. Participants were asked to rate 40 animals on three factors - familiarity, thoughtfulness, and feelings. The correlations between the thoughtfulness and feelings ratings are at the noise ceilings of both these ratings. Therefore, the average of these ratings was taken as a measure of agency. (<bold>B</bold>) A schematic of the convolutional neural network (CNN) VGG-16 is shown. The CNN contains 13 convolutional layers (shown in green), which are constrained to perform the spatially-local computations across the visual field, and three fully-connected layers (shown in blue). The network is trained to take RGB image pixels as inputs and output the label of the object in the image. Linear classifiers are trained on layer FC8 of the CNN to classify between the activation patterns in response to animate and inanimate images. The distance from the decision boundary, toward the animate direction, is the image categorizability of an object. (<bold>C</bold>) A trial from the visual search task is shown. Participants had to quickly indicate the location (in the left or right panel) of the oddball target among 15 identical distractors which varied in size. The inverse of the pairwise reaction times are arranged as shown. Either the distractors or the targets are assigned as features of a representational space on which a linear classifier is trained to distinguish between animate and inanimate exemplars (Materials and methods). These classifiers are then used to categorize the set of images relevant to subsequent analyses; the distance from the decision boundary, towards the animate direction, is a measure of the perceptual categorizability of an object.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47142-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.47142.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Pairwise similarities between the image categorizabilities of layers from VGG-16.</title><p>The image categorizabilities evolve as we go deeper into the network, as evidenced by the two middle and late clusters (in green) which are not similar to each other. The image categorizabilities of the fully connected layers (FCs) are highly similar and all the findings described in the paper are robust to a change in layer-selection among the FCs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47142-fig1-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-1-2"><title>Visual categorizability</title><p>The first measure of visual categorizability was based on the features extracted from the final layer (FC8) of a pre-trained CNN (VGG-16 [<xref ref-type="bibr" rid="bib38">Simonyan and Zisserman, 2015</xref>]; Materials and methods). This layer contains rich feature sets that can be used to accurately categorize objects as animate or inanimate by a support vector machine (SVM) classifier. This same classifier was then deployed on the 40 candidate objects (4 exemplars each) of our experiment to quantify their categorizability. This resulted, for each object, in a representational distance from the decision boundary of the animate-inanimate classifier (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Because this measure was based on a feedforward transformation of the images, which was not informed by inferred agentic properties of the objects (such as thoughtfulness), we label this measure image categorizability.</p><p>The second measure of visual categorizability was based on reaction times in an oddball detection task previously shown to predict visual categorization times (<xref ref-type="bibr" rid="bib27">Mohan and Arun, 2012</xref>; <xref ref-type="fig" rid="fig1">Figure 1C</xref>). The appeal of this task is that it provides reliable estimates of visual categorizability using simple and unambiguous instructions (unlike a direct categorization task, which relies on the participants’ concept of animacy, again potentially confounding agency and visual categorizability). Participants were instructed to detect whether an oddball image appears to the left or the right of fixation. Reaction times in this task are an index of visual similarity, with relatively slow responses to oddball objects that are visually relatively similar to the surrounding objects (e.g. a dog surrounded by cats). A full matrix of pairwise visual similarities was created by pairing all images with each other. For a given object, these similarity values constitute a perceptual representation with respect to the other objects. These visual similarity values were then used as features in an SVM trained to classify animate vs inanimate objects. Testing this classifier on the images of the fMRI experiment resulted, for each object, in a representational distance from the decision boundary of the animate-inanimate classifier (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Because this measure was based on human perception, we labeled this measure perceptual categorizability. The neural representations the reaction times in this task rely on are not fully known, and might reflect information about inferred agency of the objects. As such, accounting for the contribution of perceptual categorizability in subsequent analyses provides a conservative estimate of the independent contribution of agency to neural representations in VTC.</p><p>The two measures of visual categorizability were positively correlated for the 12 animals that were selected for the fMRI experiment (Kendall’s <italic>τ</italic> = 0.64), indicating that they partly reflect similar animate-selective visual properties of the objects. The correspondence between these two independently obtained measures of visual categorizability provides a validation of these measures and also shows that the image categorizability obtained from the CNN is meaningfully related to human perception.</p></sec><sec id="s2-1-3"><title>Selection of image set</title><p>The final set of 12 animals for the fMRI experiment were chosen from the set of 40 images such that the correlations between image categorizability, agency, and familiarity were minimized (<xref ref-type="fig" rid="fig2">Figure 2</xref>). This was successful, as indicated by low correlations between these variables (<italic>τ</italic> &lt; 0.13, for all correlations). Because perceptual categorizability was not part of the selection procedure of the stimulus set, there was a moderate residual correlation (<italic>τ</italic> = 0.30) between perceptual categorizability and agency.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.47142.004</object-id><label>Figure 2.</label><caption><title>Disentangling image categorizability and agency.</title><p>The values of agency and image categorizability are plotted for the 40 animals used in the ratings experiment. We selected 12 animals such that the correlation between agency and image categorizability is minimized. Data-points corresponding to those 12 animals are highlighted in red.</p><p><supplementary-material id="fig2sdata1"><object-id pub-id-type="doi">10.7554/eLife.47142.005</object-id><label>Figure 2—source data 1.</label><caption><title>Agency and image categorizability scores for the 40 animals.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-47142-fig2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47142-fig2-v1.tif"/></fig></sec></sec><sec id="s2-2"><title>Animacy in the ventral temporal cortex</title><p>Participants (N = 17) in the main fMRI experiment viewed the 4 exemplars of the 12 selected animals while engaged in a one-back object-level repetition-detection task (<xref ref-type="fig" rid="fig3">Figure 3</xref>). The experiment additionally included 3 inanimate objects (cars, chairs, plants) and humans. In a separate block-design animacy localizer experiment, participants viewed 72 object images (36 animate, 36 inanimate) while detecting image repetitions (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.47142.006</object-id><label>Figure 3.</label><caption><title>The fMRI paradigm.</title><p>In the main fMRI experiment, participants viewed images of the 12 selected animals and four additional objects (cars, trees, chairs, persons). Participants indicated, via button-press, one-back object repetitions (here, two parrots). In the animacy localizer experiment, participants viewed blocks of animal (top sequence) and non-animal (bottom sequence) images. All images were different from the ones used in the main experiment. Each block lasted 16s, and participants indicated one-back image repetitions (here, the fish image).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47142-fig3-v1.tif"/></fig><p>In a first analysis, we aimed to replicate the animacy continuum for the objects in the main experiment. The VTC region of interest was defined anatomically, following earlier work (<xref ref-type="bibr" rid="bib17">Haxby et al., 2011</xref>; <xref ref-type="fig" rid="fig4">Figure 4A</xref>; Materials and methods). An SVM classifier was trained on activity patterns in this region to distinguish between blocks of animate and inanimate objects in the animacy localizer, and tested on the 16 individual objects in the main experiment. The distances from the decision boundary, towards the animate direction, were taken as the animacy scores.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.47142.007</object-id><label>Figure 4.</label><caption><title>Assessing the nature of the animacy continuum in the ventral temporal cortex (VTC).</title><p>(<bold>A</bold>) The region-of-interest, VTC, is highlighted. (<bold>B</bold>) The order of objects on the VTC animacy continuum, image categorizability (IC), perceptual categorizability (PC), and agency (Ag) are shown. (<bold>C</bold>) The within-participant correlations between VTC animacy and image categorizability (IC), perceptual categorizability (PC), visual categorizability (VC, a combination of image categorizability and perceptual categorizability; Materials and methods), and agency (Ag) are shown. All four models correlated positively with VTC animacy. (<bold>D</bold>) The left panel shows the correlations between VTC animacy and VC and Ag after regressing out the other measure from VTC animacy. Both correlations are positive, providing evidence for independent contributions of both agency and visual categorizability. The right panel shows the correlation between VTC animacy and a combination of agency and visual categorizability (Materials and methods). The combined model does not differ significantly from the VTC animacy noise ceiling (Materials and methods). This suggests that visual categorizability and agency are sufficient to explain the animacy organization in VTC. Error bars indicate 95% confidence intervals for the mean correlations.</p><p><supplementary-material id="fig4sdata1"><object-id pub-id-type="doi">10.7554/eLife.47142.011</object-id><label>Figure 4—source data 1.</label><caption><title>Values of the rank-order correlations shown in the figure, for each participant.</title><p>Also includes the MNI mask for ventral temporal cortex.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-47142-fig4-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47142-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.47142.008</object-id><label>Figure 4—figure supplement 1.</label><caption><title>The contribution of the principal components of VTC activations to VTC animacy.</title><p>Principal component analysis was performed on the average of the regression weights (VTC activations) for each of the 16 objects, which were computed using a general linear model for each run of the main experiment. (<bold>A</bold>) The average of the participant-wise percentage variance in the VTC activations explained by each of the 15 principal components is shown. (<bold>B</bold>) The average participant-wise correlations between the principal components (PCs) and VTC animacy are shown. Although the first PC captures more variance than the second PC, the second PC contributes more to VTC animacy than the first. None of the individual PCs can fully account for VTC animacy. (<bold>C</bold>) The average scores of the 16 objects on the first three PCs are shown. Animacy organizations are seen in all the three PCs, where humans and other mammals lie on one end, insects in the middle, and cars and other inanimate objects lie on the other end of the representational spaces.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47142-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.47142.009</object-id><label>Figure 4—figure supplement 2.</label><caption><title>The contributions of image and perceptual categorizabilities (IC and PC), independent of agency (Ag), to VTC animacy.</title><p>Agency is regressed out of IC and PC and correlations of those two residuals (IC<sub>i</sub> and PC<sub>i</sub>) with VTC animacy are shown in (<bold>A</bold>) and (<bold>B</bold>), respectively. Both IC and PC explain variance in VTC animacy that is not accounted for by Ag (<italic>p</italic> &lt; 0.05 for both correlations). IC and PC are also separately regressed out of Ag and the correlations of the two residuals with VTC animacy are shown in (<bold>A</bold>) and (<bold>B</bold>) respectively (<italic>p</italic> &lt; 0.05 for both correlations). Echoing the observations in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, agency contributes to VTC animacy independently of either IC, PC, or a combination of both (visual categorizability - VC in <xref ref-type="fig" rid="fig4">Figure 4C</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47142-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.47142.010</object-id><label>Figure 4—figure supplement 3.</label><caption><title>The robustness of our findings to the choice of the layer of VGG-16 used to quantify image categorizability.</title><p><xref ref-type="bibr" rid="bib13">Eberhardt et al. (2016)</xref> showed that the image categorizability scores of objects (different images than the ones we used) from the second layer of the fifth group of convolutional layers (C5-2) of VGG-16 correlated highest with how quickly humans categorized those objects as animate or inanimate in a behavioral experiment. Results were highly similar when the image categorizability (IC) as used in <xref ref-type="fig" rid="fig4">Figure 4</xref> was replaced by the image categorizability of C5-2. Specifically, the independent contributions of visual categorizability and agency to VTC animacy remained significant and the correlation between the combined model and VTC animacy was at VTC animacy noise ceiling.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47142-fig4-figsupp3-v1.tif"/></fig></fig-group><p>The mean cross-validated training accuracy (animacy localizer) of animate-inanimate classification in VTC was 89.6%, while the cross-experiment accuracy in classifying the 16 stimuli from the main fMRI experiment was 71.3%, indicating reliable animacy information in both experiments. Importantly, there was systematic and meaningful variation in the animacy scores for the objects in the main experiment (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Among the animals, humans were the most animate whereas reptiles and insects were the least animate (they were classified as inanimate, on average). These results replicate previous findings of an animacy continuum (<xref ref-type="bibr" rid="bib11">Connolly et al., 2012</xref>; <xref ref-type="bibr" rid="bib36">Sha et al., 2015</xref>).</p><p>Now that we established the animacy continuum for the selected stimulus set, we can turn to our main question of interest: what are the contributions of visual categorizability and agency to the animacy continuum in VTC? To address this question, we first correlated the visual categorizability scores and the agency ratings with the VTC animacy scores (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). VTC animacy scores correlated positively with all three measures: image categorizability (<italic>τ</italic> = 0.16; <italic>p</italic> = 10<sup>-3</sup>), perceptual categorizability (<italic>τ</italic> = 0.26; <italic>p</italic> &lt; 10<sup>-4</sup>); and agency (<italic>τ</italic> = 0.30; <italic>p</italic> &lt; 10<sup>-4</sup>). A combined model of image categorizability and perceptual categorizability (<italic>visual categorizability</italic>; Materials and methods) also positively correlated with VTC animacy (<italic>τ</italic> = 0.23; <italic>p</italic> &lt; 10<sup>-4</sup>).</p><p>Because agency and visual categorizability scores were weakly correlated, it is possible that the contribution of one of these factors was partly driven by the other. To test for their independent contributions, we regressed out the contribution of the other factor(s) from VTC animacy scores before computing the correlations. The correlation between VTC animacy and agency remained positive in all individual participants (<italic>τ</italic> = 0.23; <italic>p</italic> &lt; 10<sup>-4</sup> <xref ref-type="fig" rid="fig4">Figure 4D</xref>) after regressing out both image categorizability and perceptual categorizability. Similarly, the correlation between VTC animacy and visual categorizability remained positive after regressing out agency (<italic>τ</italic> = 0.12; <italic>p</italic> = 4.7 × 10<sup>3</sup>).</p><p>Finally, to test whether a combined model including visual categorizability and agency fully explained the animacy continuum, we performed leave-one-out regression on VTC animacy with all three factors as independent measures. The resultant combined model (derived separately for each left-out participant) had a higher correlation with VTC animacy than any of the three factors alone (within-participant comparisons - Δ<italic><sub>IC</sub></italic> = 0.21, <italic>p</italic> &lt; 10<sup>-4</sup>; Δ<italic><sub>PC</sub></italic> = 0.10, <italic>p</italic> = 6 x 10<sup>-4</sup>, Δ<italic><sub>Ag</sub></italic> = 0.07, <italic>p</italic> = 8.3 x 10<sup>-3</sup>). Furthermore, the correlation between the combined model and VTC animacy (<italic>τ</italic> = 0.37; <xref ref-type="fig" rid="fig4">Figure 4D</xref>) is at VTC animacy noise ceiling (<italic>τ<sub>NC</sub></italic> = 0.39; Materials and methods). This result suggests that a linear combination of the two models (visual categorizability and agency) fully explains the animacy continuum in VTC, but the single models alone do not.</p><sec id="s2-2-1"><title>Whole-brain searchlight analysis</title><p>Our results indicate that both visual categorizability and agency contribute to the animacy continuum in VTC as a whole. Can these contributions be anatomically dissociated as well? To test this, we repeated the analyses in a whole-brain searchlight analysis (spheres of 100 proximal voxels). To reduce the number of comparisons, we constrained the analysis to clusters showing significant above-chance animacy classification (Materials and methods). Our aim was to reveal spheres showing independent contributions of visual categorizability or agency. To obtain the independent contribution of agency, we regressed out both image categorizability and perceptual categorizability from each sphere's animacy continuum and tested if the residue reflected agency. Similarly, to obtain the independent contribution of visual categorizability, we regressed out agency from the sphere's animacy continuum and tested if the residue reflected either image categorizability or perceptual categorizability. The resulting brain maps were corrected for multiple comparisons (Materials and methods).</p><p>Results (<xref ref-type="fig" rid="fig5">Figure 5</xref>) showed that both visual categorizability and agency explained unique variance in clusters of VTC, consistent with the region-of-interest analysis. Moreover, there was a consistent anatomical mapping of the two factors: the independent visual categorizability contribution (LH: 1584 mm<sup>3</sup>, center Montreal Neurological Institute (MNI) coordinates: <italic>x</italic> = -38, <italic>y</italic> = -80, <italic>z</italic> = 7; RH: 7184 mm<sup>3</sup>, center coordinates: <italic>x</italic> = 41, <italic>y</italic> = -71, <italic>z</italic> = 1) was located posterior to the independent agency contribution (LH: 592 mm<sup>3</sup>, center coordinates: <italic>x</italic> = -42, <italic>y</italic> = -56, <italic>z</italic> = -19; RH: 4000 mm<sup>3</sup>, center coordinates: <italic>x</italic> = 39, <italic>y</italic> = -52, <italic>z</italic> = -12), extending from VTC into the lateral occipital regions. The majority of the independent agency contribution was located in the anterior part of VTC. A similar posterior-anterior organization was observed in both hemispheres (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), though stronger in the right hemisphere. These results provide converging evidence for independent contributions of visual categorizability and agency to the animacy continuum, and show that these factors explain the animacy continuum at different locations in the visual system.</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.47142.012</object-id><label>Figure 5.</label><caption><title>Searchlight analysis testing for the independent contributions of agency and visual categorizability to the animacy continuum.</title><p>The analysis followed the approach performed within the VTC ROI (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, middle panel) but now separately for individual spheres (100 voxels). The independent contribution of agency is observed within anterior VTC, while the independent contribution of visual categorizability extends from posterior VTC into the lateral occipital regions. Results are corrected for multiple comparisons (Materials and methods). (<bold>B</bold>) The correlations between agency and the animacy continuum in the searchlight spheres (variance independent of visual categorizability, in red) and the mean of the correlations between image and perceptual categorizabilities and the animacy continuum in the searchlight spheres (variance independent of agency, in blue), are shown as a function of the MNI y-coordinate. For each participant, the correlations are averaged across x and z dimensions for all the searchlight spheres that survived multiple comparison correction in the searchlight analysis depicted in (<bold>A</bold>). The blue and red bounds around the means reflect the 95% confidence bounds of the average correlations across participants. The green area denotes the anatomical bounds of VTC. Visual categorizability contributes more than agency to the animacy organization in the spheres in posterior VTC. This difference in contribution switches within VTC and agency contributes maximally to the animacy organization in more anterior regions of VTC.</p><p><supplementary-material id="fig5sdata1"><object-id pub-id-type="doi">10.7554/eLife.47142.014</object-id><label>Figure 5—source data 1.</label><caption><title>Values of the correlations shown in the figure, for each participant, and the maps of the significant independent contributions of agency and visual categorizability across the brain.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-47142-fig5-data1-v1.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47142-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.47142.013</object-id><label>Figure 5—figure supplement 1.</label><caption><title>The contributions (independent of agency) of image and perceptual categorizabilities to the animacy continuum in the searchlight spheres are shown (IC<sub>Ag</sub> and PC<sub>Ag</sub>).</title><p>Both factors contribute to similar extents to the variance in the animacy continua.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47142-fig5-figsupp1-v1.tif"/></fig></fig-group></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The present study investigated the organizing principles underlying the animacy organization in human ventral temporal cortex. Our starting point was the observation that the animacy organization expresses as a continuum rather than a dichotomy (<xref ref-type="bibr" rid="bib11">Connolly et al., 2012</xref>; <xref ref-type="bibr" rid="bib36">Sha et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Carlson et al., 2014</xref>), such that some animals evoke more animate-like response patterns than others. Our results replicate this continuum, with the most animate response patterns evoked by humans and mammals and the weakest animate response patterns evoked by insects and snakes (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Unlike previous studies, our stimulus set was designed to distinguish between two possible organizing principles underlying the animacy continuum, reflecting the degree to which an animal is visually animate (visual categorizability) and the degree to which an animal is perceived to have thoughts and feelings (agency). We found that both dimensions independently explained part of the animacy continuum in VTC; together, they fully explained the animacy continuum. Whole-brain searchlight analysis revealed distinct clusters in which visual categorizability and agency explained the animacy continuum, with the agency-based organization located anterior to the visual categorizability-based organization. Below we discuss the implications of these results for our understanding of the animacy organization in VTC.</p><p>The independent contribution of visual categorizability shows that the animacy continuum in VTC is at least partly explained by the degree to which the visual features of an animal are typical of the animate category. This was observed both for the image features themselves (as represented in a CNN) and for the perceptual representations of these images in a behavioral task. These findings are in line with previous studies showing an influence of visual features on the categorical organization in high-level visual cortex (<xref ref-type="bibr" rid="bib2">Baldassi et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Coggan et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Nasr et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Rice et al., 2014</xref>; <xref ref-type="bibr" rid="bib18">Jozwik et al., 2016</xref>). Furthermore, recent work has shown that mid-level perceptual features allow for distinguishing between animate and inanimate objects (<xref ref-type="bibr" rid="bib21">Levin et al., 2001</xref>; <xref ref-type="bibr" rid="bib22">Long et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Schmidt et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Zachariou et al., 2018</xref>) and that these features can elicit a VTC animacy organization in the absence of object recognition (<xref ref-type="bibr" rid="bib23">Long et al., 2018</xref>). Our results show that (part of) the animacy continuum is similarly explained by visual features: animals that were more easily classified as animate by a CNN (based on visual features) were also more easily classified as animate in VTC. This correlation persisted when regressing out the perceived agency of the animals. Altogether, these findings support accounts that link the animacy organization in VTC to visual categorization demands (<xref ref-type="bibr" rid="bib16">Grill-Spector and Weiner, 2014</xref>).</p><p>In parallel to investigations into the role of visual features in driving the categorical organization of VTC, other studies have shown that visual features do not full explain this organization (for reviews, see <xref ref-type="bibr" rid="bib30">Peelen and Downing, 2017</xref>; <xref ref-type="bibr" rid="bib3">Bracci et al., 2017</xref>). For example, animate-selective responses in VTC are also observed for shape- and texture-matched objects (<xref ref-type="bibr" rid="bib32">Proklova et al., 2016</xref>; <xref ref-type="bibr" rid="bib4">Bracci and Op de Beeck, 2016</xref>) and animate-like VTC responses can be evoked by geometric shapes that, through their movement, imply the presence of social agents (<xref ref-type="bibr" rid="bib8">Castelli et al., 2002</xref>; <xref ref-type="bibr" rid="bib26">Martin and Weisberg, 2003</xref>; <xref ref-type="bibr" rid="bib15">Gobbini et al., 2007</xref>). The current results contribute to these findings by showing that (part of) the animacy continuum reflects the perceived agency of the animals: animals that were perceived as being relatively more capable of having thoughts and feelings were more easily classified as animate in VTC. This correlation persisted when regressing out the influence of animal-diagnostic visual features. These findings provide evidence that the animacy continuum is not fully explained by visual categorization demands, with perceived agency contributing significantly to the animacy organization. The finding of an agency contribution to the animacy continuum raises several interesting questions.</p><p>First, what do we mean with agency and how does it relate to other properties? In the current study, agency was measured as the perceived ability of an animal to think and feel. Ratings on these two scales were highly correlated with each other, and also likely correlate highly with related properties such as the ability to perform complex goal-directed actions, the degree of autonomy, and levels of consciousness (Appendix). On all of these dimensions, humans will score highest and animals that score highly will be perceived as relatively more similar to humans. As such, the agency contribution revealed in the current study may reflect a human-centric organization (<xref ref-type="bibr" rid="bib12">Contini et al., 2019</xref>). Future studies could aim to disentangle these various properties.</p><p>Second, why would agency be an organizing principle? One reason for why agency could be an important organizing principle is because the level of agency determines how we interact with an animal: we can meaningfully interact with cats but not with slugs. To predict the behavior of high-agentic animals requires inferring internal states underlying complex goal-directed behavior (<xref ref-type="bibr" rid="bib36">Sha et al., 2015</xref>). Again, these processes will be most important when interacting with humans but will also, to varying degrees, be recruited when interacting with animals. The agency organization may reflect the specialized perceptual analysis of facial and bodily signals that allow for inferring internal states, or the perceptual predictions that follow from this analysis.</p><p>Finally, how can such a seemingly high-level psychological property as agency affect responses in visual cortex? One possibility is that anterior parts of visual cortex are not exclusively visual and represent agency more abstractly, independent of input modality (<xref ref-type="bibr" rid="bib14">Fairhall et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">van den Hurk et al., 2017</xref>). Alternatively, agency could modulate responses in visual cortex through feedback from downstream regions involved in social cognition. Regions in visual cortex responsive to social stimuli are functionally connected to the precuneus and medial prefrontal cortex – regions involved in social perspective taking and reasoning about mental states (<xref ref-type="bibr" rid="bib37">Simmons and Martin, 2012</xref>). Indeed, it is increasingly appreciated that category-selective responses in visual cortex are not solely driven by bottom-up visual input but are integral parts of large-scale domain-selective networks involved in domain-specific tasks like social cognition, tool use, navigation, or reading (<xref ref-type="bibr" rid="bib30">Peelen and Downing, 2017</xref>; <xref ref-type="bibr" rid="bib31">Price and Devlin, 2011</xref>; <xref ref-type="bibr" rid="bib25">Martin, 2007</xref>; <xref ref-type="bibr" rid="bib24">Mahon and Caramazza, 2011</xref>). Considering the close connections between regions within each of these networks, stimuli that strongly engage the broader system will also evoke responses in the visual cortex node of the network, even in the absence of visual input (<xref ref-type="bibr" rid="bib30">Peelen and Downing, 2017</xref>; <xref ref-type="bibr" rid="bib1">Amedi et al., 2017</xref>).</p><p>An alternative possibility is that agency is computed within the visual system based on visual features. This scenario is consistent with our results as long as these features are different from the features leading to animate-inanimate categorization. Similar to the visual categorizability of animacy, visual categorizability of agency could arise if there was strong pressure to quickly determine the agency of an animal based on visual input. In a supplementary analysis, we found that a model based on the features represented in the final fully connected layer of a CNN allows for predicting agency ratings (Appendix). As such, it remains possible that agency is itself visually determined.</p><p>The unique contributions of agency and visual categorizability were observed in different parts of VTC, with the agency cluster located anterior to the visual categorizability cluster. This posterior-anterior organization mirrors the well-known hierarchical organization of visual cortex. A similar posterior-anterior difference was observed in studies dissociating shape and category representations in VTC, with object shape represented posterior to object category (<xref ref-type="bibr" rid="bib32">Proklova et al., 2016</xref>; <xref ref-type="bibr" rid="bib4">Bracci and Op de Beeck, 2016</xref>). The finding that visual categorizability and agency expressed in different parts of VTC is consistent with multiple scenarios. One possibility is that VTC contains two distinct representations of animals, one representing category-diagnostic visual features and one representing perceived agency. Alternatively, VTC may contain a gradient from a more visual to a more conceptual representations of animacy, with the visual representation gradually being transformed into a conceptual representation. More work is needed to distinguish between these possibilities.</p><p>In sum, our results provide evidence that two principles independently contribute to the animacy organization in human VTC: visual categorizability, reflecting the presence of animal-diagnostic visual features, and agency, reflecting the degree to which animals are perceived as thinking and feeling social agents. These two principles expressed in different parts of visual cortex, following a posterior-to-anterior distinction. The finding of an agency organization that is not explained by differences in visual categorizability raises many new questions that can be addressed in future research.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Neural network for image categorizability</title><p>Image categorizability was quantified using a convolutional neural network (CNN), VGG-16 (<xref ref-type="bibr" rid="bib38">Simonyan and Zisserman, 2015</xref>), which had 13 convolutional layers and 3 fully-connected layers which map 224 × 224 × 3 RGB images to a 1000 dimensional object category space (each neuron corresponds to distinct labels such as cat and car). The CNN was taken from the MatConvNet package (<xref ref-type="bibr" rid="bib43">Vedaldi and Lenc, 2015</xref>) and was pre-trained on images from the ImageNet ILSVRC classification challenge (<xref ref-type="bibr" rid="bib34">Russakovsky et al., 2015</xref>).</p><p>Activations were extracted from the final fully-connected layer, prior to the softmax operation, for 960 colored images obtained from <xref ref-type="bibr" rid="bib19">Kiani et al. (2007)</xref> of which 480 contain an animal or animal parts and the rest contain inanimate objects or their parts. The dimensionality of the activations was reduced to 495 dimensions using principal component analysis in order to reduce the training time while keeping the captured variance high (more details can be found in <xref ref-type="bibr" rid="bib40">Thorat, 2017</xref>). Support vector machines (SVMs) with linear kernels were trained (with the default parameters of the function fitcsvm in MATLAB r2017b, The MathWorks, Natick, MA) to distinguish between animate and inanimate object-driven activations. Training accuracies were quantified using 6-fold cross-validation. The average cross-validation accuracy was 92.2%. The image categorizability of an object was defined as the distance to the representation of that object from the decision boundary of the trained classifier. The stimuli used in the subsequent tasks, behavioral and fMRI, did not occur in this training set of 960 images.</p></sec><sec id="s4-2"><title>Visual search task for perceptual categorizability</title><p>Perceptual categorizability was quantified using a visual search task adopted from <xref ref-type="bibr" rid="bib27">Mohan and Arun (2012)</xref>. 29 healthy adults participated in this experiment, of which 21 (9 females; age range: 19–62, median: 27) were selected according to the conditions specified below to obtain an estimate of perceptual categorizability. All participants gave informed consent. All procedures were carried out in accordance with the Declaration of Helsinki and were approved by the ethics committee of Radboud University (ECSW2017-2306-517).</p><p>For details of the experimental procedure, we refer the reader to <xref ref-type="bibr" rid="bib32">Proklova et al. (2016)</xref>. Briefly, on each trial, participants were presented with 16 images and had to indicate, as fast and as accurately as possible, in which panel (left or right) the oddball image occurred (see <xref ref-type="fig" rid="fig1">Figure 1C</xref>). The 15 distractor images were identical except that they varied in size. Participants were not instructed to look for a particular category and only had to indicate the position of the different-looking object. All participants had accuracies of 90% and higher in every block. The trials on which they made an error were repeated at the end of the respective blocks. Psychtoolbox (<xref ref-type="bibr" rid="bib5">Brainard, 1997</xref>) controlled the stimuli presentation. Gray-scaled images of the animate objects (four exemplars each of 12 animals and humans) used in the fMRI experiment and 72 images (36 animate) from the functional localizer experiment of <xref ref-type="bibr" rid="bib32">Proklova et al. (2016)</xref> were used in this experiment. Images were gray-scaled to make the participants capitalize on differences in object shapes and textures rather than color.</p><p>In order to obtain perceptual categorizability scores for the animate objects used in the fMRI experiment, we trained animate-inanimate classifiers on representations capturing perceptual similarity between objects. For each participant, the images of animate objects from the fMRI experiment were the test set, and 28 (14 animate) images randomly chosen from the 72 images were the training set. In order to obtain representations which encoded perceptual similarity between objects, each of the images from the training and test set were used as either targets or distractors (randomly chosen for each participant) while the images from the training set were used as distractors or targets (corresponding to the previous choice made for each participant). The inverse of the reaction time was used as a measure of perceptual similarity (<xref ref-type="bibr" rid="bib27">Mohan and Arun, 2012</xref>). For each of the images in the train and test set, 82 values (1/RT) were obtained which were associated with a perceptual similarity-driven representational space and were used as features for the animate-inanimate classifier. A linear SVM was trained to classify the training images as animate or inanimate. The distances of the representations of the test images were then calculated from the classification boundary and were termed decision scores. This resulted, for each participant, in decision scores for images of animals and humans used in the main fMRI experiment.</p><p>For further analysis, only those participants who had both training (4-fold cross-validation) and test accuracies for animacy classification above 50% were selected. For the relevant 21 participants, the mean training accuracy was 63.4% (&gt;50%, <italic>p </italic>&lt; 10<sup>-4</sup>), and the mean test accuracy was 70.0% (&gt;50%, <italic>p </italic>&lt; 10<sup>-4</sup>). Each object’s perceptual categorizability was quantified as the average of its decision scores across participants.</p></sec><sec id="s4-3"><title>Main ratings experiment for agency</title><p>Agency was quantified using a ratings experiment. Sixteen healthy adults participated in this experiment (9 females; all students at the University of Trento). All participants gave informed consent. All procedures were carried out in accordance with the Declaration of Helsinki and were approved by the ethics committee of the University of Trento (protocol 2013–015).</p><p>In each trial, four colored images of an animal from a set of 40 animals were shown, and participants were asked to indicate, on a scale of 0 to 100, how much thoughtfulness or feelings they attributed to the animal, or how familiar they were with the animal. These three factors constituted three blocks of the experiment (the order was randomized across participants). At the beginning of each block, a description of the relevant factor was provided. Participants were encouraged to use the descriptions as guidelines for the three factors. In quantifying their familiarity with an animal, participants had to account for the knowledge about and the amount of interaction they have had with the animal. In quantifying the thoughtfulness an animal might have, participants had to account for the animal’s ability in planning, having intentions, and abstraction. In quantifying the feelings an animal might have, participants had to account for the animal’s ability to empathise, have sensations, and react to situations.</p><p>As mentioned in the Results section, the feelings and thoughtfulness ratings co-varied substantially with each other (the within-participant correlations were at the noise ceilings of the two factors). Agency was quantified as the average of the ratings for feelings and thoughtfulness.</p></sec><sec id="s4-4"><title>fMRI experiment</title><p>A functional magnetic resonance imaging (fMRI) experiment was performed to obtain the animacy continua in high-level visual cortex, specifically the ventral temporal cortex (VTC). The design was adopted from <xref ref-type="bibr" rid="bib32">Proklova et al. (2016)</xref>. Schematics of the main experiment and the animacy localizer experiment are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p><sec id="s4-4-1"><title>Participants</title><p>Seventeen healthy adults (6 females; age range: 20 − 32, median: 25) were scanned at the Center for Mind/Brain Sciences of the University of Trento. This sample size was chosen to be the same as in <xref ref-type="bibr" rid="bib32">Proklova et al. (2016)</xref>, as our animacy localizer and main experiment procedure were similar to theirs. All participants gave informed consent. All procedures were carried out in accordance with the Declaration of Helsinki and were approved by the ethics committee of the University of Trento (protocol 2013–015).</p></sec><sec id="s4-4-2"><title>Main experiment procedure</title><p>The stimuli consisted of colored images (4 exemplars each) of the 12 animals for which image categorizability and agency were orthogonalized, humans, and three inanimate objects (cars, plants, and chairs). There were a total of 64 images.</p><p>The main experiment consisted of eight runs. Each run consisted of 80 trials that were composed of 64 object trials and 16 fixation-only trials. In object trials, a single stimulus was presented for 300 ms, followed by a 3700 ms fixation period. In each run, each of the 64 images appeared exactly once. In fixation-only trials, the fixation cross was shown for 4000 ms. Trial order was randomized, with the constraints that there were exactly eight one-back repetitions of the same category (e.g., two cows in direct succession) within the object trials and that there were no two fixation trials appearing in direct succession. Each run started and ended with a 16s fixation period, leading to a total run duration of 5.9 min. Participants were instructed to press a button whenever they detected a one-back object repetition.</p></sec><sec id="s4-4-3"><title>Animacy localizer experiment procedure</title><p>In addition to the main experiment, participants completed one run of a functional localizer experiment. During the localizer, participants viewed grey-scale images of 36 animate and 36 inanimate stimuli in a block design. Each block lasted 16s, containing 20 stimuli that were each presented for 400 ms, followed by a 400 ms blank interval. There were eight blocks of each stimulus category and four fixation-only blocks per run. The order of the first 10 blocks was randomized and then mirror-reversed for the other 10 blocks. Participants were asked to detect one-back image repetitions, which happened twice during every non-fixation block.</p></sec><sec id="s4-4-4"><title>fMRI acquisition</title><p>Imaging data were acquired using a MedSpec 4-T head scanner (Bruker Biospin GmbH, Rheinstetten, Germany), equipped with an eight-channel head coil. For functional imaging, T2*-weighted EPIs were collected (repetition time = 2.0s, echo-time = 33 ms, 73° flip-angle, 3 mm × 3 mm × 3 mm voxel size, 1 mm gap, 34 slices, 192 mm field of view, 64 × 64 matrix size). A high-resolution T1-weighted image (magnetization prepared rapid gradient echo; 1 mm × 1 mm × 1 mm voxel size) was obtained as an anatomical reference.</p></sec><sec id="s4-4-5"><title>fMRI data pre-processing</title><p>The fMRI data were analyzed using MATLAB and SPM8. During preprocessing, the functional volumes were realigned, co-registered to the structural image, re-sampled to a 2 mm × 2 mm × 2 mm grid, and spatially normalized to the Montreal Neurological Institute 305 template included in SPM8. No spatial smoothing was applied.</p></sec><sec id="s4-4-6"><title>Region of interest - Ventral temporal cortex</title><p>VTC was defined as in <xref ref-type="bibr" rid="bib17">Haxby et al. (2011)</xref>. The region extended from −71 to −21 on the y-axis of the Montreal Neurological Institute (MNI) coordinates. The region was drawn to include the inferior temporal, fusiform, and lingual/parahippocampal gyri. The gyri were identified using Automated Anatomical Labelling (AAL) parcellation (<xref ref-type="bibr" rid="bib41">Tzourio-Mazoyer et al., 2002</xref>).</p></sec><sec id="s4-4-7"><title>Obtaining the animacy continua from fMRI data</title><p>Animacy continua were extracted from parts of the brain (either a region of interest such as VTC or a searchlight sphere) with a cross-decoding approach. SVM classifiers were trained on the BOLD images obtained from the animate and inanimate blocks of the localizer experiment, and tested on the BOLD images obtained from the main experiment. The degree of animacy of an object is given by the distance of its representation from the classifier decision boundary. As the BOLD response is delayed by seconds after stimulus onset, we had to decide the latency of the BOLD images we wanted to base our analysis on. The classification test accuracy and the animacy continuum noise ceiling for the objects from the main experiment were higher for the BOLD images at 4s latency than both 6s latency and the average of the images at 4 and 6s latencies. Therefore, we based our analysis on the BOLD images at 4s latency. The findings remain unchanged across the latencies mentioned.</p><p>All the images of the brain shown in this article were rendered using MRIcron.</p></sec><sec id="s4-4-8"><title>Comparing models with the animacy continua in the brain</title><p>We compared the animacy continua in the brain (such as the animacy continuum in VTC and animacy continua in searchlight spheres) with image and perceptual categorizabilities (visual categorizability), agency, their combination, and their independent components. The comparisons were performed at participant-level with rank-order correlations (Kendall's τ). The comparison between an animacy continuum and the independent component of a model was performed by regressing out other models from the animacy continuum and correlating the residue with the model of interest, for each participant.</p><p>Given a participant, the comparison between an animacy continuum and the combination of models was computed as follows. The animacy continuum was modeled as a linear combination of the models (with linear regression) for the rest of the participants. The regression weights associated with each model in the combination across those participants were averaged, and the animacy continuum of the participant of interest was predicted using a linear combination of the models using the averaged weights. The predicted animacy continuum was then correlated with the actual animacy continuum of this participant. This procedure was implemented iteratively for each participant to get a group estimate of the correlation between an animacy continuum and a combination of models.</p></sec><sec id="s4-4-9"><title>Comparing visual categorizability with the animacy continua</title><p>The contribution of visual categorizability to an animacy continuum is gauged by the comparison between that animacy continuum and a combination of image and perceptual categorizabilities in a leave-one-participant-out analysis as mentioned above. The independent contribution of visual categorizability to an animacy continuum is gauged by regressing out agency from the image and perceptual categorizabilities and combining the residues to model the animacy continuum in a leave-one-participant-out analysis as mentioned above. When visual categorizability is to be regressed out of an animacy continuum (to obtain the independent contribution of agency), image and perceptual categorizabilities are regressed out. When visual categorizability is to be included in a combination of models, image and perceptual categorizabilities are added as models.</p><p>To assess if a model or a combination of models explained all the variance in the animacy continuum across participants, for each participant we tested if the correlation between the model or the animacy continuum predicted by the combined model (in a leave-one-out fashion as above) and the average of animacy continua of the other participants was lower than the correlation between the participant’s animacy continuum and the average of animacy continua of the other participants. On the group level, if this one-sided test (see ‘Statistical tests in use’) was not significant (<italic>p</italic> &gt; 0.05), we concluded that the correlation between the model or a combination of models hit the animacy continuum noise ceiling and thus explained all the variance in the animacy continuum across participants. In the comparisons in <xref ref-type="fig" rid="fig4">Figure 4C–D</xref>, only the correlation between VTC animacy and the combination of visual categorizability and agency was at VTC animacy noise ceiling.</p></sec><sec id="s4-4-10"><title>Searchlight details</title><p>In the whole-brain searchlight analysis, the searchlight spheres contained 100 proximal voxels. SVM classifiers were trained to distinguish between the BOLD images, within the sphere, corresponding to animate and inanimate stimuli from the localizer experiment. The classifiers were tested on the BOLD images, within the sphere, from the main experiment. Threshold-free cluster enhancement (TFCE; <xref ref-type="bibr" rid="bib39">Smith and Nichols, 2009</xref>) with a permutation test was used to correct for multiple comparisons of the accuracies relative to baseline (50%). Further analysis was constrained to the clusters which showed above-chance classification (between-subjects, <italic>p</italic> &lt; 0.05, on both localizer and main experiment accuracies) of animate and inanimate objects. Within each searchlight sphere that survived the multiple comparisons correction, the animacy continuum was compared with image and perceptual categorizabilities (after regressing out agency) and agency (after regressing out both image and perceptual categorizabilities). Multiple comparisons across spheres of correlations to baseline (0) were corrected using TFCE. The independent visual categorizability clusters were computed as a union of spheres that had a significant contribution (independent of agency) from either image or perceptual categorizabilities.</p></sec><sec id="s4-4-11"><title>Statistical tests in use</title><p>Hypothesis testing was done with bootstrap analysis. We sampled 10,000 times with replacement from the observations being tested. p-Values correspond to one minus the proportion of sample means that are above or below the null hypothesis (corresponding to the test of interest). The p-values reported in the paper correspond to one-sided tests. The 95% confidence intervals were computed by identifying the values below and above which 2.5% of the values in the bootstrapped distribution lay. Exact p-values are reported except when means of all the bootstrap samples are higher or lower than hypothesized in which case we mention <italic>p </italic>&lt; 10<sup>-4</sup>.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Daniel Kaiser for his help with experimental design. The research was supported by the Autonomous Province of Trento, Call ‘Grandi Progetti 2012’, project ‘Characterizing and improving brain mechanisms of attention - ATTEND’. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 725970).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Investigation, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Writing—original draft, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants gave informed consent. All procedures were carried out in accordance with the Declaration of Helsinki and were approved by the ethics committees of the University of Trento (protocol 2013-015) and the Radboud University (ECSW2017-2306-517).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.47142.015</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-47142-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data and analysis code needed to reproduce the results reported in the manuscript can be found on OSF: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/VXWG9">https://doi.org/10.17605/OSF.IO/VXWG9</ext-link>. Source data files have been provided for Figures 2, 4 and 5.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Thorat</surname><given-names>S</given-names></name><name><surname>Peelen</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>The nature of the animacy organization in human ventral temporal cortex - Essential data and analysis code</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/OSF.IO/VXWG9</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname> <given-names>A</given-names></name><name><surname>Hofstetter</surname> <given-names>S</given-names></name><name><surname>Maidenbaum</surname> <given-names>S</given-names></name><name><surname>Heimler</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task selectivity as a comprehensive principle for brain organization</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>307</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.03.007</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassi</surname> <given-names>C</given-names></name><name><surname>Alemi-Neissi</surname> <given-names>A</given-names></name><name><surname>Pagan</surname> <given-names>M</given-names></name><name><surname>Dicarlo</surname> <given-names>JJ</given-names></name><name><surname>Zecchina</surname> <given-names>R</given-names></name><name><surname>Zoccolan</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Shape similarity, better than semantic membership, accounts for the structure of visual object representations in a population of monkey inferotemporal neurons</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003167</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003167</pub-id><pub-id pub-id-type="pmid">23950700</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Ritchie</surname> <given-names>JB</given-names></name><name><surname>de Beeck</surname> <given-names>HO</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>On the partnership between neural representations of object categories and visual features in the ventral visual pathway</article-title><source>Neuropsychologia</source><volume>105</volume><fpage>153</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.06.010</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Op de Beeck</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>432</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2314-15.2016</pub-id><pub-id pub-id-type="pmid">26758835</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Shelton</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Domain-specific knowledge systems in the brain the animate-inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>10</volume><fpage>1</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1162/089892998563752</pub-id><pub-id pub-id-type="pmid">9526080</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname> <given-names>TA</given-names></name><name><surname>Ritchie</surname> <given-names>JB</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Durvasula</surname> <given-names>S</given-names></name><name><surname>Ma</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reaction time for object categorization is predicted by representational distance</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>132</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00476</pub-id><pub-id pub-id-type="pmid">24001004</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelli</surname> <given-names>F</given-names></name><name><surname>Frith</surname> <given-names>C</given-names></name><name><surname>Happé</surname> <given-names>F</given-names></name><name><surname>Frith</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Autism, asperger syndrome and brain mechanisms for the attribution of mental states to animated shapes</article-title><source>Brain</source><volume>125</volume><fpage>1839</fpage><lpage>1849</lpage><pub-id pub-id-type="doi">10.1093/brain/awf189</pub-id><pub-id pub-id-type="pmid">12135974</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname> <given-names>LL</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Attribute-based neural substrates in temporal cortex for perceiving and knowing about objects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>913</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1038/13217</pub-id><pub-id pub-id-type="pmid">10491613</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coggan</surname> <given-names>DD</given-names></name><name><surname>Baker</surname> <given-names>DH</given-names></name><name><surname>Andrews</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The role of visual and semantic properties in the emergence of Category-Specific patterns of neural response in the human brain</article-title><source>eNeuro</source><volume>3</volume><elocation-id>ENEURO.0158-16.2016</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0158-16.2016</pub-id><pub-id pub-id-type="pmid">27517086</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Guntupalli</surname> <given-names>JS</given-names></name><name><surname>Gors</surname> <given-names>J</given-names></name><name><surname>Hanke</surname> <given-names>M</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Wu</surname> <given-names>YC</given-names></name><name><surname>Abdi</surname> <given-names>H</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The representation of biological classes in the human brain</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>2608</fpage><lpage>2618</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5547-11.2012</pub-id><pub-id pub-id-type="pmid">22357845</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Contini</surname> <given-names>EW</given-names></name><name><surname>Goddard</surname> <given-names>E</given-names></name><name><surname>Grootswagers</surname> <given-names>T</given-names></name><name><surname>Williams</surname> <given-names>M</given-names></name><name><surname>Carlson</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A humanness dimension to visual object coding in the brain</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/648998</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Eberhardt</surname> <given-names>S</given-names></name><name><surname>Cader</surname> <given-names>JG</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How deep is the feature analysis underlying rapid visual categorization?</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1100</fpage><lpage>1108</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/6218-how-deep-is-the-feature-analysis-underlying-rapid-visual-categorization">https://papers.nips.cc/paper/6218-how-deep-is-the-feature-analysis-underlying-rapid-visual-categorization</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname> <given-names>SL</given-names></name><name><surname>Porter</surname> <given-names>KB</given-names></name><name><surname>Bellucci</surname> <given-names>C</given-names></name><name><surname>Mazzetti</surname> <given-names>M</given-names></name><name><surname>Cipolli</surname> <given-names>C</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Plastic reorganization of neural systems for perception of others in the congenitally blind</article-title><source>NeuroImage</source><volume>158</volume><fpage>126</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.057</pub-id><pub-id pub-id-type="pmid">28669909</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Koralek</surname> <given-names>AC</given-names></name><name><surname>Bryan</surname> <given-names>RE</given-names></name><name><surname>Montgomery</surname> <given-names>KJ</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Two takes on the social brain: a comparison of theory of mind tasks</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>1803</fpage><lpage>1814</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.11.1803</pub-id><pub-id pub-id-type="pmid">17958483</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname> <given-names>K</given-names></name><name><surname>Weiner</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id><pub-id pub-id-type="pmid">24962370</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Guntupalli</surname> <given-names>JS</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Conroy</surname> <given-names>BR</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Hanke</surname> <given-names>M</given-names></name><name><surname>Ramadge</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A common, high-dimensional model of the representational space in human ventral temporal cortex</article-title><source>Neuron</source><volume>72</volume><fpage>404</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.08.026</pub-id><pub-id pub-id-type="pmid">22017997</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname> <given-names>KM</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual features as stepping stones toward semantics: explaining object similarity in IT and perception with non-negative least squares</article-title><source>Neuropsychologia</source><volume>83</volume><fpage>201</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.10.023</pub-id><pub-id pub-id-type="pmid">26493748</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name><name><surname>Mirpour</surname> <given-names>K</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Object category structure in response patterns of neuronal population in monkey inferior temporal cortex</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>4296</fpage><lpage>4309</lpage><pub-id pub-id-type="doi">10.1152/jn.00024.2007</pub-id><pub-id pub-id-type="pmid">17428910</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Ruff</surname> <given-names>DA</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Bodurka</surname> <given-names>J</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levin</surname> <given-names>DT</given-names></name><name><surname>Takarae</surname> <given-names>Y</given-names></name><name><surname>Miner</surname> <given-names>AG</given-names></name><name><surname>Keil</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficient visual search by category: specifying the features that mark the difference between artifacts and animals in preattentive vision</article-title><source>Perception &amp; Psychophysics</source><volume>63</volume><fpage>676</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.3758/BF03194429</pub-id><pub-id pub-id-type="pmid">11436737</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>B</given-names></name><name><surname>Störmer</surname> <given-names>VS</given-names></name><name><surname>Alvarez</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mid-level perceptual features contain early cues to animacy</article-title><source>Journal of Vision</source><volume>17</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.1167/17.6.20</pub-id><pub-id pub-id-type="pmid">28654965</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>B</given-names></name><name><surname>Yu</surname> <given-names>CP</given-names></name><name><surname>Konkle</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mid-level visual features underlie the high-level categorical organization of the ventral stream</article-title><source>PNAS</source><volume>115</volume><fpage>E9015</fpage><lpage>E9024</lpage><pub-id pub-id-type="doi">10.1073/pnas.1719616115</pub-id><pub-id pub-id-type="pmid">30171168</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahon</surname> <given-names>BZ</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What drives the organization of object knowledge in the brain?</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>97</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.01.004</pub-id><pub-id pub-id-type="pmid">21317022</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The representation of object concepts in the brain</article-title><source>Annual Review of Psychology</source><volume>58</volume><fpage>25</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.57.102904.190143</pub-id><pub-id pub-id-type="pmid">16968210</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>A</given-names></name><name><surname>Weisberg</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neural foundations for understanding social and mechanical concepts</article-title><source>Cognitive Neuropsychology</source><volume>20</volume><fpage>575</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1080/02643290342000005</pub-id><pub-id pub-id-type="pmid">16648880</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohan</surname> <given-names>K</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Similarity relations in visual search predict rapid visual categorization</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.1167/12.11.19</pub-id><pub-id pub-id-type="pmid">23092947</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname> <given-names>S</given-names></name><name><surname>Echavarria</surname> <given-names>CE</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Thinking outside the box: rectilinear shapes selectively activate scene-selective cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>6721</fpage><lpage>6735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4802-13.2014</pub-id><pub-id pub-id-type="pmid">24828628</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>New</surname> <given-names>J</given-names></name><name><surname>Cosmides</surname> <given-names>L</given-names></name><name><surname>Tooby</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Category-specific attention for animals reflects ancestral priorities, not expertise</article-title><source>PNAS</source><volume>104</volume><fpage>16598</fpage><lpage>16603</lpage><pub-id pub-id-type="doi">10.1073/pnas.0703913104</pub-id><pub-id pub-id-type="pmid">17909181</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Category selectivity in human visual cortex: beyond visual object recognition</article-title><source>Neuropsychologia</source><volume>105</volume><fpage>177</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.03.033</pub-id><pub-id pub-id-type="pmid">28377161</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname> <given-names>CJ</given-names></name><name><surname>Devlin</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The interactive account of ventral occipitotemporal contributions to reading</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>246</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.04.001</pub-id><pub-id pub-id-type="pmid">21549634</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname> <given-names>D</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Disentangling representations of object shape and object category in human visual cortex: the Animate–Inanimate Distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00924</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rice</surname> <given-names>GE</given-names></name><name><surname>Watson</surname> <given-names>DM</given-names></name><name><surname>Hartley</surname> <given-names>T</given-names></name><name><surname>Andrews</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Low-level image properties of visual objects predict patterns of neural response across category-selective regions of the ventral visual pathway</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>8837</fpage><lpage>8844</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5265-13.2014</pub-id><pub-id pub-id-type="pmid">24966383</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname> <given-names>O</given-names></name><name><surname>Deng</surname> <given-names>J</given-names></name><name><surname>Su</surname> <given-names>H</given-names></name><name><surname>Krause</surname> <given-names>J</given-names></name><name><surname>Satheesh</surname> <given-names>S</given-names></name><name><surname>Ma</surname> <given-names>S</given-names></name><name><surname>Huang</surname> <given-names>Z</given-names></name><name><surname>Karpathy</surname> <given-names>A</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Bernstein</surname> <given-names>M</given-names></name><name><surname>Berg</surname> <given-names>AC</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ImageNet Large Scale Visual Recognition Challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname> <given-names>F</given-names></name><name><surname>Hegele</surname> <given-names>M</given-names></name><name><surname>Fleming</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Perceiving animacy from shape</article-title><source>Journal of Vision</source><volume>17</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/17.11.10</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sha</surname> <given-names>L</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Abdi</surname> <given-names>H</given-names></name><name><surname>Guntupalli</surname> <given-names>JS</given-names></name><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The animacy continuum in the human ventral vision pathway</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>665</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00733</pub-id><pub-id pub-id-type="pmid">25269114</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname> <given-names>WK</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spontaneous resting-state BOLD fluctuations reveal persistent domain-specific neural networks</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>7</volume><fpage>467</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1093/scan/nsr018</pub-id><pub-id pub-id-type="pmid">21586527</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simonyan</surname> <given-names>K</given-names></name><name><surname>Zisserman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Very deep convolutional networks for Large-Scale image recognition</article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Thorat</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Using Convolutional Neural Networks to measure the contribution of visual features to the representation of object animacy in the brain</article-title><publisher-name>University of Trento</publisher-name><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.31237/osf.io/fxz4q">https://doi.org/10.31237/osf.io/fxz4q</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzourio-Mazoyer</surname> <given-names>N</given-names></name><name><surname>Landeau</surname> <given-names>B</given-names></name><name><surname>Papathanassiou</surname> <given-names>D</given-names></name><name><surname>Crivello</surname> <given-names>F</given-names></name><name><surname>Etard</surname> <given-names>O</given-names></name><name><surname>Delcroix</surname> <given-names>N</given-names></name><name><surname>Mazoyer</surname> <given-names>B</given-names></name><name><surname>Joliot</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title><source>NeuroImage</source><volume>15</volume><fpage>273</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0978</pub-id><pub-id pub-id-type="pmid">11771995</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Hurk</surname> <given-names>J</given-names></name><name><surname>Van Baelen</surname> <given-names>M</given-names></name><name><surname>Op de Beeck</surname> <given-names>HP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Development of visual category selectivity in ventral visual cortex does not require visual experience</article-title><source>PNAS</source><volume>114</volume><fpage>E4501</fpage><lpage>E4510</lpage><pub-id pub-id-type="doi">10.1073/pnas.1612862114</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vedaldi</surname> <given-names>A</given-names></name><name><surname>Lenc</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Matconvnet: convolutional neural networks for matlab</article-title><conf-name>Proceedings of the 23rd ACM International Conference on Multimedia ACM</conf-name><fpage>689</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1145/2733373.2807412</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zachariou</surname> <given-names>V</given-names></name><name><surname>Del Giacco</surname> <given-names>AC</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Yue</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bottom-up processing of curvilinear visual features is sufficient for animate/inanimate object categorization</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/18.12.3</pub-id><pub-id pub-id-type="pmid">30458511</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s8" sec-type="appendix"><title>Agency can be derived from visual feature differences</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.47142.016</object-id><p>To test whether agency ratings can be predicted based on high-level visual feature representations, agency ratings were collected for a set of 436 images. The activation patterns of these images in the final fully-connected layer (FC8) of VGG-16 was established. A regression model trained on these activation patterns could accurately predict agency ratings of the stimuli used in our fMRI experiment, as described in more detail below.</p><p>Agency ratings were collected for 436 object images, which included the 12 animal images from the main fMRI experiment. The ratings experiment was similar to the main ratings experiment. However, instead of thoughtfulness and feelings, 16 participants rated the agency of animals shown in the stimuli. All participants gave informed consent. All procedures were carried out in accordance with the Declaration of Helsinki and were approved by the ethics committee of Radboud University (ECSW2017-2306-517). One image of an object was shown at a time. Agency was defined as the capacity of individuals to act independently and to make their own free choices, and participants were instructed to consider factors such as ‘planning, intentions, abstraction, empathy, sensation, reactions, thoughtfulness, feelings’. The agency ratings for the 12 animals co-varied positively with the agency scores from the main ratings experiment (<inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>), and the mean correlation was at (main ratings experiment's) agency noise ceiling (<inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula>).</p><p>Activations from VGG-16 FC8 were extracted for these 436 images and principal component analysis was performed on the activations driven by the 388 images, excluding the 12 × 4 animal images from the main fMRI experiment. A cross-validated regression analysis was performed, with the agency ratings as the dependent variable and the principal components of FC8 as the independent variables. The first 20 principal components (regularisation cut-off) were included in the final model, as the models with more components provided with little gains in the similarities of the computed scores to the actual agency scores for the left-out images, while the similarities for the included images kept increasing (over-fitting). The agency scores were computed for the left out 12 animals and compared to the agency ratings obtained from the current experiment. They co-varied positively (<inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.61</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) but the mean correlation was not at the agency ratings noise ceiling (<inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.79</mml:mn></mml:mrow></mml:math></inline-formula>). These observations show that agency ratings can be predicted based on high-level visual feature representations in a feedforward convolutional neural network.</p></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47142.020</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Serre</surname><given-names>Thomas</given-names></name><role>Reviewing Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Haxby</surname><given-names>James</given-names> </name><role>Reviewer</role><aff><institution>Dartmouth College</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Bernhardt-Walther</surname><given-names>Dirk</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;The nature of the animacy organization in human ventral temporal cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Thomas Serre as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Michael Frank as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: James Haxby (Reviewer #2); Dirk Bernhardt-Walther (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This is a timely manuscript that seeks to explain the animacy continuum found in the Ventral Temporal Cortex (VTC). The authors used computational and behavioral methods to parameterize a set of stimuli used for an fMRI experiment to disentangle visual categorizability vs. agency. While these two measures have been confounded in previous work, the present study uses a stimulus set in which visual categorizability and agency are dissociated based on representations derived from convolutional neural networks and behavioral experiments. A linear model which incorporates both measures fully explains the continuum found in the fMRI experiment. A subsequent spotlight search shows that categorizability and agency are actually represented in separate clusters of voxels further strengthening the main result.</p><p>The reviewers unanimously agreed that the paper is of sufficient interest, quality, and novelty to merit accepting it for publication in <italic>eLife</italic>. The reviewers all agreed that no additional experiments are needed. A revision is however needed to clarify some of the methods and concepts used as well as to provide additional data analyses. We have the following suggestions to improve the manuscript.</p><p>Essential revisions:</p><p>The reviewers suggest further analyses and figures to clarify how the representation of visual categorization and of agency are structured. Is the animacy continuum based both on visual features and agentic properties, or are there two animacy continua or a gradient reflecting a transition from a representation of visual information that lays the foundation and a representation of agency? Should the term &quot;animacy continuum&quot; refer to an amalgam of visual features and agency, or should it be reserved for semantic content?</p><p>One suggestion includes a multidimensional scaling analysis similar to the analyses and figures of Connolly et al. (2016). In the original paper, the authors showed gradients of changes in representational geometry that (a) disentangled representation of animal taxonomy vs. visual appearance in VTC and (b) disentangled predacity from visual appearance and taxonomy along the STS. A similar method could be used to help clarify exactly how the animacy continuum in VTC reflects both visual categorizability and agency. They seem to be dissociable, and the anatomical and conceptual aspects of this dissociation could be explored and explicated more thoroughly.</p><p>In a similar vein, the authors pose the question &quot;How can such a seemingly high-level psychological property as agency affect responses in visual cortex?&quot; Why the need to assume VTC is purely visual? This assumption carries with it the assertion that semantic information must come from elsewhere in a large-scale network. It seems quite possible that IT cortex makes the same computations to extract this critical feature from the constellation of visual features and learned associations that could be calculated elsewhere in a network, and direct and automatic extraction of this information in VTC may be more efficient and adaptive, as the authors acknowledge later in the Discussion. Are such computations &quot;visual&quot; or should VTC be characterized as something more than &quot;visual&quot;? The authors cite numerous papers that show the influence of nonvisual information on representation in VTC, and clearer incorporation of these facts in their reasoning could help here. Incidentally, the discussion of nonvisual activation of VTC should include a paper by Fairhall et al. (2017), who show representation in VTC of agentic properties conveyed by voices in the congenitally blind.</p><p>With regard to using a CNN as a measure of visual categorizablity, the authors write, &quot;Because this measure was based on a feedforward transformation of the images, we label this measure image categorizability.&quot; This statement may not be completely accurate. The training of a CNN uses millions of semantically-labeled images, meaning that semantics are incorporated in the development of feature spaces that afford view-invariant, lighting-invariant, and most importantly exemplar-invariant labeling of new images. Thus, characterizing this CNN as solely feedforward overlooks the fact that it is trained with semantic labels and produces semantic labels. This gray area between &quot;visual categorizability&quot; and semantic classification needs to be clarified. The current manuscript tries to make a binary distinction between visual categorizability and semantic judgment of agency, when in fact it is more complicated insofar as both the CNN and behavioral criteria for visual categorizability can be influenced by semantics – the CNN in how it is trained and the behavioral task in terms of the role that automatic activation of semantic features and categories may influence response times.</p><p>The reviewers suggest a rework of the Materials and methods section as they found it difficult to decipher what exactly had been done in the experiment and in the data analysis. Below is a list of clarifications or requests for missing information.</p><p>Clarifications regarding methods for using the CNN are also requested:</p><p>1) Please confirm/clarify that the network was not specifically fine-tuned for animal/non-animal categorization. An SVM is trained for animal vs. non-animal categorization on top of features extracted from a pre-trained CNN.</p><p>2) More generally, when using a CNN to get a visual categorizability score per image, the choice of the final layer as &quot;feature representation&quot; is somewhat odd because units are already category selective and responses across category units tend to be very sparse. For these kinds of transfer learning tasks, people normally consider layers below. Prior work (see Eberhardt et al., 2016) has shown that the best correlation between layer outputs and human decisions for rapid animal/non-animal categorization was actually in the higher convolutional layers. Please comment.</p><p>3) In the statement: &quot;Training accuracies were quantified using 6-fold cross-validation. The training accuracy was 92.2%.&quot; The authors probably meant &quot;average test or validation accuracy&quot;, right (i.e., by training using 5 folds and testing on the remaining one and averaging over 6 rounds)? Please confirm.</p><p>4) The image categorizability of an object was defined as the distance to the representation of that object from the decision boundary of the trained classifier. Please confirm that this is distance when the image is used as a test.</p><p>5) There is a methodological difference in the way categorizability scores are derived for CNNs and humans. Why? It would have been better to use the cross-validation method for both but obviously, there are constraints on how perceptual measures can be derived. Why not then use the same method for CNNs as for the human experiment of categorizability?</p><p>Clarifications regarding human experiments:</p><p>1) Which classifier was used for the animacy decoding?</p><p>2) What is the &quot;animacy continuum&quot;? Is it the unthresholded decision value of the classifier that was trained on the localizer run?</p><p>3) The authors introduced two separate measures of categorizability, which turn out to be correlated. How do these two separate measures of categorizability interact with animacy?</p><p>4) The linear relationship between animacy, agency, and categorizability could be investigated solely based on behavioral data. What was the scientific contribution that we obtained from the fMRI data? There is no question that there is one, but we would like to see this worked out better.</p><p>5) There is no statement about informed consent for the psychophysics experiments.</p><p>6) The image used differ across experiments: sometimes images are grayscale, sometimes color and sometimes unspecified. Please comment.</p><p>7) The subsection about &quot;Comparing visual categorizability with the animacy continua&quot; is confusing. As written, the statistical analysis tests whether the model correlation with the animacy score is <italic>lower</italic> than that of the human to human correlation. If the corresponding p-value is &lt;.05 as stated, then the model is indeed lower than the noise ceiling not at the noise ceiling? Please clarify. At the very least the wording should be such as to remove any source of ambiguity about the null hypothesis etc.</p><p>8) It is said that &quot;Animacy classifiers were trained on the BOLD images obtained from the animate and inanimate blocks of the localizer experiment and tested on the BOLD images obtained from the main experiment.&quot; How do you then get a p-value for significance for above chance classification for each sphere as reported in the subsection “Searchlight details”?</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47142.021</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The reviewers suggest further analyses and figures to clarify how the representation of visual categorization and of agency are structured. Is the animacy continuum based both on visual features and agentic properties, or are there two animacy continua or a gradient reflecting a transition from a representation of visual information that lays the foundation and a representation of agency? Should the term &quot;animacy continuum&quot; refer to an amalgam of visual features and agency, or should it be reserved for semantic content?</p></disp-quote><p>Thanks for raising these intriguing questions. In terms of terminology, we use “animacy continuum” to refer to the continuum observed in the representational distances from the decision boundary of the animate-inanimate classifier in VTC. As we (and others before us) show, not all animals are equally animate when considering SVM decision scores – some animals (e.g., mammals and humans) are very strongly animate while others (e.g., insects and reptiles) are weakly animate. As we outline in the Introduction of the paper, this continuum in visual cortex might be explained by differences in animate-diagnostic features, agency, or both. Our results show that the animacy continuum in VTC is explained by both visual features and agency. In a new analysis (Figure 5B), we confirm that these two factors contribute at different locations in the visual system. This is consistent with a gradient reflecting a transformation from visual features to agency. However, it is also consistent with the existence of two separate continua that appear as gradients due to overlap or cross-participant averaging. We have added these considerations to the Discussion: “The finding that visual categorizability and agency expressed in different parts of VTC is consistent with multiple scenarios. One possibility is that VTC contains two distinct representations of animals, one representing diagnostic visual features and one representing perceived agency. Alternatively, VTC may contain a gradient from a more visual to a more conceptual representations of animacy, with the visual representation gradually being transformed into a conceptual representation. More work is needed to distinguish between these possibilities.”</p><disp-quote content-type="editor-comment"><p>One suggestion includes a multidimensional scaling analysis similar to the analyses and figures of Connolly et al. (2016). In the original paper, the authors showed gradients of changes in representational geometry that (a) disentangled representation of animal taxonomy vs. visual appearance in VTC and (b) disentangled predacity from visual appearance and taxonomy along the STS. A similar method could be used to help clarify exactly how the animacy continuum in VTC reflects both visual categorizability and agency. They seem to be dissociable, and the anatomical and conceptual aspects of this dissociation could be explored and explicated more thoroughly.</p></disp-quote><p>We have added new analyses in response to these suggestions:</p><p>1) Figure 5B shows the contributions of visual categorizability and agency to the animacy continuum along the posterior-anterior axis. Confirming the searchlight maps, we observed a clear posterior-to-anterior transition.</p><p>2) We have added a principal component analysis on VTC patterns, similar to Connolly et al. (2016) to Figure 4 (Figure 4—figure supplement 1). This analysis, while not directly testing our hypothesis, visualizes the main dimensions of VTC representations more generally, as well as how these relate to VTC animacy.</p><disp-quote content-type="editor-comment"><p>In a similar vein, the authors pose the question &quot;How can such a seemingly high-level psychological property as agency affect responses in visual cortex?&quot; Why the need to assume VTC is purely visual? This assumption carries with it the assertion that semantic information must come from elsewhere in a large-scale network. It seems quite possible that IT cortex makes the same computations to extract this critical feature from the constellation of visual features and learned associations that could be calculated elsewhere in a network, and direct and automatic extraction of this information in VTC may be more efficient and adaptive, as the authors acknowledge later in the Discussion. Are such computations &quot;visual&quot; or should VTC be characterized as something more than &quot;visual&quot;? The authors cite numerous papers that show the influence of nonvisual information on representation in VTC, and clearer incorporation of these facts in their reasoning could help here. Incidentally, the discussion of nonvisual activation of VTC should include a paper by Fairhall et al. (2017), who show representation in VTC of agentic properties conveyed by voices in the congenitally blind.</p></disp-quote><p>We agree that it is possible that VTC itself is not exclusively visual. We now added this possibility to the cited paragraph, which now also includes a citation to the Fairhall et al. paper: “One possibility is that anterior parts of visual cortex are not exclusively visual and represent agency more abstractly, independent of input modality (Fairhall et al., 2017; van den Hurk et al., 2017). Alternatively, agency could modulate responses in visual cortex through feedback from downstream regions involved in social cognition.”</p><disp-quote content-type="editor-comment"><p>With regard to using a CNN as a measure of visual categorizablity, the authors write, &quot;Because this measure was based on a feedforward transformation of the images, we label this measure image categorizability.&quot; This statement may not be completely accurate. The training of a CNN uses millions of semantically-labeled images, meaning that semantics are incorporated in the development of feature spaces that afford view-invariant, lighting-invariant, and most importantly exemplar-invariant labeling of new images. Thus, characterizing this CNN as solely feedforward overlooks the fact that it is trained with semantic labels and produces semantic labels. This gray area between &quot;visual categorizability&quot; and semantic classification needs to be clarified. The current manuscript tries to make a binary distinction between visual categorizability and semantic judgment of agency, when in fact it is more complicated insofar as both the CNN and behavioral criteria for visual categorizability can be influenced by semantics – the CNN in how it is trained and the behavioral task in terms of the role that automatic activation of semantic features and categories may influence response times.</p></disp-quote><p>We agree that it is likely that there are semantic influences on our visual measures, if only because the CNN was trained with semantically-labeled images. We have changed the mentioned statement to: &quot;Because this measure was based on a feedforward transformation of the images which was not informed by inferred agentic properties of the objects (such as thoughtfulness), we label this measure image categorizability.” Importantly, in our work, we are interested in a specific part of semantic information – agency. VGG-16 is trained to classify images but is not provided with any kind of information about the agency of the objects in the images.</p><p>The behavioral task might also show some influence of semantic information. We now mention this in the text: “The neural representations the reaction times in this task rely on are not fully known, and might reflect information about inferred agency of the objects. As such, accounting for the contribution of perceptual categorizability in subsequent analyses provides a conservative estimate of the independent contribution of agency to neural representations in VTC.”</p><disp-quote content-type="editor-comment"><p>The reviewers suggest a rework of the Materials and methods section as they found it difficult to decipher what exactly had been done in the experiment and in the data analysis. Below is a list of clarifications or requests for missing information.</p><p>Clarifications regarding methods for using the CNN are also requested:</p><p>1) Please confirm/clarify that the network was not specifically fine-tuned for animal/non-animal categorization. An SVM is trained for animal vs. non-animal categorization on top of features extracted from a pre-trained CNN.</p></disp-quote><p>Yes, we trained an SVM on top of features of a pre-trained CNN. This is mentioned as follows: “The CNN was taken from the MatConvNet package (Vedaldi and Lenc, 2015) and was pre-trained on images from the ImageNet ILSVRC classification challenge (Russakovsky et al., 2015). Activations were extracted from the final fully-connected layer, prior to the softmax operation, for 960 colored images obtained from Kiani et al. (2007) of which 480 contain an animal or animal parts and the rest contain inanimate objects or their parts.”</p><disp-quote content-type="editor-comment"><p>2) More generally, when using a CNN to get a visual categorizability score per image, the choice of the final layer as &quot;feature representation&quot; is somewhat odd because units are already category selective and responses across category units tend to be very sparse. For these kinds of transfer learning tasks, people normally consider layers below. Prior work (see Eberhardt et al., 2016) has shown that the best correlation between layer outputs and human decisions for rapid animal/non-animal categorization was actually in the higher convolutional layers. Please comment.</p></disp-quote><p>We chose to focus on FC8 based on our previous (unpublished) work in which we observed that the neural representations in VTC correlated as highly as any other layer with the neural representations in FC8 of VGG-16, and that the animal/non-animal classification performance was one of the highest in FC8.</p><p>Following the reviewer’s suggestion, and motivated by Eberhardt et al., we ran the main analysis again using C5-2 features to compute image categorizability. Results are reported in Figure 4—figure supplement 3. As shown in the figure, results were highly similar when image categorizability (IC) was based on features of C5-2 rather than FC8. Specifically, the independent contributions of visual categorizability and agency to VTC animacy remained significant and the correlation between the combined model and VTC animacy was at VTC animacy noise ceiling.</p><p>We now also present the correlations between the image categorizability scores across all layers in Figure 1—figure supplement 1. This shows that the image categorizability in the fully connected layers (FC6-FC8) are highly similar. All the findings described in the paper are robust to a change in layer-selection among the fully connected layers.</p><disp-quote content-type="editor-comment"><p>3) In the statement: &quot;Training accuracies were quantified using 6-fold cross-validation. The training accuracy was 92.2%.&quot; The authors probably meant &quot;average test or validation accuracy&quot;, right (i.e., by training using 5 folds and testing on the remaining one and averaging over 6 rounds)? Please confirm.</p></disp-quote><p>Yes. We have changed the statement to: &quot;The average cross-validation accuracy was 92.2%&quot;.</p><disp-quote content-type="editor-comment"><p>4) The image categorizability of an object was defined as the distance to the representation of that object from the decision boundary of the trained classifier. Please confirm that this is distance when the image is used as a test.</p></disp-quote><p>Yes. To clarify, we have added this sentence to the corresponding Materials and methods section: “The stimuli used in the subsequent tasks, behavioral and fMRI, did not occur in this training set of 960 images.”</p><disp-quote content-type="editor-comment"><p>5) There is a methodological difference in the way categorizability scores are derived for CNNs and humans. Why? It would have been better to use the cross-validation method for both but obviously, there are constraints on how perceptual measures can be derived. Why not then use the same method for CNNs as for the human experiment of categorizability?</p></disp-quote><p>In principle, the method used for extracting image categorizability scores from the CNN is the most straightforward. As noted by the reviewer, that method could not be used to extract perceptual categorizability scores. In computing perceptual categorizability we obtained a measure of visual similarity between two given images. There are numerous metrics which could be used to obtain such a measure of visual similarity between the representations of two images from a CNN (e.g. Pearson correlation, Euclidean distance, Isomap distance). Which of such metrics is appropriate is not a trivial consideration. Rather than choosing an arbitrary metric, we chose to stick to the most straightforward approach for the CNN. It should be noted that image and perceptual categorizability correlated quite strongly, despite the differences in the methodology of these measures.</p><disp-quote content-type="editor-comment"><p>Clarifications regarding human experiments</p><p>1) Which classifier was used for the animacy decoding?</p></disp-quote><p>SVMs were used for animacy decoding, and this is now indicated in the sub-section “Obtaining the animacy continua from fMRI data”.</p><disp-quote content-type="editor-comment"><p>2) What is the &quot;animacy continuum&quot;? Is it the unthresholded decision value of the classifier that was trained on the localizer run?</p></disp-quote><p>Yes, as indicated by &quot;The degree of animacy of an object is given by the distance of its representation from the classifier decision boundary.&quot; in the sub-section “Obtaining the animacy continua from fMRI data”.</p><disp-quote content-type="editor-comment"><p>3) The authors introduced two separate measures of categorizability, which turn out to be correlated. How do these two separate measures of categorizability interact with animacy?</p></disp-quote><p>We have added two new figures (Figure 4—figure supplement 2; Figure 5—figure supplement 1) to show the separate contributions of visual and perceptual categorizability to animacy after regressing out agency.</p><disp-quote content-type="editor-comment"><p>4) The linear relationship between animacy, agency, and categorizability could be investigated solely based on behavioral data. What was the scientific contribution that we obtained from the fMRI data? There is no question that there is one, but we would like to see this worked out better.</p></disp-quote><p>Our study aimed to improve our understanding of the organizing principles of human ventral temporal cortex. Without the fMRI data we would not have had access to the animacy continuum in VTC. Behavioral experiments can be used to obtain estimates of how animate an object is for humans, and how visual or conceptual features contribute to this, but would not tell us how those estimates relate to the animacy continuum in VTC.</p><disp-quote content-type="editor-comment"><p>5) There is no statement about informed consent for the psychophysics experiments.</p></disp-quote><p>We have added informed consent statements for every experiment mentioned.</p><disp-quote content-type="editor-comment"><p>6) The image used differ across experiments: sometimes images are grayscale, sometimes color and sometimes unspecified. Please comment.</p></disp-quote><p>We now mention whether the images were colored or gray-scaled in the methods description of each experiment.</p><disp-quote content-type="editor-comment"><p>7) The subsection about &quot;Comparing visual categorizability with the animacy continua&quot; is confusing. As written, the statistical analysis tests whether the model correlation with the animacy score is lower than that of the human to human correlation. If the corresponding p-value is &lt;.05 as stated, then the model is indeed lower than the noise ceiling not at the noise ceiling? Please clarify. At the very least the wording should be such as to remove any source of ambiguity about the null hypothesis etc.</p></disp-quote><p>We have adjusted the wording: “To assess if a model or a combination of models explained all the variance in the animacy continuum across participants, for each participant we tested if the correlation between the model or the animacy continuum predicted by the combined model (in a leave-one-out one fashion as above) and the average of animacy continua of the other participants was lower than the correlation between the participant's animacy continuum and the average of animacy continua of the other participants. On the group level, if this one-sided test (see “Statistical tests in use&quot;) was not significant (<italic>p</italic> &gt; 0.05), we concluded that the correlation between the model or a combination of models hit the animacy continuum noise ceiling and thus explained all the variance in the animacy continuum across participants.”</p><disp-quote content-type="editor-comment"><p>8) It is said that &quot;Animacy classifiers were trained on the BOLD images obtained from the animate and inanimate blocks of the localizer experiment and tested on the BOLD images obtained from the main experiment.&quot; How do you then get a p-value for significance for above chance classification for each sphere as reported in the subsection “Searchlight details”?</p></disp-quote><p>The “Searchlight details” section has been updated to further clarify the methods. The p-value mentioned in the question is computed in a test of above-chance classification for each sphere <italic>across participants</italic>.</p></body></sub-article></article>