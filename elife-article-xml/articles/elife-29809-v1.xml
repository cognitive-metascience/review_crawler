<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">29809</article-id><article-id pub-id-type="doi">10.7554/eLife.29809</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Distinct spatial coordinate of visual and vestibular heading signals in macaque FEFsem and MSTd</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-91874"><name><surname>Yang</surname><given-names>Lihua</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-12056"><name><surname>Gu</surname><given-names>Yong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4437-8956</contrib-id><email>guyong@ion.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Key Laboratory of Primate Neurobiology, Institute of Neuroscience</institution><institution>CAS Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>University of Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-66060"><name><surname>Laurens</surname><given-names>Jean</given-names></name><role>Reviewing Editor</role><aff><institution>Baylor College of Medicine</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>14</day><month>11</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e29809</elocation-id><history><date date-type="received" iso-8601-date="2017-06-22"><day>22</day><month>06</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2017-11-03"><day>03</day><month>11</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Yang et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Yang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-29809-v1.pdf"/><related-object ext-link-type="url" xlink:href="https://elifesciences.org/articles/e29809v1"><date date-type="v1" iso-8601-date="2017-11-14"><day>14</day><month>11</month><year>2017</year></date></related-object><abstract><object-id pub-id-type="doi">10.7554/eLife.29809.001</object-id><p>Precise heading estimate requires integration of visual optic flow and vestibular inertial motion originating from distinct spatial coordinates (eye- and head-centered, respectively). To explore whether the two heading signals may share a common reference frame along the hierarchy of cortical stages, we explored two multisensory areas in macaques: the smooth pursuit area of the frontal eye field (FEFsem) closer to the motor side, and the dorsal portion of medial superior temporal area (MSTd) closer to the sensory side. In both areas, vestibular signals are head-centered, whereas visual signals are mainly eye-centered. However, visual signals in FEFsem are more shifted towards the head coordinate compared to MSTd. These results are robust being largely independent on: (1) smooth pursuit eye movement, (2) motion parallax cue, and (3) behavioral context for active heading estimation, indicating that the visual and vestibular heading signals may be represented in distinct spatial coordinate in sensory cortices.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>spatial coordinate</kwd><kwd>self-motion perception</kwd><kwd>vestibular</kwd><kwd>visual optic flow</kwd><kwd>heading</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>Rhesus Macaque</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31471048</award-id><principal-award-recipient><name><surname>Gu</surname><given-names>Yong</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>National Key R&amp;D Program of China</institution></institution-wrap></funding-source><award-id>2016YFC1306801</award-id><principal-award-recipient><name><surname>Gu</surname><given-names>Yong</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002367</institution-id><institution>Chinese Academy of Sciences</institution></institution-wrap></funding-source><award-id>Strategic Priority Research Program (XDB02010000)</award-id><principal-award-recipient><name><surname>Gu</surname><given-names>Yong</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>In both frontal and temporal–parietal visual cortices of macaque, distinct spatial coordinates of visual and vestibular signals are predominantly eye centered and head centered, respectively.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>To navigate effectively through the environment, we usually combine multiple sensory inputs to precisely estimate our direction of self-motion (i.e, heading). Two most powerful cues are the visual optic flow (<xref ref-type="bibr" rid="bib27">Gibson, 1950</xref>; <xref ref-type="bibr" rid="bib57">Warren, 2003</xref>) and the vestibular inertial motion signals. Numerous psychophysical studies have shown that human and nonhuman primates can improve heading perception by combining optic flow and vestibular cues in a statistically optimal or near optimal way (<xref ref-type="bibr" rid="bib52">Telford et al., 1995</xref>; <xref ref-type="bibr" rid="bib37">Ohmi, 1996</xref>; <xref ref-type="bibr" rid="bib33">Harris et al., 2000</xref>; <xref ref-type="bibr" rid="bib4">Bertin and Berthoz, 2004</xref>; <xref ref-type="bibr" rid="bib29">Gu et al., 2008</xref>; <xref ref-type="bibr" rid="bib12">Butler et al., 2010</xref>, <xref ref-type="bibr" rid="bib11">2011</xref>; <xref ref-type="bibr" rid="bib24">Fetsch et al., 2011</xref>; <xref ref-type="bibr" rid="bib10">Butler et al., 2015</xref>). However, a critical problem for the brain to combine cues is that the two heading signals originate from different spatial reference frames: visual signals arise from the retina and are represented in an eye-centered coordinate, whereas vestibular signals arise from the peripheral organs in the inner ears that are represented in a head-centered coordinate. In this case, the vision information about heading is often confounded by changes of the eye ball in orbits, for example, when subjects vary gaze eccentrically or pursue moving objects (<xref ref-type="bibr" rid="bib40">Royden et al., 1992</xref>; <xref ref-type="bibr" rid="bib43">Royden, 1994</xref>; <xref ref-type="bibr" rid="bib41">Royden et al., 1994</xref>; <xref ref-type="bibr" rid="bib56">Warren and Saunders, 1995</xref>; <xref ref-type="bibr" rid="bib3">Banks et al., 1996</xref>; <xref ref-type="bibr" rid="bib42">Royden and Hildreth, 1996</xref>; <xref ref-type="bibr" rid="bib19">Crowell et al., 1998</xref>). How the brain exactly compensates these eye movements to recover true heading and correctly integrate it with the vestibular cues is unclear.</p><p>An intuitive solution to effectively combine visual and vestibular heading cues is to transform them into a common reference frame (<xref ref-type="bibr" rid="bib48">Stein et al., 1993</xref>; <xref ref-type="bibr" rid="bib16">Cohen and Andersen, 2002</xref>). Nevertheless, so far this hypothesis has not been supported by neurophysiological findings in mid-stage sensory areas including the ventral intraparietal area (VIP) and the dorsal portion of the medial superior temporal area (MSTd) (<xref ref-type="bibr" rid="bib2">Avillac et al., 2005</xref>; <xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Takahashi et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2013</xref>). Specifically, the spatial reference frame remains largely separated in these areas such that the visual optic flow signals are mainly eye-centered and the vestibular signals are mainly head/body centered. One possibility is that the visual information might be further transformed to be more head-centered when propagated to higher-stage areas. Therefore, in the current study we targets on an area at a later stage in the dorsal visual pathway: the smooth pursuit area of the frontal eye field, that is FEFsem, which receives heavy inputs from MSTd and is closer to the motor side (<xref ref-type="bibr" rid="bib23">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="bib26">Fukushima, 2003</xref>; <xref ref-type="bibr" rid="bib35">Lynch and Tian, 2006</xref>). Similar to MSTd, this area also contains robust visual and vestibular signals that may contribute to heading estimation (<xref ref-type="bibr" rid="bib30">Gu et al., 2016</xref>).</p><p>Furthermore, there are a number of limitations of methodologies used in previous works that may have led to conflict conclusions from these works. First, some researchers have measured tuning curves in a limited heading range that could have confounded tuning shift versus gain change (<xref ref-type="bibr" rid="bib5">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib6">Bremmer et al., 1997a</xref>; <xref ref-type="bibr" rid="bib7">Bremmer et al., 1997b</xref>; <xref ref-type="bibr" rid="bib38">Page and Duffy, 1999</xref>; <xref ref-type="bibr" rid="bib44">Shenoy et al., 1999</xref>; <xref ref-type="bibr" rid="bib59">Zhang et al., 2004</xref>). Second, people have conducted smooth eye movement compensation experiments (<xref ref-type="bibr" rid="bib5">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib59">Zhang et al., 2004</xref>) that are not necessarily linked with spatial reference frame which should have been measured under different static eye positions (<xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Takahashi et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib49">Sunkara et al., 2015</xref>). Third, available depth cues such as motion parallax are not consistent across the above studies. Last but not least, people have measured visual tuning curves under passive viewing tasks (<xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Takahashi et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib49">Sunkara et al., 2015</xref>). In such a behavioral context, the visual signals are not obliged to be transformed towards a head-centered reference frame for heading judgments.</p><p>In the current study, we first measured complete visual and vestibular tuning curves from single neurons in FEFsem and MSTd while the animals varied static fixation positions under a passive viewing task. We then reexamined the results under different conditions including: (1) smooth pursuit eye movements, (2) motion parallax within the visual optic flow, and (3) active versus passive behavioral context for heading signals. Our results not only revealed the reference frame properties of visual and vestibular heading signals in FEFsem, but also provided new methodology for studying spatial coordinates of multisensory signals in other brain regions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Heading stimuli were delivered through a virtual reality apparatus (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) that allowed independent control of visual optic flow and vestibular inertial motion cues (see Materials and methods). Visual simulated self-motion, or the physical translation of the body was presented along 8 directions with 45° apart in the horizontal and sagittal planes (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). A preferred plane was then identified for each isolated neuron for the eccentric fixation task. During the task, monkeys maintained fixation at a central (0°) or eccentric target (20°) while experiencing the heading stimuli. The eccentric targets were either placed in the horizontal meridian for the preferred horizontal plane, or in the vertical meridian for the preferred sagittal plane (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). During fixation, the animals were head- and body-fixed within the experimental apparatus. Thus, we can distinguish an eye- versus head-centered spatial coordinate in the current study, but cannot distinguish a head- versus body-centered coordinate. For convenience, we simply used head-centered term throughout the text to represent a reference frame that could potentially be head-, body- or even world-centered. We then recorded from well isolated single neurons in the two areas of FEFsem and MSTd (<xref ref-type="fig" rid="fig1">Figure 1D,E</xref>).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.29809.002</object-id><label>Figure 1.</label><caption><title>Experimental setup and anatomical locations.</title><p>(<bold>A</bold>) Monkeys were trained to maintain fixation while seated in a virtual-reality setup. The apparatus consists of a 6-DOF motion platform that can translate in any direction. Visual display, monkey chair, and the field coil system are mounted on the motion base. Monkeys are head-fixed within the system. (<bold>B</bold>) Heading directions are varied in the horizontal and sagittal planes. (<bold>C</bold>) Eccentric fixation experimental paradigm. The fixation spot is presented at one of three locations: left (20°), center (0°), or right (20°) in the horizontal plane (filled origin square), or up (20°), center (0°), or down (20°) in the vertical plane (open yellow square). The open blue squares are fixation locations sometimes presented at ±30° or ±10° in an additional experiment. (<bold>D</bold>) Schematic illustration of the locations of the two cortical areas studied: FEFsem at the posterior portion of the arcuate sulcus, and MSTd at the posterior part of the superior temporal sulcus. (<bold>E</bold>) Coronal sections exhibiting the recording sites from the two monkeys (Monkey Z and M) in FEFsem (orange dots) and MSTd (blue dots). Note that all the recording sites are projected onto one single coronal plane, causing some points artificially appeared outside the region of interest (ROI). The white arrow in the first panel indicates the position of an electrode probe during MRI scanning. White arrows in the last two panels indicate the recording grid used to guide electrode penetrations in the current recording experiments.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig1-v1"/></fig><sec id="s2-1"><title>Spatial coordinate of visual and vestibular signals</title><p>We first measured the spatial reference frame of the visual and vestibular signals in FEFsem and MSTd under the eccentric fixation task while the animals passively experienced the heading stimuli. This part of experiment included 171 neurons significantly tuned to visual optic flow and 71 neurons significantly tuned to inertial motion in FEFsem. In MSTd, 99 and 54 neurons that were significantly tuned to optic flow and inertial motion, respectively, were included in the final dataset (<xref ref-type="fig" rid="fig1">Figure 1E</xref>).</p><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> shows a typical FEFsem neuron with tuning curves measured under three different eye positions. Qualitatively, this neuron’s tuning curve in the visual condition is systematically shifted as a function of the animal’s gaze (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, top panel), while is largely independent on the gaze in the vestibular condition (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, bottom panel). This pattern suggests that for this neuron, the visual signal is mainly eye-centered and the vestibular signal is mainly head-centered. To quantify this, we used two methods. The first method is to compute a displacement index (DI) based on the shift of the tuning curve that maximizes its correlation coefficient with the other tuning curves (see Materials and methods). A DI value of 1 means a complete shift of the tuning curve relative to the change in eye position (20°) and thus indicates an eye-centered reference frame. In contrast, a DI equal to 0 means zero-shift of tuning curves and indicates a head-centered reference frame. This example neuron has a DI value of 0.63 and 0.04 in the visual and vestibular condition, respectively, which is consistent with our intuition.</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.29809.003</object-id><label>Figure 2.</label><caption><title>Summary of spatial reference frames as quantified by DI for visual and vestibular heading tuning measured in the eccentric fixation protocol.</title><p>(<bold>A</bold>) Heading tuning functions of an example FEFsem neuron in the visual (left panel) and vestibular condition (right panel). Firing rate is plotted as a function of heading direction. Error bars are standard error of mean (s.e.m.). Different color curves represent tunings measured at different eye-in-orbit positions. (<bold>B, C</bold>) Distributions of DI measured under 20° eccentricity in FEFsem (<bold>B</bold>) and MSTd (<bold>C</bold>). DIs were limited in the range of [−1.5 2.5]. A few cases outside of this range were plotted at the edge of this range for the demo convenience. Black bars: head-centered coordinate; Magenta bars: eye-centered coordinate; Gray bars: intermediate coordinate; Open bars: unclassified coordinate. Arrowheads indicate the mean DI value of all the cases. Vertical dashed lines indicate head-centered (DI = 0) or eye-centered (DI = 1) coordinate.</p><p><supplementary-material id="fig2sdata1"><object-id pub-id-type="doi">10.7554/eLife.29809.007</object-id><label>Figure 2—source data 1.</label><caption><title>Raw data for <xref ref-type="fig" rid="fig2">Figure 2B</xref>.</title></caption><media mime-subtype="excel" mimetype="application" xlink:href="elife-29809-fig2-data1-v1.xls"/></supplementary-material></p><p><supplementary-material id="fig2sdata2"><object-id pub-id-type="doi">10.7554/eLife.29809.008</object-id><label>Figure 2—source data 2.</label><caption><title>Raw data for <xref ref-type="fig" rid="fig2">Figure 2C</xref>.</title></caption><media mime-subtype="excel" mimetype="application" xlink:href="elife-29809-fig2-data2-v1.xls"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig2-v1"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.29809.004</object-id><label>Figure 2—figure supplement 1.</label><caption><title>DI under different noise levels in hypothetical neurons.</title><p>(<bold>A</bold>) All neurons were assumed to be completely eye-centered, that is tuning shifted by 20° under eccentric fixation protocol. These tuning curves were injected with different levels of noise by Matlab function ‘awgn’. The smaller the signal to noise ratio (SNR, from left to right), the noisier the tuning curve was. For example, the top panels exhibit one trial of the responses sampled at a resolution of 1°. We repeated this sampling for five times and computed the mean firing rate at each azimuth at a resolution of 12° as the tuning curve (bottom panels), a process of which was similar to the real neural recording experiment. Error bars are standard error of mean. (<bold>B</bold>) Under each noise level, a population of 100 cells was created, and DI was computed in the exact same way as for the real neurons. It is clear that when noise is getting larger in the tuning curves, the DI values tend to be more broadly distributed, generating some seemingly head-centered units. However, the number of these units is small, and importantly these units are statistically defined as ‘unclassified’ based on the bootstrap test procedure. (<bold>C</bold>) The strength of directional tuning was quantified using a direction discrimination index (DDI, <xref ref-type="bibr" rid="bib50">Takahashi et al., 2007</xref>) given by: <inline-formula><mml:math id="inf1"> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mo>)</mml:mo></mml:msqrt></mml:mrow></mml:mfrac></mml:math></inline-formula>. DDI not only computes the peak to trough modulation (R<sub>max</sub>-R<sub>min</sub>), but also the response variability (SSE: sum squared error around the mean response; N: total number of trials; M: total number of stimulus directions). DDI ranges within [0 1], corresponding to weak and strong SNR, respectively. (D) DI is not significantly dependent on DDI (R = 0.11, p=0.3, Spearman rank correlation) for hypothetical neurons tested under SRN = 5.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig2-figsupp1-v1"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.29809.005</object-id><label>Figure 2—figure supplement 2.</label><caption><title>The relationship between DI and DDI in FEFsem (<bold>A, C</bold>) and MSTd (<bold>B, D</bold>).</title><p>(<bold>A, B</bold>) Overall DDI is slightly smaller in the vestibular condition than in the visual condition. However, DDIs are largely overlapped in the two conditions (along the vertical axis), which certainly cannot account for the separate spatial coordinate (defined by DI) in the two stimuli conditions. (<bold>C, D</bold>) Same format as in A, B for eccentric fixation versus smooth pursuit conditions. DDIs overall are similar and even slightly higher in the pursuit condition. Hence, the spatial coordinate defined from DI (together with bootstrap test) cannot be mainly due to signal-to-noise ratio of the tuning curves.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig2-figsupp2-v1"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.29809.006</object-id><label>Figure 2—figure supplement 3.</label><caption><title>DI measured under a broader range of eccentric fixation task by introducing two extra eccentricities of 10° and 30° in addition to the 20°.</title><p>(<bold>A</bold>) An example neuron with heading tuning curves measured under a broader range of eye-in-orbit positions. (<bold>B</bold>) DI measured under different eccentric fixation (10°, 20°, 30°) are analogous. Gray curves represent the DI values from each cell. Black symbols represent mean and s.e.m.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig2-figsupp3-v1"/></fig></fig-group><p>Across population in FEFsem (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>), the mean visual DI is 0.74 ± 0.05 (mean ± s.e.m.), which is significantly different from either 0 (p=1.3E-32, t-test) or 1 (p=6.5E-7, t-test), suggesting that the visual signal is intermediate but more biased toward the eye-centered reference frame. In contrast, the mean vestibular DI is 0.14 ± 0.05, which is only slightly different from 0 (p=0.01, t-test), suggesting that the vestibular signal is predominantly head-centered. Between stimuli conditions, the mean visual DI is significantly larger than the mean vestibular DI (p=2.2E-7, t-test). Thus, overall the visual and vestibular reference frames in FEFsem are apart from each other. In MSTd (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, <xref ref-type="supplementary-material" rid="fig2sdata2">Figure 2—source data 2</xref>), the vestibular signals are similar to those in FEFsem in that they are mainly head-centered: the mean vestibular DI is 0.17 ± 0.08 and is slightly larger than 0 (p=0.03, t-test). As to the visual signals, the mean DI is 0.95 ± 0.05, which is not significantly different from 1 (p=0.3, t-test). This value is significantly larger than that in FEFsem (p=0.005, t-test), suggesting that the visual optic flow signals in MSTd are even closer to an eye-centered coordinate than in FEFsem. Hence, in general, the reference frames of visual and vestibular signals in both areas are largely separated, yet in FEFsem, they are slightly closer to each other (Δ mean DI<sub>vestibular, visual</sub> = 0.60) than in MSTd (Δ mean DI<sub>vestibular, visual</sub> = 0.78).</p><p>Since overall the DI values were broadly distributed, we computed the statistical 95% confidence interval (CI) of each cell through a bootstrap procedure (see Method). According to the CIs, each cell was categorized into one of the following four groups: (1) head-centered: CIs include 0 but not 1; (2) eye-centered: CIs include 1 but not 0; (3) intermediate: CIs are between 0 and 1 without touching 0 and 1; (4) unclassified: CIs belong to none of the above three types (e.g. including both 0 and 1). The last group usually reflects large noise in the tuning curves, but it only occupies a minor population in our data (<xref ref-type="fig" rid="fig2">Figure 2B,C</xref>). For visual signals in FEFsem, more cases are defined as the eye-centered group (46.8%) whereas some cases are defined as head-centered (15.8%) and intermediate (24.6%) group. In MSTd, there are even more cases that are defined as eye-centered group (71.7%) and fewer cases defined as the head-centered (5.1%) or intermediate (14.1%) group. As to the vestibular signals, in both areas, majority of the cases are defined as head-centered group (FEFsem: 60.6%; MSTd: 59.3%), and very few cases are defined as eye-centered (FEFsem: 11.3%; MSTd: 11.1%) and intermediate group (FEFsem: 9.9%; MSTd: 16.7%). To further explore how noise in the tuning curves may affect the DI measurement, we first ran simulations by creating populations of hypothetical neurons with different noise level (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Our results show that large noise tends to broaden the DI distributions, but does not cause systematic bias toward a certain direction (e.g. head-centered). Indeed, in our real neuronal data, DI values are not significantly dependent on the noise level in the tuning curves (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Thus, these results further support our above conclusions: the largely separated eye- and head-centered spatial coordinate respectively for the visual and vestibular signals are robust in FEFsem and MSTd.</p><p>We further measured DI under a broader range of eccentric fixation task by introducing two extra eccentricities of 10° and 30° in addition to the 20° used in the above experiment (see one example in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3A</xref>). This allows us to examine whether spatial coordinate assessed by DI is dependent on the magnitude of the eccentric fixation amplitude, which has not been tested in previous works. Our result from a subpopulation of tested neurons (N = 38, data pooled across areas and stimuli conditions, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3B</xref>) clearly shows that the average DI is not significantly different among eye positions with different amplitudes (p&gt;0.3, t test), suggesting that using 20° of gaze eccentricity in our current study neither over- nor under-estimate the reference frames of cortical neurons.</p><p>The DI method conveniently gives intuition about the overall distribution of the spatial coordinate in each cortical area. To further examine how the tuning functions under different eccentric fixations for each individual cell can be best explained by the eye- versus head-centered models, we further employed a second method. Specifically, we simultaneously fit each neuron’s tuning curves with modified wrapped Gaussian functions under all three eye positions with an eye-centered (prefer directions shifted as the varied eye positions) and a head-centered model (prefer directions unchanged) (see Materials and methods). <xref ref-type="fig" rid="fig3">Figure 3A and B</xref> show the model fitting results for the same example neuron as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Clearly the eye-centered model fits the data better in the visual condition (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), while the head-centered model fits the vestibular data better (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). To quantify this, the goodness-of-fit of each model measured by the partial correlation coefficient between the fit and the data was Z-scored to categorize each neuron’s spatial coordinate (eye-centered versus head-centered, p&lt;0.05, dotted lines in <xref ref-type="fig" rid="fig3">Figure 3C,D</xref>).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.29809.009</object-id><label>Figure 3.</label><caption><title>Spatial reference frames as assessed by head- and eye-centered model fittings.</title><p>(<bold>A, B</bold>) Heading tuning functions from an example neuron in the visual (<bold>A</bold>) and vestibular (<bold>B</bold>) condition. Red, black and blue curves represent eccentric fixation at −20°, 0° and 20°, respectively. Superimposed green curves depict the fitting of eye-centered or head-centered models. Vertical green lines are the prefer directions of the fitting curves. (<bold>C, D</bold>) Eye- and head-centered model correlation coefficients (Z-transformed) in FEFsem (<bold>C</bold>) and MSTd (<bold>D</bold>). Filled symbols: vestibular; Open symbols: visual. Significance regions are based on the difference between eye- and head-centered Z scores corresponding to p&lt;0.05 (top left, eye centered; bottom right, head centered; central diagonal region, intermediate). The two asterisks, one open and one filled, represent the example cell in (<bold>A</bold>) and (<bold>B</bold>), respectively. Diagonal histograms represent distributions of differences between Z-scores for each pair of models (eye- minus head-centered). Arrowheads indicate the mean values of each distribution. Filled bars: vestibular; Open bars: visual.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig3-v1"/></fig><p>Across population in FEFsem (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), 36.3%, 7.0% and 56.7% neurons were classified as eye centered, head-centered and unclassified group in the visual condition, respectively. In the vestibular condition, 23.9%, 42.3%, 33.8% neurons were classified as eye-centered, head-centered and unclassified, respectively. The mean difference (eye - head) in Z-score between the two models is 0.10 and −1.28 in the visual and vestibular condition, respectively, which is significantly different from 0 in both cases (visual: p=0.0025, vestibular: p=2.0E-11, t-test). In MSTd, for the eye-centered, head-centered and unclassified category, there are 75.5%, 9.2% and 15.3% neurons respectively in the visual condition, and 24.1%, 61.1% and 14.8% neurons respectively in the vestibular conditions. The mean Z-score difference is 1.50 and −3.21 in the visual and vestibular condition respectively, both of which are significantly different from 0 (p=3.4E-4, p=1.4E-18,, t-test, <xref ref-type="fig" rid="fig3">Figure 3D</xref>). Hence, in line with the DI results, the model-fitting analysis also revealed that in both areas, generally there are more cases showing eye-centered reference frame for the visual signals, and more cases showing head-centered reference frame for the vestibular signals. However, compared between areas, the reference frames of the visual and vestibular signals are closer in FEFsem than in MSTd.</p><p>In the following, we further examined whether the spatial coordinates of the heading signals in FEFsem and MSTd were dependent on other factors including smooth pursuit eye movement, motion parallax cues in the visual optic flow, and the behavioral context for heading estimation.</p></sec><sec id="s2-2"><title>Smooth pursuit eye movement compensation</title><p>In addition to the static eye fixations varied at different eccentricities, other type of eye behavior, such as smooth pursuit is also frequently accompanied during spatial navigation which could potentially distort perceived flow field (<xref ref-type="bibr" rid="bib54">Warren and Hannon, 1988</xref>, <xref ref-type="bibr" rid="bib55">1990</xref>; <xref ref-type="bibr" rid="bib40">Royden et al., 1992</xref>; <xref ref-type="bibr" rid="bib3">Banks et al., 1996</xref>). In fact, some studies have used smooth pursuit paradigm to infer cortical neurons’ spatial coordinate (<xref ref-type="bibr" rid="bib5">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib38">Page and Duffy, 1999</xref>; <xref ref-type="bibr" rid="bib44">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib45">2002</xref>; <xref ref-type="bibr" rid="bib34">Ilg et al., 2004</xref>; <xref ref-type="bibr" rid="bib59">Zhang et al., 2004</xref>). However, recent studies argued that the more or less pursuit compensations as observed in these studies is not necessarily linked to spatial reference frames as measured under varied static eye positions during stimulus presentation (<xref ref-type="bibr" rid="bib49">Sunkara et al., 2015</xref>). To test this, for some of the neurons that have been recorded under the eccentric fixation protocol, we further ran a pursuit block (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, see Method). Briefly, the animals were required to pursue a moving target that was crossing the screen at a constant speed of 16°/s. During the steady pursuit, we presented visual optic flow stimuli that simulated real motion in the prefer plane as used in the eccentric fixation condition. Notice that in this protocol, the motion platform was always stationary such that there was no vestibular input to the animals.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.29809.010</object-id><label>Figure 4.</label><caption><title>Comparison of spatial reference frames with smooth pursuit eye movement compensation.</title><p>(<bold>A</bold>) Experimental paradigm for smooth pursuit eye movement. Monkeys were required to pursue a smooth moving target crossing the screen either from right to left (blue curve) or from left to right (red curve). These conditions were interleaved with a no-pursuit condition (central fixation, black curve). Gray curves are the raw eye traces from an example block. (<bold>B</bold>) Heading tuning curves from two hypothetical (top) and two example FEFsem (bottom) neurons under different pursuit conditions. Color is the same as in (<bold>A</bold>). (<bold>C</bold>) DI distributions under the pursuit protocol in FEFsem (upper panel) and MSTd (lower panel). Arrowheads indicate mean values. Vertical dashed lines indicate head-centered/complete compensation (DI = 0) or eye-centered/complete shift (DI = 1). Black bars: complete compensation for eye rotation; Magenta bars: complete shift from eye rotation; Gray bars: partial shift; Open bars: unclassified group. (<bold>D</bold>) Direct comparison of DIs between pursuit and eccentric fixation on a cell by cell basis. Each point represents one neuron. Circle: FEFsem, N = 36; Triangle: MSTd, N = 32.</p><p><supplementary-material id="fig4sdata1"><object-id pub-id-type="doi">10.7554/eLife.29809.012</object-id><label>Figure 4—source data 1.</label><caption><title>Raw data for <xref ref-type="fig" rid="fig4">Figure 4C</xref>.</title></caption><media mime-subtype="excel" mimetype="application" xlink:href="elife-29809-fig4-data1-v1.xls"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig4-v1"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.29809.011</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Direct comparison of DI between pursuit and eccentric fixation on a cell by cell basis.</title><p>Each point represents one neuron. DI for the pursuit protocol was computed in the same way, that is from the whole tuning curve at once as for the eccentric fixation data. Circle: FEFsem, N = 36; Triangle: MSTd, N = 32. DI under pursuit is significantly smaller compared to that under the eccentric fixation task (FEFsem: p=1.9E-4; MSTd: p=6.4E-8, paired t-test), and they are not significantly correlated with each other (p&gt;0.5, Spearman rank correlation). The same data set as <xref ref-type="fig" rid="fig4">Figure 4D</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig4-figsupp1-v1"/></fig></fig-group><p><xref ref-type="fig" rid="fig4">Figure 4B</xref> shows two hypothetical (top panels) and two examples of real FEFsem (bottom panels) units tested under the pursuit protocol. Unlike the eccentric fixation protocol with unchanged gaze direction across the whole stimulus duration, continuous rotation of the eyes during pursuit causes shifts in tuning curves in a more complex way that is dependent on the heading preference of the neurons (<xref ref-type="bibr" rid="bib49">Sunkara et al., 2015</xref>). Specifically, for neurons with lateral heading preference (0°/180°, top left panel in <xref ref-type="fig" rid="fig4">Figure 4B</xref>), tuning peak and trough remain unchanged. Instead, responses at the two sides of the tuning peak are shifted in opposite directions, causing the overall bandwidth increases or decreases for leftward or rightward pursuit, respectively. For neurons with forward/backward heading preference (90°/270°, top right panel in <xref ref-type="fig" rid="fig4">Figure 4B</xref>), the tuning peak and trough are shifted in opposite directions, also causing a change of the tuning width. Thus for each neuron, shift in the tuning curve was first computed separately from two parts: one between [0° 180°], and the other between [180° 360°], leading to a total of 4 values under the two pursuit directions. These values were then averaged to compute a single displacement index (DI) under the pursuit protocol, in a similar way as for the eccentric fixation task (see detail in Data Analysis). In this case, a DI value of 1 means complete tuning shift and implies that the neuron responds to resultant optic flow due to eye rotations. On the contrary, a DI value of 0 means unchanged tuning, and implies complete eye rotation compensation for representing true headings.</p><p><xref ref-type="fig" rid="fig4">Figure 4C</xref> summarizes the results for 54 FEFsem neurons and 51 MSTd neurons (<xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>). The mean DI under pursuit is 0.24 ± 0.08 and 0.31 ± 0.10 in FEFsem and MSTd, respectively, both of which are substantially smaller than 1 (p=3.5E-13, p=3.0E-9, t-test) and slightly larger than 0 (p=0.002, p=0.002, t-test). In both areas, majority of the cells were classified as complete (FEFsem: 61.1%; MSTd: 39.2%) or intermediate compensation (FEFsem: 9.3%; MSTd: 35.3%). In contrast, there are very few cells exhibiting complete tuning shift (FEFsem: 5.6%; MSTd: 11.8%). This result suggests that on average, both FEFsem and MSTd do not represent resultant optic flow under pursuit, but rather signal heading in a manner that is fairly tolerant to eye rotations.</p><p>Such a pattern is in sharp contrast to the spatial reference frame results under varied static eye positions (<xref ref-type="fig" rid="fig2">Figure 2B,C</xref>, left panel), suggesting that the two metrics are unlikely to be linked with each other. Indeed, a direct comparison on a cell by cell basis (<xref ref-type="fig" rid="fig4">Figure 4D</xref>) indicates that DI under pursuit is significantly smaller compared to that under the eccentric fixation task (FEFsem: p=1.1E-4; MSTd: p=7.9E-5, paired t-test), and they are not significantly correlated with each other (FEFsem: p=0.10; MSTd: p=0.82 Spearman rank correlation). This conclusion holds even when we use identical method to compute DI values as for the eccentric fixation data (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Thus, even complete eye rotation compensation for simulated heading observed in cortical neurons (e.g. FEFsem and MSTd) does not necessarily imply a head-centered spatial reference frame.</p></sec><sec id="s2-3"><title>Motion parallax cue in the visual optic flow</title><p>Depth cue such as motion parallax plays an important role in deciphering self-motion based on the depth structure of a visual scene (<xref ref-type="bibr" rid="bib27">Gibson, 1950</xref>; <xref ref-type="bibr" rid="bib53">von Helmholtz, 1963</xref>), yet this cue has not been included consistently across previous studies. In our above experiment, the visual optic flow has contained motion parallax cue. Thus in this section, we excluded this cue to test whether it might be a key to affecting the neurons’ tuning shift under the eccentric fixation task (<xref ref-type="fig" rid="fig5">Figure 5A–C</xref>) or the smooth pursuit protocol (<xref ref-type="fig" rid="fig5">Figure 5D–F</xref>).</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.29809.013</object-id><label>Figure 5.</label><caption><title>Influence of motion parallax on spatial reference frame (<bold>A–C</bold>) and pursuit compensation (<bold>D–F</bold>) measurements.</title><p>(<bold>A</bold>) Visual tuning functions of an example FEFsem neuron with and without motion parallax in the optic flow. Red, black and blue curves represent eccentric fixation at −20°, 0° and 20°, respectively. (<bold>B</bold>) Distribution of DI without motion parallax cue in FEFsem (upper panel, N = 38) and MSTd (bottom panel, N = 34). Black bars: head-centered coordinate; Magenta bars: eye-centered coordinate; Gray bars: intermediate coordinate; Open bars: unclassified coordinate. Arrowheads indicate the mean value. (<bold>C</bold>) Direct comparison of DIs between with and without motion parallax cues on a cell by cell basis. Each symbol represents a neuron. Circle: FEFsem, N = 27; Triangle: MSTd, N = 17. (<bold>D–F</bold>) Same format as in (<bold>A–C</bold>) but for the pursuit protocol. Red, black and blue curves represent rightward, fixation only (no-pursuit) and leftward pursuit, respectively. In (<bold>E</bold>), for FEFsem, N = 39; for MSTd, N = 38; In (<bold>F</bold>), for FEFsem, N = 24; for MSTd, N = 29.</p><p><supplementary-material id="fig5sdata1"><object-id pub-id-type="doi">10.7554/eLife.29809.014</object-id><label>Figure 5—source data 1.</label><caption><title>Raw data for <xref ref-type="fig" rid="fig5">Figure 5B</xref>.</title></caption><media mime-subtype="excel" mimetype="application" xlink:href="elife-29809-fig5-data1-v1.xls"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><object-id pub-id-type="doi">10.7554/eLife.29809.015</object-id><label>Figure 5—source data 2.</label><caption><title>Raw data for <xref ref-type="fig" rid="fig5">Figure 5E</xref>.</title></caption><media mime-subtype="excel" mimetype="application" xlink:href="elife-29809-fig5-data2-v1.xls"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig5-v1"/></fig><p><xref ref-type="fig" rid="fig5">Figure 5A</xref> shows one typical neuron tested with and without motion parallax under the eccentric fixation protocol. Qualitatively the response pattern is similar under the two experimental conditions. This is also reflected in the population: the average DI is 0.80 ± 0.07 and 0.92 ± 0.06 in FEFsem and MSTd, respectively (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>). In addition, majority of the cells are within the eye-centered category in FEFsem (60.5%) and MSTd (61.8%), whereas few cells are head-centered (FEFsem: 10.5%; MSTd: 2.9%) or intermediate (FEFsem: 15.8%; MSTd: 29.4%) reference frames. Such a result is very close to that under the condition with motion parallax cue (<xref ref-type="fig" rid="fig2">Figure 2B,C</xref>). When compared on a cell by cell basis (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), DIs with and without motion parallax are highly correlated (R = 0.70, p=1.5E-7, Spearman rank correlation), and their means are not significantly different from each other (FEFsem: p=0.37; MSTd: p=0.29, paired t-test).</p><p>Under the pursuit protocol, excluding motion parallax slightly affect the neuronal responses (<xref ref-type="fig" rid="fig5">Figure 5D–F</xref>). The average DI is 0.26 ± 0.10 and 0.45 ± 0.14 in FEFsem and MSTd, respectively (<xref ref-type="fig" rid="fig5">Figure 5E</xref>, <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>). The proportion of complete compensation, complete shift, and intermediate cells is 48.7%, 7.7% and 33.3%, respectively, in FEFsem, and 31.6%, 23.7% and 23.7%, respectively, in MSTd. Again, these results are similar to those under the condition with motion parallax cues (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Compared on a cell by cell basis (<xref ref-type="fig" rid="fig5">Figure 5F</xref>), DIs with and without motion parallax are significantly correlated (FEFsem: R = 0.68, p=2.9E-4, MSTd: R = 0.74, p=4.4E-6, Spearman rank correlation), and the mean DI under the no-motion parallax cue condition is slightly but significantly larger than that under the motion parallax contion (FEFsem: p=0.01; MSTd: p=0.07, paired t-test). Thus, similar to the effect in the eccentric fixation protocol, excluding motion parallax from the visual stimuli has limited effect on FEFsem and MSTd’s tolerance of the eye rotations.</p></sec><sec id="s2-4"><title>Behavioral context for heading estimation</title><p>In previous studies, spatial reference frames have been measured under the condition in which the animals passively experienced heading stimuli during eccentric fixations (<xref ref-type="bibr" rid="bib2">Avillac et al., 2005</xref>; <xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Takahashi et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Fan et al., 2015</xref>). Thus, it is possible that the predominant eye-centered reference frame of the visual optic flow may have been overestimated, because under this behavioral context, the brain may not use these signals for heading judgment based on a head coordinate. To test this hypothesis, we introduced an active behavioral paradigm in the current study (see Materials and methods). Briefly, the animals were trained to perform a heading estimation task in which they were required to report perceived headings simulated from optic flow by making oculomotor responses from center to a peripheral ring that appeared at the end of each trial (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The tricky part in this task was that the headings were varied in the whole horizontal plane, while the ring was presented on the frontal parallel plane. As a result, the animals needed to correctly associate the headings with their oculomotor responses: left/right headings correspond to left/right saccade, and importantly, forward/backward headings correspond to upward/downward saccade. In each trial, the center of the presented ring was always aligned with the fixation location during heading presentation.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.29809.016</object-id><label>Figure 6.</label><caption><title>Heading estimation task in the visual condition.</title><p>(<bold>A</bold>) Schematic illustration of the heading estimation task. The four panels depict the event sequence in one trial. Each trial begins with one of the three fixation locations (−20°, 0°, or 20°). After capturing fixation, visual optic flow is provided simulating heading in the horizontal plane. Headings are varied with a resolution of 20°, spanning a full 360° range. At the end of the trial, a target ring appears for ocular motor response. The ring is made of 36 dots apart by 10°. It is 20° in diameter, and its center is aligned with the fixation location in each trial (−20°, 0°, or 20°). Saccade endpoints within a window of 5 × 5° are taken as correct choice and the animals will be rewarded. Red, black and blue symbols represent eccentric fixation at −20°, 0° and 20°, respectively, and are the same for the rest of the figure. (<bold>B</bold>) Behavioral data from one experimental session with 30 repetitions for each stimulus condition. Note that saccade endpoints are plotted relative to the center of the target ring instead of the visual screen, thus data from the three fixation locations are roughly overlapped in this plot. (<bold>C</bold>) Mean and circular SD of the monkey’s heading estimates are plotted as a function of the real heading direction. The green diagonal line represents perfect performance. (<bold>D</bold>) Performance of the two monkeys in the training (unfilled areas) and recording sessions (shaded areas). Heading estimation errors are evaluated by computing the difference in the heading estimate between the eccentric and central fixation conditions. Right histograms represent the mean ± s.e.m. of the data in the recording sessions (shaded area).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig6-v1"/></fig><p><xref ref-type="fig" rid="fig6">Figure 6B,C</xref> show one monkey’s performance under three eccentric fixation conditions (−20°, 0° and 20°) in one experimental session. Compared to the heading stimuli, the animal showed more or less estimation error in a manner qualitatively similar to human being’s performance (<xref ref-type="bibr" rid="bib17">Crane, 2015</xref>, <xref ref-type="bibr" rid="bib18">2017</xref>). Notice though, across the whole training process, the estimation errors were large (~20°) at the early phase (<xref ref-type="fig" rid="fig6">Figure 6D</xref>), suggesting the animals judged headings simulated from optic flow relative to their eye positions. After trained with feedback signals of reward for a week or two, this error was reduced to a few degrees, indicating that the animals learned to judge headings largely based on a head-centered coordinate (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). We then started collecting neural data from the two monkeys after their behavioral performance reached a plateau. Specifically, across the whole recording period, the average heading estimation error was only less than ~20% (3.6 ± 0.52° and 4.3 ± 0.56°) in monkey Z, and less than ~10% (0.4 ± 0.62° and 2.2 ± 0.87°) in monkey M (marginal bar graphs corresponding to the shaded area in <xref ref-type="fig" rid="fig6">Figure 6D</xref>).</p><p>For each neuron, we collected data under two blocks: passive viewing condition (i.e. fixation only) and active estimation condition (i.e. oculormotor response). We first compared the visual spatial coordinates under the fixation only condition before and after the animals were trained with the estimation task (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, <xref ref-type="supplementary-material" rid="fig7sdata1">Figure 7—source data 1</xref>). Interestingly, we found after training, there was a tendency that the mean DI became smaller in both FEFsem (after: 0.58 ± 0.07, before: 0.74 ± 0.05, p=0.08, t-test) and MSTd (after: 0.67 ± 0.04, before: 0.95 ± 0.05, p=1.6E-5, t-test). Compared between the two areas, the training effect seems to be more obvious in MSTd: the population of the eye-centered cells was reduced by about 40% (after: 26.5%, before: 71.7%) and the intermediate population was increased by about 50% (after: 66.2%, before: 14.1%). By contrast in FEFsem, after training, the eye-centered population remained almost the same (after: 41.5%, before: 46.8%) while the intermediate population was increased by about 15% (after: 40.0%, before: 24.6%). Notice though, the degree of this coordinate shift in MSTd resulted from training is much limited such that the visual signals are still largely separated from the head-centered coordinate.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.29809.017</object-id><label>Figure 7.</label><caption><title>Spatial reference frame of cortical neurons during heading estimation task.</title><p>(<bold>A</bold>) DI distributions post training of heading estimation task in FEFsem (upper panel: N = 65) and MSTd (bottom panel: N = 68). Black bars: head-centered coordinate; Magenta bars: eye-centered coordinate; Gray bars: intermediate coordinate; Open bars: unclassified coordinate. Arrowheads indicate mean values. Vertical dashed lines indicate head- (DI = 0) or eye-centered (DI = 1) coordinate. (<bold>B</bold>) Tuning functions of an example FEFsem neuron during the fixation only (post training) and active estimation conditions. (<bold>C</bold>) Comparison of DIs between fixation only and active estimation conditions on a cell by cell basis. Circle: FEFsem, N = 36; Triangle: MSTd, N = 35.</p><p><supplementary-material id="fig7sdata1"><object-id pub-id-type="doi">10.7554/eLife.29809.018</object-id><label>Figure 7—source data 1.</label><caption><title>Raw data for <xref ref-type="fig" rid="fig7">Figure 7A</xref>.</title></caption><media mime-subtype="excel" mimetype="application" xlink:href="elife-29809-fig7-data1-v1.xls"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig7-v1"/></fig><p>We next assessed and compared whether after training, the visual reference frame would be different under the passive viewing and the active estimation conditions (<xref ref-type="fig" rid="fig7">Figure 7B,C</xref>). For example, <xref ref-type="fig" rid="fig7">Figure 7B</xref> shows one example neuron exhibiting similar response patterns under the two behavioral conditions (passive viewing: DI = 0.63; active estimation: DI = 0.64). This pattern also holds across population on a cell by cell basis (<xref ref-type="fig" rid="fig7">Figure 7C</xref>): DIs are highly correlated (FEFsem: R = 0.81, p=2.3E-9; MSTd: R = 0.77, p=7.3E-8, Spearman rank correlation) and their means are not significantly different from each other (FEFsem: p=0.59; MSTd: p=0.55, paired t-test). Hence, after training, the spatial coordinates of the visual optic flow in both MSTd and FEFsem are not significantly affected by the behavioral context.</p></sec><sec id="s2-5"><title>Gain modulation</title><p>So far our results have suggested that the multisensory heading signals may not share a common reference frame. Then how does the brain combine the visual and vestibular signals to represent heading that is based on a head reference during spatial navigation? One possibility is that the brain may employ a ‘gain field’ mechanism for coordinate transformations as proposed in previous computational studies (<xref ref-type="bibr" rid="bib60">Zipser and Andersen, 1988</xref>; <xref ref-type="bibr" rid="bib58">Xing and Andersen, 2000</xref>). For example, MSTd units’ activities under optic flow are modulated by different eye positions, and these eye position signals, together with the eye-centered visual responses, could be combined by downstream neurons to implement coordinate transformation, leading to a head-centered heading representation (<xref ref-type="bibr" rid="bib31">Gu et al., 2006</xref>). In this section, we further examine the gain field property in FEFsem and compare it with that in MSTd.</p><p>To quantify the gain field, we computed the difference in the maximum evoked response at different eye positions. The maximum evoked response in each tuning curve was the maximum mean firing rate subtracted by the minimum activity. Thus, any difference in this metric across eccentric fixation conditions would mainly reflect a gain modulation effect rather than an additive eye position effect. <xref ref-type="fig" rid="fig8">Figure 8A–D</xref> show four example neurons with significant gain modulations. The first two neurons’ activities are monotonically increased or decreased as a function of eye positions (<xref ref-type="fig" rid="fig8">Figure 8A,B</xref>), and are thus defined as ‘monotonic’ group. In contrast, the second two neurons show increased (<xref ref-type="fig" rid="fig8">Figure 8C</xref>) or decreased (<xref ref-type="fig" rid="fig8">Figure 8D</xref>) activities at central fixation compared to the responses at eccentric eye positions. These neurons are defined as ‘non-monotonic’ group.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.29809.019</object-id><label>Figure 8.</label><caption><title>Gain modulations in FEFsem and MSTd.</title><p>(<bold>A–D</bold>) Four example units with significant gain modulations across fixation locations. A and B show two cases with monotonic gain effect. In C and D, the effect is not monotonic, but rather that the responses are either strongest or weakest during central fixation. Tuning curves are the evoked responses with the minimum mean firing rate subtracted. Red, black and blue symbols represent eccentric fixation at −20°, 0° and 20°, respectively, and are the same for the rest of the figure. (<bold>E, F</bold>) Proportion of different category of neurons in the visual and vestibular conditions in FEFsem (<bold>E</bold>) and MSTd (<bold>F</bold>). Red: monotonic group; Black: non-monotonic group; Blue: no-gain modulations.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-29809-fig8-v1"/></fig><p>In FEFsem, overall there are 23.9% neurons showing significant gain modulation in the visual condition (<xref ref-type="fig" rid="fig8">Figure 8E</xref>). Among them, 9.2% and 14.7% neurons belong to the ‘non-monotonic’ and ‘monotonic’ category, respectively. This pattern is similar to that in MSTd: 40.2% neurons show significant gain modulation, with 16.5% and 23.7% neurons in the ‘non-monotonic’ and ‘monotonic’ category, respectively. Notice that these results are acquired under only three eccentric eye positions varied in one axis. Thus the proportion of cells with significant gain modulation, especially for the monotonic category, might have been underestimated. Indeed, in a previous study when the eye positions were varied in multiple points in a two dimensional plane, more than 80% neurons in MST were found to be modulated by eye positions, and among them, majorities exhibited a ‘monotonic’ effect (<xref ref-type="bibr" rid="bib7">Bremmer et al., 1997b1997b</xref>).</p><p>In any case, less than half of the neurons in both FEFsem and MSTd contain gain fields that could potentially be used by downstream neurons to transform the eye-centered visual signals into a head- or medial coordinate. Similarly, in the vestibular condition, there are also a number of neurons with gain fields, although its proportion is relatively smaller compared to the visual signals (FEFsem: 14.3%; MSTd: 14.8%). Hence, downstream neurons could also potentially use these signals to transform the vestibular signals into a mediate coordinate that better match the reference of visual signals.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In the current study, we measured spatial coordinate of visual and vestibular heading signals in two cortical areas of FEFsem and MSTd. Compared to MSTd which is mainly a mid-stage sensory area, FEFsem is at a later stage along the dorsal visual pathway and is closer to the motor side (<xref ref-type="bibr" rid="bib23">Felleman and Van Essen, 1991</xref>). Consistent with this notion, we discover that the visual reference frame in FEFsem is significantly skewed towards the head centered coordinate by roughly 20% compared to that in MSTd. However, the average visual DI in FEFsem (0.74) is still largely separated from the vestibular DI (0.14), suggesting the spatial coordinates of the two heading signals in FEFsem are distinct. This result is robust as the measured visual coordinate is independent on a number of factors including smooth pursuit eye movements and motion parallax cue in the optic flow. Interestingly, training the animals to judge heading directions relative to their head slightly shifts the reference frame of the visual signals towards the head centered coordinate in both areas. However, after training, the reference frames of the two heading signals remain fairly separated under both passive and active behavioral contexts. Hence, neither MSTd nor FEFsem has fully completed coordinate transformations of visual and vestibular signals for multisensory heading perception. Other sensory regions need to be explored in future experiments, but with proper methods including complete tuning function measures under active behavioral performance.</p><sec id="s3-1"><title>Factors confounding spatial coordinate measures</title><p>The visual optic flow is generated due to self-moving in the environment, and the focus of expanding flow (FOE) has a zero velocity that can be used to estimate the translation direction of the body, that is, heading (<xref ref-type="bibr" rid="bib51">Tanaka et al., 1986</xref>; <xref ref-type="bibr" rid="bib54">Warren and Hannon, 1988</xref>; <xref ref-type="bibr" rid="bib21">Duffy and Wurtz, 1995</xref>; <xref ref-type="bibr" rid="bib9">Britten, 2008</xref>). However, this information is based on the retina, and it is often confounded by different eye behavior during spatial navigation. For example, fixating at an eccentric target during forward moving will shift the FOE on the retina and subsequently may cause bias in heading estimate when solely relying on the visual information. In laboratory, researchers have designed experimental paradigms by varying eye positions while measuring heading performance in human subjects (<xref ref-type="bibr" rid="bib17">Crane, 2015</xref>, <xref ref-type="bibr" rid="bib18">2017</xref>), or neuronal tuning functions in animals (<xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib49">Sunkara et al., 2015</xref>). On the other hand, some researchers have adopted smooth pursuit eye movement that is also frequently accompanied during spatial navigation and could distort the optical flow field in either human psychophysical studies (<xref ref-type="bibr" rid="bib54">Warren and Hannon, 1988</xref>; <xref ref-type="bibr" rid="bib55">1990</xref>; <xref ref-type="bibr" rid="bib40">Royden et al., 1992</xref>; <xref ref-type="bibr" rid="bib3">Banks et al., 1996</xref>) or neurophysiological studies on monkeys (<xref ref-type="bibr" rid="bib5">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib38">Page and Duffy, 1999</xref>; <xref ref-type="bibr" rid="bib44">Shenoy et al., 1999</xref>; <xref ref-type="bibr" rid="bib34">Ilg et al., 2004</xref>; <xref ref-type="bibr" rid="bib59">Zhang et al., 2004</xref>). In general, these works reported that both behavior and neurons could compensate for smooth pursuit eye movements and recover true heading simulated from optic flow, implying a head-centered spatial reference frame.</p><p>Although the above two types of eye behavior share similar properties such as varied gaze, they dramatically differ in several aspects. First, on the behavioral level, the eccentric fixation protocol has a fixed eccentric gaze across the stimulus duration, and its impact on the shift of FOE projected on the retina is solely determined by and equal to the magnitude of the gaze eccentricity. By contrast in the pursuit condition, the gaze direction is continuously changed across the stimulus duration, and its impact on the resultant optic flow is not related with the momentary eye positions, but rather a number of other factors including pursuit speed, flow speed and flow depth (<xref ref-type="bibr" rid="bib59">Zhang et al., 2004</xref>). Second, on the neuronal level, tuning curves are simply shifted consistently in one direction if they represent a predominant eye-centered spatial coordinate in the eccentric fixation protocol. Instead in the pursuit condition, tuning curves in the whole plane (instead of in a limited heading range) are expected to shift in a more complex way if they represent the resultant optic flow on the retina.</p><p>In our current work, for the first time to our knowledge, we have measured tuning curves from the same population of neurons in both eccentric fixation and smooth pursuit protocols. We discover that there is no significant relationship between the two metrics computed from the two experimental conditions. Our results demonstrate that full or near full eye rotation compensation does not have to imply a head centered spatial reference frame. In another word, the pursuit compensations are not necessarily linked to reference frames as measured under the varied static eye positions, which is consistent with the opinion proposed in a recent study (<xref ref-type="bibr" rid="bib49">Sunkara et al., 2015</xref>).</p><p>In addition to the type of different eye behavior, the depth cue of the motion parallax has also been used inconsistently in previous studies. In our current work, we have measured the tuning curves in both with and without motion parallax conditions. Our results indicate that motion parallax does not affect the reference frame measures. However, there is a weak yet statistically significant effect in the smooth pursuit compensation. This effect is sort of expected since the induced FOE shift under pursuit is also determined by the depth of the optic flow (<xref ref-type="bibr" rid="bib59">Zhang et al., 2004</xref>). In our experiment, the motion parallax cue in the optic flow simulates a cube of dots (40 cm in depth) symmetrically crossing the fixation plane. Thus, the pursuit effect on heading perception may roughly be similar, but not identical between the 2-dimensional flow restricted in one single plane (fixation plane) and the 3-dimensional flow across multiple planes. On the other hand, motion parallax has been suggested to play an important role in deciphering rotational (due to pursuit) and translational components of self-motion in the optic flow field. Thus it is somehow surprising that we have not observed too much difference in the neural activity of cortical neurons under the motion parallax conditions. It is possible that a much larger impact of motion parallax could be observed in a simulated pursuit condition that has been missing in our current experimental design (<xref ref-type="bibr" rid="bib8">Bremmer et al., 2010</xref>). Future experiments including both real and simulated pursuit protocols need to be conducted to examine this hypothesis.</p></sec><sec id="s3-2"><title>Behavioral contexts</title><p>A recent study shows that human subjects estimate heading directions irrelevant of the eye positions under the vestibular condition, whereas in the visual condition, the eccentric gaze causes roughly 46% shift in the perceived heading (<xref ref-type="bibr" rid="bib17">Crane, 2015</xref>). This result suggests that heading perception based on visual optic flow may be biased somehow towards retina coordinate. However, there is no feedback signals provided to the subjects in this study, thus it is unclear how feedback signals and learning process may help recover true headings under eccentric fixations. Indeed, we found that the monkey’s perceived heading was largely affected by eye positions initially. After training the animals to judge headings relative to the head by rewarding for a week or two, the estimation bias was reduced to only a few degrees, roughly 10 –20% of the gaze magnitude, implying a predominant head coordinate.</p><p>On the neuronal level, we assess the training effect mainly from three aspects. First, after training, the visual reference frame in FEFsem and MSTd is slightly shifted towards the head coordinate. However, this change is modest (~20%). Second, once trained, the visual reference frame does not show significant difference under the passive viewing and active estimation tasks. Third, compared to the passive viewing task, the overall response magnitude is increased or decreased on a minor proportion of neurons (11.3% and 8.5%, respectively) under the active estimation task. Thus generally training/learning seems to have a limited effect on the visual coordinate measured in the current two brain areas (FEFsem and MSTd). However, it remains possibility that this effect may be larger in other brain regions such as the ventral intraparietal area (VIP, [<xref ref-type="bibr" rid="bib14">Chen et al., 2011b</xref>]) and the visual posterior sylvian fissure (VPS, [<xref ref-type="bibr" rid="bib13">Chen et al., 2011a</xref>]). In future experiments, proper methods need to be employed including complete tuning curve measures under active heading estimation contexts.</p></sec><sec id="s3-3"><title>Distinct visual and vestibular spatial coordinate in sensory cortices</title><p>The distinct visual and vestibular spatial coordinate is potentially an obstacle for cue integration. This is because if the heading information conveyed from each sensory cue is confounded by different types of eye behavior that are frequently accompanied during natural navigation, it would be hard to imagine how the brain could integrate inconsistent sensory evidence across different modalities in a statistically optimal way (<xref ref-type="bibr" rid="bib20">Deneve and Pouget, 2004</xref>). One straightforward intuition is that somewhere in the brain, the visual and the vestibular heading signals are transformed into a common reference frame. Computational works have proposed that this is feasible through integration of the eye position signals: the neural network could either transform one coordinate to the other (e.g. from eye to head, or from head to eye), or transform both coordinates to an intermediate one (<xref ref-type="bibr" rid="bib60">Zipser and Andersen, 1988</xref>; <xref ref-type="bibr" rid="bib46">Siegel, 1998</xref>; <xref ref-type="bibr" rid="bib31">Gu et al., 2006</xref>; <xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>). In these cases, the hidden units in the model exhibit a ‘gain’ field as observed in many cortical areas including the premotor area (<xref ref-type="bibr" rid="bib39">Pesaran et al., 2006</xref>), parietal area of 7a (<xref ref-type="bibr" rid="bib60">Zipser and Andersen, 1988</xref>; <xref ref-type="bibr" rid="bib46">Siegel, 1998</xref>; <xref ref-type="bibr" rid="bib58">Xing and Andersen, 2000</xref>), V6A (<xref ref-type="bibr" rid="bib32">Hadjidimitrakis et al., 2014</xref>), V6 (<xref ref-type="bibr" rid="bib22">Fan et al., 2015</xref>), VIP (<xref ref-type="bibr" rid="bib15">Chen et al., 2013</xref>) and MSTd (<xref ref-type="bibr" rid="bib31">Gu et al., 2006</xref>; <xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>). In our current work, we have also observed that roughly a quarter of FEFsem neurons exhibit gain fields for the visual optic flow signals. Interestingly, this number is relatively smaller compared to that in MSTd (~40%). Considering the fact that the overall visual coordinate of FEFsem is more intermediate than in MSTd (<xref ref-type="fig" rid="fig2">Figure 2B,C</xref>), these results may suggest that compared to extrastriate visual cortex, FEFsem is at a later stage for spatial coordinate transformations.</p><p>For downstream areas that receive both eye-centered visual optic flow and eye position signals for coordinate transformations, we expect to observe a head- or near head-centered coordinate of the heading signals in these regions. However, no such areas have been discovered so far. For all the sensory areas that researcher have explored including VIP (<xref ref-type="bibr" rid="bib15">Chen et al., 2013</xref>), MSTd (<xref ref-type="bibr" rid="bib31">Gu et al., 2006</xref>; <xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>), V6 (<xref ref-type="bibr" rid="bib22">Fan et al., 2015</xref>) and even FEFsem in the current study, majority of neurons exhibit fairly eye-centered visual optic flow signals, and most of them carry eye position signals at the same time. Thus, overall these areas may still serve as an intermediate stage for coordinate transformations. In the future, one strategy is to keep searching for head-centered visual reference frames in the other sensory cortices. On the other hand, coordinate transformations may never be accomplished in the sensory cortices, instead they may be implemented in the sensory-motor association areas in which decisions and ocular motor responses are formed for multisensory heading perception. So the other strategy is probably to study the posterior parietal cortex, prefrontal cortex, or the subcortical area of the superior colliculus.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animal preparation</title><p>Surgical preparation and training have been described in detail in previous studies (<xref ref-type="bibr" rid="bib31">Gu et al., 2006</xref>; <xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Takahashi et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2013</xref>). Briefly, two male rhesus monkeys (Macaca mulata), weighing 6–10 kg were chronically implanted, under sterile conditions, with a light-weight plastic head-restraint ring that was anchored to the skull using titanium inverted T-bolts and dental acrylic. The ring was 5–6 cm in diameter, serving as the head post and recording chamber at the same time. Scleral search coil was implanted in one eye for tracking and measuring eye movements in a magnetic field. After surgical recovery, behavioral training was accomplished using standard operant conditioning procedures through water/juice reward. Before recording experiments, a plastic grid containing staggered rows of holes (0.8 mm spacing) was stereotaxically secured inside the head ring covering majority of the space, allowing for accessing multiple areas at the same time. The grid was positioned in the horizontal plane. Vertical microelectrode penetrations were made via transdural guide tubes inserted in the grid holes. All procedures were approved by the Animal Care Committee of Shanghai Institutes for Biological Sciences, Chinese Academy of Sciences (Shanghai, China).</p></sec><sec id="s4-2"><title>Anatomical localization</title><p>FEFsem was initially identified via a combination of structural MRI scans and the pattern of eye movements evoked by electrical microstimulation (~50 μA, 200 Hz) as described previously (<xref ref-type="bibr" rid="bib30">Gu et al., 2016</xref>). Briefly, the arcuate sulcus in the frontal lobe was first identified to locate FEF. Electrical stimulation was then applied to evoke eye movements for distinguishing the subregion of FEFsem from FEFsac (<xref ref-type="bibr" rid="bib36">MacAvoy et al., 1991</xref>; <xref ref-type="bibr" rid="bib28">Gottlieb et al., 1994</xref>). Specifically, microstimulation in FEFsem usually evoked smooth eye movements to the ipsilateral direction with the recording hemisphere, which was in contrast to the fast saccade eye movement in the direction opposite to the recording hemisphere. Within FEFsem, we further used a pursuit protocol to verify whether the isolated single unit was indeed a pursuit neuron. In particular, the animals were required to pursue a fixation spot that moved linearly in one of 8 equally-spaced directions in the frontoparallel plane at a speed of 20°/s (<xref ref-type="bibr" rid="bib28">Gottlieb et al., 1994</xref>; <xref ref-type="bibr" rid="bib30">Gu et al., 2016</xref>). Only neurons significantly tuned under the pursuit protocol (p&lt;0.05, One-way ANOVA) were further recorded for other parts of experiments in the current study.</p><p>Extracellular single-unit recordings were performed with tungsten microelectrodes (tip diameter 3 μm, impedance 1–2 MΩ at 1 kHz, FHC, Inc.) that was advanced into the cortex through a transdural guide tube, using a micromanipulator (FHC, Inc.). Single neurons were isolated using a conventional amplifier and a dual voltage-time window discriminator (Bak Electronics, Mount Airy, MD). The times of occurrence of action potentials and all behavioral events were recorded with 1 ms resolution by the data acquisition computer. Raw neural signals were also digitized at 25 kHz and stored to disk for off-line spike sorting (CED Spike2, UK). To allow a direct comparison of the response properties between FEFsem and MSTd on the same animals, we also recorded neurons in MSTd. MSTd was identified using procedures similar to those in previous studies (<xref ref-type="bibr" rid="bib31">Gu et al., 2006</xref>; <xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>). Briefly, MSTd was at the posterior tip of the superior temporal sulcus (AP: ~−2 mm, ML:~15 mm). MSTd neurons usually had large receptive fields that contained the fovea and part of the ipsilateral visual field. MSTd neurons were also sensitive to visual motion defined by global optic flow stimuli. After reaching MSTd, if advancing electrode further down (in the vertical way) for another few millimeters, the middle temporal area MT was usually encountered with neurons containing much small receptive fields that were typically in the contralateral visual field.</p></sec><sec id="s4-3"><title>Behavioral task and experimental procedures</title><p>Translation of the monkeys in 3D space was accomplished by a motion platform (MOOG 6DOF2000E; Moog, East Aurora, NY). During experiments, the monkey was seated comfortably in a primate chair, which was secured to the platform and inside the magnetic field coil frame. A LCD-screen was mounted on the motion platform (subtending 90 × 90° of visual angle) placed 30 cm in front of the monkey (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The screen and the field coil frame were mounted on the motion platform. In order to activate vestibular otolith organs, each transient inertial motion stimulus followed a smooth trajectory with a Gaussian velocity profile, providing the ‘vestibular’ stimulus condition. In the ‘visual’ condition, global optic flow that occupied the whole screen was provided to simulate self-motion through a 3D random dot field (OpenGL graphics library). All the dots were moving coherently (100%), generating a strong motion signal.</p><p>Heading stimuli including eight directions equally apart were first delivered in two planes: horizontal and sagittal planes (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). A ‘preferred’ plane was then chosen for each isolated neuron with significant (p&lt;0.05, One-way ANOVA) and strongest modulation. Five experimental blocks were subsequently applied: (1) eccentric fixation in visual (with motion parallax), and/or vestibular condition, (2) eccentric fixation in visual condition without motion parallax, (3) smooth pursuit in visual condition with motion parallax, (4) smooth pursuit in visual condition without motion parallax, and (5) active heading estimation task. In general, in all experiments, the animals were required to maintain fixation at the fixation spot within an electronic window (2 × 2°). Exceeding the window would result in abandon of the trial.</p><sec id="s4-3-1"><title>Eccentric fixation task</title><p>The monkeys were presented with a fixation spot (0.2° in diameter) at one of three locations: one central, that is straight forward (0°), and two peripheral targets with equal eccentricity of 20° either in the horizontal or vertical axis (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In some additional experiments, seven possible fixation locations were introduced and interleaved including:±30,±20,±10, and 0°. The monkey initiated each trial by acquiring fixation within a 2 × 2° electronic window. After fixation, visual and/or vestibular heading stimuli were presented in the preferred plane that was determined in advance. Heading typically included eight directions that were spaced at 45° intervals, leading to a full range of 360°. In the horizontal plane, four additional directions apart by 11.25° were interpolated around straight ahead (0 ± 11.25°, 0 ± 22.5°). Each stimulus condition was repeated at least three times, yet majority of the neurons (92%) were collected for five or more repetitions. Visual and vestibular heading stimuli had same Gaussian velocity profile (duration: 2 s; travelling distance: 0.11 m; peak acceleration: 0.85 m/s<sup>2</sup>; peak velocity: 0.25 m/s). The visual stimulus had two conditions, either with motion parallax (namely ‘3D’), or without motion parallax (namely ‘2D’). In the ‘3D’ case, the virtual workspace was 100 cm wide, 100 cm high, and 40 cm deep. Star density was 0.01/cm<sup>3</sup>, with each star being a 0.15 × 0.15 cm triangle. Thus the visual stimulus simulates the animal’s approaching a 3D cloud of dots. In the ‘2D’ case, the flow dots were distributed only within the fixation plane with a density of 0.4/cm<sup>2</sup>, whereas all the other experimental parameters were identical as in the ‘3D’ case. Stimuli were viewed binocularly but without disparity information (no stereo cues). The animals were required to maintain fixation during the stimulus duration of 2 s while passively experiencing heading stimuli. At the end of the trial, the animals were rewarded with a drop of liquid after successful fixation.</p></sec><sec id="s4-3-2"><title>Smooth pursuit</title><p>During the visual optic flow presentation, the animals were required to pursue the fixation spot that was moving smoothly at a constant speed of 16°/s. The pursuit direction could be either in the horizontal or the vertical axis. Pursuit started from an eccentric position of 12 °, and the average eye position during a pursuit trial was aligned with the center of the screen. The two pursuit directions and a control condition of central fixation were randomly interleaved in one experimental block. Similar to the eccentric fixation task, the monkeys were required to maintain gaze within a 2 × 2° eye window during pursuit, except that at the first and last 250 ms of a trial, this window is larger (4 × 4°). Spikes in these two windows were excluded from the analysis window (see data analysis). Monkeys were rewarded after accomplishing pursuit at the end the trial.</p></sec><sec id="s4-3-3"><title>Heading estimation task</title><p>In this context, the animals were required to actively report their perceived headings simulated from visual optic flow by making saccade to a choice ring that appeared at the end of the trial (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The simulated heading directions were varied at 20° intervals in the horizontal plane. The animals maintained fixation at one of the three locations (−20°, 0°, 20°) across the stimulus duration. At the end of the trial, the fixation spot disappeared, and the choice ring, 20° in diameter, appeared at the screen. The center of the ring was always aligned with the fixation location in a certain trial, which could be at one of the three fixation locations (−20°, 0°, 20°). The ring consisted of 36 dots uniformly spaced at 10° intervals. Notice that the headings and the choice targets were in different planes, thus, the animals needed to correctly associate their perceived headings (in the horizontal plane) to the choice targets (in the fronto-parallel plane). Specifically, lateral headings corresponded to lateral choice targets, and forward/backward headings corresponded to upward/downward targets. The size of the reward window was 5 × 5°. Each block consisted of 54 conditions (18 headings × 3 eye positions). Each condition was repeated at least 5 times, leading to more than 270 trials. In addition, another control block was included in which the animals only maintained fixation during stimulus presentation and were not required to make choice at the end of trial. Thus, the total trials in this experiment was &gt;540 (270 × 2).</p></sec></sec><sec id="s4-4"><title>Data analysis</title><p>All data analyses and statistical tests were performed using MATLAB (MathWorks, Natick, MA). Tuning profiles for the different stimulus and task conditions were constructed by plotting the mean firing rate (spikes/s) as a function of heading direction. Firing rate was computed over the middle 1 s of each successfully completed trial. This analysis window was chosen because of two reasons. First, most of the velocity variation occurred in the central 1 s of Gaussian velocity profile (<xref ref-type="bibr" rid="bib31">Gu et al., 2006</xref>). Second, this is the time period when the animal’s pursuit eye movement is stable in the smooth pursuit protocol.</p><sec id="s4-4-1"><title>Displacement index (DI) under eccentric fixation task</title><p>For each pair of tuning curves (p&lt;0.05, One-way ANOVA) at two different eye positions, the amount of shift is quantified by computing the cross-covariance metric (<xref ref-type="bibr" rid="bib2">Avillac et al., 2005</xref>; <xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>max</mml:mi><mml:mo>(</mml:mo><mml:mi>cov</mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>, where <italic>k</italic> (in degrees) is the relative displacement of the tuning functions (denoted <italic>R<sub>i</sub></italic> and <italic>R<sub>j</sub></italic>), and the superscript above k refers to the maximum covariance between the tuning curves as a function of <italic>k</italic> (ranging from −180° to +180°). Tuning functions were linearly interpolated with a resolution of 1°. The denominator represents the difference between the eye positions (<italic>P<sub>i</sub></italic> and <italic>P<sub>j</sub></italic>) under which the tuning functions were measured. If the shift between a pair of tuning curves is equal to the change in eye position, the DI equals to 1, meaning eye-centered coordinate. If tuning curves are not shifted at all, DI will be 0, meaning head-centered coordinate. Any single neuron is included in the dataset if it has at least two tuning curves (at two of the three fixation locations) that passed the statistical criterion (p&lt;0.05, One-way ANOVA). If all tuning curves were significant, there would be three DI values (combinations from three fixation locations), and a single averaged DI is assigned for the unit. For each DI value, the 95% confidence intervals were computed from a bootstrap resampling procedure. Briefly, bootstrapped tuning functions under each eccentric fixation condition were obtained by resampling (with replacement) the same number of repetition from the original tuning functions. A new DI value was computed from the new tuning curves across the eccentric fixation conditions. This process was repeated 1000 times, producing a corresponding distribution of DIs from which 95% confidence intervals could be derived.</p></sec><sec id="s4-4-2"><title>Displacement index (DI) under smooth pursuit protocol</title><p>DI was also used to quantify tuning shift under the smooth pursuit protocol. However, in this case, the simple cross-correlation method as used in the eccentric fixation task was not sufficient to characterize all of the changes in the tuning curves (<xref ref-type="bibr" rid="bib49">Sunkara et al., 2015</xref>). Instead, a 3-step partial shift analysis procedure was applied (<xref ref-type="bibr" rid="bib49">Sunkara et al., 2015</xref>). (1) Peak-to-trough modulations in the tuning curves of pursuit trials were linearly scaled to match that in the no-pursuit trials). (2) Each tuning function was split into two halves: one with heading ranges [0° 180°], and one [180° 360°]. The split tuning curve needed to be significant (p&lt;0.05, One-way ANOVA), and was linearly interpolated to a resolution of 1°. (3) Within each half heading range, tuning curve under the no-pursuit condition was circularly shifted (in step of 1°) to maximize the correlation coefficient between the two comparison tuning curves. Thus, each neuron has four shift values at most (two pursuit conditions plus two heading ranges). These values are averaged to serve as the numerator in Equation [1]. The denominator is the predicted shift of tuning curves that respond to resultant optic flow under eye rotations, which is roughly 30° in our case (<xref ref-type="bibr" rid="bib5">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib45">Shenoy et al., 2002</xref>; <xref ref-type="bibr" rid="bib59">Zhang et al., 2004</xref>).</p></sec><sec id="s4-4-3"><title>Eye- and head-centered models</title><p>Tuning curves were fit with a modified wrapped Gaussian function (<xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>) of the following form:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:mo>[</mml:mo><mml:mi>e</mml:mi><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>cos</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⋅</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:mi>e</mml:mi><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>cos</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>180</mml:mn><mml:mo>°</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></disp-formula></p><p>There are six free parameters: <italic>θ<sub>p</sub></italic> is the peak location, <italic>σ</italic> is the tuning width, <italic>A<sub>1</sub></italic> is the overall amplitude, and <italic>R<sub>0</sub></italic> is the baseline response level. The second exponential term produces a second peak 180° out of phase with the first peak, but only when <italic>A<sub>2</sub></italic> is sufficiently large. This extra term is necessary for fitting a small population of neurons with more than one peak. The relative widths of the two peaks are determined by the parameter κ. The goodness of fit is quantified by the correlation coefficients of R<sup>2</sup> between the fitting and the raw data. To eliminate bad fits from our analysis, we excluded a minority of data (~4%) with R<sup>2</sup> &lt;0.6.</p><p>Tuning curves under the three eccentric fixation task (−20°, 0°, 20°) are fit with an eye-centered and a head-centered model simultaneously (using the Matlab function fmincon). Thus, the total number of free parameters in each fitting is 16 (3 eye positions × 5 free parameters of <italic>A<sub>1</sub></italic>, <italic>σ,</italic> κ, <italic>A<sub>2</sub></italic> and <italic>R<sub>0</sub></italic> in Equation [2] + θ<sub>0</sub>). <italic>θ<sub>0</sub></italic> is the eccentric fixation task in the eye-centered model (<italic>θ<sub>0</sub></italic> for 0° fixation, <italic>θ<sub>0</sub></italic> - 20° for leftward fixation, and <italic>θ<sub>0</sub></italic> +20° for rightward fixation). In the head-centered model, <italic>θ<sub>0</sub></italic> is always the same across eye positions.</p><p>For each fit, the correlation coefficient between the fit and the data is used to assess the goodness-of-fit. To remove the influence of correlations between the models themselves, partial correlation coefficients are calculated with the following formula (<xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>, where <italic>r<sub>e</sub></italic> and <italic>r<sub>h</sub></italic> are the correlation coefficients between the data and the eye- and head-centered models, respectively. <italic>r<sub>eh</sub></italic> is the correlation between the two models. Partial correlation coefficients <italic>R<sub>e</sub></italic> and <italic>R<sub>h</sub></italic> are normalized using Fisher’s r-to-Z transform so that Z-scores from two the models are independent of the number of data points (<xref ref-type="bibr" rid="bib1">Angelaki et al., 2004</xref>; <xref ref-type="bibr" rid="bib47">Smith et al., 2005</xref>; <xref ref-type="bibr" rid="bib25">Fetsch et al., 2007</xref>). A criterion of 1.645 for the Z-score (equivalent to p=0.05) is used to categorize cells into three groups: eye-centered (<xref ref-type="fig" rid="fig3">Figure 3C,D</xref>, dashed lines).</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgement</title><p>We thank Wenyao Chen for monkey care and training, Ying Liu for software programming. This work was supported by grants from the National Natural Science Foundation of China Project (grant 31471048), the National Key R and D Program of China (2016YFC1306801), and the Strategic Priority Research Program of CAS (XDB02010000).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Resources, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con2"><p>Supervision, Funding acquisition, Investigation, Visualization, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All animals in this research are lawfully acquired and their retention and use are in every case in compliance with national and local laws and regulations, and in accordance with the Guide for Care and Use of Laboratory Animals of Instituted for Laboratory Animal Research. All experimental procedures are approved by the Animal Care Committee of Shanghai Institutes for Biological Science, Chinese Academy of Science (ER-SIBS-221409P for Professor Yong Gu).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.29809.020</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-29809-transrepform-v1.docx"/></supplementary-material></sec><ref-list><title>Reference</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angelaki</surname> <given-names>DE</given-names></name><name><surname>Shaikh</surname> <given-names>AG</given-names></name><name><surname>Green</surname> <given-names>AM</given-names></name><name><surname>Dickman</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neurons compute internal models of the physical laws of motion</article-title><source>Nature</source><volume>430</volume><fpage>560</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1038/nature02754</pub-id><pub-id pub-id-type="pmid">15282606</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avillac</surname> <given-names>M</given-names></name><name><surname>Denève</surname> <given-names>S</given-names></name><name><surname>Olivier</surname> <given-names>E</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name><name><surname>Duhamel</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Reference frames for representing visual and tactile locations in parietal cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>941</fpage><lpage>949</lpage><pub-id pub-id-type="doi">10.1038/nn1480</pub-id><pub-id pub-id-type="pmid">15951810</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banks</surname> <given-names>MS</given-names></name><name><surname>Ehrlich</surname> <given-names>SM</given-names></name><name><surname>Backus</surname> <given-names>BT</given-names></name><name><surname>Crowell</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Estimating heading during real and simulated eye movements</article-title><source>Vision Research</source><volume>36</volume><fpage>431</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(95)00122-0</pub-id><pub-id pub-id-type="pmid">8746233</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertin</surname> <given-names>RJ</given-names></name><name><surname>Berthoz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Visuo-vestibular interaction in the reconstruction of travelled trajectories</article-title><source>Experimental Brain Research</source><volume>154</volume><fpage>11</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1007/s00221-003-1524-3</pub-id><pub-id pub-id-type="pmid">14600796</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname> <given-names>DC</given-names></name><name><surname>Maxwell</surname> <given-names>M</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Mechanisms of heading perception in primate visual cortex</article-title><source>Science</source><volume>273</volume><fpage>1544</fpage><lpage>1547</lpage><pub-id pub-id-type="doi">10.1126/science.273.5281.1544</pub-id><pub-id pub-id-type="pmid">8703215</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bremmer</surname> <given-names>F</given-names></name><name><surname>Distler</surname> <given-names>C</given-names></name><name><surname>Hoffmann</surname> <given-names>KP</given-names></name></person-group><year iso-8601-date="1997">1997a</year><article-title>Eye position effects in monkey cortex. II. Pursuit- and fixation-related activity in posterior parietal areas LIP and 7A</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>962</fpage><lpage>977</lpage><pub-id pub-id-type="pmid">9065861</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bremmer</surname> <given-names>F</given-names></name><name><surname>Ilg</surname> <given-names>UJ</given-names></name><name><surname>Thiele</surname> <given-names>A</given-names></name><name><surname>Distler</surname> <given-names>C</given-names></name><name><surname>Hoffmann</surname> <given-names>KP</given-names></name></person-group><year iso-8601-date="1997">1997b</year><article-title>Eye position effects in monkey cortex. I. Visual and pursuit-related activity in extrastriate areas MT and MST</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>944</fpage><lpage>961</lpage><pub-id pub-id-type="pmid">9065860</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bremmer</surname> <given-names>F</given-names></name><name><surname>Kubischik</surname> <given-names>M</given-names></name><name><surname>Pekel</surname> <given-names>M</given-names></name><name><surname>Hoffmann</surname> <given-names>KP</given-names></name><name><surname>Lappe</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual selectivity for heading in monkey area MST</article-title><source>Experimental Brain Research</source><volume>200</volume><fpage>51</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1007/s00221-009-1990-3</pub-id><pub-id pub-id-type="pmid">19727690</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname> <given-names>KH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mechanisms of self-motion perception</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>389</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.112953</pub-id><pub-id pub-id-type="pmid">18558861</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname> <given-names>JS</given-names></name><name><surname>Campos</surname> <given-names>JL</given-names></name><name><surname>Bülthoff</surname> <given-names>HH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Optimal visual-vestibular integration under conditions of conflicting intersensory motion profiles</article-title><source>Experimental Brain Research</source><volume>233</volume><fpage>587</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1007/s00221-014-4136-1</pub-id><pub-id pub-id-type="pmid">25361642</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname> <given-names>JS</given-names></name><name><surname>Murray</surname> <given-names>DW</given-names></name><name><surname>Hurson</surname> <given-names>CJ</given-names></name><name><surname>O'Brien</surname> <given-names>J</given-names></name><name><surname>Doran</surname> <given-names>PP</given-names></name><name><surname>O'Byrne</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The role of Dkk1 in bone mass regulation: correlating serum Dkk1 expression with bone mineral density</article-title><source>Journal of Orthopaedic Research</source><volume>29</volume><fpage>414</fpage><lpage>418</lpage><pub-id pub-id-type="doi">10.1002/jor.21260</pub-id><pub-id pub-id-type="pmid">20939046</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname> <given-names>JS</given-names></name><name><surname>Smith</surname> <given-names>ST</given-names></name><name><surname>Campos</surname> <given-names>JL</given-names></name><name><surname>Bülthoff</surname> <given-names>HH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Bayesian integration of visual and vestibular signals for heading</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.1167/10.11.23</pub-id><pub-id pub-id-type="pmid">20884518</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>A</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Convergence of vestibular and visual self-motion signals in an area of the posterior sylvian fissure</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>11617</fpage><lpage>11627</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1266-11.2011</pub-id><pub-id pub-id-type="pmid">21832191</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>A</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Representation of vestibular and visual cues to self-motion in ventral intraparietal cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>12036</fpage><lpage>12052</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0395-11.2011</pub-id><pub-id pub-id-type="pmid">21849564</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Eye-centered representation of optic flow tuning in the ventral intraparietal area</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>18574</fpage><lpage>18582</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2837-13.2013</pub-id><pub-id pub-id-type="pmid">24259579</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>YE</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A common reference frame for movement plans in the posterior parietal cortex</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>553</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1038/nrn873</pub-id><pub-id pub-id-type="pmid">12094211</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crane</surname> <given-names>BT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Coordinates of human visual and inertial heading perception</article-title><source>PLoS One</source><volume>10</volume><fpage>e0135539</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0135539</pub-id><pub-id pub-id-type="pmid">26267865</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crane</surname> <given-names>BT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Effect of eye position during human visual-vestibular integration of heading perception</article-title><source>Journal of Neurophysiology</source><volume>118</volume><fpage>1609</fpage><lpage>1621</lpage><pub-id pub-id-type="doi">10.1152/jn.00037.2017</pub-id><pub-id pub-id-type="pmid">28615328</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowell</surname> <given-names>JA</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Visual self-motion perception during head turns</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>732</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.1038/3732</pub-id><pub-id pub-id-type="pmid">10196591</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneve</surname> <given-names>S</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Bayesian multisensory integration and cross-modal spatial links</article-title><source>Journal of Physiology-Paris</source><volume>98</volume><fpage>249</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1016/j.jphysparis.2004.03.011</pub-id><pub-id pub-id-type="pmid">15477036</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duffy</surname> <given-names>CJ</given-names></name><name><surname>Wurtz</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Response of monkey MST neurons to optic flow stimuli with shifted centers of motion</article-title><source>Journal of Neuroscience</source><volume>15</volume><fpage>5192</fpage><lpage>5208</lpage><pub-id pub-id-type="pmid">7623145</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname> <given-names>RH</given-names></name><name><surname>Liu</surname> <given-names>S</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Heading tuning in macaque area V6</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>16303</fpage><lpage>16314</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2903-15.2015</pub-id><pub-id pub-id-type="pmid">26674858</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname> <given-names>DJ</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname> <given-names>CR</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural correlates of reliability-based cue weighting during multisensory integration</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>146</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1038/nn.2983</pub-id><pub-id pub-id-type="pmid">22101645</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname> <given-names>CR</given-names></name><name><surname>Wang</surname> <given-names>S</given-names></name><name><surname>Gu</surname> <given-names>Y</given-names></name><name><surname>Deangelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatial reference frames of visual, vestibular, and multimodal heading signals in the dorsal subdivision of the medial superior temporal area</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>700</fpage><lpage>712</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3553-06.2007</pub-id><pub-id pub-id-type="pmid">17234602</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukushima</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Frontal cortical control of smooth-pursuit</article-title><source>Current Opinion in Neurobiology</source><volume>13</volume><fpage>647</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2003.10.007</pub-id><pub-id pub-id-type="pmid">14662364</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="1950">1950</year><article-title>The perception of visual surfaces</article-title><source>The American Journal of Psychology</source><volume>63</volume><fpage>367</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.2307/1418003</pub-id><pub-id pub-id-type="pmid">15432778</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottlieb</surname> <given-names>JP</given-names></name><name><surname>MacAvoy</surname> <given-names>MG</given-names></name><name><surname>Bruce</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Neural responses related to smooth-pursuit eye movements and their correspondence with electrically elicited smooth eye movements in the primate frontal eye field</article-title><source>Journal of Neurophysiology</source><volume>72</volume><fpage>1634</fpage><lpage>1653</lpage><pub-id pub-id-type="pmid">7823092</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname> <given-names>Y</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name><name><surname>Deangelis</surname> <given-names>GC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural correlates of multisensory cue integration in macaque MSTd</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1201</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1038/nn.2191</pub-id><pub-id pub-id-type="pmid">18776893</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname> <given-names>Y</given-names></name><name><surname>Cheng</surname> <given-names>Z</given-names></name><name><surname>Yang</surname> <given-names>L</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multisensory convergence of visual and vestibular heading cues in the pursuit area of the frontal eye field</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3785</fpage><lpage>3801</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv183</pub-id><pub-id pub-id-type="pmid">26286917</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname> <given-names>Y</given-names></name><name><surname>Watkins</surname> <given-names>PV</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Visual and nonvisual contributions to three-dimensional heading selectivity in the medial superior temporal area</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>73</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2356-05.2006</pub-id><pub-id pub-id-type="pmid">16399674</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hadjidimitrakis</surname> <given-names>K</given-names></name><name><surname>Bertozzi</surname> <given-names>F</given-names></name><name><surname>Breveglieri</surname> <given-names>R</given-names></name><name><surname>Bosco</surname> <given-names>A</given-names></name><name><surname>Galletti</surname> <given-names>C</given-names></name><name><surname>Fattori</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Common neural substrate for processing depth and direction signals for reaching in the monkey medial posterior parietal cortex</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>1645</fpage><lpage>1657</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht021</pub-id><pub-id pub-id-type="pmid">23382514</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>LR</given-names></name><name><surname>Jenkin</surname> <given-names>M</given-names></name><name><surname>Zikovitz</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Visual and non-visual cues in the perception of linear self-motion</article-title><source>Experimental Brain Research</source><volume>135</volume><fpage>12</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1007/s002210000504</pub-id><pub-id pub-id-type="pmid">11104123</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ilg</surname> <given-names>UJ</given-names></name><name><surname>Schumann</surname> <given-names>S</given-names></name><name><surname>Thier</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Posterior parietal cortex neurons encode target motion in world-centered coordinates</article-title><source>Neuron</source><volume>43</volume><fpage>145</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.06.006</pub-id><pub-id pub-id-type="pmid">15233924</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lynch</surname> <given-names>JC</given-names></name><name><surname>Tian</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortico-cortical networks and cortico-subcortical loops for the higher control of eye movements</article-title><source>Progress in Brain Research</source><volume>151</volume><fpage>461</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(05)51015-X</pub-id><pub-id pub-id-type="pmid">16221598</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacAvoy</surname> <given-names>MG</given-names></name><name><surname>Gottlieb</surname> <given-names>JP</given-names></name><name><surname>Bruce</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Smooth-pursuit eye movement representation in the primate frontal eye field</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>95</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.95</pub-id><pub-id pub-id-type="pmid">1822728</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohmi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Egocentric perception through interaction among many sensory systems</article-title><source>Cognitive Brain Research</source><volume>5</volume><fpage>87</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(96)00044-4</pub-id><pub-id pub-id-type="pmid">9049074</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Page</surname> <given-names>WK</given-names></name><name><surname>Duffy</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>MST neuronal responses to heading direction during pursuit eye movements</article-title><source>Journal of Neurophysiology</source><volume>81</volume><fpage>596</fpage><lpage>610</lpage><pub-id pub-id-type="pmid">10036263</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesaran</surname> <given-names>B</given-names></name><name><surname>Nelson</surname> <given-names>MJ</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dorsal premotor neurons encode the relative position of the hand, eye, and goal during reach planning</article-title><source>Neuron</source><volume>51</volume><fpage>125</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.05.025</pub-id><pub-id pub-id-type="pmid">16815337</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname> <given-names>CS</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name><name><surname>Crowell</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The perception of heading during eye movements</article-title><source>Nature</source><volume>360</volume><fpage>583</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1038/360583a0</pub-id><pub-id pub-id-type="pmid">1461280</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname> <given-names>CS</given-names></name><name><surname>Crowell</surname> <given-names>JA</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Estimating heading during eye movements</article-title><source>Vision Research</source><volume>34</volume><fpage>3197</fpage><lpage>3214</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)90084-1</pub-id><pub-id pub-id-type="pmid">7975351</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname> <given-names>CS</given-names></name><name><surname>Hildreth</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Human heading judgments in the presence of moving objects</article-title><source>Perception &amp; Psychophysics</source><volume>58</volume><fpage>836</fpage><lpage>856</lpage><pub-id pub-id-type="doi">10.3758/BF03205487</pub-id><pub-id pub-id-type="pmid">8768180</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname> <given-names>CS</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Analysis of misperceived observer motion during simulated eye rotations</article-title><source>Vision Research</source><volume>34</volume><fpage>3215</fpage><lpage>3222</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)90085-X</pub-id><pub-id pub-id-type="pmid">7975352</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Bradley</surname> <given-names>DC</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Influence of gaze rotation on the visual response of primate MSTd neurons</article-title><source>Journal of Neurophysiology</source><volume>81</volume><fpage>2764</fpage><lpage>2786</lpage><pub-id pub-id-type="pmid">10368396</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Crowell</surname> <given-names>JA</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Pursuit speed compensation in cortical area MSTd</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>2630</fpage><lpage>2647</lpage><pub-id pub-id-type="doi">10.1152/jn.00002.2001</pub-id><pub-id pub-id-type="pmid">12424299</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Representation of visual space in area 7a neurons using the center of mass equation</article-title><source>Journal of Computational Neuroscience</source><volume>5</volume><fpage>365</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1023/A:1008844027878</pub-id><pub-id pub-id-type="pmid">9877020</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>MA</given-names></name><name><surname>Majaj</surname> <given-names>NJ</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dynamics of motion signaling by neurons in macaque area MT</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>220</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1038/nn1382</pub-id><pub-id pub-id-type="pmid">15657600</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname> <given-names>BE</given-names></name><name><surname>Meredith</surname> <given-names>MA</given-names></name><name><surname>Wallace</surname> <given-names>MT</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The visually responsive neuron and beyond: multisensory integration in cat and monkey</article-title><source>Progress in Brain Research</source><volume>95</volume><fpage>79</fpage><lpage>90</lpage><pub-id pub-id-type="pmid">8493355</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sunkara</surname> <given-names>A</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Role of visual and non-visual cues in constructing a rotation-invariant representation of heading in parietal cortex</article-title><source>eLife</source><volume>4</volume><elocation-id>e04693</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04693</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname> <given-names>K</given-names></name><name><surname>Gu</surname> <given-names>Y</given-names></name><name><surname>May</surname> <given-names>PJ</given-names></name><name><surname>Newlands</surname> <given-names>SD</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Multimodal coding of three-dimensional rotation and translation in area MSTd: comparison of visual and vestibular selectivity</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>9742</fpage><lpage>9756</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0817-07.2007</pub-id><pub-id pub-id-type="pmid">17804635</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname> <given-names>K</given-names></name><name><surname>Hikosaka</surname> <given-names>K</given-names></name><name><surname>Saito</surname> <given-names>H</given-names></name><name><surname>Yukie</surname> <given-names>M</given-names></name><name><surname>Fukada</surname> <given-names>Y</given-names></name><name><surname>Iwai</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Analysis of local and wide-field movements in the superior temporal visual areas of the macaque monkey</article-title><source>Journal of Neuroscience</source><volume>6</volume><fpage>134</fpage><lpage>144</lpage><pub-id pub-id-type="pmid">3944614</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Telford</surname> <given-names>L</given-names></name><name><surname>Howard</surname> <given-names>IP</given-names></name><name><surname>Ohmi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Heading judgments during active and passive self-motion</article-title><source>Experimental Brain Research</source><volume>104</volume><fpage>502</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1007/BF00231984</pub-id><pub-id pub-id-type="pmid">7589301</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>von Helmholtz</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1963">1963</year><chapter-title>Helmholtz's treatise on physiological optics</chapter-title><source>Polarized Light</source><fpage>30</fpage></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname> <given-names>WH</given-names></name><name><surname>Hannon</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Direction of self-motion is perceived from optical flow</article-title><source>Nature</source><volume>336</volume><fpage>162</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1038/336162a0</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname> <given-names>WH</given-names></name><name><surname>Hannon</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Eye movements and optical flow</article-title><source>Journal of the Optical Society of America A</source><volume>7</volume><fpage>160</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.7.000160</pub-id><pub-id pub-id-type="pmid">2299447</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname> <given-names>WH</given-names></name><name><surname>Saunders</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Perceiving heading in the presence of moving objects</article-title><source>Perception</source><volume>24</volume><fpage>315</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1068/p240315</pub-id><pub-id pub-id-type="pmid">7617432</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Warren</surname> <given-names>WH</given-names></name></person-group><year iso-8601-date="2003">2003</year><person-group person-group-type="editor"><name><surname>Chalupa</surname> <given-names>L. M</given-names></name><name><surname>Werner</surname> <given-names>L. S</given-names></name></person-group><source>Optic Flow in the Visual Neurosciences</source><publisher-loc>Cambridge</publisher-loc><fpage>1247</fpage><lpage>1259</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xing</surname> <given-names>J</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Models of the posterior parietal cortex which perform multimodal integration and represent space in several coordinate frames</article-title><source>Journal of Cognitive Neuroscience</source><volume>12</volume><fpage>601</fpage><lpage>614</lpage><pub-id pub-id-type="doi">10.1162/089892900562363</pub-id><pub-id pub-id-type="pmid">10936913</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>T</given-names></name><name><surname>Heuer</surname> <given-names>HW</given-names></name><name><surname>Britten</surname> <given-names>KH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Parietal area VIP neuronal responses to heading stimuli are encoded in head-centered coordinates</article-title><source>Neuron</source><volume>42</volume><fpage>993</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.06.008</pub-id><pub-id pub-id-type="pmid">15207243</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zipser</surname> <given-names>D</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons</article-title><source>Nature</source><volume>331</volume><fpage>679</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.1038/331679a0</pub-id><pub-id pub-id-type="pmid">3344044</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.29809.021</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Laurens</surname><given-names>Jean</given-names></name><role>Reviewing Editor</role><aff><institution>Baylor College of Medicine</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Distinct spatial coordinate of visual and vestibular heading signals in macaque FEFsem and MSTd&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor (Richard Ivry).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The paper studies the representation of visual and vestibular heading in primate cortical areas MSTd and FEFsem. Besides providing novel data for FEFsem, an area for which a heading representation has only recently been described, the main point of the paper is a comparison of references frames (eye-centered vs head-centered) in the two areas and for the two modalities. Results show that the representation is mostly modality specific (eye-centered for optic flow and head-centered for vestibular signals) in both areas. All three reviewers were very positive about the interest of the question, the quality of the experimental approach and the overall presentation of the data.</p><p>Essential revisions:</p><p>The conclusions of the manuscript strongly depend on the DI statistics. The DI seems suitable for separating between head and eye reference frames in the sense that responses in head reference frames will tend to have smaller values than responses in the eye reference frame. Therefore, it supports the main conclusion of the manuscript. However, the reviewers are concerned that estimation of the DI might be noisy and the exact values of the DI might be biased depending on how well the tuning curves are estimated. Biases in the estimation of the DI might be especially problematic when comparing between conditions. For example, could the difference in DI between FEF and MSTd (<xref ref-type="fig" rid="fig2">Figure 2B and C</xref>) result from the difference in the response size of the cell and not a true difference in the reference frames? Could the difference between pursuit and fixation (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>) be a result of the difference in the magnitudes of the responses – or the difference in the way the DI are computed? One way to partly overcome this would be to calculate, when possible, the DI from the population average rather than for each cell separately. Population averages would also provide further important information about the magnitude and pattern of the responses. Alternatively, the authors could perform bootstrapping with their data to assess the statistical significance of DI, or perform simulations to support the use of DI.</p><p>The statistical analysis in subsection “Behavioral context for heading estimation” is incorrect. The authors use a one tailed paired t-test; however, a paired t-test may be used only when comparing values that were recorded in the same neurons. Here the authors compare neuronal responses recorded two weeks apart. These can't originate from the same neurons since they author don't perform chronic recordings; therefore the authors can't use a paired t-test. Furthermore, there is no justification for using a one tailed test in this context. The authors should use the correct statistics (two-tailed unpaired test) and update their results accordingly.</p><p>Regarding the investigation of smooth pursuit and motion parallax: the most interesting condition is missing. Theoretically, motion parallax is needed to distinguish rotational and translational components of self-motion in the optic flow field. Rotational components are introduced by smooth pursuit. To test the ability of the visual system to deal with rotational components based on flow analysis, researchers have used the paradigm of simulated pursuit, in which the observer fixates while the flow simulates both a translational heading and a rotational pursuit (see for example, Bremmer et al., 2010). It is conceivable that a much larger impact of motion parallax could be observed in a simulated pursuit condition. The reviewers realize that this condition was not part of the experiment, and do NOT think it is necessary to provide this data. However, the limitations of the study in that regard should be discussed. It may explain why excluding motion parallax from the visual stimuli has limited effect on FEFsem and MSTd's tolerance of the eye rotations (subsection “Motion parallax cue in the visual optic flow”).</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Distinct spatial coordinate of visual and vestibular heading signals in macaque FEFsem and MSTd&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated a Reviewing editor along with three reviewers, and overseen by Richard Ivry as Senior Editor.</p><p>The reviewers have evaluated the revised version and found that all their comments have been addressed except for one point. The reviewing editor has helped draft the following summary of how we would like to see this issue addressed.</p><p>The concern is that the DIs computed in the eccentric fixation and smooth pursuit tasks might be biased towards 0 for cells with noisy tuning curves. Therefore, cells with a lower response magnitude would have a lower signal to noise ratio and the DI would be biased towards zero. As a result, differences between the visual and vestibular cases could result from differences in the magnitude of the responses and not from differences in the coordinates. The bootstrapping methods performed by the authors would not eliminate this bias since the bootstraps would also contain the bias. To test if the DI statistics are biased, the authors could perform simulations in which they construct noiseless tunings curves with a DI of one and then add different levels of noise, calculating the DI in the exact same way they calculate the DI for neurons.</p><p>If it turns out that DI is biased, it would still be possible to show that this bias does not affect conclusions. This would entail controlling for the magnitude of the response. For example, calculate the DI as a function of the tuning magnitude. This would allow you to examine the following:</p><p>1) Cells with the same magnitude of response have a DI close to one during visual condition and close to 0 in vestibular condition.</p><p>2) Cells with the same magnitude of the response during fixation and pursuit have a different DI.</p><p>The authors could also use <xref ref-type="fig" rid="fig3">Figure 3</xref> as a control.</p><p>To sum up, the authors should, as a first step, run simulations to check if the DIs are biased towards 0 when the signal/noise ratio is low. If this is the case, they should demonstrate that it does not affect their conclusions (for instance using the approach suggested above).</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.29809.022</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>The paper studies the representation of visual and vestibular heading in primate cortical areas MSTd and FEFsem. Besides providing novel data for FEFsem, an area for which a heading representation has only recently been described, the main point of the paper is a comparison of references frames (eye-centered vs head-centered) in the two areas and for the two modalities. Results show that the representation is mostly modality specific (eye-centered for optic flow and head-centered for vestibular signals) in both areas. All three reviewers were very positive about the interest of the question, the quality of the experimental approach and the overall presentation of the data.</p></disp-quote><p>We thank reviewers for their thoughtful comments on our work which have spurred us to perform additional data analyses and revise the context to make the paper much stronger. We have attempted to respond to each comment with new analyses or text revisions, as detailed below. The revised file is provided in two versions: one clean file, and one with highlighted changes. We feel that these major additions, combined with many other revisions to address specific points of the reviewers, make the paper much stronger and we hope that the reviewers will now find it acceptable for publication.</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The conclusions of the manuscript strongly depend on the DI statistics. The DI seems suitable for separating between head and eye reference frames in the sense that responses in head reference frames will tend to have smaller values than responses in the eye reference frame. Therefore, it supports the main conclusion of the manuscript. However, the reviewers are concerned that estimation of the DI might be noisy and the exact values of the DI might be biased depending on how well the tuning curves are estimated. Biases in the estimation of the DI might be especially problematic when comparing between conditions. For example, could the difference in DI between FEF and MSTd (<xref ref-type="fig" rid="fig2">Figure 2B and C</xref>) result from the difference in the response size of the cell and not a true difference in the reference frames? Could the difference between pursuit and fixation (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>) be a result of the difference in the magnitudes of the responses – or the difference in the way the DI are computed? One way to partly overcome this would be to calculate, when possible, the DI from the population average rather than for each cell separately. Population averages would also provide further important information about the magnitude and pattern of the responses. Alternatively, the authors could perform bootstrapping with their data to assess the statistical significance of DI, or perform simulations to support the use of DI.</p></disp-quote><p>We understand the reviewers’ concern and appreciate their constructive suggestions. To address this question, we performed a bootstrapping procedure to compute the 95% confidence interval for each DI value. We then labeled each cell’s categorization, i.e. eye-, head-, intermediate or unclassified according to this measurement in all the main figures of DI distributions. In particular, the last group indicates cells with noisy tuning curves. Fortunately, this group only occupies a minor proportion. We can clearly see that our previous main results and conclusions remain consistent. In another word, our original observations of the difference in the DI across experimental conditions are not due to the different noise or magnitude in each condition. We have now added this information in the Material and methods and Results sections. With this additional analysis, we are hoping that it can release the reviewers’ concerns.</p><p>As to the reviewer’s concern about the different way of computing DI between the pursuit and fixation conditions (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>), we re-calculated DI values for the pursuit data with the identical method as in the fixation condition (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Specifically, DI for pursuit was not computed from each half of the tuning curve, but rather from the whole tuning curve at once as for the eccentric fixation data. Obviously, the results and conclusions remain similar (the new <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><disp-quote content-type="editor-comment"><p>The statistical analysis in subsection “Behavioral context for heading estimation” is incorrect. The authors use a one tailed paired t-test; however, a paired t-test may be used only when comparing values that were recorded in the same neurons. Here the authors compare neuronal responses recorded two weeks apart. These can't originate from the same neurons since they author don't perform chronic recordings; therefore the authors can't use a paired t-test. Furthermore, there is no justification for using a one tailed test in this context. The authors should use the correct statistics (two-tailed unpaired test) and update their results accordingly.</p></disp-quote><p>Thank you for capturing this mistake. We actually have used independent t test but have mistakenly described as “paired t test” in the old text. We now corrected this error. We also agree with the review in that it is not justified to use one-tailed test here, so we have changed it to the two-tailed test. Not surprisingly, this increased the p value a bit, which was 0.08, and we thus modified our wording in the text accordingly (subsection “Behavioral context for heading estimation”).</p><disp-quote content-type="editor-comment"><p>Regarding the investigation of smooth pursuit and motion parallax: the most interesting condition is missing. Theoretically, motion parallax is needed to distinguish rotational and translational components of self-motion in the optic flow field. Rotational components are introduced by smooth pursuit. To test the ability of the visual system to deal with rotational components based on flow analysis, researchers have used the paradigm of simulated pursuit, in which the observer fixates while the flow simulates both a translational heading and a rotational pursuit (see for example, Bremmer et al., 2010). It is conceivable that a much larger impact of motion parallax could be observed in a simulated pursuit condition. The reviewers realize that this condition was not part of the experiment, and do NOT think it is necessary to provide this data. However, the limitations of the study in that regard should be discussed. It may explain why excluding motion parallax from the visual stimuli has limited effect on FEFsem and MSTd's tolerance of the eye rotations (subsection “Motion parallax cue in the visual optic flow”).</p></disp-quote><p>We totally agree with the reviewer and appreciate this comment. We realized about this limit in our methodology, and have included it in the Discussion section.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>[…] To sum up, the authors should, as a first step, run simulations to check if the DIs are biased towards 0 when the signal/noise ratio is low. If this is the case, they should demonstrate that it does not affect their conclusions (for instance using the approach suggested above).</p></disp-quote><p>As the reviewer suggested, we first ran simulations as illustrated in the new <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. Our results show that as the signal/noise ratio is low in the tuning curves, the distribution of DI across hypothetical neurons becomes broader, generating some cases of seemingly head-centered coordinate (DI=0). However, there are three points indicating that this pattern is different from that for the real neurons: (1) the number of the case toward 0 is small compared to the majority of the cases with DI=1 (eye-centered); (2) the distribution of DI is symmetric around 1, i.e. extending in both directions instead of systematically biased toward 0; (3) the seemingly head-centered cases under the low signal/noise ratio situations turned out to be “unclassified” group instead of “head-centered group” based on the bootstrap test (open bars, top marginal distributions in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>).</p><p>Indeed, as shown in the real neurons (new <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), there is no significant relationship between the signal/noise ratio and DIs. In fact, many cases carrying similar signal/noise ratio exhibit clearly separated eye-centered (DI=1) and head-centered (DI=0) coordinate for the visual and vestibular signals, respectively. Similarly, the signal/noise ratio is analogous under the pursuit and eccentric fixation condition (the pursuit signal/noise ratio is even slightly higher, which is in the opposite direction as the reviewer’s concern), the pursuit DDI is still close to 0 and the eccentric fixation DDI is close to 1.</p><p>Based on these analyses, we believe that the noise in the tuning curves cannot change the main conclusions in the current paper. In another word, the vestibular signals are mainly head centered and the visual signals are more eye-centered in cortices. Under pursuit, the visual optic flow signals are more tolerant to eye rotations. We have incorporated these new results in the text (subsection “Spatial coordinate of visual and vestibular signals”).</p></body></sub-article></article>