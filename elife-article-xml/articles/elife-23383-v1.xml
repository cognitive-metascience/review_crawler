<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">23383</article-id><article-id pub-id-type="doi">10.7554/eLife.23383</article-id><article-categories><subj-group subj-group-type="heading"><subject>Cancer Biology</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Reproducibility in cancer biology</subject></subj-group><subj-group subj-group-type="display-channel"><subject>Feature article</subject></subj-group></article-categories><title-group><article-title>Making sense of replications</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-17443"><name><surname>Nosek</surname><given-names>Brian A</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/><x>is at the</x></contrib><contrib contrib-type="author" corresp="yes" id="author-16276"><name><surname>Errington</surname><given-names>Timothy M</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4959-5143</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/><x>and the</x></contrib><aff id="aff1"><label>1</label><institution>Center for Open Science</institution>, <addr-line><named-content content-type="city">Charlottesville</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution>University of Virginia</institution>, <addr-line><named-content content-type="city">Charlottesville</named-content></addr-line>, <country>United States</country></aff></contrib-group><author-notes><corresp id="cor1"><email>tim@cos.io</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>19</day><month>01</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e23383</elocation-id><history><date date-type="received"><day>28</day><month>12</month><year>2016</year></date><date date-type="accepted"><day>28</day><month>12</month><year>2016</year></date></history><permissions><copyright-statement>© 2017, Nosek et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Nosek et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-23383-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.23383.001</object-id><p>The first results from the Reproducibility Project: Cancer Biology suggest that there is scope for improving reproducibility in pre-clinical cancer research.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23383.001">http://dx.doi.org/10.7554/eLife.23383.001</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>Reproducibility Project: Cancer Biology</kwd><kwd>replication</kwd><kwd>metascience</kwd><kwd>reproducibility</kwd><kwd>methodology</kwd><kwd>open science</kwd></kwd-group><funding-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The first results from the Reproducibility Project: Cancer Biology suggest that there is scope for improving reproducibility in pre-clinical cancer research.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p>What is replication? In one sense, the answer is easy. Replication is independently repeating the methodology of a previous study and obtaining the same results. In another sense, the answer is difficult. What does it mean to repeat the methodology? And what qualifies as the &quot;same&quot; results? Now that the first five Replication Studies in the <ext-link ext-link-type="uri" xlink:href="https://osf.io/e81xl/wiki/home/">Reproducibility Project: Cancer Biology</ext-link> have been published (see <xref ref-type="box" rid="B1">Box 1</xref>), it is timely to explore how we might answer these questions. The results of the first set of Replication Studies are mixed, and while it is too early to draw any conclusions, it is clear that assessing reproducibility in cancer biology is going to be as complex as it was in a similar project in psychology (<xref ref-type="bibr" rid="bib9">Open Science Collaboration, 2015</xref>).</p><boxed-text id="B1"><object-id pub-id-type="doi">10.7554/eLife.23383.002</object-id><label>Box 1:</label><caption><title>The first results from the Reproducibility Project: Cancer Biology</title></caption><p>The first five Replication Studies published are listed below, along with a link to the Open Science Framework, where all the methods, data and analyses associated with the replication are publicly accessible. Each Replication Study also has a figure that shows the original result, the result from the replication, and a meta-analysis that combines these results. The meta-analysis indicates the cumulative evidence across both studies for the size of an effect and the uncertainty of the existing evidence.</p><p>Aird F, Kandela I, Mantis C, Reproducibility Project: Cancer Biology. 2017. Replication Study: BET bromodomain inhibition as a therapeutic strategy to target c-Myc. <italic>eLife</italic> <bold>6</bold>:e21253.</p><p>Methods, data and analysis available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/7zqxp/">https://osf.io/7zqxp/</ext-link></p><p>Horrigan SK, Reproducibility Project: Cancer Biology. 2017a. Replication Study: The CD47-signal regulatory protein alpha (SIRPa) interaction is a therapeutic target for human solid tumors. <italic>eLife</italic> <bold>6</bold>:e18173.</p><p>Methods, data and analysis available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/9pbos/">https://osf.io/9pbos/</ext-link></p><p>Horrigan SK, Courville P, Sampey D, Zhou F, Cai S, Reproducibility Project: Cancer Biology. 2017b. Replication Study: Melanoma genome sequencing reveals frequent PREX2 mutations. <italic>eLife</italic> <bold>6</bold>:e21634.</p><p>Methods, data and analysis available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/jvpnw/">https://osf.io/jvpnw/</ext-link></p><p>Kandela I, Aird F, Reproducibility Project: Cancer Biology. 2017. Replication Study: Discovery and preclinical validation of drug indications using compendia of public gene expression data. <italic>eLife</italic> <bold>6:</bold>e17044.</p><p>Methods, data and analysis available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/hxrmm/">https://osf.io/hxrmm/</ext-link></p><p>Mantis C, Kandela I, Aird F, Reproducibility Project: Cancer Biology. 2017. Replication Study: Coadministration of a tumor-penetrating peptide enhances the efficacy of cancer drugs. <italic>eLife</italic> <bold>6:</bold>e17584</p><p>Methods, data and analysis available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/xu1g2/">https://osf.io/xu1g2/</ext-link></p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23383.002">http://dx.doi.org/10.7554/eLife.23383.002</ext-link></p></boxed-text><sec id="s1"><title>What does it mean to repeat the methodology?</title><p>There is no such thing as exact replication because there are always differences between the original study and the replication. These differences could be obvious (like the date, the location of the experiment, or the experimenters) or they could be more subtle (like small differences in reagents or the execution of experimental protocols). As a consequence, repeating the methodology does not mean an exact replication, but rather the repetition of what is presumed to matter for obtaining the original result.</p><p>Direct replication is defined as attempting to reproduce a previously observed result with a procedure that provides no a priori reason to expect a different outcome (<xref ref-type="bibr" rid="bib9">Open Science Collaboration, 2015</xref>; <xref ref-type="bibr" rid="bib11">Schmidt, 2009</xref>). In a direct replication, protocols from the original study are followed with different samples of the same or similar materials: as such, a direct replication reflects the current beliefs about what is needed to produce a finding. Conducting a direct replication tests those beliefs empirically. In a conceptual replication, on the other hand, a different methodology (such as a different experimental technique or a different model of a disease) is used to test the same hypothesis: as such, by employing multiple methodologies conceptual replications can provide evidence that enables researchers to converge on an explanation for a finding that is not dependent on any one methodology.</p><p>Both direct and conceptual replications are vital to scientific progress. Direct replication can establish that a finding is reproducible. But, on its own, reproducibility does not guarantee validity. For example, the methodology that was repeated could have confounds, or the theoretical explanation for why the finding occurred could simply be wrong. By providing convergent evidence across methodologies, conceptual replication can foster confidence in the explanation for a finding but such evidence, on its own, does not guarantee the reproducibility of any individual piece of evidence. For example, individual findings could have occurred by chance. Together, direct and conceptual replication provides confidence in the reproducibility of a finding and the explanation for the finding.</p><p>When a direct replication succeeds, confidence in the original finding increases as does the generalizability of the result (due to the differences between the original and replication methodologies). When a direct replication fails, confidence in the original result decreases, but that does not necessarily mean that the original result was incorrect. It is possible, for example, that differences in the methodologies that were thought to be irrelevant are actually important (<xref ref-type="bibr" rid="bib6">Hines et al., 2014</xref>). Indeed, a failed replication can lead to a better understanding of a phenomenon if it results in the generation of new hypotheses to explain how the original and replication methodologies produced different results and, critically, leads to follow-up experiments to test these hypotheses (<xref ref-type="bibr" rid="bib4">Ebersole et al., 2017</xref>).</p><disp-quote><p>Together, direct and conceptual replication provides confidence in the reproducibility of a finding and the explanation for the finding.</p></disp-quote><p>Discrepancies between the original study and the replication can also be due to error rather than meaningful differences in methodology. For example, a false positive might have been observed by chance in the original study, or a false negative in the replication study: such errors can be caused by low statistical power (<xref ref-type="bibr" rid="bib1">Button et al., 2013</xref>; <xref ref-type="bibr" rid="bib3">Cohen, 1969</xref>), or by researchers making design and analysis decisions that increase the likelihood of mistaking noise for signal (<xref ref-type="bibr" rid="bib12">Simmons et al., 2011</xref>). Errors can also be caused by the improper execution of an experimental technique or by problems with samples and materials (such as the contamination of cell lines; <xref ref-type="bibr" rid="bib10">Peterson, 2008</xref>). Discrepancies due to error are less interesting than those due to previously unidentified differences in methodology: unfortunately, results rarely provide clear evidence for whether it is one or the other.</p><p>The Reproducibility Project: Cancer Biology used a number of strategies to minimize the likelihood that any failure to replicate could be attributed to error (<xref ref-type="bibr" rid="bib5">Errington et al., 2014</xref>). The teams performing the replications used experimental designs with high statistical power, undertook authentication of key biological materials (such as STR profiling of cell lines), and employed methods to avoid bias (such as randomization). The authors of the original papers were contacted in advance for details of the research methodology that may not have appeared in their paper, and were asked to share any original reagents, protocols and data in order to maximize the quality and fidelity of the replication designs.</p><disp-quote><p>There is no straightforward answer to the question &quot;what counts as a successful replication of an original result?&quot;</p> </disp-quote><p>Moreover, the project is using the Registered Report/Replication Study approach to publish its work and results. The Registered Report details the experimental designs and protocols that will be used for the replications, and experiments cannot begin until this report has been peer reviewed and accepted for publication. The results of the experiments are then published as a Replication Study, irrespective of outcome but subject to peer review to check that the experimental designs and protocols were followed. Finally, all methods, proposed analyses and data are made publicly accessible via the Open Science Framework to maximize transparency and accountability (<xref ref-type="bibr" rid="bib8">Nosek et al., 2015</xref>). This approach has two main benefits: first, it improves the experimental designs and protocols with expert input prior to performing the experiments; second, by removing the possibility that the results of the experiments will influence the peer review process, it avoids certain biases (such as the bias against negative results, and the possibility that referees will accept results that are favorable to their point of view and reject results that are not; <xref ref-type="bibr" rid="bib2">Chambers, 2013</xref>; <xref ref-type="bibr" rid="bib7">Nosek and Lakens, 2014</xref>). Cumulatively, these safeguards maximize rigor, but they do not eliminate the possibility of error.</p></sec><sec id="s2"><title>What qualifies as the &quot;same&quot; results?</title><p>There is no straightforward answer to the question &quot;what counts as a successful replication of an original result?&quot; (<xref ref-type="bibr" rid="bib9">Open Science Collaboration, 2015</xref>; <xref ref-type="bibr" rid="bib13">Valentine et al., 2011</xref>). However, asking the following questions will provide some insight: Does the replication produce a statistically significant effect in the same direction as the original? Is the effect size in the replication similar to the effect size in the original? Does the original effect size fall within the confidence or prediction interval of the replication (and vice versa)? Does a meta-analytic combination of results from the original experiment and the replication yield a statistically significant effect? And do the results of the original experiment and the replication appear to be consistent?</p><p>Scientific claims gain credibility by accumulating evidence from multiple experiments, and a single study cannot provide conclusive evidence for or against a claim. Equally, a single replication cannot make a definitive statement about the original finding. However, the new evidence provided by a replication can increase or decrease confidence in the reproducibility of the original finding. When a replication &quot;fails&quot; it can spur productive theorizing about the source of that irreproducibility. For example, it could be that the experimental model did not behave as expected (for example, the rate of tumor onset observed in the replication might be higher than the rate observed in the original research in both the control and experimental conditions). In such circumstances, the original hypothesis and finding may not have been evaluated directly because the experimental circumstances necessary to test them did not recur. Alternatively, the model might have behaved as expected, but the experimental intervention did not result in an effect similar to the original study. These two scenarios have different implications for hypothesizing the underlying causes of irreproducibility, and for deciding on the follow-up investigations that are needed to establish reproducibility.</p></sec><sec id="s3"><title>Conclusion</title><p>Replication is a core value of science, and the credibility of scientific claims is based on their reproducibility rather than the authority of their originators. As part of the Reproducibility Project: Cancer Biology the results of all the Replication Studies will be combined to gain insight into the factors that lead to irreproducible results and the opportunities for improving reproducibility (<xref ref-type="bibr" rid="bib5">Errington et al., 2014</xref>). The results of the first set of Replication Studies suggest that there is a substantial opportunity to improve reproducibility in cancer biology: the challenge facing all of us is to identify how best to achieve this goal.</p></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The Reproducibility Project: Cancer Biology is funded by the Laura and John Arnold Foundation.</p></ack><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Button</surname><given-names>KS</given-names></name><name><surname>Ioannidis</surname><given-names>JP</given-names></name><name><surname>Mokrysz</surname><given-names>C</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Flint</surname><given-names>J</given-names></name><name><surname>Robinson</surname><given-names>ES</given-names></name><name><surname>Munafò</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Power failure: why small sample size undermines the reliability of neuroscience</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>365</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1038/nrn3475</pub-id><pub-id pub-id-type="pmid">23571845</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Registered reports: a new publishing initiative at Cortex</article-title><source>Cortex</source><volume>49</volume><fpage>609</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2012.12.016</pub-id><pub-id pub-id-type="pmid">23347556</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1969">1969</year><source>Statistical Power Analysis for the Behavioral Sciences</source><publisher-loc>New York</publisher-loc><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebersole</surname><given-names>CR</given-names></name><name><surname>Alaei</surname><given-names>R</given-names></name><name><surname>Atherton</surname><given-names>OE</given-names></name><name><surname>Bernstein</surname><given-names>MJ</given-names></name><name><surname>Brown</surname><given-names>M</given-names></name><name><surname>Chartier</surname><given-names>CR</given-names></name><name><surname>Chung</surname><given-names>LY</given-names></name><name><surname>Hermann</surname><given-names>AD</given-names></name><name><surname>Joy-Gaba</surname><given-names>JA</given-names></name><name><surname>Line</surname><given-names>MJ</given-names></name><name><surname>Rule</surname><given-names>NO</given-names></name><name><surname>Sacco</surname><given-names>DF</given-names></name><name><surname>Vaughn</surname><given-names>LA</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Observe, hypothesize, test, repeat: Luttrell, Petty and Xu (2017) demonstrate good science</article-title><source>Journal of Experimental Social Psychology</source><volume>69</volume><fpage>184</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.jesp.2016.12.005</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Gunn</surname><given-names>W</given-names></name><name><surname>Tan</surname><given-names>FE</given-names></name><name><surname>Lomax</surname><given-names>J</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An open investigation of the reproducibility of cancer biology research</article-title><source>eLife</source><volume>3</volume><elocation-id>e04333</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04333</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>WC</given-names></name><name><surname>Su</surname><given-names>Y</given-names></name><name><surname>Kuhn</surname><given-names>I</given-names></name><name><surname>Polyak</surname><given-names>K</given-names></name><name><surname>Bissell</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sorting out the FACS: a devil in the details</article-title><source>Cell Reports</source><volume>6</volume><fpage>779</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2014.02.021</pub-id><pub-id pub-id-type="pmid">24630040</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Lakens</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Registered reports: A method to increase the credibility of published results</article-title><source>Social Psychology</source><volume>45</volume><fpage>137</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1027/1864-9335/a000192</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Alter</surname><given-names>G</given-names></name><name><surname>Banks</surname><given-names>GC</given-names></name><name><surname>Borsboom</surname><given-names>D</given-names></name><name><surname>Bowman</surname><given-names>SD</given-names></name><name><surname>Breckler</surname><given-names>SJ</given-names></name><name><surname>Buck</surname><given-names>S</given-names></name><name><surname>Chambers</surname><given-names>CD</given-names></name><name><surname>Chin</surname><given-names>G</given-names></name><name><surname>Christensen</surname><given-names>G</given-names></name><name><surname>Contestabile</surname><given-names>M</given-names></name><name><surname>Dafoe</surname><given-names>A</given-names></name><name><surname>Eich</surname><given-names>E</given-names></name><name><surname>Freese</surname><given-names>J</given-names></name><name><surname>Glennerster</surname><given-names>R</given-names></name><name><surname>Goroff</surname><given-names>D</given-names></name><name><surname>Green</surname><given-names>DP</given-names></name><name><surname>Hesse</surname><given-names>B</given-names></name><name><surname>Humphreys</surname><given-names>M</given-names></name><name><surname>Ishiyama</surname><given-names>J</given-names></name><name><surname>Karlan</surname><given-names>D</given-names></name><name><surname>Kraut</surname><given-names>A</given-names></name><name><surname>Lupia</surname><given-names>A</given-names></name><name><surname>Mabry</surname><given-names>P</given-names></name><name><surname>Madon</surname><given-names>T</given-names></name><name><surname>Malhotra</surname><given-names>N</given-names></name><name><surname>Mayo-Wilson</surname><given-names>E</given-names></name><name><surname>McNutt</surname><given-names>M</given-names></name><name><surname>Miguel</surname><given-names>E</given-names></name><name><surname>Paluck</surname><given-names>EL</given-names></name><name><surname>Simonsohn</surname><given-names>U</given-names></name><name><surname>Soderberg</surname><given-names>C</given-names></name><name><surname>Spellman</surname><given-names>BA</given-names></name><name><surname>Turitto</surname><given-names>J</given-names></name><name><surname>VandenBos</surname><given-names>G</given-names></name><name><surname>Vazire</surname><given-names>S</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name><name><surname>Wilson</surname><given-names>R</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Promoting an open research culture</article-title><source>Science</source><volume>348</volume><fpage>1422</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1126/science.aab2374</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Estimating the reproducibility of psychological science</article-title><source>Science</source><volume>349</volume><elocation-id>aac4716</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id><pub-id pub-id-type="pmid">26315443</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>From bench to cageside: Risk assessment for rodent pathogen contamination of cells and biologics</article-title><source>ILAR Journal</source><volume>49</volume><fpage>310</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1093/ilar.49.3.310</pub-id><pub-id pub-id-type="pmid">18506064</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Shall we really do it again? The powerful concept of replication is neglected in the social sciences</article-title><source>Review of General Psychology</source><volume>13</volume><fpage>90</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1037/a0015108</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>JP</given-names></name><name><surname>Nelson</surname><given-names>LD</given-names></name><name><surname>Simonsohn</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant</article-title><source>Psychological Science</source><volume>22</volume><fpage>1359</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1177/0956797611417632</pub-id><pub-id pub-id-type="pmid">22006061</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>JC</given-names></name><name><surname>Biglan</surname><given-names>A</given-names></name><name><surname>Boruch</surname><given-names>RF</given-names></name><name><surname>Castro</surname><given-names>FG</given-names></name><name><surname>Collins</surname><given-names>LM</given-names></name><name><surname>Flay</surname><given-names>BR</given-names></name><name><surname>Kellam</surname><given-names>S</given-names></name><name><surname>Mościcki</surname><given-names>EK</given-names></name><name><surname>Schinke</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Replication in prevention science</article-title><source>Prevention Science</source><volume>12</volume><fpage>103</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1007/s11121-011-0217-6</pub-id><pub-id pub-id-type="pmid">21541692</pub-id></element-citation></ref></ref-list></back></article>