<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">34467</article-id><article-id pub-id-type="doi">10.7554/eLife.34467</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Uncovering temporal structure in hippocampal output patterns</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-103721"><name><surname>Maboudi</surname><given-names>Kourosh</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9675-4031</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-103720"><name><surname>Ackermann</surname><given-names>Etienne</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-111035"><name><surname>de Jong</surname><given-names>Laurel Watkins</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-76148"><name><surname>Pfeiffer</surname><given-names>Brad E</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-92565"><name><surname>Foster</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-16654"><name><surname>Diba</surname><given-names>Kamran</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5128-4478</contrib-id><email>kdiba@umich.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-103035"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2054-0234</contrib-id><email>caleb.kemere@rice.edu</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Departmentof Anesthesiology</institution><institution>University of Michigan</institution><addr-line><named-content content-type="city">Ann Arbor</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychology</institution><institution>University of Wisconsin-Milwaukee</institution><addr-line><named-content content-type="city">Milwaukee</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Electrical and Computer Engineering</institution><institution>Rice University</institution><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Neuroscience</institution><institution>University of Texas Southwestern</institution><addr-line><named-content content-type="city">Dallas</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of Psychology</institution><institution>University of California, Berkeley</institution><addr-line><named-content content-type="city">Berkeley</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Helen Wills Neuroscience Institute</institution><institution>University of California, Berkeley</institution><addr-line><named-content content-type="city">Berkeley</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-16452"><name><surname>Colgin</surname><given-names>Laura</given-names></name><role>Reviewing Editor</role><aff id="aff7"><institution>The University of Texas at Austin, Center for Learning and Memory</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="equal" id="fn1"><p>[1]These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>05</day><month>06</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e34467</elocation-id><history><date date-type="received" iso-8601-date="2017-12-20"><day>20</day><month>12</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-05-14"><day>14</day><month>05</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Maboudi et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Maboudi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-34467-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.34467.001</object-id><p>Place cell activity of hippocampal pyramidal cells has been described as the cognitive substrate of spatial memory. Replay is observed during hippocampal sharp-wave-ripple-associated population burst events (PBEs) and is critical for consolidation and recall-guided behaviors. PBE activity has historically been analyzed as a phenomenon subordinate to the place code. Here, we use hidden Markov models to study PBEs observed in rats during exploration of both linear mazes and open fields. We demonstrate that estimated models are consistent with a spatial map of the environment, and can even decode animals’ positions during behavior. Moreover, we demonstrate the model can be used to identify hippocampal replay without recourse to the place code, using only PBE model congruence. These results suggest that downstream regions may rely on PBEs to provide a substrate for memory. Additionally, by forming models independent of animal behavior, we lay the groundwork for studies of non-spatial memory.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>hippocampus</kwd><kwd>replay</kwd><kwd>sharp wave ripples</kwd><kwd>hidden Markov models</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>IOS-1550994</award-id><principal-award-recipient><name><surname>Ackermann</surname><given-names>Etienne</given-names></name><name><surname>Kemere</surname><given-names>Caleb</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGY0088</award-id><principal-award-recipient><name><surname>Ackermann</surname><given-names>Etienne</given-names></name><name><surname>Kemere</surname><given-names>Caleb</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Ken Kennedy Institute</institution></institution-wrap></funding-source><award-id>ERIT</award-id><principal-award-recipient><name><surname>Kemere</surname><given-names>Caleb</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01MH109170</award-id><principal-award-recipient><name><surname>Maboudi</surname><given-names>Kourosh</given-names></name><name><surname>Diba</surname><given-names>Kamran</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01MH085823</award-id><principal-award-recipient><name><surname>Pfeiffer</surname><given-names>Brad E</given-names></name><name><surname>Foster</surname><given-names>David</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000879</institution-id><institution>Alfred P. Sloan Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Pfeiffer</surname><given-names>Brad E</given-names></name><name><surname>Foster</surname><given-names>David</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000874</institution-id><institution>Brain and Behavior Research Foundation</institution></institution-wrap></funding-source><award-id>NARSAD Young Investigator Grant</award-id><principal-award-recipient><name><surname>Pfeiffer</surname><given-names>Brad E</given-names></name><name><surname>Foster</surname><given-names>David</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000872</institution-id><institution>McKnight Endowment Fund for Neuroscience</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Foster</surname><given-names>David</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>CBET-1351692</award-id><principal-award-recipient><name><surname>Ackermann</surname><given-names>Etienne</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Machine learning models of coordinated hippocampal ensemble activity during sharp wave ripple activity encode structure that mirrors the place cell map expressed during exploration, and enable a new paradigm for analyzing and understanding this offline activity.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Large populations of neurons fire in tandem during hippocampal sharp-waves and their accompanying CA1 layer ripple oscillations (<xref ref-type="bibr" rid="bib5">Buzsáki, 1986</xref>). By now, multiple studies have shown that during many sharp-wave ripple-associated PBEs, hippocampal ‘place cells’ (<xref ref-type="bibr" rid="bib44">O'Keefe and O’Keefe, 1976</xref>) fire in compressed sequences that reflect the firing order determined by the sequential locations of their individual place fields (<xref ref-type="bibr" rid="bib18">Diba and Buzsáki, 2007</xref>; <xref ref-type="bibr" rid="bib23">Foster and Wilson, 2006</xref>; <xref ref-type="bibr" rid="bib40">Lee and Wilson, 2002</xref>; <xref ref-type="bibr" rid="bib43">Nádasdy et al., 1999</xref>). While the firing patterns during active exploration are considered to represent the brain’s global positioning system and provide a substrate for spatial and episodic memory, instead it is the synchronized activity during PBEs that is most likely to affect cortical activity beyond the hippocampus (<xref ref-type="bibr" rid="bib6">Buzsáki, 1989</xref>; <xref ref-type="bibr" rid="bib8">Carr et al., 2011</xref>; <xref ref-type="bibr" rid="bib19">Diekelmann and Born, 2010</xref>; <xref ref-type="bibr" rid="bib51">Siapas and Wilson, 1998</xref>). Likewise, widespread activity modulation is seen throughout the brain following these sharp-wave ripple population bursts (<xref ref-type="bibr" rid="bib41">Logothetis et al., 2012</xref>).</p><p>The literature on PBEs has largely focused on developing templates of firing patterns during active behavior and evaluating the extent to which these templates’ patterns are reprised during subsequent PBE. But what if the fundamental mode of the hippocampus is not the re-expression of place fields, but rather the PBE sequences during SWR? PBE sequences are enhanced during exploration of novel environments (<xref ref-type="bibr" rid="bib12">Cheng and Frank, 2008</xref>; <xref ref-type="bibr" rid="bib23">Foster and Wilson, 2006</xref>), they presage learning-related changes in place fields (<xref ref-type="bibr" rid="bib20">Dupret et al., 2010</xref>), and appear to be critical to task learning (<xref ref-type="bibr" rid="bib21">Ego-Stengel and Wilson, 2010</xref>; <xref ref-type="bibr" rid="bib26">Girardeau et al., 2009</xref>; <xref ref-type="bibr" rid="bib31">Jadhav et al., 2012</xref>). Here, we examine the information provided by CA1 and CA3 pyramidal neurons, the output nodes of the hippocampus, through the looking glass of PBE firing patterns.</p><p>We developed a technique to build models of PBE sequences strictly outside of active exploration and independent of place fields and demonstrate that this nevertheless allows us to uncover spatial maps. Furthermore, these models can be used to detect congruent events that are consistent with replay but without any explicit place cell template. Our technique therefore provides new possibilities for evaluating hippocampal output patterns in single-trial and other fast learning paradigms, where a reliable sequential template pattern is not readily available. Overall, our work suggests that a sequence-first approach can provide an alternative view of hippocampal activity that may shed new light on how memories are formed, stored, and recalled.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Awake population burst events</title><p>We began by analyzing the activity of large numbers of individual neurons in areas CA1 and CA3 of the dorsal hippocampus as rats navigated linear mazes for water reward (linear track: <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> rats, <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula> sessions; previously used by [<xref ref-type="bibr" rid="bib18">Diba and Buzsáki, 2007</xref>]). Using pooled multiunit activity, we detected PBEs during which many neurons were simultaneously active. The majority of these events occurred when animals paused running (speed <inline-formula><mml:math id="inf3"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> cm/s, corresponding to <inline-formula><mml:math id="inf4"><mml:mrow><mml:mn>54.0</mml:mn><mml:mi>%</mml:mi><mml:mo>±</mml:mo><mml:mn>20.1</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></inline-formula> sd of events) to obtain reward, groom, or survey their surroundings (<xref ref-type="bibr" rid="bib4">Buzsáki et al., 1983</xref>), and were accompanied by SWR complexes, distinguished by a burst of oscillatory activity in the 150–250 Hz band of the CA1 LFP. Because we are interested in understandinginternally generated activity during PBEs, we included only these periods without active behavior, ensuring that theta sequences would not bias our results. While we identified active behavior using a speed criterion, we found similar results when we instead used a theta-state detection approach (not shown). We did not add any other restrictions on behavior, LFP, or the participation of place cells. We found that inactive PBEs occupied an average of 1.8% of the periods during which animals were on the linear track (<inline-formula><mml:math id="inf5"><mml:mrow><mml:mn>16.9</mml:mn><mml:mo>±</mml:mo><mml:mn>15.1</mml:mn></mml:mrow></mml:math></inline-formula> s of <inline-formula><mml:math id="inf6"><mml:mrow><mml:mn>832.6</mml:mn><mml:mo>±</mml:mo><mml:mn>390.5</mml:mn></mml:mrow></mml:math></inline-formula> s). In comparison, classical Bayesian approaches to understand PBE activity require the 34.8% of time animals are running (speed &gt;10 cm/s) on the track (<inline-formula><mml:math id="inf7"><mml:mrow><mml:mn>254.4</mml:mn><mml:mo>±</mml:mo><mml:mn>106.6</mml:mn></mml:mrow></mml:math></inline-formula> s of<inline-formula><mml:math id="inf8"><mml:mrow><mml:mn>832.6</mml:mn><mml:mo>±</mml:mo><mml:mn>390.5</mml:mn></mml:mrow></mml:math></inline-formula> s) to build models of place fields.</p></sec><sec id="s2-2"><title>Learning hidden Markov models from PBE data</title><p>Activity during PBEs is widely understood to be internally-generated in the hippocampal-entorhinal formation, and likely to affect neuronal firing in downstream regions (<xref ref-type="bibr" rid="bib6">Buzsáki, 1989</xref>; <xref ref-type="bibr" rid="bib14">Chrobak and Buzsáki, 1996</xref>; <xref ref-type="bibr" rid="bib41">Logothetis et al., 2012</xref>; <xref ref-type="bibr" rid="bib54">Yamamoto and Tonegawa, 2017</xref>). Given the prevalence of PBEs during an animal’s early experience, we hypothesized that the neural activity during these events would be sufficient to train a machine learning model of sequential patterns—a hidden Markov model—and that thismodel would capture the relevant spatial information encoded in the hippocampus independent of exploration itself.</p><p>Hidden Markov models have been very fruitfully used to understand sequentially structured data in a variety of contexts. A hidden Markov model captures information about data in two ways. First, it clusters observations into groups (‘states’) with shared patterns. In our case, this corresponds to finding time bins in which the same sets of neurons are co-active. This is equivalent to reducing the dimension of the ensemble observations into a discretized latent space or manifold. Second, it models the dynamics of state transitions. This model is Markovian because it is assumed that the probability to transition to the next state only depends on the current state. Critically, these operations of clustering and sequence modeling are jointly optimized, allowing the structure of ensemble firing corresponding to each of the final states to combine information over many observations. Given the role of the hippocampus in memory, in our HMMs, the unobserved latent variable presumably corresponds to the temporal evolution of a memory trace that is represented by co-active ensembles of CA1 and CA3 neurons. The full model will correspond to the structure which connects all the memory traces activated during PBEs.</p><p>The parameters of our model that are fit to data include the observation model (the cluster descriptions, or predicted activity of each excitatory neuron within the CA1/CA3 ensemble for a given state), the state transition model (the probability that the CA1/CA3 ensemble will transition from a start state to a destination state in the next time bin), and the initial state distribution (the probability for sequences to start in each given state). In prior work using HMMs to model neural activity, a variety of statistical distributions have been used to characterize ensemble firing during a specific state (the observation model, (<xref ref-type="bibr" rid="bib11">Chen and Wilson, 2017</xref>; <xref ref-type="bibr" rid="bib10">Chen et al., 2012</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2014</xref>; <xref ref-type="bibr" rid="bib17">Deppisch et al., 1994</xref>; <xref ref-type="bibr" rid="bib36">Kemere et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Radons et al., 1994</xref>). We opted for the Poisson distribution to minimize the number of parameters per state and per neuron (see Materials and methods). We used the standard iterative EM algorithm (<xref ref-type="bibr" rid="bib48">Rabiner, 1989</xref>) to learn the parameters of an HMM from binned PBE data (20 ms bins). <xref ref-type="fig" rid="fig1">Figure 1</xref> depicts the resultant state transition matrix and observation model for an example linear-track session.</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.34467.002</object-id><label>Figure 1.</label><caption><title>A hidden Markov model of ensemble activity during PBEs.</title><p>A hidden Markov model of ensemble activity during PBEs. (<bold>a</bold>) Examples of three PBEs and a run epoch. (<bold>b</bold>) Spikes during seven example PBEs (top) and their associated (30 state HMM-decoded) latent space distributions (bottom). The place cells are ordered by their place fields on the track, whereas the non-place cells are unordered. The latent states are ordered according to the peak densities of the lsPFs (lsPFs, see Materials and methods). (<bold>c</bold>) The transition matrix models the dynamics of the unobserved internally-generated state. The sparsity and banded-diagonal shape are suggestive of sequential dynamics. (<bold>d</bold>) The observation model of our HMM is a set of Poisson probability distributions (one for each neuron) for each hidden state. Looking across columns (states), the mean firing rate is typically elevated for only a few of the neurons and individual neurons have elevated firing rates for only a few states.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig1-v2"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Hidden Markov models capture state dynamics beyond pairwise co-firing.</title><p>Actual cross-validated test data and surrogate test data evaluated in actual-data-optimized HMMs for all 18 linear track sessions. For each session, we performed five-fold cross validation to score the validation (=test) set in an HMM that was learned on the corresponding training set. In addition, two surrogate datasets of the validation data (obtained by either temporal shuffle or time-swap shuffle) were scored in the same HMM as the actual validation data. <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> shuffles of each event and of each type were performed. (<bold>a</bold>) Difference between the data log likelihoods of actual and time-swap surrogate test events, evaluated in the actual train-data-optimized models. (<bold>b</bold>) Same as in (<bold>a</bold>), except that the differences between the actual data and the temporal surrogates are shown. For each of the <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula> sessions, the actual test data had a significantly higher likelihood than either of the shuffled counterparts (<inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Wilcoxon signed-rank test). Sessions are arranged first by animal, and then by number of PBEs, in decreasing order.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig1-figsupp1-v2"/></fig></fig-group><p>Using separate training- and test-datasets (cross-validation) mitigates over-fitting to training data, but it is still possible for the cross-validated goodness-of-fit to increase with training without any underlying dynamics, e.g., if groups of neurons tend to activate in a correlated fashion. Does the model we have learned reflect underlying sequential structure of memory traces beyond pairwise co-firing? To answer this question, we cross-validated the model against both real ‘test’ data and against surrogate ‘test’ data derived from shuffling each PBE in two ways: one in which the binned spiking activity was circularly permuted across time for each neuron independently of the other neurons (‘temporal shuffle’, which removes co-activation), and one in which the order of the binned data was scrambled coherently across all neurons (‘time-swap’, which maintains co-activation). Note that the second shuffle preserves pairwise correlations while removing the order of any sequential patterns that might be present. Using five-fold cross-validation, we compared learned models against both actual and surrogate test data and found that the model likelihood was significantly greater for real data (vs. temporal shuffle, <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, vs. time-swap, <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula> sessions, Wilcoxon signed-rank test, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p></sec><sec id="s2-3"><title>What do the learned model parameters tell us about PBEs?</title><p>To begin to understand what structure we learn from PBE activity, we compared our HMMs (trained on real data) against models trained on multiple different surrogate datasets (<xref ref-type="fig" rid="fig2">Figure 2a,b</xref>). These surrogate datasets were obtained from actual data following: (1) temporal shuffles and (2) time-swaps, as above, and (3) by producing a surrogate PBE from independent Poisson simulations according to each unit’s mean firing rate within the original PBEs. First, we investigated the sparsity of the transition matrices using the Gini coefficient (see Materials and methods and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). A higher Gini coefficient corresponds to higher sparsity. Strikingly, the actual data yielded models in which the state transition matrix was sparser than in each of the surrogate counterparts (<inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig2">Figure 2c</xref>), reflecting that each state transitions only to a few other states. Thus, intricate yet reliable details are captured by the HMMs. Next, we quantified the sparsity of the observation model. We found that actual data yielded mean firing rates which were highly sparse (<xref ref-type="fig" rid="fig2">Figure 2d</xref>), indicating that individual neurons were likely to be active during only a small fraction of the states. Using a graph search algorithm (see Materials and methods), we simulated paths through state space generated by these transition matrices, and found that this increased sparsity accompanied longer trajectories (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>) through the state space of the model. Thus, the state transition matrices we learn are suggestive of dynamics in which each sparse state is preceded and followed by only a few other, in turn, sparse states, providing long sequential paths through state space-consistent with spatial relationships in the environment in which the animal was behaving, but generated from PBEs. The increased sparsity of the observation model and transition matrix in the example session was representative of a significant increase over all remaining sessions (<inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula> sessions, Wilcoxon signed-rank tests, <xref ref-type="fig" rid="fig2">Figure 2e,f</xref>).</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.34467.004</object-id><label>Figure 2.</label><caption><title>A hidden Markov model of ensemble activity during population burst events.</title><p>Models of PBE activity are sparse. We trained HMMs on neural activity during PBEs (in 20 ms bins), as well as on surrogate transformations of those PBEs. (<bold>a</bold>) (top) The transition matrices for the actual and surrogate PBE models with states ordered to maximize the transition probability from state <inline-formula><mml:math id="inf18"><mml:mi>i</mml:mi></mml:math></inline-formula> to state <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. (bottom) Undirected connectivity graphs corresponding to the transition matrices. The nodes correspond to states (progressing clockwise, starting at the top). The weights of the edges are proportional to the transition probabilities between the nodes (states). The transition probabilities from state i to every other state except<inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> are shown in the interior of the graph, whereas for clarity, transition probabilities from state <inline-formula><mml:math id="inf21"><mml:mi>i</mml:mi></mml:math></inline-formula> to itself, as well as to neighboring state <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> are shown between the inner and outer rings of nodes (the nodes on the inner and outer rings represent the same states). (<bold>b</bold>) The observation matrices for actual and surrogate PBE models show the mean firing rate for neurons in each state. For visualization, neurons are ordered by their firing rates. (<bold>c</bold>) We quantified the sparsity of transitions from one state to all other states using the Gini coefficient of rows of the transition matrix for the example session in (<bold>a</bold>). Actual data yielded sparser transition matrices than shuffles. (<bold>d</bold>) The observation models—each neuron’s expected activity for each state—learned from actual data for the example session are significantly sparser than those learned after shuffling. This implies that as the hippocampus evolves through the learned latent space, each neuron is activeduring only a few states. (<bold>e</bold>) Summary of transition matrix sparsity and f. Observation model sparsity with corresponding shuffle data pooled over all sessions/animals. (***: <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, *: <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>; single session comparisons: <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>250</mml:mn></mml:mrow></mml:math></inline-formula>realizations, Welch’s t-test; aggregated comparisons - <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula> sessions, Wilcoxon signed-rank test).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.005</object-id><label>Figure 2—figure supplement 1.</label><caption><title>PBE model states typically only transition to a few other states.</title><p>We trained HMMs on neural activity during PBE (in 20 ms bins), and asked how sparse the resulting state transitions were. In particular, we calculated the Gini coefficient for each row of our state transition matrix, so that the Gini coefficient for a particular row reflects the sparsity of state transitions from that state (row) to all other states (so-called ‘departure sparsity’). A high (closeto one) Gini coefficient implies that the state is likely to only transition to a few other states, whereas a low (close to zero) Gini coefficient implies that the state is likely to transition to many other states. For each transition matrix, we computed the mean departure sparsity for <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>250</mml:mn></mml:mrow></mml:math></inline-formula> initializations, and for <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>250</mml:mn></mml:mrow></mml:math></inline-formula> shuffled counterparts for each of the surrogate datasets (<bold>a</bold>) time-swap shuffle, (<bold>b</bold>) temporal shuffle, (<bold>c</bold>) Poisson surrogate), and in each case we show the difference between the actual test data, and the surrogate test data. The actual data are significantly more sparse than both the temporal and time-swap surrogates for all sessions (<inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test) and significantly more sparse than the Poisson surrogate for 14 of the 18 sessions (<inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig2-figsupp1-v2"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.006</object-id><label>Figure 2—figure supplement 2.</label><caption><title>Each neuron is active in only a few model states.</title><p>Using the same PBE models and surrogate datasets (<inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>250</mml:mn></mml:mrow></mml:math></inline-formula> shuffles each) as in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, we investigated the sparse participation of neurons/units in our models by calculating the Gini coefficient of each row (that is, for each unit) of the observation matrix. A high Gini coefficient implies that the unit is active in only a small number of states, whereas a low Gini coefficient implies that the unit is active in many states. For each initialization/shuffle, we calculate the mean Gini coefficient over all units, and the differences between those obtained using actual data and those obtained using surrogate data are shown: differences between actual and (<bold>a</bold>) time-swap, (<bold>b</bold>) temporal, and (<bold>c</bold>) Poisson surrogates. We find that the actual data are significantly more sparse than the temporal and Poisson surrogates for most of the sessions (p&lt;0.001, Mann–Whitney <italic>U</italic> test), but that for many (10 out of 18) sessions, there is no significant difference between the mean row-wise observation sparsity of the actual data compared to the time-swap surrogate. This is an expected result, since the time-swap shuffle leaves the observation matrix largely unchanged.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig2-figsupp2-v2"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.007</object-id><label>Figure 2—figure supplement 3.</label><caption><title>The sparse transitions integrate into long sequences through the state space.</title><p>We calculated the longest path within an unweighted directed graph corresponding to the transition matrices of HMMs, with nodes representing states and edges reflecting the transition probabilities (see Materials and methods). (<bold>a</bold>) The graph—displayed using the ‘force-directed layout’ (<xref ref-type="bibr" rid="bib25">Fruchterman and Reingold, 1991</xref>)—represents a model trained on actual data. For illustration purposes, we ignored transition probabilities below 0.1. The green path shows the longest path in the example. (<bold>b</bold>) For this example session, we computed the maximum path length (the number of nodes in the longest path) for actual and corresponding shuffle datasets (temporal, time-swap, and Poisson) (<inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>250</mml:mn></mml:mrow></mml:math></inline-formula>initializations/shuffles). (<bold>c</bold>) The panel shows aggregate results built of median maximum path lengths from all sessions. We find that the actual data results in longer paths compared to time-swap (<inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.008</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test) and temporal surrogate datasets (<inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test). On the contrary, no significant difference is found in comparison with the Poisson datasets (<inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.57</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test). Nevertheless, due to non-sparseness of the observation matrix for a Poisson model (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), in most instances these paths correspond to highly overlapping ensemble sequences. In panels (<bold>d–f</bold>), difference between maximum path lengths obtained from actual data and surrogate datasets are shown separately for all sessions: actual versus (<bold>d</bold>) time-swap, (<bold>e</bold>) temporal, and (<bold>f</bold>) Poisson. The data results in longer paths compared to time-swap and temporal shuffle datasets in most sessions (15 out of 18) (<inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test), though in only five sessions compared to Poisson surrogate datasets.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-34467-fig2-figsupp3-v2"/></fig></fig-group><p>These observations indicate that PBEs inform an HMM about extant spatial relationships within the environment. So, next we asked how the firing patterns of neurons during actual behavior project into the learned latent spaces. To observe the evolution of the latent states during behavior, we used our model to determine the most likely sequence of latent states corresponding to decode the neural activity observed in 100 ms bins during epochs that displayed strong theta oscillations (exclusive of PBEs) when rats were running (speed &gt;10 cm/s; see Materials and methods). If the learned model was distinct from ensemble patterns during behavior, we might expect the resulting state space probability distributions at each point in time to be randomly spread among multiple states. Instead, we found distributions that resembled sequential trajectories through the latent space (<xref ref-type="fig" rid="fig3">Figure 3a</xref>) in parallel with the physical trajectories made by the animal along the track, further demonstrating that the latent state dynamics learned from PBEs corresponds to an internalized model of physical space.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.34467.008</object-id><label>Figure 3.</label><caption><title>Latent states capture positional code.</title><p>Latent states capture positional code. (<bold>a</bold>) Using the model parameters estimated from PBEs, we decoded latent state probabilities from neural activity during periods when the animal was running. An example shows the trajectory of the decoded latent state probabilities during six runs across the track. (<bold>b</bold>) Mapping latent state probabilities to associated animal positions yields latent-state place fields (lsPFs) which describe the probability of each state for positions along the track. (<bold>c</bold>) Shuffling the position associations yields uninformative state mappings. (<bold>d</bold>) For an example session, position decoding during run periods through the latent space gives significantly better accuracy than decoding using the shuffled tuning curves. The dotted line shows the animal’s position during intervening non run periods. (<bold>e</bold>) The distribution of position decoding accuracy over all sessions (<inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula>) was significantly greater than chance. (<inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.009</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Latent states capture positional code over wide range of model parameters.</title><p>We investigated to what extent our PBE models encoded information related to the animal’s positional code by learning an additional mapping from the latent-state space to the animal’s position (resulting in a latent-space place field, lsPF), and then using this mapping, we decoded run epochs to position and assessed the decoding accuracy. (<bold>a</bold>) We computed the median position decoding accuracy (via the latent space) for each session on the linear track (<inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula> sessions) using cross validation. In particular, we learned a PBE model for each session, and then using cross validation we learned the latent space to animal position mapping on a training set, and recorded the position decoding accuracy on the corresponding test set by first decoding to the state space using the PBE model, and then mapping the state space to the animal position using the lsPF learned on the training set. The position decoding accuracy was significantly greater than chance for each of the 18 sessions (<inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Wilcoxon signed-rank test). (<bold>b</bold>) For an example session, we calculated the median decoding accuracy as we varied the number of states in our PBE model (<inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> realizations per number of states considered). Gray curves show the individual realizations, and the black curve shows the mean decoding accuracy as a function of the number of states. The decoding accuracy is informative over a very wide range of number of states, and we chose <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> states for the analysis in the main text. (<bold>c</bold>) For the same example session, we show the lsPF for different numbers of states. The lsPF are also informative over a wide range of number of states, suggesting that our analyses are largely insensitive to this particular parameter choice (the number of states). The coloration of the lsPF is only for aesthetic reasons.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig3-figsupp1-v2"/></fig></fig-group><p>To better understand the relationship between the latent space and physical space, we used the latent state trajectories decoded during running to form an estimate of the likelihood of each state as a function of location on the track (see Materials and methods). These lsPF ‘lsPFs’ (lsPF, <xref ref-type="fig" rid="fig3">Figure 3b</xref>) in many ways resembled neuronal place fields and similarly tiled the extent of the track. This spatial localization went away when we re-estimated the lsPF with shuffled positions (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). To quantify how informative the latent states were about position, we used the lsPF to map decoded state sequences to position during running periods (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). In our example session, decoding through the latent space resulted in a median accuracy of 5 cm, significantly greater than the 47 cm obtained from shuffled lsPF (<inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Wilcoxon signed-rank test, <xref ref-type="fig" rid="fig3">Figure 3d</xref>). When we evaluated decoding error over our full set of sessions, we observed a similar result (<inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Wilcoxon signed-rank test, <xref ref-type="fig" rid="fig3">Figure 3e</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). As our method required discretizing the state space, a potential caveat is that the number of latent states is a relevant parameter, which we arbitrarily chose to be 30. However, latent-state place fields were informative of position over a wide range of values of this parameter (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Note that decoding into the latent space and then mapping to position resulted in slightly higher error than simply performing Bayesian decoding on the neural activity during behavior. This suggests that the latent space we learn from PBEs may not capture all the information about space that is present in hippocampal activity during behavior, though this may also reflect the limited number of PBEs from which we can learn.</p></sec><sec id="s2-4"><title>HMM-congruent PBEs capture sequence replay</title><p>We and others have previously described how the pattern of place cell firing during many PBEs recapitulates the order in which they are active when animals run on the track (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). We employed the versatile and widely-used Bayesian decoding method to ascribe a replay score to sequential patterns during PBEs. Briefly, for each PBE, we used place-field maps to estimate a spatial trajectory (an <italic>a posteriori</italic> distribution of positions) in 20 ms bins. We generated surrogate data via a column-cycle shuffle (i.e., a circular shift across positions for each time bin [<xref ref-type="bibr" rid="bib16">Davidson et al., 2009</xref>]) of the <italic>a posteriori</italic> distributions during PBEs. The real and surrogate trajectories were scored (see Materials and methods), and we defined replay events as those for which the score of the actual trajectory was larger than a threshold fraction of the null distribution generated by the surrogate scores. Using this approach, we found that 57% of PBE (1064 of 1883) were identified as replay beyond a threshold of 99% (median across datasets 54.2%, <inline-formula><mml:math id="inf45"><mml:mrow><mml:mtext>interquartile range</mml:mtext><mml:mo>=</mml:mo><mml:mn>32.8</mml:mn></mml:mrow></mml:math></inline-formula>–<inline-formula><mml:math id="inf46"><mml:mrow><mml:mn>61.0</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Thus, as has been reported many times (<xref ref-type="bibr" rid="bib16">Davidson et al., 2009</xref>; <xref ref-type="bibr" rid="bib18">Diba and Buzsáki, 2007</xref>; <xref ref-type="bibr" rid="bib23">Foster and Wilson, 2006</xref>; <xref ref-type="bibr" rid="bib34">Karlsson and Frank, 2009</xref>), only a fraction of PBEs (but many more than expected by chance) represent statistically significant replay. Given that we use all PBEs for model learning and our models capture the structure of the environment and the patterns expressed by place cells during exploration, we were interested in understanding whether we could also use our latent-space models to find these replay events. Indeed, for many events when we decode trajectories through state space, they resemble the sequential patterns observed when we decode position using Bayesian techniques and the place cell map (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, left). However, given previous evidence for replay of environments not recently experienced (<xref ref-type="bibr" rid="bib28">Gupta et al., 2010</xref>; <xref ref-type="bibr" rid="bib34">Karlsson and Frank, 2009</xref>), we hypothesized that some PBEs might contain ensemble neural activity which is unstructured and thus unrelated to the learned model, and that these would correspond to the ‘non-replay’ events found using traditional methods.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.34467.010</object-id><label>Figure 4.</label><caption><title>Replay Events Can Be Detected Via HMM Congruence.</title><p>(<bold>a</bold>) Example PBEs decoded to position using Bayesian decoding. (<bold>b</bold>) (left) Same examples decoded to the latent space using the learned HMM. (right) Examples decoded after shuffling the transition matrix, and (middle) the sequence likelihood using actual and shuffled models. (<bold>c</bold>) Effect of significance threshold on the fraction of events identified as replay using Bayesian decoding and model congruent events using the HMM approach. (<bold>d</bold>) Comparing Bayesian and model-congruence approaches for all PBEs recorded, we find statistically significant agreement in event identification (60.9% agreement, <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1883</mml:mn></mml:mrow></mml:math></inline-formula> events from 18 sessions, <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Fisher’s exact test two sided).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.011</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Number of significant PBEs.</title><p>(<bold>a</bold>) The number of Bayesian significant PBEs, as well as the total number of PBEs are shown for each session (<inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula>) when using a significance threshold of 99%. We find that 57% of PBEs (1064 of 1883) are Bayesian significant at this threshold. When using this same threshold for the model-congruence (HMM) significance testing, we find that only 35% of PBEs (651 of 1883) are model congruent. In order to compare the Bayesian and model-congruence approaches more directly, we therefore lowered the model-congruence threshold to 94.46%, at which point both methods had the same number of significant events (1064 of 1883). (<bold>b</bold>) For each Bayesian significance threshold, we can determine the corresponding model-congruence threshold that would result in the same number of significant PBEs. (<bold>c</bold>) Using the thresholds from (<bold>b</bold>) such that at each point, both Bayesian and model-congruence approaches have the same number of significant PBEs, we calculate the event agreement between the two approaches. We note that our chosen threshold of 57% significant events has among the worst agreement between the two approaches.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig4-figsupp1-v2"/></fig></fig-group><p>To assess how well the pattern of ensemble activity during individual PBEs related to the overall state-space model learned from PBE activity (‘congruence’), we developed a statistical approach for identifying the subset of strongly structured PBEs. Specifically, rather than comparing real and surrogate PBEs, we compared the goodness-of-fit for each event to a null distribution generated via a computationally-efficient manipulation of the transition matrix of the model (<xref ref-type="fig" rid="fig4">Figure 4b</xref>); we row-wise shuffled the non-diagonal elements of the transition matrix to assess whether an individual PBEs is a more ordered sequence through state space than would be expected by chance. Maintaining the diagonal avoids identifying as different from chance sequences which consist of few repeated states, marked by transitions between state <inline-formula><mml:math id="inf50"><mml:mi>i</mml:mi></mml:math></inline-formula> and itself. As described above, the fraction of events identified as replay using Bayesian decoding is strongly tied to how the null-distribution is generated (i.e., what shuffle is used), some secondary criteria (e.g., number of active cells, unit cluster quality, peak firing rate, trajectory ‘jumps’, etc.), and the value of the significance threshold arbitrarily chosen to be 90%, 95%, or 99% of shuffles in different reports. When we combined across datasets, we found that our transition matrix shuffle yielded a null distribution for which a 99% confidence interval identified slightly fewer PBEs as significant than the column-cycle shuffle did for Bayesian decoding (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). To make a principled comparison of Bayesian- and HMM-based replay detection schemes, we fixed the Bayesian-based significance threshold at 99% but selected the significance threshold for the HMM-congruence null distribution so that the fraction of replay events detected would be the same between the two schemes. Following this approach, we found that model-congruent/incongruent PBEs largely overlapped with the replay/non-replay events detected using Bayesian decoding of the place cell map (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). Thus, using only the neural activity during PBEs, without access to any place cell activity, we are remarkably able to detect the sequential patterns typically described as ‘replay’ based only on their consistency with the structure of other PBE activity.</p><p>There were, however, also differences between the Bayesian and HMM-congruent approaches, including events that reached significance in one but not the other formalism. We wanted to understand where and why these approaches differed in identifying significant sequences. When we examined individual PBEs, we found sequences for which both Bayesian and model-congruence replay detection approaches appeared to malfunction (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). This was not a failure of the choice of significance threshold, as for both techniques we found what appeared to be false-negatives (patterns which looked like replay but were not significant) as well as false-positives (patterns which looked noisy but were identified as replay). Thus, in order to quantitatively compare the two approaches, we asked eight humans to visually examine all the PBEs in our database. They were instructed to label as replay PBEs in which the animal’s Bayesian decoded position translated sequentially without big jumps (<xref ref-type="bibr" rid="bib52">Silva et al., 2015</xref>; see Materials and methods).</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.34467.012</object-id><label>Figure 5.</label><caption><title>Comparing HMM congruence and Bayesian decoding in replay detection.</title><p>(<bold>a</bold>) Eight examples from one session show that Bayesian decoding and HMM model-congruence can differ in labeling of significant replay events. For each event, spike rasters (ordered by the location of each neuron’s place field) and the Bayesian decoded trajectory are shown. ‘+' (‘-') label corresponds to significant (insignificant) events. (left) Both methods can fail to label events that appear to be sequential as replay and (right) label events replay that appear non-sequential. (<bold>b</bold>) We recruited human scorers to visually inspect Bayesian decoded spike trains and identify putative sequential replay events. Using their identifications as labels, we can define an ROC curve for both Bayesian and HMM model-congruence which shows how detection performance changes as the significance threshold is varied. (inset) Human scorers identify 24% of PBEs as replay. Setting thresholds to match this value results in agreement of 70% between Bayesian and HMM model-congruence. (<bold>c</bold>) Using the same thresholds, we find <inline-formula><mml:math id="inf51"><mml:mrow><mml:mo>≈</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:math></inline-formula>% agreement between algorithmic and human replay identification. (All comparison matrices, <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Fisher’s exact test two-tailed.).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig5-v2"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.013</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Human scoring of PBEs and session quality.</title><p>(<bold>a</bold>) Manual scoring results from eight human scorers (six individuals scored <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1883</mml:mn></mml:mrow></mml:math></inline-formula> events, two individuals scored a subset of <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1423</mml:mn></mml:mrow></mml:math></inline-formula> events). Events were presented to each participant in a randomized order, and individuals were allowed to go back to modify their results before submission. Here, events are ordered according to individual #8’s classifications. (<bold>b</bold>) The model-congruence (HMM) approach appearsto have higher accuracy when the session quality is higher (<inline-formula><mml:math id="inf55"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.17</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>2.9</mml:mn></mml:mrow></mml:math></inline-formula>), which is consistent with our expectation that we need many congruent events in the training set in order to learn a consistent and meaningful model. (<bold>c</bold>) The session quality is strongly correlated with the number of PBEs recorded within a session (<inline-formula><mml:math id="inf57"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.96</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>392.6</mml:mn></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig5-figsupp1-v2"/></fig></fig-group><p>We marked each event as a ‘true’ community replay if it was identified by a majority of scorers (six individuals scored <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1883</mml:mn></mml:mrow></mml:math></inline-formula> events, two individuals scored a subset of <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1423</mml:mn></mml:mrow></mml:math></inline-formula> events, individual scores are shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We calculated an ROC curve which compared the rate of true positive and false positive detections as the significance thresholds for Bayesian and model-congruence approaches were varied (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). A perfect detector would have an AUC of unity. We did not find a significant difference between the AUC of Bayesian decoding and model-congruence (<inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.14</mml:mn></mml:mrow></mml:math></inline-formula>, bootstrap, see Materials and methods). If we select thresholds such that our algorithms yield a similar fraction of significant vs. total events as the 24% denoted by our human scorers, we find that both Bayesian and model-congruence yield agreement of <inline-formula><mml:math id="inf62"><mml:mrow><mml:mo>≈</mml:mo><mml:mn>70</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></inline-formula> labeled events with each other and with human scorers (<xref ref-type="fig" rid="fig5">Figure 5c</xref>).</p><p>Thus, congruence with an HMM trained only on PBEs appears to work as reliably as Bayesian decoding in detecting sequential reactivation of linear track behaviors. However, when we examined individual sessions, we noticed that performance was quite variable. Given that our models are learned only from PBEs, we reasoned that the statistics or structure of the PBEs within each session might yield models which vary in quality depending on the number of recorded units, the number of PBEs detected, and their self-consistency across events. We created a model quality metric by comparing cross-validated learning statistics to models which were learned from shuffled events (see Materials and methods). We found that the performance of model-congruence detection was tied to model quality (<inline-formula><mml:math id="inf63"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.17</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>2.9</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula> sessions, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Model quality, in turn, was highly correlated with the number of PBEs during the session (<inline-formula><mml:math id="inf66"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.96</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>392.6</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula> sessions, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Not surprisingly, the performance of Bayesian decoding relative to human scorers was independent of the quality of the HMM, or the number of PBEs, as the place field model is learned from ensemble neural activity during running. Thus, we find an intriguing contrast—when there is an abundance of PBEs (indicating novelty, learning, hippocampus-dependent planning, etc. [<xref ref-type="bibr" rid="bib7">Buzsáki, 2015</xref>]), even in the absence of repeated experience, replay detection based on PBE activity is highly effective. Conversely, when there are few PBEs (i.e., scenarios in which PBEs are uncorrelated with cognitive function), but an abundance of repeated behavioral trials, Bayesian decoding of these limited events proves more effective.</p></sec><sec id="s2-5"><title>Modeling internally generated activity during open field behavior</title><p>The linear track environment represents a highly-constrained behavior. We therefore asked whether the HMM approach could generalize to more complex environments and behavioral tasks. (<xref ref-type="bibr" rid="bib45">Pfeiffer and Foster, 2013</xref>, <xref ref-type="bibr" rid="bib46">Pfeiffer and Foster, 2015</xref>) had previously recorded activity of CA1 neurons in rats as they explored in a 2 m <inline-formula><mml:math id="inf69"><mml:mo>×</mml:mo></mml:math></inline-formula>2 m open field arena for liquid reward. Briefly, animals were trained to discover which one of 36 liquid reward wells would be the ‘home’ well on a given day. Then, they then were required to alternate between searching for a randomly rewarded well and returning to the home well. Using the place cell map in this task and Bayesian decoding, many PBEs were decoded to trajectories through two-dimensional space that were predictive of behavior and shaped by reward. Using this same dataset, we trained a HMMs on neural activity during PBEs in the open field. Here, we used the same PBE detected previously (<xref ref-type="bibr" rid="bib45">Pfeiffer and Foster, 2013</xref>; <xref ref-type="bibr" rid="bib46">Pfeiffer and Foster, 2015</xref>) which occupied an average of <inline-formula><mml:math id="inf70"><mml:mrow><mml:mn>2.53</mml:mn><mml:mo>±</mml:mo><mml:mn>0.42</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></inline-formula> of the periods during which animals were behaving (<inline-formula><mml:math id="inf71"><mml:mrow><mml:mn>77.91</mml:mn><mml:mo>±</mml:mo><mml:mn>21.16</mml:mn></mml:mrow></mml:math></inline-formula> s out of <inline-formula><mml:math id="inf72"><mml:mrow><mml:mn>3064.86</mml:mn><mml:mo>±</mml:mo><mml:mn>540.26</mml:mn></mml:mrow></mml:math></inline-formula> s). Given the large number of units available in this dataset and the increased behavioral variability in the open field environment compared to the linear track, we chose to estimate HMMs with 50 latent states. The transition matrix and observation model from a sample session are shown in <xref ref-type="fig" rid="fig6">Figure 6a,b</xref>. Despite the complex and varied trajectories displayed by animals, the HMM captured sequential dynamics in PBE activity, as in the 1D case, when we compared learned models against both actual and surrogate test data, we found that the model likelihood was significantly greater for real data (<inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Wilcoxon signed-rank test).</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.34467.014</object-id><label>Figure 6.</label><caption><title>Modeling PBEs in open field.</title><p>Modeling PBEs in open field. (<bold>a</bold>) The transition matrix estimated from activity detected during PBEs in an example session in the open field. (<bold>b</bold>) The corresponding observation model (203 neurons) shows sparsity similar to the linear track. (<bold>c</bold>) Example latent state place fields show spatially-limited elevated activity in two dimensions. (<bold>d</bold>) For an example session, position decoding through the latent space gives significantly better accuracy than decoding using the shuffled latent state place fields. (<bold>e</bold>) Comparing the sparsity of the transition matrices (mean Gini coefficient of the departure probabilities) between the linear track and open field reveals that, as expected, over the sessions we observed, the open field is significantly <italic>less sparse</italic> (<inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>), since the environment is less constrained. (<bold>f</bold>) In contrast, there is not a significant difference between the sparsity of the observation model (mean Gini coefficient of the rows) between the linear track and the open field. Note that the linear track models are sparser than in <xref ref-type="fig" rid="fig2">Figure 2</xref> due to using 50 states rather than 30 to match the open field.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig6-v2"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.015</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Open field PBE model states typically only transition to a few other states.</title><p>Similar to the linear track (one dimensional) case, we find that models learned on actual open field PBE data are significantly more sparse (here showing mean departure sparsity) than their shuffled (<inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> shuffles) counterparts. This is true for each of the <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> open field sessions (<inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test). (<bold>a</bold>) Difference [in departure Gini coefficients] between actual and time-swap test data, (<bold>b</bold>) between actual and temporal test data, and <bold>c</bold>) between actual and Poisson surrogate data.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig6-figsupp1-v2"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.016</object-id><label>Figure 6—figure supplement 2.</label><caption><title>Each neuron is active in only a few model states in the open field.</title><p>(<bold>a</bold>) Difference [in observation sparsity Gini coefficients across states] between actual and time-swap test data, (<bold>b</bold>) between actual and temporal test data, and (<bold>c</bold>) between actual and Poisson surrogate data. Similar to the linear track (one dimensional) case, we find that the observation sparsity across states for actual data are significantly greater than that of both the (<bold>b</bold>) temporal and (<bold>c</bold>) Poisson surrogates (for each session, <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test), and that (<bold>a</bold>) for some sessions, there are no significant differences between the actual and time-swap surrogates.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig6-figsupp2-v2"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.017</object-id><label>Figure 6—figure supplement 3.</label><caption><title>lsPF and position decoding in an open field.</title><p>(<bold>a</bold>) lsPF for 49 of the 50 latent states from an example session. (<bold>b</bold>) (Top) Effect of model-congruence threshold on the number of significant PBEs. (Bottom) Comparison matrix between Bayesian replay detection and our model-congruence approach, where the threshold was chosen to match the total number of significant events pooled over all eight sessions. (<bold>c</bold>) Comparison between number of significant Bayesian events vs number of significant events using our model-congruence approach, when choosing the threshold as in (<bold>b</bold>). Sessions are ordered in decreasing numbers of total PBEs. Note that session one is a significant outlier, causing mismatches between many other sessions (2, 5, 7, 8), suggesting that matching on a per-session basis may be more appropriate in this case. (<bold>d</bold>) Median position decoding error (via the latent space and lsPF) was evaluated using cross-validation in an example session (<inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> realizations for each model considered, shown in gray, mean shown in black), indicating that (i) the PBE-learned latent space encodes underlying spatial information, and (ii) that our PBE models are informative aboutthe underlying position over a wide range of numbers of states.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig6-figsupp3-v2"/></fig><fig id="fig6s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34467.018</object-id><label>Figure 6—figure supplement 4.</label><caption><title>Examples of open field PBEs.</title><p>(<bold>a</bold>) Three example PBEs are shown that were classified as non-significant by both the Bayesian and model-congruence approaches. The top row shows the PBE decoded with place fields using a Bayesian decoder in 20 ms bins, with a 5 ms stride. The bottom row shows the same events, but decoded in 20 ms non-overlapping time bins using the lsPF. (<bold>b</bold>) Three example PBEs are shown that were classified as significant replay by the Bayesian approach, but not by the model-congruence approach. (<bold>c</bold>) Three example PBEs are shown that were classified as significant replay by the model-congruence approach, but not by the Bayesian approach. (<bold>d</bold>) Three example PBEs are shown that were classified as significant by both approaches.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34467-fig6-figsupp4-v2"/></fig></fig-group><p>In the case of the linear track, we linked sparsity of the transition matrix to the sequential nature of behaviors in that environment. An unconstrained, two-dimensional environment permits a much richer repertoire of behavioral trajectories. However, behavior is still constrained by the structure of space—arbitrary teleportation from one location to another is impossible. We found that learning from PBEs in the open field yielded transition matrices (<xref ref-type="fig" rid="fig6">Figure 6a</xref>) that were significantly sparser thanmodels learned from shuffled data (<inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, Wilcoxon signed-rank test, <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> sessions, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). However, consistent with increased freedom of potential behaviors, when we compared the sparsity of models learned from open field acpPBEs with 50-state models learned from PBEs in linear tracks, the open field transition matrices were less sparse (<inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test comparing 8 and 18 sessions, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Likewise, when we examined the observation model for the open field, we found that the activity across states for individual neurons was significantly more sparse than in models learned from shuffled data (<inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, Wilcoxon signed-rank test, <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> sessions, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). The sparsity of linear track and open field observation models were not significantly different (<inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.44</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test).</p><p>Do the latent states learned from PBEs capture spatial information in a 2D environment? We used the PBE-trained model to decode run data, as in the linear track case. We found that the latent states corresponded with specific locations in the open field, as we expected (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). Moreover, we were able to decode animals’ movements with significantly greater than chance accuracy by converting decoded latent states to positions using the lsPF (<inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig6">Figure 6d</xref>). Finally, we examined model-congruency for PBEs detected in the open field. Previously, it was reported that 27.3% (815 of 2980, <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> sessions) were identified as ‘trajectory events’ (<xref ref-type="bibr" rid="bib46">Pfeiffer and Foster, 2015</xref>). We chose a significance threshold to match this fraction (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>) and found that there was significant overlap between the events detected through Bayesian and model-congruence techniques (<inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, Fisher’s exact test). These events overlapped significantly with replay events detected using traditional Bayesian decoding (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>). Thus, an HMM of the activity during population bursts captures the structure of neural activity in two dimensional environments during complex tasks and can be used to decode events consistent with trajectories through that environment.</p></sec><sec id="s2-6"><title>Extra-spatial information</title><p>As described earlier, while we observed a similar fraction of events to be similar by HMM-congruence and Bayesian decoding, there was not an exact event-to-event correspondence. An intriguing potential explanation is that the latent space represented in PBE sequential firing and captured by the HMM is richer than simply the spatial structure of the present environment. In most hippocampal ensemble recording experiments, maze or open field tasks are structured to intentionally map memory elements to spatial behavior, and thus this potential richness is difficult to test. We used two sample datasets to explore the potential of the HMM to capture extra-spatial richness in the PBE sequences.</p><p>First, we considered the possibility that in the awake behaving animal, PBE activity might be sequential reactivation of environments other than the one being explored (‘remote replay’). We reasoned that we could enhance the model’s representation of remote environments by filtering out local replay from the training data. We evaluated how the model-quality of our HMM changed as progressively more sequences labeled as replay by Bayesian decoding were removed from the training data. In the linear track sessions we considered, we found that refining the training data resulted in models that lowered in quality at different rates as the threshold for Bayesian replay was decreased (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Most, but not all, models dropped precipitously in quality: <inline-formula><mml:math id="inf89"><mml:mo>&gt;</mml:mo></mml:math></inline-formula>50% when we removed events detected as Bayesian replay at a 95% threshold, as would be expected if the HMM represented only the local environment. In many outlier sessions in which model quality decreased more slowly, the initial (baseline)model quality was low. Intriguingly, however, in at least one outlier session where model quality decreased slowly with refinement (blue line,<xref ref-type="fig" rid="fig7">Figure 7a</xref>), the initial model quality was still high, and we further noted that position decoding using lsPF yielded relatively high error (blue dot, <xref ref-type="fig" rid="fig7">Figure 7b</xref>). Thus, we wondered whether this and similar sessions might have contained non-local or extra-spatial PBEs that were captured by the HMM.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.34467.019</object-id><label>Figure 7.</label><caption><title>Extra-spatial structure.</title><p>Examples of remote replay events identified with HMM-congruence. We trained and evaluated HMMs on the events that were not Bayesian significant (residual events) to identify potential extra-spatial structure. (<bold>a</bold>) The normalized session quality drops as local-replay events above the Bayesian significance threshold are removed from the data. Each trace corresponds to one of the 18 linear track sessions, with the stroke width and the stroke intensity proportional to the baseline (all-events) session quality. The blue line identifies a session in which model quality drops more slowly, indicating the potential presence of extra-spatial information. The reduction in session quality for a W maze experiment with known extra-spatial information is even slower (green). When, instead, Bayesian-significant <italic>remote</italic> events are removed, rapid reduction in session quality is again revealed (red). (<bold>b</bold>) The lsPF-based median decoding errors are shown as a function of baseline session quality for all 18 linear track sessions. The blue dot indicates the outlier session from panel (<bold>a</bold>) with potential extra-spatial information: this session shows high decoding error combined with high session quality. Session quality of the W maze session is also indicated on the x-axis (decoding error is not directly comparable). (<bold>c–n</bold>) Two example HMM-congruent but not Bayesian-significant events from the W maze session are depicted to highlight the fact that congruence can correspond to remote replay. (<bold>c</bold>) Spikes during ripple with local place cells highlighted (top panel) and the corresponding latent state probabilities (bottom panel) decoded using the HMM show sequential structure (grayscale intensity corresponds to probability). (<bold>d</bold>) In this event, the Bayesian score relative to the shuffle distribution (top panel) indicates that the event is not-significant, whereas the HMM score relative to shuffles indicates (bottom panel) the ripple event is HMM-congruent. (<bold>e</bold>) Estimates of position using local place fields show jumpy, multi-modal <italic>a posteriori</italic> distributions over space in 1D (top left panel) and 2D (top right panel; distribution modes and time is denoted in color). Bayesian decoding using the remote environment place fields (bottom panel) indicates that the sample event is a remote replay. Note that in a typical experiment, only the local place fields would be available. (<bold>f–h</bold>) Same as (<bold>c–e</bold>), but for a different ripple event.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-34467-fig7-v2"/></fig><p>In order to validate the concept of model-training refinement, we considered a dataset in which multiple environments were explored on the same day and remote replay was previously observed (<xref ref-type="bibr" rid="bib32">Karlsson et al., 2015</xref>). These data consisted of a series of short exploratory sessions in which an animal first explored a novel maze (E1) and then was placed in a familiar one (E2). We identified awake PBEs during the familiar E2 session and used them to train an HMM. When we refined this model by removing Bayesian-significant local replay events from the training data, we found that the model quality decreased comparatively slowly (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, green line), indicating that the HMM was capturing more than the local spatial structure. In contrast, when we used place fields from E1 to identify Bayesian-significant remote replay events and removed these from the training data, we found that the model quality decreased rapidly as with the general linear track cases (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, red line). When we examined individual events in detail in this data, we found many examples in which HMM-significant, Bayesian non-significant PBEs decoded to extended state sequences which turned out to correspond to reactivation of the remote track (two are shown in <xref ref-type="fig" rid="fig7">Figure 7c–l</xref>). If we imagine that in this experiment data were only recorded during exploration of the familiar environment, classical Bayesian decoding would treat these events as noise, as shown in the bottom half of the two examples. In contrast, our HMM-based analysis finds these events to be significant, as shown in the top half of the two examples. Thus, by combining classical Bayesian decoding and HMM-congruence, we are able to identify a signature of when a HMM trained on PBEs captures sequential structure distinct from that dictated by the local environment. Additionally, in these cases, we show that specific non-local reactivation events can be identified.</p><p>Finally, we considered the potential of our methodology for uncovering temporal patterns in PBE activity under scenarios where complex behavior does not permit identification of well-defined place-fields or in the absence of behavior, such as during sleep. As we have emphasized, a remarkable aspect of learning HMMs from PBE activity is that the model can be built entirely without behavioral data, so can our model capture significant sequential information outside of immobility periods during quiet waking? To demonstrate this potential, we examined HMMs trained on PBEs in sleep following the learning phase of an object-location memory task when animals explored three objects in an open field (see Material and methods). Previous studies have demonstrated that subsequent recall of this memory is hippocampus-dependent, and requires consolidation in post-task sleep (<xref ref-type="bibr" rid="bib47">Prince et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Inostroza et al., 2013</xref>). However, while this task involves spatial exploration of objects in an arena, whether the subsequent post-task sleep contains sequential structure and whether object memory is contained in this code has remained elusive (<xref ref-type="bibr" rid="bib39">Larkin et al., 2014</xref>). In order to assess the presence of sequential structure in the PBEs, we first used cross validation to generate a distribution of sequence HMM-congruence scores. For each set of test PBEs, we also generated surrogates by shuffling time bins across events (pooled time-swap). Using our HMM-congruence score which explicitly tests for sequences through state space, the large difference between actual and shuffled score distributions indicates evidence for significant sequential structure in the PBEs (<inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test, <xref ref-type="fig" rid="fig8">Figure 8</xref>). While more work is needed to evaluate the mnemonic relevance of these HMM-congruent sequences, these data support the notion that the HMM can uncover sequential activity in sleep away from the task environment. This approach further demonstrates the utility of the HMM approach as an initial analysis of a novel dataset, or as a way of comparing the sequential content encoded in PBEs during different periods.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.34467.020</object-id><label>Figure 8.</label><caption><title>Temporal structure during a sleep period following object-location memory task.</title><p>Using cross validation, we calculate the HMM-congruence score (which ranges from 0 to 1) for test PBEs. For each event, we also calculate the score of a surrogate chosen using a pooled time-swap shuffle across all test events. The distribution of scores of actual events is significantly higher than that of the surrogate data (<inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, Mann–Whitney <italic>U</italic> test).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-34467-fig8-v2"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Increasing lines of evidence point to the importance of hippocampal ensemble activity during PBEs in guiding on-going behavior and active learning. Despite being the strongest output patterns of the hippocampus, however, this activity has been assumed to be interpretable only in the context of other theta-associated place cell activity expressed during behavior. Our findings demonstrate that over the course of a behavioral session, ensemble activity during PBEs alone is sufficient to form a model which captures the spatial relationships within an environment. This suggests that areas downstream of the hippocampus might be able to make use solely of PBE activity to form models of external space. In an extreme view, place cell activity might merely subserve the internal mechanisms in the hippocampus which generate PBE sequences. To the extent that animals might wish to use the spatial code obtained from PBEs to identify their current location, we show that this can be done after translating ensemble activity into the latent states of the model. Do the PBEs contain ‘full information’ about the environment? Bayesian decoding of location from place cell activity results in lower error than location estimates generated using the latent states and lsPF. This suggests that the manifold defined by the HMM may not capture all the dimensions of information represented during exploration. However, it is possible that with more PBE data, we would learn a more refined state space. Thus, the difference between the latent space represented during behavior and within PBEs may be an interesting focus of future study.</p><p>When we examined the transition matrices we learned from PBEs, we found that they were marked by significant sparsity. This sparsity results from the sequential patterns generated during PBEs. Latent variable models have previously been used to analyze the structure of hippocampal place cell activity (<xref ref-type="bibr" rid="bib10">Chen et al., 2012</xref>; <xref ref-type="bibr" rid="bib9">2014</xref>; <xref ref-type="bibr" rid="bib15">Dabaghian et al., 2014</xref>). In these studies, the learned transition matrices were mapped to undirected graphs which could be analyzed using topological measures. It is intriguing that similar structure is apparent in PBE activity. For example, we observed that transition matrices learned from PBEs associated with linear track behavior were significantly sparser than those learned from the open field, which we hypothesize is a consequence of the greater freedom of behavior in the latter (a topological difference). Whether hippocampal PBE activity must always be sequential, i.e., evolve through a sparsely-connected latent space, is an open and interesting question, as are differences between the latent state space dynamics learned during PBEs and those learned from place cell activity.</p><sec id="s3-1"><title>Graded, non-binary replay detection</title><p>Remarkably, evaluating the congruence or likelihood of test data against our HMM provided a highly novel method to detect events that are consistent with replay, without a need to access the ‘play’ itself. In the process of evaluating the potential of HMMs for detecting replay, we developed an approach to compare different replay-detection strategies. Our results highlight how the data do not readily admit to a strict separation between ‘replay’ and ‘non-replay’ events. While it is possible that with additional shuffles or other restrictions (<xref ref-type="bibr" rid="bib52">Silva et al., 2015</xref>), automated performance might be rendered closer to human-labeling, even human scorers had variation in their opinions. This calls into doubt judgments of memory-related functions which build on a binary distinction between replay and non-replay sequences. Model congruence, either as a raw statistical likelihood or weighted against a shuffle distribution, seems to be a very reasonable metric to associate with individual PBEs. Moreover, evaluating congruence with an HMM does not require access to repeated behavioral sequences, which may be infeasible under widely-used single- or few-trial learning paradigms or when the events involve replay of a remote internalized environment. Given these benefits, along with computational efficiency, we would suggest that future analyses of the downstream impact of hippocampal reactivation regress effects against this measure rather than assuming a binary distinction.</p></sec><sec id="s3-2"><title>Learning, Model Congruence and Replay Quality</title><p>Not surprisingly, the rate of PBEs had a large effect on our ability to measure model congruence. Interestingly, it has been noted that the density of PBE is higher during early exposure to a novel environment (<xref ref-type="bibr" rid="bib13">Cheng and Frank, 2011</xref>; <xref ref-type="bibr" rid="bib24">Frank et al., 2004</xref>; <xref ref-type="bibr" rid="bib35">Kemere et al., 2013</xref>; <xref ref-type="bibr" rid="bib38">Kudrimoti et al., 1999</xref>). This might suggest that for the animal, PBE activity could be an important source for generating models of the world when the animal is actively learning about the environment. If as hypothesized, replay is a form of rehearsal signal generated by the hippocampus to train neocortical modules (<xref ref-type="bibr" rid="bib42">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="bib6">Buzsáki, 1989</xref>), then indeed the brain’s internal machinery may also be evaluating whether a given sequential PBE pattern is congruent and consistent with previously observed PBEs. In later sessions, as animals have been repeatedly exposed to the same environments, downstream regions will have already witnessed many PBEs from which to estimate the structure of the world. Overall, our approach provides a novel viewpoint from the perspective of hippocampal PBEs. An interesting future line of inquiry would be to assess the extent to which a model built on PBEs during first experience of a novel environment is slower or faster to converge to the final spatial map than models built on theta-associated place activity.</p></sec><sec id="s3-3"><title>Application to Extra-spatial behaviors</title><p>We have analyzed data gathered in experiments in which rats carried out simple spatial navigation tasks. Thus, to some extent it is not surprising that when we decoded ensemble activity during behavior we found that spatial positions the animal is exploring are strongly associated with the latent states.</p><p>We anticipate that our approach for calculating lsPF would be equally useful in tasks in which the hippocampal map is organized around time (<xref ref-type="bibr" rid="bib22">Eichenbaum, 2014</xref>; <xref ref-type="bibr" rid="bib50">Rodriguez and Levy, 2001</xref>) or other continuous variables (e.g. sound frequency [<xref ref-type="bibr" rid="bib2">Aronov et al., 2017</xref>]). Our two proof-of-concept analyses, however, suggest that it should be possible to use HMMs to infer the presence of extra-spatial sequential reactivation in PBEs. For example, we showed that there is significant sequential structure during sleep after an animal explores novel objects in an environment. We anticipate that careful experimental design and further algorithmic development would allow for the conjunctive coding of object identity and spatial locations to be detected in the latent states we learn from PBEs, with model-congruence providing a tool to study sequential hippocampal reactivation in these types of tasks.</p><p>Conjunctive, non-spatial information might be one source of the apparent variability that results in many PBEs not being detected as replay using traditional Bayesian decoding. Another proposed source of this variability is reactivation of other environments. Our second proof-of-concept analysis suggests that HMMs learned from PBEs can, in fact, capture the spatial structure of environments beyond the one the animal is currently exploring. It appears that it should be possible to use only the PBEs and information about the place-cell map of the local environment to refine the training set for remote replay activity and learn the structure of a remote environment. While we used Bayesian decoding to detect putative local replays, we anticipate related approaches might use an HMM or other approaches to model local place cell activity.</p></sec><sec id="s3-4"><title>Future possibilities</title><p>It has been previously observed that the rate of hippocampal reactivations in PBEs during awake behavior is much higher than during sleep (<xref ref-type="bibr" rid="bib27">Grosmark and Buzsáki, 2016</xref>; <xref ref-type="bibr" rid="bib33">Karlsson and Frank, 2008</xref>), but the reasons for this are not well understood. One hypothesis is that many sleep PBEs contain the reactivation of contexts other than those measured during a behavioral experiment. Another hypothesis is that sleep activity involves remodeling of dynamic network architectures (<xref ref-type="bibr" rid="bib3">Buhry et al., 2011</xref>; <xref ref-type="bibr" rid="bib53">Tononi and Cirelli, 2014</xref>). Our approach has the potential to illuminate some sources of variability during sleep. While we have given preliminary evidence that information about a remote context can be present in PBEs along with the local context, further work is required to understand how our model’s ability to capture this structure scales with the number of different contexts. With sufficient data, our HMM approach should be able to learn disjoint sets of latent states (or ‘sub-models’) which would capture these separate contexts and allow us to test this possibility. Alternatively, sleep PBEs could yield models which represent a known behavioral context but are markedly different (e.g., less sparse) than those learned from awake PBEs. This might support the network remodeling function of sleep. In the latter case, we might imagine that only a small subset of sleep PBEs—corresponding to learning-related replay—would be congruent with a model learned from awake PBE data.</p></sec><sec id="s3-5"><title>Conclusions</title><p>We have demonstrated a new analytical framework for studying hippocampal ensemble activity which enables primacy of PBEs in model formation. We use an unsupervised learning technique commonly used in the machine learning field to study sequential patterns, the hidden Markov model. This contrasts with existing approaches in which the model—estimated place fields for the ensemble—is formed using the theta-associated place cell activity. We find that our PBE-first approach results in a model which still captures the spatial structure of the behavioral tasks we studied. Additionally, we demonstrate that we can use model-congruence as a tool for assessing whether individual PBEs contain hippocampal replay. Finally, we present proofs-of-concept that this analytical approach can detect the presence of sequential reactivation in experimental scenarios in which existing approaches are insufficient. Thus, the use of unsupervised learning of latent variable models—specifically HMMs and statistical congruence as a marker of individual events—bears much promise for expanding our ability to understand how PBEs enable the cognitive functions of the hippocampus.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experiment paradigm/Neural data recording</title><p>We analyzed neural activity recorded from the hippocampus of rats during periods in which they performed behavioral tasks in different paradigms. First, we considered data from animals running back and forth in a linear track 150 or 200 cm long. As previously reported using these same data (<xref ref-type="bibr" rid="bib18">Diba and Buzsáki, 2007</xref>), we recorded neural activity using chronically-implanted silicon probes to acquire the activity of hippocampal CA1/CA3 neurons. From these experiments, we chose sessions during which we observed at least 20 place cells during active place-field exploration, and at least 30 PBEs (see below). Place cells were identified as pyramidal cells which had (i) a minimum peak firing rate of 2 Hz, (ii) a maximum mean firing rate of 5 Hz, and (iii) a peak-to-mean firing rate ratio of at least 3, all estimated exclusively during periods of run (as defined before, that is, when the animal was running &gt;10 cm/s). This selection yielded <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula> session with 41–203 neurons (36–186 pyramidal cells). All procedures were approved by the Institutional Animal Care and Use Committee of Rutgers University and followed US National Institutes of Health animal use guidelines (protocol 90–042).</p><p>A second dataset used tetrodes to record a large number (101–242) of putative pyramidal neurons in area CA1 during two sessions each in four rats. Briefly, as was previously reported using these data (<xref ref-type="bibr" rid="bib45">Pfeiffer and Foster, 2013</xref>, <xref ref-type="bibr" rid="bib46">Pfeiffer and Foster, 2015</xref>), rats explored an arena in which there were 36 reward sites. In each session, one site was designated as ‘home’. During a session, rats would repeatedly alternate between retrieving a random reward site in one of the remaining 35 locations and retrieving a reward at the home location. All procedures were approved by the Johns Hopkins University Animal Care and Use Committee and followed US National Institutes of Health animal use guidelines (protocols RA08M138, RA11M16, and RA14M48).</p><p>In order to investigate remote replay, we used data from an experiment in which this phenomenon has been previously reported (<xref ref-type="bibr" rid="bib34">Karlsson and Frank, 2009</xref>). Briefly, rats were implanted with multi-electrode microdrives with tetrodes targeting CA1 and CA3. They were trained to carry out a continuous-alternation task in an initially novel ‘w’-shaped maze (E2) for liquid reward for multiple daily run sessions interspersed by rest-periods in an enclosed box. After they learned the task, they were introduced to a novel w-maze (E1) in a different orientation in which they had two run sessions followed by a run in the now-familiar E2. For our proof-of-concept analysis (<xref ref-type="fig" rid="fig7">Figure 7</xref>), we used data from the second day of the novel maze (i.e., third and fourth exposures) in animal ‘Bon’.</p><p>Finally, we recorded neural activity during an object-location memory task using a 32-channel silicon probes (Buzsaki32, Neuronexus, MI) equipped with light fibers lowered to area CA1 of the dorsal hippocampus. The animal was previously infused with AAV-CamKIIa-ArchT-GFP for the purpose of another experiment. Putative pyramidal cells and interneurons were distinguished based on their spike waveforms and spike auto-correlograms. On the day before the recordings, the animal was repeatedly exposed to an empty test chamber on four successive six minute blocks, interleaved by three minute rest periods in the home cage. On the recording day, the first of these six-minute blocks was again the empty test chamber, but on the remaining blocks, the animal was exposed to a fixed configuration of three different novel objects placed in the northeast, center and southeast corners of the box. These blocks were again interleaved with three minute rest periods in the home cage. The test chamber was a 60 <inline-formula><mml:math id="inf93"><mml:mo>×</mml:mo></mml:math></inline-formula> 60 cm<inline-formula><mml:math id="inf94"><mml:mrow><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> box with a local cue (8.5 in. <inline-formula><mml:math id="inf95"><mml:mo>×</mml:mo></mml:math></inline-formula> 11 in. sheet printout) placed on one test wall. Following the last acquisition exposure, the animal was returned to its home cage for a four hour extended sleep period. The subsequent day, one of the objects in the box was displaced and the animal was reintroduced into the box to test for interactions with the displaced versus non-displaced objects. All procedures were approved by the Institutional Animal Care and Use Committee of the University of Wisconsin-Milwaukee and followed US National Institutes of Health animal use guidelines (protocol 13–14 #28)</p></sec><sec id="s4-2"><title>Population burst events</title><p>To identify PBEs in the linear track data, a SDF was calculated by counting the total number of spikes across all recorded single and multi-units in non-overlapping 1 ms time bins. The SDF was then smoothed using a Gaussian kernel (20 ms standard deviation, 60 ms half-width). Candidate events were identified as time windows with a peak SDF of at least three standard deviations above the mean calculated over all the session. The boundaries of each event were set to time points of crossing the mean, preceding and following the peak. Events during which animals were moving (average movement speed of &gt;5 cm/s) were excluded from all further analyses to prevent possible theta sequences from biasing our results. For analysis, we then binned each PBE into 20 ms (non-sliding) time bins. Spikes from putative interneurons (mean firing rate when moving &gt;10 Hz) were excluded, as were events with duration less than four time bins or with fewer than four active pyramidal cells. For the open field data, we used the previously reported criteria (<xref ref-type="bibr" rid="bib45">Pfeiffer and Foster, 2013</xref>) for identifying PBEs prior to binning (10 ms standard deviation kernel, minimum of 10% of units active, duration between 50 ms and 2000 ms).</p></sec><sec id="s4-3"><title>Hidden markov model of PBE activity</title><p>We trained HMMshe complete sequence on the PBEs. In an HMM, an unobserved discrete latent state <inline-formula><mml:math id="inf96"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> evolves through time according to a first order Markov process. The temporal evolution of the latent state is described by the <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf98"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>A</mml:mi></mml:mstyle></mml:math></inline-formula>, whose elements <inline-formula><mml:math id="inf99"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mo>{</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> signify the probability after each time bin of transitioning from state <inline-formula><mml:math id="inf100"><mml:mi>i</mml:mi></mml:math></inline-formula> to state <inline-formula><mml:math id="inf101"><mml:mi>j</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf102"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The number of states, <inline-formula><mml:math id="inf103"><mml:mi>M</mml:mi></mml:math></inline-formula>, is a specified hyperparameter. We found that our results were insensitive to the value of <inline-formula><mml:math id="inf104"><mml:mi>M</mml:mi></mml:math></inline-formula> through a wide range of values from 20 to 100 (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). During each time bin of an event, the identity of the latent state influences what is observed via a state-dependent probability distribution. We modeled the <inline-formula><mml:math id="inf105"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional vector of binned spiking from our ensemble of <inline-formula><mml:math id="inf106"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons at time <inline-formula><mml:math id="inf107"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf108"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, as a Poisson process. Specifically, for each state, <inline-formula><mml:math id="inf109"><mml:mi>i</mml:mi></mml:math></inline-formula>, we model neuron <inline-formula><mml:math id="inf110"><mml:mi>n</mml:mi></mml:math></inline-formula> as independently firing according to a Poisson process with rate <inline-formula><mml:math id="inf111"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:munderover><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the number of spikes observed from neuron <inline-formula><mml:math id="inf113"><mml:mi>n</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf114"><mml:mi>t</mml:mi></mml:math></inline-formula>. The final parameter which specifies our model is the probability. distribution of the initial state for a given event, <inline-formula><mml:math id="inf115"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. Thus, our model is specified by parameters <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>A</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>Λ</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is an <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> matrix and <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> is an <inline-formula><mml:math id="inf120"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional vector.</p><p>To learn model parameters, we follow the well-known iterative EM procedure (<xref ref-type="bibr" rid="bib48">Rabiner, 1989</xref>), treating each training PBE as an observation sequence. In order to regularize the model, we impose a minimum firing rate for each neuron of 0.001 (0.05 Hz) during the M-step of EM. For a given PBE (i.e., observation sequence) with <inline-formula><mml:math id="inf121"><mml:mi>K</mml:mi></mml:math></inline-formula> bins, we use the ‘forward-backward algorithm’ (<xref ref-type="bibr" rid="bib48">Rabiner, 1989</xref>) to calculate the probability distribution of the latent state for each time bin, <inline-formula><mml:math id="inf122"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. For a particular time bin, <inline-formula><mml:math id="inf123"><mml:mi>t</mml:mi></mml:math></inline-formula>, in a given sequence, the forward-backward algorithm allows information from all observation bins, previous and subsequent, to affect this state probability distribution (as well the observation bin at time <inline-formula><mml:math id="inf124"><mml:mi>t</mml:mi></mml:math></inline-formula>). The forward-backward algorithm also efficiently calculates the ‘score’, or likelihood of the complete sequence, <inline-formula><mml:math id="inf125"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. All HMMs learned in this work used five-fold cross validation, that is, the PBEs were divided into five randomly selected fifths (‘folds’), and then each fold was evaluated as a test set, with the model trained using the remaining four folds. We define the model likelihood of an HMM as the product of the scores of each event using this five-fold cross validation. To initially evaluate model learning, we compared model likelihoods calculated using real and shuffled test data. Models which have learned to properly represent the data should show significant increases. To quantify the presence of PBE sequences in a model we used a model quality metric as described below.</p></sec><sec id="s4-4"><title>Ordering states for visualization</title><p>For visualization, we wanted to order the states to maximize the super diagonal of the transition matrix. We used a greedy approach which typically yields this solution. We started by assigning the first index to the state with the highest initial probability and added states based on the most probable state transitions. The undirected connectivity graphs were then generated from this transition matrix, averaging the strength of reciprocal connections, <inline-formula><mml:math id="inf126"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-5"><title>Surrogate datasets and shuffle methods</title><p>In order to analyze the HMMs we learned, we compared them against different types of surrogate datasets obtained by shuffling the neural activity during PBEs in distinct ways. (1) Temporal shuffle: within each event, the binned spiking activity was circularly permuted across time for each unit, independently of other units. This goal of this shuffle is to disrupt unit co-activation, while maintaining the temporal dynamics for each unit. (2) Time-swap shuffle: within each event, the order of the binned columns of neural activity was randomly permuted across time, coherently across units. The goal of this shuffle is to change the temporal dynamics of ensemble activity, while maintaining unit co-activation. (3) Poisson surrogate ‘shuffle’: we estimated each unit’s mean firing rate across all PBEs, and then produced surrogate PBEs from independent Poisson simulations according to each unit’s mean firing rate. (4) Pooled time-swap shuffle: the order of the binned columns of neural activity was randomly permuted across all pooled events, coherently across units. This shuffle has been previously used in Bayesian replay detection (<xref ref-type="bibr" rid="bib16">Davidson et al., 2009</xref>).</p></sec><sec id="s4-6"><title>Calculating sparsity and connectivity of the model parameters</title><p>Sparsity of the transitions from individual states (departure sparsity) was measured by calculating the Gini coefficient of corresponding rows of the transition matrix (<xref ref-type="bibr" rid="bib29">Hurley and Rickard, 2009</xref>). The Gini coefficient is a measure of how variable the values of this probability distribution are, with equality across states corresponding to a coefficient of zero (minimal sparsity), and a singular distribution with a probability-one transition to a single other state corresponding to a coefficient of one (maximal sparsity). The sparsity of the full transition matrix was calculated by averaging the Gini coefficient across rows. For analyses of PBE models from linear tracks, we computed the mean sparsity across states for each of the 250 surrogate datasets, and these means were used to generate the box plots of <xref ref-type="fig" rid="fig2">Figure 2c</xref>. Note that for the actual data, we generate a distribution by randomly initializing the model 250 times and calculating the mean sparsity over all initializations. For analyses of models learned from PBEs in open fields (and the linear track comparison with 50 states), we created 50 surrogates/random initializations (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). To compare across sessions, we calculated the mean sparsity by averaging over all 250 surrogate datasets to obtain a single mean sparsity per session, so that <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula> per-session means were used to create the box-plots of <xref ref-type="fig" rid="fig2">Figure 2e</xref>.</p><p>Firing rates can be highly variable for different units. Thus, when evaluating the sparsity of the observation matrix, we measured the extent to which individual units were specifically active in a few states by calculating the Gini coefficients of the rows of the observation matrix. As with transitions, we calculated mean sparsity across units for each surrogate dataset (e.g., linear track, <xref ref-type="fig" rid="fig2">Figure 2d</xref>; open field, <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>), and we then averaged over all surrogate datasets to obtain aper-session average, used in <xref ref-type="fig" rid="fig2">Figure 2f</xref>.</p></sec><sec id="s4-7"><title>Model connectivity and sequences</title><p>To measure the degree of sequential connectivity within the graph corresponding to the transition matrix—with nodes and edges representing the states and transitions, respectively—we developed an algorithm for measuring the length of the longest path that can be taken through the graph. This method is analogous to the ‘depth-first search’ algorithm for traversing the graph’s tree structure without backtracking. First, we made an adjacency matrix for a corresponding unweighted directed graph by binarizing the transition matrix using a threshold of 0.2 on the transition probabilities. Starting from each node, we then found the longest path that ended at either a previously visited node or a terminal node (a node without any outgoing edges). To compare models trained on actual versus surrogate datasets, we adjusted the thresholds to match the average degree (defined as the average number of edges per node) between the models, thus ruling out possible effects due to differences in the number of graph edges. We carried out this analysis on the same set of models that were generated for analyzing sparsity. To compare across sessions, we calculated the median maximum path length for each session (<inline-formula><mml:math id="inf129"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math></inline-formula>) and used the per-session medians to generate box plots of <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3c</xref>.</p></sec><sec id="s4-8"><title>Latent state place fields</title><p>To calculate the latent state place fields, we first identified bouts of running by identifying periods when animals were running (speed &gt;10 cm/s). We then binned the spiking during each of these bouts in 100 ms bins. Using the forward-backward algorithm (<xref ref-type="bibr" rid="bib48">Rabiner, 1989</xref>) and the HMM model parameters learned from PBEs, we decoded each bout into a sequence of latent state probability distributions, <inline-formula><mml:math id="inf130"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. Using the track positions corresponding to each time bin, we then found the average state distribution for each position bin, <inline-formula><mml:math id="inf131"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and normalized to yield a distribution for each state, <inline-formula><mml:math id="inf132"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-9"><title>Decoding position from latent state sequences</title><p>We used the lsPF to decode the animal’s position after determining the probability of different latent state trajectories during bouts of running. With five-fold cross validation, we estimated lsPF in a training dataset, then used the HMM model to decode latent state trajectory distributions from ensemble neural activity in the test data. The product of lsPF and decoded latent state distribution at time <inline-formula><mml:math id="inf133"><mml:mi>t</mml:mi></mml:math></inline-formula> is the joint distribution <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We decode position as the mean of the marginal distribution <inline-formula><mml:math id="inf135"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-10"><title>Bayesian replay detection</title><p>We followed a frequently used Bayesian decoding approach to detect replay in our 1D data (<xref ref-type="bibr" rid="bib37">Kloosterman, 2012</xref>). For each 20 ms time bin <inline-formula><mml:math id="inf136"><mml:mi>t</mml:mi></mml:math></inline-formula> within a PBE, given a vector comprised of spike counts from <inline-formula><mml:math id="inf137"><mml:mi>N</mml:mi></mml:math></inline-formula> units, <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in that bin, the posterior probability distribution over the binned track positions was calculated using Bayes’ rule:<disp-formula id="equ2"><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf139"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the center of <inline-formula><mml:math id="inf140"><mml:mi>p</mml:mi></mml:math></inline-formula>-th linearized position bin (of <inline-formula><mml:math id="inf141"><mml:mi>P</mml:mi></mml:math></inline-formula> total bins). We assumed Poisson firing statistics, thus the prior probability, <inline-formula><mml:math id="inf142"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for the firing of each unit <inline-formula><mml:math id="inf143"><mml:mi>n</mml:mi></mml:math></inline-formula> is equal to<disp-formula id="equ3"><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:munderover><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf144"><mml:mi>τ</mml:mi></mml:math></inline-formula> is the duration of time bin (100 ms during estimation, 20 ms during decoding), and <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> characterizes the mean firing rate of the <inline-formula><mml:math id="inf146"><mml:mi>n</mml:mi></mml:math></inline-formula>-th unit in the <inline-formula><mml:math id="inf147"><mml:mi>p</mml:mi></mml:math></inline-formula>-th position bin. We assumed a uniform prior distribution <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> over the position bins.</p><p>For each PBE, the estimated posterior probability distribution was used to detect replay as follows. Many (35,000) lines with different slopes and intercepts were sampled randomly following the approach in (<xref ref-type="bibr" rid="bib37">Kloosterman, 2012</xref>). The Bayesian replay score for a given event was the maximum score obtained from all candidate lines, where the score for a particular line was defined as the mean probability mass under the line, within a bandwidth (of 3 cm). For time bins during which the sampled line fell outside of the extent of the track, the median probability mass of the corresponding time bin was used, and for time bins during which no spikes were observed, we used the median probability mass across all on-track time bins. To evaluate the significance of this score, for each event we generated 5000 surrogates of the posterior probability distribution by cycling the columns (i.e., for each time bin, circularly permuting the distribution over positions by a random amount) and calculated the replay score for each surrogate. The Monte Carlo <inline-formula><mml:math id="inf149"><mml:mi>p</mml:mi></mml:math></inline-formula>-value for each event was obtained from the number of shuffled events with replay scores higher than the raw data. The threshold for significance was varied as described in the text. For the open field, we used previously reported criteria (<xref ref-type="bibr" rid="bib45">Pfeiffer and Foster, 2013</xref>) to identify replay events from PBEs.</p></sec><sec id="s4-11"><title>Replay detection via PBE model congruence</title><p>To identify replay as model congruence, for each PBEs, we used the forward-backward algorithm to calculate the sequence likelihood <inline-formula><mml:math id="inf150"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, as defined earlier. Using five-fold cross validation, the parameters of a HMM were learned from training PBE. The sequence score was then calculated for each event in the test data. To evaluate the significance of this score, for each event we generated 5000 surrogate scores using a computationally-efficient scheme. Specifically, for each surrogate, we randomly shuffle the rows of the transition matrix, excepting the diagonal. By maintaining the diagonal (i.e., transitions that begin and end in the same state) and leaving the observation model unchanged, this shuffle specifically selects against PBEs in which the latent states do not evolve in temporal sequences. The Monte Carlo <inline-formula><mml:math id="inf151"><mml:mi>p</mml:mi></mml:math></inline-formula>-value for each event was calculated as the fraction of shuffled events with HMM sequence scores higher than the raw data. The threshold for significance was varied as described in the text. Note that while we describe this as HMM-congruence, we have maintained the diagonal of the transition matrix, which specifically selects against PBEs which might be model-congruent by maintaining a single state over many time bins. In reality there are other dimensions of the HMM that we could assess congruence against, for example the observation model, the initial state distribution, or combinations of these and the transition matrix. In comparing against Bayesian decoding, our current definition seemed most appropriate for sequence detection, but we can imagine future studies expanding on our approach.</p></sec><sec id="s4-12"><title>Human scoring and detection comparison</title><p>We organized a group of human scorers to visually evaluate whether individual PBEs should be described as replay. More specifically, scorers were only presented with Bayesian decoded probability distributions such as those in <xref ref-type="fig" rid="fig4">Figure 4a</xref>, but without access to the spike raster or any additional information. The scorers included six graduate students (including one of the authors) and two undergraduates, all of whom were generally familiar with the concept of hippocampal replay. We built an automatic presentation system which would display each event in random order, and record one of six possible scores: ‘excellent’ (highly sequential with no jumps and covering most of the track), ‘good’ (highly sequential with few or no jumps), ‘flat’ (decoded position stayed mostly in the same place, i.e. no temporal dynamics), ‘uncertain’ (some semblance of structure, but not enough to fall into any of the previous categories) or ‘noise’ (no apparent structure, or nonsensical trajectories such as teleportation). An event was then designated as replay if it was labeled as ‘excellent’ or ‘good’ by a majority of scorers (ties were labeled as non-replay).</p><p>To calculate an ROC curve for replay detection algorithms, we used our shuffle statistics for each event to create a vector which related the significance threshold (e.g., 99%) to the label supplied by the algorithm (i.e., significant replay or not). Then, as a function of threshold, the sensitivity (fraction of true positives identified) and selectivity (fraction of true negatives identified) were averaged over events to yield an ROC curve. To evaluate whether the AUC differed between Bayesian and model-congruence techniques we used a bootstrap approach. To generate a null hypothesis, we combined the event/threshold vectors from both groups, and then sampled two random groups (A and B) with replacement from the pooled data. The AUC for these two random groups of events were measured, and a distribution for the difference between the randomly chosen AUC was calculated. The two-sided <inline-formula><mml:math id="inf152"><mml:mi>p</mml:mi></mml:math></inline-formula>-value we report is the fraction of differences in random AUC which are more extreme than the actual difference.</p></sec><sec id="s4-13"><title>HMM model quality across sessions</title><p>In order to understand the extent to which an HMM trained on PBEs from a given session contained sequentially-structured temporal dynamics, we calculated the ‘session quality’ (equivalently model quality) as follows. Again using five-fold cross validation, we learn an HMM on the training subset of PBEs, and score (using the forward-backward algorithm, as before), the remaining subset of test PBEs. Then, we also score a pooled time-swap surrogate of the test PBE and we repeat this pooled time-swap scoring <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2500</mml:mn></mml:mrow></mml:math></inline-formula> times. Finally, we obtain a z score for each PBE by comparing the score from the actual test PBE, to the distribution of pooled time-swap scores of the corresponding PBE. The session quality is then defined as the average of these z scores, over all events in a session. This measure of session quality was then used to detect the presence of putative remote replay events or other extra-spatial structure in PBEs, since a high session quality after removing local Bayesian significant events is highly suggestive of remaining (unexplained) sequential structure.</p></sec><sec id="s4-14"><title>Software and data analysis</title><p>Data analyses were performed using <sc>Matlab</sc>r and Python. Jupyter notebooks (using Python) are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/kemerelab/UncoveringTemporalStructureHippocampus">https://github.com/kemerelab/UncoveringTemporalStructureHippocampus</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/UncoveringTemporalStructureHippocampus">https://github.com/elifesciences-publications/UncoveringTemporalStructureHippocampus</ext-link>), where most of the results presented here are reproduced. We have also developed and open-sourced a Python package (namely nelpy) to support the analyses of electrophysiology data with HMMs, which is available from <ext-link ext-link-type="uri" xlink:href="https://github.com/nelpy">https://github.com/nelpy</ext-link> (<xref ref-type="bibr" rid="bib1">Ackermann et al., 2018</xref>; copies archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/nelpy">https://github.com/elifesciences-publications/nelpy</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/tutorials">https://github.com/elifesciences-publications/tutorials</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/example-analyses">https://github.com/elifesciences-publications/example-analyses</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/example-data">https://github.com/elifesciences-publications/example-data</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/test-data">https://github.com/elifesciences-publications/test-data</ext-link>).</p></sec></sec></body><back><sec id="s5-2" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Software, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Gathered experimental data, Data curation, Data curation, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Gathered experimental data, Data curation, Supervision, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Software, Supervision, Methodology, Writing—original draft, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: As reported previously, all procedures were approved by the Johns Hopkins University, Rutgers University, and University of California, San Francisco Animal Care and Use Committees and followed US National Institutes of Health animal use guidelines (protocol 90-042).</p></fn></fn-group></sec><sec id="s5-1" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.34467.021</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-34467-transrepform-v2.pdf"/></supplementary-material><sec id="s5-3" sec-type="data-availability"><title>Data availability</title><p>We analyzed data from neural recording experiments. Data for Figures 1-5 has been previously reported in Diba and Buzsáki (2007, Nature Neuroscience). These data and also data for Figure 8 are available from Kamran Diba on request. Data for Figure 6 was previously reported in Pfeiffer and Foster (2015, Science) and is available from Brad Pfeiffer and David Foster on request. Data for Figure 7 was previously reported in Karlsson and Frank (2008, Nature Neuroscience) is available from the CRCNS.org archive ('hc-6'). All analysis code and sample recording epochs for Figures 1-7 are available on <ext-link ext-link-type="uri" xlink:href="https://github.com/kemerelab/UncoveringTemporalStructureHippocampus">https://github.com/kemerelab/UncoveringTemporalStructureHippocampus</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/UncoveringTemporalStructureHippocampus">https://github.com/elifesciences-publications/UncoveringTemporalStructureHippocampus</ext-link>). These make use of our broader open-source Python analysis software <ext-link ext-link-type="uri" xlink:href="https://github.com/nelpy">https://github.com/nelpy</ext-link> (copies archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/nelpy">https://github.com/elifesciences-publications/nelpy</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/tutorials">https://github.com/elifesciences-publications/tutorials</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/example-analyses">https://github.com/elifesciences-publications/example-analyses</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/example-data">https://github.com/elifesciences-publications/example-data</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/test-data">https://github.com/elifesciences-publications/test-data</ext-link>).</p><p>The following previously published dataset was used:</p><p><related-object content-type="generated-dataset" id="dataset1" source-id="http://dx.doi.org/10.6080/K0NK3BZJ" source-id-type="uri"><collab collab-type="author">Karlsson M</collab><collab collab-type="author">Carr M</collab><collab collab-type="author">Frank LM</collab><year>2015</year><source>Simultaneous extracellular recordings from hippocampal areas CA1 and CA3 (or MEC and CA1) from rats performing an alternation task in two W-shapped tracks that are geometrically identically but visually distinct.</source><ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6080/K0NK3BZJ">http://dx.doi.org/10.6080/K0NK3BZJ</ext-link><comment>Publicly available at CRCNS - Collaborative Research in Computational Neuroscience</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ackermann</surname> <given-names>E</given-names></name><name><surname>Chu</surname> <given-names>J</given-names></name><name><surname>Dutta</surname> <given-names>S</given-names></name><name><surname>Kemere</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Nelpy: Neuroelectrophysiology Object Model and Data Analysis in Python</source></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aronov</surname> <given-names>D</given-names></name><name><surname>Nevers</surname> <given-names>R</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mapping of a non-spatial dimension by the hippocampal-entorhinal circuit</article-title><source>Nature</source><volume>543</volume><fpage>719</fpage><lpage>722</lpage><pub-id pub-id-type="doi">10.1038/nature21692</pub-id><pub-id pub-id-type="pmid">28358077</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buhry</surname> <given-names>L</given-names></name><name><surname>Azizi</surname> <given-names>AH</given-names></name><name><surname>Cheng</surname> <given-names>S</given-names></name><name><surname>Reactivation</surname> <given-names>CS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reactivation, replay, and preplay: how it might all fit together</article-title><source>Neural Plasticity</source><volume>2011</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1155/2011/203462</pub-id><pub-id pub-id-type="pmid">21918724</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Leung</surname> <given-names>LW</given-names></name><name><surname>Vanderwolf</surname> <given-names>CH</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Cellular bases of hippocampal EEG in the behaving rat</article-title><source>Brain Research Reviews</source><volume>287</volume><fpage>139</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/0165-0173(83)90037-1</pub-id><pub-id pub-id-type="pmid">6357356</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Hippocampal sharp waves: their origin and significance</article-title><source>Brain Research</source><volume>398</volume><fpage>242</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(86)91483-6</pub-id><pub-id pub-id-type="pmid">3026567</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Two-stage model of memory trace formation: a role for &quot;noisy&quot; brain states</article-title><source>Neuroscience</source><volume>31</volume><fpage>551</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1016/0306-4522(89)90423-5</pub-id><pub-id pub-id-type="pmid">2687720</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hippocampal sharp wave-ripple: A cognitive biomarker for episodic memory and planning</article-title><source>Hippocampus</source><volume>25</volume><fpage>1073</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1002/hipo.22488</pub-id><pub-id pub-id-type="pmid">26135716</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname> <given-names>MF</given-names></name><name><surname>Jadhav</surname> <given-names>SP</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>147</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1038/nn.2732</pub-id><pub-id pub-id-type="pmid">21270783</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Gomperts</surname> <given-names>SN</given-names></name><name><surname>Yamamoto</surname> <given-names>J</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural representation of spatial topology in the rodent hippocampus</article-title><source>Neural Computation</source><volume>26</volume><fpage>1</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00538</pub-id><pub-id pub-id-type="pmid">24102128</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Kloosterman</surname> <given-names>F</given-names></name><name><surname>Brown</surname> <given-names>EN</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Uncovering spatial topology represented by rat hippocampal population neuronal codes</article-title><source>Journal of Computational Neuroscience</source><volume>33</volume><fpage>227</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1007/s10827-012-0384-x</pub-id><pub-id pub-id-type="pmid">22307459</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deciphering neural codes of memory during sleep</article-title><source>Trends in Neurosciences</source><volume>40</volume><fpage>260</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2017.03.005</pub-id><pub-id pub-id-type="pmid">28390699</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname> <given-names>S</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>New experiences enhance coordinated neural activity in the hippocampus</article-title><source>Neuron</source><volume>57</volume><fpage>303</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.11.035</pub-id><pub-id pub-id-type="pmid">18215626</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname> <given-names>S</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The structure of networks that produce the transformation from grid cells to place cells</article-title><source>Neuroscience</source><volume>197</volume><fpage>293</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2011.09.002</pub-id><pub-id pub-id-type="pmid">21963867</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chrobak</surname> <given-names>JJ</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>High-frequency oscillations in the output networks of the hippocampal-entorhinal axis of the freely behaving rat</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>3056</fpage><lpage>3066</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-09-03056.1996</pub-id><pub-id pub-id-type="pmid">8622135</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dabaghian</surname> <given-names>Y</given-names></name><name><surname>Brandt</surname> <given-names>VL</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reconceiving the hippocampal map as a topological template</article-title><source>eLife</source><volume>3</volume><elocation-id>e03476</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03476</pub-id><pub-id pub-id-type="pmid">25141375</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname> <given-names>TJ</given-names></name><name><surname>Kloosterman</surname> <given-names>F</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Hippocampal replay of extended experience</article-title><source>Neuron</source><volume>63</volume><fpage>497</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.027</pub-id><pub-id pub-id-type="pmid">19709631</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deppisch</surname> <given-names>J</given-names></name><name><surname>Pawelzik</surname> <given-names>K</given-names></name><name><surname>Geisel</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Uncovering the synchronization dynamics from correlated neuronal activity quantifies assembly formation</article-title><source>Biological Cybernetics</source><volume>71</volume><fpage>387</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1007/BF00198916</pub-id><pub-id pub-id-type="pmid">7993929</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diba</surname> <given-names>K</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1241</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1038/nn1961</pub-id><pub-id pub-id-type="pmid">17828259</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diekelmann</surname> <given-names>S</given-names></name><name><surname>Born</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The memory function of sleep</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>114</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/nrn2762</pub-id><pub-id pub-id-type="pmid">20046194</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dupret</surname> <given-names>D</given-names></name><name><surname>O'Neill</surname> <given-names>J</given-names></name><name><surname>Pleydell-Bouverie</surname> <given-names>B</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The reorganization and reactivation of hippocampal maps predict spatial memory performance</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>995</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1038/nn.2599</pub-id><pub-id pub-id-type="pmid">20639874</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ego-Stengel</surname> <given-names>V</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Disruption of ripple-associated hippocampal activity during rest impairs spatial learning in the rat</article-title><source>Hippocampus</source><volume>20</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1002/hipo.20707</pub-id><pub-id pub-id-type="pmid">19816984</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Time cells in the hippocampus: a new dimension for mapping memories</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>732</fpage><lpage>744</lpage><pub-id pub-id-type="doi">10.1038/nrn3827</pub-id><pub-id pub-id-type="pmid">25269553</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>DJ</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title><source>Nature</source><volume>440</volume><fpage>680</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1038/nature04587</pub-id><pub-id pub-id-type="pmid">16474382</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>LM</given-names></name><name><surname>Stanley</surname> <given-names>GB</given-names></name><name><surname>Brown</surname> <given-names>EN</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Hippocampal plasticity across multiple days of exposure to novel environments</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>7681</fpage><lpage>7689</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1958-04.2004</pub-id><pub-id pub-id-type="pmid">15342735</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fruchterman</surname> <given-names>TMJ</given-names></name><name><surname>Reingold</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Graph drawing by force-directed placement</article-title><source>Software: Practice and Experience</source><volume>21</volume><fpage>1129</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1002/spe.4380211102</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girardeau</surname> <given-names>G</given-names></name><name><surname>Benchenane</surname> <given-names>K</given-names></name><name><surname>Wiener</surname> <given-names>SI</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Zugaro</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Selective suppression of hippocampal ripples impairs spatial memory</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1222</fpage><lpage>1223</lpage><pub-id pub-id-type="doi">10.1038/nn.2384</pub-id><pub-id pub-id-type="pmid">19749750</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosmark</surname> <given-names>AD</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Diversity in neural firing dynamics supports both rigid and learned hippocampal sequences</article-title><source>Science</source><volume>351</volume><fpage>1440</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1126/science.aad1935</pub-id><pub-id pub-id-type="pmid">27013730</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname> <given-names>AS</given-names></name><name><surname>van der Meer</surname> <given-names>MA</given-names></name><name><surname>Touretzky</surname> <given-names>DS</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Hippocampal replay is not a simple function of experience</article-title><source>Neuron</source><volume>65</volume><fpage>695</fpage><lpage>705</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.034</pub-id><pub-id pub-id-type="pmid">20223204</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hurley</surname> <given-names>N</given-names></name><name><surname>Rickard</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Comparing measures of sparsity</article-title><source>IEEE Transactions on Information Theory</source><volume>55</volume><fpage>4723</fpage><lpage>4741</lpage><pub-id pub-id-type="doi">10.1109/TIT.2009.2027527</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inostroza</surname> <given-names>M</given-names></name><name><surname>Binder</surname> <given-names>S</given-names></name><name><surname>Born</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sleep-dependency of episodic-like memory consolidation in rats</article-title><source>Behavioural Brain Research</source><volume>237</volume><fpage>15</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2012.09.011</pub-id><pub-id pub-id-type="pmid">22989412</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jadhav</surname> <given-names>SP</given-names></name><name><surname>Kemere</surname> <given-names>C</given-names></name><name><surname>German</surname> <given-names>PW</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Awake hippocampal sharp-wave ripples support spatial memory</article-title><source>Science</source><volume>336</volume><fpage>1454</fpage><lpage>1458</lpage><pub-id pub-id-type="doi">10.1126/science.1217230</pub-id><pub-id pub-id-type="pmid">22555434</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Karlsson</surname> <given-names>MP</given-names></name><name><surname>Carr</surname> <given-names>MF</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Simultaneous extracellular recordings from hippocampal areas CA1 and CA3 (or MEC and CA1) from rats performing an alternation task in two W-shapped tracks that are geometrically identically but visually distinct</data-title><source>CRCNS</source><pub-id pub-id-type="doi">10.6080/K0NK3BZJ</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlsson</surname> <given-names>MP</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Network dynamics underlying the formation of sparse, informative representations in the hippocampus</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>14271</fpage><lpage>14281</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4261-08.2008</pub-id><pub-id pub-id-type="pmid">19109508</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlsson</surname> <given-names>MP</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Awake replay of remote experiences in the hippocampus</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>913</fpage><lpage>918</lpage><pub-id pub-id-type="doi">10.1038/nn.2344</pub-id><pub-id pub-id-type="pmid">19525943</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemere</surname> <given-names>C</given-names></name><name><surname>Carr</surname> <given-names>MF</given-names></name><name><surname>Karlsson</surname> <given-names>MP</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rapid and continuous modulation of hippocampal network state during exploration of new places</article-title><source>PLoS One</source><volume>8</volume><elocation-id>e73114</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0073114</pub-id><pub-id pub-id-type="pmid">24023818</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemere</surname> <given-names>C</given-names></name><name><surname>Santhanam</surname> <given-names>G</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Afshar</surname> <given-names>A</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Meng</surname> <given-names>TH</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Detecting neural-state transitions using hidden Markov models for motor cortical prostheses</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>2441</fpage><lpage>2452</lpage><pub-id pub-id-type="doi">10.1152/jn.00924.2007</pub-id><pub-id pub-id-type="pmid">18614757</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kloosterman</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Analysis of hippocampal memory replay using neural population decoding</chapter-title><person-group person-group-type="editor"><name><surname>Fellin</surname> <given-names>T</given-names></name><name><surname>Halassa</surname> <given-names>M</given-names></name></person-group><source>Neuronal Network Analysis: Concepts and Experimental Approaches</source><publisher-name>Humana Press</publisher-name><fpage>259</fpage><lpage>282</lpage></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kudrimoti</surname> <given-names>HS</given-names></name><name><surname>Barnes</surname> <given-names>CA</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Reactivation of hippocampal cell assemblies: effects of behavioral state, experience, and EEG dynamics</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>4090</fpage><lpage>4101</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-10-04090.1999</pub-id><pub-id pub-id-type="pmid">10234037</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larkin</surname> <given-names>MC</given-names></name><name><surname>Lykken</surname> <given-names>C</given-names></name><name><surname>Tye</surname> <given-names>LD</given-names></name><name><surname>Wickelgren</surname> <given-names>JG</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hippocampal output area CA1 broadcasts a generalized novelty signal during an object-place recognition task</article-title><source>Hippocampus</source><volume>24</volume><fpage>773</fpage><lpage>783</lpage><pub-id pub-id-type="doi">10.1002/hipo.22268</pub-id><pub-id pub-id-type="pmid">24596296</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>AK</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Memory of sequential experience in the hippocampus during slow wave sleep</article-title><source>Neuron</source><volume>36</volume><fpage>1183</fpage><lpage>1194</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01096-6</pub-id><pub-id pub-id-type="pmid">12495631</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Eschenko</surname> <given-names>O</given-names></name><name><surname>Murayama</surname> <given-names>Y</given-names></name><name><surname>Augath</surname> <given-names>M</given-names></name><name><surname>Steudel</surname> <given-names>T</given-names></name><name><surname>Evrard</surname> <given-names>HC</given-names></name><name><surname>Besserve</surname> <given-names>M</given-names></name><name><surname>Oeltermann</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Hippocampal-cortical interaction during periods of subcortical silence</article-title><source>Nature</source><volume>491</volume><fpage>547</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1038/nature11618</pub-id><pub-id pub-id-type="pmid">23172213</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>O'Reilly</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Why there are complementary learning systems in the Hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title><source>Psychological Review</source><volume>102</volume><fpage>419</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.102.3.419</pub-id><pub-id pub-id-type="pmid">7624455</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nádasdy</surname> <given-names>Z</given-names></name><name><surname>Hirase</surname> <given-names>H</given-names></name><name><surname>Czurkó</surname> <given-names>A</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Replay and time compression of recurring spike sequences in the hippocampus</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>9497</fpage><lpage>9507</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-21-09497.1999</pub-id><pub-id pub-id-type="pmid">10531452</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>O’Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Place units in the hippocampus of the freely moving rat</article-title><source>Experimental Neurology</source><volume>51</volume><fpage>78</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1016/0014-4886(76)90055-8</pub-id><pub-id pub-id-type="pmid">1261644</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname> <given-names>BE</given-names></name><name><surname>Foster</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title><source>Nature</source><volume>497</volume><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1038/nature12112</pub-id><pub-id pub-id-type="pmid">23594744</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname> <given-names>BE</given-names></name><name><surname>Foster</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Autoassociative dynamics in the generation of sequences of hippocampal place cells</article-title><source>Science</source><volume>349</volume><fpage>180</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1126/science.aaa9633</pub-id><pub-id pub-id-type="pmid">26160946</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prince</surname> <given-names>TM</given-names></name><name><surname>Wimmer</surname> <given-names>M</given-names></name><name><surname>Choi</surname> <given-names>J</given-names></name><name><surname>Havekes</surname> <given-names>R</given-names></name><name><surname>Aton</surname> <given-names>S</given-names></name><name><surname>Abel</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sleep deprivation during a specific 3-hour time window post-training impairs hippocampal synaptic plasticity and memory</article-title><source>Neurobiology of Learning and Memory</source><volume>109</volume><fpage>122</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2013.11.021</pub-id><pub-id pub-id-type="pmid">24380868</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabiner</surname> <given-names>LR</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>A tutorial on hidden Markov models and selected applications in speech recognition</article-title><source>Proceedings of the IEEE</source><volume>77</volume><fpage>257</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1109/5.18626</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radons</surname> <given-names>G</given-names></name><name><surname>Becker</surname> <given-names>JD</given-names></name><name><surname>Dülfer</surname> <given-names>B</given-names></name><name><surname>Krüger</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Analysis, classification, and coding of multielectrode spike trains with hidden Markov models</article-title><source>Biological Cybernetics</source><volume>71</volume><fpage>359</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1007/BF00239623</pub-id><pub-id pub-id-type="pmid">7948227</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez</surname> <given-names>P</given-names></name><name><surname>Levy</surname> <given-names>WB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A model of hippocampal activity in trace conditioning: where's the trace?</article-title><source>Behavioral Neuroscience</source><volume>115</volume><fpage>1224</fpage><lpage>1238</lpage><pub-id pub-id-type="doi">10.1037/0735-7044.115.6.1224</pub-id><pub-id pub-id-type="pmid">11770054</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siapas</surname> <given-names>AG</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Coordinated interactions between hippocampal ripples and cortical spindles during slow-wave sleep</article-title><source>Neuron</source><volume>21</volume><fpage>1123</fpage><lpage>1128</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80629-7</pub-id><pub-id pub-id-type="pmid">9856467</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silva</surname> <given-names>D</given-names></name><name><surname>Feng</surname> <given-names>T</given-names></name><name><surname>Foster</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Trajectory events across hippocampal place cells require previous experience</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1772</fpage><lpage>1779</lpage><pub-id pub-id-type="doi">10.1038/nn.4151</pub-id><pub-id pub-id-type="pmid">26502260</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tononi</surname> <given-names>G</given-names></name><name><surname>Cirelli</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sleep and the price of plasticity: from synaptic and cellular homeostasis to memory consolidation and integration</article-title><source>Neuron</source><volume>81</volume><fpage>12</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.12.025</pub-id><pub-id pub-id-type="pmid">24411729</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamamoto</surname> <given-names>J</given-names></name><name><surname>Tonegawa</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Direct medial entorhinal cortex input to hippocampal CA1 is crucial for extended quiet awake replay</article-title><source>Neuron</source><volume>96</volume><fpage>217</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.017</pub-id><pub-id pub-id-type="pmid">28957670</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.34467.025</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Colgin</surname><given-names>Laura</given-names></name><role>Reviewing Editor</role><aff id="aff8"><institution>The University of Texas at Austin, Center for Learning and Memory</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>[Editors' note: the authors’ plan for revisions was approved and the authors made a formal revised submission.]</p><p>Thank you for sending your article entitled &quot;Uncovering temporal structure in hippocampal output patterns&quot; for peer review at <italic>eLife</italic>. Your article is being evaluated by three peer reviewers, one of whom is a member of our Board of Reviewing Editors and the evaluation is being overseen by a Reviewing Editor and Michael Frank as the Senior Editor.</p><p>Given the list of essential revisions, including new experiments, the editors and reviewers invite you to respond within the next two weeks with an action plan and timetable for the completion of the additional work. We plan to share your responses with the reviewers and then issue a binding recommendation.</p><p>The enthusiasm for the new technique presented in the paper was high. However, the reviewers recommend a clear demonstration that this new technique can be used to discover something new. In other words, the reviewers feel that the paper would be more exciting to a broad readership if results were obtained that would not have been possible with standard analyses already used in the field. The reviewers are confident that the recommended revisions could be done with the authors' existing data sets (i.e. without requiring collection of new data, which would take a long time). Please find the reviewers' specific comments below:</p><p>Reviewer #1:</p><p>This is a potentially very interesting paper that uses hidden Markov models to uncover structure in hippocampal place cell ensembles during sharp wave-related states. The advantage of this method, compared to existing methods, is that place cell sequences can be identified without the use of a place cell training data set from active running behaviors. This could allow for the identification of sequences that represent non-spatial memories or sequences from single trials. However, the paper is written in a way that may not be readily understandable by a general readership. My specific comments are as follows:</p><p>1) I am afraid that the HMM approach is not explained in a way that is entirely clear to a general audience. For example, the authors write, &quot;the unobserved latent variable represents the temporal evolution of a memory trace which is coherent across the CA1 ensemble&quot; (subsection “Learning hidden Markov models from PBE data”). What does this mean exactly? Similarly, the meaning of &quot;the probability that the CA1 ensemble will transition from a start state to a destination state in the next time bin&quot; is unclear. It seems like a state basically corresponds to a representation of a location, but this is not really explicitly explained. I did not understand the cartoon and its explanation in Figure 1A. Does this represent sequences? Do the colors correspond to the place cells shown? In Figure 2, I did not understand what was meant by &quot;nodes on the same radius correspond to the same state&quot;. Also, in Figure 2, I did not understand the difference between panels C and E, and between panels D and F. The authors should probably better explain the measures from graph theory since they will probably be unknown to many readers. In subsection “Ordering states for visualization”, the authors refer to the state with the highest initial probability, but I did not understand what this meant exactly. The readers use HMM jargon like &quot;the forward-backward algorithm&quot; that may need to be briefly explained.</p><p>2) It seems concerning that none of the data from rat 3 seems to show a difference between the actual data and the surrogate data. (Figure 1—figure supplement 1, Figure 2—figure supplement 1).</p><p>3) It would be useful to have more information about the human scorers. Who were they? How were they trained? The authors write (subsection “Human scoring and detection comparison”) that an event was designated as replay if it was labeled as &quot;excellent&quot; or &quot;good&quot; by a majority of scorers. Do they mean just more than half of the scorers?</p><p>Reviewer #2:</p><p>Maboudi et al., propose the use of Hidden Markov Models to represent multi-neuron activity patterns in the hippocampal subfield CA1. The model is trained from activity in Population Bursts Events (Likely to a large extent coinciding with sharp wave ripple events). It is shown that HMM can successfully capture at least part of the statistical structure of the data, and while they are trained on spontaneous activity, they can recover the spatial responses observed during active behavior. The method has several attractive features, most importantly, that it detects patterns in an unsupervised fashion. In the literature, spontaneous activity in the hippocampus has been typically analyzed in the context of &quot;replay&quot; and making use of Bayesian decoders trained on the spatial responses of the neurons. While this approach has been quite successful, it also restricts analysis to patterns that are representative of the behavioral situation (spatial environment etc.) that has been used to train the decoder itself. Whatever information is carried by the activity, but not covered by the decoder, goes undetected. Unsupervised methods hold the promise of uncovering structure in a template free way.</p><p>I am quite sympathetic with the approach, but I think that it needs a better demonstration of its success and usefulness. I am fairly confident this can be done based on the data the authors use here, and to a large extent the same methods. My main points:</p><p>1) The used surrogate data methods (essentially column-wise and row-wise shuffling), is the intuitive one, and probably correct. We see that the sparseness of the model trained from real data is higher than for the shuffled data. However, the effects (Figure 2C-F) seem to be very small. One is then left wondering about what the model is actually doing. For example, one could imagine this pattern of results may emerge in the extreme sparsity case, when each state only represents activation of one neuron. In that case, shuffling would clearly leave the sparsity of the observation matrix unchanged. Why the sparsity of the transition matrix doesn't change with shuffling is more puzzling and should be investigated</p><p>2) The PBE derived model recovers the spatial response properties of the ensemble quite well. I am a bit puzzled about how this reflects the known statistics of hippocampal activity though. We know that on a given track (e.g. Figure 3), about 30% of CA1 neurons fire, while during SWRs most neurons activate. Then, how is it possible that all states derived by PBE are activated on the track? wouldn't we expect only a subset to do so?</p><p>3) I am a bit puzzled by the congruency measure. We see in Figure 4 that the model with the shuffled transition matrix yields lower likelihood scores, however in the examples shown in Figure 4B we see that the sequential structure is by and large preserved even in the shuffled model. This seems to suggest that the HMM optimization (forward/backward algorithm) is dominated by the observation probabilities, and the transition matrix is less important. But then one should ask what of the essential structure of the data is the transition matrix capturing.</p><p>4) I understand that organizing a &quot;human scorers team&quot; was probably a huge effort. Yet I am not sure that this is the best comparison for the analytical procedure, given its strongly subjective nature.</p><p>5) A more general question: The bottom line of the paper is that the outcome of PBE/HMM parallels the outcome of Bayesian decoding. Yet what would be most interesting is what you may get <italic>in addition</italic> to what you get from Bayesian methods. In particular, focusing on the events that are not detected by Bayesian decoding but are detected by PBE/HMM may be interesting, and give more detail on their time distribution, structure etc.</p><p>Reviewer #3:</p><p>The main contribution of this paper is the Introduction of a standard analysis framework (hidden Markov models – HMMs) to the study of place cells in the hippocampus during population burst events (PBEs) when the animal is <italic>not</italic> moving and reply events are believed to be happening. In the standard analysis techniques, behavioral data (e.g. location) is used during navigation to characterize tuning properties of individual neurons and then activity sequences during PBE events is viewed through this tuning to determine if there is replay activity consistent with the measured tuning properties. In the proposal of this manuscript, the HMM analysis technique can be used only on the PBE events and the resulting latent states appear to discover the same localized spatial tuning as what has been found during active navigation. These result show that a completely unsupervised method (i.e. NOT using behavioral location data) can be used to find structure and assess questions about replay, demonstrating that the structure of activity during PBEs is strong enough to discover in a data-driven way.</p><p>In my opinion, the technique is promising and has potential to uncover very interesting information coding principles and test hypotheses. As a methods paper, I think it is a strong contribution to the literature. For <italic>eLife</italic>, I am unclear whether it is a good fit because I find the current draft lacking in the articulation of the scientific discovery that the technique enables. What have we learned about the system by doing the analysis in the paper? While I find the analysis interesting, I'm having trouble understanding what we've learned that has significant impact on our understanding. Similarly, for the future, the manuscript points toward some novel analysis methods that this approach would allow but never quite makes a compelling case for what we would learn from it. This is a good paper and should be published, and I am open to be convinced that it should be published in <italic>eLife</italic>. But, I think the manuscript needs to make a more compelling case for what we can learn from the data being analyzed in this way that we cannot learn otherwise.</p><p>- Some of the language in the Introduction regarding the motivation for the study is vague and unclear. What does it mean for the &quot;fundamental mode&quot; of the hippocampus to be &quot;PBE sequences&quot;? This seems like a statement trying to make a big claim that their study could support, but it's unclear and doesn't quite make the connection for me.</p><p>- In the Results section, I think you could be a little more clear on the details like 1) what is the latent state representing (position), what is the prior, and what is the mapping to Λ (e.g., the tuning curve from state to firing rate). These come up in the Materials and methods but giving a little hint here would help make things more clear.</p><p>- I find some of the figure captions too abbreviated to actually understand what's going on in the figures. For example, in Figure 1A, I don't really understand the plot on the bottom left.</p><p>- In subsection “What do the learned model parameters tell us about PBEs?”, the latent states are used to estimate a likelihood for position on the track. The comparison is made with the shuffled data, but there is no comparison to the decoding using the standard technique (i.e., place cell tuning curves). I would expect the standard decoding to do better (since it's developed with a supervised learning approach), but it would be good to show the comparison and discuss.</p><p>- In subsection “What do the learned model parameters tell us about PBEs?” where the manuscript discusses assessing the patterns of activity for congruence, I think I am confused about the approach. The method is described as comparing against a single null model. As far as I understand, these results would depend entirely on the specific form of the null model that is the baseline for comparison. Why is this an acceptable approach? It seems so dependent on that arbitrary decision that I don't understand how the results are interpretable as a generalized consistency metric.</p><p>- There were some punctuation and wording issues that affected clarity slightly. Subsection “HMM-congruent PBEs capture sequence replay”: &quot;between model quality or the number of PBEs and Bayesian Decoding&quot;. Subsection “Modeling internally generated activity during open 1eld behaviour”: &quot;Can an HMM trained on PBEs indeed&quot;.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Uncovering temporal structure in hippocampal output patterns&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Michael Frank (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been greatly improved but there are some remaining issues that need to be addressed before acceptance, as outlined below.</p><p>In particular, the authors' analyses of events that were non-significant with the Bayesian approach and demonstration that many of them correspond to replayed representations of a different, previously explored environment (i.e., new Figure 7) satisfactorily address the major concerns raised during the first round of review.</p><p>However, the description of Figure 7 is not entirely clear. For example, the x-axes in panels D and I are labeled &quot;time&quot;, but the color bar presumably corresponding to space is also shown below the x-axis. What is this supposed to indicate? Also, panels E-G and J-L need a more thorough explanation. The figure legend states that panel e (bottom) shows decoded positions using E2, which is the local environment, yet the top of panel e presumably corresponds to the remote environment (i.e. E1). This is confusing. The descriptions of Figure 7A-B in the Results section and the figure legend are also not clear. The text states that the model quality decreased rapidly when remote replay events were removed, and that model quality decreased slowly when local replay events were removed. But, the figure appears to show the opposite. The light blue line (local removed) decreases more rapidly than the green line (remote removed). Also, are the top two slowest high quality decay sessions indicated in panel B the ones that were used to find remote replay events? It isn't clear what panel B is supposed to show- it should be explained clearly in the text and in the figure legend.</p><p>There is also insufficient detail provided about Figure 8. How do these figures relate to the object-location memory task? Couldn't these sleep sequences simply have been detected from exploration of the 60 x 60 enclosure regardless of object-location memory? And wouldn't they also have been potentially detected with Bayesian decoding? The authors claim in subsection “Extra-spatial Information” that the HMM can uncover sequential activity that is not readily detectable with the Bayesian approach, but it is unclear how Figure 8 shows this. In any case, Figure 8 is not really necessary as long as Figure 7 is explained clearly.</p><p>A few minor issues also remain to be clarified. In subsection “Awake population burst events”, the authors state that similar results were obtained when a theta detection approach was used, but it is unclear where this approach is shown or described. Also, it is unclear to what &quot;these&quot; refers. Also, in Figure 1B, the &quot;pyramidal cells (non place)&quot; label should be changed to &quot;putative pyramidal cells (non place)&quot;. Also, the difference between the mean departure Gini and mean Λ row Gini in Figure 6E-F should be explained more clearly for a general readership. Finally, as was raised in the initial review, the authors should explicitly state in the first two paragraphs of the Materials and methods section that the Diba and Buzsaki and Pfeiffer and Foster datasets were recorded previously and used in other publications. Otherwise, readers may misunderstand and assume that methods were adopted from these papers, but that similar data were collected again for the present study.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.34467.026</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>The enthusiasm for the new technique presented in the paper was high. However, the reviewers recommend a clear demonstration that this new technique can be used to discover something new. In other words, the reviewers feel that the paper would be more exciting to a broad readership if results were obtained that would not have been possible with standard analyses already used in the field. The reviewers are confident that the recommended revisions could be done with the authors' existing data sets (i.e. without requiring collection of new data, which would take a long time).</p></disp-quote><p>In addition to revising the text to better explain the methodology to a lay reader, we have performed a series of new analyses on our existing data as well as two additional datasets that highlight the novelty and power of our approach. Inspired by the second reviewer’s suggestions, we removed events that were detected as significant using Bayesian replay and retrained the hidden Markov model (HMM) on the remaining (non-significant) events. We note, as suggested by the reviewer, that this approach could not even be attempted with a Bayesian decoding approach. Interestingly, we found that in some sessions the HMM still captured a large of amount of the structure in the remaining population burst events (PBEs) (new Figure 7A). We conjectured that these events may correspond to replay of a remote environment. By analyzing a dataset where the recording spanned behavior of an animal in two separate environments, we confirmed that indeed many HMM-congruent events that are found as non-significant with the Bayesian approach correspond to replay of the remote environment. As a final test, we applied the HMM to PBEs in sleep following the exploration phase of a hippocampus-dependent object-location task (Figure 8) and found that we could detect a significant number of HMM-congruent events in post-task sleep, without reference to behavior. We believe these new results significantly elevate the impact of our report and we thank the reviewers for their suggestions in our acknowledgements.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>This is a potentially very interesting paper that uses hidden Markov models to uncover structure in hippocampal place cell ensembles during sharp wave-related states. The advantage of this method, compared to existing methods, is that place cell sequences can be identified without the use of a place cell training data set from active running behaviors. This could allow for the identification of sequences that represent non-spatial memories or sequences from single trials. However, the paper is written in a way that may not be readily understandable by a general readership. My specific comments are as follows:</p><p>1) I am afraid that the HMM approach is not explained in a way that is entirely clear to a general audience. For example, the authors write, &quot;the unobserved latent variable represents the temporal evolution of a memory trace which is coherent across the CA1 ensemble&quot; (subsection “Learning hidden Markov models from PBE data”). What does this mean exactly? Similarly, the meaning of &quot;the probability that the CA1 ensemble will transition from a start state to a destination state in the next time bin&quot; is unclear. It seems like a state basically corresponds to a representation of a location, but this is not really explicitly explained.</p></disp-quote><p>We have now revised the manuscript with an eye towards further clarity and well-explained terminology. With regards to these specific examples, we now clarify that a HMM is a model of ensemble firing (coactive neurons corresponding to latent states) and how these ensembles change over time. While we do find that states correspond to location, that is only because location is the organizing principle of the activity in the task under consideration. In a different task, states might very well correspond to time, frequency space, or some more complex arbitrary manifold.</p><p>The specific lines mentioned by the reviewer have been eliminated or revised and the beginning of the Results section now provides a better introduction to the HMM methodology.</p><p>&quot;Hidden Markov models have been very fruitfully used to understand sequentially structured data in a variety of contexts. A hidden Markov model captures information about data in two ways. […] Given the role of the hippocampus in memory, in our HMM, the unobserved latent variable presumably corresponds to the temporal evolution of a memory trace that is represented by co-active ensembles of CA1 and CA3 neurons. The full model will correspond to the structure which connects all the memory traces activated during PBEs.&quot;</p><disp-quote content-type="editor-comment"><p>I did not understand the cartoon and its explanation in Figure 1A. Does this represent sequences? Do the colors correspond to the place cells shown?</p></disp-quote><p>The cartoon illustration has been removed from Figure 1 and has been replaced by a latent-state-space-view that hopefully helps readers understand the salient parts of the HMM approach better than the cartoon did.</p><disp-quote content-type="editor-comment"><p>In Figure 2, I did not understand what was meant by &quot;nodes on the same radius correspond to the same state&quot;.</p></disp-quote><p>We have now clarified that the inner and outer rings represent the same nodes – this makes it easier to visualize the self-transitions (starting and ending in the same state) and the transitions to the next node. In the caption, we now state:</p><p>&quot;The nodes correspond to states (progressing clockwise, starting at the top). The weights of the edges are proportional to the transition probabilities between the nodes (states). The transition probabilities from state i to every other state except i+1 are shown in the interior of the graph, whereas for clarity transition probabilities from state i to itself, as well as to neighboring state i+1 are shown between the inner and outer rings of nodes (the nodes on the inner and outer rings represent the same states).&quot;</p><disp-quote content-type="editor-comment"><p>Also, in Figure 2, I did not understand the difference between panels C and E, and between panels D and F.</p></disp-quote><p>We have revised the captions of the figure to clarify that panels C and E are the transition matrices (state dynamics) for the example sessions and all sessions, respectively, and D and F are the observation models (firing rate / state) for the example session and all sessions, respectively.</p><disp-quote content-type="editor-comment"><p>The authors should probably better explain the measures from graph theory since they will probably be unknown to many readers.</p></disp-quote><p>In the Materials and methods section, we now expand on the Gini coefficient when it is introduced:</p><p>&quot;Sparsity of the transitions from individual states (departure sparsity) was measured by calculating the Gini coefficient of corresponding rows of the transition matrix (Hurley and Rickard, 2009). The Gini coefficient is a measure of how variable the values of this probability distribution are, with equality across states corresponding to a coefficient of zero (minimal sparsity), and a singular distribution with a probability-one transition to a single other state corresponding to a coefficient of one (maximal sparsity). The sparsity of the full transition matrix was calculated by averaging the Gini coefficient across rows.&quot;</p><p>We also replaced the term “measures from graph theory” in the main text and also referred to Materials and methods section for a more detailed explanation of the search algorithm, “Using a graph search algorithm (see Materials and methods section), we simulated paths through state space generated by […]”</p><disp-quote content-type="editor-comment"><p>In subsection “Ordering states for visualization”, the authors refer to the state with the highest initial probability, but I did not understand what this meant exactly.</p></disp-quote><p>The Initial State probability is one of the parameter estimated during model learning, along with the Observation Model and State Transition Probabilities. It describes the probability that a given state will be the beginning of a sequence. We now explicitly state:</p><p>“The parameters of our model that are fit to data include the observation model (the cluster descriptions or predicted activity of each excitatory neuron within the CA1/CA3 ensemble for a given state), the state transition model (the probability that the CA1/CA3 ensemble will transition from a start state to a destination state in the next time bin), and the initial state distribution (the probability for sequences to start in each given state). In prior work using HMMs to model neural activity, a variety of statistical distributions have been used to characterize ensemble firing during a specific state (the observation model.”</p><disp-quote content-type="editor-comment"><p>The readers use HMM jargon like &quot;the forward-backward algorithm&quot; that may need to be briefly explained.</p></disp-quote><p>We have revised the text throughout to replace jargon with more readily understandable explanations. e.g.</p><p>“To observe the evolution of the latent states during behavior, we used our model to determine the most likely sequence of latent states corresponding to the neural activity observed in 100 ms bins during epochs that displayed strong theta oscillations (exclusive of PBEs) when rats were running (speed &gt; 10 cm/s; see Materials and methods section).”</p><p>And in the Materials and methods section, we have expanded the description to say:</p><p>&quot;For a given PBE (i.e. observation sequence) with K bins, we use the “forward-backward algorithm” (Rabiner, 1989) to calculate both the probability distribution of the latent state for each time bin, Pr(<italic>q<sub>t</sub></italic>|<italic>O</italic><sub>1</sub>,…,<italic>O</italic><sub>t</sub>,…,<italic>O<sub>K</sub></italic>). For a particular time bin, <italic>t</italic>, in a given sequence, the forward-backward algorithm allows information from all observation bins, previous and subsequent, to affect this state probability distribution (as well the observation bin at time <italic>t</italic>). The forward-backward algorithm also can efficiently calculate, and the “score”, or likelihood of the complete sequence, Pr(<italic>O</italic><sub>1</sub>,…,<italic>O<sub>K</sub></italic>).&quot;</p><disp-quote content-type="editor-comment"><p>2) It seems concerning that none of the data from rat 3 seems to show a difference between the actual data and the surrogate data. (Figure 1—figure supplement 1, Figure 2—figure supplement 1).</p></disp-quote><p>The models from rat 3 were indeed generally of lower quality largely because of a lower number of PBEs (Figure 5—figure supplement 1C). However, we would point out that the sessions from this animal still yielded transition matrices that were sparser than shuffles (significance stars in Figure 2—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>3) It would be useful to have more information about the human scorers. Who were they? How were they trained? The authors write (subsection “Human scoring and detection comparison”) that an event was designated as replay if it was labeled as &quot;excellent&quot; or &quot;good&quot; by a majority of scorers. Do they mean just more than half of the scorers?</p></disp-quote><p>We now describe the scorers:</p><p>&quot;The scorers included six graduate students (including one of the authors) and two undergraduates, all of whom were generally familiar with the concept of hippocampal replay.</p><p>And indeed, majority by definition means more than half the scorers:</p><p>&quot;An event was then designated as replay if it was thus labeled as “excellent” or “good” by a majority of scorers (ties were labeled as non-replay).&quot;</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…] 1) The used surrogate data methods (essentially column-wise and row-wise shuffling), is the intuitive one, and probably correct. We see that the sparseness of the model trained from real data is higher than for the shuffled data. However, the effects (Figure 2C-F) seem to be very small. One is then left wondering about what the model is actually doing. For example, one could imagine this pattern of results may emerge in the extreme sparsity case, when each state only represents activation of one neuron. In that case, shuffling would clearly leave the sparsity of the observation matrix unchanged. Why the sparsity of the transition matrix doesn't change with shuffling is more puzzling, and should be investigated.</p></disp-quote><p>We recognize that no single measure captures the entire structure of spatial representations and each shuffle alters this structure in unique ways that preserves some of this structure, as the reviewer has pointed out. For example, because we used a very conservative per-event shuffle in these surrogates, much of the sparsity of the original events was retained. Additionally, the fact that our measure is between 0 and 1 tends to limit the range of the values of sparsity that are found. Thus, differences can appear small even when they are significant. The large variability of session sparsity values across sessions, can further moderate consistent differences. This is why we show the per-session differences in the supplemental figure. Finally, when there is little structure in either the surrogate or actual data, the EM algorithm will tend to converge to solutions in which the observation matrix is degenerate. In these instances when the state likelihoods approach zero, the sparsity measure loses meaning. This reflects the fact that for limited sizes of training data, EM will tend towards solutions which overfit individual observations rather than yielding a transition matrix that would be fully connected. Despite these limitations we show that for the majority of sessions in our dataset the sparsity measure provides a useful measure of the structure of the transition and observation matrices.</p><disp-quote content-type="editor-comment"><p>2) The PBE derived model recovers the spatial response properties of the ensemble quite well. I am a bit puzzled about how this reflects the known statistics of hippocampal activity though. We know that on a given track (e.g. Figure 3), about 30% of CA1 neurons fire, while during SWRs most neurons activate. Then, how is it possible that all states derived by PBE are activated on the track? wouldn't we expect only a subset to do so?</p></disp-quote><p>The reviewer may be thinking of SWRs during deep sleep, whereas here we are reporting on SWRs during quiet waking on the track, during which, in fact, on a subset of neurons are active during SWRs. It is worth noting (and possibly future study) that we observed a bias (~1.5 times) for neurons with place fields vs nonplace field neurons to be activated during PBEs. Nevertheless, a major benefit of an HMM model with unsupervised learning is that with sufficient training data the learned model is often insensitive to variability that non-maze-active neurons might represent. Nevertheless, inspired by the reviewer’s comments, we have performed new analyses where we investigate events which do not decode well to trajectories in the animal’s current environment (Figure 7). Remarkably, we find that some of these events decode to trajectories in a previously-experienced remote environment.</p><disp-quote content-type="editor-comment"><p>3) I am a bit puzzled by the congruency measure. We see in Figure 4 that the model with the shuffled transition matrix yields lower likelihood scores, however in the examples shown in Figure 4B we see that the sequential structure is by and large preserved even in the shuffled model. This seems to suggest that the HMM optimization (forward/backward algorithm) is dominated by the observation probabilities, and the transition matrix is less important. But then one should ask what of the essential structure of the data is the transition matrix capturing</p></disp-quote><p>The reviewer is correct that the transition matrix shuffle that we have implemented by design retains much of the structure of the original data that is captured by the model. It is worth noting that we shuffle only across rows of the transition matrix, but not columns, and we exclude elements on the diagonal Our aim with this shuffle is strictly to provide a test comparison against the sequential component of the original model. To make this point better and avoid confusion, we now clarify in the text:</p><p>“Note that while we describe this as HMM-congruence, we have maintained the diagonal of the transition matrix, which specifically selects against PBEs which might be model-congruent by maintaining a single state over many time bins. In reality there are other dimensions of the HMM that we could assess congruence against, for example the observation model, the initial state distribution, or combinations of these and the transition matrix. In comparing against Bayesian decoding, our current definition seemed most appropriate for sequence detection, but we can imagine future studies expanding on our approach.”</p><disp-quote content-type="editor-comment"><p>4) I understand that organizing a &quot;human scorers team&quot; was probably a huge effort. Yet I am not sure that this is the best comparison for the analytical procedure, given its strongly subjective nature.</p></disp-quote><p>While some subjectivity is inherent in human scoring of replay, we argue here that methods and thresholds used in different studies also inherently involve subjective criterion. This inherent subjectivity is precisely what we want to reveal for readers who may have the sense that patterns which exceed a 95% threshold against a shuffle distribution are very different from those at 94.5%. We do not make the claim that human scorers provide “the best comparison” for replay detection. The point of showing results from human scorers is to contradict the sense in the literature that a binary distinction between replay sequences and non-replay can be easily identified in the <italic>a posteriori</italic> likelihood of Bayesian decoding.</p><disp-quote content-type="editor-comment"><p>5) A more general question: The bottom line of the paper is that the outcome of PBE/HMM parallels the outcome of Bayesian decoding. Yet what would be most interesting is what you may get in addition to what you get from Bayesian methods. In particular, focusing on the events that are not detected by Bayesian decoding but are detected by PBE/HMM may be interesting, and give more detail on their time distribution, structure etc.</p></disp-quote><p>We appreciate this suggestion of the reviewer, as it led us to add an additional section in the revised paper. Specifically, we investigate how our model quality measure changes as we remove Bayesian decoding significant sequences from the training data. We note that such an analysis is not conceivable using the Bayesian methodology. For many of our data sets, the model quality drops very quickly, which suggests to us that the PBEs in these data are largely encoding information about the local environment. However, in other data sets, including in sessions during which remote replay had previously been reported, we find that the model quality changes more gradually when Bayesian replay events are dropped. Remarkably, when we investigate specific HMM-congruent events that fail to pass the threshold for Bayesian replay in these data, we find that they correspond to remote replay of the previously-experienced environment. Thus, we now suggest that combining information from the Bayesian-decoding approach and the HMM-congruence measure in this way would be a way to specifically look for extra-spatial or remote replay.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>[…] In my opinion, the technique is promising and has potential to uncover very interesting information coding principles and test hypotheses. As a methods paper, I think it is a strong contribution to the literature. For eLife, I am unclear whether it is a good fit because I find the current draft lacking in the articulation of the scientific discovery that the technique enables. What have we learned about the system by doing the analysis in the paper? While I find the analysis interesting, I'm having trouble understanding what we've learned that has significant impact on our understanding. Similarly, for the future, the manuscript points toward some novel analysis methods that this approach would allow but never quite makes a compelling case for what we would learn from it. This is a good paper and should be published, and I am open to be convinced that it should be published in eLife. But, I think the manuscript needs to make a more compelling case for what we can learn from the data being analyzed in this way that we cannot learn otherwise.</p></disp-quote><p>We have revised the manuscript and performed a number of new analyses to highlight the novel benefits of our approach. First, we would argue that no previous analysis has revealed that PBEs contain enough information to form a complete map of the environment. Inspired by the second reviewer, we have also now shown how we can combine Bayesian decoding and HMM-congruence to create a signature for remote replay. While remote replay has been previously reported, no prior approach would allow for remote replays to be identified without behavioral data. Finally, we also now note how the distribution of HMM-congruence can be used to indicate the presence of consistent, ordered sequential activity in PBE data. Bayesian decoding maps between behavior tracking, neural activity during behavior, and neural activity during PBEs. By performing unsupervised learning on the PBEs, we reveal that we can infer important useful structure without these other data, and when we have them, can use them to further illuminate. We believe this is a powerful and useful technique that may have significant, widespread potential and will be of interest to many <italic>eLife</italic> readers.</p><disp-quote content-type="editor-comment"><p>- Some of the language in the Introduction regarding the motivation for the study is vague and unclear. What does it mean for the &quot;fundamental mode&quot; of the hippocampus to be &quot;PBE sequences&quot;? This seems like a statement trying to make a big claim that their study could support, but it's unclear and doesn't quite make the connection for me.</p></disp-quote><p>We have revised the manuscript throughout to make it more precise and attempted to remove any unsupported claims. The motivation of our study was to figure out if we could infer the structure of hippocampal activity only from PBEs. We believe that we have presented substantial evidence that this is possible. We agree that calling PBEs a &quot;fundamental mode&quot; would be self-aggrandizing. However, ours is a question rather than a claim and is meant to provoke further consideration; if a reader believes that &quot;place cell activity during exploration is a fundamental mode of the hippocampus&quot;, we hope to provoke them to ask whether it is possible that PBEs might also be and to consider what evidence would demonstrate this.</p><disp-quote content-type="editor-comment"><p>- In the Results section, I think you could be a little more clear on the details like (1) what is the latent state representing (position), what is the prior, and what is the mapping to Λ (e.g., the tuning curve from state to firing rate). These come up in the Materials and methods but giving a little hint here would help make things more clear.</p></disp-quote><p>The text and Materials and methods section have been revised for improved clarity. We note, however, that the HMM is agnostic as to whether the latent state represents position or another construct (e.g. time or memory). In the Results section we now provide additional description:</p><p>“Hidden Markov models have been very fruitfully used to understand sequentially structured data in a variety of contexts. A hidden Markov model captures information about data in two ways. […] In prior work using HMMS to model neural activity, a variety of statistical distributions have been used to characterize ensemble firing during a specific state (the observation model.”</p><disp-quote content-type="editor-comment"><p>- I find some of the figure captions too abbreviated to actually understand what's going on in the figures. For example, in Figure 1A, I don't really understand the plot on the bottom left.</p></disp-quote><p>The cartoon illustration has been removed from Figure 1 and has been replaced by a latent-state-space-view that hopefully helps readers understand the salient parts of the HMM approach better than the cartoon did.</p><disp-quote content-type="editor-comment"><p>- In subsection “What do the learned model parameters tell us about PBEs?”, the latent states are used to estimate a likelihood for position on the track. The comparison is made with the shuffled data, but there is no comparison to the decoding using the standard technique (i.e., place cell tuning curves). I would expect the standard decoding to do better (since it's developed with a supervised learning approach), but it would be good to show the comparison and discuss.</p></disp-quote><p>We have added median Bayesian decoding error to the first panel of the Figure 3—figure supplement 1. At the end of that section, we have added the text:</p><p>&quot;Note that decoding into the latent space and then mapping to position resulted in slightly higher error than simply performing Bayesian decoding on the neural activity during behavior. This suggests that the latent space we learn from PBEs may not capture all the information about space that is present in hippocampal activity during behavior, though this may reflect the limited number of PBEs from which we can learn.&quot;</p><disp-quote content-type="editor-comment"><p>- In subsection “What do the learned model parameters tell us about PBEs?” where the manuscript discusses assessing the patterns of activity for congruence, I think I am confused about the approach. The method is described as comparing against a single null model. As far as I understand, these results would depend entirely on the specific form of the null model that is the baseline for comparison. Why is this an acceptable approach? It seems so dependent on that arbitrary decision that I don't understand how the results are interpretable as a generalized consistency metric.</p></disp-quote><p>The reviewer makes a very insightful remark. We have added additional text to the Materials and methods section to clarify:</p><p>&quot;Note that while we describe this as HMM-congruence, we have maintained the diagonal of the transition matrix, which specifically selects against PBEs which might be model-congruent by representing a single state for many time bins. In reality there are other dimensions of the HMM that we could assess congruence against, for example the observation model, the initial state distribution, or combinations of these and the transition matrix. In comparing against Bayesian decoding, our current definition seemed most appropriate, but we can imagine future studies expanding on our approach.&quot;</p><disp-quote content-type="editor-comment"><p>- There were some punctuation and wording issues that affected clarity slightly. Subsection “HMM-congruent PBEs capture sequence replay”: &quot;between model quality or the number of PBEs and Bayesian Decoding&quot;. Subsection “Modeling internally generated activity during open 1eld behaviour”: &quot;Can an HMM trained on PBEs indeed&quot;</p></disp-quote><p>The first sentence has been rephrased:</p><p>&quot;Not surprisingly, the performance of Bayesian decoding relative to human scorers was independent of the quality of the HMM model or the number of PBEs as the place field model is learned from ensemble neural activity during running.&quot;</p><p>And the second sentence has been rephrased:</p><p>&quot;Do the latent states learned from Can an HMM trained on PBEs indeed capture spatial information about in a 2D environment?&quot;</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been greatly improved but there are some remaining issues that need to be addressed before acceptance, as outlined below.</p><p>In particular, the authors' analyses of events that were non-significant with the Bayesian approach and demonstration that many of them correspond to replayed representations of a different, previously explored environment (i.e., new Figure 7) satisfactorily address the major concerns raised during the first round of review.</p><p>However, the description of Figure 7 is not entirely clear. For example, the x-axes in panels D and I are labeled &quot;time&quot;, but the color bar presumably corresponding to space is also shown below the x-axis. What is this supposed to indicate?</p></disp-quote><p>Color in panel (c) and the bottom of (e) corresponds to time (this copies the original Karlsson and Frank scheme). To make things clearer, we’ve split panel (e) into (e)/(f) and updated the caption for panel (f) to indicate that the color is the same as in (c). This allows us to remove the color bar from the middle of (d) which was understandably confusing.</p><disp-quote content-type="editor-comment"><p>Also, panels E-G and J-L need a more thorough explanation. The figure legend states that panel e (bottom) shows decoded positions using E2, which is the local environment, yet the top of panel e presumably corresponds to the remote environment (i.e., E1). This is confusing.</p></disp-quote><p>Alongside the lettering changes, we’ve updated the figure caption to better explain this figure:</p><p>“c–n. Two example HMM-congruent but not Bayesian-significant events from the W maze session are depicted to highlight the fact that congruence can correspond to remote replay. c. Spikes during ripple with local place cells highlighted. d. In this event, the Bayesian estimates of position using linearized local place fields have jumpy, multi-modal <italic>a posteriori</italic> distributions over space in 1D and 2D (2D depicts distribution modes and time is denoted in color). e. The replay score relative to the shuffle distribution indicates that this event is not Bayesian significant. e. Latent state probabilities decoded using the HMM trained on PBEs show sequential structure (grayscale intensity corresponds to probability). f. The distribution of HMM-score shuffles indicate the ripple event is HMM-congruent. h. Bayesian decoding using the remote environment indicate that the sample event is a remote replay. Note that in a typical experiment, only the local place fields would be available. i–n. Same as c–h, but for a different ripple event.”</p><disp-quote content-type="editor-comment"><p>The descriptions of Figure 7A-B in the Results section and the figure legend are also not clear. The text states that the model quality decreased rapidly when remote replay events were removed, and that model quality decreased slowly when local replay events were removed. But, the figure appears to show the opposite. The light blue line (local removed) decreases more rapidly than the green line (remote removed).</p></disp-quote><p>We apologize for this error. Indeed, as understood by the editors, the caption and text were correct, but the legend text on the figure itself mis-identified the lines and which events were being removed. This has now been corrected in the figure.</p><disp-quote content-type="editor-comment"><p>Also, are the top two slowest high quality decay sessions indicated in panel B the ones that were used to find remote replay events? It isn't clear what panel B is supposed to show- it should be explained clearly in the text and in the figure legend.</p></disp-quote><p>The flow of text was indeed confusing. We’ve removed the red dot from the figure, and specifically discussed the blue line and blue dot in the figure caption and the text. From the text:</p><p>“Captions: The blue line identifies a session in which model quality drops more slowly, indicating the potential presence of extra-spatial information. The reduction in session quality for a W maze experiment with known extra-spatial information is even slower (green) […] The blue dot indicates the outlier session from panel a with potential extra-spatial information: this session shows high decoding error combined with high session quality.</p><p>Text: Intriguingly, however, in at least one outlier session where model quality decreased slowly with refinement (blue line, Figure 7A), the initial model quality was still high, and we further noted that position decoding using lsPFs yielded relatively high error (blue dot, Figure 7B). Thus, we wondered whether this and similar sessions might have contained non-local or extra-spatial PBEs that were captured by the HMM.”</p><disp-quote content-type="editor-comment"><p>There is also insufficient detail provided about Figure 8. How do these figures relate to the object-location memory task? Couldn't these sleep sequences simply have been detected from exploration of the 60 x 60 enclosure regardless of object-location memory? And wouldn't they also have been potentially detected with Bayesian decoding? The authors claim in subsection “Extra-spatial Information” that the HMM can uncover sequential activity that is not readily detectable with the Bayesian approach, but it is unclear how Figure 8 shows this. In any case, Figure 8 is not really necessary as long as Figure 7 is explained clearly.</p></disp-quote><p>We agree with this assessment and have revised the text to better avoid unjustified claims. We’ve removed the phrase “not readily detectable with the Bayesian approach”. eloThe motivation for this analysis is also better explained—to examine the potential for HMM is uncovering sequence structure during sleep (rather than only waking as elsewhere in our manuscript), without reference to behavior. We’ve streamlined the text to emphasize model quality assessment and removed the second panel from the figure. If the editor prefers, we are also willing to remove this figure entirely.</p><disp-quote content-type="editor-comment"><p>A few minor issues also remain to be clarified. In subsection “Awake population burst events”, the authors state that similar results were obtained when a theta detection approach was used, but it is unclear where this approach is shown or described.</p></disp-quote><p>For the sake of space, we did not show these results. We’ve updated the text to state this.</p><p>“While we identified active behavior using a speed criterion, we found similar results when we instead used a theta-state detection approach (not shown).”</p><disp-quote content-type="editor-comment"><p>Also, in subsection “Awake population burst events”, it is unclear to what &quot;these&quot; refers.</p></disp-quote><p>We changed the text to state:</p><p>“We found that <underline>inactive PBEs</underline> occupied an average of 1.8% of the periods during which animals were on the linear track.”</p><disp-quote content-type="editor-comment"><p>Also, in Figure 1B, the &quot;pyramidal cells (non place)&quot; label should be changed to &quot;putative pyramidal cells (non place)&quot;.</p></disp-quote><p>This has been updated.</p><disp-quote content-type="editor-comment"><p>Also, the difference between the mean departure Gini and mean Λ row Gini in Figure 6E-F should be explained more clearly for a general readership.</p></disp-quote><p>We modified the earlier text to better explain the intuition behind Gini coefficients:</p><p>“First, we investigated the sparsity of the transition matrices using the Gini coefficient (see Materials and methods and Figure 2—figure supplement 1). A higher Gini coefficient corresponds to higher sparsity. Strikingly, the actual data yielded models in which the state transition matrix was sparser than in each of the surrogate counterparts (p &lt; 0.001, Figure 2C), reflecting that each state transitions only to a few other states. Thus, intricate yet reliable details are captured by the HMM. Next, we quantified the sparsity of the observation model. We found that actual data yielded mean firing rates which were highly sparse (Figure 2D), indicating that individual neurons were likely to be active during only a small fraction of the states.”</p><p>And, we changed the labels in Figure 6 to match Figure 2, and updated the caption to say:</p><p>“Comparing the sparsity of the transition matrices (mean Gini coefficient of the departure probabilities) between the linear track and open field reveals that, as expected, over the sessions we observed, the open field is significantly <italic>less sparse</italic> (p&lt;0.001), since the environment is less constrained. f. In contrast, there is not a significant difference between the sparsity of the observation model (mean Gini coefficient of the rows) between the linear track and the open field.”</p><disp-quote content-type="editor-comment"><p>Finally, as was raised in the initial review, the authors should explicitly state in the first two paragraphs of the Materials and methods section that the Diba and Buzsaki and Pfeiffer and Foster datasets were recorded previously and used in other publications. Otherwise, readers may misunderstand and assume that methods were adopted from these papers, but that similar data were collected again for the present study.</p></disp-quote><p>We now say “As previously reported using these same data” in reference to Diba and Buzsaki, and “Briefly, as was previously reported using this dataset” in reference to Pfeiffer and Foster.</p></body></sub-article></article>