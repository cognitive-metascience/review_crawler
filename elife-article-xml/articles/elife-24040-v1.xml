<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">24040</article-id><article-id pub-id-type="doi">10.7554/eLife.24040</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Advance</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Genetic control of encoding strategy in a food-sensing neural circuit</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-76466"><name><surname>Diana</surname><given-names>Giovanni</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-23835"><name><surname>Patel</surname><given-names>Dhaval S</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-23834"><name><surname>Entchev</surname><given-names>Eugeni V</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23836"><name><surname>Zhan</surname><given-names>Mei</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23839"><name><surname>Lu</surname><given-names>Hang</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-23291"><name><surname>Ch'ng</surname><given-names>QueeLim</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1941-3828</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-5"/><xref ref-type="other" rid="par-6"/><xref ref-type="other" rid="par-7"/><xref ref-type="other" rid="par-8"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Centre for Developmental Neurobiology</institution>, <institution>King's College London</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Interdisciplinary Bioengineering Graduate Program</institution>, <institution>Georgia Institute of Technology</institution>, <addr-line><named-content content-type="city">Atlanta</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Wallace H Coulter Department of Biomedical Engineering</institution>, <institution>Georgia Institute of Technology</institution>, <addr-line><named-content content-type="city">Atlanta</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">School of Chemical and Biomolecular Engineering</institution>, <institution>Georgia Institute of Technology</institution>, <addr-line><named-content content-type="city">Atlanta</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Hobert</surname><given-names>Oliver</given-names></name><role>Reviewing editor</role><aff id="aff5"><institution>Howard Hughes Medical Institute, Columbia University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>queelim@kcl.ac.uk</email></corresp><fn fn-type="con" id="equal-contrib"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="pub" publication-format="electronic"><day>07</day><month>02</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e24040</elocation-id><history><date date-type="received"><day>08</day><month>12</month><year>2016</year></date><date date-type="accepted"><day>16</day><month>01</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Diana et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Diana et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-24040-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="article-reference" xlink:href="10.7554/eLife.06259"/><abstract><object-id pub-id-type="doi">10.7554/eLife.24040.001</object-id><p>Neuroendocrine circuits encode environmental information via changes in gene expression and other biochemical activities to regulate physiological responses. Previously, we showed that <italic>daf-7</italic> TGF<inline-formula><mml:math id="inf1"><mml:mi>β</mml:mi></mml:math></inline-formula> and <italic>tph-1</italic> tryptophan hydroxylase expression in specific neurons encode food abundance to modulate lifespan in <italic>Caenorhabditis elegans</italic>, and uncovered cross- and self-regulation among these genes (<xref ref-type="bibr" rid="bib11">Entchev et al., 2015</xref>). Here, we now extend these findings by showing that these interactions between <italic>daf-7</italic> and <italic>tph-1</italic> regulate redundancy and synergy among neurons in food encoding through coordinated control of circuit-level signal and noise properties. Our analysis further shows that <italic>daf-7</italic> and <italic>tph-1</italic> contribute to most of the food-responsiveness in the modulation of lifespan. We applied a computational model to capture the general coding features of this system. This model agrees with our previous genetic analysis and highlights the consequences of redundancy and synergy during information transmission, suggesting a rationale for the regulation of these information processing features.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.001">http://dx.doi.org/10.7554/eLife.24040.001</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>Information Theory</kwd><kwd>gene expression</kwd><kwd>neural code</kwd><kwd>Transforming Growth Factor Beta</kwd><kwd>Serotonin</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd><italic>C. elegans</italic></kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>0946809 GRFP</award-id><principal-award-recipient><name><surname>Zhan</surname><given-names>Mei</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01AG035317</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Hang</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>0954578</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Hang</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01GM088333</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Hang</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>087146</award-id><principal-award-recipient><name><surname>Ch'ng</surname><given-names>QueeLim</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/H020500/1</award-id><principal-award-recipient><name><surname>Ch'ng</surname><given-names>QueeLim</given-names></name></principal-award-recipient></award-group><award-group id="par-7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>NeuroAge 242666</award-id><principal-award-recipient><name><surname>Ch'ng</surname><given-names>QueeLim</given-names></name></principal-award-recipient></award-group><award-group id="par-8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/M00757X/1</award-id><principal-award-recipient><name><surname>Ch'ng</surname><given-names>QueeLim</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Building on previous work (Entchev et al, 2015), computational analyses reveal that the choice between redundant versus synergistic encoding in a gene expression code for food abundance is controlled by cross-talk and auto-regulation among TGF-beta and serotonin pathways.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Signaling pathways convey information about the environment, enabling organisms to generate appropriate physiological response to changing conditions (<xref ref-type="bibr" rid="bib13">Gendron et al., 2015</xref>). We recently established that <italic>tph-1</italic> tryptophan hydroxylase expressed in ADF and NSM neurons and <italic>daf-7</italic> TGF<inline-formula><mml:math id="inf2"><mml:mi>β</mml:mi></mml:math></inline-formula> expressed in ASI neurons in <italic>Caenorhabditis elegans</italic> transmit environmental information to physiology by modulating the response of lifespan to food (<xref ref-type="bibr" rid="bib11">Entchev et al., 2015</xref>). Our previous analytical framework estimated the accuracy of <italic>tph-1</italic> and <italic>daf-7</italic> expression in decoding food input; however, it could not reveal the type of encoding strategy used by <italic>tph-1</italic> and <italic>daf-7</italic> within these neurons, nor could it quantify the contribution of these genes to lifespan modulation. Here, we applied information theory (<xref ref-type="bibr" rid="bib24">Shannon, 1948</xref>) to address these issues. Information theory has been proposed as a general framework to characterize how biological signals are encoded and transmitted (<xref ref-type="bibr" rid="bib6">Bowsher and Swain, 2014</xref>; <xref ref-type="bibr" rid="bib16">Levchenko and Nemenman, 2014</xref>) and has been used to study information processing in the nervous system (<xref ref-type="bibr" rid="bib4">Borst and Theunissen, 1999</xref>) as well as biochemical and genetic pathways (<xref ref-type="bibr" rid="bib9">Cheong et al., 2011</xref>; <xref ref-type="bibr" rid="bib25">Tkačik et al., 2015</xref>).</p><p>Groups of neurons can encode information redundantly or synergistically (<xref ref-type="bibr" rid="bib7">Brenner et al., 2000</xref>; <xref ref-type="bibr" rid="bib18">Puchalla et al., 2005</xref>). This form of informational redundancy is conceptually distinct from genetic redundancy. Redundant encoding systems replicate the same information in more than one neuron, analogous to a computer backup, which provides robustness to perturbations in single neurons at the expense of coding efficiency. In contrast, synergistic circuits encode more information than the sum of their component neurons, but this efficiency is vulnerable to disruptions in the constituent neurons. Redundancy and synergy have been defined using information-theoretic measures (<xref ref-type="bibr" rid="bib2">Averbeck et al., 2006</xref>; <xref ref-type="bibr" rid="bib20">Schneidman et al., 2003</xref>), and both of these strategies for encoding information have been characterized in many neural and genetic circuits (<xref ref-type="bibr" rid="bib2">Averbeck et al., 2006</xref>; <xref ref-type="bibr" rid="bib18">Puchalla et al., 2005</xref>; <xref ref-type="bibr" rid="bib21">Schneidman et al., 2011</xref>; <xref ref-type="bibr" rid="bib25">Tkačik et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Tkačik and Walczak, 2011</xref>).</p><p>Previously, we identified regulatory interactions among <italic>tph-1</italic> and <italic>daf-7</italic> that influence their coding accuracy (<xref ref-type="bibr" rid="bib11">Entchev et al., 2015</xref>). Here, we show that cross-talk between <italic>daf-7</italic> and <italic>tph-1</italic> further affects the adoption of redundancy or synergy during discrimination between food levels. We found that the regulation of signal-to-noise in gene expression underlies shifts between redundancy and synergy across genotypes. Finally, we use a computational model to explore the consequences of redundant and synergistic coding at the level of downstream targets.</p></sec><sec id="s2" sec-type="results|discussion"><title>Results and discussion</title><p>Information theory allows us to quantify the information encoded by <italic>daf-7</italic> and <italic>tph-1</italic> based on the overlap of their expression distributions (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). By associating environmental stimuli (food level) and neuronal responses (gene expression) with the input and the output of a communication system, the encoding capacity of ASI, ADF, and NSM is given by the mutual information (MI) between gene expression responses (G) and food stimuli (F),<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM4">G</mml:mi><mml:mo>;</mml:mo><mml:mi id="XM5">F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi id="XM1">G</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM2">F</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM6">F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM3">G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM7">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the chances of encountering the food condition <inline-formula><mml:math id="inf4"><mml:mi>F</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the response under each specific food level, and <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM8">G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the average response across all the food stimuli (see Appendix and <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>). The MI measures the ability of the gene expression response to discriminate between food conditions.<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.24040.002</object-id><label>Figure 1.</label><caption><title>Redundancy and synergy in a gene expression code.</title><p>(<bold>A</bold>) Information content depends on the overlap between gene expression distributions under different environmental conditions, which in turn depends on both the response magnitude (signal) and the variability across the population (noise). (<bold>B</bold>) Diagrams illustrating redundancy versus synergy, calculated as the difference between the whole (combinatorial information in NSM/ASI/ADF; darkest bar) and the sum of parts (information in NSM + ASI + ADF; stacked bars). (<bold>C</bold>–<bold>E</bold>) Analysis of redundancy and synergy based on <italic>tph-1</italic> expression in ADF and NSM, and <italic>daf-7</italic> expression in ASI. Genotype color key: Wild-type (black), <italic>tph-1(-)</italic> (blue), <italic>daf-7(-)</italic> (red), and <italic>tph-1(-); daf-7(-)</italic> (purple). (<bold>C</bold>) Effect of <italic>tph-1(-)</italic> and <italic>daf-7(-)</italic> mutations on food encoding in the whole circuit (darkest bars) and the sum of parts (lighter stacked bars). (<bold>D</bold>) Effect of <italic>tph-1(-)</italic> and <italic>daf-7(-)</italic> on redundancy and synergy among ADF, NSM, and ASI, as defined in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and (<bold>B</bold>). As described in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and in the main text, redundancy and synergy are indicated by positive and negative R values, respectively. (<bold>E</bold>) Fraction of redundant or synergistic information in ADF, NSM, and ASI, which is the amount of redundancy or synergy in (<bold>D</bold>) normalized to the information encoded. (<bold>F</bold>–<bold>H</bold>) Analysis of redundancy and synergy only in the <italic>tph-1</italic> expressing neurons, ADF, and NSM. (<bold>F</bold>) Effect of <italic>daf-7(-)</italic> in the information encoded by <italic>tph-1</italic> expression in ADF and NSM (darkest bars) and the sum of their parts (lighter stacked bars). (<bold>G</bold>) Effect of <italic>daf-7(-)</italic> on redundancy/synergy of ADF and NSM. (<bold>H</bold>) Fraction of redundant or synergistic information in <italic>tph-1</italic> expression in ADF and NSM, which is the amount of redundancy or synergy in (<bold>G</bold>) normalized to the total information encoded from (<bold>F</bold>). (<bold>I</bold>) Loss of <italic>tph-1</italic> and <italic>daf-7</italic> degrades information about food abundance at the level of lifespan responses.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.002">http://dx.doi.org/10.7554/eLife.24040.002</ext-link></p><p><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.24040.003</object-id><label>Figure 1—source data 1.</label><caption><title>Information and redundancy across genotypes.</title><p>(<bold>Tab 1</bold>) Combinatorial mutual information in the NSM/ASI/ADF neural circuit (‘Whole’ column) and the individual mutual information in ADF, ASI and NSM neurons across different genotypes. (<bold>Tab 2</bold>) Combinatorial information in the NSM and ADF neurons (‘Whole’ column) and the individual information in ADF and NSM neurons in wild-type and <italic>daf-7(-)</italic> strains. (<bold>Tab 3</bold>) Mutual information in the lifespan response of different genotypes. All values are presented as bits <inline-formula><mml:math id="inf7"><mml:mo>±</mml:mo></mml:math></inline-formula> error.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.003">http://dx.doi.org/10.7554/eLife.24040.003</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-24040-fig1-data1-v1.xlsx"/></supplementary-material></p><p><supplementary-material id="SD2-data"><object-id pub-id-type="doi">10.7554/eLife.24040.004</object-id><label>Figure 1—source data 2.</label><caption><title>Fluorescence values for animals carrying both <italic>Pdaf-7::mCherry</italic> and <italic>Pdaf-7::Venus</italic> across four food levels for <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.004">http://dx.doi.org/10.7554/eLife.24040.004</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-24040-fig1-data2-v1.xlsx"/></supplementary-material></p><p><supplementary-material id="SD3-data"><object-id pub-id-type="doi">10.7554/eLife.24040.005</object-id><label>Figure 1—source data 3.</label><caption><title>Optimal input distributions for ADF, ASI and NSM neurons across genotypes (data for <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>).</title><p>Optimal input distributions obtained by maximizing the information encoded individually by ADF, ASI, and NSM neurons. Values are presented as probabilities <inline-formula><mml:math id="inf8"><mml:mo>±</mml:mo></mml:math></inline-formula> uncertainty.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.005">http://dx.doi.org/10.7554/eLife.24040.005</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-24040-fig1-data3-v1.xlsx"/></supplementary-material></p><p><supplementary-material id="SD4-data"><object-id pub-id-type="doi">10.7554/eLife.24040.006</object-id><label>Figure 1—source data 4.</label><caption><title>Validation of information and redundancy estimates for <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>.</title><p>(<bold>Tab 1</bold>) Information (MI/channel capacity) and redundancy encoded by food-responsive gene expression in wild-type animals computed using different methodologies for density estimation. From left to right: plug-in method, least squares cross-validation, and smoothing cross-validation (kernel density estimation with fixed bandwidth selection), balloon estimator (kNN), Jack-knife correction of sample size bias. (<bold>Tab 2</bold>) Jack-knife analysis for information and redundancy across all genotypes. Values are calculated using a fraction of the total dataset indicated in first column.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.006">http://dx.doi.org/10.7554/eLife.24040.006</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-24040-fig1-data4-v1.xlsx"/></supplementary-material></p><p><supplementary-material id="SD5-data"><object-id pub-id-type="doi">10.7554/eLife.24040.007</object-id><label>Figure 1—source data 5.</label><caption><title>Information, redundancy, and optimal input distribution by food level across genotypes.</title><p>Data for <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>. Mutual information of ADF, ASI, and NSM neurons, redundancy and optimal input distribution of the whole circuit by food level across genotypes.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.007">http://dx.doi.org/10.7554/eLife.24040.007</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-24040-fig1-data5-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig1-v1"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24040.008</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Schematic of experimental and analytical workflow.</title><p>(<bold>A</bold>) Experimental procedure for imaging gene expression responses to different food levels in adult <italic>C. elegans</italic>. Animals carrying fluorescent reporters were cultured and exposed to six food levels. A custom microfluidics-based platform was used for quantitative high-throughput imaging of the reporters. (<bold>B</bold>) Image analysis pipeline to identify individual neurons and quantify their fluorescence. (<bold>C</bold>) Information theoretic analysis for dissecting coding strategy in multicellular gene expression circuits. We first used a kernel density estimator to obtain gene expression response probabilities from our data. Next, we obtained theoptimal food distributions and the maximal mutual information between food stimuli and gene expression response. This analysis highlights the relationships between several parameters that describe the multi-neuron gene expression responses (light green boxes) and their contributions to the overall encoding strategy (dark green box).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.008">http://dx.doi.org/10.7554/eLife.24040.008</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig1-figsupp1-v1"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24040.009</object-id><label>Figure 1—figure supplement 2.</label><caption><title>Experimental variability.</title><p>(<bold>A</bold>) Animals carrying <italic>Pdaf-7::mCherry</italic> and <italic>Pdaf-7::Venus</italic> at different genomic locations were used to estimate experimental variability. (<bold>B</bold>) The strain described in (<bold>A</bold>) was shifted to four different food levels (legend) and then imaged simultaneously for mCherry and Venus fluorescence. The graph shows a good correlation between mCherry and Venus reporter expression (<inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8319</mml:mn></mml:mrow></mml:math></inline-formula>). A total of 400 animals were imaged in this experiment.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.009">http://dx.doi.org/10.7554/eLife.24040.009</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig1-figsupp2-v1"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24040.010</object-id><label>Figure 1—figure supplement 3.</label><caption><title>Neurons differ in their optimal input distributions.</title><p>Optimal input distributions obtained by maximizing the information encoded individually by ADF, ASI and NSM neurons qualitatively differ. This feature may allow different neurons to detect different food input levels to broaden the sensory range of the whole circuit. The optimal input distribution for each neuron also differ by genotype: (<bold>A</bold>) wild-type, (<bold>B</bold>) <italic>tph-1(-)</italic> mutants, (<bold>C</bold>) <italic>daf-7(-)</italic> mutants, and (<bold>D</bold>) <italic>tph-1(-); daf-7(-)</italic> double mutants. Uncertainties are obtained from sampling the 80% of the data and taking the standard deviation.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.010">http://dx.doi.org/10.7554/eLife.24040.010</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig1-figsupp3-v1"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24040.011</object-id><label>Figure 1—figure supplement 4.</label><caption><title>Robustness of information theoretic analyses.</title><p>(<bold>A</bold>) Information (MI/channel capacity) encoded by food-responsive gene expression in wild-type animals computed using different methodologies for density estimation. From left to right: plug-in method, least squares cross-validation, and smoothing cross-validation (kernel density estimation with fixed bandwidth selection), balloon estimator (kNN), Jack-knife correction of sample size bias. All this methods result in similar information values. (<bold>B</bold>) Information encoded by food-responsive gene expression in wild-type, <italic>tph-1(-)</italic>, <italic>daf-7(-)</italic>, and <italic>tph-1(-); daf-7(-)</italic> animals. The relative changes in information between different approaches do not display significant differences. (<bold>C</bold>) and (<bold>D</bold>) illustrate the same analysis for redundancy. The switch from redundancy in wild-type to synergy in <italic>daf-7(-)</italic> is consistently present for all the methodologies used. (<bold>E</bold>) Details of jack-knife analysis for information and redundancy across all genotypes. Information and redundancy values are calculated using a fraction of the total data indicated in the <inline-formula><mml:math id="inf10"><mml:mi>x</mml:mi></mml:math></inline-formula>-axis. Dashed line (bottom) indicates a redundancy value of zero, separating redundancy and synergy. Both information and redundancy are stable to the sample size, as indicated by the flat lines of best fit. Error bars are standard deviation derived from sampling 80% of the data.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.011">http://dx.doi.org/10.7554/eLife.24040.011</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig1-figsupp4-v1"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24040.012</object-id><label>Figure 1—figure supplement 5.</label><caption><title>Information and redundancy by food level.</title><p>(<bold>A</bold>) The sum of information encoded by ADF, ASI, and NSM for each food level across different genotypes is indicated by stacked bars. Information in each neuron is indicated by the legend (bottom right). Dashed lines indicate the information encoded by the combinatorial gene expression in the whole circuit, which is constant across food levels (see Supplemental Materials and methods for mathematical details). (<bold>B</bold>) Redundancy values across food levels for each genotype. (<bold>C</bold>) The optimal distribution of food input that maximizes information encoded by the whole circuit.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.012">http://dx.doi.org/10.7554/eLife.24040.012</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig1-figsupp5-v1"/></fig></fig-group></p><p>To define the redundancy of the system (<xref ref-type="bibr" rid="bib20">Schneidman et al., 2003</xref>), we considered the difference between the sum of the information independently encoded by gene expression in the ADF, ASI, and NSM neurons, and the MI obtained from their combinatorial expression (<xref ref-type="fig" rid="fig1">Figure 1B</xref>):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>I</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false">(</mml:mo><mml:msub id="XM9"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>D</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo movablelimits="false">;</mml:mo><mml:mi id="XM10">F</mml:mi><mml:mo movablelimits="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">+</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>I</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false">(</mml:mo><mml:msub id="XM11"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>S</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo movablelimits="false">;</mml:mo><mml:mi id="XM12">F</mml:mi><mml:mo movablelimits="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">+</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>I</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false">(</mml:mo><mml:msub id="XM13"><mml:mi>G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>S</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo movablelimits="false">;</mml:mo><mml:mi id="XM14">F</mml:mi><mml:mo movablelimits="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>m</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>f</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:mo>-</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>M</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>I</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false">(</mml:mo><mml:msub id="XM15"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>D</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo movablelimits="false">,</mml:mo><mml:msub id="XM16"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>S</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo movablelimits="false">,</mml:mo><mml:msub id="XM17"><mml:mi>G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>S</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo movablelimits="false">;</mml:mo><mml:mi id="XM18">F</mml:mi><mml:mo movablelimits="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:munder></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Conceptually, redundancy occurs when the whole is less than the sum of parts (<inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>R</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), whereas synergy occurs when the whole is greater than the sum of parts (<inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>R</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>This analysis revealed that ASI, ADF, and NSM neurons encode <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> bits of information about food abundance in wild-type animals (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), which is in the same range of information encoded by other biochemical pathways (<xref ref-type="bibr" rid="bib9">Cheong et al., 2011</xref>), and it is consistent with the requirement for sensing the two states (boom or bust) experienced by <italic>C. elegans</italic> in the wild (<xref ref-type="bibr" rid="bib12">Félix and Braendle, 2010</xref>). Approximately 40% of this information is encoded redundantly in wild-type animals (<xref ref-type="fig" rid="fig1">Figure 1D–E</xref>), consistent with the genetic evidence that <italic>tph-1</italic> and <italic>daf-7</italic> act in parallel pathways to modulate lifespan (<xref ref-type="bibr" rid="bib11">Entchev et al., 2015</xref>). <italic>tph-1(-)</italic> and <italic>daf-7(-)</italic> mutants show respective increases and decreases in food information (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), consistent with our prior decoding analysis. <italic>tph-1(-)</italic> mutants also show a modest decrease in the fraction of redundant information (<xref ref-type="fig" rid="fig1">Figure 1E</xref>), suggesting that the added information is more efficiently but less robustly encoded.</p><p>Remarkably, changes in the expression distributions of the <italic>daf-7</italic> and <italic>tph-1</italic> reporters in <italic>daf-7(-)</italic> mutants shift the encoding strategy of ASI, ADF, and NSM from redundancy to synergy (<xref ref-type="fig" rid="fig1">Figure 1C–D</xref>), such that <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mrow><mml:mn>40</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the total information in the circuit is now encoded synergistically (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). This effect is not due to the loss of ASI function in <italic>daf-7(-)</italic> mutants, as we observed the same shift to synergy when only <italic>tph-1(-)</italic> expressing neurons are analyzed (<xref ref-type="fig" rid="fig1">Figure 1F–H</xref>), indicating that crosstalk between <italic>daf-7</italic> and <italic>tph-1</italic> as well as <italic>daf-7</italic> autoregulation control the coding strategy adopted by the circuit. Importantly, the coding strategy shift is <italic>daf-7-</italic>specific, as disruption of <italic>tph-1</italic> does not result in a similar phenotype (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In the <italic>tph-1(-); daf-7(-)</italic> double mutant, cross- and self-regulation are abolished, and ASI, ADF, and NSM neurons approach the independence regime (<inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) (<xref ref-type="fig" rid="fig1">Figure 1C–E</xref>), confirming the idea that redundancy and synergy arise from the communication between neurons via <italic>daf-7</italic> and <italic>tph-1</italic>.</p><p>The same information-theoretic analysis can be applied to quantify more directly the contribution of <italic>daf-7</italic> and <italic>tph-1</italic> to the food-responsiveness of the physiological output. The lifespan response to food abundance consists of <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula> bits of information in wild-type animals, and approximately 80% of this food information is lost in the <italic>tph-1(-); daf-7(-)</italic> double mutant (<xref ref-type="fig" rid="fig1">Figure 1I</xref>), strengthening our previous assertion that the majority of the food information encoded in the lifespan response is mediated by <italic>tph-1</italic> and <italic>daf-7</italic>. While other genetic pathways may also play important roles, this central role of <italic>tph-1</italic> and <italic>daf-7</italic> suggests that their coding features weigh heavily on the physiological outcome.</p><p>Multicellular coding strategies rely on response correlations between cells (<xref ref-type="bibr" rid="bib20">Schneidman et al., 2003</xref>). Specifically, redundancy can be dissected into two components: the signal correlation, which reflects correlated average responses (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) and increases redundancy; and the noise correlation, which captures co-fluctuations among different cells under fixed food levels (<xref ref-type="fig" rid="fig2">Figure 2B–C</xref>) and promotes synergy (<xref ref-type="bibr" rid="bib20">Schneidman et al., 2003</xref>) (Appendix). As opposed to the wild-type animals, where the negligible value of noise correlation leads to redundancy (<xref ref-type="fig" rid="fig2">Figure 2D–E</xref>), all mutants display a general increase of noise correlations. <italic>tph-1(-)</italic> animals retain redundancy by compensating this effect with an increase of signal correlation; however, this balance shifts in the <italic>daf-7(-)</italic> mutant due to the dramatic reduction of signal correlation (<xref ref-type="fig" rid="fig2">Figure 2F</xref>), bringing the system to the synergistic regime (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). The <italic>tph-1(-); daf-7(-)</italic> double mutant has nearly equal signal and noise correlations which generate independent encoding.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.24040.013</object-id><label>Figure 2.</label><caption><title>Signal and noise correlations influence redundancy and synergy.</title><p>(<bold>A</bold>–<bold>B</bold>) Hypothetical expression distributions of two neurons at three food levels, illustrating signal and noise correlations and their effects on redundancy (<xref ref-type="bibr" rid="bib20">Schneidman et al., 2003</xref>). Centre: their 2D distributions. Top and side: the distributions of each neuron. Signal correlation between two neurons across three food levels, and noise correlation at one selected food level are denoted by dotted lines marked ‘S’ and ‘N’ in (<bold>A</bold>) and (<bold>B</bold>), respectively. (<bold>C</bold>) shows how signal and noise correlations are related to redundancy and synergy as previously established (<xref ref-type="bibr" rid="bib20">Schneidman et al., 2003</xref>). When signal correlations are higher (<bold>A</bold>), each neuron provides similar information (top and side distributions), reflecting redundancy. When noise correlations are higher (<bold>B</bold>), the combinatorial expression shows reduced overlaps and contains more information than individual neurons, providing synergy. (<bold>D</bold>–<bold>E</bold>) The effects of <italic>daf-7</italic> and <italic>tph-1</italic> on redundancy and synergy are explained by their effects on the signal correlation (<bold>D</bold>) and noise correlation (<bold>E</bold>). (<bold>F</bold>) Signal and noise correlation in each genotype and their relation to redundancy and synergy as indicated in (<bold>C</bold>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.013">http://dx.doi.org/10.7554/eLife.24040.013</ext-link></p><p><supplementary-material id="SD6-data"><object-id pub-id-type="doi">10.7554/eLife.24040.014</object-id><label>Figure 2—source data 1.</label><caption><title>Signal and noise correlations across genotypes.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.014">http://dx.doi.org/10.7554/eLife.24040.014</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-24040-fig2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig2-v1"/></fig></p><p>Redundancy and synergy is strongly affected by noise and correlation among neurons. To characterize their effects, we rescaled noise and correlations in the original response distributions of <italic>daf-7</italic> and <italic>tph-1</italic> over a biologically relevant range (<xref ref-type="fig" rid="fig3">Figure 3</xref>, Appendix). In wild-type animals, redundancy is highly sensitive to noise, and weakly sensitive to correlation, providing a rationale for <italic>daf-7</italic> in noise reduction (<xref ref-type="bibr" rid="bib11">Entchev et al., 2015</xref>). <italic>tph-1(-)</italic> mutants displayed increased sensitivity to both noise and correlations. Redundancy in <italic>daf-7(-)</italic> mutants was more sensitive to correlation than noise, a reversal of the wild-type situation. <italic>tph-1(-); daf-7(-)</italic> double mutants were less sensitive to noise and correlations than either single mutant. These results suggest that the sensitivity of redundancy to noise is controlled by <italic>daf-7</italic>, while robustness to correlation is maintained by both <italic>daf-7</italic> and <italic>tph-1</italic>.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.24040.015</object-id><label>Figure 3.</label><caption><title>Interplay between noise and correlation affects redundancy.</title><p>(<bold>A</bold>) Heat maps showing redundancy when correlation and noise are scaled from their baseline values in wild-type and mutants. Redundancy values are indicated by legend. Contour lines denote equal redundancy. The number of contour lines crossed along each axis indicates the sensitivity to that parameter. (<bold>B</bold>) The steps leading from genes to coding strategy.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.015">http://dx.doi.org/10.7554/eLife.24040.015</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig3-v1"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24040.016</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Sensitivity analysis of channel capacity, signal, and noise correlation.</title><p>Heat maps showing how channel capacity (<bold>A</bold>), signal correlation (<bold>B</bold>) and noise correlation (<bold>C</bold>) vary under a rescaling of the baseline covariance matrix. For this analysis, response distributions were modeled using multivariate normal distributions (see Appendix).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.016">http://dx.doi.org/10.7554/eLife.24040.016</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig3-figsupp1-v1"/></fig></fig-group></p><p>Redundancy or synergy in <italic>daf-7</italic> and <italic>tph-1</italic> expressing neurons serves as one constraint but does not necessarily lead to the same coding strategy in their targets. The coding strategy used by these targets will depend on their connectivity to ASI, ADF, and NSM, as well as their noise, correlation, and dynamic range. Since little is known about the immediate targets of TGF<inline-formula><mml:math id="inf17"><mml:mi>β</mml:mi></mml:math></inline-formula> and serotonin signaling in relation to the food response in <italic>C. elegans</italic>, we considered a minimal model of three ideal sensors detecting an input and transmitting to a target that integrates linearly their signals (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, Appendix). This simple model shows that decreasing signal-to-noise ratio favors synergy (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, Appendix), in agreement with the observation that <italic>daf-7(-)</italic> mutants show reduced signal-to-noise, and adopt synergistic encoding (<xref ref-type="fig" rid="fig1">Figure 1D–F</xref>). This model also explains the decrease in synergy in <italic>tph-1(-); daf-7(-)</italic> double mutants compared to <italic>daf-7(-)</italic> single mutants (<xref ref-type="fig" rid="fig1">Figure 1D–F</xref>): loss of <italic>tph-1</italic> increases signal separation (<xref ref-type="bibr" rid="bib11">Entchev et al., 2015</xref>), which increases signal-to-noise, thus reducing synergy. Thus, that signal-to-noise ratios can contribute significantly to the coding strategy.<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.24040.017</object-id><label>Figure 4.</label><caption><title>Computational model reveals advantages of redundancy.</title><p>(<bold>A</bold>) Model for information encoding and transmission, where three sensors activate one target that integrates their signals linearly (see Appendix). (<bold>B</bold>) Effect of signal-to-noise ratio on coding strategy. (<bold>C</bold>) Effect of coding strategy on transmitted information. (<bold>D</bold>) Sensors that transmit more information tend to use redundancy.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.017">http://dx.doi.org/10.7554/eLife.24040.017</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig4-v1"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24040.018</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Gaussian model of sensory neurons and information transmission.</title><p>(<bold>A</bold>) Illustration of the model. <inline-formula><mml:math id="inf18"><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> represent single sensors receiving information from the binary input. The sensor response is then combined linearly by the output. (<bold>B</bold>) The joint sensor response distribution is modeled as a three-dimensional Gaussian centered at input-dependent average values. (<bold>C</bold>, <bold>D</bold>, <bold>E</bold>, <bold>F</bold>) display the result of the numerical analysis of the model. Each point represents a specific choice of the parameters used in the model. For all sampled parameter sets (<inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500000</mml:mn></mml:mrow></mml:math></inline-formula>), we obtained the relevant information-theoretic measures shown. Red and blue colors are used to distinguish redundant and synergistic regimes, respectively. (<bold>G</bold>) The sampled conditions sliced according to dynamic range (<inline-formula><mml:math id="inf20"><mml:mi>a</mml:mi></mml:math></inline-formula>) and noise (<inline-formula><mml:math id="inf21"><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>) (left). Redundant configurations populating the low signal-to-noise ratio (<inline-formula><mml:math id="inf22"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) provide typically low information. <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> becomes a discriminant between redundancy and synergy when applying a non-zero cutoff to the information encoded by the sensors (right). (<bold>H</bold>) Fraction of redundant/synergistic configurations obtained by in the numerical exploration of the model (left). When sensors are required to encode a minimum level of information, parametric configurations with low <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> are forced to be synergistic, providing a rational for the coding strategy switch observed in <italic>daf-7(-)</italic>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24040.018">http://dx.doi.org/10.7554/eLife.24040.018</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24040-fig4-figsupp1-v1"/></fig></fig-group></p><p>Our model also illustrates the advantages of redundancy in the case of linear integration. Redundant strategies increase the minimum information transmitted to a downstream target when compared to a synergistic encoding (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Additionally, redundant encoding not only allows higher information transmission, but can also be accommodated by a broader set of signaling parameters (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), avoiding the need to fine tune biological properties. When considering lifespan as the downstream target, our model suggests that lifespan responsiveness to food should decrease in <italic>daf-7(-)</italic> mutants, because wild-type animals employ redundancy, whereas <italic>daf-7(-)</italic> mutants employ a synergistic encoding. Indeed, we find that the ability to accurately discriminate between different food inputs based on lifespan is degraded in <italic>daf-7(-)</italic> mutants (<xref ref-type="fig" rid="fig1">Figure 1I</xref>) (<xref ref-type="bibr" rid="bib11">Entchev et al., 2015</xref>).</p><p>By extending the analysis of our previous work, we have found that the ADF, NSM and ASI neurons employ a redundant strategy to encode food information. Critically, this redundant encoding strategy is controlled by <italic>daf-7</italic> TGF<inline-formula><mml:math id="inf25"><mml:mi>β</mml:mi></mml:math></inline-formula> and modified by <italic>tph-1</italic> tryptophan hydroxylase; this is a novel effect of neuromodulators on circuit function. In particular, we revealed two roles for <italic>daf-7</italic>: as an encoder of food information, and as a regulator of redundancy via regulation of <italic>tph-1</italic>. In principle, redundancy and synergy could be specified by many different biological mechanisms, with obvious candidates being developmental changes in sensor types or numbers in a neural circuit. These mechanisms are ruled out in <italic>daf-7(-)</italic> and <italic>tph-1(-)</italic> animals, as the mutations do not affect the development of the ASI, ADF, and NSM neurons, which remain food-responsive. Instead, we show that <italic>daf-7</italic> and <italic>tph-1</italic> influence information processing via effects on the signal and noise properties of these sensory neurons, and on their correlations, representing additional roles for these genes in controlling information encoding. The discovery of other genes that regulate the signal-to-noise ratio will likely provide further insights into genetic regulatory mechanisms that modulate neural coding.</p></sec><sec id="s3"><title>Computational methods</title><sec id="s3-1"><title>Minimization and quantification of experimental noise</title><p>Information theory relies on accurate estimates of response distributions, requiring the minimisation of experimental variability. We took several steps to achieve this. First, we only considered animals oriented in a dorso-ventral position. The microfluidic chip was constructed to bias animals towards this correct orientation, the orientation was checked during automated cell identification and verified manually, ensuring that only image stacks with animals in dorso-ventral orientations were used in the analysis. Second, we used direct imaging of transcriptional fusions to fluorescent protein reporters integrated in single copy. This approach ensures that biological variance in promoter activity is not artificially washed out by averaging in conventional high-copy reporters that are more traditionally used to generate <italic>C. elegans</italic> transgenics. Using fluorescent reporters also eliminates experimental noise associated with antibody staining due to variability in fixation, in permeabilizing the <italic>C. elegans</italic> cuticle, and in signal amplification from secondary antibodies. Third, we minimized bleaching by using a combination of low excitation from an LED light source, and rapid image acquisition using a Piezo Z stage (Prior Scientific) that precisely moves the sample in the Z axis at high speed.</p><p>In addition, we used simultaneous quantification of mCherry and Venus/YFP driven by the same promoter to estimate our experimental noise (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). We generated animals with <italic>Pdaf-7::mCherry</italic> and <italic>Pdaf-7::Venus</italic> reporters integrated at single copy in precise genomic locations on LG I and LG II, respectively (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). These animals were shifted to four different food levels and imaged 1 day after the food shift. This experimental measurement incorporates experimental noise associated with different fluorescent proteins (mCherry and Venus) and different chromosomal locations for reporters, as well as other methodological noise. We found that the two measurements were in good agreement (<inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>R</mml:mi><mml:mo>∼</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>). Dissecting the variance in these measurements showed that 30% (<inline-formula><mml:math id="inf27"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) of the observed variability in these measurements was due to variability between the mCherry and Venus readouts. We note that this variability includes intrinsic noise as the reporters are on different chromosomes; the actual experimental variability would therefore be lower, since intrinsic noise is non-zero.</p></sec><sec id="s3-2"><title>Computational analysis</title><p>The computational analysis of all the data was performed using custom-made C++ programs and built-in implementations of standard multivariate analysis algorithms in R (<xref ref-type="bibr" rid="bib19">R Core Team, 2016</xref>). C++ programs are available through GitHub repositories (<ext-link ext-link-type="uri" xlink:href="https://github.com/giovannidiana/Information">https://github.com/giovannidiana/Information</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/giovannidiana/KDE">https://github.com/giovannidiana/KDE</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/giovannidiana/ModelRS">https://github.com/giovannidiana/ModelRS</ext-link>). Mathematical details of these procedures and the results are discussed in the Appendix.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank the Bargmann and Horvitz labs for reagents. Some strains were provided by the CGC, which is funded by NIH Office of Research Infrastructure Programs (P40 OD010440). We also thank R Endres, J Clarke, S Oliferenko, P Gordon, E Makeyev, O Marin, and D Passaro for comments on the manuscript; and B Handley, K Gers-Barlag, O Leyshon, and H Tunbak for technical assistance. This research was supported by the Wellcome Trust (Project Grant 087146 to QC), BBSRC (BB/H020500/1 and BB/M00757X/1 to QC), European Research Council (NeuroAge 242666 to QC), US National Institutes of Health (R01AG035317 and R01GM088333 to HL), and US National Science Foundation (0954578 to HL, 0946809 GRFP to MZ).</p></ack><sec id="s4" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>GD, Conceived and performed the computational analyses, Wrote the paper</p></fn><fn fn-type="con" id="con2"><p>DSP, Devised and performed experiments, Wrote the paper</p></fn><fn fn-type="con" id="con3"><p>EVE, Devised and performed experiments</p></fn><fn fn-type="con" id="con4"><p>MZ, Designed the high-throughput imaging system, Built hardware, Wrote software for the operation of this system</p></fn><fn fn-type="con" id="con5"><p>HL, Conceived the computational analyses, Designed the high-throughput imaging system</p></fn><fn fn-type="con" id="con6"><p>QLC, Conceived the computational analyses, Devised experiments, Wrote the paper</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arimoto</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>An algorithm for computing the capacity of arbitrary discrete memoryless channels</article-title><source>IEEE Transactions on Information Theory</source><volume>18</volume><fpage>14</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/TIT.1972.1054753</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural correlations, population coding and computation</article-title><source>Nature Reviews Neuroscience</source><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn1888</pub-id><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blahut</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Computation of channel capacity and rate-distortion functions</article-title><source>IEEE Transactions on Information Theory</source><volume>18</volume><fpage>460</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1109/TIT.1972.1054855</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Information theory and neural coding</article-title><source>Nature neuroscience</source><volume>2</volume><fpage>947</fpage><lpage>957</lpage><pub-id pub-id-type="doi">10.1038/14731</pub-id><pub-id pub-id-type="pmid">10526332</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowman</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>An alternative method of cross-validation for the smoothing of density estimates</article-title><source>Biometrika</source><volume>71</volume><fpage>353</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1093/biomet/71.2.353</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowsher</surname><given-names>CG</given-names></name><name><surname>Swain</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Environmental sensing, information transfer, and cellular decision-making</article-title><source>Current Opinion in Biotechnology</source><volume>28</volume><fpage>149</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.copbio.2014.04.010</pub-id><pub-id pub-id-type="pmid">24846821</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brenner</surname><given-names>N</given-names></name><name><surname>Strong</surname><given-names>SP</given-names></name><name><surname>Koberle</surname><given-names>R</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Synergy in a neural code</article-title><source>Neural Computation</source><volume>12</volume><fpage>1531</fpage><lpage>1552</lpage><pub-id pub-id-type="doi">10.1162/089976600300015259</pub-id><pub-id pub-id-type="pmid">10935917</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chacón</surname><given-names>JE</given-names></name><name><surname>Duong</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multivariate plug-in bandwidth selection with unconstrained pilot bandwidth matrices</article-title><source>TEST</source><volume>19</volume><fpage>375</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1007/s11749-009-0168-4</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheong</surname><given-names>R</given-names></name><name><surname>Rhee</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>CJ</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name><name><surname>Levchenko</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Information transduction capacity of noisy biochemical signaling networks</article-title><source>Science</source><volume>334</volume><fpage>354</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1126/science.1204553</pub-id><pub-id pub-id-type="pmid">21921160</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cover</surname><given-names>TM</given-names></name><name><surname>Thomas</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Elements of information theory</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Entchev</surname><given-names>EV</given-names></name><name><surname>Patel</surname><given-names>DS</given-names></name><name><surname>Zhan</surname><given-names>M</given-names></name><name><surname>Steele</surname><given-names>AJ</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Ch'ng</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A gene-expression-based neural code for food abundance that modulates lifespan</article-title><source>eLife</source><volume>4</volume><elocation-id>e06259</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06259</pub-id><pub-id pub-id-type="pmid">25962853</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Félix</surname><given-names>MA</given-names></name><name><surname>Braendle</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The natural history of Caenorhabditis elegans</article-title><source>Current Biology</source><volume>20</volume><fpage>R965</fpage><lpage>R969</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2010.09.050</pub-id><pub-id pub-id-type="pmid">21093785</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gendron</surname><given-names>CM</given-names></name><name><surname>Chung</surname><given-names>BY</given-names></name><name><surname>Pletcher</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The sensory system: More than just a window to the external world</article-title><source>Communicative &amp; Integrative Biology</source><volume>8</volume><elocation-id>e1017159</elocation-id><pub-id pub-id-type="doi">10.1080/19420889.2015.1017159</pub-id><pub-id pub-id-type="pmid">26480026</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>MC</given-names></name><name><surname>Marron</surname><given-names>JS</given-names></name><name><surname>Park</surname><given-names>BU</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>A simple root <italic>n</italic> bandwidth selector</article-title><source>The Annals of Statistics</source><volume>19</volume><fpage>1919</fpage><lpage>1932</lpage><pub-id pub-id-type="doi">10.1214/aos/1176348378</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnaswamy</surname><given-names>S</given-names></name><name><surname>Spitzer</surname><given-names>MH</given-names></name><name><surname>Mingueneau</surname><given-names>M</given-names></name><name><surname>Bendall</surname><given-names>SC</given-names></name><name><surname>Litvin</surname><given-names>O</given-names></name><name><surname>Stone</surname><given-names>E</given-names></name><name><surname>Pe'er</surname><given-names>D</given-names></name><name><surname>Nolan</surname><given-names>GP</given-names></name><name><surname>Pe’er</surname><given-names>D</given-names></name><name><surname>Nolan</surname><given-names>GP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Conditional density-based analysis of T cell signaling in single-cell data</article-title><source>Science</source><volume>346</volume><fpage>621</fpage><pub-id pub-id-type="doi">10.1126/science.1250689</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levchenko</surname><given-names>A</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cellular noise and information transmission</article-title><source>Current Opinion in Biotechnology</source><volume>28</volume><fpage>156</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/j.copbio.2014.05.002</pub-id><pub-id pub-id-type="pmid">24922112</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loftsgaarden</surname><given-names>DO</given-names></name><name><surname>Quesenberry</surname><given-names>CP</given-names></name></person-group><year iso-8601-date="1965">1965</year><article-title>A nonparametric estimate of a multivariate density function</article-title><source>The Annals of Mathematical Statistics</source><volume>36</volume><fpage>1049</fpage><lpage>1051</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177700079</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puchalla</surname><given-names>JL</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name><name><surname>Harris</surname><given-names>RA</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Redundancy in the population code of the retina</article-title><source>Neuron</source><volume>46</volume><fpage>493</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.03.026</pub-id><pub-id pub-id-type="pmid">15882648</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="software"><person-group person-group-type="author"><collab>R Core Team</collab></person-group><year iso-8601-date="2016">2016</year><source>R: a language and environment for statistical computing</source><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Statistical Computing</publisher-name></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname><given-names>E</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Synergy, redundancy, and independence in population codes</article-title><source>Journal of Neuroscience</source><volume>23</volume><fpage>11539</fpage><lpage>11553</lpage><pub-id pub-id-type="pmid">14684857</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname><given-names>E</given-names></name><name><surname>Puchalla</surname><given-names>JL</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Harris</surname><given-names>RA</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Synergy from silence in a combinatorial neural code</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>15732</fpage><lpage>15741</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0301-09.2011</pub-id><pub-id pub-id-type="pmid">22049416</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>Multivariate density estimation: theory, practice, and visualization</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Selimkhanov</surname><given-names>J</given-names></name><name><surname>Taylor</surname><given-names>B</given-names></name><name><surname>Yao</surname><given-names>J</given-names></name><name><surname>Pilko</surname><given-names>A</given-names></name><name><surname>Albeck</surname><given-names>J</given-names></name><name><surname>Hoffmann</surname><given-names>A</given-names></name><name><surname>Tsimring</surname><given-names>L</given-names></name><name><surname>Wollman</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Accurate information transmission through dynamic biochemical signaling networks</article-title><source>Science</source><volume>346</volume><fpage>1370</fpage><lpage>1373</lpage><pub-id pub-id-type="doi">10.1126/science.1254933</pub-id><pub-id pub-id-type="pmid">25504722</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A mathematical theory of communication</article-title><source>Bell System Technical Journal</source><volume>27</volume><fpage>379</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Dubuis</surname><given-names>JO</given-names></name><name><surname>Petkova</surname><given-names>MD</given-names></name><name><surname>Gregor</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Positional information, positional error, and readout precision in morphogenesis: a mathematical framework</article-title><source>Genetics</source><volume>199</volume><fpage>39</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1534/genetics.114.171850</pub-id><pub-id pub-id-type="pmid">25361898</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Walczak</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Information transmission in genetic regulatory networks: a review</article-title><source>Journal of Physics: Condensed Matter</source><volume>23</volume><elocation-id>153102</elocation-id><pub-id pub-id-type="doi">10.1088/0953-8984/23/15/153102</pub-id><pub-id pub-id-type="pmid">21460423</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uda</surname><given-names>S</given-names></name><name><surname>Saito</surname><given-names>TH</given-names></name><name><surname>Kudo</surname><given-names>T</given-names></name><name><surname>Kokaji</surname><given-names>T</given-names></name><name><surname>Tsuchiya</surname><given-names>T</given-names></name><name><surname>Kubota</surname><given-names>H</given-names></name><name><surname>Komori</surname><given-names>Y</given-names></name><name><surname>Ozaki</surname><given-names>Y</given-names></name><name><surname>Kuroda</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Robustness and compensation of information transmission of signaling pathways</article-title><source>Science</source><volume>341</volume><fpage>558</fpage><lpage>561</lpage><pub-id pub-id-type="doi">10.1126/science.1234511</pub-id><pub-id pub-id-type="pmid">23908238</pub-id></element-citation></ref></ref-list><app-group><app id="app1"><title>Appendix</title><boxed-text><sec id="s7" sec-type="appendix"><title>Supplementary computational methods</title><p>To uncover the information processing features of the <italic>daf-7</italic>/<italic>tph-1</italic> genetic circuit embedded in the ASI, ADF and NSM neurons, we performed an information-theoretic analysis of the gene expression responses of <italic>daf-7</italic> in the ASI neuron, and <italic>tph-1</italic> in the ADF and NSM neurons. In the main text we introduced the mutual information (MI) as a measure of the correlation between food level and gene expression. In this section we discuss in greater detail the properties of this quantity and the procedure used to estimate MI from our gene expression data. The type of encoding system that we are interested in maps an input <inline-formula><mml:math id="inf28"><mml:mi>F</mml:mi></mml:math></inline-formula> (food level) taking <inline-formula><mml:math id="inf29"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula> distinct values <inline-formula><mml:math id="inf30"><mml:mrow><mml:mo>{</mml:mo><mml:msub id="XM22"><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi id="XM20" mathvariant="normal">⋯</mml:mi><mml:mo>,</mml:mo><mml:msub id="XM23"><mml:mi>f</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> onto three continuous variables denoted by the vector <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub id="XM27"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM28"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM29"><mml:mi>G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (gene expression in the three neurons).</p></sec><sec id="s8" sec-type="appendix"><title>Mutual information</title><p>The multivariate gene expression response under a specific food condition is given by the set of conditional probabilities <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, (<inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn id="XM30">1</mml:mn><mml:mo>,</mml:mo><mml:mi id="XM31" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub id="XM32"><mml:mi>N</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). To characterize the information transmission of a communication system we also need to specify the probabilities <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM33">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with which the ADF-ASI-NSM encoder is exposed to each food condition. Given the input probabilities <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM34">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> we can compute averages across food level, in particular the marginal probabilities of gene expression<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM35">G</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:munderover><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>From input and response distributions we can build up three information entropies. First, the joint information entropy of both food and gene expression is defined as (<xref ref-type="bibr" rid="bib10">Cover and Thomas, 2006</xref>)<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM37">F</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM38">G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM39">G</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM40">F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM41">G</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM42">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and it measures the variability of input and output. Second, we can quantify the variability of the gene expression response to food by the conditional entropy<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mi>G</mml:mi><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:munderover><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Third, the entropy of the marginal distributions in <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM43">G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM44">G</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM45">G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>measures the variability of the average response. The mutual information defined as<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo>;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∫</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∫</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>≥</mml:mo><mml:mn>0.</mml:mn></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>is always positive due to the log-sum inequality and it can be expressed as the difference between the gene expression entropy and the conditional entropy with respect to food level, that is<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM65">G</mml:mi><mml:mo>;</mml:mo><mml:mi id="XM66">F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM67">G</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which yields to the standard interpretation of <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> as the amount of information entropy shared between stochastic variables.</p><p>As mentioned in the main text and <xref ref-type="fig" rid="fig1">Figure 1A</xref>, the mutual information is strongly affected by the signal-to-noise ratio (<inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>). For univariate distributions, we use the definition<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>≡</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mi>var</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM68">G</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mi>var</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mi>var</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub></mml:mfrac></mml:msqrt></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>var</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the variance under the condition <inline-formula><mml:math id="inf39"><mml:mi>F</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf40"><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mo id="XM69">⋅</mml:mo><mml:mo>⟩</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub></mml:math></inline-formula> denotes the average across all conditions.</p><p>MI can be decomposed as<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo>;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where the components <inline-formula><mml:math id="inf41"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM73"><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> are defined as<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM74"><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>≡</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mpadded width="+2.8pt"><mml:mi>G</mml:mi></mml:mpadded><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM75">G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>and represent the relative entropy between conditional and average response.</p><p>Our estimates of mutual information provide a lower bound of the true information encoded by <italic>tph-1</italic> and <italic>daf-7</italic> due to noise inherent in all experiments.</p></sec><sec id="s9" sec-type="appendix"><title>Channel capacity</title><p>A common question in biology is to understand how phenotypic changes are related to the environmental input. To address this question it is natural to design experiments where relevant input variables are controlled. These types of experiments provide a good sampling of the responses, but the frequencies of environmental conditions at which biological systems are exposed in the wild are not always known. On the other hand, the level of information encoded about the environment depends on the input distribution. A common procedure to infer the input distribution is to assume that the set of gene expression responses is designed to maximize the information stored (<xref ref-type="bibr" rid="bib26">Tkačik and Walczak, 2011</xref>; <xref ref-type="bibr" rid="bib23">Selimkhanov et al., 2014</xref>; <xref ref-type="bibr" rid="bib27">Uda et al., 2013</xref>). With this assumption we can obtain the food distribution by maximising the mutual information between food and gene expression. The maximal MI achievable given the set of conditional responses is known as channel capacity<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder id="XM79"><mml:mi>max</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM76">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM80"><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM77">G</mml:mi><mml:mo>;</mml:mo><mml:mi id="XM78">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and it is an intrinsic property of the encoding system.</p><p>An important aspect of MI is that all the relative entropies <inline-formula><mml:math id="inf42"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM81"><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> in the decomposition (10) become identical under the optimality condition, which is easy to prove by maximizing the action<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM82">G</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf43"><mml:mi>λ</mml:mi></mml:math></inline-formula> is a Lagrange multiplier to assure the normalization of the input distribution. By considering the derivative over <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM83"><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> we get<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM84"><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM85"><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which implies that at the maximum, all the <inline-formula><mml:math id="inf45"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM86"><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> are equal to <inline-formula><mml:math id="inf46"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula>. Therefore, since the channel capacity is defined as an average of <inline-formula><mml:math id="inf47"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM87"><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf48"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM88"><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf49"><mml:mi>k</mml:mi></mml:math></inline-formula>. This property implies that the optimal input distribution obtained by maximizing MI is such that all the conditional responses are equally distant from their average, when relative entropy is used as a measure of distance between probability distributions.</p><p>In <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5A</xref> we compare the channel capacity obtained from <italic>tph-1</italic>/<italic>daf-7</italic> expression in ADF, ASI and NSM neurons (dotted line) with the components <inline-formula><mml:math id="inf50"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM89"><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> of the mutual information between gene expression and food abundance in each neuron (ADF, ASI, NSM) for all genetic backgrounds. For this comparison we first obtained the channel capacity and the optimal input distribution from the three-dimensional data for each genotype (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5C</xref>) and then we used this optimal distribution to calculate the mutual information of each neuron. The components of the mutual information for individual neurons are obtained from <xref ref-type="disp-formula" rid="equ11">Equation (11)</xref> by using the corresponding marginal distribution. Unlike the components <inline-formula><mml:math id="inf51"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM90"><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> of the maximized joint mutual information, the <inline-formula><mml:math id="inf52"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM91"><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> are not constant over the food level, reflecting the fact that single neurons are optimized for different input distributions (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>).</p><p>The optimal input frequencies (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5C</xref>) reveal that wild-type animals encode the most information when they are most likely to encounter the highest, an intermediate, and the lowest food levels. This result implies that wild-type animals are best at detecting these food levels, compatible with the boom and bust lifestyle of <italic>C. elegans</italic> in the wild (<xref ref-type="bibr" rid="bib12">Félix and Braendle, 2010</xref>). This optimal is altered in <italic>daf-7(-)</italic> mutants (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5C</xref>), indicating that it is genetically controlled. By maximizing the mutual information between individual neurons and food conditions we find that each neuron is specialized to sense different food levels (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>), which broadens their combined range of detectable food levels. For example, <italic>tph-1</italic> expression in ADF in wild-type animals is best at detecting the food extremes (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3A</xref>). At these extreme food levels, ADF carries more information than at other food levels. Thus specialization among food sensing neurons ultimately leads to food-dependent heterogeneity in coding (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5A</xref>).</p><p>We note that the switch from redundancy in wild-type to synergy in <italic>daf-7(-)</italic> mutants still occurs when we use the wild-type optimal input frequency for calculating redundancy values for <italic>daf-7(-)</italic> mutants. Thus, our conclusions are not sensitive to the choice of using channel capacity and the corresponding optimal input distribution for each genotype. All the estimates of channel capacity in this work were done by using the standard Arimoto-Blahut algorithm (<xref ref-type="bibr" rid="bib1">Arimoto, 1972</xref>; <xref ref-type="bibr" rid="bib3">Blahut, 1972</xref>).</p></sec><sec id="s10" sec-type="appendix"><title>Redundancy and synergy</title><p>In the main text we introduce redundancy <inline-formula><mml:math id="inf53"><mml:mi>R</mml:mi></mml:math></inline-formula> as the difference (<xref ref-type="bibr" rid="bib20">Schneidman et al., 2003</xref>)<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM92"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi id="XM93">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM94"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi id="XM95">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM96"><mml:mi>G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi id="XM97">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM98">G</mml:mi><mml:mo>;</mml:mo><mml:mi id="XM99">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and synergy as the negative of the redundancy. By following the work of Schneidman <italic>et al.</italic> the redundancy can be written as the difference between signal and noise correlation defined as<disp-formula id="equ16"><label>(s)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-16_3"><mml:mtext>(16)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-17_3"><mml:mtext>(17)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mi>C</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo>;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf54"><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM120">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is the ‘shuffle’ information defined as<disp-formula id="equ17"><label>(18)</label><mml:math id="m17"><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM127">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo largeop="true" symmetric="true">∫</mml:mo></mml:mstyle><mml:msup><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mpadded width="+2.8pt"><mml:mi>G</mml:mi></mml:mpadded><mml:mstyle displaystyle="true"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder></mml:mstyle><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM128">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM129">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM130">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and corresponds to a modified version of the mutual information between gene expression and food level where the joint distribution is replaced by the product of the marginal densities <inline-formula><mml:math id="inf55"><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM133">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>,<disp-formula id="equ18"><label>(19)</label><mml:math id="m18"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM134">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>By using the definition of the mutual information per neuron we can rewrite the signal correlation in the form of a relative entropy<disp-formula id="equ19"><label>(20)</label><mml:math id="m19"><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo largeop="true" symmetric="true">∫</mml:mo></mml:mstyle><mml:msup><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mpadded width="+2.8pt"><mml:mi>G</mml:mi></mml:mpadded><mml:mstyle displaystyle="true"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder></mml:mstyle><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM142">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM143">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM145"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM146"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM147"><mml:mi>G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>which shows that <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> is a non-negative quantity. As opposite to the signal correlation, which can only increase the level of redundancy, noise correlation can be positive or negative. Depending on the sign of <inline-formula><mml:math id="inf57"><mml:mi>R</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ15">Equation (15)</xref> the system operates in a redundant (<inline-formula><mml:math id="inf58"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) or synergistic (<inline-formula><mml:math id="inf59"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) regime.</p><p>These information-theoretic measures of correlation reveal changes in different genotypes that contribute to shifts in coding strategy and capture different features of the interaction between the neurons. Consider for instance the case of independent encoders where the probability distribution of the joint response is factorized into the product of the responses of each neuron<disp-formula id="equ20"><label>(21)</label><mml:math id="m20"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM149">s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In this case the noise correlation vanishes identically, however, signal correlation can be non-zero due to the correlation induced by the stimulus, thus we obtain the intuitive result that the level of redundancy in a system of independent encoders is always non-negative. Therefore, the synergistic encoding that we observe in the <italic>daf-7(-)</italic> mutant is caused by the change in the interaction network of ADF, ASI and NSM neurons. In the wild-type this network is tuned to guarantee a robust encoding of food abundance. When <italic>daf-7</italic> is knocked-out, the sign of <inline-formula><mml:math id="inf60"><mml:mi>R</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ15">Equation (15)</xref> changes, which namely corresponds to a switch from redundancy to synergy.</p><p>Analogously to the mutual information, also redundancy can be decomposed as<disp-formula id="equ21"><label>(22)</label><mml:math id="m21"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM151"><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM150"><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where we defined the food components <inline-formula><mml:math id="inf61"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM152"><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> as<disp-formula id="equ22"><label>(23)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM153"><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM154"><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM155"><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM156"><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5B</xref> we show the redundancy components at each food level across all genotypes. We observe that the quality of the encoding (synergistic or redundant) varies under different food conditions. In particular, both wild-type and <italic>tph-1(-)</italic> mutant tend to adopt a synergistic behaviour under non optimal food conditions (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3A–B</xref> for comparison with input distributions) whereas <italic>daf-7(-)</italic> mutant and <italic>tph-1(-); daf-7(-)</italic> double mutant are always synergistic. The input distribution obtained by maximizing <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM157"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM158"><mml:mi>G</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM159"><mml:mi>G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi id="XM160">F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was used as a reference food distribution for all the genotypes analyzed in this work. <italic>tph-1</italic> and <italic>daf-7</italic> promoter activity was available also for mutant strains because the reporters were separate from the endogenous genes. To confirm that the synergistic character of the encoding in the <italic>daf-7(-)</italic> mutant is not an artifact of including ASI (where <italic>daf-7</italic> is expressed) in the estimation of the redundancy, we performed the same analysis by using only ADF and NSM read-outs. As a result, by comparing wild-type and <italic>daf-7(-)</italic> mutant, we obtained the same qualitative switch from redundant to synergistic encoding as obtained from the inclusion of all neurons (<xref ref-type="fig" rid="fig1">Figure 1F–H</xref>).</p></sec><sec id="s11" sec-type="appendix"><title>Kernel density estimation</title><p>Information entropies, and thus mutual information, are functionals of the probability distribution of the readouts. To quantify the conditional distributions <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> we used Kernel Density Estimation (KDE) (<xref ref-type="bibr" rid="bib22">Scott, 1992</xref>), which provides a mathematical framework to estimate distributions of continuous variables.</p><p>Compared to the standard methodology of frequency histograms to estimate distributions, this technique does not require bin size selection. In the KDE approach, the probability density is estimated by the sum of reference distributions (kernel) centered at the observed values, thus for any expression vector <inline-formula><mml:math id="inf64"><mml:mi>g</mml:mi></mml:math></inline-formula> we have the estimated density <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> reads<disp-formula id="equ23"><label>(24)</label><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where the kernel <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>K</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:math></inline-formula> is a multivariate Gaussian distribution, the ‘bandwidth’ <inline-formula><mml:math id="inf67"><mml:mi>H</mml:mi></mml:math></inline-formula> corresponds to its variance matrix and the sum is over all the measured expressions <inline-formula><mml:math id="inf68"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub id="XM164"><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:math></inline-formula>.</p><p>An accurate estimation of the density relies on the choice of the bandwidth, which can be constant across the support of the probability or adapted to the local density. The Mean Squared Error (MSE)<disp-formula id="equ24"><label>(25)</label><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>and its integral (MISE) are commonly minimized to find the appropriate bandwidth. Selector algorithms differ in the trade-off between bias and variance of the estimator.</p><p>To check the robustness of our calculation, we compared the results obtained by using different fixed bandwidth selector algorithms (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>). In particular, we used the plug-in method (<xref ref-type="bibr" rid="bib8">Chacón and Duong, 2010</xref>), least squares cross-validation (<xref ref-type="bibr" rid="bib5">Bowman, 1984</xref>) and smoothing cross validation (<xref ref-type="bibr" rid="bib14">Jones et al., 1991</xref>), all of which provide a uniform bandwidth. The general, fixed bandwidth estimators tends to oversmooth the main part of the distribution and undersmooth the tails. To confirm that this effect did not introduce artificial biases we also used the ‘baloon’ (<inline-formula><mml:math id="inf69"><mml:mi>k</mml:mi></mml:math></inline-formula>-nearest neighbours) estimator (<xref ref-type="bibr" rid="bib17">Loftsgaarden and Quesenberry, 1965</xref>), where the probability distribution is proportional to the local density of observations (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>).</p><p>Once we obtained the conditional response distributions, averages over expression levels as in <xref ref-type="disp-formula" rid="equ7">Equation (7)</xref> were computed by evaluating <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on a three-dimensional grid (a different approach would be to resample from the obtained distribution (<xref ref-type="bibr" rid="bib15">Krishnaswamy et al., 2014</xref>). By testing different grid resolutions we found that a grid of size <inline-formula><mml:math id="inf71"><mml:msup><mml:mn>30</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math></inline-formula> was sufficient to guarantee the convergence of averages. The uncertainty in the estimation of channel capacity was obtained by calculating the variance associated with sampling the 80% of the data. As shown in <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, the estimates of both channel capacity and redundancy/synergy are robust to KDE algorithm in all genetic backgrounds.</p></sec><sec id="s12" sec-type="appendix"><title>Sample size bias</title><p>A well known issue in the estimation of channel capacity is the bias due to sample size. The general jack-knife procedure to remove this effect involves expanding the channel capacity in inverse powers of sample size (<xref ref-type="bibr" rid="bib9">Cheong et al., 2011</xref>; <xref ref-type="bibr" rid="bib23">Selimkhanov et al., 2014</xref>),<disp-formula id="equ25"><label>(26)</label><mml:math id="m25"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>N</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒪</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac id="XM168"><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and obtaining the unbiased term by a linear fit of the channel capacity calculated using increasing fraction of the data. By applying this procedure, we found a very small sample-size correction to channel capacity in all genetic backgrounds (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4A–B</xref>). The same analysis applied to the redundancy/synergy (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4C–D</xref>), showed that our conclusions are independent on the sample size. All our linear fits of channel capacity and redundancy/synergy (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4E</xref>) from 60% to 100% of the data were above the 95% of confidence level, indicating that our data is far from the undersampled regime.</p></sec><sec id="s13" sec-type="appendix"><title>Covariance sensitivity analysis</title><p>To explore how channel capacity and redundancy depend on linear correlation and noise among ADF, ASI and NSM neurons requires a way to scale these two properties in silico from the baselines obtained in each genotype in experimental measurements. To do so, we first approximated the gene expression densities as multivariate normal distributions. This approximation captures most of the global features of our three-dimensional responses and allows us to control noise and correlations in terms of covariance matrices. The Gaussian assumption was also used in our previous decoding analysis (<xref ref-type="bibr" rid="bib11">Entchev et al., 2015</xref>). The agreement between our present study and the decoding analysis shows indirectly that the Gaussian approximation can be used here for information-theoretic purposes. We used the maximum-likelihood estimates of the covariance matrices <inline-formula><mml:math id="inf72"><mml:mi>C</mml:mi></mml:math></inline-formula> for each genotype as a reference and then we transformed each entry of the covariance matrix according to the rule<disp-formula id="equ26"><label>(27)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow id="XM170"><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM169"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The transformation above rescales all the standard deviations of the responses by a factor <inline-formula><mml:math id="inf73"><mml:mi>α</mml:mi></mml:math></inline-formula> and the Pearson’s correlation index for all pairs of neurons by a factor <inline-formula><mml:math id="inf74"><mml:mi>β</mml:mi></mml:math></inline-formula>. Thus we studied the sensitivity of information-theoretic variables to noise and correlation by varying <inline-formula><mml:math id="inf75"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf76"><mml:mi>β</mml:mi></mml:math></inline-formula> over a biologically relevant range.</p><p>In the main text we presented the sensitivity analysis of redundancy, in <xref ref-type="fig" rid="fig3">Figure 3A</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A–C</xref> we show the color-coded contour maps of channel capacity, signal and noise correlation obtained by varying the parameters <inline-formula><mml:math id="inf77"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:mi>β</mml:mi></mml:math></inline-formula> from 0.5 to 2 independently. We checked numerically the positivity of the covariance matrix for all pairs of <inline-formula><mml:math id="inf79"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf80"><mml:mi>β</mml:mi></mml:math></inline-formula>. The major factor that controls information capacity in wild-type is noise, which provides a rationale for the noise regulation by <italic>daf-7</italic> revealed in our previous study (<xref ref-type="bibr" rid="bib11">Entchev et al., 2015</xref>). Scaling the linear correlation has a more pronounced effect in all the mutants and especially in the <italic>daf-7(-)</italic> mutant. This is due to the synergistic encoding in <italic>daf-7(-)</italic> mutants - since interactions between neurons are a crucial to a synergistic strategy, the system becomes much more sensitive to linear correlations. This effect is particularly evident in <italic>daf-7(-)</italic> mutants, where the signal correlation is almost unchanged under noise rescaling (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>), making noise correlation the more prominent contributing factor.</p></sec><sec id="s14" sec-type="appendix"><title>Gaussian model</title><p>We can explore the consequences of redundant or synergistic strategies by modeling how the information encoded by ADF, ASI and NSM neuron is read by an ideal output which conveys the information from the sensory neurons. The essential features of the ADF-ASI-NSM system are captured by using the model depicted in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A–B</xref>. Here, the information about a binary input <inline-formula><mml:math id="inf81"><mml:mi>B</mml:mi></mml:math></inline-formula> is encoded by three sensors <inline-formula><mml:math id="inf82"><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf83"><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>S</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> whose joint response is a multivariate normal distribution with mean vector <inline-formula><mml:math id="inf85"><mml:msub><mml:mi>μ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math></inline-formula><disp-formula id="equ27"><label>(28)</label><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="center center center center center center" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">b</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where the parameter <inline-formula><mml:math id="inf86"><mml:mi>a</mml:mi></mml:math></inline-formula> is associated with the dynamic range of the response. The covariance matrix <inline-formula><mml:math id="inf87"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM181"><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM182"><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> associated to the joint distribution is assumed to be stimulus-independent. This simplification is consistent with the observation that variances and correlations between neurons do not change considerably across food levels. The covariance matrix was parametrized as<disp-formula id="equ28"><label>(29)</label><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="center center center center center center" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the correlation coefficients between <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> such that <inline-formula><mml:math id="inf91"><mml:mrow><mml:mrow><mml:mo>det</mml:mo><mml:mo>⁡</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>The information about the binary input encoded by the three Gaussian sensors is then integrated linearly by the output variable<disp-formula id="equ29"><label>(30)</label><mml:math id="m29"><mml:mrow><mml:mi>output</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf92"><mml:mrow><mml:mrow><mml:mi id="XM186">α</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM187">β</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM188">γ</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf93"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. This choice implies that the output is also normally distributed with mean <inline-formula><mml:math id="inf94"><mml:mrow><mml:mo>±</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula> dependently on the value of the input, whereas its variance reads<disp-formula id="equ30"><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-31_3"><mml:mtext>(31)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi/><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>α</mml:mi><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>β</mml:mi><mml:mi>γ</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>α</mml:mi><mml:mi>γ</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In our setting we assume the two states of the input <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn id="XM203">0</mml:mn><mml:mo>,</mml:mo><mml:mn id="XM204">1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to be equally probable, leading to an information entropy of 1 bit. The information encoded by the sensors about the input, <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM205"><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM206"><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM207"><mml:mi>S</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mi id="XM208">B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, is upper bounded by the input entropy, moreover the input information encoded by each component, <inline-formula><mml:math id="inf97"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM209"><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi id="XM210">B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, is always smaller than the joint information. We can combine these constraints into the inequality<disp-formula id="equ31"><label>(32)</label><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo form="prefix">max</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>I</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thickmathspace"/><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Furthermore, since the output is a function of the sensor responses, the mutual information between input and output <inline-formula><mml:math id="inf98"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM212">output</mml:mi><mml:mo>;</mml:mo><mml:mi id="XM213">B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is bounded by the information encoded by the three sensors<disp-formula id="equ32"><label>(33)</label><mml:math id="m32"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In order to understand the consequences of synergy and redundancy from the perspective of the output node in the network which receives the input information from the three sensors, we explored the parametric space of the model and calculated the information encoded by the sensors and by the output. By using the variance of the first sensor as a reference scale we set <inline-formula><mml:math id="inf99"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and sampled the eight parameters left uniformly within the range<disp-formula id="equ33"><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-34_26"><mml:mtext>(34)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0.1</mml:mn><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-35_26"><mml:mtext>(35)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0.1</mml:mn><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-36_26"><mml:mtext>(36)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:mn>0.7</mml:mn><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0.7</mml:mn></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-37_26"><mml:mtext>(37)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-38_26"><mml:mtext>(38)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-39_26"><mml:mtext>(39)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The range above was selected based on the following considerations:</p><list list-type="order"><list-item><p>All variances have a lower bound (set to 0.1) to avoid singular regimes where the 3D normal distribution becomes too narrow around the mean.</p></list-item><list-item><p>From <xref ref-type="disp-formula" rid="equ33">Equations (34,35)</xref> <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which implies <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>The upper bound of 0.7 on the absolute correlation coefficients <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was used to keep the correlations within a biologically relevant range. Correlations between ADF, ASI and NSM are lower than 0.5 in all food conditions and genetic backgrounds.</p></list-item><list-item><p>The conditions in <xref ref-type="disp-formula" rid="equ33">Equations (34–36)</xref> do not guarantee the covariance matrix in <xref ref-type="disp-formula" rid="equ28">Equation (29)</xref> to be positive definite, therefore in our sampling algorithm we rejected all parameter sets with <inline-formula><mml:math id="inf103"><mml:mrow><mml:mrow><mml:mi>det</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM214">cov</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>In our sampling we choose the value of <inline-formula><mml:math id="inf104"><mml:mi>a</mml:mi></mml:math></inline-formula> to be lower than <inline-formula><mml:math id="inf105"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. Larger values of <inline-formula><mml:math id="inf106"><mml:mi>a</mml:mi></mml:math></inline-formula> generate extreme regimes where <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> is approximately one bit, and the inequality for the joint information <inline-formula><mml:math id="inf108"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:mi>I</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> implies a positive redundancy</p></list-item></list><p><disp-formula id="equ34"><label>(40)</label><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo>≈</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0.</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In this condition the output information is very sensitive to the value of <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. For <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the output is only receiving input from <inline-formula><mml:math id="inf111"><mml:msub><mml:mi>S</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>, leading to an efficient transmission of 1 bit of information. Lower values of <inline-formula><mml:math id="inf112"><mml:mi>γ</mml:mi></mml:math></inline-formula> lead to a decrease of the transmitted information due to the noisier contribution of <inline-formula><mml:math id="inf113"><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf114"><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> to the output.</p><p>In <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> we show the calculation of information-theoretic quantities from a sample of <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mn>500000</mml:mn></mml:mrow></mml:math></inline-formula> parametric sets. Red and blue populations correspond respectively to redundant and synergistic configurations. The majority of the sampled configurations (65%) displays a positive redundancy. As discussed in the main text, the minimum value of the output information increases proportionally to the level of redundancy. This feature matches the intuitive view that redundant system allow to transmit infomation more reliably. The redundancy value is lower bounded by the negative of the total information encoded by the sensors and upper bounded by two bits (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref>), due to the inequalities<disp-formula id="equ35"><label>(41)</label><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mn>2.</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Synergistic regimes occupy the region of low signal correlation and positive noise correlation (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1D–F</xref>) and are generally characterized by low values of the information carried by single sensors (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1E</xref>).</p><p>In <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1G</xref>, we show the distribution of redundant/synergistic regimes with respect to the parameters <inline-formula><mml:math id="inf116"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf117"><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>, which represent dynamic range and noise in the model. The ratio between these two parameters quantifies the signal-to-noise ratio of the system<disp-formula id="equ36"><label>(42)</label><mml:math id="m36"><mml:mrow><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>≡</mml:mo><mml:mfrac><mml:mi>a</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In the absence of extra constraints, redundant configurations are permitted for any value of the signal-to-noise ratio, whereas the population of synergistic regimes is depleted for high values of <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1H</xref>). When we require a non-zero lower bound to the information encoded by the sensors, <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, we see the appearance of a critical value for <inline-formula><mml:math id="inf120"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM215"><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> which separates two regions (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1H</xref>, right panel): a synergy-dominated region, for <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and a mixed region where both coding strategies are permitted <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The critical <inline-formula><mml:math id="inf123"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> value depends on threshold applied to the sensor information, in particular, <inline-formula><mml:math id="inf124"><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM220"><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> increases for increasing threshold <inline-formula><mml:math id="inf125"><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>.</p><p>This observation can be used to predict how changes in signal-to-noise ratio affect coding strategy. Consider a system operating redundantly at high <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>. Our model shows that independently of the details of the system, if we apply a perturbation to the system which reduces the <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> below <inline-formula><mml:math id="inf128"><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM221"><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, then in order to carry at least <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> bits of information the system will necessarily adopt a synergistic strategy. Remarkably, this feature of the model is in perfect agreement with the switch from redundancy to synergy observed in the <italic>daf-7(-)</italic> mutant with respect to the wild-type animal.</p><p>Reduction of <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> accompanied by a sufficient level of information encoded is always associated to a switch to synergy. This behaviour is easy to explain. When <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>a</mml:mi><mml:mo>≪</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> the most informative sensor <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>S</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> stores a very small amount of information due to the small <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> however the joint information can still reach one bit by increasing the eccentricity of the distributions, i.e. by increasing the linear correlations between sensors. This has the clear consequence of increasing the noise correlation, therefore shifting redundancy to negative values.</p></sec></boxed-text></app></app-group></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.24040.020</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Hobert</surname><given-names>Oliver</given-names></name><role>Reviewing editor</role><aff id="aff6"><institution>Howard Hughes Medical Institute, Columbia University</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>[Editors’ note: minor issues and corrections have not been included, so there is not an accompanying Author response.]</p><p>Thank you for submitting your article &quot;Genetic Control of Encoding Strategy in a Food-sensing Neural Circuit&quot; for consideration as a Research Advance by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor (Oliver Hobert) and a Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and are in agreement that this work qualifies as an interesting Research Advance to your original <italic>eLife</italic> paper.</p><p>The only comment we would like you address is extremely minor in nature: <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>: panels A-D are shown, but the legend refers to A-E.</p></body></sub-article></article>