<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">49130</article-id><article-id pub-id-type="doi">10.7554/eLife.49130</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Humans can efficiently look for but not select multiple visual objects</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-144694"><name><surname>Ort</surname><given-names>Eduard</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5546-3561</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-98851"><name><surname>Fahrenfort</surname><given-names>Johannes Jacobus</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9025-3436</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-145808"><name><surname>Cate</surname><given-names>Tuomas ten</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-145809"><name><surname>Eimer</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-114281"><name><surname>Olivers</surname><given-names>Christian N L</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7470-5378</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Experimental and Applied Psychology</institution>, <institution>Vrije Universiteit Amsterdam</institution>, <addr-line><named-content content-type="city">Amsterdam</named-content></addr-line>, <country>Netherlands</country></aff><aff id="aff2"><institution content-type="dept">Experimental Psychology</institution>, <institution>Utrecht University</institution>, <addr-line><named-content content-type="city">Utrecht</named-content></addr-line>, <country>Netherlands</country></aff><aff id="aff3"><institution content-type="dept">Department of Psychological Sciences</institution>, <institution>Birkbeck College, University of London</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-19516"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing editor</role><aff><institution>Peking University</institution>, <country>China</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>eduardxort@gmail.com</email> (EO);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>27</day><month>08</month><year>2019</year></pub-date><volume>8</volume><elocation-id>e49130</elocation-id><history><date date-type="received"><day>07</day><month>06</month><year>2019</year></date><date date-type="accepted"><day>26</day><month>08</month><year>2019</year></date></history><permissions><copyright-statement>Â© 2019, Ort et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Ort et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-49130-v1.pdf"/><abstract><p>The human brain recurrently prioritizes task-relevant over task-irrelevant visual information. A central, question is whether multiple objects can be prioritized simultaneously. To answer this, we let observers search for two colored targets among distractors. Crucially, we independently varied the number of target colors that observers anticipated, and the number of target colors actually used to distinguish the targets in the display. This enabled us to dissociate the preparation of selection mechanisms from the actual engagement of such mechanisms. Multivariate classification of electroencephalographic activity allowed us to track selection of each target separately across time. The results revealed only small neural and behavioral costs associated with preparing for selecting two objects, but substantial costs when engaging in selection. Further analyses suggest this cost is the consequence of neural competition resulting in limited parallel processing, rather than a serial bottleneck. The findings bridge diverging theoretical perspectives on capacity limitations of feature-based attention.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>464-13-003</award-id><principal-award-recipient><name><surname>Olivers</surname><given-names>Christian N L</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>ERC-2013-CoG-615423</award-id><principal-award-recipient><name><surname>Olivers</surname><given-names>Christian N L</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants gave written informed consent in line with the Declaration of Helsinki. The study was approved by the Scientific and Ethics Review Board of the Faculty of Behavioural and Movement Sciences at the Vrije Universiteit Amsterdam (Reference number: VCWE-2016-215).</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>All data and material will be made freely accessible at https://osf.io/3bn64.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Ort E</collab><collab>Fahrenfort JJ</collab><collab>ten Cate T</collab><collab>Eimer M</collab><collab>Olivers CNL</collab></person-group><year iso-8601-date="2019">2019</year><source>Data from: Humans can efficiently look for but not select multiple visual objects</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/3bn64">https://osf.io/3bn64</ext-link><comment>Open Science Framework, 3bn64</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-49130-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>