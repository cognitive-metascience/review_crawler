<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="review-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">51140</article-id><article-id pub-id-type="doi">10.7554/eLife.51140</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Review Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Hippocampal remapping as hidden state inference</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-153701"><name><surname>Sanders</surname><given-names>Honi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9018-2001</contrib-id><email>honi@mit.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-13092"><name><surname>Wilson</surname><given-names>Matthew A</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-42557"><name><surname>Gershman</surname><given-names>Samuel J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6546-3298</contrib-id><email>gershman@fas.harvard.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Center for Brains Minds and Machines, Harvard University</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Picower Institute for Learning and Memory and Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Department of Psychology, Harvard University</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution>University of Texas at Austin</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>09</day><month>06</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e51140</elocation-id><history><date date-type="received" iso-8601-date="2019-08-20"><day>20</day><month>08</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-05-09"><day>09</day><month>05</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Sanders et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Sanders et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-51140-v1.pdf"/><abstract><p>Cells in the hippocampus tuned to spatial location (place cells) typically change their tuning when an animal changes context, a phenomenon known as remapping. A fundamental challenge to understanding remapping is the fact that what counts as a ‘‘context change’’ has never been precisely defined. Furthermore, different remapping phenomena have been classified on the basis of how much the tuning changes after different types and degrees of context change, but the relationship between these variables is not clear. We address these ambiguities by formalizing remapping in terms of hidden state inference. According to this view, remapping does not directly reflect objective, observable properties of the environment, but rather subjective beliefs about the hidden state of the environment. We show how the hidden state framework can resolve a number of puzzles about the nature of remapping.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>hippocampus</kwd><kwd>place cell</kwd><kwd>context</kwd><kwd>bayesian inference</kwd><kwd>hidden state</kwd><kwd>learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>NSF</institution></institution-wrap></funding-source><award-id>STC award CCF-1231216</award-id><principal-award-recipient><name><surname>Sanders</surname><given-names>Honi</given-names></name><name><surname>Wilson</surname><given-names>Matthew A</given-names></name><name><surname>Gershman</surname><given-names>Samuel J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Context representation in the hippocampus incorporates uncertainty and requires inference based on past experience.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Place cells of the hippocampus fire when an animal occupies specific spatial locations (place fields; <xref ref-type="bibr" rid="bib84">O'Keefe, 1976</xref>). Each place cell has its own respective place fields, so collectively the population of place cell comprise a map of an environment, in which each location corresponds to activity of a particular subset of place cells. The hippocampus is thought to use independent maps for each context. These independent maps can be observed through ‘‘place field remapping’’, in which the location of a place field may change or the place field may disappear entirely between contexts (<xref ref-type="bibr" rid="bib80">Muller and Kubie, 1987</xref>; <xref ref-type="bibr" rid="bib17">Colgin et al., 2008</xref>; <xref ref-type="bibr" rid="bib65">Kubie et al., 2019</xref>). The sensitivity of place cells to context changes is consistent with many other studies implicating the hippocampus in context-dependent behavior (<xref ref-type="bibr" rid="bib47">Holland and Bouton, 1999</xref>; <xref ref-type="bibr" rid="bib29">Gershman et al., 2010</xref>; <xref ref-type="bibr" rid="bib5">Anagnostaras et al., 2001</xref>; <xref ref-type="bibr" rid="bib107">Smith and Mizumori, 2006a</xref>). Despite its acknowledged importance, the precise relationship between context changes and remapping has remained elusive, due in part to ambiguity as to what counts as context change.</p><p>Researchers have operationalized context in many different ways. For example, some researchers investigated the role of sensory cues (<xref ref-type="bibr" rid="bib63">Knierim et al., 1998</xref>; <xref ref-type="bibr" rid="bib87">O’Keefe and Conway, 1978</xref>; <xref ref-type="bibr" rid="bib80">Muller and Kubie, 1987</xref>), whereas others investigated the effect of changing spatial location or geometry (<xref ref-type="bibr" rid="bib106">Skaggs and McNaughton, 1998</xref>; <xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>), or changing the task (<xref ref-type="bibr" rid="bib85">O'Keefe and Speakman, 1987</xref>; <xref ref-type="bibr" rid="bib75">Markus et al., 1995</xref>). Not surprisingly, different effects have been observed for these different manipulations, without cohering into a unified picture of how context changes determine remapping.</p><p>Some of the confusion about what counts as a context change is due to inconsistent definitions of the word 'context'. Sometimes 'context' refers to experimenter-defined variables, such as physical location or sensory cues. In other cases, 'context' refers to the animal’s internal assessment of the environment as indicated by neural activity or behavioral response. For example, in the fear conditioning literature, animals are assumed to preferentially freeze in the 'same' context as that in which they received the shock. This doesn’t necessarily have to be physically the same environment, as long as the animal infers that it is the same environment (<xref ref-type="bibr" rid="bib14">Chang and Liang, 2017</xref>; <xref ref-type="bibr" rid="bib29">Gershman et al., 2010</xref>). Invoking subjective inferential factors in the interpretation of remapping compels us to consider basic questions about the nature of these inferences. What is the animal’s hypothesis space? How does it represent and update beliefs over this hypothesis space?</p><p>The goal of this paper is to develop formal answers to these questions, and thereby provide a coherent account of diverse experimental findings. Key to this account is the idea that the relationship between observable properties of the environment (including context) and remapping is mediated by inferences about unobservable properties of the environment (<italic>hidden states</italic>). We emphasize for clarity that the ‘‘observable’’ properties of the environment have themselves been inferred through sensory processing and therefore are in a sense hidden, but when we refer to hidden states, we are referring to regularities in the environment that could not be observed even with perfect sensory reproduction of the environment. According to this view (see also <xref ref-type="bibr" rid="bib27">Fuhs and Touretzky, 2007</xref>; <xref ref-type="bibr" rid="bib30">Gershman et al., 2014</xref>; <xref ref-type="bibr" rid="bib88">Penny et al., 2013</xref>), place fields remap when the animal believes that it has entered a new hidden state. By specifying the animal’s internal model of how hidden states relate to observable stimuli, we can make principled predictions about when, why and how place fields remap.</p><p>Before describing the details and applications of this computational framework, we will briefly review some of the key empirical and theoretical background.</p><sec id="s1-1"><title>Empirical background</title><p>Remapping phenomena have been divided into several classes (<xref ref-type="bibr" rid="bib17">Colgin et al., 2008</xref>; <xref ref-type="bibr" rid="bib79">Muller, 1996</xref>; <xref ref-type="bibr" rid="bib65">Kubie et al., 2019</xref>). At the extremes, there is ‘global’ or ‘complete’ remapping (where no place fields are shared between contexts) and ‘null’ or ‘lack of’ remapping (where all place fields are shared between contexts). Between these extremes is ‘partial remapping’ (where some place fields are shared between contexts but some are not) and ‘rate remapping’ (where place fields are shared between contexts but have characteristically different firing rates). However, none of these categories can be regarded as strictly exclusive.</p><p>The extent to which place fields are shared between contexts can be quantified by looking at the spatial correlations of place cell firing rates between contexts. Although studies report correlations near zero between place fields in different contexts (<xref ref-type="bibr" rid="bib69">Leutgeb et al., 2004</xref>; <xref ref-type="bibr" rid="bib80">Muller and Kubie, 1987</xref>; <xref ref-type="bibr" rid="bib102">Schlesiger et al., 2015</xref>), there are reasons to believe that correlations are not actually zero. A recent report suggests that previous observations of global remapping might be artifacts of misalignment of maps between contexts (<xref ref-type="bibr" rid="bib61">Kinsky et al., 2018</xref>). Some place cells have been found to consistently encode reward across virtual reality contexts that otherwise express ‘global remapping’ (<xref ref-type="bibr" rid="bib28">Gauthier and Tank, 2018</xref>), so there is at least one class of place cells that have recently been found not to remap across contexts. More generally, many studies reporting global remapping report low but non-zero correlations (<xref ref-type="bibr" rid="bib69">Leutgeb et al., 2004</xref>; <xref ref-type="bibr" rid="bib106">Skaggs and McNaughton, 1998</xref>; <xref ref-type="bibr" rid="bib109">Spiers et al., 2015</xref>).</p><p>Conversely, studies reporting lack of remapping never report perfect place field overlap between contexts. Indeed, even within a single context, patterns of spatial firing show variability over time, as if more than a single map is used in a given context (<xref ref-type="bibr" rid="bib22">Fenton and Muller, 1998</xref>; <xref ref-type="bibr" rid="bib55">Kay et al., 2019</xref>; <xref ref-type="bibr" rid="bib57">Kelemen and Fenton, 2016</xref>). Additionally, the extent of remapping for repeated presentations of the same context depends on the amount of experience the animal has had (<xref ref-type="bibr" rid="bib66">Law et al., 2016</xref>).</p><p>Rate remapping is also not a strict category. Manipulations used to generate rate remapping do so for a fraction of the place cell population, while other cells in the population maintain or lose their place fields (<xref ref-type="bibr" rid="bib114">Wood et al., 2000</xref>; <xref ref-type="bibr" rid="bib70">Leutgeb et al., 2005a</xref>). In this way, rate remapping is always accompanied by partial remapping. Additionally, protocols for generating rate remapping can sometimes produce a range of remapping states during learning, ranging from no remapping to global remapping. For example, <xref ref-type="bibr" rid="bib70">Leutgeb et al., 2005a</xref> found rate remapping when comparing place field maps between circle and square enclosures. However, <xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref> make the same comparison between circle and square enclosures, and find rate remapping as an intermediate state as the animal transitions from no remapping to global remapping over the course of learning.</p><p>The complications discussed above highlight the fact that virtually all remapping is partial remapping. Place cell responses to manipulations are extremely heterogeneous (<xref ref-type="bibr" rid="bib67">Lee et al., 2004</xref>; <xref ref-type="bibr" rid="bib103">Shapiro et al., 1997</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Anderson and Jeffery, 2003</xref>). Additionally, remapping behavior can vary across animals (<xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>; <xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>) as well as across laboratories (<xref ref-type="bibr" rid="bib42">Guzowski et al., 2004</xref>; <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>; <xref ref-type="bibr" rid="bib71">Leutgeb et al., 2005b</xref>; <xref ref-type="bibr" rid="bib18">Colgin et al., 2010</xref>), see the ‘‘Morph Experiments’’ section of the Results for an in-depth exploration of one example). We will argue that this heterogeneity arises from variability in beliefs across animals.</p></sec><sec id="s1-2"><title>Theoretical background</title><p>Our theory of hidden state inference is motivated by, and builds upon, prior research into the nature of context-dependent learning. Since Pavlov, experimentalists have recognized that extinguishing an association after Pavlovian conditioning is not the same as unlearning it. The association can return under a variety of circumstances (<xref ref-type="bibr" rid="bib12">Bouton, 2004</xref>), such as returning the animal to the conditioning context, or simply waiting a period of time before testing the animal. These phenomena seem to suggest that the animal is forming a new memory during extinction, which could compete with the conditioning memory at the time of retrieval. Context, on this view, serves as a particularly powerful retrieval cue. The fundamental challenge posed by this interpretation is to define precisely the conditions under which a new memory is formed or an old memory is updated, and the conditions under which a particular memory is retrieved at the time of test.</p><p>One approach to these questions is to frame them in terms of hidden state inference (<xref ref-type="bibr" rid="bib29">Gershman et al., 2010</xref>; <xref ref-type="bibr" rid="bib33">Gershman et al., 2017a</xref>): new memories are formed when an animal has inferred that it has encountered an unfamiliar (previously unvisited) state, and old memories are updated when it has inferred that it has encountered a familiar state. As we formalize below, these inferences can be calculated using Bayes’ rule, which computes a posterior probability distribution over hidden states by integrating prior beliefs about the hidden states with the likelihood of those hidden states given the animal’s observations. The hidden states are sometimes interpreted as <italic>latent causes</italic> (<xref ref-type="bibr" rid="bib19">Courville et al., 2006</xref>; <xref ref-type="bibr" rid="bib37">Gershman and Niv, 2012b</xref>), to emphasize the idea that the animal is forming beliefs about the causal structure of the environment.</p><p>The state inference framework can naturally explain many animal learning phenomena (see <xref ref-type="bibr" rid="bib31">Gershman et al., 2015</xref>), for a review). For example, a conditioned response takes longer to extinguish when reward is delivered probabilistically during the acquisition phase, a phenomenon known as the <italic>partial reinforcement extinction effect</italic> (e.g., <xref ref-type="bibr" rid="bib39">Gibbon et al., 1980</xref>). This phenomenon is surprising for classical associative learning accounts, since the learned association should be weaker under partial reinforcement, and hence should be <italic>faster</italic> to extinguish. According to the state inference framework, partial reinforcement renders the hidden state ambiguous; it takes more extinction trials until the animal is confident that acquisition and extinction trials were generated by different states (<xref ref-type="bibr" rid="bib19">Courville et al., 2006</xref>; <xref ref-type="bibr" rid="bib35">Gershman and Blei, 2012a</xref>).</p><p>In this paper, we argue that the same framework can unify many different place field remapping phenomena, under the assumptions that (i) each map corresponds to a unique hidden state, and (ii) a map is activated in proportion to the posterior probability of the corresponding hidden state. A closely related idea was pursued by <xref ref-type="bibr" rid="bib27">Fuhs and Touretzky, 2007</xref>, to which we owe the inspiration for the present work. Our goal is to explain a significantly broader range of phenomena using a somewhat simpler model, and to resolve a number of lingering empirical puzzles. In particular, we stress the role of uncertainty in hidden state inference and its connection with partial remapping, rate remapping, and population heterogeneity. This connection allows us to explain phenomena such as the stabilization of place cell maps over time and the potential role of experience in place cell responses to morph enclosures, among other phenomena.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Conceptual overview of the model</title><p>The computational problem facing the animal is to infer the posterior probability of each hidden state <inline-formula><mml:math id="inf1"><mml:mi>c</mml:mi></mml:math></inline-formula> given its observations <inline-formula><mml:math id="inf2"><mml:mi mathvariant="bold">𝐲</mml:mi></mml:math></inline-formula> (e.g., geometric or color features of a box), as stipulated by Bayes’ rule:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the likelihood of the observations under the hypothetical state <inline-formula><mml:math id="inf4"><mml:mi>c</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the prior probability of state <inline-formula><mml:math id="inf6"><mml:mi>c</mml:mi></mml:math></inline-formula>. A more detailed formal description of these terms can be found in the Materials and methods. In this section, we describe intuitively what they mean and how they work.</p><p>The animal is presented with observations that are generated by an unknown number of states through a process that the animal is not aware of (left side of <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The animal builds an <italic>internal model</italic> of the world (thought bubble in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). That model doesn’t have to mimic the world exactly, it simply needs to be flexible enough to be able to capture the structure that it is presented with. We suggest that the animal’s internal model provides a generative ‘‘recipe’’ through which it assumes observations are produced: first a state is sampled from <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and then an observation is sampled from the distribution associated with that state <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The job of the animal is to invert this generative process and infer the posterior probability of each hidden state <inline-formula><mml:math id="inf9"><mml:mi>c</mml:mi></mml:math></inline-formula> given its observations <inline-formula><mml:math id="inf10"><mml:mi mathvariant="bold">𝐲</mml:mi></mml:math></inline-formula>. Since different states could theoretically produce the same observations, the animal is faced with fundamental ambiguity. The posterior distribution <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐲</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the animal’s uncertainty about the hidden state. As it collects more observations and thereby reduces its uncertainty, the posterior will tend to progressively concentrate on a single explanation of which observations come from which states.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The hidden state inference framework.</title><p>(<bold>A</bold>) Schematic of hidden state inference. We impute an internal generative model to the animal, according to which observations are generated by a small number of hidden states. States are sampled from the Chinese Restaurant Process, parametrized by <inline-formula><mml:math id="inf12"><mml:mi>α</mml:mi></mml:math></inline-formula> (see Materials and methods for details). Each state is associated with a particular distribution over observations. The animal receives those observations but does not have direct access to the states that generated them. We model the animal as probabilistically inverting this generative model by computing the posterior distribution over hidden states given observations. (<bold>B</bold>) Example inference problem. Given a set of observations (x’s), the animal must infer how many hidden states there are. There is a tradeoff between increasing the number of hidden states in order to better fit the observations vs. decreasing the number of states in order to decrease the complexity of the explanation. The partition evidence ratio can be calculated given a particular set of observations to express the relative preference for the 1-state model vs. the 2-state model. See <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> in the Materials and methods section for more details. (<bold>C</bold>) Another example inference problem. Given an assignment of past observations (green and orange x’s) to hidden states (green and orange) and a novel observation (gray x), the animal forms a belief about hidden state assignment of the novel observation. This belief consists of probabilities of assigning the novel observation to each of the past hidden states (green or orange) or alternatively to a novel hidden state (purple). We can compare any two of these alternatives with the state evidence ratio. See <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> in the Materials and methods section for more details.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51140-fig1-v1.tif"/></fig><p>Because there is no reason to assume that the animal has a priori knowledge about the set of states, we allow the state space to potentially grow as the animal collects new observations. The animal starts off with a single state, and at each new observation it can assign some probability to a new state or one of its previously inferred states. As detailed in the Materials and methods, we accomplish this using a Bayesian nonparametric prior over hidden states. Importantly, this prior favors a small number of hidden states, encoding a form of ‘simplicity bias' or Occam’s razor.</p><p>As mentioned in the Introduction, we assume a one-to-one correspondence between hidden states and maps. Thus, we transpose the question ‘did the place field remap?’ to ‘were these observations generated by the same hidden state?’ More precisely, we report the log posterior probability ratio between 1-state and 2-state hypotheses (or <italic>evidence ratio</italic>, for brevity), which we take to be related to the degree of remapping (see Materials and methods for definitions of two versions of the evidence ratio: the partition evidence ratio and the state evidence ratio). When the evidence ratio is near 0, the animal is indifferent between the two hypotheses, and in this case we expect partial remapping. No remapping occurs when the evidence ratio is strongly positive (favoring the 1-state hypothesis), rate remapping occurs when the log probability ratio is weakly positive, and global remapping occurs when it is strongly negative. Keep in mind, following our overview of the literature in the Introduction, that these are heuristic categories without strict boundaries. On the probabilistic view, these categories occupy different points along a spectrum.</p></sec><sec id="s2-2"><title>The effect of sensory cues</title><p>One of the first questions asked about hippocampal remapping was which sensory cue controls whether a map is used. The first study of remapping <xref ref-type="bibr" rid="bib87">O’Keefe and Conway, 1978</xref> found that in an environment with four cues, some place fields disappeared with the removal of one or two cues, but most place fields maintained their firing with the removal of any two cues. In more modern terms, removal of a subset of cues caused partial remapping, but there was not a one-to-one correspondence between place fields and cues. Thus, from the very beginning it was clear that remapping is not in response to cues but in response to cue constellations (see also <xref ref-type="bibr" rid="bib103">Shapiro et al., 1997</xref>; <xref ref-type="bibr" rid="bib21">Fenton et al., 2000</xref>; <xref ref-type="bibr" rid="bib80">Muller and Kubie, 1987</xref>). Each of these studies involved separately rotating or removing groups of stimuli, finding that many place fields that rotated when a given stimuli was rotated still maintained their firing when that stimuli was removed. A similar early result was that of <xref ref-type="bibr" rid="bib85">O'Keefe and Speakman, 1987</xref>, where cues necessary for orientation of the map were removed, but the place cell map was maintained. The significance of these results is that the place field map is responsive to cues but is not controlled by cues in a one-to-one fashion.</p><p>Viewing remapping as hidden state inference provides an important insight into this behavior. Our model posits that the cues jointly inform the posterior over hidden states. Individual cues will typically only exert a weak effect on the posterior, and hence exert only a weak effect on remapping.</p><p>To simulate the effect of cue configurations on remapping, we assume that the observation vector consists of four features, each drawn from a Gaussian with mean 0 and standard deviation of 0.2. We provide the model with 20 observations drawn from that distribution and then provide one of four probe observations. For each probe, we compute the state evidence ratio (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Hidden state inference is informed by cue constellations.</title><p>Observations are generated from a distribution with four features, each drawn from a Gaussian with mean 0 and standard deviation of 0.2.We train the model with 20 observations drawn from that distribution. We then compare the posterior probability of assigning a probe observation to the same hidden state as the previous observations vs. assigning it to a novel hidden state (<xref ref-type="disp-formula" rid="equ11">Equation 11</xref> for same <inline-formula><mml:math id="inf13"><mml:mi>c</mml:mi></mml:math></inline-formula> vs. novel <inline-formula><mml:math id="inf14"><mml:mi>c</mml:mi></mml:math></inline-formula>). The first probe is an observation where each feature has a value of 0 (no cues changed). The model prefers assigning this probe observation to the same hidden state as the previous observations, corresponding to no remapping. The second probe is an observation where the first feature has a value of 1 and the other features have values of 0 (cue one changed). The third probe is an observation where the first and last features have a value of 1 and the other features have values of 0 (cues 1 and 4 changed). For both of these, the model assigns a state evidence ratio near 0, representing relatively high uncertainty about hidden state assignment, which corresponds to partial remapping. The grey background has saturation proportional to a Gaussian centered at 0 with a standard deviation of 5; values with a grey background can be heuristically thought of as partial remapping, whereas values with a white background can be thought of as either complete remapping or lack of remapping depending on whether two states are more likely (negative values) or one state is more likely (positive values). The fourth probe is an observation where all four features have values of 1 (all cues changed), for which the model prefers assigning the probe observation to a new hidden state, corresponding to global remapping.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51140-fig2-v1.tif"/></fig><p>The first probe is an observation where each feature has a value of 0 (no cues changed). The model prefers assigning the probe observation to the same hidden state as the previous observations, corresponding to no remapping. The second probe is an observation where the first feature has a value of 1 and the other features have a value of 0 (cue 1 changed). The third probe is an observation where the first and last features have a value of 1 and the other features have a value of 0 (cues 1 and 4 changed). For both of these, the model produces an evidence ratio near 0, registering a high level of uncertainty about the hidden state (i.e., partial remapping). The fourth probe is an observation where all four features have a value of 1 (all cues changed), for which the model prefers assigning the probe observation to a new hidden state, corresponding to global remapping. These simulations demonstrate how the model is sensitive to the configuration of cues; no one cue completely controls remapping, consistent with the experimental data reviewed above.</p><p>Another aspect of these simulations worth highlighting is the fact that they are probabilistic. The representation of uncertainty in hidden state identity corresponds in an important way with the result that hippocampal maps during two experiences are almost never entirely overlapping nor entirely independent. From the perspective of our model, this ‘partial remapping’ reflects the inherent uncertainty about whether different observations are drawn from the same distribution.</p></sec><sec id="s2-3"><title>Experience-dependent remapping</title><p>The previous section addressed the study of how sensory cues control place field remapping. Another line of research has studied how more diffuse contextual cues control remapping, but the answer was invariably that it depended on prior experience (<xref ref-type="bibr" rid="bib62">Knierim et al., 1995</xref>; <xref ref-type="bibr" rid="bib104">Sharp et al., 1990</xref>; <xref ref-type="bibr" rid="bib85">O'Keefe and Speakman, 1987</xref>; <xref ref-type="bibr" rid="bib13">Breese et al., 1989</xref>; <xref ref-type="bibr" rid="bib63">Knierim et al., 1998</xref>; <xref ref-type="bibr" rid="bib10">Bostock et al., 1991</xref>; <xref ref-type="bibr" rid="bib103">Shapiro et al., 1997</xref>). One prime example of this is the role of environmental geometry (the shape of the recording arena). Initially, it was thought that different geometries necessarily corresponded to different maps (<xref ref-type="bibr" rid="bib80">Muller and Kubie, 1987</xref>; <xref ref-type="bibr" rid="bib90">Quirk et al., 1992</xref>) , but recordings had always been done in familiar environments. The first group to record throughout the course of learning found that there was no consistent relationship between environment shape and inferred hidden state (<xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>). In this experiment, place cells were recorded in rats who were alternately placed in square and circle boxes occupying the same location in the recording room day after day. Early in learning, there was limited remapping. Only after extensive experience in the two boxes did the animals remap between the two boxes (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). This indicates that the sensitivity to context changes changes with experience. Analogous results have been found for the effects of experience on remapping in response to other manipulations (<xref ref-type="bibr" rid="bib10">Bostock et al., 1991</xref>; <xref ref-type="bibr" rid="bib103">Shapiro et al., 1997</xref>). These effects are hard to explain in terms of fixed contextual boundaries governing remapping. It is naturally explained by the hidden state inference perspective, which posits that uncertainty about hidden states evolves as more data are observed. In particular, distinctions between hidden states are acquired gradually, such that substantial remapping should only be observed after sufficient experience to counteract the ‘‘simplicity bias’’ favoring a small number of hidden states.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Learning to distinguish.</title><p>(<bold>A</bold>) Adapted from <xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>, who compared place cell representations between alternating presentations of square and circle boxes. Field Divergence is expressed in percent and represents the fraction of place fields that remap between the two enclosures. The representations of the enclosures are initially similar, but diverge with learning. (<bold>B</bold>) Simulated observations (black dots) are generated from Gaussians centered at −1, 1. The model compares the posterior probability of the observations coming from one inferred hidden state (red) or two inferred hidden states (blue). (<bold>C</bold>) The relative probability assigned to the observations coming from two hidden states vs. one hidden state (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) is shown as a function of amount of experience. Early on, there is uncertainty about how many hidden states there are, whereas later two hidden states is more probable, similar to the empirical observations. As in <xref ref-type="fig" rid="fig2">Figure 2</xref>, values with a grey background can be thought of as partial remapping whereas values with a white background can be thought of as either complete remapping or lack of remapping depending on whether two states are more likely or one state is more likely. Note that the axis here has been flipped relative to <xref ref-type="fig" rid="fig2">Figure 2</xref> in order to match the axis of the empirical results shown in panel A.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51140-fig3-v1.tif"/><permissions><copyright-statement>© 2002 Springer Nature. All rights reserved</copyright-statement><copyright-year>2002</copyright-year><copyright-holder>Springer Nature</copyright-holder><license><license-p>Panel A is adapted from <xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref> with permission (originally published as Supplementary Information Sheet 5). It is not covered by the CC-BY 4.0 licence and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions></fig><p>We simulate these experiments qualitatively in the following way. We take observations to be 1D for simplicity, where the single dimension is the feature along which the distinction is learned. For example, in the circle-square experiment (<xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>), the dimension would be the shape of the enclosure. We generate observations from two Gaussians (corresponding to the circle and square contexts) with <inline-formula><mml:math id="inf15"><mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). We alternate drawing observations from each distribution. After each pair of draws, we compute the partition evidence ratio (in this case, the relative probability of the hypothesis that all observations up to that point were drawn from a single hidden state against the hypothesis that all observations up to that point had been drawn from two alternating hidden states).</p><p>Early in training, there is uncertainty about how many hidden states there are (<xref ref-type="fig" rid="fig3">Figure 3C</xref>); the evidence provided by the observations is not yet sufficiently strong to overwhelm the simplicity bias of the prior. As more data are observed, the two-state hypothesis is eventually favored over the one-state hypothesis. The hidden state inference perspective thus explains why context-dependent remapping only emerges gradually with experience.</p></sec><sec id="s2-4"><title>Stabilization of maps over time</title><p>Maps take time to stabilize: repetition of a novel environment induces less map similarity than repetitions of a familiar environment (<xref ref-type="bibr" rid="bib25">Frank et al., 2004</xref>; <xref ref-type="bibr" rid="bib69">Leutgeb et al., 2004</xref>; <xref ref-type="bibr" rid="bib66">Law et al., 2016</xref>). In particular, <xref ref-type="bibr" rid="bib66">Law et al., 2016</xref> alternated presentation of two environments. They found that intra-environment map similarity went up as a function of experience (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). These results are difficult to explain under the assumption that remapping is induced by the discrepancy between expectations and current cues exceeding a fixed threshold (<xref ref-type="bibr" rid="bib52">Jeffery, 2003</xref>). Long-term potentiation (LTP) had been tied to map stabilization (<xref ref-type="bibr" rid="bib59">Kentros et al., 1998</xref>; <xref ref-type="bibr" rid="bib16">Cobar et al., 2017</xref>), but the speed with which LTP can create place fields (single trials; <xref ref-type="bibr" rid="bib8">Bittner et al., 2017</xref>) is inconsistent with the slowness of map stabilization. The hidden state inference perspective offers a different interpretation of map stabilization: as an animal gains more experience with a particular state, it sharpens its representation of that state (i.e., its uncertainty about the distributional statistics decreases), and consequently it becomes more confident in recognizing repetitions of that state.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Map stabilization requires certainty about distributional statistics.</title><p>(<bold>A</bold>) Data from <xref ref-type="bibr" rid="bib66">Law et al., 2016</xref>, showing the spatial correlation of the hippocampal map in repeated presentations of the same environment over multiple training days. Initially, the correlation is low, indicating extensive remapping between observations, but over the course of training the extent of remapping between observations decreases. (<bold>B</bold>) Observations (black dots) are generated from two Gaussians, both of which are centered at 0. The model compares the posterior probability of the observations coming from one inferred hidden state (red) or two inferred hidden states (blue). (<bold>C</bold>) The relative probability assigned to the observations coming from one hidden state vs. two hidden states (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) is shown as a function of amount of experience. Early in training, the two hypotheses have similar probabilities, whereas later one hidden state is overwhelmingly more probable. This corresponds to an increase in certainty over training, which would translate into a decreased tendency to remap, similar to the empirical observations. Note that the axis here has been flipped relative to <xref ref-type="fig" rid="fig3">Figure 3C</xref> in order to match the axis of the empirical results shown in panel A.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51140-fig4-v1.tif"/><permissions><copyright-statement>© 2016 Wiley Periodicals, Inc. All rights reserved</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Wiley Periodicals, Inc</copyright-holder><license><license-p>Panel A is reproduced from <xref ref-type="bibr" rid="bib66">Law et al., 2016</xref> with permission (originally published as Figure 2A). It is not covered by the CC-BY 4.0 licence and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions></fig><p>We can model the dynamics of stabilization by considering observations which are generated from a single distribution with mean 0. We can consider the same hypotheses as were considered in <xref ref-type="fig" rid="fig3">Figure 3</xref>, namely, that there are either 1 or two hidden states. We consider the same hypotheses but the actual generative process has the opposite structure as <xref ref-type="fig" rid="fig3">Figure 3</xref>. Through the course of learning, the partition evidence ratio accumulates evidence in favor of the one-state hypothesis, corresponding to the emergence of a ‘‘stable’’ map. Indeed, early in learning, the animal does not know whether it is receiving observations from the simulation of <xref ref-type="fig" rid="fig3">Figure 3</xref> or the simulation of <xref ref-type="fig" rid="fig4">Figure 4</xref>, as they are indistinguishable. Only after extensive experience is the animal able to identify which generative process is generating its observations.</p></sec><sec id="s2-5"><title>Remapping due to non-sensory changes</title><p>Remapping is not solely driven by sensory aspects of experience. For example, place fields can remap depending on internal variables such as movement direction or task (<xref ref-type="bibr" rid="bib108">Smith and Mizumori, 2006b</xref>; <xref ref-type="bibr" rid="bib98">Sanders et al., 2019</xref>; <xref ref-type="bibr" rid="bib114">Wood et al., 2000</xref>; <xref ref-type="bibr" rid="bib78">Muller et al., 1994</xref>). In general, it is known that place fields can remap depending on which direction the animal is running on a linear track (<xref ref-type="bibr" rid="bib75">Markus et al., 1995</xref>; <xref ref-type="bibr" rid="bib7">Battaglia et al., 2004</xref>). However, place fields tend not to remap based on running direction in an open field. This is most clearly shown in <xref ref-type="bibr" rid="bib75">Markus et al., 1995</xref>. They compared two conditions, both of which occurred in an open field: one in which the animal was randomly foraging, and one in which the animal was running between four specific locations in one of two directions. They found that the extent of remapping in response to movement direction was larger in the directed foraging condition than in the random foraging condition (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) despite having the same sensory cues in the two conditions.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Place field directionality depends on statistics of behavior.</title><p>(<bold>A</bold>) Data from <xref ref-type="bibr" rid="bib75">Markus et al., 1995</xref>, showing that place field remapping depends on the animal’s direction more when the animal is running in a stereotyped path than when the animal is running in random directions. (<bold>B</bold>) The model receives circular observations corresponding to the animal’s running direction. The model either receives observations drawn from a uniform distribution (red dots) or alternating from two Von Mises distributions with means of 0 and 180 degrees, and <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> (blue dots). These observations are separated into two groups with a line that is the farthest from any observations (red and blue lines). (<bold>C</bold>) The partition evidence ratio between the hypothesis that all observations have been drawn from two hidden states separated by the lines in panel B vs. the hypothesis that all observations have been drawn from a single hidden state (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) after 10 observations. The model is more likely to put probability on the hypothesis that there are two hidden states when given the directional observations as opposed to the uniform observations. This is similar to the empirical results, where place fields are more likely to remap (more likely to infer two hidden states) when the animal is running in a directed fashion.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51140-fig5-v1.tif"/><permissions><copyright-statement>© 1995 Society for Neuroscience. All rights reserved</copyright-statement><copyright-year>1995</copyright-year><copyright-holder>Society for Neuroscience</copyright-holder><license><license-p>Panel A is reproduced from <xref ref-type="bibr" rid="bib75">Markus et al., 1995</xref> with permission (originally published as Figure 6A). It is not covered by the CC-BY 4.0 licence and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions></fig><p>From the perspective of hidden state inference, we can draw an analogy with the remapping observed after training in the circle and square boxes (<xref ref-type="fig" rid="fig3">Figure 3</xref>), replacing the sensory features of the environment with the non-sensory information about self-motion. In the directed foraging case, observations are clearly separated into two states (clockwise movement and counterclockwise movement), whereas in the random foraging case, there is no consistent partition that could support the inference of multiple states.</p><p>We model this experiment in the following way. Again, we take observations to be 1-dimensional for simplicity, where the single feature is the animal’s movement direction. This feature is represented as a circular (angular) variable, as movement direction is circular. We model the random foraging condition as observations drawn from a uniform distribution over the circle (red dots in <xref ref-type="fig" rid="fig5">Figure 5B</xref>). We model the directed foraging as observations drawn from a Von Mises distribution with <inline-formula><mml:math id="inf17"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> alternating with a Von Mises distribution with <inline-formula><mml:math id="inf18"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (blue dots in <xref ref-type="fig" rid="fig5">Figure 5B</xref>). For each condition, we separate the observations into two groups with a line for which the distance from any observations is maximum (red and blue lines in <xref ref-type="fig" rid="fig5">Figure 5B</xref>). After 10 observations, we ask the model what the relative probability is that the observations were drawn from a single hidden state or drawn from two hidden states split by the line of maximum separation. The model assigns greater probability to the two-state hypothesis for directed foraging. In contrast, it assigns greater probability to the one-state hypothesis for random foraging (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). This corresponds to the empirical finding that place fields were more likely to remap under the directed foraging condition compared to the random foraging condition.</p></sec><sec id="s2-6"><title>Cue rotation experiments</title><p>One series of experiments used rotation of cues with respect to the recording arena to ask how the place cell representation responds to such changes. The most simple version of these experiments had a circular arena with a cue card on one side of the arena. The cue card could be rotated to any position in the arena, reported as an angle with respect to the original cue card orientation in the room reference frame (<xref ref-type="bibr" rid="bib94">Rotenberg and Muller, 1997</xref>; <xref ref-type="bibr" rid="bib63">Knierim et al., 1998</xref>; <xref ref-type="bibr" rid="bib44">Hargreaves et al., 2007</xref>). Experiments reported two types of changes in place field behavior in response to a given manipulation. One is extent of remapping, as we have been discussing in this paper. The other is which rotational angle the map is oriented towards. This added question is due to the inherent ambiguity in circular variables. Even if a place field moves to a different location in a given reference frame, it is still possible that remapping did not occur if the relative locations of place fields are preserved. Therefore, one must check whether the place field had the same location subject to a rotational offset. This rotational offset frequently corresponds to the rotational offset observed in head direction cells simultaneously recorded from a variety of brain regions (<xref ref-type="bibr" rid="bib62">Knierim et al., 1995</xref>; <xref ref-type="bibr" rid="bib44">Hargreaves et al., 2007</xref>). Experimental papers thus report 1) whether place fields remap and 2) if not, whether there is a rotational offset in their locations (<xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Response to cue rotation depends on experimental protocol.</title><p>(<bold>A</bold>) Data from <xref ref-type="bibr" rid="bib94">Rotenberg and Muller, 1997</xref>. The black curve represents the location of the cue card. The heat map represents the firing rate of a given place cell. On rotation of the cue card by 180°, the place field is maintained, but rotated 180° with respect to the room reference frame. (<bold>B–C</bold>) Results of simulation of several experimental manipulations: In ‘180°, clean’, the cue card is rotated 180° while the animal is absent and the maze is cleaned before returning the animal. In ‘180°, dirty’, the cue card is rotated 180° while the animal is present and odor cues left by the animal are not removed. In ‘45°, dirty’, the cue card is rotated 45° while the animal is present and odor cues left by the animal are not removed. (<bold>B</bold>) The state evidence ratio is in favor of assigning to the same hidden state in all three manipulations. However, there is more uncertainty under the ‘180°, dirty’ manipulation. (<bold>C</bold>) The highest probability reference direction is depicted for each manipulation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51140-fig6-v1.tif"/><permissions><copyright-statement>© 1997 The Royal Society (UK). All rights reserved</copyright-statement><copyright-year>1997</copyright-year><copyright-holder>The Royal Society (UK)</copyright-holder><license><license-p>Panel A is reproduced from <xref ref-type="bibr" rid="bib94">Rotenberg and Muller, 1997</xref> with permission (originally published as parts of Figure 2A). It is not covered by the CC-BY 4.0 licence and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions></fig><p>We model these experiments as follows. Similar to other simulations in this paper, each observation is a feature vector. However, instead of each entry in the vector containing the value of that feature on some sensory axis, the entry contains the angle between a given cue and an uncued direction in the room reference frame. Feature vectors with different values can potentially be identical if there is an offset that can be subtracted from each entry in one vector to give the other vector, corresponding to usage of a different uncued direction as the reference. Therefore, before performing hidden state inference, the animal must decide what reference direction to use in comparing the current observation to past observations from each hidden state. We model reference direction inference in the following way. Given the past observations previously assigned to a given hidden state, we can calculate the offset <inline-formula><mml:math id="inf19"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> to apply to the current observation that gives the maximum value of the posterior predictive distribution (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>) <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Say for example that a certain cue had always been +30° from the reference direction, but this time the cue location is provided as +120°. An offset of −90° would give the maximum probability of generating this observation from the same hidden state. This offset is calculated independently for each hidden state, and the state evidence ratio is calculated using the best offset for each hidden state. The offset of the most likely hidden state would correspond to the rotational offset in the place field locations.</p><p>We capture several empirical findings.</p><p>The animal is trained with a cue card consistently at 0° with respect to the minimally-cued room reference frame. The animal is removed from the maze, which is cleaned and the cue card is rotated 180°, before returning the animal. The finding is that place fields retain their positions relative to each other (no remapping) and relative to the card, so they rotate 180° with respect to the room reference frame (offset of 180°) (<xref ref-type="bibr" rid="bib94">Rotenberg and Muller, 1997</xref>; <xref ref-type="bibr" rid="bib62">Knierim et al., 1995</xref>). We model this by providing 10 single-dimensional training observations, each drawn from a wrapped normal with <inline-formula><mml:math id="inf21"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>18</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, representing the position of the cue card. Then we test with an observation with value 180°. The best offset for the current observation is −175° for the same hidden state as the previous observations (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, red). With that offset, the state evidence ratio is in favor of assigning to the same hidden state (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, red). Assigning to the same hidden state and 175° offset in the model correspond to the empirical finding of limited remapping and ∼180° rotation of place fields.</p><p>A similar experiment was performed where the cue card is rotated 180° without removing the animal or cleaning the maze. The finding is that the place fields did not remap or rotate in response to this manipulation (<xref ref-type="bibr" rid="bib94">Rotenberg and Muller, 1997</xref>). We model this with an expanded feature vector because the animal has access to additional cues, albeit cues that are less reliable than the cue card: namely, a preserved internal orientation from path integration and odor cues that the animal has left on the maze. The first entry in the feature vector is the same as in the previous simulation, that is the cue card position drawn from a wrapped normal with <inline-formula><mml:math id="inf22"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>18</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Five additional entries are included in the feature vector with uniformly distributed means and <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mn>18</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>*</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mn>54</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. The larger standard deviations on the positions of these cues correspond to their lower fidelity (<xref ref-type="bibr" rid="bib100">Save et al., 2000</xref>; <xref ref-type="bibr" rid="bib43">Hardcastle et al., 2015</xref>). The test observation has a value of 180° for the first entry (cue card) and values of the cue means for the other entries. Our model finds that the best offset is −2° (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, blue) and the state evidence ratio is in favor of assigning to the same hidden state (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, blue). However, the evidence ratio is much closer to 0, which would predict a larger degree of heterogeneity in place field behavior than the earlier experiment, which is a comparison for which there was not sufficient empirical power (<xref ref-type="bibr" rid="bib94">Rotenberg and Muller, 1997</xref>). See also <xref ref-type="bibr" rid="bib44">Hargreaves et al., 2007</xref>, <xref ref-type="bibr" rid="bib67">Lee et al., 2004</xref>, and <xref ref-type="bibr" rid="bib103">Shapiro et al., 1997</xref> for other reports of heterogeneity during cue conflict rotation experiments.</p><p>What if the cue card was only moderately rotated in the animal’s presence? <xref ref-type="bibr" rid="bib94">Rotenberg and Muller, 1997</xref> rotated the cue card by 45° in the animal’s presence without cleaning the maze. They found that the place fields did not remap and rotated by 45°. We model this experiment the same way as the last experiment except that the test observation has 45° as its first entry. Our model finds that the best offset is 22° (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, purple) and the state evidence ratio is in favor of assigning to the same hidden state (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, purple). The difference between the results of this and the last experiment is due to the fact that the other cues have large enough variance to accommodate a 45° rotation without requiring a new hidden state.</p><p>To summarize this section, rotation experiments share a framework with other cue manipulation experiments with the added complication of estimation of the appropriate rotational reference direction. It is therefore possible for place fields to retain their relative arrangement while also rotating with respect to some reference frame, which we consider to be a lack of remapping (assignment to the same hidden state). If the posterior probability of an observation is sufficiently low even after picking the best rotational reference, then a new hidden state would be inferred and place fields would lose their relative arrangement.</p></sec><sec id="s2-7"><title>Morph experiments</title><p>A persistent puzzle in the field is the inconsistent results from ‘morph’ environments that interpolate between different geometries (e.g., square and circle). Different labs have found different results with experimental setups that are not directly comparable (<xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>; <xref ref-type="bibr" rid="bib71">Leutgeb et al., 2005b</xref>; <xref ref-type="bibr" rid="bib18">Colgin et al., 2010</xref>). We summarize the past results here and suggest an interpretation that leads to a novel prediction.</p><p>In 2005, two groups each performed an experiment to answer the question, ‘How does the hippocampus represent a novel environment that is intermediate between two familiar environments?’ Both groups familiarized rats in square and circle environments, and then tested them in intermediate environments (polygons with a variable number of sides). The two papers had different results (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), characterized at the time in terms of whether the similarity curve had a discrete switch or a gradual switch. However, this difference is extremely hard to robustly characterize, considering that the variation in similarity between repetitions of the same environment was half as large as the entire range of similarity variations for the entire morph sequence (compare first and last points in <xref ref-type="bibr" rid="bib71">Leutgeb et al., 2005b</xref>, their Figure 6E). The other difference that was discussed at the time was whether the population response was coherent or heterogeneous. While both studies showed heterogeneous population responses, they did show different levels of heterogeneity, and we discuss this in the next Results section (Population heterogeneity and rate remapping).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Morph experiments.</title><p>(<bold>A</bold>) Different experimental protocols give different results for the morph experiment. The results in the fourth column show the similarities in population representation of the intermediate morph shapes compared to the square shape. The results in the fourth column are adapted from the corresponding paper cited in the first column. All values are shown on a scale ranging from 0 to 1, where one is complete concordance of population representations and 0 is random concordance. We classify the results into two qualitative classes: the first and third rows have results where all levels of morph result in partial remapping, whereas the second and fourth rows switch between no remapping and complete remapping as morph level increases. Scrambling during testing does not seem to be related to this effect. Moreover, the same experimental protocol can have qualitatively different results in different labs (compare second and third rows). (<bold>B</bold>) We provide observations from two alternating Gaussians with means −1 and +1, just as in <xref ref-type="fig" rid="fig3">Figure 3</xref>. We test after 5 (red) or 25 (blue) training observations by providing intermediate values and measuring the relative probability of being assigned to the same hidden state as the −1 mean observations. We thus predict that both qualitative results can be achieved in the same lab simply by performing the morph testing at different points of training.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51140-fig7-v1.tif"/></fig><p>A much more striking point of comparison was the difference in the extent of remapping between the extreme square and circle environments. Complete remapping was observed between the square and circle in <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>, whereas partial remapping was observed in <xref ref-type="bibr" rid="bib71">Leutgeb et al., 2005b</xref>. We believe that the findings of partial vs. complete remapping is the major difference in the findings of these papers, and is the one we focus on explaining.</p><p>What differences in protocols led to these differences in results? In addition to all the idiosyncrasies of individual lab protocols, there were two major explicitly described differences between their protocols. One is that they used different training protocols. <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref> used a training protocol designed for inducing complete remapping between square and circle in 6 days, and excluded animals that did not meet that criterion. <xref ref-type="bibr" rid="bib71">Leutgeb et al., 2005b</xref> used a similar training as <xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>, see <xref ref-type="fig" rid="fig3">Figure 3</xref>) for three weeks. The second difference was that <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref> presented the intermediate shapes in a scrambled order on the test day, whereas <xref ref-type="bibr" rid="bib71">Leutgeb et al., 2005b</xref> presented the intermediate shapes sequentially based on number of sides on the test day.</p><p>The second difference (scrambled test order) was the focus of several theoretical explanations (<xref ref-type="bibr" rid="bib9">Blumenfeld et al., 2006</xref>; <xref ref-type="bibr" rid="bib30">Gershman et al., 2014</xref>), but a replication of <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref> using scrambled presentation resulted in limited remapping (<xref ref-type="bibr" rid="bib18">Colgin et al., 2010</xref>), demonstrating that a scrambled presentation was not sufficient to force the hippocampus to use complete coherent remapping. Differences in the training protocol remain as a possible explanation. However, the problem remains that <xref ref-type="bibr" rid="bib18">Colgin et al., 2010</xref> attempted an exact replication of <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>, but got the opposite result. These differences can be seen in <xref ref-type="fig" rid="fig7">Figure 7A</xref>.</p><p>These results fit into a broader pattern of inconsistent results across two labs. Two experiments that led to complete remapping in the O’Keefe lab ended up leading to partial (and/or rate) remapping in the Moser lab. Training in alternating square and circle environments led to partial remapping initially and to complete remapping after 18 days in the O’Keefe lab (<xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>), but led to partial remapping after 18 days of comparable training in the Moser lab (<xref ref-type="bibr" rid="bib71">Leutgeb et al., 2005b</xref>). A 6 day white/morph circle-square training protocol led to complete remapping in the O’Keefe lab (<xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>), but led to partial remapping in the Moser lab (<xref ref-type="bibr" rid="bib18">Colgin et al., 2010</xref>). We do not believe either lab’s training to be inherently superior, but we do wish to point out that there are likely unreported idiosyncrasies of training that cause animals to consistently progress through partial remapping to global remapping more slowly in the Moser lab than in the O’Keefe lab (at least during the years 2000–2010). The main implication of this is that remapping behavior does not have a one-to-one mapping to the experimenter-defined conditions; rather, remapping behavior responds to a huge array of experiential factors, and the experimenter is only aware of a subset of these factors. Practically, this means that attempts to compare remapping behavior must be done between comparable controlled setups (as performed in the internal comparisons of <xref ref-type="bibr" rid="bib18">Colgin et al., 2010</xref>), and comparisons should ideally not be made across labs.</p><p>To summarize, various experimental protocols for measuring remapping behavior in response to intermediate ‘morph’ environments give divergent results, which can be split into two categories: heterogeneous responses when there is only partial remapping between the extremes, and population-wide coherent responses when there is complete remapping between the extremes (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). As we explored above (<xref ref-type="fig" rid="fig3">Figure 3</xref>), partial remapping and complete remapping can be observed in a single experimental protocol early and late in training, respectively. We therefore predict that both sets of results can be observed in the same lab, with the same experimental protocol, simply by presenting the intermediate ‘‘morph’’ environments early or late in training.</p><p>We show simulations of this prediction in <xref ref-type="fig" rid="fig7">Figure 7B</xref>. Specifically, we compute the probability that the training observations came from a single hidden state <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐜𝟏</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the probability they came from two hidden states <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐜𝟐</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> according to <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>. We then calculate the probability that the morph test is assigned to the same hidden state as the square assuming that the training observations came from two hidden states <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐜𝟐</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>). The hypotheses that correspond to the morph being assigned the same hidden state as the square are <bold><italic>S1</italic></bold>) that there is a single hidden state for the training and the morph is from the same state and <italic><bold>S2</bold></italic>) that there are two hidden states for the training and the morph is from the same state as the square. The hypotheses that correspond to the morph being assigned a different hidden state than the square are <bold><italic>D1</italic></bold>) that there is a single hidden state for the training and the morph is from a novel state and <bold><italic>D2</italic></bold>) that there are two hidden states for the training and the morph is from the same state as the circle and <italic><bold>D3</bold></italic>) that there are two hidden states for the training and the morph is from a novel state. We take the log posterior ratio between the <italic><bold>S</bold></italic> hypotheses and the <italic><bold>D</bold></italic> hypotheses and plot that in <xref ref-type="fig" rid="fig7">Figure 7B</xref> for varying number of training observations. The probability of assigning intermediate ‘‘morph’’ environments to the same hidden state as one of the extreme environments increases with the amount of training.</p><p>Thus, we suggest that a key distinction between classes of past morph results is whether there is complete or partial remapping between the extreme environments, and that complete or partial remapping can be achieved by a wide range of training protocols (as described throughout the paper) including amount of experience (as described in <xref ref-type="fig" rid="fig3">Figure 3</xref>).</p></sec><sec id="s2-8"><title>Population heterogeneity and rate remapping</title><p>So far, we have drawn the correspondence between evidence ratios and ‘extent of remapping’. There are a variety of ways to empirically quantify extent of remapping over the population, including average population vector correlations, average firing rate map correlations, and average change in location of place fields (<xref ref-type="bibr" rid="bib70">Leutgeb et al., 2005a</xref>). One important question to ask is how the extent of remapping is distributed across the population. One option is that some fraction of the place fields are perfectly retained and some fraction disappear or appear. This would be called partial remapping. Another possibility is that each place field modulates its peak firing rate. This would be called rate remapping. As mentioned in the Empirical Background section of the Introduction, both types coexist. We can qualitatively model this in the following way. For a given experimental manipulation, there is some distribution across the population of how much the firing rate is modulated. We can quantify for a given place field the extent of firing rate modulation using a measure such as 1-(lower firing rate)/(higher firing rate), ranging from 0 (identical firing in both conditions, i.e., no remapping) to 1 (place field exists in one condition but not in other condition, i.e., remapping). Intermediate values between 0 and 1 correspond to the extent of rate remapping. A common distribution with support on the interval [0,1] is the Beta distribution, which we will use for illustration. The Beta distribution has two parameters <inline-formula><mml:math id="inf27"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf28"><mml:mi>b</mml:mi></mml:math></inline-formula>, which correspond to relative probability mass on 1 and 0 respectively. We can map our evidence ratio loosely onto these parameters for illustration by saying that the value of the parameter corresponding to the preferred hypothesis is the magnitude of the evidence ratio + 1 and the value of the other parameter is 1. In this manner, the evidence ratio equals <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>b</mml:mi><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula>. For example an evidence ratio of +6 would have <inline-formula><mml:math id="inf30"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> because a positive evidence ratio prefers no remapping, that is more probability mass on 0 (red line in <xref ref-type="fig" rid="fig8">Figure 8A</xref>). This evidence ratio is at the border of our range of partial remapping. If we put thresholds at 0.15 and 0.85 (dotted lines) for indistinguishable from no remapping and complete remapping respectively, we see that 0% of place fields completely remap, 31% rate remap, and 69% do not remap, which is consistent to what we would expect for that magnitude of evidence ratio. Conversely, an evidence ratio of −0.5 would correspond to parameter values of <inline-formula><mml:math id="inf31"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (blue line in <xref ref-type="fig" rid="fig8">Figure 8A</xref>). This evidence ratio is near the center of our range of partial remapping, and we see that 22% of place fields completely remap, 72% rate remap, and 6% do not remap.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Relationship between Rate Remapping and Partial Remapping.</title><p>(<bold>A</bold>) The Beta distribution is used to illustrate the distribution in remapping responses over the place field population. Examples of the Beta distribution for parameter values <inline-formula><mml:math id="inf32"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (red) and <inline-formula><mml:math id="inf33"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (blue). We can draw a correspondence between the difference between these parameters and the evidence ratio. We can also characterize remapping behavior by saying that place fields with rate modulation less than the left-hand black dotted line do not remap, place fields between the black dotted lines rate remap, and place fields with rate modulation greater than the right-hand dotted black line completely remap. The extent of partial remapping would then be the fraction of cells that completely remap (fall to the right of the right-hand dotted black line). (<bold>B</bold>) The average extent of rate remapping has a positive relationship with the extent of partial remapping for a range of evidence ratios. Error bars are the standard deviation of the Beta distribution for those parameter values. (<bold>C</bold>) The amount of heterogeneity in rate remapping extents across the population of place fields has a positive relationship with the amount of uncertainty that the animal has over a range of evidence ratios.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51140-fig8-v1.tif"/></fig><p>This framework can organize some empirical observations into patterns that we would predict would generalize. One is that the fraction of place fields that completely remap seems to be correlated with the magnitude of rate remapping that is observed across different protocols. For example, the paper that coined the term ‘rate remapping’ (<xref ref-type="bibr" rid="bib70">Leutgeb et al., 2005a</xref>) explores several different experimental manipulations. Rank ordering of their manipulations according to their measures of rate remapping (firing rate changes and population vector correlations) matches well with the rank ordering according to their measures of complete remapping (place field correlation and center of mass change). Similar patterns of partial remapping occurring in concert with rate remapping can be observed in many other reports (<xref ref-type="bibr" rid="bib114">Wood et al., 2000</xref>, their Figure 5, <xref ref-type="bibr" rid="bib6">Anderson and Jeffery, 2003</xref>, their Figure 2, <xref ref-type="bibr" rid="bib74">Lu et al., 2013</xref>, their Figure 3, <xref ref-type="bibr" rid="bib98">Sanders et al., 2019</xref>, their Figure 1). The framework of a distribution of remapping behaviors over the place field population captures this phenomenon. If we look at parameter values ranging from <inline-formula><mml:math id="inf34"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf35"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, we see a positive relationship between the extent of rate remapping and the extent of partial remapping (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). The only report we are aware of in which rate remapping is observed in the absence of partial remapping is that of <xref ref-type="bibr" rid="bib3">Allen et al., 2012</xref>. It is notable that they observe much weaker rate changes than the conditions studied in other papers: visual inspection of their Figure 6B1 (top) shows an average rate modulation of ∼(17-14)/17 = 17%. This would roughly correspond to the distribution shown as the red line in <xref ref-type="fig" rid="fig8">Figure 8A</xref> which does indeed show a population of place fields that rate remap but a negligible fraction of place fields that completely remap. Overall, a comprehensive literature review suggests that extent of rate remapping is correlated with the extent of partial remapping, as would be expected if there is a distribution over the population in sensitivity to manipulations. Future experiments are needed to verify this hypothesis directly.</p><p>Another pattern that seems to occur in the literature is that increased uncertainty seems to correlate with heterogeneity in remapping behavior across the population of place fields. One example of this pattern is the difference in heterogeneity observed between the protocol of <xref ref-type="bibr" rid="bib71">Leutgeb et al., 2005b</xref> compared to that of <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>. More heterogeneity was observed in the experiment of <xref ref-type="bibr" rid="bib71">Leutgeb et al., 2005b</xref>, who were in the partial remapping regime (red line in our <xref ref-type="fig" rid="fig7">Figure 7B</xref>), compared to the relatively coherent response observed across the population in the experiment of <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>, who were in the global remapping regime (blue line in our <xref ref-type="fig" rid="fig7">Figure 7B</xref>). Other examples of population heterogeneity occurring with uncertainty include findings by <xref ref-type="bibr" rid="bib67">Lee et al., 2004</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Gothard et al., 1996</xref>. The framework of a distribution of remapping behaviors over the place field population captures this phenomenon. If we look at parameter values ranging from <inline-formula><mml:math id="inf36"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf37"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, we see a positive relationship between heterogeneity in response (as measured by the standard deviation of rate remapping extents across the population) and level of uncertainty (as measured by inverse exponentiated absolute value of evidence ratio). Future experiments are needed to test this hypothesis directly. Ideally, behavioral measures of uncertainty would be compared to neural measures of population heterogeneity.</p></sec><sec id="s2-9"><title>Animal-to-animal variability</title><p>One challenge in the study of hippocampal remapping is that different animals respond differently to the same environments. Indeed, many of the previously discussed studies reported significant heterogeneity across animals in remapping behavior. Studies of the development of remapping over the course of learning frequently report that different animals learn at differing rates (<xref ref-type="bibr" rid="bib10">Bostock et al., 1991</xref>; <xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>). In fact, the variability across animals is frequently a nuisance in running experiments. The pre-training for one of the morph experiments described above (<xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>) had three different ways that the observations could be partitioned. Out of the six animals they trained, four animals partitioned the observations in the way the experimenters expected, and the other two animals partitioned the observations in the other two possible ways (and therefore were excluded from the rest of the study).</p><p>The hidden state inference model offers one way to capture this heterogeneity across animals. The concentration parameter <inline-formula><mml:math id="inf38"><mml:mi>α</mml:mi></mml:math></inline-formula> (see Materials and methods) controls the tendency to infer new hidden states when unexpected data are observed. Variation in this parameter was previously used to model age-dependent (<xref ref-type="bibr" rid="bib29">Gershman et al., 2010</xref>; <xref ref-type="bibr" rid="bib34">Gershman et al., 2017b</xref>) and individual (<xref ref-type="bibr" rid="bib36">Gershman and Hartley, 2015a</xref>) variability in learning. While partitioning large amounts of cleanly separated data is insensitive to changes of <inline-formula><mml:math id="inf39"><mml:mi>α</mml:mi></mml:math></inline-formula> over several orders of magnitude, <inline-formula><mml:math id="inf40"><mml:mi>α</mml:mi></mml:math></inline-formula> can have effects on partitioning of ambiguous or insufficient data. For example, if we take the learning of remapping explored in <xref ref-type="fig" rid="fig3">Figure 3</xref>, changes in the value of <inline-formula><mml:math id="inf41"><mml:mi>α</mml:mi></mml:math></inline-formula> can alter the speed at which the model switches from preferring a one-state hypothesis to a two-state hypothesis (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). Moreover, if we take evidence ratios around 0 as indicative of partial remapping, different <inline-formula><mml:math id="inf42"><mml:mi>α</mml:mi></mml:math></inline-formula> values can lead to different lengths of time spent in the partial remapping regime, even for the exact same set of experiences.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Animal-to-animal variability may be the result of animal-specific parameter settings.</title><p>(<bold>A</bold>) Simulations from <xref ref-type="fig" rid="fig3">Figure 3C</xref> with different values of <inline-formula><mml:math id="inf43"><mml:mi>α</mml:mi></mml:math></inline-formula>. Larger values of alpha lead to a greater tendency to infer a larger number of hidden states, and therefore a faster transition from preferring the single-state model to the two-state model. (<bold>B</bold>) The training protocol from Supporting Figure 1C of <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>. (<bold>C</bold>) In red is the probability assigned to the hypothesis that the white circle, morph circle, and morph square are all generated by separate hidden states. In blue is the probability assigned to the hypothesis that the white circle and morph circle are generated by the same hidden state and the morph square is generated by a separate hidden state, which is the hypothesis that the authors expected. In purple is the probability assigned to the hypothesis that all of the enclosures are generated by the same hidden state (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>). Different settings of <inline-formula><mml:math id="inf44"><mml:mi>α</mml:mi></mml:math></inline-formula> result in different preferred assignments of observations to hidden states, corresponding to the finding that different animals had different remapping behaviors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51140-fig9-v1.tif"/><permissions><copyright-statement>© 2005 AAAS. All rights reserved</copyright-statement><copyright-year>2005</copyright-year><copyright-holder>AAAS</copyright-holder><license><license-p>Panel B is reproduced from <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref> with permission (originally published as Supporting Figure 1C). It is not covered by the CC-BY 4.0 licence and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions></fig><p>To explore a second manifestation of animal variability, we ran a simulation resembling the training of <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>. We characterize observations with two features: shape and color of the enclosure. The white circle is characterized by a 2D Gaussian with means [1, 1], the morph circle is characterized by means [1, -1], and the morph square is characterized by means [−1,–1]; all standard deviations are 0.1. We provided the model with observations from these generative distributions according to the schedule used by <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref> (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). We then asked the model to assign an unnormalized posterior probability to the following hypotheses:</p><list list-type="order"><list-item><p>Each of the environments were drawn from separate hidden states (<xref ref-type="fig" rid="fig9">Figure 9C</xref>, red bars), corresponding to ‘did not show wooden circle to morph-circle pattern transfer.’</p></list-item><list-item><p>The circles were the same and were different from the square (<xref ref-type="fig" rid="fig9">Figure 9C</xref>, blue bars), corresponding to the selection criterion adopted by <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>.</p></list-item><list-item><p>All the observations were drawn from a single hidden state (<xref ref-type="fig" rid="fig9">Figure 9C</xref>, purple bars), corresponding to ‘failed to show rapid remapping in the morph-square and the wooden circle'.</p></list-item></list><p>Different values of <inline-formula><mml:math id="inf45"><mml:mi>α</mml:mi></mml:math></inline-formula> lead to variation in relative preferences for these hypotheses.</p><p>These results invite the interpretation that animal variability may be understood in terms of individual differences in the <inline-formula><mml:math id="inf46"><mml:mi>α</mml:mi></mml:math></inline-formula> parameter (though of course other parametric variations might produce some of the same effects).</p></sec><sec id="s2-10"><title>The effect of cue variability</title><p>In this section, we explore an experimental prediction of the model that highlights one of its key insights: remapping critically depends on past experience. Consider an environment that is characterized by two features. We can separate animals into two training groups: one in which feature one is highly variable and one in which feature two is highly variable (cyan and magenta dots in <xref ref-type="fig" rid="fig10">Figure 10A</xref>). We then probe with an observation that has a novel value in feature 1 (red x in <xref ref-type="fig" rid="fig10">Figure 10A</xref>). The model predicts that an animal trained with higher variability in feature one will be more likely to assign the novel observation to the same state as the previous observations (i.e., not to remap; <xref ref-type="fig" rid="fig10">Figure 10B</xref>). Intuitively, high variability will make the place fields more ‘‘tolerant’’ of deviations from the central tendency of the distribution. After initial submission of this paper, a similar result was posted as a preprint (<xref ref-type="bibr" rid="bib89">Plitt and Giocomo, 2019</xref>).</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Cue variability should affect remapping behavior.</title><p>(<bold>A</bold>) Two training protocols (cyan and magenta) give (<bold>B</bold>) qualitatively different hidden state inferences when presented with the same novel observation (red dot in A). The cyan training is drawn from a Gaussian with mean [−5,0] and standard deviations [2, 0.1], whereas the magenta training is drawn from a Gaussian with mean [0,0] and standard deviations [0.1, 2]. The probe is presented after 20 training observations. The state evidence ratio here is the comparison between the assignment of the probe to the same hidden state as the training samples vs. a novel hidden state (<xref ref-type="disp-formula" rid="equ11">Equation 11</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51140-fig10-v1.tif"/></fig><p>By analogy, imagine a building with many similar conference rooms. One conference room always has its chairs arranged in a particular configuration (a low variability context), whereas another conference room frequently has different configurations (a high variability context). Intuitively, a change in the expected configuration in the low variability context will prompt the inference that you must be in a different room (and hence the place cells in your hippocampus will remap), whereas a change in the expected configuration in the high variability context will not. In the high variability context, you expect the unexpected (cf. the concept of ‘expected uncertainty’ in <xref ref-type="bibr" rid="bib115">Yu and Dayan, 2005</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have proposed that hippocampal remapping provides a window into the process of hidden state inference. According to our framework, animals receive a stream of observations (data points), which they attempt to partition according to the hypothetical hidden states that generated them. Bayesian inference offers a natural solution to this problem. The specific form of Bayesian nonparametric model that we employed here has been previously invoked to explain a number of other hippocampal-dependent behavioral phenomena (<xref ref-type="bibr" rid="bib29">Gershman et al., 2010</xref>; <xref ref-type="bibr" rid="bib33">Gershman et al., 2017a</xref>; <xref ref-type="bibr" rid="bib30">Gershman et al., 2014</xref>). In this paper, we showed that this model recapitulates a broad range of remapping phenomena.</p><p>Central to our account is the idea that remapping reflects inferences about the hidden state, and in particular that partial remapping corresponds to high levels of uncertainty. Manipulations of sensory cues, environmental geometry, and training can all be understood in terms of their effects on state uncertainty. While this account has the potential to unify many phenomena under a common theoretical umbrella, there are still many limitations, loose ends and open questions, which we discuss below.</p><sec id="s3-1"><title>What is the feature space?</title><p>Our model takes feature vectors as its inputs, but what are these features? In our simulations, we allowed them to be highly abstract idealizations. Ultimately, a biologically grounded theory must specify these features in terms of the inputs to the hippocampus. Furthermore, it will be necessary to more explicitly specify what timescale the model is operating on, since different features are relevant at different timescales. Although we have focused on the timescale of hours to days, map switches can occur on the subsecond timescale (<xref ref-type="bibr" rid="bib86">Olypher et al., 2002</xref>; <xref ref-type="bibr" rid="bib54">Jezek et al., 2011</xref>; <xref ref-type="bibr" rid="bib57">Kelemen and Fenton, 2016</xref>).</p><p>One general hypothesis about the feature space encoded by the hippocampus is the <italic>successor representation</italic> theory (<xref ref-type="bibr" rid="bib110">Stachenfeld et al., 2017</xref>), which posits that place cells encode a predictive map of the state space. On this view, the feature inputs to the hippocampus correspond to state features. This raises the intriguing possibility that remapping should be sensitive to predictive relationships between states. Many studies have observed that place cells are modulated by prospective information like the animal’s future trajectory (e.g., <xref ref-type="bibr" rid="bib7">Battaglia et al., 2004</xref>; <xref ref-type="bibr" rid="bib23">Ferbinteanu and Shapiro, 2003</xref>). It is less clear whether there is any evidence for global remapping as a function of changes in prospective information.</p></sec><sec id="s3-2"><title>What is an observation?</title><p>In this paper, we characterize ‘observations’ as feature vectors. A question is what qualifies as a single observation? For the experiments we highlight in the Results section, we define a single observation as an entire session, as temporal continuity constrains within-session variability in a way that between-session variability is not constrained. However, a session is not the only time scale over which an animal may perform hidden state inference. Different definitions of a single observation can highlight phenomena occurring on different time scales, such as the map switches that can occur on a timescale of 100 ms-1s (<xref ref-type="bibr" rid="bib86">Olypher et al., 2002</xref>; <xref ref-type="bibr" rid="bib57">Kelemen and Fenton, 2016</xref>).</p><p>One limitation is that an observation must be sufficiently long for sampling of all features of an environment. Indeed, it takes some time for the representation of an environment to settle down as the animal samples the environment (<xref ref-type="bibr" rid="bib69">Leutgeb et al., 2004</xref>). The process of sampling the environment is a research question in its own right and the nature of that process will have important effects on the hidden state inference process we describe here. </p><p>A related question is how often hidden state beliefs are recalculated. One possibility is that it is recalculated every theta cycle. Another is that change detection or event segmentation may be used (<xref ref-type="bibr" rid="bib26">Franklin et al., 2019</xref>).</p></sec><sec id="s3-3"><title>Approximate inference</title><p>As discussed in the first section of the Results, exact inference over assignments of observations to hidden states is intractable, because the number of possible partitions is too large. As a result of this intractability, for most of the paper, we have limited ourselves to comparisons between a small number of hypotheses (selected based on the fact that most of the posterior probability will be concentrated on these hypotheses). This should be understood as an analytical heuristic rather than as an algorithmic theory of how the brain approximates probabilistic inference. It may be that the hippocampus does explicitly compare a small number of hypotheses. The key step in that algorithm would be generating appropriate proposals. A complete algorithmic theory must explain how the brain deals with arbitrarily large hypothesis spaces.</p><p>One idea is to model the hippocampus as stochastically sampling the hypothesis space (<xref ref-type="bibr" rid="bib24">Fox and Prescott, 2010</xref>; <xref ref-type="bibr" rid="bib101">Savin et al., 2014</xref>). According to this view, a sampling approximation approach would discretely represent each hypothesis with a frequency proportional to its probability. This fits nicely with the empirical finding that multiple maps can alternate rapidly (<xref ref-type="bibr" rid="bib57">Kelemen and Fenton, 2016</xref>; <xref ref-type="bibr" rid="bib56">Kelemen and Fenton, 2010</xref>; <xref ref-type="bibr" rid="bib51">Jackson and Redish, 2007</xref>; <xref ref-type="bibr" rid="bib55">Kay et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Jezek et al., 2011</xref>). Some of these findings suggest an oscillatory implementation, whereby each theta cycle plays the role of a single sample from the distribution of possible hidden states, and the extent of map switching corresponds to the degree of uncertainty about hidden state assignment. Indeed, map switching increases at points of uncertainty (<xref ref-type="bibr" rid="bib54">Jezek et al., 2011</xref>). We would additionally predict that measures of map switching such as overdispersion would decrease over the course of experience in protocols such as that of <xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref> as one hypothesis dominates (i.e., as the evidence ratio between alternative hypotheses gets farther from 0 in <xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p></sec><sec id="s3-4"><title>Priors</title><p>We have focused on examples where specification of the appropriate generative distribution has been relatively straightforward: Gaussians (or the analogous Von Mises for circular variables) have fit nicely. Not every problem corresponds to these generative distributions, however, and the animal may have to perform inference and/or meta-learning over which generative distributions are appropriate for each cue class.</p><p>Another type of prior that we use is the Chinese Restaurant Process (CRP) prior over partitions of observations into hidden states, as discussed in the Materials and methods. We explore the utility of the CRP’s <inline-formula><mml:math id="inf47"><mml:mi>α</mml:mi></mml:math></inline-formula> parameter in capturing animal-to-animal variability in remapping response to similar experimental protocols. The <inline-formula><mml:math id="inf48"><mml:mi>α</mml:mi></mml:math></inline-formula> parameter also has the potential to itself be learned. For example, animals trained with a larger number of hidden states (e.g., enriched environment) may grow to employ a larger magnitude <inline-formula><mml:math id="inf49"><mml:mi>α</mml:mi></mml:math></inline-formula> than animals trained with a smaller number of hidden states. As <inline-formula><mml:math id="inf50"><mml:mi>α</mml:mi></mml:math></inline-formula> defines the animal’s relative preference for a larger number of hidden states, modulation of the value of <inline-formula><mml:math id="inf51"><mml:mi>α</mml:mi></mml:math></inline-formula> in response to the complexity of past experience would be adaptive.</p></sec><sec id="s3-5"><title>Hierarchical inference</title><p>Throughout this paper, we have assumed that hidden states are independent, but in reality, hidden states can share some structure while continuing to be distinct. Hierarchical inference can be useful to solve these problems (<xref ref-type="bibr" rid="bib38">Gershman and Niv, 2015b</xref>). Our model does not directly address the question of hierarchical inference in the hippocampus. One possibility is that hidden state inference is explicitly hierarchical even within a co-localized population. <xref ref-type="bibr" rid="bib77">McKenzie et al., 2014</xref> found a hierarchy of representational similarities in dorsal hippocampus. This could correspond to a single population of place cells performing hidden state inference simultaneously at different levels of a hierarchy. For example, although we have focused on hidden states corresponding to ‘context’, similar inference could be applied to identifying ‘position’ or ‘item’ categories used by <xref ref-type="bibr" rid="bib77">McKenzie et al., 2014</xref>. Another way that the model could be extended is based on the organization of place fields by size along the dorso-ventral axis of the hippocampus. An analogy between place field sizes and hidden state inference on the level of context raises an interesting possibility. The range of locations that are categorized as the same in terms of being included in the same place field increases along the dorso-ventral axis (<xref ref-type="bibr" rid="bib76">Maurer et al., 2005</xref>), so too the range of observations that would be categorized as the same in terms of context-level hidden state inference may increase along the dorso-ventral axis. This gradient could be implemented if the same hidden state inference process would occur independently at different distances along that axis with different values of <inline-formula><mml:math id="inf52"><mml:mi>α</mml:mi></mml:math></inline-formula>, leading to different proclivities for opening new hidden states. In that way, it would be possible for two observations to be assigned to the same hidden state at one location along the axis and assigned to different hidden states at another location along the axis, leading to a partial sharing of learning between the two observations. A test of this suggestion would be that remapping behavior should be different at different locations along the dorsoventral axis. More research is needed to determine how hierarchical inference is performed in the hippocampus.</p></sec><sec id="s3-6"><title>Long-term instability of place fields</title><p>Early reports of place field stability demonstrated the existence of place fields that maintained spatial preference over the period of a month (<xref ref-type="bibr" rid="bib111">Thompson and Best, 1990</xref>). However, recent work with sufficient statistical power have painted a different picture. Large scale recordings of hundreds of place cells over the course of a month have shown that place field instability is the norm over long time periods (<xref ref-type="bibr" rid="bib116">Ziv et al., 2013</xref>; <xref ref-type="bibr" rid="bib95">Rubin et al., 2015</xref>). It is possible that these reports of place field instability are the result of misattribution of place cell identity when comparing across sessions, but much effort has been put into avoiding such methodological issues (<xref ref-type="bibr" rid="bib105">Sheintuch et al., 2017</xref>). One possible way to incorporate this finding within the hidden state inference framework is to appeal to computational work that allows for maintained fidelity of representation even with changing neuronal substrate (<xref ref-type="bibr" rid="bib91">Raman et al., 2019</xref>; <xref ref-type="bibr" rid="bib96">Rule et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Druckmann and Chklovskii, 2012</xref>). On the other hand, changes in neural representation may reflect the cognitive finding that memory content can actually be modified over time through processes such as memory reconsolidation (<xref ref-type="bibr" rid="bib68">Lee et al., 2017</xref>). Another approach is to assert that drift over time in place cell activity reflects an interest in maintaining different hidden states for the same observation occurring at different points in time. The extension of our framework with a generative model that explicitly accounts for time would be fruitful (see for example <xref ref-type="bibr" rid="bib33">Gershman et al., 2017a</xref>).</p></sec><sec id="s3-7"><title>Behavioral relevance of remapping</title><p>An interesting fact about the remapping literature is that there has been relatively little work done relating remapping to behavior. The widespread assumption in the field is that hippocampal remapping is the neurophysiological substrate for context-dependent learning (<xref ref-type="bibr" rid="bib17">Colgin et al., 2008</xref>), and that is an assumption that we have followed here. There is some correlational evidence of a connection (<xref ref-type="bibr" rid="bib60">Kentros et al., 2004</xref>; <xref ref-type="bibr" rid="bib58">Kennedy and Shapiro, 2009</xref>). However, to our knowledge, there has not been a demonstration of a causal connection (<xref ref-type="bibr" rid="bib65">Kubie et al., 2019</xref>). In fact, <xref ref-type="bibr" rid="bib53">Jeffery et al., 2003</xref> show task performance transfer between two conditions that show near-global remapping.</p><p>We suggest an experiment that emphasizes the role of the hippocampus in ‘latent learning’ (<xref ref-type="bibr" rid="bib112">Tolman, 1948</xref>), and relies on our prediction that different training would give rise to different remapping behavior (<xref ref-type="fig" rid="fig10">Figure 10</xref>). Train two groups of animals to either remap or not remap to a given manipulation. For example, train one group of animals in a morph box where the configuration of the walls changes every day, which we would expect to lead to a lack of remapping between circle and square configurations. Train the other group with the same morph box but only presenting the square or circle configurations, similar to the training protocol of <xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>, which we would expect to eventually lead to remapping between the circle and square configurations. Once the expected remapping behavior is neurophysiologically verified, the animals would undergo fear conditioning in one configuration (square or circle). The prediction would be that generalization of the fear memory to the other configuration would depend on which training the animal had received and correspondingly, which remapping behavior the animal had exhibited. An experiment such as that would be a first step towards ascertaining the behavioral relevance of hippocampal remapping.</p></sec><sec id="s3-8"><title>Rotation experiments</title><p>There are several classes of empirical results that are related to the results explained in this paper, but not directly explained by our model. For example, in rotation experiments, the experimenters manipulated cues associated with the environment itself (‘proximal cues’ or ‘maze cues’) and/or manipulated cues associated with the room that the recording environment was placed in (‘distal cues’ or ‘room cues’). They asked questions such as whether the place cells followed the rotation of the maze cues or the room cues (<xref ref-type="bibr" rid="bib103">Shapiro et al., 1997</xref>), and whether the place cells followed the animal’s own motion or the motion of the cues (<xref ref-type="bibr" rid="bib63">Knierim et al., 1998</xref>). The answers to these questions were generally inconclusive, as they were sensitive to slight differences in protocol across labs. However, a consistent finding was that the results changed over the course of experience. For example, when a cue was repeatedly moved relative to other cues in an unstructured way, the cue lost control over the rotational alignment of the place fields (<xref ref-type="bibr" rid="bib62">Knierim et al., 1995</xref>). While we do not explicitly model spatial relationships in our simulations, Knierim’s finding is similar to the training variance effect described in <xref ref-type="fig" rid="fig10">Figure 10</xref>: when the model is trained with observations for which a cue has high variance, further variation in that cue is less likely to cause a new hidden state to be inferred.</p><p>Conversely, in <xref ref-type="bibr" rid="bib103">Shapiro et al., 1997</xref>, the maze cues and room cues were each rotated 90 degrees in opposite directions. Initially, the place cell representation split, some following room cues and some following maze cues. However, after a few repetitions, the place cell representation remapped between the two conditions. This is reminiscent of the simulations in <xref ref-type="fig" rid="fig3">Figure 3</xref>, where a particular cue manipulation (square-circle) initially does not cause remapping, but after sufficient repetition, the place cells remap between the conditions; more evidence has been gathered to support the hypothesis that two distinct hidden states exist.</p></sec><sec id="s3-9"><title>Types of remapping</title><p>An influential interpretation of the literature has been that there are two main types of remapping: ‘global remapping’ and ‘rate remapping’. In particular, it has been argued that global remapping corresponds to changes in physical location whereas rate remapping corresponds to changes in condition that occur at those locations (<xref ref-type="bibr" rid="bib70">Leutgeb et al., 2005a</xref>; <xref ref-type="bibr" rid="bib18">Colgin et al., 2010</xref>; <xref ref-type="bibr" rid="bib4">Alme et al., 2014</xref>; <xref ref-type="bibr" rid="bib73">Lisman et al., 2017</xref>). As discussed in the Introduction, the lines between global remapping and rate remapping are not so sharp. Global remapping can occur between conditions at the same physical location (<xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref>), and rate remapping can occur between different physical locations (<xref ref-type="bibr" rid="bib109">Spiers et al., 2015</xref>). Moreover, the same manipulation can cause global remapping or rate remapping at different points in training (<xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>). Our work provides an explanation for why there are not clear delineations of which manipulations cause which types of remapping. The animal must <italic>infer</italic> hidden states from its observations. Alternative hypotheses must be considered as long as ambiguity exists about the appropriate assignment of observations to hidden states. This uncertainty about hidden state assignment can manifest as ‘partial’ or ‘rate’ remapping. The statistics of these hidden states can be learned over the course of experience, leading to increased certainty about hidden state assignments. This increased certainty can be observed as more definitive ‘global’ remapping or conversely, lack of remapping. One of our key points is that these categories are better thought of as existing along a continuum defined by state uncertainty.</p></sec><sec id="s3-10"><title>Relationship to other theories</title><p>How does this proposal relate to other theoretical perspectives on hippocampal remapping? We can contrast our model with a basic similarity threshold model, according to which each state is associated with a fixed set of features, and new observations would be classified as the same or different based on whether they exceed some threshold of change detection. This model does not capture some of the key phenomena associated with remapping; in particular, it cannot account for any of the ways in which learning affects remapping.</p><p>One major model of remapping is the attractor network. Based on early work by <xref ref-type="bibr" rid="bib48">Hopfield, 1982</xref>, the idea is that activity patterns associated with particular observations are learned by the network so as to be able to recover those activity patterns when degraded versions are presented. One attractor network implementation that has been specifically used to model remapping results was proposed by <xref ref-type="bibr" rid="bib9">Blumenfeld et al., 2006</xref>. They sought to explain the difference in results between <xref ref-type="bibr" rid="bib113">Wills et al., 2005</xref> and <xref ref-type="bibr" rid="bib71">Leutgeb et al., 2005b</xref> by focusing on the scrambled order of the morph sequence. Their model was a conventional Hopfield network augmented with a ‘weight’ term to change the pattern strength based on the novelty of that pattern. This led to attractors that were lumped together when the morph experiences were presented in sequential order instead of in a scrambled order. However, later work <xref ref-type="bibr" rid="bib18">Colgin et al., 2010</xref> demonstrated that the order of presentation of the morph experiences was not the decisive factor in the qualitative results of the morph experiments (as described in more detail in the ‘Morph Experiments’ section of the Results).</p><p>The attractor network perspective can be connected to our hidden state inference model by examining the probabilistic version of the Hopfield network, known as the <italic>Boltzmann machine</italic> (<xref ref-type="bibr" rid="bib1">Ackley et al., 1985</xref>). The basins of attraction can be understood heuristically as feature configurations for distinct hidden states. One can make this heuristic connection more precise by defining an explicitly state-dependent energy function combined with a distribution over states, which would correspond to a mixture of Boltzmann machines (<xref ref-type="bibr" rid="bib83">Nair and Hinton, 2009</xref>; <xref ref-type="bibr" rid="bib97">Salakhutdinov et al., 2013</xref>).</p><p>In computational neuroscience, attractor networks are usually used as mechanistic descriptions of neuronal dynamics, unlike our hidden state inference model that operates at a higher level of abstraction. Thus, comparison of the two approaches is not entirely straightforward. It is possible that an attractor network could be used as an implementation of parts of the hidden state inference model. For example, inference about new states vs. old states is conceptually similar to the distinction between ‘‘pattern separation’’ in the dentate gyrus and ‘‘pattern completion’’ in CA3 (<xref ref-type="bibr" rid="bib64">Knierim and Neunuebel, 2016</xref>; <xref ref-type="bibr" rid="bib93">Rolls and Kesner, 2006</xref>). The attractor network describes <italic>how</italic> pattern separation and completion work. The hidden state inference model describes <italic>why</italic> pattern separation and completion work the way they do.</p><p>Our hidden state inference model is similar in spirit to the probabilistic model of remapping developed by <xref ref-type="bibr" rid="bib27">Fuhs and Touretzky, 2007</xref>. In that model, each context is represented by a Hidden Markov Model. Remapping is then formalized as a model comparison problem. Like our model, their calculation weighs both simplicity of a hypothetical partition and its fit with the observed data. They use their model to explain gradual remapping (<xref ref-type="bibr" rid="bib72">Lever et al., 2002</xref>), failure to generalize (<xref ref-type="bibr" rid="bib45">Hayman et al., 2003</xref>), and some aspects of reversal learning and sequence learning. The technical differences between our models are subtle and do not change the general conclusions, which we share. Our work can be thought of as an update to the work of <xref ref-type="bibr" rid="bib27">Fuhs and Touretzky, 2007</xref>. The main addition of this work is to stress the role of uncertainty and its relationship to partial remapping, rate remapping, and population heterogeneity, relationships that are highlighted in <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig4">4</xref>, <xref ref-type="fig" rid="fig7">7</xref> and <xref ref-type="fig" rid="fig8">8</xref>, for example.</p><p>The Temporal Context Model (TCM, <xref ref-type="bibr" rid="bib50">Howard and Kahana, 2002</xref>) was originally motivated by human episodic memory, but has also been applied to hippocampal/entorhinal recordings (<xref ref-type="bibr" rid="bib49">Howard et al., 2005</xref>). TCM defines a temporal context, which is a filtered version of the observations preceding the current observation. Later observations can be used to recall the temporal context of earlier observations. This model is particularly well suited to capturing hippocampal response to temporally modulated observations, such as those we explore in <xref ref-type="fig" rid="fig5">Figure 5</xref> (<xref ref-type="bibr" rid="bib75">Markus et al., 1995</xref>). Analogies can be drawn between TCM and our account (see <xref ref-type="bibr" rid="bib33">Gershman et al., 2017a</xref>, for discussion) by drawing a correspondence between temporal context and hidden state. The process of retrieving a past temporal context on presentation of a novel observation would be analogous to assigning that novel observation to the same hidden state as the previous observations with the same temporal context. One difference is that recall of temporal context is strictly similarity-based whereas hidden state inference explicitly models the generative distribution and therefore would have different predictions in cases that depend on training variance such as <xref ref-type="fig" rid="fig10">Figure 10</xref>.</p><p>There has been discussion about the role that context plays in context-dependent learning (<xref ref-type="bibr" rid="bib65">Kubie et al., 2019</xref>). One approach is to consider context to be a cue that can acquire associations and competes with other cues (<xref ref-type="bibr" rid="bib92">Rescorla and Wagner, 1972</xref>; <xref ref-type="bibr" rid="bib41">Grau and Rescorla, 1984</xref>). Another is that context modifies the associations that are learned with other cues (<xref ref-type="bibr" rid="bib11">Bouton, 1993</xref>; <xref ref-type="bibr" rid="bib82">Nadel and Willner, 1980</xref>; <xref ref-type="bibr" rid="bib46">Hirsh, 1974</xref>). We have previously proposed a model in which the animal performs inference over the alternative causal structures of the environment (<xref ref-type="bibr" rid="bib32">Gershman, 2017</xref>). Context can play each of the previously mentioned roles depending on the previous training that the animal received. We would postulate that the hidden state calculated through the process outlined in this paper would be used as an input to the inference described by <xref ref-type="bibr" rid="bib34">Gershman et al., 2017b</xref>.</p></sec><sec id="s3-11"><title>Conclusion</title><p>Place field remapping has long been one of the most puzzling aspects of hippocampal physiology, yet still lacks a comprehensive theoretical account. In this paper, we have taken steps towards such an account, starting with a normative formulation of the problem that we believe remapping is solving, namely hidden state inference. The algorithmic and biological underpinnings of this theory remain incomplete, setting a clear agenda for future theoretical work.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Generative model</title><p>We model the animal’s sensory inputs (observations) as a vector <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> consisting of <inline-formula><mml:math id="inf54"><mml:mi>D</mml:mi></mml:math></inline-formula> features. The specific representation of these features varies across experimental paradigms. The animal assumes that observations are generated by discrete hidden states. At each time point, a state is stochastically selected according to prior <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and the observation features are sampled from the observation distribution associated with that state, <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf57"><mml:msub><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> represents the parameters of the observation distribution for state <inline-formula><mml:math id="inf58"><mml:mi>c</mml:mi></mml:math></inline-formula>. For notational simplicity we will omit the time index <inline-formula><mml:math id="inf59"><mml:mi>t</mml:mi></mml:math></inline-formula> whenever it is unnecessary for the exposition.</p><p>We place a prior <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> over the parameters and then marginalize to obtain the likelihood that a set of <inline-formula><mml:math id="inf61"><mml:mi>m</mml:mi></mml:math></inline-formula> observations <inline-formula><mml:math id="inf62"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐘</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> came from a single state c<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>which we can extend to obtain the likelihood that a set of observations <inline-formula><mml:math id="inf63"><mml:mi mathvariant="bold">𝐘</mml:mi></mml:math></inline-formula> came from a set of <inline-formula><mml:math id="inf64"><mml:mi>K</mml:mi></mml:math></inline-formula> hidden states <inline-formula><mml:math id="inf65"><mml:mi mathvariant="bold">𝐜</mml:mi></mml:math></inline-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐘</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐘</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We model real-valued features with a multivariate normal observation distribution. The parameter vector is given by <inline-formula><mml:math id="inf66"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf67"><mml:msub><mml:mi>μ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the mean vector, and <inline-formula><mml:math id="inf68"><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the covariance matrix. We place a conjugate normal-Wishart distribution over these parameters (see <xref ref-type="bibr" rid="bib81">Murphy, 2007</xref> for more details), with hyperparameter values <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (prior mean), <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula> (scale parameter), <inline-formula><mml:math id="inf71"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></inline-formula> (degrees of freedom), and <inline-formula><mml:math id="inf72"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.02</mml:mn><mml:mo>*</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (scale matrix), where <inline-formula><mml:math id="inf73"><mml:mi>I</mml:mi></mml:math></inline-formula> is the <inline-formula><mml:math id="inf74"><mml:mi>D</mml:mi></mml:math></inline-formula>-dimensional identity matrix.</p><p>We model circular variables with a Von Mises observation distribution and a normal-gamma prior over the parameters. The hyperparamters of the prior are given by: <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (prior mean), <inline-formula><mml:math id="inf76"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula> (scale parameter), <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> (shape parameter), and <inline-formula><mml:math id="inf78"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> (rate parameter). Because in this case we cannot marginalize over parameters analytically, we used numerical integration.</p><p>To motivate our prior over hidden states, we start with a few basic desiderata: (i) the prior should be defined over an unbounded state space, allowing new states to be continually created; and (ii) the prior should prefer a small number of states, to facilitate generalization across observations (a form of Occam’s razor). These assumptions are satisfied by a simple nonparametric distribution known as the <italic>Chinese restaurant process</italic> (CRP; <xref ref-type="bibr" rid="bib2">Aldous, 1985</xref>; <xref ref-type="bibr" rid="bib35">Gershman and Blei, 2012a</xref>), which samples states according to the following sequential process:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mi>K</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is the number of previous observations assigned to state <inline-formula><mml:math id="inf80"><mml:mi>k</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf81"><mml:mi>K</mml:mi></mml:math></inline-formula> is the total number of states created prior to time point <inline-formula><mml:math id="inf82"><mml:mi>t</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>α</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is a concentration parameter that controls the propensity to create new states. When <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, all observations will be generated by the same state. As <inline-formula><mml:math id="inf85"><mml:mi>α</mml:mi></mml:math></inline-formula> approaches infinity, each observations will be generated by a unique state. More generally, the expected number of states after <inline-formula><mml:math id="inf86"><mml:mi>N</mml:mi></mml:math></inline-formula> observations is <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Another way of using the CRP prior is to analytically calculate an unnormalized log probability for a list of hidden state assignments <inline-formula><mml:math id="inf88"><mml:mi mathvariant="bold">𝐜</mml:mi></mml:math></inline-formula>:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>∗</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We set <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> for all figures except <xref ref-type="fig" rid="fig9">Figure 9</xref>, in which we explicitly explore the effects of variation in <inline-formula><mml:math id="inf90"><mml:mi>α</mml:mi></mml:math></inline-formula>. We emphasize that using this prior does not mean that the world actually generates hidden states through this process; it simply means that we are imputing this to the animal as its <italic>internal model</italic> of the world.</p></sec><sec id="s4-2"><title>Inference</title><p>To compute the posterior over hidden states, the likelihood is combined with a prior over state assignments, <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, according to Bayes’ rule (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). Because we are typically dealing with a set of observations, and hence a combinatorial space of state partitions (i.e., all possible assignments of observations to states), exact inference is intractable. However, because we are generally only interested in a small number of ‘plausible’ partitions, we can simplify the problem by only assessing the relative probability of those states. The probability of each of those partitions <inline-formula><mml:math id="inf92"><mml:mi>c</mml:mi></mml:math></inline-formula> given a set of observations <inline-formula><mml:math id="inf93"><mml:mi>Y</mml:mi></mml:math></inline-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐘</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐘</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In particular, most of our simulations concern the question of whether two or three sets of observations are assigned to the same or different states. If we assume that all other partitions have probability close to 0, then we can ignore them without too much loss in accuracy. We use <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> in <xref ref-type="fig" rid="fig9">Figure 9C</xref>.</p><p>The partition evidence ratio reported in the main text is the log odds ratio between the posterior probabilities of two hypotheses (partitions <inline-formula><mml:math id="inf94"><mml:mi mathvariant="bold">𝐜</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf95"><mml:msup><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="italic">P</mml:mi></mml:mrow><mml:mo mathvariant="italic" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mrow><mml:mo mathvariant="italic" stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo mathvariant="italic" stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="italic">P</mml:mi></mml:mrow><mml:mrow><mml:mo mathvariant="italic" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo mathvariant="italic" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">P</mml:mi></mml:mrow><mml:mo mathvariant="italic" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mrow><mml:mo mathvariant="italic" stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mrow><mml:mo mathvariant="bold">′</mml:mo></mml:mrow></mml:msup><mml:mo mathvariant="italic" stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="italic">P</mml:mi></mml:mrow><mml:mrow><mml:mo mathvariant="italic" stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mrow><mml:mo mathvariant="bold">′</mml:mo></mml:mrow></mml:msup><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf96"><mml:mi mathvariant="bold">𝐘</mml:mi></mml:math></inline-formula> denotes the set of observations, <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> and <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is given by <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>. We use <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> in <xref ref-type="fig" rid="fig3">Figures 3C</xref>, <xref ref-type="fig" rid="fig4">4D</xref>, <xref ref-type="fig" rid="fig5">5C</xref> and <xref ref-type="fig" rid="fig9">9A</xref>.</p><p>In some cases, we are interested in computing the posterior probability that a new observation <inline-formula><mml:math id="inf99"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is assigned to a particular state conditional on a hypothetical assignment of all past observations:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐘</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐘</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is from <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> and<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐘</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐘</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>is the posterior predictive distribution characterizing the probability of observing a value of <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> generated by a given hidden state <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> given all previous observations <inline-formula><mml:math id="inf103"><mml:msub><mml:mi mathvariant="bold">𝐘</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:math></inline-formula> with that hidden state assignment. For a Multivariate Normal likelihood function with a normal-Wishart prior, this is given by:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is the generalized Student-t distribution with hyperparameters <inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf108"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> as discussed in Section 8.3 of <xref ref-type="bibr" rid="bib81">Murphy, 2007</xref>.</p><p>The state evidence ratio reported in the main text is the log odds ratio between the posterior probabilities of two state assignments <inline-formula><mml:math id="inf109"><mml:mi>c</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf110"><mml:msup><mml:mi>c</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> for a given observation <inline-formula><mml:math id="inf111"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> given past state assignments <inline-formula><mml:math id="inf112"><mml:msub><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for past observations <inline-formula><mml:math id="inf113"><mml:msub><mml:mi mathvariant="bold">𝐘</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula><disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We use <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> in <xref ref-type="fig" rid="fig1">Figures 1B–C</xref>, <xref ref-type="fig" rid="fig2">2</xref>, <xref ref-type="fig" rid="fig4">4B</xref> and <xref ref-type="fig" rid="fig10">10B</xref>.</p></sec><sec id="s4-3"><title>Code</title><p>All code necessary to generate all figures can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/HoniSanders/Sanders-et-al-2020-Elife">https://github.com/HoniSanders/Sanders-et-al-2020-Elife</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/Sanders-et-al-2020-Elife">https://github.com/elifesciences-publications/Sanders-et-al-2020-Elife</ext-link>; <xref ref-type="bibr" rid="bib99">Sanders, 2020</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Hector Penagos for helpful comments on the manuscript. We thank Federico Claudi and <ext-link ext-link-type="uri" xlink:href="https://scidraw.io/">Scidraw.io</ext-link> for the rat illustration in <xref ref-type="fig" rid="fig1">Figure 1</xref>. This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-51140-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>No data were analyzed in this work.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackley</surname> <given-names>DH</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>A learning algorithm for boltzmann machines*</article-title><source>Cognitive Science</source><volume>9</volume><fpage>147</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog0901_7</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Aldous</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1985">1985</year><chapter-title>Exchangeability and Related Topics</chapter-title><source>École d'Été De Probabilités De Saint-Flour XIII</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>1</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1007/BFb0099421</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname> <given-names>K</given-names></name><name><surname>Rawlins</surname> <given-names>JNP</given-names></name><name><surname>Bannerman</surname> <given-names>DM</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Hippocampal Place Cells Can Encode Multiple Trial-Dependent Features through Rate Remapping</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>14752</fpage><lpage>14766</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6175-11.2012</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alme</surname> <given-names>CB</given-names></name><name><surname>Miao</surname> <given-names>C</given-names></name><name><surname>Jezek</surname> <given-names>K</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Place cells in the hippocampus: eleven maps for eleven rooms</article-title><source>PNAS</source><volume>111</volume><fpage>18428</fpage><lpage>18435</lpage><pub-id pub-id-type="doi">10.1073/pnas.1421056111</pub-id><pub-id pub-id-type="pmid">25489089</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anagnostaras</surname> <given-names>SG</given-names></name><name><surname>Gale</surname> <given-names>GD</given-names></name><name><surname>Fanselow</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Hippocampus and contextual fear conditioning: Recent controversies and advances</article-title><source>Hippocampus</source><volume>11</volume><fpage>8</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1002/1098-1063(2001)11:1&lt;8::AID-HIPO1015&gt;3.0.CO;2-7</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>MI</given-names></name><name><surname>Jeffery</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Heterogeneous modulation of place cell firing by changes in context</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>8827</fpage><lpage>8835</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-26-08827.2003</pub-id><pub-id pub-id-type="pmid">14523083</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battaglia</surname> <given-names>FP</given-names></name><name><surname>Sutherland</surname> <given-names>GR</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Local sensory cues and place cell directionality: additional evidence of prospective coding in the Hippocampus</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>4541</fpage><lpage>4550</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4896-03.2004</pub-id><pub-id pub-id-type="pmid">15140925</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittner</surname> <given-names>KC</given-names></name><name><surname>Milstein</surname> <given-names>AD</given-names></name><name><surname>Grienberger</surname> <given-names>C</given-names></name><name><surname>Romani</surname> <given-names>S</given-names></name><name><surname>Magee</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Behavioral time scale synaptic plasticity underlies CA1 place fields</article-title><source>Science</source><volume>357</volume><fpage>1033</fpage><lpage>1036</lpage><pub-id pub-id-type="doi">10.1126/science.aan3846</pub-id><pub-id pub-id-type="pmid">28883072</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blumenfeld</surname> <given-names>B</given-names></name><name><surname>Preminger</surname> <given-names>S</given-names></name><name><surname>Sagi</surname> <given-names>D</given-names></name><name><surname>Tsodyks</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dynamics of memory representations in networks with novelty-facilitated synaptic plasticity</article-title><source>Neuron</source><volume>52</volume><fpage>383</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.08.016</pub-id><pub-id pub-id-type="pmid">17046699</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bostock</surname> <given-names>E</given-names></name><name><surname>Muller</surname> <given-names>RU</given-names></name><name><surname>Kubie</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Experience-dependent modifications of hippocampal place cell firing</article-title><source>Hippocampus</source><volume>1</volume><fpage>193</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1002/hipo.450010207</pub-id><pub-id pub-id-type="pmid">1669293</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouton</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Context, time, and memory retrieval in the interference paradigms of pavlovian learning</article-title><source>Psychological Bulletin</source><volume>114</volume><fpage>80</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.114.1.80</pub-id><pub-id pub-id-type="pmid">8346330</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouton</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Context and behavioral processes in extinction</article-title><source>Learning &amp; Memory</source><volume>11</volume><fpage>485</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1101/lm.78804</pub-id><pub-id pub-id-type="pmid">15466298</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breese</surname> <given-names>CR</given-names></name><name><surname>Hampson</surname> <given-names>RE</given-names></name><name><surname>Deadwyler</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Hippocampal place cells: stereotypy and plasticity</article-title><source>The Journal of Neuroscience</source><volume>9</volume><fpage>1097</fpage><lpage>1111</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.09-04-01097.1989</pub-id><pub-id pub-id-type="pmid">2703869</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>SD</given-names></name><name><surname>Liang</surname> <given-names>KC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Hippocampus integrates context and shock into a configural memory in contextual fear conditioning</article-title><source>Hippocampus</source><volume>27</volume><fpage>145</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1002/hipo.22679</pub-id><pub-id pub-id-type="pmid">27806432</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>G</given-names></name><name><surname>King</surname> <given-names>JA</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>How vision and movement combine in the hippocampal place code</article-title><source>PNAS</source><volume>110</volume><fpage>378</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1073/pnas.1215834110</pub-id><pub-id pub-id-type="pmid">23256159</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cobar</surname> <given-names>LF</given-names></name><name><surname>Yuan</surname> <given-names>L</given-names></name><name><surname>Tashiro</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Place cells and long-term potentiation in the Hippocampus</article-title><source>Neurobiology of Learning and Memory</source><volume>138</volume><fpage>206</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2016.10.010</pub-id><pub-id pub-id-type="pmid">27794463</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colgin</surname> <given-names>LL</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Understanding memory through hippocampal remapping</article-title><source>Trends in Neurosciences</source><volume>31</volume><fpage>469</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.06.008</pub-id><pub-id pub-id-type="pmid">18687478</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colgin</surname> <given-names>LL</given-names></name><name><surname>Leutgeb</surname> <given-names>S</given-names></name><name><surname>Jezek</surname> <given-names>K</given-names></name><name><surname>Leutgeb</surname> <given-names>JK</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Moser</surname> <given-names>M-B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attractor-Map Versus Autoassociation Based Attractor Dynamics in the Hippocampal Network</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>35</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1152/jn.00202.2010</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Courville</surname> <given-names>AC</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Touretzky</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian theories of conditioning in a changing world</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>294</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.05.004</pub-id><pub-id pub-id-type="pmid">16793323</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Druckmann</surname> <given-names>S</given-names></name><name><surname>Chklovskii</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuronal circuits underlying persistent representations despite time varying activity</article-title><source>Current Biology</source><volume>22</volume><fpage>2095</fpage><lpage>2103</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.08.058</pub-id><pub-id pub-id-type="pmid">23084992</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fenton</surname> <given-names>AA</given-names></name><name><surname>Csizmadia</surname> <given-names>G</given-names></name><name><surname>Muller</surname> <given-names>RU</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Conjoint control of hippocampal place cell firing by two visual stimuli</article-title><source>Journal of General Physiology</source><volume>116</volume><fpage>191</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1085/jgp.116.2.191</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fenton</surname> <given-names>AA</given-names></name><name><surname>Muller</surname> <given-names>RU</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Place cell discharge is extremely variable during individual passes of the rat through the firing field</article-title><source>PNAS</source><volume>95</volume><fpage>3182</fpage><lpage>3187</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.6.3182</pub-id><pub-id pub-id-type="pmid">9501237</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferbinteanu</surname> <given-names>J</given-names></name><name><surname>Shapiro</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Prospective and retrospective memory coding in the Hippocampus</article-title><source>Neuron</source><volume>40</volume><fpage>1227</fpage><lpage>1239</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00752-9</pub-id><pub-id pub-id-type="pmid">14687555</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fox</surname> <given-names>C</given-names></name><name><surname>Prescott</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Hippocampus as unitary coherent particle filter</article-title><conf-name>The 2010 International Joint Conference on Neural Networks (IJCNN), IEEE</conf-name><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/IJCNN.2010.5596681</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>LM</given-names></name><name><surname>Stanley</surname> <given-names>GB</given-names></name><name><surname>Brown</surname> <given-names>EN</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Hippocampal plasticity across multiple days of exposure to novel environments</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>7681</fpage><lpage>7689</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1958-04.2004</pub-id><pub-id pub-id-type="pmid">15342735</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franklin</surname> <given-names>NT</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Ranganath</surname> <given-names>C</given-names></name><name><surname>Zacks</surname> <given-names>JM</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Structured event memory: a neuro-symbolic model of event cognition</article-title><source>Psychological Review</source><volume>127</volume><fpage>327</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1037/rev0000177</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuhs</surname> <given-names>MC</given-names></name><name><surname>Touretzky</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Context learning in the rodent Hippocampus</article-title><source>Neural Computation</source><volume>19</volume><fpage>3173</fpage><lpage>3215</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.12.3173</pub-id><pub-id pub-id-type="pmid">17970649</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A dedicated population for reward coding in the Hippocampus</article-title><source>Neuron</source><volume>99</volume><fpage>179</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.06.008</pub-id><pub-id pub-id-type="pmid">30008297</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Blei</surname> <given-names>DM</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Context, learning, and extinction</article-title><source>Psychological Review</source><volume>117</volume><fpage>197</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1037/a0017808</pub-id><pub-id pub-id-type="pmid">20063968</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Radulescu</surname> <given-names>A</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Statistical computations underlying the dynamics of memory updating</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003939</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Discovering latent causes in reinforcement learning</article-title><source>Current Opinion in Behavioral Sciences</source><volume>5</volume><fpage>43</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2015.07.007</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Context-dependent learning and causal structure</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>24</volume><fpage>557</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1110-x</pub-id><pub-id pub-id-type="pmid">27418259</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Monfils</surname> <given-names>MH</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>The computational nature of memory modification</article-title><source>eLife</source><volume>6</volume><elocation-id>e23763</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.23763</pub-id><pub-id pub-id-type="pmid">28294944</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Pouncy</surname> <given-names>HT</given-names></name><name><surname>Gweon</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>Learning the structure of social influence</article-title><source>Cognitive Science</source><volume>41 Suppl 3</volume><fpage>545</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1111/cogs.12480</pub-id><pub-id pub-id-type="pmid">28294384</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Blei</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>A tutorial on bayesian nonparametric models</article-title><source>Journal of Mathematical Psychology</source><volume>56</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2011.08.004</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Hartley</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Individual differences in learning predict the return of fear</article-title><source>Learning &amp; Behavior</source><volume>43</volume><fpage>243</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.3758/s13420-015-0176-z</pub-id><pub-id pub-id-type="pmid">26100524</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Exploring a latent cause theory of classical conditioning</article-title><source>Learning &amp; Behavior</source><volume>40</volume><fpage>255</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.3758/s13420-012-0080-8</pub-id><pub-id pub-id-type="pmid">22927000</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Novelty and inductive generalization in human reinforcement learning</article-title><source>Topics in Cognitive Science</source><volume>7</volume><fpage>391</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1111/tops.12138</pub-id><pub-id pub-id-type="pmid">25808176</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibbon</surname> <given-names>J</given-names></name><name><surname>Farrell</surname> <given-names>L</given-names></name><name><surname>Locurto</surname> <given-names>CM</given-names></name><name><surname>Duncan</surname> <given-names>HJ</given-names></name><name><surname>Terrace</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Partial reinforcement in autoshaping with pigeons</article-title><source>Animal Learning &amp; Behavior</source><volume>8</volume><fpage>45</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.3758/BF03209729</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gothard</surname> <given-names>KM</given-names></name><name><surname>Skaggs</surname> <given-names>WE</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Dynamics of mismatch correction in the hippocampal ensemble code for space: interaction between path integration and environmental cues</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>8027</fpage><lpage>8040</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-24-08027.1996</pub-id><pub-id pub-id-type="pmid">8987829</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grau</surname> <given-names>JW</given-names></name><name><surname>Rescorla</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Role of context in autoshaping</article-title><source>Technical Report</source><volume>3</volume><elocation-id>324</elocation-id><pub-id pub-id-type="doi">10.1037/0097-7403.10.3.324</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guzowski</surname> <given-names>JF</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Ensemble dynamics of hippocampal regions CA3 and CA1</article-title><source>Neuron</source><volume>44</volume><fpage>581</fpage><lpage>584</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.11.003</pub-id><pub-id pub-id-type="pmid">15541306</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardcastle</surname> <given-names>K</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name><name><surname>Giocomo</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Environmental boundaries as an error correction mechanism for grid cells</article-title><source>Neuron</source><volume>86</volume><fpage>827</fpage><lpage>839</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.039</pub-id><pub-id pub-id-type="pmid">25892299</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hargreaves</surname> <given-names>EL</given-names></name><name><surname>Yoganarasimha</surname> <given-names>D</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Cohesiveness of spatial and directional representations recorded from neural ensembles in the anterior thalamus, parasubiculum, medial entorhinal cortex, and Hippocampus</article-title><source>Hippocampus</source><volume>17</volume><fpage>826</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1002/hipo.20316</pub-id><pub-id pub-id-type="pmid">17598156</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayman</surname> <given-names>RM</given-names></name><name><surname>Chakraborty</surname> <given-names>S</given-names></name><name><surname>Anderson</surname> <given-names>MI</given-names></name><name><surname>Jeffery</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Context-specific acquisition of location discrimination by hippocampal place cells</article-title><source>European Journal of Neuroscience</source><volume>18</volume><fpage>2825</fpage><lpage>2834</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2003.03035.x</pub-id><pub-id pub-id-type="pmid">14656331</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirsh</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>The Hippocampus and contextual retrieval of information from memory: a theory</article-title><source>Behavioral Biology</source><volume>12</volume><fpage>421</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1016/S0091-6773(74)92231-7</pub-id><pub-id pub-id-type="pmid">4217626</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holland</surname> <given-names>PC</given-names></name><name><surname>Bouton</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hippocampus and context in classical conditioning</article-title><source>Current Opinion in Neurobiology</source><volume>9</volume><fpage>195</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(99)80027-0</pub-id><pub-id pub-id-type="pmid">10322181</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Neural networks and physical systems with emergent collective computational abilities</article-title><source>PNAS</source><volume>79</volume><fpage>2554</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id><pub-id pub-id-type="pmid">6953413</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname> <given-names>MW</given-names></name><name><surname>Fotedar</surname> <given-names>MS</given-names></name><name><surname>Datey</surname> <given-names>AV</given-names></name><name><surname>Hasselmo</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The temporal context model in spatial navigation and relational learning: toward a common explanation of medial temporal lobe function across domains</article-title><source>Psychological Review</source><volume>112</volume><fpage>75</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.112.1.75</pub-id><pub-id pub-id-type="pmid">15631589</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname> <given-names>MW</given-names></name><name><surname>Kahana</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A distributed representation of temporal context</article-title><source>Journal of Mathematical Psychology</source><volume>46</volume><fpage>269</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1006/jmps.2001.1388</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname> <given-names>J</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Network dynamics of hippocampal cell-assemblies resemble multiple spatial maps within single tasks</article-title><source>Hippocampus</source><volume>17</volume><fpage>1209</fpage><lpage>1229</lpage><pub-id pub-id-type="doi">10.1002/hipo.20359</pub-id><pub-id pub-id-type="pmid">17764083</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeffery</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>The Neurobiology of Spatial Behaviour</source><publisher-loc> Cambridge</publisher-loc><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1007/s10071-004-0211-5</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeffery</surname> <given-names>KJ</given-names></name><name><surname>Gilbert</surname> <given-names>A</given-names></name><name><surname>Burton</surname> <given-names>S</given-names></name><name><surname>Strudwick</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Preserved performance in a hippocampal-dependent spatial task despite complete place cell remapping</article-title><source>Hippocampus</source><volume>13</volume><fpage>175</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1002/hipo.10047</pub-id><pub-id pub-id-type="pmid">12699326</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jezek</surname> <given-names>K</given-names></name><name><surname>Henriksen</surname> <given-names>EJ</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Theta-paced flickering between place-cell maps in the Hippocampus</article-title><source>Nature</source><volume>478</volume><fpage>246</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1038/nature10439</pub-id><pub-id pub-id-type="pmid">21964339</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname> <given-names>K</given-names></name><name><surname>Chung</surname> <given-names>JE</given-names></name><name><surname>Sosa</surname> <given-names>M</given-names></name><name><surname>Schor</surname> <given-names>JS</given-names></name><name><surname>Karlsson</surname> <given-names>MP</given-names></name><name><surname>Larkin</surname> <given-names>MC</given-names></name><name><surname>Liu</surname> <given-names>DF</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Regular cycling between representations of alternatives in the Hippocampus</article-title><source>Cell </source><volume>180</volume><fpage>552</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.01.014</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelemen</surname> <given-names>E</given-names></name><name><surname>Fenton</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dynamic grouping of hippocampal neural activity during cognitive control of two spatial frames</article-title><source>PLOS Biology</source><volume>8</volume><elocation-id>e1000403</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000403</pub-id><pub-id pub-id-type="pmid">20585373</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelemen</surname> <given-names>E</given-names></name><name><surname>Fenton</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Coordinating different representations in the Hippocampus</article-title><source>Neurobiology of Learning and Memory</source><volume>129</volume><fpage>50</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2015.12.011</pub-id><pub-id pub-id-type="pmid">26748023</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennedy</surname> <given-names>PJ</given-names></name><name><surname>Shapiro</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Motivational states activate distinct hippocampal representations to guide goal-directed behaviors</article-title><source>PNAS</source><volume>106</volume><fpage>10805</fpage><lpage>10810</lpage><pub-id pub-id-type="doi">10.1073/pnas.0903259106</pub-id><pub-id pub-id-type="pmid">19528659</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kentros</surname> <given-names>C</given-names></name><name><surname>Hargreaves</surname> <given-names>E</given-names></name><name><surname>Hawkins</surname> <given-names>RD</given-names></name><name><surname>Kandel</surname> <given-names>ER</given-names></name><name><surname>Shapiro</surname> <given-names>M</given-names></name><name><surname>Muller</surname> <given-names>RV</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Abolition of long-term stability of new hippocampal place cell maps by NMDA receptor blockade</article-title><source>Science</source><volume>280</volume><fpage>2121</fpage><lpage>2126</lpage><pub-id pub-id-type="doi">10.1126/science.280.5372.2121</pub-id><pub-id pub-id-type="pmid">9641919</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kentros</surname> <given-names>CG</given-names></name><name><surname>Agnihotri</surname> <given-names>NT</given-names></name><name><surname>Streater</surname> <given-names>S</given-names></name><name><surname>Hawkins</surname> <given-names>RD</given-names></name><name><surname>Kandel</surname> <given-names>ER</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Increased attention to spatial context increases both place field stability and spatial memory</article-title><source>Neuron</source><volume>42</volume><fpage>283</fpage><lpage>295</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(04)00192-8</pub-id><pub-id pub-id-type="pmid">15091343</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kinsky</surname> <given-names>NR</given-names></name><name><surname>Sullivan</surname> <given-names>DW</given-names></name><name><surname>Mau</surname> <given-names>W</given-names></name><name><surname>Hasselmo</surname> <given-names>ME</given-names></name><name><surname>Eichenbaum</surname> <given-names>HB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Hippocampal place fields maintain a coherent and flexible map across long timescales</article-title><source>Current Biology</source><volume>28</volume><fpage>3578</fpage><lpage>3588</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.09.037</pub-id><pub-id pub-id-type="pmid">30393037</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knierim</surname> <given-names>JJ</given-names></name><name><surname>Kudrimoti</surname> <given-names>HS</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Place cells, head direction cells, and the learning of landmark stability</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>1648</fpage><lpage>1659</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-03-01648.1995</pub-id><pub-id pub-id-type="pmid">7891125</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knierim</surname> <given-names>JJ</given-names></name><name><surname>Kudrimoti</surname> <given-names>HS</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Interactions between idiothetic cues and external landmarks in the control of place cells and head direction cells</article-title><source>Journal of Neurophysiology</source><volume>80</volume><fpage>425</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.80.1.425</pub-id><pub-id pub-id-type="pmid">9658061</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knierim</surname> <given-names>JJ</given-names></name><name><surname>Neunuebel</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tracking the flow of hippocampal computation: pattern separation, pattern completion, and attractor dynamics</article-title><source>Neurobiology of Learning and Memory</source><volume>129</volume><fpage>38</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2015.10.008</pub-id><pub-id pub-id-type="pmid">26514299</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubie</surname> <given-names>JL</given-names></name><name><surname>Levy</surname> <given-names>ERJ</given-names></name><name><surname>Fenton</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Is hippocampal remapping the physiological basis for context?</article-title><source>Hippocampus</source><volume>543</volume><elocation-id>23160</elocation-id><pub-id pub-id-type="doi">10.1002/hipo.23160</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Law</surname> <given-names>LM</given-names></name><name><surname>Bulkin</surname> <given-names>DA</given-names></name><name><surname>Smith</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Slow stabilization of concurrently acquired hippocampal context representations</article-title><source>Hippocampus</source><volume>26</volume><fpage>1560</fpage><lpage>1569</lpage><pub-id pub-id-type="doi">10.1002/hipo.22656</pub-id><pub-id pub-id-type="pmid">27650572</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>I</given-names></name><name><surname>Yoganarasimha</surname> <given-names>D</given-names></name><name><surname>Rao</surname> <given-names>G</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Comparison of population coherence of place cells in hippocampal subfields CA1 and CA3</article-title><source>Nature</source><volume>430</volume><fpage>456</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1038/nature02739</pub-id><pub-id pub-id-type="pmid">15229614</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>JLC</given-names></name><name><surname>Nader</surname> <given-names>K</given-names></name><name><surname>Schiller</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An update on memory reconsolidation updating</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>531</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.04.006</pub-id><pub-id pub-id-type="pmid">28495311</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leutgeb</surname> <given-names>S</given-names></name><name><surname>Leutgeb</surname> <given-names>JK</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Distinct ensemble codes in hippocampal Areas CA3 and CA1</article-title><source>Science</source><volume>305</volume><fpage>1295</fpage><lpage>1298</lpage><pub-id pub-id-type="doi">10.1126/science.1100265</pub-id><pub-id pub-id-type="pmid">15272123</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leutgeb</surname> <given-names>S</given-names></name><name><surname>Leutgeb</surname> <given-names>JK</given-names></name><name><surname>Barnes</surname> <given-names>CA</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2005">2005a</year><article-title>Independent codes for spatial and episodic memory in hippocampal neuronal ensembles</article-title><source>Science</source><volume>309</volume><fpage>619</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.1126/science.1114037</pub-id><pub-id pub-id-type="pmid">16040709</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leutgeb</surname> <given-names>JK</given-names></name><name><surname>Leutgeb</surname> <given-names>S</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name><name><surname>Meyer</surname> <given-names>R</given-names></name><name><surname>Barnes</surname> <given-names>CA</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005b</year><article-title>Progressive transformation of hippocampal neuronal representations in &quot;morphed&quot; environments</article-title><source>Neuron</source><volume>48</volume><fpage>345</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.09.007</pub-id><pub-id pub-id-type="pmid">16242413</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lever</surname> <given-names>C</given-names></name><name><surname>Wills</surname> <given-names>T</given-names></name><name><surname>Cacucci</surname> <given-names>F</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Long-term plasticity in hippocampal place-cell representation of environmental geometry</article-title><source>Nature</source><volume>416</volume><fpage>90</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1038/416090a</pub-id><pub-id pub-id-type="pmid">11882899</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname> <given-names>J</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name><name><surname>Nadel</surname> <given-names>L</given-names></name><name><surname>Ranganath</surname> <given-names>C</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Viewpoints: how the Hippocampus contributes to memory, navigation and cognition</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1434</fpage><lpage>1447</lpage><pub-id pub-id-type="doi">10.1038/nn.4661</pub-id><pub-id pub-id-type="pmid">29073641</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname> <given-names>L</given-names></name><name><surname>Leutgeb</surname> <given-names>JK</given-names></name><name><surname>Tsao</surname> <given-names>A</given-names></name><name><surname>Henriksen</surname> <given-names>EJ</given-names></name><name><surname>Leutgeb</surname> <given-names>S</given-names></name><name><surname>Barnes</surname> <given-names>CA</given-names></name><name><surname>Witter</surname> <given-names>MP</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Impaired hippocampal rate coding after lesions of the lateral entorhinal cortex</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1085</fpage><lpage>1093</lpage><pub-id pub-id-type="doi">10.1038/nn.3462</pub-id><pub-id pub-id-type="pmid">23852116</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markus</surname> <given-names>EJ</given-names></name><name><surname>Qin</surname> <given-names>YL</given-names></name><name><surname>Leonard</surname> <given-names>B</given-names></name><name><surname>Skaggs</surname> <given-names>WE</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Barnes</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Interactions between location and task affect the spatial and directional firing of hippocampal neurons</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>7079</fpage><lpage>7094</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-11-07079.1995</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maurer</surname> <given-names>AP</given-names></name><name><surname>VanRhoads</surname> <given-names>SR</given-names></name><name><surname>Sutherland</surname> <given-names>GR</given-names></name><name><surname>Lipa</surname> <given-names>P</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Self-motion and the origin of differential spatial scaling along the septo-temporal axis of the hippocampus</article-title><source>Hippocampus</source><volume>15</volume><fpage>841</fpage><lpage>852</lpage><pub-id pub-id-type="doi">10.1002/hipo.20114</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKenzie</surname> <given-names>S</given-names></name><name><surname>Frank</surname> <given-names>AJ</given-names></name><name><surname>Kinsky</surname> <given-names>NR</given-names></name><name><surname>Porter</surname> <given-names>B</given-names></name><name><surname>Rivière</surname> <given-names>PD</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hippocampal representation of related and opposing memories develop within distinct, hierarchically organized neural schemas</article-title><source>Neuron</source><volume>83</volume><fpage>202</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.05.019</pub-id><pub-id pub-id-type="pmid">24910078</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname> <given-names>RU</given-names></name><name><surname>Bostock</surname> <given-names>E</given-names></name><name><surname>Taube</surname> <given-names>JS</given-names></name><name><surname>Kubie</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>On the directional firing properties of hippocampal place cells</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>7235</fpage><lpage>7251</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-12-07235.1994</pub-id><pub-id pub-id-type="pmid">7996172</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A quarter of a century of place cells</article-title><source>Neuron</source><volume>17</volume><fpage>813</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80214-7</pub-id><pub-id pub-id-type="pmid">8938115</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname> <given-names>RU</given-names></name><name><surname>Kubie</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>The effects of changes in the environment on the spatial firing of hippocampal complex-spike cells</article-title><source>The Journal of Neuroscience</source><volume>7</volume><fpage>1951</fpage><lpage>1968</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.07-07-01951.1987</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>KP</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Conjugate Bayesian Analysis of the Gaussian Distribution</source><publisher-name>University of British Columbia</publisher-name><ext-link ext-link-type="uri" xlink:href="https://www.cse.iitk.ac.in/users/piyush/courses/tpmi_winter19/readings/bayesGauss.pdf">https://www.cse.iitk.ac.in/users/piyush/courses/tpmi_winter19/readings/bayesGauss.pdf</ext-link></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadel</surname> <given-names>L</given-names></name><name><surname>Willner</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Context and conditioning: A place for space</article-title><source>Physiological Psychology</source><volume>8</volume><fpage>218</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.3758/BF03332853</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nair</surname> <given-names>V</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Implicit mixtures of restricted boltzmann machines</article-title><conf-name>Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems</conf-name><fpage>1145</fpage><lpage>1152</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/3536-implicit-mixtures-of-restricted-boltzmann-machines">https://papers.nips.cc/paper/3536-implicit-mixtures-of-restricted-boltzmann-machines</ext-link></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Place units in the Hippocampus of the freely moving rat</article-title><source>Experimental Neurology</source><volume>51</volume><fpage>78</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1016/0014-4886(76)90055-8</pub-id><pub-id pub-id-type="pmid">1261644</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Speakman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Single unit activity in the rat Hippocampus during a spatial memory task</article-title><source>Experimental Brain Research</source><volume>68</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1007/BF00255230</pub-id><pub-id pub-id-type="pmid">3691688</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olypher</surname> <given-names>AV</given-names></name><name><surname>Lánský</surname> <given-names>P</given-names></name><name><surname>Fenton</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>On the location-specific positional and extra-positional information in the discharge of rat hippocampal cells</article-title><source>Biosystems</source><volume>67</volume><fpage>167</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/S0303-2647(02)00074-6</pub-id><pub-id pub-id-type="pmid">12459296</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname> <given-names>J</given-names></name><name><surname>Conway</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Hippocampal place units in the freely moving rat: why they fire where they fire</article-title><source>Experimental Brain Research</source><volume>590</volume><fpage>573</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1007/BF00239813</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penny</surname> <given-names>WD</given-names></name><name><surname>Zeidman</surname> <given-names>P</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Forward and backward inference in spatial cognition</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003383</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003383</pub-id><pub-id pub-id-type="pmid">24348230</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Plitt</surname> <given-names>MH</given-names></name><name><surname>Giocomo</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Experience dependent contextual codes in the Hippocampus</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/864090</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quirk</surname> <given-names>GJ</given-names></name><name><surname>Muller</surname> <given-names>RU</given-names></name><name><surname>Kubie</surname> <given-names>JL</given-names></name><name><surname>Ranck</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The positional firing properties of medial entorhinal neurons: description and comparison with hippocampal place cells</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>1945</fpage><lpage>1963</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.12-05-01945.1992</pub-id><pub-id pub-id-type="pmid">1578279</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raman</surname> <given-names>DV</given-names></name><name><surname>Rotondo</surname> <given-names>AP</given-names></name><name><surname>O'Leary</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fundamental bounds on learning performance in neural circuits</article-title><source>PNAS</source><volume>116</volume><fpage>10537</fpage><lpage>10546</lpage><pub-id pub-id-type="doi">10.1073/pnas.1813416116</pub-id><pub-id pub-id-type="pmid">31061133</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rescorla</surname> <given-names>RA</given-names></name><name><surname>Wagner</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>A theory of pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement BT - Clasical conditioning II: current research and theory</chapter-title><person-group person-group-type="editor"><name><surname>Black</surname> <given-names>A. H</given-names></name><name><surname>Prokasy</surname> <given-names>W. F</given-names></name></person-group><source>Clasical Conditioning II: Current Research and Theory</source><publisher-name>Appleton-Century-Crofts</publisher-name><fpage>64</fpage><lpage>99</lpage></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname> <given-names>ET</given-names></name><name><surname>Kesner</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A computational theory of hippocampal function, and empirical tests of the theory</article-title><source>Progress in Neurobiology</source><volume>79</volume><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2006.04.005</pub-id><pub-id pub-id-type="pmid">16781044</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rotenberg</surname> <given-names>A</given-names></name><name><surname>Muller</surname> <given-names>RU</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Variable place–cell coupling to a continuously viewed stimulus: evidence that the hippocampus is part of a perceptual system</article-title><source>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</source><volume>352</volume><fpage>1505</fpage><lpage>1513</lpage><pub-id pub-id-type="doi">10.1098/rstb.1997.0137</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname> <given-names>A</given-names></name><name><surname>Geva</surname> <given-names>N</given-names></name><name><surname>Sheintuch</surname> <given-names>L</given-names></name><name><surname>Ziv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hippocampal ensemble dynamics timestamp events in long-term memory</article-title><source>eLife</source><volume>4</volume><elocation-id>e12247</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12247</pub-id><pub-id pub-id-type="pmid">26682652</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rule</surname> <given-names>ME</given-names></name><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causes and consequences of representational drift</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>141</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.08.005</pub-id><pub-id pub-id-type="pmid">31569062</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Salakhutdinov</surname> <given-names>R</given-names></name><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Learning with Hierarchical-Deep models</article-title><conf-name>IEEE Transactions on Pattern Analysis and Machine Intelligence</conf-name><fpage>1958</fpage><lpage>1971</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2012.269</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanders</surname> <given-names>H</given-names></name><name><surname>Ji</surname> <given-names>D</given-names></name><name><surname>Sasaki</surname> <given-names>T</given-names></name><name><surname>Leutgeb</surname> <given-names>JK</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name><name><surname>Lisman</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Temporal coding and rate remapping: representation of nonspatial information in the Hippocampus</article-title><source>Hippocampus</source><volume>29</volume><fpage>111</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1002/hipo.23020</pub-id><pub-id pub-id-type="pmid">30129985</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sanders</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Sanders-et-al-2020-Elife</data-title><source>GitHub</source><version designator="6">6</version><ext-link ext-link-type="uri" xlink:href="https://github.com/HoniSanders/Sanders-et-al-2020-Elife">https://github.com/HoniSanders/Sanders-et-al-2020-Elife</ext-link></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Save</surname> <given-names>E</given-names></name><name><surname>Nerad</surname> <given-names>L</given-names></name><name><surname>Poucet</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Contribution of multiple sensory information to place field stability in hippocampal place cells</article-title><source>Hippocampus</source><volume>10</volume><fpage>64</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(2000)10:1&lt;64::AID-HIPO7&gt;3.0.CO;2-Y</pub-id><pub-id pub-id-type="pmid">10706218</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savin</surname> <given-names>C</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Lengyel</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal recall from bounded metaplastic synapses: predicting functional adaptations in hippocampal area CA3</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003489</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003489</pub-id><pub-id pub-id-type="pmid">24586137</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlesiger</surname> <given-names>MI</given-names></name><name><surname>Cannova</surname> <given-names>CC</given-names></name><name><surname>Boublil</surname> <given-names>BL</given-names></name><name><surname>Hales</surname> <given-names>JB</given-names></name><name><surname>Mankin</surname> <given-names>EA</given-names></name><name><surname>Brandon</surname> <given-names>MP</given-names></name><name><surname>Leutgeb</surname> <given-names>JK</given-names></name><name><surname>Leibold</surname> <given-names>C</given-names></name><name><surname>Leutgeb</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The medial entorhinal cortex is necessary for temporal organization of hippocampal neuronal activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1123</fpage><lpage>1132</lpage><pub-id pub-id-type="doi">10.1038/nn.4056</pub-id><pub-id pub-id-type="pmid">26120964</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapiro</surname> <given-names>ML</given-names></name><name><surname>Tanila</surname> <given-names>H</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Cues that hippocampal place cells encode: dynamic and hierarchical representation of local and distal stimuli</article-title><source>Hippocampus</source><volume>7</volume><fpage>624</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1997)7:6&lt;624::AID-HIPO5&gt;3.0.CO;2-E</pub-id><pub-id pub-id-type="pmid">9443059</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharp</surname> <given-names>PE</given-names></name><name><surname>Kubie</surname> <given-names>JL</given-names></name><name><surname>Muller</surname> <given-names>RU</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Firing properties of hippocampal neurons in a visually symmetrical environment: contributions of multiple sensory cues and mnemonic processes</article-title><source>The Journal of Neuroscience</source><volume>10</volume><fpage>3093</fpage><lpage>3105</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.10-09-03093.1990</pub-id><pub-id pub-id-type="pmid">2398374</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheintuch</surname> <given-names>L</given-names></name><name><surname>Rubin</surname> <given-names>A</given-names></name><name><surname>Brande-Eilat</surname> <given-names>N</given-names></name><name><surname>Geva</surname> <given-names>N</given-names></name><name><surname>Sadeh</surname> <given-names>N</given-names></name><name><surname>Pinchasof</surname> <given-names>O</given-names></name><name><surname>Ziv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Tracking the same neurons across multiple days in Ca<sup>2+</sup> Imaging Data</article-title><source>Cell Reports</source><volume>21</volume><fpage>1102</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2017.10.013</pub-id><pub-id pub-id-type="pmid">29069591</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skaggs</surname> <given-names>WE</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Spatial firing properties of hippocampal CA1 populations in an environment containing two visually identical regions</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>8455</fpage><lpage>8466</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-20-08455.1998</pub-id><pub-id pub-id-type="pmid">9763488</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>DM</given-names></name><name><surname>Mizumori</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2006">2006a</year><article-title>Hippocampal place cells, context, and episodic memory</article-title><source>Hippocampus</source><volume>16</volume><fpage>716</fpage><lpage>729</lpage><pub-id pub-id-type="doi">10.1002/hipo.20208</pub-id><pub-id pub-id-type="pmid">16897724</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>DM</given-names></name><name><surname>Mizumori</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2006">2006b</year><article-title>Learning-related development of context-specific neuronal responses to places and events: the hippocampal role in context processing</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>3154</fpage><lpage>3163</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3234-05.2006</pub-id><pub-id pub-id-type="pmid">16554466</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spiers</surname> <given-names>HJ</given-names></name><name><surname>Hayman</surname> <given-names>RM</given-names></name><name><surname>Jovalekic</surname> <given-names>A</given-names></name><name><surname>Marozzi</surname> <given-names>E</given-names></name><name><surname>Jeffery</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Place field repetition and purely local remapping in a multicompartment environment</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>10</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht198</pub-id><pub-id pub-id-type="pmid">23945240</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname> <given-names>KL</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Hippocampus as a predictive map</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1038/nn.4650</pub-id><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname> <given-names>LT</given-names></name><name><surname>Best</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Long-term stability of the place-field activity of single units recorded from the dorsal Hippocampus of freely behaving rats</article-title><source>Brain Research</source><volume>509</volume><fpage>299</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(90)90555-P</pub-id><pub-id pub-id-type="pmid">2322825</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>Cognitive maps in rats and men</article-title><source>Psychological Review</source><volume>55</volume><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1037/h0061626</pub-id><pub-id pub-id-type="pmid">18870876</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wills</surname> <given-names>TJ</given-names></name><name><surname>Lever</surname> <given-names>C</given-names></name><name><surname>Cacucci</surname> <given-names>F</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Attractor dynamics in the hippocampal representation of the local environment</article-title><source>Science</source><volume>308</volume><fpage>873</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1126/science.1108905</pub-id><pub-id pub-id-type="pmid">15879220</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname> <given-names>ER</given-names></name><name><surname>Dudchenko</surname> <given-names>PA</given-names></name><name><surname>Robitsek</surname> <given-names>RJ</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Hippocampal Neurons Encode Information about Different Types of Memory Episodes Occurring in the Same Location</article-title><source>Neuron</source><volume>27</volume><fpage>623</fpage><lpage>633</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)00071-4</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname> <given-names>AJ</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Uncertainty, Neuromodulation, and Attention</article-title><source>Neuron</source><volume>46</volume><fpage>681</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.04.026</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziv</surname> <given-names>Y</given-names></name><name><surname>Burns</surname> <given-names>LD</given-names></name><name><surname>Cocker</surname> <given-names>ED</given-names></name><name><surname>Hamel</surname> <given-names>EO</given-names></name><name><surname>Ghosh</surname> <given-names>KK</given-names></name><name><surname>Kitch</surname> <given-names>LJ</given-names></name><name><surname>Gamal</surname> <given-names>AE</given-names></name><name><surname>Schnitzer</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Long-term dynamics of CA1 hippocampal place codes</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>264</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nn.3329</pub-id></element-citation></ref></ref-list></back></article>