<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">47001</article-id><article-id pub-id-type="doi">10.7554/eLife.47001</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Shared neural underpinnings of multisensory integration and trial-by-trial perceptual recalibration in humans</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-135205"><name><surname>Park</surname><given-names>Hame</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2191-2055</contrib-id><email>hame.park@uni-bielefeld.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-5124"><name><surname>Kayser</surname><given-names>Christoph</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7362-5704</contrib-id><email>christoph.kayser@uni-bielefeld.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department for Cognitive Neuroscience, Faculty of Biology</institution><institution>Bielefeld University</institution><addr-line><named-content content-type="city">Bielefeld</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Center of Excellence Cognitive Interaction Technology</institution><institution>Bielefeld University</institution><addr-line><named-content content-type="city">Bielefeld</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Institute of Neuroscience and Psychology</institution><institution>University of Glasgow</institution><addr-line><named-content content-type="city">Glasgow</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Maddox</surname><given-names>Ross K</given-names></name><role>Reviewing Editor</role><aff><institution>University of Rochester</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>27</day><month>06</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e47001</elocation-id><history><date date-type="received" iso-8601-date="2019-03-19"><day>19</day><month>03</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-06-26"><day>26</day><month>06</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Park and Kayser</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Park and Kayser</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-47001-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.47001.001</object-id><p>Perception adapts to mismatching multisensory information, both when different cues appear simultaneously and when they appear sequentially. While both multisensory integration and adaptive trial-by-trial recalibration are central for behavior, it remains unknown whether they are mechanistically linked and arise from a common neural substrate. To relate the neural underpinnings of sensory integration and recalibration, we measured whole-brain magnetoencephalography while human participants performed an audio-visual ventriloquist task. Using single-trial multivariate analysis, we localized the perceptually-relevant encoding of multisensory information within and between trials. While we found neural signatures of multisensory integration within temporal and parietal regions, only medial superior parietal activity encoded past and current sensory information and mediated the perceptual recalibration within and between trials. These results highlight a common neural substrate of sensory integration and perceptual recalibration, and reveal a role of medial parietal regions in linking present and previous multisensory evidence to guide adaptive behavior.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.47001.002</object-id><title>eLife digest</title><p>A good ventriloquist will make their audience experience an illusion. The speech the spectators hear appears to come from the mouth of the puppet and not from the puppeteer. Moviegoers experience the same illusion: they perceive dialogue as coming from the mouths of the actors on screen, rather than from the loudspeakers mounted on the walls. Known as the ventriloquist effect, this ‘trick’ exists because the brain assumes that sights and sounds which occur at the same time have the same origin, and it therefore combines the two sets of sensory stimuli.</p><p>A version of the ventriloquist effect can be induced in the laboratory. Participants hear a sound while watching a simple visual stimulus (for instance, a circle) appear on a screen. When asked to pinpoint the origin of the noise, volunteers choose a location shifted towards the circle, even if this was not where the sound came from. In addition, this error persists when the visual stimulus is no longer present: if a standard trial is followed by a trial that features a sound but no circle, participants perceive the sound in the second test as ‘drawn’ towards the direction of the former shift. This is known as the ventriloquist aftereffect.</p><p>By scanning the brains of healthy volunteers performing this task, Park and Kayser show that a number of brain areas contribute to the ventriloquist effect. All of these regions help to combine what we see with what we hear, but only one maintains representations of the combined sensory inputs over time. Called the medial superior parietal cortex, this area is unique in contributing to both the ventriloquist effect and its aftereffect.</p><p>We must constantly use past and current sensory information to adapt our behavior to the environment. The results by Park and Kayser shed light on the brain structures that underpin our capacity to combine information from several senses, as well as our ability to encode memories. Such knowledge should be useful to explore how we can make flexible decisions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>multisensory integration</kwd><kwd>sensory recalibration</kwd><kwd>ventriloquist effect</kwd><kwd>ventriloquist after-effect</kwd><kwd>precuneus</kwd><kwd>sound localization</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>ERC-2014-CoG No 646657</award-id><principal-award-recipient><name><surname>Kayser</surname><given-names>Christoph</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Facing discrepancies in the sensory environment, multisensory information is combined in the medial superior parietal cortex to guide immediate judgements and to also adjust subsequent unisensory perception.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Multisensory information offers substantial benefits for behavior. For example, acoustic and visual cues can be combined to derive a more reliable estimate of where an object is located (<xref ref-type="bibr" rid="bib2">Alais and Burr, 2004</xref>; <xref ref-type="bibr" rid="bib28">Ernst and Banks, 2002</xref>; <xref ref-type="bibr" rid="bib46">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="bib86">Wozny and Shams, 2011b</xref>). Yet, the process of multisensory perception does not end once an object is removed. In fact, multisensory information can be exploited to calibrate subsequent perception in the absence of external feedback (<xref ref-type="bibr" rid="bib31">Frissen et al., 2012</xref>; <xref ref-type="bibr" rid="bib85">Wozny and Shams, 2011a</xref>). In a ventriloquist paradigm, for example, the sight of the puppet and the actor’s voice are combined when localizing the speech source, and both cues influence the localization of subsequent unisensory acoustic cues, if probed experimentally (<xref ref-type="bibr" rid="bib14">Bosen et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Bosen et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Bruns and Röder, 2015</xref>; <xref ref-type="bibr" rid="bib20">Bruns and Röder, 2017</xref>; <xref ref-type="bibr" rid="bib21">Callan et al., 2015</xref>; <xref ref-type="bibr" rid="bib62">Radeau and Bertelson, 1974</xref>; <xref ref-type="bibr" rid="bib65">Recanzone, 1998</xref>). This trial-by-trial recalibration of perception by previous multisensory information has been demonstrated for spatial cues, temporal cues, and speech signals (<xref ref-type="bibr" rid="bib43">Kilian-Hütten et al., 2011a</xref>; <xref ref-type="bibr" rid="bib49">Lüttke et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Lüttke et al., 2018</xref>; <xref ref-type="bibr" rid="bib80">Van der Burg et al., 2013</xref>), and has been shown to be modulated by attention (<xref ref-type="bibr" rid="bib27">Eramudugolla et al., 2011</xref>). Despite the importance of both facets of multisensory perception for adaptive behavior - the combination of information within a trial and the trial-by-trial adjustment of perception - it remains unclear whether they originate from shared neural mechanisms.</p><p>In fact, the neural underpinnings of trial-by-trial recalibration remain largely unclear. Those studies that have investigated neural correlates of multisensory recalibration mostly focused on the adaptation following long-term (that is, often minutes of) exposure to consistent multisensory discrepancies (<xref ref-type="bibr" rid="bib17">Bruns et al., 2011</xref>; <xref ref-type="bibr" rid="bib87">Zierul et al., 2017</xref>). However, we interact with our environment using sequences of actions dealing with different stimuli, and thus systematic sensory discrepancies as required for long-term effects are possibly seldom encountered. Hence, while the behavioral patterns of multisensory trial-by-trial recalibration are frequently studied (<xref ref-type="bibr" rid="bib14">Bosen et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Bruns and Röder, 2015</xref>; <xref ref-type="bibr" rid="bib26">Delong et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">Van der Burg et al., 2018</xref>; <xref ref-type="bibr" rid="bib85">Wozny and Shams, 2011a</xref>) it remains unclear when and where during sensory processing their neural underpinnings emerge.</p><p>In contrast to this, the neural underpinnings of multisensory integration of simultaneously received information have been investigated in many paradigms and model systems (<xref ref-type="bibr" rid="bib4">Angelaki et al., 2009</xref>; <xref ref-type="bibr" rid="bib9">Bizley et al., 2016</xref>; <xref ref-type="bibr" rid="bib29">Fetsch et al., 2013</xref>). Studies on spatial ventriloquist-like paradigms, for example, demonstrate contributions from auditory and parietal cortex (<xref ref-type="bibr" rid="bib13">Bonath et al., 2014</xref>; <xref ref-type="bibr" rid="bib18">Bruns and Röder, 2010</xref>; <xref ref-type="bibr" rid="bib19">Bruns and Röder, 2015</xref>; <xref ref-type="bibr" rid="bib21">Callan et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Harvey et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Bonath et al., 2007</xref>; <xref ref-type="bibr" rid="bib76">Starke et al., 2017</xref>). A series of recent studies demonstrates that posterior parietal regions automatically fuse multisensory information, while anterior parietal regions give way to a more flexible spatial representation that follows predictions from Bayesian causal inference (<xref ref-type="bibr" rid="bib22">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Rohe et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Rohe and Noppeney, 2015b</xref>; <xref ref-type="bibr" rid="bib70">Rohe and Noppeney, 2016</xref>). Given that parietal regions also contribute to the maintenance of sensory information within or between trials (<xref ref-type="bibr" rid="bib38">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib54">Morcos and Harvey, 2016</xref>; <xref ref-type="bibr" rid="bib63">Raposo et al., 2014</xref>; <xref ref-type="bibr" rid="bib73">Schott et al., 2018</xref>; <xref ref-type="bibr" rid="bib79">Uncapher and Wagner, 2009</xref>; <xref ref-type="bibr" rid="bib82">Vilberg and Rugg, 2008</xref>) this raises the possibility that parietal regions are in fact mediating both the combination of sensory information within a trial, and the influence of such an integrated representation on guiding subsequent adaptive behavior.</p><p>To link the neural mechanisms underlying multisensory integration and trial-by-trial recalibration, we measured whole-brain activity using magnetoencephalography (MEG) while human participants performed a spatial localization task (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The paradigm was designed to reveal the behavioral correlates of audio-visual integration (i.e. the ventriloquist effect, VE) and the influence of this on the localization of a subsequent unisensory sound (the ventriloquist aftereffect, VAE) (<xref ref-type="bibr" rid="bib85">Wozny and Shams, 2011a</xref>). Using single-trial classification we determined the relevant neural representations of auditory and visual spatial information and quantified when and where these are influenced by previous sensory evidence. We then modeled the influence of these candidate neural representations on the participant-specific trial-by-trial response biases. As expected based on previous work, our results reveal neural correlates of sensory integration in superior temporal and parietal regions. Importantly, of these, only activity within the superior parietal cortex encodes current multisensory information and retains information from preceding trials, and uses both to guide adaptive behavior within and across trials.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.47001.003</object-id><label>Figure 1.</label><caption><title>Paradigm and behavioral results (N = 24).</title><p>(<bold>A</bold>) Experimental design. Participants localized auditory (or visual) targets and indicated the perceived location using a mouse cursor. Audio-visual (AV) and auditory (A) trials alternated. (<bold>B</bold>) Response bias induced by the ventriloquist effect (VE) as a function of audio-visual discrepancy in the AV trial. VE: the difference between the reported location (R<sub>AV</sub>) and the location at which the sound (A<sub>AV</sub>) was actually presented (R<sub>AV</sub> - A<sub>AV</sub>). (<bold>C</bold>) Sound localization response in the A trial was significantly influenced by the current sound (A<sub>A</sub>; black), the previous sound (A<sub>AV</sub>; blue) and the previous visual (V<sub>AV</sub>; red) stimulus. (<bold>D</bold>) Response bias induced by the ventriloquist effect (VAE) as a function of audio-visual discrepancy in the AV trial. VAE: the difference between the reported location (R<sub>A</sub>) minus the mean reported location for all trials of the same stimulus position (R<sub>A</sub> – mean(R<sub>A</sub>)). (<bold>E</bold>) Example trials dissociating a pure visual bias from a genuine multisensory bias in the VAE. Trials for which the expected visual (v.bias) and multisensory (recal) biases are in opposite directions were selected; these satisfied either case 1: V<sub>AV</sub> - A<sub>AV</sub> &lt; 0 and A<sub>A</sub> ≤V<sub>AV</sub> or case 2; V<sub>AV</sub> - A<sub>AV</sub> &gt; 0 and A<sub>A</sub> ≥V<sub>AV</sub>. (<bold>F</bold>) Recalibration bias for trials from (<bold>E</bold>). Solid lines indicate mean across participants. Shaded area is the estimated 95% confidence interval based on the bootstrap hybrid method. Dots denote individual participants. Asterisks denote p-values&lt;0.05 from two-sided Wilcoxon signed rank tests, corrected with the Holm method for multiple comparisons, A<sub>A</sub>: sound location in A trial. A<sub>AV</sub>: sound location in AV trial. V<sub>AV</sub>: visual location in AV trial. Deposited data: Data_behav (folder).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47001-fig1-v2.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><p>24 volunteering participants localized sounds in alternating sequences of audio-visual (AV) and auditory (A) trials (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). During audio-visual trials spatially localized sounds were accompanied by visual stimuli at the same or a different location. Importantly, the AV trial always preceded the A trial. Within and between trials, the positons of auditory and visual stimuli were sampled randomly and semi-independently from five locations (<italic>i</italic> = −17°, −8.5°, 0°, 8.5°, 17° from the midline (0°); see Materials and methods for additional details). Participants fixated a central fixation dot before, during, and after the stimuli, but were free to move their eyes during the response period.</p><sec id="s2-1"><title>Behavioral results - Ventriloquist effect</title><p>Behavioral responses in AV trials revealed a clear ventriloquist effect (VE) as a function of the presented audio-visual discrepancy (ΔVA = V<sub>AV </sub> - A<sub>AV</sub>), whereby the visual stimulus biased the perceived sound location (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The VE was computed as the difference between participant’s response (R) and the actual sound location for that trial (i.e., R<sub>AV</sub> – A<sub>AV</sub>, where subscript denotes the trial type). Model comparison revealed that both stimuli had a significant influence on the participants’ responses (relative BIC values of three candidate models, c.f. Materials and methods Section: mi<sub>1</sub>: 938, mi<sub>2</sub>: 3816, mi<sub>3</sub>: 0; relative AIC values; mi<sub>1</sub>: 945, mi<sub>2</sub>: 3823, mi<sub>3</sub>: 0; protected exceedance probability (<xref ref-type="bibr" rid="bib66">Rigoux et al., 2014</xref>); mi<sub>1</sub>: 0, mi<sub>2</sub>: 0, mi<sub>3</sub>: 1; winning model: mi<sub>3</sub>: VE ~ 1 + β⋅A<sub>AV</sub> + β⋅V<sub>AV</sub>), with significant contributions from both the auditory (A<sub>AV</sub>), and visual stimuli (V<sub>AV</sub>) (β<sub>A_AV</sub> = -0.48, β<sub>V_AV</sub> = 0.22, t<sub>A_AV</sub> = -70.0, t<sub>V_AV</sub> = 31.7, p<sub>A_AV</sub>, p<sub>V_AV</sub> &lt;0.01, d.f. = 8064). Across participants, the VE bias was significant for each non-zero audio-visual discrepancy (all p&lt;10<sup>−4</sup>; Wilcoxon signed rank tests, corrected for multiple tests with the Holm procedure).</p></sec><sec id="s2-2"><title>Behavioral results - Ventriloquist aftereffect</title><p>Participants localized the sound in the A trials reliably (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, black graph), with the data exhibiting a well-known central bias (<xref ref-type="bibr" rid="bib68">Rohe and Noppeney, 2015a</xref>). This confirms that the convolution with HRTFs indeed led to sounds that were perceived as spatially dispersed. Behavioral responses in A trials revealed a significant ventriloquist aftereffect (VAE; <xref ref-type="fig" rid="fig1">Figure 1D</xref>) as a function of the audio-visual discrepancy (ΔVA) in the previous AV trial, demonstrating that the preceding multisensory stimuli had a lasting influence on the localization of subsequent sounds. The VAE for each sound location was computed as R<sub>A</sub> – mean(R<sub>A</sub>); whereby mean(R<sub>A</sub>) reflects the mean over all localization responses for this position and participant. This approach ensures that any bias in pure auditory localization does not confound the VAE effect (<xref ref-type="bibr" rid="bib68">Rohe and Noppeney, 2015a</xref>; <xref ref-type="bibr" rid="bib85">Wozny and Shams, 2011a</xref>). Model comparison revealed that both previous stimuli had a significant influence on the VAE (relative BIC values of three candidate models, c.f. Materials and methods section; mr<sub>1</sub>: 27, mr<sub>2</sub>: 357, mr<sub>3</sub>: 0; relative AIC values: mr<sub>1</sub>: 34, mr<sub>2</sub>: 364, mr<sub>3</sub>: 0; protected exceedance probability; mr<sub>1</sub>: 0, mr<sub>2</sub>: 0, mr<sub>3</sub>: 1; winning model: mr<sub>3</sub>: VAE ~ 1 + β⋅A<sub>AV</sub> + β⋅V<sub>AV</sub>), with significant contributions from the previous sound (A<sub>AV</sub>), and the previous visual stimulus (V<sub>AV</sub>) (β<sub>A_AV</sub> = -0.09, β<sub>V_AV</sub> = 0.03, t<sub>A_AV</sub> = -19.4, t<sub>V_AV</sub> = 6.0, p<sub>A_AV</sub>, p<sub>V_AV</sub> &lt;0.01, d.f. = 8064). Note that because the VAE was defined relative to the average perceived location for each sound position, the actual sound position (A<sub>A</sub>) does not contribute to the VAE. Across participants, the VAE bias was significant for each non-zero audio-visual discrepancy (all p&lt;10<sup>−2</sup>; two-sided Wilcoxon signed rank tests, corrected for multiple tests with the Holm procedure).</p><p>We performed two control analyses to further elucidate the nature of the VAE. First, we asked whether the shift in the perceived sound location was the result of a bias towards the previous visual stimulus location, or a bias induced specifically by the previous audio-visual discrepancy (<xref ref-type="bibr" rid="bib85">Wozny and Shams, 2011a</xref>). To dissect these hypotheses, we selected trials for which the expected biases arise from the direction of the VE, and not from a visual bias towards V<sub>AV</sub> (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). The data were clearly in favor of a genuine multisensory bias, as the VAE remained significant for these trials (all p&lt;0.05; except for +25.5 condition; two-sided Wilcoxon signed rank tests, corrected for multiple tests; <xref ref-type="fig" rid="fig1">Figure 1F</xref>) (<xref ref-type="bibr" rid="bib85">Wozny and Shams, 2011a</xref>). Second, we asked whether the response bias in the A trial was better accounted for by the sensory information in the previous trial (i.e. the previous multisensory discrepancy: ΔVA) or the participant’s response in that trial (R<sub>AV</sub>). Formal model comparison revealed that the model R<sub>A</sub> ~1 + A<sub>A</sub> + ΔVA provided a better account of the data than a response-based model R<sub>A</sub> ~1 + A<sub>A </sub>+ R<sub>AV</sub> (relative BIC: 0, 393; BIC weights: 1, 0), supporting the notion that recalibration is linked more to the physical stimuli than the participants response (<xref ref-type="bibr" rid="bib81">Van der Burg et al., 2018</xref>).</p></sec><sec id="s2-3"><title>Representations of single trial sensory information in MEG source data</title><p>The analysis of the MEG data was designed to elucidate the neural underpinnings of the VAE and to contrast these to the neural correlates of the VE. Specifically, we first determined neural representations of the task-relevant sensory information, or of the upcoming participant’s response. We then used these representations in a neuro-behavioral analysis to probe which neural representations of acoustic or visual spatial information are directly predictive of the participant-specific VE and VAE single trial biases.</p><p>We applied linear discriminant analysis to the time-resolved MEG source data to determine neural representations of the spatial lateralization of the auditory and visual stimuli (<xref ref-type="fig" rid="fig2">Figure 2</xref>). From the MEG activity during the A trials, we obtained significant classification (cluster-based permutation test, correcting for multiple comparisons, for details refer to Materials and methods - Statistical Analysis) performance for the current sound (A<sub>A</sub>; peaking at 80 ms in the left inferior parietal and at 160 ms in the middle temporal gyrus) and for the location of the sound in the previous trial (A<sub>AV</sub>; peaking around 120 ms in the left middle occipital lobe and the bilateral precuneus; at p≤0.01 FWE corrected for multiple comparisons in source space). This characterizes neural representations of acoustic spatial information currently received and persisting from the previous trial in a wider network of temporal and parietal brain regions. Classification of the lateralization of previous visual stimuli (V<sub>AV</sub>) was not significant at the whole brain level in the activity of the A trial, suggesting that persistent visual information was weaker than that of the acoustic information. However, the whole brain classification maps revealed meaningful clusters in early left inferior temporal areas and the right inferior/superior parietal areas. Classification of the upcoming response (R<sub>A</sub>) was significant with a similar pattern as observed for the current sound (A<sub>A</sub>) in the A trial.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.47001.004</object-id><label>Figure 2.</label><caption><title>Neural representation of current and previous sensory information and upcoming responses.</title><p>The figure shows the performance (AUC) of linear discriminants for different variables of interest. (<bold>A</bold>) Time-course of discriminant performance for all grid points in source space. (<bold>B</bold>) Time-course of the 95th percentile across source locations. (<bold>C</bold>) Surface projections of significant (p≤0.01; FWE corrected across multiple tests using cluster-based permutation) performance at the peak times extracted from panel B (open circles). The performance of LDA V<sub>AV</sub> was not significant when tested across all source locations, and the maps for V<sub>AV</sub> are not masked with significance. A<sub>A</sub>: sound location in A trial. A<sub>AV</sub>: sound location in AV trial. V<sub>AV</sub>: visual location in AV trial. Deposited data: Atrial_LDA_AUC.mat.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47001-fig2-v2.tif"/></fig></sec><sec id="s2-4"><title>Neural correlates of the VAE</title><p>To reveal the neural correlates of the VAE we investigated three regression models capturing different aspects of how current and previous sensory information shape i) the neural encoding of current sensory information in the A trial (i.e. A<sub>A</sub>), ii) the encoding of the upcoming response (R<sub>A</sub>), and iii) how neural representations of previous sensory information contribute to the single trial VAE bias.</p><p>The first model tested how the previous stimuli affect the encoding of the current sound, that is, how the encoding of sound A<sub>A</sub> in the MEG activity of the A trial was affected by the previous stimulus positions A<sub>AV</sub> and V<sub>AV</sub> (<xref ref-type="fig" rid="fig3">Figure 3A</xref>; <xref ref-type="table" rid="table1">Table 1</xref>). There was a significant (cluster-based permutation test FWE corrected at p≤0.05) influence of A<sub>AV</sub>, starting around 80 ms in the cingulum, precuneus, shifting towards inferior/superior parietal areas around 220 ms. There was also significant influence of V<sub>AV</sub> in the left occipital/parietal areas around 160 ms. Importantly, the significant effects from the previous acoustic and visual stimuli overlapped in the left parietal areas (<xref ref-type="fig" rid="fig3">Figure 3A</xref>; red inset). The second model revealed that the previous stimuli also influenced neural activity discriminative of the participants’ response (R<sub>A</sub>; <xref ref-type="fig" rid="fig3">Figure 3B</xref>; <xref ref-type="table" rid="table1">Table 1</xref>). In particular, both previous sound and visual stimulus influenced the activity predictive of the current response around 80 ms in the right parietal cortex (precuneus in particular), with the effect of A<sub>AV</sub> including also frontal and temporal regions. The significant effects of A<sub>AV</sub> and V<sub>AV</sub> overlapped in the cingulum and precuneus (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; red inset). These results demonstrate that parietal regions represent information about previous multisensory stimuli, and this information affects the neural encoding of the currently perceived sound.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.47001.005</object-id><label>Figure 3.</label><caption><title>Neural correlates of trial-by-trial recalibration (VAE bias).</title><p>(<bold>A</bold>) Contribution of previous stimuli to the neural representation of the sound (A<sub>A</sub>) in the A trial (here the effect for A<sub>A</sub> itself is not shown). (<bold>B</bold>) Contribution of current and previous stimuli to the neural representation of the response (R<sub>A</sub>) in the A trial. (<bold>C</bold>) Ventriloquist–aftereffect in the A trial predicted by the neural representation of information about previous stimuli. Red insets: Grid points with overlapping significant effects for both A<sub>AV</sub> and V<sub>AV</sub> (<bold>A, B</bold>), and for both LDA<sub>A_AV</sub> and LDA<sub>V_AV</sub> (<bold>C</bold>) across time. Surface projections were obtained from whole-brain statistical maps (at p≤0.05, FWE corrected). See <xref ref-type="table" rid="table1">Table 1</xref> for detailed coordinates and statistical results. A<sub>A</sub>: sound location in A trial. A<sub>AV</sub>: sound location in AV trial. V<sub>AV</sub>: visual location in AV trial. Deposited data: Atrial_LDA_AUC.mat; Atrial_LDA_beta.mat; VAE_beta.mat.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47001-fig3-v2.tif"/></fig><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.47001.006</object-id><label>Table 1.</label><caption><title>Neuro-behavioral modeling of the VAE.</title><p>The significance of each predictor was tested at selected time points at the whole-brain level (p≤0.05, FWE corrected). The table provides the peak coordinates of significant clusters, the anatomical regions contributing to significant clusters (based on the AAL Atlas), as well as beta and cluster-based t-values (df = 23). The overlap was defined as grid points contributing to both a significant effect for A<sub>AV</sub> and V<sub>AV</sub> (at any time). The effect of A<sub>A</sub> is not indicated, as this was significant for a large part of the temporal and parietal lobe, and was not of primary interest. L: left hemisphere; R: right hemisphere. BA: Brodmann area. **sum of 2 spatially separate clusters, ***sum of 4 clusters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center">Regressor</th><th align="center">Post-stim. time (ms)</th><th align="center">Anatomical labels</th><th align="center">MNI coord. (peak) <break/>Brodmann Area</th><th align="center">β t-value <break/>(t<sub>sum</sub>)</th></tr></thead><tbody><tr><td align="center" colspan="5"><bold>LDA<sub>A_A</sub> ~ 1 + A<sub>A </sub>+ A<sub>AV</sub> + V<sub>AV</sub></bold></td></tr><tr><td align="center" rowspan="3"><bold>A<sub>AV</sub></bold></td><td align="center">80</td><td align="center">L/R: Cingulum Mid., Precuneus <break/>L: Supp. Motor Area</td><td align="center">−3,–20, 29 <break/>BA 23</td><td align="center">−5.6 <break/>(−1220)</td></tr><tr><td align="center">160</td><td align="center">R: Fusiform, Temporal Mid/Inf</td><td align="center">28,–39, −20 <break/>BA 37</td><td align="center">−3.8 <break/>(−203)</td></tr><tr><td align="center">220</td><td align="center">L/R: Postcentral <break/>L: Parietal Inf/Sup <break/>R: Precentral, Supp. Motor Area</td><td align="center">−24,–36, 77 <break/>BA 03</td><td align="center">−6.6 <break/>(−1560)</td></tr><tr><td align="center"><bold>V<sub>AV</sub></bold></td><td align="center">160</td><td align="center">L: Occipital Mid/Sup, Parietal Inf/Sup</td><td align="center">−40,–76, 37 <break/>BA 19</td><td align="center">5.3 <break/>(246)</td></tr><tr><td align="center"><bold>overlap</bold></td><td align="center">-</td><td align="center">L: Angular, Parietal Inf., Occipital Mid.</td><td align="center">−40,–62, 47 <break/>BA 39</td><td align="center">-</td></tr><tr><td align="center" colspan="5"><bold>LDA<sub>R_A</sub> ~ 1 + A<sub>A </sub>+ A<sub>AV</sub> + V<sub>AV</sub></bold></td></tr><tr><td align="center" rowspan="3"><bold>A<sub>A</sub></bold></td><td align="center">80</td><td align="center">L: Angular, Temporal Mid. <break/>L/R: Postcentral, Precuneus</td><td align="center">−40,–52, 21 <break/>BA 39</td><td align="center">17.9 <break/>(13862)</td></tr><tr><td align="center">190</td><td align="center">R: Precuneus, Lingual <break/>Temporal Sup. <break/>L: Pre/Postcentral, Precuneus</td><td align="center">8,–51, 5 <break/>BA 30</td><td align="center">9.3 <break/>(8323)</td></tr><tr><td align="center">290</td><td align="center">L: Occipital Mid/Sup. <break/>R: Temporal Mid/Sup., Parietal Sup.</td><td align="center">−24,–100, 5 <break/>BA 17</td><td align="center">6.9 <break/>(2390)</td></tr><tr><td align="center" rowspan="3"><bold>A<sub>AV</sub></bold></td><td align="center">80</td><td align="center">L: Lingual, Precuneus <break/>R: Lingual, Parietal Sup.</td><td align="center">−24,–52, 13 <break/>BA 17</td><td align="center">−5.8 <break/>(−1152)</td></tr><tr><td align="center">190</td><td align="center">R: Frontal Mid/Inf, Precentral</td><td align="center">33, 21, 21 <break/>BA 48</td><td align="center">−6.1 <break/>(−1343)</td></tr><tr><td align="center">290</td><td align="center">R: Precuneus, Cingulum Mid/Post. <break/>Parietal Inf/Sup., Postcentral</td><td align="center">16,–42, 41 <break/>BA 23</td><td align="center">−4.2 <break/>(−256)</td></tr><tr><td align="center"><bold>V<sub>AV</sub></bold></td><td align="center">80</td><td align="center">R: Fusiform, Angular, Parietal Inf. Temporal Mid.Inf, Precuneus</td><td align="center">32,–51, −3 <break/>BA 37</td><td align="center">4.3 <break/>(225)</td></tr><tr><td align="center"><bold>overlap</bold></td><td align="center">-</td><td align="center">R: Calcarine, Precuneus, Cingulum Mid.</td><td align="center">17,–67, 23 <break/>BA 18</td><td align="center">-</td></tr><tr><td align="center" colspan="5"><bold>VAE ~ 1 + LDA<sub>A_AV</sub> + LDA<sub>V_AV</sub></bold></td></tr><tr><td align="center" rowspan="2"><bold>LDA<sub>A_AV</sub></bold></td><td align="center">100</td><td align="center">L: Occipital Mid/Sup., Temporal Inf., Parietal Mid/Sup <break/>L/R: Precuneus</td><td align="center">−32,–87, 37 <break/>BA 19</td><td align="center">−7.4 <break/>(−3571)**</td></tr><tr><td align="center">240</td><td align="center">L: Precentral, Frontal Mid, Precuneus, Temporal Pole Sup</td><td align="center">−32,–20, 45 <break/>BA 03</td><td align="center">−4.7 <break/>(−413)***</td></tr><tr><td align="center"><bold>LDA<sub>V_AV</sub></bold></td><td align="center">130</td><td align="center">R: Occipital Mid/Sup, Parietal Inf/Sup, Angular</td><td align="center">24,–95, 29 <break/>BA 18</td><td align="center">4.7 <break/>(579)</td></tr><tr><td align="center"><bold>overlap (C<sub>VAE</sub>)</bold></td><td align="center">-</td><td align="center">L/R: Precuneus <break/>R: Angular</td><td align="center">−3,–65, 51 <break/>BA 07</td><td align="center">-</td></tr></tbody></table></table-wrap><p>Using the third model, we directly tested whether these neural signatures of the previous stimuli in the MEG activity during the A trial are significantly related to the participants’ single trial response bias (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; <xref ref-type="table" rid="table1">Table 1</xref>). The significant influences of the neural representations of previous acoustic and visual stimuli overlapped again in parietal cortex (angular gyrus, precuneus; <xref ref-type="fig" rid="fig3">Figure 3C</xref>; red inset). The converging evidence from these three analyses demonstrates that the same parietal regions retain information about both previously received acoustic and visual spatial information, and that single trial variations in these neural representations directly influence the participants’ bias of subsequent sound localization.</p></sec><sec id="s2-5"><title>Neural correlates of the VE</title><p>To be able to directly compare the neural correlates of the ventriloquist aftereffect to multisensory integration (i.e. the VE effect), we repeated the same analysis focusing on the MEG activity in the AV trial. As expected from the above, classification for both auditory (A<sub>AV</sub>) and visual (V<sub>AV</sub>) locations was significant in a network of temporal and occipital regions (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). To directly link the encoding of multisensory information to behavior, we again modeled the single trial VE response bias based on the representations of current acoustic and visual information (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This revealed overlapping representations of both stimuli that directly correlated with the response bias within superior parietal regions (precuneus and superior parietal lobule), and, in a separate cluster, within inferior temporal areas (<xref ref-type="fig" rid="fig4">Figure 4</xref>; <xref ref-type="table" rid="table2">Table 2</xref>).</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.47001.007</object-id><label>Figure 4.</label><caption><title>Neural correlates of audio-visual integration within a trial (VE bias).</title><p>Contribution of the representations of acoustic and visual information to the single trial bias in the AV trial. Red inset: Grid points with overlapping significant effects for both LDA<sub>A_AV</sub> and LDA<sub>V_AV</sub>. Surface projections were obtained from whole-brain statistical maps (at p≤0.05, FWE corrected). See <xref ref-type="table" rid="table2">Table 2</xref> for detailed coordinates and statistical results. A<sub>AV</sub>: sound location in AV trial. V<sub>AV</sub>: visual location in AV trial. Deposited data: AVtrial_LDA_AUC.mat; VE_beta.mat.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47001-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.47001.008</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Neural representation of sensory information in AV trials.</title><p>The figure shows the performances (AUC) of linear discriminants for variables of interest, applied to the MEG data from the AV trial. (<bold>A</bold>) Time-courses of discriminant performance for all points in source space. (<bold>B</bold>) Time-courses of the 95th percentile across source locations. (<bold>C</bold>) Surface projections of significant (p≤0.01; FWE corrected across multiple tests using cluster-based permutation) performance at the peak times extracted from panel B (open circles). Classification performance peaked around 70 ms and 150 ms for sound location (A<sub>AV</sub>), around 120 ms for visual location (V<sub>AV</sub>), and around 120 ms and 180 ms for the response (R<sub>AV</sub>). Deposited data: AVtrial_LDA_AUC.mat.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47001-fig4-figsupp1-v2.tif"/></fig></fig-group><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.47001.009</object-id><label>Table 2.</label><caption><title>Neuro-behavioral modeling of the VE.</title><p>The significance of each predictor was tested at selected time points at the whole-brain level (p≤0.05, FWE corrected). The table provides the peak coordinates of significant clusters, the anatomical regions contributed to significant clusters (based on the AAL Atlas), peak beta values and cluster-based t-values (df = 23). The overlap was defined as grid points contributing to both a significant effect for LDA<sub>A_AV</sub> and LDA<sub>V_AV</sub> (at any time). L: left hemisphere; R: right hemisphere. BA: Brodmann area. **sum of 2 spatially separate clusters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" colspan="5">VE ~ 1 + LDA<sub>A_AV</sub> + LDA<sub>V_AV</sub></th></tr><tr><th align="center">Regressor</th><th align="center">Post-stim. time (ms)</th><th align="center">Anatomical labels</th><th align="center">MNI coord. (peak) <break/>Brodmann Area</th><th align="center">β t-value <break/>(t<sub>sum</sub>)</th></tr></thead><tbody><tr><td align="center" rowspan="2"><bold>LDA<sub>A_AV</sub></bold></td><td align="center">70</td><td align="center">L: Temporal Mid/Sup., Rolandic Oper, Postcentral, Heschl</td><td align="center">−47,–19, −19 <break/>BA 20</td><td align="center">−4.1 <break/>(−392)</td></tr><tr><td align="center">160</td><td align="center">L: Parietal Inf/Sup., Precuneus, Cuneus <break/>Occipital Sup</td><td align="center">−24,–60, 69 <break/>BA 07</td><td align="center">−4.0 <break/>(−181)</td></tr><tr><td align="center"><bold>LDA<sub>V_AV</sub></bold></td><td align="center">120</td><td align="center">L/R: Occipital Mid., Calcarine <break/>R: Occipital Sup., Temporal Mid., Lingual, Cuneus</td><td align="center">24,–92, 13 <break/>BA 18</td><td align="center">9.1 <break/>(7188)**</td></tr><tr><td align="center"><bold>overlap</bold> <break/><bold>(C<sub>TEMP,</sub> C<sub>PAR</sub>)</bold></td><td align="center">-</td><td align="center">L: Temporal Mid <break/>Parietal Sup, Cuneus, Precuneus</td><td align="center">−58,–41, −6 <break/>(C<sub>TEMP</sub>, BA 21) <break/>−14,–60, 70 <break/>(C<sub>PAR</sub>, BA 05, 07)</td><td align="center">-</td></tr></tbody></table></table-wrap></sec><sec id="s2-6"><title>The same parietal regions contribute to integration within a trial and recalibration between trials</title><p>The above reveals neural representations of audio-visual information in parietal regions that either contribute to integration within a trial (VE bias) or that shape the localization of auditory information based on previous sensory information (VAE bias). Given that each effect was localized independently (as overlapping clusters in <xref ref-type="fig" rid="fig3">Figure 3C</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref>, respectively), we asked whether the same neural sources significantly contribute to both effects. To this end we subjected the above identified clusters to both neuro-behavioral models (VAE and VE; <xref ref-type="disp-formula" rid="equ4">Equations 4</xref>/<xref ref-type="disp-formula" rid="equ5">5</xref>) to assess the significance of each regressor and to compare the strength of the VAE and VE effects between clusters (<xref ref-type="table" rid="table3">Table 3</xref>).</p><table-wrap id="table3" position="float"><object-id pub-id-type="doi">10.7554/eLife.47001.010</object-id><label>Table 3.</label><caption><title>Overlapping neural substrates for integration and recalibration.</title><p>Both neuro-behavioral models, VE and VAE (<xref ref-type="disp-formula" rid="equ4">Equations 4</xref>/<xref ref-type="disp-formula" rid="equ5">5</xref>), were tested within the clusters significantly contributing to the VAE effect (from <xref ref-type="fig" rid="fig3">Figure 3C</xref>, C<sub>VAE</sub>) and the two clusters contributing to the VE effect (from <xref ref-type="fig" rid="fig4">Figure 4</xref>, C<sub>TEMP</sub>, C<sub>PAR</sub>). The table lists regression betas and group-level t-values. The expected effects (based on <xref ref-type="fig" rid="fig3">Figure 3C</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref>) are shown in normal font, the effects of interest (cross-tested) in BOLD. We directly compared the effect strengths between clusters (one-sided paired t-test, p&lt;0.05, FDR adjusted). Significant results are indicated by *. In particular, both C<sub>VAE</sub> and C<sub>PAR</sub> have significant VAE and VE effects (tcrit = 2.81, and their respective effect sizes do not differ between clusters (ns beta differences).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center">Model</th><th align="center" colspan="3">VAE ~ 1 + β*LDA<sub>A_AV</sub> + β*LDA<sub>V_AV</sub></th><th align="center" colspan="2">VE ~ 1 + β*LDA<sub>A_AV</sub> + β*LDA<sub>V_AV</sub></th></tr><tr><th align="center">Cluster</th><th align="center">C<sub>VAE</sub></th><th align="center">C<sub>TEMP</sub></th><th align="center">C<sub>PAR</sub></th><th align="center">C<sub>VAE</sub></th><th align="center">C<sub>PAR</sub></th></tr></thead><tbody><tr><td align="center"><bold>t-value</bold> <break/><bold>(β<sub>LDAA_AV</sub>)</bold></td><td align="center">−3.29 <break/>(−0.12)</td><td align="center"><bold>−2.86</bold> <break/><bold>(−0.13)<sup>ns</sup></bold></td><td align="center"><bold>−2.97</bold> <break/><bold>(−0.09)<sup>ns</sup></bold></td><td align="center"><bold>−2.50</bold> <break/><bold>(−0.30)<sup>ns</sup></bold></td><td align="center">−3.31 <break/>(−0.32)</td></tr><tr><td align="center"><bold>t-value</bold> <break/><bold>(β<sub>LDAV_AV</sub>)</bold></td><td align="center">3.17 <break/>(0.12)</td><td align="center"><bold>0.91</bold> <break/><bold>(0.03)*</bold></td><td align="center"><bold>2.23</bold> <break/><bold>(0.08)<sup>ns</sup></bold></td><td align="center"><bold>6.61</bold> <break/><bold>(3.88)<sup>ns</sup></bold></td><td align="center">6.93 <break/>(3.35)</td></tr></tbody></table></table-wrap><p>This revealed that the spatially selective activity contributing to the VE effect (C<sub>PAR</sub>, from <xref ref-type="fig" rid="fig4">Figure 4</xref>) also significantly contributes to the VAE effect. That is, the single trial variations in the encoding of auditory and visual information in this cluster also contributed significantly (at p≤0.05) to the recalibration effect. Further, the effect strength in this cluster for recalibration did not differ from that observed in the cluster directly identified as significantly contributing to the VAE bias (C<sub>VAE</sub>, at p&lt;0.05; FDR adjusted; <xref ref-type="table" rid="table3">Table 3</xref>). Vice versa, we found that the parietal sources mediating recalibration (cluster C<sub>VAE</sub>; from <xref ref-type="fig" rid="fig3">Figure 3C</xref>) also significantly contributed to sensory integration within the AV trial (<xref ref-type="table" rid="table3">Table 3</xref>). These results confirm that spatially selective activity within superior parietal regions (identified by both clusters, C<sub>PAR</sub>, and C<sub>VAE</sub>, comprising precuneus and superior parietal regions) is significantly contributing to both sensory integration and trial-by-trial recalibration.</p></sec><sec id="s2-7"><title>Hemispheric lateralization of audio-visual integration</title><p>While the cluster predictive of the recalibration effect comprised significant grid points in both hemispheres, the model predicting the VE bias in the AV trial based on brain activity was significant only within the left hemisphere (clusters C<sub>TEMP</sub> and C<sub>PAR</sub>). We performed an additional analysis to directly test whether this effect is indeed lateralized in a statistical sense, that is whether the underlying effect is significantly greater in the left vs. the right hemisphere. First, we compared the ability to discriminate stimulus locations (AUC values) between the actual cluster and the corresponding grid points in the opposite hemisphere: there was no significant difference for either cluster for discriminating the auditory (A<sub>AV</sub>)(C<sub>TEMP</sub>; p=0.10, C<sub>PAR</sub>; p=0.65, FDR corrected) or visual stimulus locations (V<sub>AV</sub>)(C<sub>TEMP</sub>; p=0.24, C<sub>PAR</sub>; p=0.35, FDR corrected). Second, we compared the contributions of each cluster to the VE bias. The auditory contribution (regression beta for A<sub>AV</sub>) differed significantly between hemispheres for C<sub>TEMP</sub> (p=0.03, FDR corrected) but not for C<sub>PAR</sub> (p=0.56, FDR corrected). The visual contribution differed for neither cluster (regression beta for V<sub>AV, </sub>p=0.68, FDR corrected). Hence, the overall evidence for the neural correlates of the VE bias to be lateralized was weak, and absent for the parietal contribution.</p></sec><sec id="s2-8"><title>Parietal and temporal regions encode combined multisensory information</title><p>The results so far demonstrate that medial superior parietal activity reflects both, integration and recalibration. Given that the behavioral recalibration in the A trial was driven by the combined audio-visual information in the preceding AV trial (c.f. <xref ref-type="fig" rid="fig1">Figure 1F</xref>) this raises the question as to whether the neurally encoded information (in the MEG activity in the A trial) about the previous stimuli (from the AV trial) reflects previous unisensory information, or the behaviorally combined information. To test this, we compared the classification performance of the MEG activity in each cluster of interest (C<sub>VAE</sub>, C<sub>TEMP</sub>, C<sub>PAR</sub>) for the location of the previous sound (A<sub>AV</sub>) and the combined sensory information, as predicted by each participant’s behavioral weighting model (i.e., the VE bias predicted by mi<sub>3:</sub> β<sub>s</sub>*A<sub>AV</sub> + β<sub>s</sub>*V<sub>AV</sub>, s: participant c.f. <xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>For parietal activity in the AV trial (clusters C<sub>VAE</sub> and C<sub>PAR</sub>) discriminant performance was significantly higher for the weighted multisensory than for unisensory A<sub>AV</sub> information (two-sided paired t-test, p≤3*10<sup>−6</sup>, for both comparisons, FDR corrected; <xref ref-type="fig" rid="fig5">Figure 5</xref>), confirming that these regions indeed encode the integrated multisensory information. This difference was no longer significant when tested using the brain activity in the A trial, possibly because the overall classification performance was lower for previous than for current stimuli. In contrast, temporal activity (C<sub>TEMP</sub>) was equally sensitive to unisensory and combined multisensory information in both trials, in line with temporal regions participating both in sensory integration and unisensory processing (<xref ref-type="bibr" rid="bib7">Beauchamp et al., 2004</xref>).</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.47001.011</object-id><label>Figure 5.</label><caption><title>Classification performance for unisensory and combined multisensory information.</title><p>The bar graphs show the classification performance for each cluster of interest (C<sub>VAE</sub> from <xref ref-type="fig" rid="fig3">Figure 3C</xref>, C<sub>PAR</sub> and C<sub>TEMP</sub> from <xref ref-type="fig" rid="fig4">Figure 4</xref>) based on the activity in the AV trial or the A trial. Classification was applied to either the sound location in the AV trial (A<sub>AV</sub>), or the combined multisensory information in the AV trial (Comb), derived from the participant specific VE bias (derived from model mi<sub>3;</sub>VE ~ 1 + β⋅A<sub>AV</sub> + β⋅V<sub>AV</sub> for the behavioral data). Asterisks denote p&lt;0.01, two-sided paired t-test, FDR corrected for multiple comparisons at p≤0.05. Gray dots are individual participant values averaged within each cluster, red stars are the mean across participants, and red lines are standard errors of mean. A<sub>AV</sub>: sound location in AV trial. V<sub>AV</sub>: visual location in AV trial. Deposited data: LDA_AUC_comb.mat.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47001-fig5-v2.tif"/></fig></sec><sec id="s2-9"><title>Eye movements and the encoding of spatial information</title><p>Given potential influences of eye position on behavioral sound localization (<xref ref-type="bibr" rid="bib45">Kopco et al., 2009</xref>; <xref ref-type="bibr" rid="bib64">Razavi et al., 2007</xref>) it is important to rule out that eye movements consistently influenced the above results. Because eye tracking was technically impossible due to the close participant-to-screen distance, which was essential in order to promote audio-visual co-localization, we used the MEG data to confirm that participants indeed maintained fixation. We extracted ICA components known to reflect eye movement related artifacts based on their topographies and time courses (<xref ref-type="bibr" rid="bib40">Hipp and Siegel, 2013</xref>; <xref ref-type="bibr" rid="bib42">Keren et al., 2010</xref>) and determined the presence of potential EOG signals during the fixation-, stimulus-, and post-stimulus periods. This revealed that only 4.9 ± 1.7% (mean ± s.e.m) of trials exhibited evidence of potential eye-movements across all participants. In particular, eye-movements during the stimulus presentation period were rare (0.35 ± 0.12%, max 2.2%). Given that participants fixated the central fixation dot in both AV and A trials, this suggests that sound localization performance, or the VAE bias, are not affected by systematic differences in eye position across trials.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Adaptive behavior in multisensory environments requires the combination of relevant information received at each moment in time, and the adaptation to contextual changes over time, such as discrepancies in multisensory evidence. This multi-facetted profile of flexible multisensory behavior raises the question of whether the sensory integration at a given moment, and the use of previous multisensory information to recalibrate subsequent perception, arise from the same or distinct neural mechanisms. We here directly compared multisensory integration and trial-by-trial recalibration in an audio-visual spatial localization paradigm. Using single trial MEG analysis we determined a network of temporal and parietal brain regions that mediate behavioral sound localization. Of these regions, superior medial parietal activity represents current auditory and visual information, encodes the combined multisensory estimate as reflected in participant’s behavior, and retains information during the subsequent trial. Importantly, these parietal representations mediate both the multisensory integration within a trial and the subsequent recalibration of unisensory auditory perception, suggesting a common neural substrate for sensory integration and trial-by-trial recalibration of subsequent unisensory perception.</p><sec id="s3-1"><title>Neural signatures of previous sensory information</title><p>Despite many behavioral studies demonstrating the robustness of multisensory trial-by-trial recalibration (<xref ref-type="bibr" rid="bib14">Bosen et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Bruns and Röder, 2015</xref>; <xref ref-type="bibr" rid="bib85">Wozny and Shams, 2011a</xref>), little is known about the underlying neural substrate. Reasoning that recalibration relies on the persistence of information about previous stimuli, our study was guided by the quantification of where in the brain sensory information experienced during one trial can be recovered during the subsequent trial. This revealed persistent representations of previous acoustic information in a temporal-parietal network, suggesting that the regions known to reflect auditory spatial information within a trial also retain previous sensory evidence to mediate behavior (<xref ref-type="bibr" rid="bib5">At et al., 2011</xref>; <xref ref-type="bibr" rid="bib10">Bizley and Cohen, 2013</xref>; <xref ref-type="bibr" rid="bib47">Lewald et al., 2008</xref>; <xref ref-type="bibr" rid="bib88">Zimmer and Macaluso, 2005</xref>).</p><p>Neural representations of previous visual information were also strongest in temporal and parietal regions, although classification performance was not significant at the whole brain level. Importantly, the behavioral data clearly revealed a lasting effect of visual information on behavior. Furthermore, the neuro-behavioral analysis revealed a significant contribution to behavior of neural representations of previous visual information in the superior parietal cortex. One reason for the weaker classification performance for previous visual stimuli could be a bias towards acoustic information in the participant’s task, which was to localize the sound rather than the visual stimulus in both trials. Alternatively, it could be that the visual information is largely carried by neural activity reflecting the combined audio-visual information, and hence persists directly in form of a genuine multisensory representation. This integrated multisensory representation comprises a stronger component of acoustic over visual spatial information, as reflected by the ventriloquist bias in the present paradigm. Our data indeed support this conclusion, as parietal activity within the audio-visual trial was encoding the behaviorally combined information more than the acoustic information. Previous work has shown that parietal activity combines multisensory information flexibly depending on task-relevance and crossmodal disparity (<xref ref-type="bibr" rid="bib27">Eramudugolla et al., 2011</xref>; <xref ref-type="bibr" rid="bib70">Rohe and Noppeney, 2016</xref>; <xref ref-type="bibr" rid="bib71">Rohe and Noppeney, 2018</xref>) and the focus on reporting the sound location in our task may have led to the attenuation of the combined visual information in the subsequent trial. An interesting approach to test this could be to reverse the VE and thus task-relevance with very blurred visual stimulus (<xref ref-type="bibr" rid="bib2">Alais and Burr, 2004</xref>) and investigate if a symmetry in the VAE and its neural correlates hold. Furthermore, it is also possible, that eye movements during the response period may have contributed to reducing a persistent representation of visual information, in part as additional visual information was seen and processed during the response period (e.g. the response cursor). Future work is required to better understand the influence of task-relevance on uni- and multisensory representations and how these are maintained over time.</p></sec><sec id="s3-2"><title>Multiple facets of multisensory integration in parietal cortex</title><p>Several brain regions have been implied in the merging of simultaneous audio-visual spatial information (<xref ref-type="bibr" rid="bib6">Atilgan et al., 2018</xref>; <xref ref-type="bibr" rid="bib9">Bizley et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Sereno and Huang, 2014</xref>). Our results suggest that the perceptual bias induced by vision on sound localization (i.e. the ventriloquist effect) is mediated by the posterior middle temporal gyrus and the superior parietal cortex, with parietal regions encoding the perceptually combined multisensory information. These regions are in line with previous studies, which have pinpointed superior temporal regions, the insula and parietal-occipital areas as hubs for multisensory integration (<xref ref-type="bibr" rid="bib8">Bischoff et al., 2007</xref>; <xref ref-type="bibr" rid="bib13">Bonath et al., 2014</xref>; <xref ref-type="bibr" rid="bib21">Callan et al., 2015</xref>; <xref ref-type="bibr" rid="bib12">Bonath et al., 2007</xref>). In particular, a series of studies revealed that both temporal and parietal regions combine audio-visual information in a reliability- and task-dependent manner (<xref ref-type="bibr" rid="bib3">Aller and Noppeney, 2019</xref>; <xref ref-type="bibr" rid="bib67">Rohe et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Rohe and Noppeney, 2015b</xref>; <xref ref-type="bibr" rid="bib70">Rohe and Noppeney, 2016</xref>). However, while posterior parietal regions reflect the automatic fusion of multisensory information, more anterior parietal regions reflect an adaptive multisensory representation that follows predictions of Bayesian inference models. These anterior regions (sub-divisions IPS3 and 4 in <xref ref-type="bibr" rid="bib69">Rohe and Noppeney, 2015b</xref>) combine multisensory information when two cues seem to arise from a common origin and only partially integrate when there is a chance that the two cues arise from distinct sources, in accordance with the flexible use of discrepant multisensory information for behavior (<xref ref-type="bibr" rid="bib22">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib46">Körding et al., 2007</xref>). Noteworthy, the peak effect for the ventriloquist aftereffect found here was located at the anterior-posterior location corresponding to the border of IPS2 and IPS3 (<xref ref-type="bibr" rid="bib83">Wang et al., 2015</xref>), albeit more medial. While the significant clusters were more pronounced on the left hemisphere, a direct assessment did not provide evidence for these effects to be lateralized in a strict sense (<xref ref-type="bibr" rid="bib48">Liégeois et al., 2002</xref>). Our results hence corroborate the behavioral relevance of superior-anterior parietal representations and fit with an interpretation that these regions mediate the flexible use of multisensory information, depending on task and sensory congruency, to mediate adaptive behavior.</p><p>In contrast, very little is known about the brain regions implementing the trial-by-trial recalibration of unisensory perception by previous multisensory information. In fact, most studies have relied on prolonged adaptation to multisensory discrepancies. Hence these studies investigated long-term recalibration, which seems to be mechanistically distinct from the trial-by-trial recalibration investigated here (<xref ref-type="bibr" rid="bib14">Bosen et al., 2017</xref>; <xref ref-type="bibr" rid="bib17">Bruns et al., 2011</xref>; <xref ref-type="bibr" rid="bib18">Bruns and Röder, 2010</xref>; <xref ref-type="bibr" rid="bib12">Bonath et al., 2007</xref>; <xref ref-type="bibr" rid="bib87">Zierul et al., 2017</xref>). The study most closely resembling the present one suggested that the ventriloquist after-effect is mediated by an interaction of auditory and parietal regions (<xref ref-type="bibr" rid="bib87">Zierul et al., 2017</xref>), a network centered view also supported by work on the McGurk after-effect (<xref ref-type="bibr" rid="bib43">Kilian-Hütten et al., 2011a</xref>; <xref ref-type="bibr" rid="bib44">Kilian-Hütten et al., 2011b</xref>). Yet, no study to date has investigated the direct underpinnings of multisensory recalibration at the trial-by-trial level, or attempted to directly link the neural signature of the encoded sensory information about previous stimuli to the participant-specific perceptual bias. Our results close this gap by demonstrating the behavioral relevance of anterior medial parietal representations of previous multisensory information, which seem to reflect the flexible combination of spatial information following multisensory causal inference, and have a direct influence on participants’ perceptual bias in localizing a subsequent unisensory stimulus.</p><p>The retention of information about previous stimuli in parietal cortex directly links to animal work, which has revealed a mixed pattern of neural selectivity in parietal cortex, with individual neurons encoding both unisensory and multisensory information, reflecting the accumulation of this over time, and the transformation into perceptual choice (<xref ref-type="bibr" rid="bib63">Raposo et al., 2014</xref>). For example, parietal neurons are involved in maintaining the history of prior stimulus information, a role that is directly in line with our results (<xref ref-type="bibr" rid="bib1">Akrami et al., 2018</xref>). Also, the observation that the previously experienced stimuli shape both the neural encoding of the subsequent sound and neural correlates of the upcoming response (<xref ref-type="fig" rid="fig3">Figure 3A,B</xref>) suggests that these representations of prior evidence emerge in a neural system intermediate between pure sensory and pure motor (or choice) representations. Again, this fits with the observation of mixed neural representation in parietal neurons. Our results set the stage to directly probe the correlates of multisensory recalibration at the single neuron level, for example, to address whether integration and recalibration are mediated by the very same neural populations.</p><p>In humans, the medial superior parietal cortex pinpointed here as mediator of recalibration has been implied in maintaining spatial and episodic memory (<xref ref-type="bibr" rid="bib55">Müller et al., 2018</xref>; <xref ref-type="bibr" rid="bib60">Pollmann et al., 2003</xref>; <xref ref-type="bibr" rid="bib73">Schott et al., 2018</xref>; <xref ref-type="bibr" rid="bib79">Uncapher and Wagner, 2009</xref>; <xref ref-type="bibr" rid="bib82">Vilberg and Rugg, 2008</xref>) used for example during navigation, spatial updating or spatial search (<xref ref-type="bibr" rid="bib16">Brodt et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Pollmann et al., 2003</xref>). Our results broaden the functional scope of these parietal regions in multisensory perception, by showing that these regions are also involved in the integration of multiple simultaneous cues to guide subsequent spatial behavior. This places the medial parietal cortex at the interface of momentary sensory inference and memory, and exposes multisensory recalibration as a form of implicit episodic memory, mediating the integration of past and current information into a more holistic percept.</p></sec><sec id="s3-3"><title>Integrating multisensory information across multiple time scales</title><p>Similar to other forms of memory, the history of multisensory spatial information influences perception across a range of time scales (<xref ref-type="bibr" rid="bib14">Bosen et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Bosen et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Bruns and Röder, 2015</xref>). In particular, recalibration emerges on a trial-by-trial basis, as investigated here, and after several minutes of exposure to consistent discrepancies (<xref ref-type="bibr" rid="bib31">Frissen et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Kopco et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Mendonça et al., 2015</xref>; <xref ref-type="bibr" rid="bib62">Radeau and Bertelson, 1974</xref>; <xref ref-type="bibr" rid="bib64">Razavi et al., 2007</xref>; <xref ref-type="bibr" rid="bib65">Recanzone, 1998</xref>). Behavioral studies have suggested that the mechanisms underlying the trial-by-trial and long-term effects may be distinct (<xref ref-type="bibr" rid="bib19">Bruns and Röder, 2015</xref>; <xref ref-type="bibr" rid="bib20">Bruns and Röder, 2017</xref>). Yet, it remains possible that both are mediated by the same neural mechanisms, such as the same source of spatial memory (<xref ref-type="bibr" rid="bib55">Müller et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Schott et al., 2018</xref>). Indeed, an EEG study on multisensory long-term recalibration has reported neural correlates compatible with an origin in parietal cortex (<xref ref-type="bibr" rid="bib17">Bruns et al., 2011</xref>). The finding that medial parietal regions are involved in spatial and episodic memory and mediate trial-by-trial perceptual recalibration clearly lends itself to hypothesize that the very same regions should also contribute to long term recalibration as well.</p></sec><sec id="s3-4"><title>The role of coordinate systems in spatial perception</title><p>Eye movement patterns can affect sound localization. For example, changes in fixation affect early auditory cortex (<xref ref-type="bibr" rid="bib32">Fu et al., 2004</xref>; <xref ref-type="bibr" rid="bib84">Werner-Reiss et al., 2003</xref>), visual prism-adaptation results in changes in sound localization behavior (<xref ref-type="bibr" rid="bib89">Zwiers et al., 2003</xref>), and both prolonged peripheral fixation and the continuous use of fixation while searching for sounds can bias sound localization (<xref ref-type="bibr" rid="bib64">Razavi et al., 2007</xref>). This raises the question as to whether audio-visual recalibration emerges in head- or eye-centered coordinate systems (<xref ref-type="bibr" rid="bib45">Kopco et al., 2009</xref>). Spatial representations in the brain emerge in a number of coordinate systems (<xref ref-type="bibr" rid="bib23">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib72">Schechtman et al., 2012</xref>; <xref ref-type="bibr" rid="bib77">Town et al., 2017</xref>) raising the possibility that visual and auditory information is combined in any, or possibly multiple, coordinate systems. Indeed, one study suggested that the long-term ventriloquist after-effect arises in mixed eye-head coordinates (<xref ref-type="bibr" rid="bib45">Kopco et al., 2009</xref>). However, this study also noted that a transition from head- to mixed-spatial representations emerges slowly and only after prolonged exposure, while recalibration after a few exposure trials is best explained in head coordinates. With respect to the trial-by-trial recalibration investigated here, this suggests an origin in head-centered coordinates. The present study was not designed to disentangle different types of spatial representations, as the participants maintained fixation before, during and after the stimulus, and hence eye and head coordinates were aligned. Any, if present, systematic biases in fixations were eliminated on a trial-by-trial level by randomizing stimulus locations across trials and allowing participants to move their eyes while responding (<xref ref-type="bibr" rid="bib64">Razavi et al., 2007</xref>). This makes it unlikely that systematic patterns of eye movements would have affected our results, and calls for further work to elucidate the precise coordinate systems encoding the different forms or recalibration.</p><p>The use of virtual sound locations, rather than for example an array of speakers, may have affected the participants’ tendency to bind auditory and visual cues (<xref ref-type="bibr" rid="bib33">Fujisaki et al., 2004</xref>). While the use of HRTFs is routine in neuroimaging studies on spatial localization (<xref ref-type="bibr" rid="bib69">Rohe and Noppeney, 2015b</xref>), individual participants may perceive sounds more ‘within’ the head in contrast to these being properly externalized. While this can be a concern when determining whether audio-visual integration follows a specific (e.g. Bayes optimal) model (<xref ref-type="bibr" rid="bib52">Meijer et al., 2019</xref>), it would not affect our results, as these are concerned with relating the trial specific bias expressed in participants behavior with the underlying neural representations. Even if visual and acoustic stimuli were not perceived as fully co-localized, this may have reduced the overall ventriloquist bias, but would not affect the neuro-behavioral correlation. Indeed, the presence of both the ventriloquist bias and the trial-by-trial recalibration effect suggests that participants were able to perceive the spatially disparate sound sources, and co-localize the sound and visual stimulus when the disparity was small.</p></sec><sec id="s3-5"><title>Conclusion</title><p>Navigating an ever-changing world, the flexible use of past and current sensory information lies at the heart of adaptive behavior. Our results show that multiple regions are involved in the momentary integration of spatial information, and specifically expose the medial superior parietal cortex as a hub that maintains multiple sensory representations to flexibly interface the past with the environment to guide adaptive behavior.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th>Reagent <break/>type (species) <break/>or resource</th><th>Designation</th><th>Source or <break/>reference</th><th>Identifiers</th><th>Additional <break/>information</th></tr></thead><tbody><tr><td>Species (Human)</td><td>Participants</td><td>Volunteers recruited from adverts</td><td/><td/></tr><tr><td>Software, algorithm</td><td>MATLAB R2017A</td><td>MathWorks</td><td><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">https://www.mathworks.com/</ext-link></td><td/></tr><tr><td>Software, algorithm</td><td>Psychtoolbox-3</td><td>Brainard, 1997; Pelli, 1997</td><td><ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/">http://psychtoolbox.org/</ext-link></td><td/></tr><tr><td>Software, algorithm</td><td>SPM8</td><td>Wellcome Trust</td><td><ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/software/spm8/">http://www.fil.ion.ucl.ac.uk/spm/software/spm8/</ext-link></td><td/></tr><tr><td>Software, algorithm</td><td>FreeSurfer</td><td><xref ref-type="bibr" rid="bib30">Fischl, 2012</xref></td><td><ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/">https://surfer.nmr.mgh.harvard.edu/</ext-link></td><td/></tr><tr><td>Software, algorithm</td><td>PKU and IOA HRTF database</td><td><xref ref-type="bibr" rid="bib61">Qu et al., 2009</xref></td><td><ext-link ext-link-type="uri" xlink:href="http://www.cis.pku.edu.cn/auditory/Staff/Dr.Qu.files/Qu-HRTF-Database.html">http://www.cis.pku.edu.cn/auditory/Staff/Dr.Qu.files/Qu-HRTF-Database.html</ext-link></td><td/></tr><tr><td>Software, algorithm</td><td>Fieldtrip</td><td><xref ref-type="bibr" rid="bib57">Oostenveld et al., 2011</xref></td><td><ext-link ext-link-type="uri" xlink:href="http://www.fieldtriptoolbox.org/">http://www.fieldtriptoolbox.org/</ext-link></td><td/></tr><tr><td>Other</td><td>Behavioral data</td><td>This paper</td><td><ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.5061/dryad.t0p9c93">https://dx.doi.org/10.5061/dryad.t0p9c93</ext-link></td><td>data generated in this study</td></tr><tr><td>Other</td><td>MEG data</td><td>This paper</td><td><ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.5061/dryad.t0p9c93">https://dx.doi.org/10.5061/dryad.t0p9c93</ext-link></td><td>data generated in this study</td></tr></tbody></table></table-wrap><p>Twenty-six healthy right-handed adults participated in this study (15 females, age 24.2 ± 4.7 years). Sample size was determined based on previous studies using similar experimental protocols (<xref ref-type="bibr" rid="bib25">Clarke et al., 2015</xref>; <xref ref-type="bibr" rid="bib24">Clarke et al., 2013</xref>) and recommendation for sample sizes in empirical psychology (<xref ref-type="bibr" rid="bib75">Simmons et al., 2011</xref>). Data from two participants (both females) had to be excluded as these were incomplete due to technical problems during acquisition, hence results are reported for 24 participants. All participants submitted written informed consent, and reported normal vision and hearing, and indicated no history of neurological diseases. The study was conducted in accordance with the Declaration of Helsinki and was approved by the local ethics committee (College of Science and Engineering, University of Glasgow) (Ethics Application No: 300140078).</p><sec id="s4-1"><title>Task Design and Stimuli</title><p>The paradigm was based on an audio-visual localization task (<xref ref-type="bibr" rid="bib85">Wozny and Shams, 2011a</xref>). Trials and conditions were designed to probe both the ventriloquist effect and the ventriloquist-aftereffect. A typical sequence of trials is depicted in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. The participants’ task was to localize a sound during either Audio-Visual (AV) or Auditory (A), trials, or, on a subset of trials (~8% of total trials), to localize a visual stimulus (V trials). The locations of auditory and visual stimuli were each drawn semi-independently from five locations (−17°,–8.5°, 0°, 8.5°, 17° of visual angle from the midline), to yield nine different audio-visual discrepancies (−34°,–25.5°, −17°,–8.5°, 0°, 8.5°, 17°, 25.5°, 34°). Importantly, AV and A trials were always presented sequentially (AV trial preceding A trial) to probe the influence of audio-visual integration on subsequent unisensory perception.</p><p>Acoustic stimuli were spatially dispersed white noise bursts, created by applying head related transfer functions (HRTF) (PKU and IOA HRTF database, <xref ref-type="bibr" rid="bib61">Qu et al., 2009</xref>) to white noise (duration = 50 ms) defined at specific azimuths, elevations and distances. Here we used a distance of 50 cm and 0 elevation. The behavioral data obtained during A trials confirm that participants perceived these sounds as lateralized, Sounds were sampled at 48 kHz, delivered binaurally by an Etymotic ER-30 tube-phone at ~84.3 dB (root-mean-square value, measured with a Brüel and Kjær Type 2205 sound-level meter, A-weighted). An inverse filtering procedure was applied to compensate for the acoustic distortion introduced by plastic tubes required for the use in the MEG shield-room (<xref ref-type="bibr" rid="bib35">Giordano et al., 2018</xref>). The visual stimulus was a white Gaussian disk of 50 ms duration covering 1.5° of visual angle (at full-width half-maximum). This was back-projected onto a semi-transparent screen located 50 cm in front of the participant, via a DLP projector (Panasonic D7700). Stimulus presentation was controlled with Psychophysics toolbox (Brainard, 1997) for MATLAB (The MathWorks, Inc, Natick, MA), with ensured temporal synchronization of auditory and visual stimuli.</p><p>In total we repeated each discrepancy (within or between trials) 40 times, resulting in a total of 360 AV-A trial pairs. In addition, 70 visual trials were interleaved to maintain attention (V trials always came after A trials, thus not interrupting the AV-A pairs), resulting in a total of 790 trials (2 × 360 + 70) for each participant. Trials were pseudo-randomized, and divided into 10 blocks of ~8 mins each. Each trial started with a fixation period (uniform range 800 ms–1200 ms), followed by the stimulus (50 ms). After a random post-stimulus period (uniform range 600 ms–800 ms) the response cue emerged, and was shown as a horizontal bar along which participants could move a cursor. Participants responded by moving a trackball mouse (Current Designs Inc, Philadelphia, PA 19104 USA) with their right hand by moving the cursor to the location of the perceived stimulus and clicking the button. A letter ‘S’ was displayed on the cursor for ‘sound’, and ‘V’ for the visual trials. There was no constraint on response times. Inter-trial intervals varied randomly (uniform 1100 ms–1500 ms) and the experiment lasted about 3.5 hr including preparation and breaks. Importantly, participants were asked to maintain fixation during the entire pre-stimulus fixation period, the stimulus, and the post-stimulus period until the response cue appeared. During the response itself, they could freely move their eyes.</p></sec><sec id="s4-2"><title>Analysis of behavioral data</title><sec id="s4-2-1"><title>Ventriloquist effect (VE)</title><p>For AV trials, we defined the VE as the difference between the reported location (R<sub>AV</sub>) and the location at which the sound (A<sub>AV</sub>) was actually presented (R<sub>AV</sub> - A<sub>AV</sub>). To determine whether the response bias captured by the VE was systematically related to any of the sensory stimuli, we compared the power of different linear mixed models for predicting this responses bias as computational accounts for the VE. These models relied either on only the auditory, only the visual stimulus location, or their combination: mi<sub>1</sub>: VE ~ 1 + β·A<sub>AV</sub> +subj, mi<sub>2</sub>: VE ~ 1 + β·V<sub>AV</sub> +subj, mi<sub>3</sub>: VE ~ 1 + β·A<sub>AV</sub> + β·V<sub>AV</sub> +subj, where A<sub>AV</sub> and V<sub>AV</sub> were the main effects, and the participant ID (subj) was included as random effect. Models were fit using maximum-likelihood procedures and we calculated the relative Bayesian information criterion (BIC) (BIC - mean(BIC<sub>m</sub>), and relative Akaike information criterion (AIC) (AIC - mean(AIC<sub>m</sub>)), and the protected exceedance probability (<xref ref-type="bibr" rid="bib66">Rigoux et al., 2014</xref>) for formal model comparison.</p></sec><sec id="s4-2-2"><title>Ventriloquist-aftereffect (VAE)</title><p>The VAE was defined as the difference between the reported location (R<sub>A</sub>) minus the mean reported location for all trials of the same stimulus position (R<sub>A</sub> – mean(R<sub>A</sub>)). This was done to ensure that any overall bias in sound localization (e.g. a tendency to perceive sounds are closer to the midline than they actually are) would not influence this bias measure (<xref ref-type="bibr" rid="bib85">Wozny and Shams, 2011a</xref>). This was then expressed as a function of the audio-visual discrepancy in the previous trial ΔVA (i.e., V<sub>AV </sub>- A<sub>AV</sub>). Again we used linear mixed-effects models to compare different accounts of how the response bias depends on the stimuli: mr<sub>1</sub>: VAE ~ 1 + β·A<sub>AV</sub> +subj, mr<sub>2</sub>: VAE ~ 1 + β·V<sub>AV</sub> +subj, mr<sub>3</sub>: VAE ~ 1 + β·A<sub>AV</sub> + β·V<sub>AV</sub> +subj. We also quantified the response bias as a function of the all stimulus locations (i.e. A<sub>A</sub>, V<sub>AV</sub>, A<sub>AV</sub>), in order to determine the influence of each individual stimulus on behavior (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p></sec></sec><sec id="s4-3"><title>Magnetoencephalography (MEG) acquisition</title><p>Participants were seated in a magnetically shielded room, 50 cm in front of a screen (30.5 cm x 40.5 cm, 1024 × 768 resolution). The MEG data was recorded with a 248-magnetometer, whole-head MEG system (MAGNES 3600 WH, 4-D Neuroimaging, San Diego, CA) at a sampling rate of 1017.25 Hz. Head positions were measured at the beginning and end of each block, using five coils marking fiducial landmarks on the head of the participants, to monitor head movements. Coil positions were co-digitized with the head shape (FASTRAK, Polhemus Inc, Colchester, VT). Mean head movement across all participants for all blocks was 2.7 mm ± 0.2 mm (mean ± s.e.m.).</p></sec><sec id="s4-4"><title>Preprocessing of MEG data</title><p>MEG data were preprocessed with MATLAB (The MathWorks, Inc, Natick, MA) using the Fieldtrip toolbox (version 20171001, <xref ref-type="bibr" rid="bib57">Oostenveld et al., 2011</xref>). Each block was preprocessed individually (ft_preprocessing). Epochs of −0.6 s ~ 0.6 s (0 = stimulus onset) were extracted from the continuous data, and denoised using the MEG reference (ft_denoise_pca). Resulting data was filtered between 1 ~ 48 Hz (4-order Butterworth filter, forward and reverse), and down-sampled to 100 Hz. Known faulty channels (N = 3) were removed. Then, variance, maximum, minimum, and range of data across trials were calculated for each channel, and channels with extreme data were excluded. Outliers were defined based on interquartiles (IQR) (<xref ref-type="bibr" rid="bib78">Tukey, 1977</xref>); Q1 – 4.5 × IQR or above Q3 +4.5 × IQR. Weight 4.5 instead of the standard 1.5 was used since 1.5 was eliminating too many channels. Overall about 6% of all channels were excluded (15.1 ± 5.8 channels per participant; mean ± SD). Heart and eye-movement artifacts were removed using independent component analysis (ICA) with Fieldtrip (ft_componentanalysis, ft_rejectcomponent), which was calculated based on 30 principal components. Trials with SQUID (superconducting quantum interference device) jumps were detected and removed (ft_artifact_jump) with a cutoff z-value of 20. Finally, the data was manually inspected using the interquartile method (across channels weight 2.5) to exclude outlier trials. On average about 2% of trials had to be discarded (18.3 ± 7.7 trials per participant; mean ± SD), including very few trials on which the mouse button did not react properly).</p></sec><sec id="s4-5"><title>MEG source reconstruction</title><p>Source reconstruction was performed using Fieldtrip (<xref ref-type="bibr" rid="bib57">Oostenveld et al., 2011</xref>), SPM8 (Wellcome Trust, London, United Kingdom), and the Freesurfer toolbox (<xref ref-type="bibr" rid="bib30">Fischl, 2012</xref>). For each participant whole-brain T1-weighted structural magnetic resonance images (MRIs, 192 sagittal slices, 256 × 256 matrix size, 1 mm<sup>3</sup> voxel size) were acquired using a Siemens 3T Trio scanner (32-channel head coil). These were co-registered to the MEG coordinate system using a semi-automatic procedure. Individual MRIs were segmented and linearly normalized to a template brain (MNI space). Next, a volume conduction model was constructed using a single-shell model based on an 8 mm isotropic grid. We projected the sensor-level waveforms into source space using a linear constraint minimum variance (LCMV) beamformer with a regularization parameter of 7%. Then the data was collapsed onto the strongest dipole orientation based on singular value decomposition. Source reconstruction was performed on each block separately, and then concatenated for further analyses.</p></sec><sec id="s4-6"><title>Discriminant analysis</title><p>To extract neural signatures of the encoding of different variables of interest we applied a cross-validated regularized linear discriminant analysis (LDA) (<xref ref-type="bibr" rid="bib11">Blankertz et al., 2011</xref>; <xref ref-type="bibr" rid="bib58">Parra and Sajda, 2003</xref>) to the single trial MEG source data. LDA was applied to the data aligned to stimulus onset in 60 ms sliding windows, with 10 ms time-steps, using a spatial searchlight around each voxel consisting of the 27 neighboring voxels. For each source point <inline-formula><mml:math id="inf1"><mml:mi mathvariant="normal">υ</mml:mi></mml:math></inline-formula>, the LDA identifies a projection of the multidimensional source data, <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, that maximally discriminates between the two conditions of interest, defined by a weight vector, <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which describes a one dimensional combination of the source data, <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>27</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf5"><mml:mi>i</mml:mi></mml:math></inline-formula> summing over grid points within a spatial searchlight, and <italic>c</italic> being a constant. Classification performance was quantified using the area under the receiver operator characteristic (AUC) based on 6-fold cross validation. We identified clusters with significant classification performance at the group level by applying a cluster-based permutation procedure (see below). Having established a set of discriminant weights, <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, one can derive single trial predictions of the neurally encoded information, <inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, using equation (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). Importantly, using cross-validation one can determine the classification weights on one set of trials, and then predict the discrimination performance for a separate trials. The value, <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, of such an LDA projection can serve as a proxy to the neurally encoded signal trial information about a specific stimulus variable, and can be related for example to behavioral performance (<xref ref-type="bibr" rid="bib37">Grootswagers et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Grootswagers et al., 2017</xref>; <xref ref-type="bibr" rid="bib41">Kayser et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Philiastides et al., 2014</xref>).</p></sec><sec id="s4-7"><title>Neural en- and decoding analysis</title><p>We computed separate linear discriminant classifiers based on MEG activity in either the AV trial or the A trial, and for different to-be-classified conditions of interest. These were the location of the auditory and visual stimuli in the AV trial (A<sub>AV</sub>, V<sub>AV</sub>), the auditory stimulus in the A trial (A<sub>A</sub>), and the responses in either trial (R<sub>AV,</sub> R<sub>A</sub>). For each stimulus location or response variable, we classified whether this variable was left- or right-lateralized. That is, we reduced the five potential stimulus locations, or the continuous response, into two conditions to derive the classifier: we grouped trials on which the respective stimulus was to the left (−17°,–8.5°) or right (17°, 8.5°) of the fixation point, or grouped trials on which the response was to the left (&lt;0°) or the right (&gt;0°). The center stimuli were not used to derive the LDA classifiers. We used these classifiers in different neuro-behavioral models to elucidate neural mechanisms of the VE and VAE. We investigated models capturing potential influences of each stimulus on the neural representation of sensory information, or on the neural representation of the upcoming participant’s response. In addition, we investigated models directly capturing the neuro-behavioral relation between the encoded sensory information and the response bias.</p><p>First, we determined when and where neural signatures of the encoding of single trial information, as reflected by their LDA discriminant values (c.f. <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), were influenced by the sensory stimuli. For each searchlight and time point within a trial we determined the following models:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>which captures sensory integration within the AV trials, and,<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>which captures how the encoding of the current sound in the A trial is influenced by previous audio-visual stimuli. Second, and analogously, we investigated models for the encoding of the participant’s response (i.e. LDA<sub>R_A</sub> and LDA<sub>R_AV</sub>). It is important to note that here, the MEG activity from the stimulus- and post-stimulus period in the AV trial was used for <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, while the MEG activity from the A trial was used for <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>.</p><p>Third, we determined the contribution of the single trial representations of acoustic and visual information to the single trial behavioral response bias with the following models:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi>V</mml:mi><mml:mi>E</mml:mi><mml:mo>~</mml:mo> <mml:mi/><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>_</mml:mo><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>_</mml:mo><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mi>V</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo>~</mml:mo> <mml:mi/><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>_</mml:mo><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>_</mml:mo><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></disp-formula></p><p>In these models, the response biases VE and VAE were the continuous localization data (in visual degrees) obtained from the participant response, while the LDA components (from <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) reflect the continuous-valued prediction of the degree of lateralization of the respective stimulus extracted from the MEG activity. Importantly, each model is computed based on the MEG activity and the behavioral data in the respective trials (VE in the AV trial, VAE in the A trial. That is, the VE model (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) uses the behavioral response (VE) from the AV trial and the representation (LDA component) of the two stimuli in the MEG activity from the same trial. In contrast, the VAE model (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) uses the behavioral response (VAE) from the A trial, and the representation (LDA component) of the two stimuli presented in the previous AV trial, reflected in the MEG activity from the A trial. Hence, each model uses the neural representation of stimuli from a different trial. A predictor of the current sound (A<sub>A</sub>) was not included in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>, as the VAE bias quantifies the deviation of the behavioral response from the current sound (c.f. <xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>To avoid overfitting, we computed these models based on 6-fold cross-validation, using distinct sets of trials to determine the weights of the LDA and to compute the regression models. We then computed group-level t-values for the coefficients for each regressor at each grid and time point, and assessed their significance using cluster-based permutation statistics (below). Note that for AV trials, we excluded the AV pairs with the most extreme discrepancies (±34°), as these were inducing strong correlations between the regressors.</p></sec><sec id="s4-8"><title>Statistical analysis</title><p>To test the significance of the behavioral biases we used two-sided Wilcoxon signed rank tests, correcting for multiple tests using the Holm procedure with a family-wise error rate of p=0.05.</p><p>Group-level inference on the 3-dimensional MEG source data was obtained using randomization procedures and cluster-based statistical enhancement controlling for multiple comparisons in source space (<xref ref-type="bibr" rid="bib51">Maris and Oostenveld, 2007</xref>; <xref ref-type="bibr" rid="bib56">Nichols and Holmes, 2002</xref>). First, we shuffled the sign of the true single-subject effects (the signs of the chance-level corrected AUC values; the signs of single-subject regression beta’s) and obtained distributions of group-level effects (means or t-values) based on 2000 randomizations. We then applied spatial clustering based on a minimal cluster size of 6 and using the sum as cluster-statistics. For testing the LDA performance, we thresholded effects based on the 99th percentile of the full-brain distribution of randomized AUC values. For testing the betas in regression models, we used parametric thresholds corresponding to a two-sided p=0.01 (tcrit = 2.81, d.f. = 23; except for the analysis for the visual location in the AV trial (tcrit = 6.60; c.f. <xref ref-type="fig" rid="fig4">Figure 4</xref>). The threshold for determining significant clusters for classification performance was p≤0.01 (two-sided), that for significant neuro-behavioral effects (<xref ref-type="disp-formula" rid="equ2 equ3 equ4 equ5">Equation 2-5</xref>) p≤0.05 (two-sided). To simplify the statistical problem, we tested for significant spatial clusters at selected time points only. These time points were defined based on local peaks of the time courses of respective LDA AUC performance (for models in <xref ref-type="disp-formula" rid="equ2 equ3">Equations 2-3</xref>), and the peaks for the beta time-course for the behavioral models (<xref ref-type="disp-formula" rid="equ4 equ5">Equations 4-5</xref>). Furthermore, where possible, to test for the significance of individual regressors we applied a spatial a priori mask derived from the significance of the respective LDA AUC values to further ensure that neuro-behavioral effects originate from sources with significant encoding effects (<xref ref-type="bibr" rid="bib34">Giordano et al., 2017</xref>). Note that this was not possible for LDA<sub>V_AV</sub> for which we used the full brain to test for significant model effects.</p></sec><sec id="s4-9"><title>Data sharing</title><p>The behavioral data presented in <xref ref-type="fig" rid="fig1">Figure 1</xref> and LDA performance data and source regression data used to calculate the t-values in <xref ref-type="fig" rid="fig2">Figures 2</xref>–<xref ref-type="fig" rid="fig5">5</xref>, as well as data for <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, have been deposited to Dryad (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.t0p9c93">https://doi.org/10.5061/dryad.t0p9c93</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the European Research Council (to CK ERC-2014-CoG; grant No 646657). We would like to thank Yinan Cao and Bruno Giordano for helping with the acoustic stimuli, Bruno Giordano for helpful discussions and Gavin Paterson for support with hardware and data acquisition along with Frances Crabbe.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation, Visualization, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Validation, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants submitted written informed consent. The study was conducted in accordance with the Declaration of Helsinki and was approved by the local ethics committee. Ethics Application No: 300140078 (College of Science and Engineering, University of Glasgow).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.47001.012</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-47001-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The behavioral data presented in Figure 1 and LDA performance data and source regression data used to calculate the t-values in Figures 2-5, as well as data for Figure 4-figure supplement 1, have been deposited to Dryad (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.t0p9c93">https://doi.org/10.5061/dryad.t0p9c93</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Data from: Shared neural underpinnings of multisensory integration and trial-by-trial perceptual recalibration in humans</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.t0p9c93</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akrami</surname> <given-names>A</given-names></name><name><surname>Kopec</surname> <given-names>CD</given-names></name><name><surname>Diamond</surname> <given-names>ME</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Posterior parietal cortex represents sensory history and mediates its effects on behaviour</article-title><source>Nature</source><volume>554</volume><fpage>368</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/nature25510</pub-id><pub-id pub-id-type="pmid">29414944</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alais</surname> <given-names>D</given-names></name><name><surname>Burr</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title><source>Current Biology</source><volume>14</volume><fpage>257</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.01.029</pub-id><pub-id pub-id-type="pmid">14761661</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aller</surname> <given-names>M</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>To integrate or not to integrate: temporal dynamics of hierarchical bayesian causal inference</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000210</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000210</pub-id><pub-id pub-id-type="pmid">30939128</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angelaki</surname> <given-names>DE</given-names></name><name><surname>Gu</surname> <given-names>Y</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Multisensory integration: psychophysics, neurophysiology, and computation</article-title><source>Current Opinion in Neurobiology</source><volume>19</volume><fpage>452</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2009.06.008</pub-id><pub-id pub-id-type="pmid">19616425</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>At</surname> <given-names>A</given-names></name><name><surname>Spierer</surname> <given-names>L</given-names></name><name><surname>Clarke</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The role of the right parietal cortex in sound localization: a chronometric single pulse transcranial magnetic stimulation study</article-title><source>Neuropsychologia</source><volume>49</volume><fpage>2794</fpage><lpage>2797</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2011.05.024</pub-id><pub-id pub-id-type="pmid">21679720</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atilgan</surname> <given-names>H</given-names></name><name><surname>Town</surname> <given-names>SM</given-names></name><name><surname>Wood</surname> <given-names>KC</given-names></name><name><surname>Jones</surname> <given-names>GP</given-names></name><name><surname>Maddox</surname> <given-names>RK</given-names></name><name><surname>Lee</surname> <given-names>AKC</given-names></name><name><surname>Bizley</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Integration of visual information in auditory cortex promotes auditory scene analysis through multisensory binding</article-title><source>Neuron</source><volume>97</volume><fpage>640</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.12.034</pub-id><pub-id pub-id-type="pmid">29395914</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname> <given-names>MS</given-names></name><name><surname>Argall</surname> <given-names>BD</given-names></name><name><surname>Bodurka</surname> <given-names>J</given-names></name><name><surname>Duyn</surname> <given-names>JH</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Unraveling multisensory integration: patchy organization within human STS multisensory cortex</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>1190</fpage><lpage>1192</lpage><pub-id pub-id-type="doi">10.1038/nn1333</pub-id><pub-id pub-id-type="pmid">15475952</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bischoff</surname> <given-names>M</given-names></name><name><surname>Walter</surname> <given-names>B</given-names></name><name><surname>Blecker</surname> <given-names>CR</given-names></name><name><surname>Morgen</surname> <given-names>K</given-names></name><name><surname>Vaitl</surname> <given-names>D</given-names></name><name><surname>Sammer</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Utilizing the ventriloquism-effect to investigate audio-visual binding</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>578</fpage><lpage>586</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.03.008</pub-id><pub-id pub-id-type="pmid">16620884</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname> <given-names>JK</given-names></name><name><surname>Jones</surname> <given-names>GP</given-names></name><name><surname>Town</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Where are multisensory signals combined for perceptual decision-making?</article-title><source>Current Opinion in Neurobiology</source><volume>40</volume><fpage>31</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.06.003</pub-id><pub-id pub-id-type="pmid">27344253</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname> <given-names>JK</given-names></name><name><surname>Cohen</surname> <given-names>YE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The what, where and how of auditory-object perception</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>693</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1038/nrn3565</pub-id><pub-id pub-id-type="pmid">24052177</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blankertz</surname> <given-names>B</given-names></name><name><surname>Lemm</surname> <given-names>S</given-names></name><name><surname>Treder</surname> <given-names>M</given-names></name><name><surname>Haufe</surname> <given-names>S</given-names></name><name><surname>Müller</surname> <given-names>KR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Single-trial analysis and classification of ERP components--a tutorial</article-title><source>NeuroImage</source><volume>56</volume><fpage>814</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.048</pub-id><pub-id pub-id-type="pmid">20600976</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonath</surname> <given-names>B</given-names></name><name><surname>Noesselt</surname> <given-names>T</given-names></name><name><surname>Martinez</surname> <given-names>A</given-names></name><name><surname>Mishra</surname> <given-names>J</given-names></name><name><surname>Schwiecker</surname> <given-names>K</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural basis of the ventriloquist illusion</article-title><source>Current Biology : CB</source><volume>17</volume><fpage>1697</fpage><lpage>1703</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.08.050</pub-id><pub-id pub-id-type="pmid">17884498</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonath</surname> <given-names>B</given-names></name><name><surname>Noesselt</surname> <given-names>T</given-names></name><name><surname>Krauel</surname> <given-names>K</given-names></name><name><surname>Tyll</surname> <given-names>S</given-names></name><name><surname>Tempelmann</surname> <given-names>C</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Audio-visual synchrony modulates the ventriloquist illusion and its neural/spatial representation in the auditory cortex</article-title><source>NeuroImage</source><volume>98</volume><fpage>425</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.04.077</pub-id><pub-id pub-id-type="pmid">24814210</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosen</surname> <given-names>AK</given-names></name><name><surname>Fleming</surname> <given-names>JT</given-names></name><name><surname>Allen</surname> <given-names>PD</given-names></name><name><surname>O'Neill</surname> <given-names>WE</given-names></name><name><surname>Paige</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Accumulation and decay of visual capture and the ventriloquism aftereffect caused by brief audio-visual disparities</article-title><source>Experimental Brain Research</source><volume>235</volume><fpage>585</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1007/s00221-016-4820-4</pub-id><pub-id pub-id-type="pmid">27837258</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosen</surname> <given-names>AK</given-names></name><name><surname>Fleming</surname> <given-names>JT</given-names></name><name><surname>Allen</surname> <given-names>PD</given-names></name><name><surname>O'Neill</surname> <given-names>WE</given-names></name><name><surname>Paige</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multiple time scales of the ventriloquism aftereffect</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0200930</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0200930</pub-id><pub-id pub-id-type="pmid">30067790</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodt</surname> <given-names>S</given-names></name><name><surname>Pöhlchen</surname> <given-names>D</given-names></name><name><surname>Flanagin</surname> <given-names>VL</given-names></name><name><surname>Glasauer</surname> <given-names>S</given-names></name><name><surname>Gais</surname> <given-names>S</given-names></name><name><surname>Schönauer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rapid and independent memory formation in the parietal cortex</article-title><source>PNAS</source><volume>113</volume><fpage>13251</fpage><lpage>13256</lpage><pub-id pub-id-type="doi">10.1073/pnas.1605719113</pub-id><pub-id pub-id-type="pmid">27803331</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruns</surname> <given-names>P</given-names></name><name><surname>Liebnau</surname> <given-names>R</given-names></name><name><surname>Röder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cross-modal training induces changes in spatial representations early in the auditory processing pathway</article-title><source>Psychological Science</source><volume>22</volume><fpage>1120</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.1177/0956797611416254</pub-id><pub-id pub-id-type="pmid">21771962</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruns</surname> <given-names>P</given-names></name><name><surname>Röder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Tactile capture of auditory localization: an event-related potential study</article-title><source>European Journal of Neuroscience</source><volume>31</volume><fpage>1844</fpage><lpage>1857</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2010.07232.x</pub-id><pub-id pub-id-type="pmid">20584189</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruns</surname> <given-names>P</given-names></name><name><surname>Röder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sensory recalibration integrates information from the immediate and the cumulative past</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>12739</elocation-id><pub-id pub-id-type="doi">10.1038/srep12739</pub-id><pub-id pub-id-type="pmid">26238089</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruns</surname> <given-names>P</given-names></name><name><surname>Röder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatial and frequency specificity of the ventriloquism aftereffect revisited</article-title><source>Psychological Research</source><volume>14</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1007/s00426-017-0965-4</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callan</surname> <given-names>A</given-names></name><name><surname>Callan</surname> <given-names>D</given-names></name><name><surname>Ando</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>An fMRI study of the ventriloquism effect</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>4248</fpage><lpage>4258</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu306</pub-id><pub-id pub-id-type="pmid">25577576</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname> <given-names>Y</given-names></name><name><surname>Summerfield</surname> <given-names>C</given-names></name><name><surname>Park</surname> <given-names>H</given-names></name><name><surname>Giordano</surname> <given-names>BL</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causal inference in the multisensory brain</article-title><source>Neuron</source><volume>102</volume><fpage>1076</fpage><lpage>1087</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.03.043</pub-id><pub-id pub-id-type="pmid">31047778</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible egocentric and allocentric representations of heading signals in parietal cortex</article-title><source>PNAS</source><volume>115</volume><fpage>E3305</fpage><lpage>E3312</lpage><pub-id pub-id-type="doi">10.1073/pnas.1715625115</pub-id><pub-id pub-id-type="pmid">29555744</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname> <given-names>A</given-names></name><name><surname>Taylor</surname> <given-names>KI</given-names></name><name><surname>Devereux</surname> <given-names>B</given-names></name><name><surname>Randall</surname> <given-names>B</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>From perception to conception: how meaningful objects are processed over time</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>187</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs002</pub-id><pub-id pub-id-type="pmid">22275484</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname> <given-names>A</given-names></name><name><surname>Devereux</surname> <given-names>BJ</given-names></name><name><surname>Randall</surname> <given-names>B</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predicting the time course of individual objects with MEG</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3602</fpage><lpage>3612</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu203</pub-id><pub-id pub-id-type="pmid">25209607</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delong</surname> <given-names>P</given-names></name><name><surname>Aller</surname> <given-names>M</given-names></name><name><surname>Giani</surname> <given-names>AS</given-names></name><name><surname>Rohe</surname> <given-names>T</given-names></name><name><surname>Conrad</surname> <given-names>V</given-names></name><name><surname>Watanabe</surname> <given-names>M</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Invisible flashes alter perceived sound location</article-title><source>Scientific Reports</source><volume>8</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41598-018-30773-3</pub-id><pub-id pub-id-type="pmid">30120294</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eramudugolla</surname> <given-names>R</given-names></name><name><surname>Kamke</surname> <given-names>MR</given-names></name><name><surname>Soto-Faraco</surname> <given-names>S</given-names></name><name><surname>Mattingley</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Perceptual load influences auditory space perception in the ventriloquist aftereffect</article-title><source>Cognition</source><volume>118</volume><fpage>62</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2010.09.009</pub-id><pub-id pub-id-type="pmid">20979992</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname> <given-names>MO</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id><pub-id pub-id-type="pmid">11807554</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname> <given-names>CR</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Bridging the gap between theories of sensory cue integration and the physiology of multisensory neurons</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>429</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1038/nrn3503</pub-id><pub-id pub-id-type="pmid">23686172</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frissen</surname> <given-names>I</given-names></name><name><surname>Vroomen</surname> <given-names>J</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The aftereffects of ventriloquism: the time course of the visual recalibration of auditory localization</article-title><source>Seeing and Perceiving</source><volume>25</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1163/187847611X620883</pub-id><pub-id pub-id-type="pmid">22353565</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname> <given-names>KM</given-names></name><name><surname>Shah</surname> <given-names>AS</given-names></name><name><surname>O'Connell</surname> <given-names>MN</given-names></name><name><surname>McGinnis</surname> <given-names>T</given-names></name><name><surname>Eckholdt</surname> <given-names>H</given-names></name><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>Smiley</surname> <given-names>J</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Timing and laminar profile of Eye-Position effects on auditory responses in primate auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>92</volume><fpage>3522</fpage><lpage>3531</lpage><pub-id pub-id-type="doi">10.1152/jn.01228.2003</pub-id><pub-id pub-id-type="pmid">15282263</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujisaki</surname> <given-names>W</given-names></name><name><surname>Shimojo</surname> <given-names>S</given-names></name><name><surname>Kashino</surname> <given-names>M</given-names></name><name><surname>Nishida</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Recalibration of audiovisual simultaneity</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>773</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1038/nn1268</pub-id><pub-id pub-id-type="pmid">15195098</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giordano</surname> <given-names>BL</given-names></name><name><surname>Ince</surname> <given-names>RAA</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Schyns</surname> <given-names>PG</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Contributions of local speech encoding and functional connectivity to audio-visual speech perception</article-title><source>eLife</source><volume>6</volume><elocation-id>e24763</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.24763</pub-id><pub-id pub-id-type="pmid">28590903</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Giordano</surname> <given-names>BL</given-names></name><name><surname>Whiting</surname> <given-names>C</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Kotz</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>From categories to dimensions: spatio-temporal dynamics of the cerebral representations of emotion in voice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/265843</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname> <given-names>T</given-names></name><name><surname>Wardle</surname> <given-names>SG</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decoding dynamic brain patterns from evoked responses: a tutorial on multivariate pattern analysis applied to time series neuroimaging data</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>677</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01068</pub-id><pub-id pub-id-type="pmid">27779910</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname> <given-names>T</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Finding decodable information that can be read out in behaviour</article-title><source>NeuroImage</source><volume>179</volume><fpage>252</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.06.022</pub-id><pub-id pub-id-type="pmid">29886145</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title><source>Nature</source><volume>484</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1038/nature10918</pub-id><pub-id pub-id-type="pmid">22419153</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname> <given-names>C</given-names></name><name><surname>Van der Burg</surname> <given-names>E</given-names></name><name><surname>Alais</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rapid temporal recalibration occurs crossmodally without stimulus specificity but is absent unimodally</article-title><source>Brain Research</source><volume>1585</volume><fpage>120</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2014.08.028</pub-id><pub-id pub-id-type="pmid">25148705</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hipp</surname> <given-names>JF</given-names></name><name><surname>Siegel</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dissociating neuronal gamma-band activity from cranial and ocular muscle activity in EEG</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>338</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00338</pub-id><pub-id pub-id-type="pmid">23847508</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>SJ</given-names></name><name><surname>McNair</surname> <given-names>SW</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prestimulus influences on auditory perception from sensory representations and decision processes</article-title><source>PNAS</source><volume>113</volume><fpage>4842</fpage><lpage>4847</lpage><pub-id pub-id-type="doi">10.1073/pnas.1524087113</pub-id><pub-id pub-id-type="pmid">27071110</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keren</surname> <given-names>AS</given-names></name><name><surname>Yuval-Greenberg</surname> <given-names>S</given-names></name><name><surname>Deouell</surname> <given-names>LY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Saccadic spike potentials in gamma-band EEG: characterization, detection and suppression</article-title><source>NeuroImage</source><volume>49</volume><fpage>2248</fpage><lpage>2263</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.10.057</pub-id><pub-id pub-id-type="pmid">19874901</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kilian-Hütten</surname> <given-names>N</given-names></name><name><surname>Valente</surname> <given-names>G</given-names></name><name><surname>Vroomen</surname> <given-names>J</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Auditory cortex encodes the perceptual interpretation of ambiguous sound</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>1715</fpage><lpage>1720</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4572-10.2011</pub-id><pub-id pub-id-type="pmid">21289180</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kilian-Hütten</surname> <given-names>N</given-names></name><name><surname>Vroomen</surname> <given-names>J</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Brain activation during audiovisual exposure anticipates future perception of ambiguous speech</article-title><source>NeuroImage</source><volume>57</volume><fpage>1601</fpage><lpage>1607</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.05.043</pub-id><pub-id pub-id-type="pmid">21664279</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopco</surname> <given-names>N</given-names></name><name><surname>Lin</surname> <given-names>IF</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name><name><surname>Groh</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reference frame of the ventriloquism aftereffect</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>13809</fpage><lpage>13814</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2783-09.2009</pub-id><pub-id pub-id-type="pmid">19889992</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname> <given-names>KP</given-names></name><name><surname>Beierholm</surname> <given-names>U</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Quartz</surname> <given-names>S</given-names></name><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name><name><surname>Shams</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Causal inference in multisensory perception</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e943</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000943</pub-id><pub-id pub-id-type="pmid">17895984</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewald</surname> <given-names>J</given-names></name><name><surname>Riederer</surname> <given-names>KA</given-names></name><name><surname>Lentz</surname> <given-names>T</given-names></name><name><surname>Meister</surname> <given-names>IG</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Processing of sound location in human cortex</article-title><source>European Journal of Neuroscience</source><volume>27</volume><fpage>1261</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2008.06094.x</pub-id><pub-id pub-id-type="pmid">18364040</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liégeois</surname> <given-names>F</given-names></name><name><surname>Connelly</surname> <given-names>A</given-names></name><name><surname>Salmond</surname> <given-names>CH</given-names></name><name><surname>Gadian</surname> <given-names>DG</given-names></name><name><surname>Vargha-Khadem</surname> <given-names>F</given-names></name><name><surname>Baldeweg</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A direct test for lateralization of language activation using fMRI: comparison with invasive assessments in children with epilepsy</article-title><source>NeuroImage</source><volume>17</volume><fpage>1861</fpage><lpage>1867</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1327</pub-id><pub-id pub-id-type="pmid">12498760</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lüttke</surname> <given-names>CS</given-names></name><name><surname>Ekman</surname> <given-names>M</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>McGurk illusion recalibrates subsequent auditory perception</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>32891</elocation-id><pub-id pub-id-type="doi">10.1038/srep32891</pub-id><pub-id pub-id-type="pmid">27611960</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lüttke</surname> <given-names>CS</given-names></name><name><surname>Pérez-Bellido</surname> <given-names>A</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rapid recalibration of speech perception after experiencing the McGurk illusion</article-title><source>Royal Society Open Science</source><volume>5</volume><elocation-id>170909</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.170909</pub-id><pub-id pub-id-type="pmid">29657743</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijer</surname> <given-names>D</given-names></name><name><surname>Veselič</surname> <given-names>S</given-names></name><name><surname>Calafiore</surname> <given-names>C</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Integration of audiovisual spatial signals is not consistent with maximum likelihood estimation</article-title><source>Cortex</source><volume>119</volume><fpage>74</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2019.03.026</pub-id><pub-id pub-id-type="pmid">31082680</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mendonça</surname> <given-names>C</given-names></name><name><surname>Escher</surname> <given-names>A</given-names></name><name><surname>van de Par</surname> <given-names>S</given-names></name><name><surname>Colonius</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predicting auditory space calibration from recent multisensory experience</article-title><source>Experimental Brain Research</source><volume>233</volume><fpage>1983</fpage><lpage>1991</lpage><pub-id pub-id-type="doi">10.1007/s00221-015-4259-z</pub-id><pub-id pub-id-type="pmid">25795081</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morcos</surname> <given-names>AS</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>History-dependent variability in population dynamics during evidence accumulation in cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1672</fpage><lpage>1681</lpage><pub-id pub-id-type="doi">10.1038/nn.4403</pub-id><pub-id pub-id-type="pmid">27694990</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname> <given-names>NG</given-names></name><name><surname>Riemer</surname> <given-names>M</given-names></name><name><surname>Brandt</surname> <given-names>L</given-names></name><name><surname>Wolbers</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Repetitive transcranial magnetic stimulation reveals a causal role of the human precuneus in spatial updating</article-title><source>Scientific Reports</source><volume>8</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1038/s41598-018-28487-7</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname> <given-names>TE</given-names></name><name><surname>Holmes</surname> <given-names>AP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Nonparametric permutation tests for functional neuroimaging: a primer with examples</article-title><source>Human Brain Mapping</source><volume>15</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id><pub-id pub-id-type="pmid">11747097</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Parra</surname> <given-names>L</given-names></name><name><surname>Sajda</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Converging evidence of linear independent components in EEG</article-title><conf-name>First International IEEE EMBS Conference on Neural Engineering</conf-name><fpage>525</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1109/cne.2003.1196879</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Philiastides</surname> <given-names>MG</given-names></name><name><surname>Heekeren</surname> <given-names>HR</given-names></name><name><surname>Sajda</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Human scalp potentials reflect a mixture of decision-related signals during perceptual choices</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>16877</fpage><lpage>16889</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3012-14.2014</pub-id><pub-id pub-id-type="pmid">25505339</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollmann</surname> <given-names>S</given-names></name><name><surname>Weidner</surname> <given-names>R</given-names></name><name><surname>Humphreys</surname> <given-names>GW</given-names></name><name><surname>Olivers</surname> <given-names>CN</given-names></name><name><surname>Müller</surname> <given-names>K</given-names></name><name><surname>Lohmann</surname> <given-names>G</given-names></name><name><surname>Wiggins</surname> <given-names>CJ</given-names></name><name><surname>Watson</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Separating distractor rejection and target detection in posterior parietal cortex--an event-related fMRI study of visual marking</article-title><source>NeuroImage</source><volume>18</volume><fpage>310</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(02)00036-8</pub-id><pub-id pub-id-type="pmid">12595185</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Qu</surname> <given-names>T</given-names></name><name><surname>Xiao</surname> <given-names>Z</given-names></name><name><surname>Gong</surname> <given-names>M</given-names></name><name><surname>Huang</surname> <given-names>Y</given-names></name><name><surname>Li</surname> <given-names>X</given-names></name><name><surname>Wu</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Distance-Dependent Head-Related transfer functions measured with high spatial resolution using a spark gap</article-title><conf-name>IEEE Transactions on Audio, Speech, and Language Processing</conf-name><fpage>1124</fpage><lpage>1132</lpage><pub-id pub-id-type="doi">10.1109/tasl.2009.2020532</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radeau</surname> <given-names>M</given-names></name><name><surname>Bertelson</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>The after-effects of ventriloquism</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>26</volume><fpage>63</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1080/14640747408400388</pub-id><pub-id pub-id-type="pmid">4814864</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname> <given-names>D</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A category-free neural population supports evolving demands during decision-making</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1784</fpage><lpage>1792</lpage><pub-id pub-id-type="doi">10.1038/nn.3865</pub-id><pub-id pub-id-type="pmid">25383902</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Razavi</surname> <given-names>B</given-names></name><name><surname>O'Neill</surname> <given-names>WE</given-names></name><name><surname>Paige</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Auditory spatial perception dynamically realigns with changing eye position</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>10249</fpage><lpage>10258</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0938-07.2007</pub-id><pub-id pub-id-type="pmid">17881531</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recanzone</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Rapidly induced auditory plasticity: the ventriloquism aftereffect</article-title><source>PNAS</source><volume>95</volume><fpage>869</fpage><lpage>875</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.3.869</pub-id><pub-id pub-id-type="pmid">9448253</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoux</surname> <given-names>L</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bayesian model selection for group studies - revisited</article-title><source>NeuroImage</source><volume>84</volume><fpage>971</fpage><lpage>985</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.065</pub-id><pub-id pub-id-type="pmid">24018303</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname> <given-names>T</given-names></name><name><surname>Ehlis</surname> <given-names>AC</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The neural dynamics of hierarchical bayesian causal inference in multisensory perception</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1907</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09664-2</pub-id><pub-id pub-id-type="pmid">31015423</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname> <given-names>T</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Sensory reliability shapes perceptual inference via two mechanisms</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>22</elocation-id><pub-id pub-id-type="doi">10.1167/15.5.22</pub-id><pub-id pub-id-type="pmid">26067540</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname> <given-names>T</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Cortical hierarchies perform bayesian causal inference in multisensory perception</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002073</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002073</pub-id><pub-id pub-id-type="pmid">25710328</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname> <given-names>T</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distinct computational principles govern multisensory integration in primary sensory and association cortices</article-title><source>Current Biology</source><volume>26</volume><fpage>509</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.12.056</pub-id><pub-id pub-id-type="pmid">26853368</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname> <given-names>T</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reliability-Weighted integration of audiovisual signals can be modulated by Top-down control</article-title><source>eNeuro</source><volume>2018</volume><elocation-id>ENEURO.0315-17.2018</elocation-id><pub-id pub-id-type="doi">10.1523/eneuro.0315-17.2018</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schechtman</surname> <given-names>E</given-names></name><name><surname>Shrem</surname> <given-names>T</given-names></name><name><surname>Deouell</surname> <given-names>LY</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spatial localization of auditory stimuli in human auditory cortex is based on both head-independent and head-centered coordinate systems</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>13501</fpage><lpage>13509</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1315-12.2012</pub-id><pub-id pub-id-type="pmid">23015439</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schott</surname> <given-names>BH</given-names></name><name><surname>Wüstenberg</surname> <given-names>T</given-names></name><name><surname>Lücke</surname> <given-names>E</given-names></name><name><surname>Pohl</surname> <given-names>IM</given-names></name><name><surname>Richter</surname> <given-names>A</given-names></name><name><surname>Seidenbecher</surname> <given-names>CI</given-names></name><name><surname>Pollmann</surname> <given-names>S</given-names></name><name><surname>Kizilirmak</surname> <given-names>JM</given-names></name><name><surname>Richardson-Klavehn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gradual acquisition of visuospatial associative memory representations via the dorsal precuneus</article-title><source>Human Brain Mapping</source><volume>2018</volume><fpage>1554</fpage><lpage>1570</lpage><pub-id pub-id-type="doi">10.1002/hbm.24467</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Huang</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multisensory maps in parietal cortex</article-title><source>Current Opinion in Neurobiology</source><volume>24</volume><fpage>39</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.08.014</pub-id><pub-id pub-id-type="pmid">24492077</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname> <given-names>JP</given-names></name><name><surname>Nelson</surname> <given-names>LD</given-names></name><name><surname>Simonsohn</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant</article-title><source>Psychological Science</source><volume>22</volume><fpage>1359</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1177/0956797611417632</pub-id><pub-id pub-id-type="pmid">22006061</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Starke</surname> <given-names>J</given-names></name><name><surname>Ball</surname> <given-names>F</given-names></name><name><surname>Heinze</surname> <given-names>H-J</given-names></name><name><surname>Noesselt</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The spatio-temporal profile of multisensory integration</article-title><source>European Journal of Neuroscience</source><volume>15</volume><pub-id pub-id-type="doi">10.1111/ejn.13753</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Town</surname> <given-names>SM</given-names></name><name><surname>Brimijoin</surname> <given-names>WO</given-names></name><name><surname>Bizley</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Egocentric and allocentric representations in auditory cortex</article-title><source>PLOS Biology</source><volume>15</volume><elocation-id>e2001878</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2001878</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tukey</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="1977">1977</year><source>Exploratory Data Analysis</source><publisher-name>Addison-Wesley Publishing Company</publisher-name></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uncapher</surname> <given-names>MR</given-names></name><name><surname>Wagner</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Posterior parietal cortex and episodic encoding: insights from fMRI subsequent memory effects and dual-attention theory</article-title><source>Neurobiology of Learning and Memory</source><volume>91</volume><fpage>139</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2008.10.011</pub-id><pub-id pub-id-type="pmid">19028591</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Burg</surname> <given-names>E</given-names></name><name><surname>Alais</surname> <given-names>D</given-names></name><name><surname>Cass</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rapid recalibration to audiovisual asynchrony</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>14633</fpage><lpage>14637</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1182-13.2013</pub-id><pub-id pub-id-type="pmid">24027264</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Burg</surname> <given-names>E</given-names></name><name><surname>Alais</surname> <given-names>D</given-names></name><name><surname>Cass</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rapid recalibration to audiovisual asynchrony follows the physical—not the perceived—temporal order</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>80</volume><fpage>2060</fpage><lpage>2068</lpage><pub-id pub-id-type="doi">10.3758/s13414-018-1540-9</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vilberg</surname> <given-names>KL</given-names></name><name><surname>Rugg</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Memory retrieval and the parietal cortex: a review of evidence from a dual-process perspective</article-title><source>Neuropsychologia</source><volume>46</volume><fpage>1787</fpage><lpage>1799</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2008.01.004</pub-id><pub-id pub-id-type="pmid">18343462</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Mruczek</surname> <given-names>RE</given-names></name><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probabilistic maps of visual topography in human cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id><pub-id pub-id-type="pmid">25452571</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Werner-Reiss</surname> <given-names>U</given-names></name><name><surname>Kelly</surname> <given-names>KA</given-names></name><name><surname>Trause</surname> <given-names>AS</given-names></name><name><surname>Underhill</surname> <given-names>AM</given-names></name><name><surname>Groh</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Eye position affects activity in primary auditory cortex of primates</article-title><source>Current Biology</source><volume>13</volume><fpage>554</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1016/S0960-9822(03)00168-4</pub-id><pub-id pub-id-type="pmid">12676085</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wozny</surname> <given-names>DR</given-names></name><name><surname>Shams</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Recalibration of auditory space following milliseconds of cross-modal discrepancy</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>4607</fpage><lpage>4612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6079-10.2011</pub-id><pub-id pub-id-type="pmid">21430160</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wozny</surname> <given-names>DR</given-names></name><name><surname>Shams</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Computational characterization of visually induced auditory spatial adaptation</article-title><source>Frontiers in Integrative Neuroscience</source><volume>5</volume><elocation-id>75</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2011.00075</pub-id><pub-id pub-id-type="pmid">22069383</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zierul</surname> <given-names>B</given-names></name><name><surname>Röder</surname> <given-names>B</given-names></name><name><surname>Tempelmann</surname> <given-names>C</given-names></name><name><surname>Bruns</surname> <given-names>P</given-names></name><name><surname>Noesselt</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The role of auditory cortex in the spatial ventriloquism aftereffect</article-title><source>NeuroImage</source><volume>162</volume><fpage>257</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.09.002</pub-id><pub-id pub-id-type="pmid">28889003</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmer</surname> <given-names>U</given-names></name><name><surname>Macaluso</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>High binaural coherence determines successful sound localization and increased activity in posterior auditory areas</article-title><source>Neuron</source><volume>47</volume><fpage>893</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.07.019</pub-id><pub-id pub-id-type="pmid">16157283</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zwiers</surname> <given-names>MP</given-names></name><name><surname>Van Opstal</surname> <given-names>AJ</given-names></name><name><surname>Paige</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Plasticity in human sound localization induced by compressed spatial vision</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>175</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1038/nn999</pub-id><pub-id pub-id-type="pmid">12524547</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47001.016</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Maddox</surname><given-names>Ross K</given-names></name><role>Reviewing Editor</role><aff><institution>University of Rochester</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Maddox</surname><given-names>Ross K</given-names></name><role>Reviewer</role><aff><institution>University of Rochester</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Baum Miller</surname><given-names>Sarah</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib><contrib contrib-type="reviewer"><name><surname>Bosen</surname><given-names>Adam</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Shared neural underpinnings of multisensory integration and trial-by-trial perceptual recalibration&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Ross K Maddox as the guest Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individuals involved in review of your submission have also agreed to reveal their identity: Sarah Baum Miller (Reviewer #2) and Adam Bosen (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This manuscript by Park and Kayser investigates two complementary aspects of multisensory perception: the integration of audiovisual information within a trial, as well as the impact of a multisensory event on subsequent trials. They use single trial analysis of MEG data while human participants completed a spatial localization task which probes both processes. The experimental design follows that of Wozny and Shams, 2011, which is a good method of dissociating the ventriloquism effect from the ventriloquism aftereffect. This study represents a much needed addition to the field and leverages the spatial and temporal resolution of MEG. All three reviewers viewed the manuscript positively. However, the following reviewer concerns and questions should be addressed in the revised version.</p><p>Also note that the title should provide a clear indication of the biological system under investigation. Please revise your title with this advice in mind.</p><p>Essential revisions:</p><p>The first concern relates to the time course of eye movements during the experiment. Each trial starts with a fixation period, but the Materials and methods do not specify if participants were instructed to maintain fixation during target presentation and/or localization. Eye position can bias auditory localization (Razavi et al., 2007) and the ventriloquism aftereffect is coded in a mixed eye-head reference frame (Kopco et al., 2009), so eye movements during the experiment could substantially influence the behavioral results. We would like to see a more detailed description of how eye movements were controlled or could have influenced the behavioral results. Additionally, the absence of a significant electrophysiological coding of visual location on previous trials (Figure 2A, third panel), could be a result of a shifting visual reference frame caused by eye movements. Because auditory stimuli were presented via insert tubes, the auditory stimuli would not be altered by head movements, which could explain why auditory but not visual representations were evident.</p><p>The classification is not explained clearly. In subsection “Neural en- and decoding analysis” you state that &quot;each location was considered as a binary variable,&quot; with both left locations (-17, -8.5) collapsed, and the same done for the right locations. Was this done only for the classification outcome, or were the classification features also binarized? If they were, would this be an issue in trials where there was a VE shift but it did not cross into the other hemifield? If it was not the case, please make clear exactly what was binarized and what remained continuous.</p><p>Both the VE and VAE models (Equation 4 and 5) appear to be the same model. If this is not the case, then perhaps this could be clarified for the reader. We assume VAE would include β<sub>LDAAn</sub> * LDA<sub>An</sub> as in Equation 3. Please address this.</p><p>The ROC for each of the linear discriminants appears to just barely go above the chance line (Figure 2), so it seems the neural correlates of VE/VAE are very subtle. Is the small effect size more reflective of the nature of MEG signals or the nature of VE/VAE?</p><p>There seems to be a LH dominant response for β<sub>LDAAn-1</sub> in the VAE and VE neural representations. Do you have any sense as to why these would be LH dominant processes, and could you comment on this? One could have (perhaps naively) assumed that any hemispheric biases would be more RH dominant in a spatial localization context.</p><p>In the MEG results, the neural locations observed to be associated with the ventriloquism effect and ventriloquism aftereffect are broadly in agreement with our expectations.</p><p>The use of generic HRTFs to simulate auditory source location, rather than presenting auditory targets from free-field speakers, requires some assumptions about how participants perceive auditory targets simulated with those HRTFs. However, given the need to electromagnetically isolate the MEG equipment and the fact that stimuli are only presented in azimuth indicates that the use of HRTFs is justified and should not alter the results substantially. If anything, the general HRTF would produce an &quot;in the head&quot; feeling, which may decrease the probability of fusing the auditory and visual stimuli. Given that some participants showed very little VE or VAE (Figure 2, panels B and D, individuals with means near zero), this may have occurred, but some individuals show little of either effect even with free field stimuli, so it seems that the use of generic HRTFs did not alter the expected behavioral trends. Please add justification for your use of non-individualized HRTFs and discuss any effects this may have had on your findings.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47001.017</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>[…] Also note that the title should provide a clear indication of the biological system under investigation. Please revise your title with this advice in mind.</p></disp-quote><p>We revised the title to “Shared neural underpinnings of multisensory integration and trial-by-trial perceptual recalibration in humans”, to include the biological system investigated.</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The first concern relates to the time course of eye movements during the experiment. Each trial starts with a fixation period, but the Materials and methods do not specify if participants were instructed to maintain fixation during target presentation and/or localization. Eye position can bias auditory localization (Razavi et al., 2007) and the ventriloquism aftereffect is coded in a mixed eye-head reference frame (Kopco et al., 2009), so eye movements during the experiment could substantially influence the behavioral results. We would like to see a more detailed description of how eye movements were controlled or could have influenced the behavioral results.</p></disp-quote><p>Indeed, eye movements are a concern during spatial tasks. We had originally intended to use eye tracking for this study. However, as we placed the visual screen such as to cover a large field of view and to be close to the participants (to enhance AV binding), this became technically impossible. However, there are several reasons that make us believe that our results are not confounded by eye-movements:</p><p>First, and in line with previous studies (Wozny and Shams, 2011), we required participants to fixate a central fixation dot (Figure 1A) during the pre-stimulus, stimulus and post-stimulus intervals. Participants were generally asked to refrain from any eye-movements or blinks whenever the fixation dot was present. During the response period, i.e. while the cursor by which participants responded was displayed, participants could freely move their eyes to guide the cursor. We have extended the descriptions in the Materials and methods to clarify this:</p><p>“Importantly, participants were asked to maintain fixation during the entire pre-stimulus fixation period, the stimulus, and the post-stimulus period until the response cue appeared. During the response itself, they could freely move their eyes”.</p><p>To quantify whether eye movements were present during the pre-stimulus, stimulus and post-stimulus intervals we detected signatures of eye-movements by inspecting typical eye-related ICA components in the MEG data. Eye movement related ICA components were defined as those with frontal or frontal-lateralized topographies and time courses resembling those known for EOG artifacts (e.g. Keren et al., 2010; Hipp and Siegel, 2013) and as routinely used by our group in conjunction with MEG data, Giordano et al., 2017; Cao et al., 2019). This revealed that only 4.9% ± 1.7% (group mean ± s.e.m) of trials exhibited evidence of eye-movements, as judged from the ICA time courses. In particular, eye-movements during the stimulus presentation were rare (0.35% ± 0.61%, max 2.2%). Importantly, among those few eye-movement trials, the relevant ICA components did not exhibit the typical jump-like pattern known from changes in fixation position, and were rather suggestive of blinks (Vigário et al., 1998). This suggests that participants were indeed following the instructions and maintained fixation in the centre of the screen for the vast majority of trials. We have added these results to the manuscript (subsection: “Eye movements and the encoding of spatial information”).</p><p>Second, during the response period participants were free to move their eyes and hence eye movements were an essential part of the localization response. To avoid potential biases from the appearance of the response cursor, the starting position of this was always the centre. As a result, we expect that the relative spatial positions between fixation position, the appearance of the response cursor, and the final response given by the participant, were random.</p><p>Third, the response bias (VAE) was calculated as ‘VAE = (single trial response) – (mean of all response for this target sound location)’, as is common practice in the field (Rohe and Noppeney, 2015; Wozny and Shams, 2011). Hence, even if participants exhibited a general bias in pure auditory sound localization (e.g. the well-known central bias, Rohe and Noppeney, 2015), this would not affect our bias measure. We clarified this in the manuscript, both the Results, e.g.</p><p>“The VAE for each sound location was computed as R<sub>A</sub> – mean(R<sub>A</sub>); whereby mean(R<sub>A</sub>) reflects the mean over all localization responses for this position and participant. This approach ensures that any bias in pure auditory localization does not confound the VAE effect (Rohe and Noppeney, 2015b; Wozny and Shams, 2011b).”</p><p>And the Materials and methods:</p><p>“The VAE was defined as the difference between the reported location (R<sub>A</sub>) minus the mean reported location for all trials of the same stimulus position. This was done to ensure that any overall bias in sound localization (e.g. a tendency to perceive sounds are closer to the midline than they actually are) would not influence this bias measure (Wozny and Shams, 2011b)”.</p><p>As the reviewers note, the precise reference frames in which audio-visual integration and the ventriloquist after-effects occurs remain unclear. Importantly, our study was not designed to pinpoint these precise reference frames, but rather to elucidate the neural mechanisms. Still, this point requires discussion, and we have devoted a new section to the Discussion (subsection: ‘The role of coordinate systems in spatial perception’). In addition, and since the reviewers pointed to two specific studies, we would like to discuss a number of issues related to these studies here. We deemed some of these details too specific to be included in the manuscript itself.</p><p>When relating our results to the two cited studies, a number of critical differences emerge. Razavi et al. report the effects of eye movements or fixation position DURING stimulus presentation AND response. In their experiments, the stimulus was presented UNTIL the participants made a response. In contrast, in the present and similar previous studies (Wozny and Shams, 2011) stimuli were brief (50 ms), in order to rule out eye movement effects during the perceptual phase. This critical difference explains, possibly, why our behavioral results differ in a number of ways from those reported in Razavi et al. In their study, the authors observe an overshoot of localization responses when participants are free to move their eyes. In our data, we observe the well-known undershoot (i.e. a central bias; see also Wozny et al., 2010, Rohe and Noppeney, 2015). This difference possibly results from a decoupling of stimulus encoding and response periods in our paradigm, while Razavi et al. effectively studied an active sound foraging behavior, in which perception and localization were continuously coupled. In another version of their experiment, Razavi et al. show that MAINTAINING a lateralized fixation position, WITHOUT moving the eyes to respond, results in a systematic spatial bias after a prolonged time period (with reported time constants varying between 9 and 52 seconds across participants). Such prolonged and systematic fixations were prohibited by our experimental paradigm, as participants reported every 4-5 seconds a new (and random) stimulus position by moving a cursor to the left (right). Hence, such prolonged maintained fixations and related effects as described in Razavi et al. cannot account for the trial-by-trial recalibration described here.</p><p>Kopčo et al. investigated spatial recalibration following prolonged adaptation to audio-visual discrepancies. Hence, again, the effect under investigation is not directly comparable to the trial-by-trial effect studied here. By dissociating fixation position and target location, they were able to show that recalibration was best explained in a mixed reference frame, composed of both head- and eye-centered coordinates. While this suggests that fixation position influences long-term recalibration, it is also worth noting that Kopčo et al. directly report that the eye position influence emerges only slowly during the prolonged adaptation (their Supplemental Figure 1, about after about 100 trials). In fact, they explicitly state (their Discussion) that initial phases of long-term adaptation emerge in head-centered coordinates. In relation to the present study this suggests that eye position effects are small at the trial-by-trial level, and short-term recalibration can be accounted for in head-centered coordinates.</p><p>Wozny, D. R., Beierholm, U. R., and Shams, L. (2010). Probability Matching as a Computational Strategy Used in Perception. PLOS Computational Biology, 6(8), e1000871. Retrieved from https://doi.org/10.1371/journal.pcbi.1000871</p><p>R. Vigário, V. Jousmäki, M. Hämäläinen, R. Hari, and E. Oja.</p><p>Independent component analysis for identification of artifacts in magnetoencephalographic recordings. In Advances in Neural Information Processing Systems 10, pages 229-235. MIT Press, 1998.</p><disp-quote content-type="editor-comment"><p>Additionally, the absence of a significant electrophysiological coding of visual location on previous trials (Figure 2A, third panel), could be a result of a shifting visual reference frame caused by eye movements. Because auditory stimuli were presented via insert tubes, the auditory stimuli would not be altered by head movements, which could explain why auditory but not visual representations were evident.</p></disp-quote><p>This is an interesting idea, albeit not totally trivial to conceive. As argued above, participants were fixating before, during and after the stimulus, both in the AV and A trials, while the eyes moved during the intermediate response period. Hence, even if the relevant spatial information was coded partly in eye-centered coordinates, during the relevant time period used for the MEG analysis (the post-stimulus period) participants were fixating and keeping their head still (mean head movement across participants: 2.7 ± 0.2 mm (mean ± s.e.m.)) and hence head- and eye-centered coordinates the same. As a result, a change in coordinate systems seems unlikely to underlie the rather low decoding performance for V<sub>AV</sub>. One possibly, however, could be that eye movements during the response period may have, in part, wiped out some of the relevant representations, as additional visual information was present (the response bar and response cursor). We have added this speculation to the Discussion:</p><p>“Furthermore, it is also possible, that eye movements during the response period may have contributed to reducing a persistent representation of visual information, in part as additional visual information was seen and processed during the response period (e.g. the response cursor).”</p><disp-quote content-type="editor-comment"><p>The classification is not explained clearly. In subsection “Neural en- and decoding analysis” you state that &quot;each location was considered as a binary variable,&quot; with both left locations (-17, -8.5) collapsed, and the same done for the right locations. Was this done only for the classification outcome, or were the classification features also binarized? If they were, would this be an issue in trials where there was a VE shift but it did not cross into the other hemifield? If it was not the case, please make clear exactly what was binarized and what remained continuous.</p></disp-quote><p>The LDA analysis was designed to extract MEG activity that is sensitive to spatial information. The experiment featured five discrete spatial positions for the auditory/visual stimuli, and a continuous spatial response of the participants. To enter these spatial coordinates into a linear discriminant analysis, we grouped stimulus coordinates or participant responses to the left or right of the central midline. We have clarified the Materials and methods for how the LDA was computed as follows:</p><p>“We computed separate linear discriminant classifiers based on MEG activity in either the AV trial or the A trial, and for different to-be-classified conditions of interest. These were the location of the auditory and visual stimuli in the AV trial (A<sub>AV</sub>, V<sub>AV</sub>), the auditory stimulus in the A trial (A<sub>A</sub>), and the responses in either trial (R<sub>AV</sub>, R<sub>A</sub>). For each stimulus location or response variable, we classified whether this variable was left- or right-lateralized. That is, we reduced the 5 potential stimulus locations, or the continuous response, into two conditions to derive the classifier: we grouped trials on which the respective stimulus was to the left (-17, -8.5 °) or right (17, 8.5 °) of the fixation point, or grouped trials on which the response was to the left (&lt;0°) or the right (&gt;0°). The center stimuli were not used to derive the LDA classifiers.”</p><p>Importantly, this binarization was only used when grouping trials for deriving the LDA weights, but not for the subsequent regression analyses. We have clarified this:</p><p>“In these models, the response biases VE and VAE were the continuous localization data (in visual degrees) obtained from the participant response, while the LDA components (from Equation 1) reflect the continuous-valued prediction of the degree of lateralization of the respective stimulus extracted from the MEG activity.”</p><disp-quote content-type="editor-comment"><p>Both the VE and VAE models (Equation 4 and 5) appear to be the same model. If this is not the case, then perhaps this could be clarified for the reader. We assume VAE would include β<sub>LDAAn</sub> * LDA<sub>An</sub> as in Equation 3. Please address this.</p></disp-quote><p>Indeed, Eq. 4 and Eq. 5 may have been a bit confusing. What was missing was an explanation of which MEG activity was actually used for each model. The VE effect is studied in the AV trial, and hence the LDA is computed from the MEG activity in that trial. The VAE effect is studied in the A trial, hence the LDA is computed from MEG activity in this A trial. We now have clarified this in the descriptions of all Equations 2-5:</p><p>“Importantly, each model is computed based on the MEG activity and the behavioral data in the respective trials (VE in the AV trial, VAE in the A trial. That is, the VE model (Equation 4) uses the behavioral response (VE) from the AV trial and the representation (LDA component) of the two stimuli in the MEG activity from the same trial. In contrast, the VAE model (Equation 5) uses the behavioral response (VAE) from the A trial, and the representation (LDA component) of the two stimuli presented in the previous AV trial, reflected in the MEG activity from the A trial. Hence, each model uses the neural representation of stimuli from a different trial”</p><p>We now also explain why the regressor A<sub>A</sub> was not included in Equation 5: VAE reflects the deviation of the behavioral response from the actual sound location (c.f. Figure 1) and by way of how the VAE bias was defined, the contribution of A<sub>A</sub> is effectively removed. We now explicitly state this in the Results section:</p><p>“Note that because the VAE was defined relative to the average perceived location for each sound position, the actual sound position (A<sub>A</sub>) does not contribute to the VAE.”, as well as the Materials and methods (p26, L652-L654): “A predictor of the current sound (A<sub>A</sub>) was not included in Equation 5, as the VAE bias quantifies the deviation of the behavioral response from the current sound (c.f. Figure 1)”.</p><disp-quote content-type="editor-comment"><p>The ROC for each of the linear discriminants appears to just barely go above the chance line (Figure 2), so it seems the neural correlates of VE/VAE are very subtle. Is the small effect size more reflective of the nature of MEG signals or the nature of VE/VAE?</p></disp-quote><p>We can only offer speculative responses to this question. We report classification performance using the ROC, which is more principled than reporting e.g. the% of correctly decoded trials, as many studies do. The use of different metrics makes it difficult to compare performance values across studies. We can say that our classification method worked well, as for the AV-trial we can classify the visual target position (LDA<sub>V_AV</sub>; Figure 4—figure supplement 1) within the AV-trial with an ROC of about 0.9. Other studies on visual scene or objects report performance in binary classification tasks of around 60- 80% correct (e.g. Grootswagers et al., 2016; Cichy et al., 2016, Mohsenzadeh et al., 2018; Guggenmos et al., 2018). In contrast, classification performance for auditory locations is much lower in the present study. However, inspection of the literature shows that performance for classifying sound types from human EEG data using LDA (Kayser et al., 2017; Correia et al., 2015)both report an ROC of about 0.55) exhibits a similar performance. In particular, one study directly investigating the decoding of left vs. right lateralized sounds (but along the head-midline, rather than along an external fronto-parallel plane) in human EEG data reported an average performance of 75%, with some individuals only allowing only around 60% classification (Bednar et al., 2017).</p><p>This difference between classification performance for visual and acoustic stimuli remains unclear to us. It could result from a number of factors, such as i) the depth of relevant brain regions relative to the surface, and hence more noisier MEG/EEG signals; ii) more distributed representations within auditory pathways, e.g. auditory spatial maps have proven much more difficult to find than visual spatial maps (Alain et al., 2001; Alain et al., 2004; Town et al., 2017); or iii) the fact that auditory signals are simply not reflected linearly in large scale physiological signals (e.g. M/EEG), possibly because the underlying neurophysiological generators differ from those giving rise to visual evoked responses (Panzeri et al., 2015). Given the very speculative nature of this reply, we have refrained from including this in the manuscript.</p><p>Cichy, R. M., and Pantazis, D. (2016). Multivariate pattern analysis of MEG and EEG: a comparison of representational structure in time and space. <italic>BioRxiv</italic>, 95620. https://doi.org/10.1101/095620</p><p>Mohsenzadeh, Y., Mullin, C., Oliva, A., and Pantazis, D. (2018). The Perceptual Neural Trace of Memorable Unseen Scenes. <italic>BioRxiv</italic>, 414052. https://doi.org/10.1101/414052</p><p>Guggenmos, M., Sterzer, P., and Cichy, R. M. (2018). Multivariate pattern analysis for MEG: A comparison of dissimilarity measures. <italic>NeuroImage, 173</italic>, 434–447. https://doi.org/https://doi.org/10.1016/j.neuroimage.2018.02.044</p><p>Correia, J. M., Jansma, B., Hausfeld, L., Kikkert, S., and Bonte, M. (2015). EEG decoding of spoken words in bilingual listeners: from words to language invariant semantic-conceptual representations. <italic>Frontiers in Psychology.</italic></p><p>Bednar, A., Boland, F. M., and Lalor, E. C. (2017). Different spatio-temporal electroencephalography features drive the successful decoding of binaural and monaural cues for sound localization. <italic>European Journal of Neuroscience, 45</italic>(5), 679–689. https://doi.org/10.1111/ejn.13524</p><p>Alain C, Arnott SR, Hevenor S, Graham S, Grady CL (2001) “What” and “where” in the human auditory system. Proc Natl Acad Sci USA 98: 12301–12306.</p><p>Arnott SR, Binns MA, Grady CL, Alain C (2004) Assessing the auditory dual-pathway model in humans. Neuroimage 22: 401–408.</p><p>Panzeri, S., Macke, J.H., Gross, J., and Kayser, C. (2015). Neural population coding: combining insights from microscopic and mass signals.. Trends in Cognitive Sciences 19(3), 162-172.</p><disp-quote content-type="editor-comment"><p>There seems to be a LH dominant response for β<sub>LDA</sub>_An-1 in the VAE and VE neural representations. Do you have any sense as to why these would be LH dominant processes, and could you comment on this? One could have (perhaps naively) assumed that any hemispheric biases would be more RH dominant in a spatial localization context. In the MEG results, the neural locations observed to be associated with the ventriloquism effect and ventriloquism aftereffect are broadly in agreement with our expectations.</p></disp-quote><p>We did not have specific expectations as to the lateralization of sound encoding based on previous work. In fact, a closer look at previous studies suggests that the question about a potential lateralization of sound encoding remains largely unresolved. Clearly, sounds with lateralized azimuth generally seem to evoke stronger responses in the contralateral auditory cortex, as least when assessed using neuroimaging methods (e.g. Zierul et al., 2017; Fujiki et al., 2002). However, it is not a priori clear that such an activation difference also translates to a significant difference in the quality of neural representations, i.e. a difference in how well spatial information can be decoded from brain activity. A previous EEG study decoding sound location using a linear classifier, as we used, in fact reported bilateral contributions to sound azimuth encoding (Bednar et al., 2017). The picture seems somewhat different for sound elevation (Fujiki et al., 2002), but this is not of relevance for our paradigm. Concerning the neural correlates of multisensory recalibration the question about lateralization remains also unclear. A previous study on the long-term ventriloquist after-effect showed that the neural correlates of the perceptual effect were stronger in the left hemisphere, albeit no direct test for lateralization was performed (Zierul et al., 2017).</p><p>To directly address this question using data analysis, we subjected the respective clusters of interest (C<sub>PAR</sub>, C<sub>TEMP</sub>) to a direct tests for lateralization (Liégeois et al., 2002). Note that C<sub>VAE</sub> already comprised clusters from both hemispheres, hence reflects a genuine bilateral effect. For the remaining clusters we compared classification performance (AUC) and the model β’s between the significant cluster on the left hemisphere and the corresponding site on the other hemisphere, following an approach we have already used in previous work (Giordano et al., 2017). This revealed no clear significant and systematic differences between hemispheres, suggesting that for the present data we can’t speak of statistically lateralized effects. These results are now reported in a new section in the Results (subsection: ‘Hemispheric lateralization of audio-visual integration’) and are briefly mentioned in the Discussion: “While the significant clusters were more pronounced on the left hemisphere, a direct assessment did not provide evidence for these effects to be lateralized in a strict sense (Liégeois et al., 2002)”.</p><p>Fujiki N, Riederer KA, Jousmäki V, Mäkelä JP, Hari R. (2002). Human cortical representation of virtual auditory space: differences between sound azimuth and elevation. Eur J Neurosci. 16(11):2207-13.</p><p>Bednar A, Boland F, Lalor E. (2017). Different spatio-temporal electroencephalography features drive the successful decoding of binaural and monaural cues for sound localization. Eur J Neurosci. 45(5):679-689. doi: 10.1111/ejn.13524</p><disp-quote content-type="editor-comment"><p>The use of generic HRTFs to simulate auditory source location, rather than presenting auditory targets from free-field speakers, requires some assumptions about how participants perceive auditory targets simulated with those HRTFs. However, given the need to electromagnetically isolate the MEG equipment and the fact that stimuli are only presented in azimuth indicates that the use of HRTFs is justified and should not alter the results substantially. If anything, the general HRTF would produce an &quot;in the head&quot; feeling, which may decrease the probability of fusing the auditory and visual stimuli. Given that some participants showed very little VE or VAE (Figure 2, panels B and D, individuals with means near zero), this may have occurred, but some individuals show little of either effect even with free field stimuli, so it seems that the use of generic HRTFs did not alter the expected behavioral trends. Please add justification for your use of non-individualized HRTFs and discuss any effects this may have had on your findings.</p></disp-quote><p>We had to use HRTFs (as opposed to speakers) given the circumstances in the MEG suite. We initially considered the possibility of using the pseudo-individualized HRTF with the CIPIC database. However after discussion with a colleague who actually used both the pseudo-individualized HRTFs and non-individualized HRTFs, we opted for the non-individualized HRTF (PKU-IOA HRTF database), since our priority was to bring the screen as close as possible to the participant in order to enhance co-localization, whereby we could choose different distances from the PKU-IOA database, and also we chose practical and efficient procedure, since even with the pseudo-individualized HRTFs, perfect externalization similar to real speakers will not be possible, and due to measurement errors, wrong HRTF can be chosen. We tested the sounds beforehand to make sure the participants were able to perceive the different directions of sound, and proceeded when through pilot tests, we concluded the sounds would serve its purpose in our experiment. The behavioral data demonstrate that the HRTFs produced a spatially balanced percept, i.e. there was no evidence for a specific spatial (left- or right-wards) bias in pure sound localization. As the reviewers note, the use of HRTFs may have shifted the perceived location from the external space to within-participants heads. This would effectively reduce the perceived co-localization of visual and acoustic stimuli, and hence reduce the chance to see any VE and VAE biases (Rohe and Noppeney, 2016; Wozny and Shams, 2011). However, this would only reduce the magnitude of the VAE/VE bias, but not spatially distort those. As a result, the use of non-individualized HRTFs may have added noise to the behavioral or MEG data, but should not bias the results in any specific way, as these rely on predicting the spatial direction between behavioral biases and their neural underpinnings. We have added a brief note about the use of HRTF in general as opposed to real speakers to the Materials and methods:</p><p>“The behavioral data obtained during A trials confirm that participants perceived these sounds as lateralized”</p><p>And the Discussion:</p><p>“The use of virtual sound locations, rather than e.g. an array of speakers, may have affected the participants’ tendency to bind auditory and visual cues (Fujisaki, Shimojo, Kashino, and Nishida, 2004). While the use of HRTFs is routine in neuroimaging studies on spatial localization (Rohe and Noppeney, 2015a), individual participants may perceive sounds more ‘within’ the head in contrast to these being properly externalized. While this can be a concern when determining whether audio-visual integration follows a specific (e.g. Bayes optimal) model (Meijer, Veselič, Calafiore, and Noppeney, 2019), it would not affect our results, as these are concerned with relating the trial specific bias expressed in participants behavior with the underlying neural representations. Even if visual and acoustic stimuli were not perceived as fully co-localized, this may have reduced the overall ventriloquist bias, but would not affect the neuro-behavioral correlation. Indeed, the presence of both the ventriloquist bias and the trial-by-trial recalibration effect suggests that participants were able to perceive the spatially disparate sound sources, and co-localize the sound and visual stimulus when the disparity was small.”</p></body></sub-article></article>