<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">55613</article-id><article-id pub-id-type="doi">10.7554/eLife.55613</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Low-frequency neural activity reflects rule-based chunking during speech listening</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-173883"><name><surname>Jin</surname><given-names>Peiqing</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5302-4079</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-173884"><name><surname>Lu</surname><given-names>Yuhan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2684-1484</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-51085"><name><surname>Ding</surname><given-names>Nai</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3428-2723</contrib-id><email>ding_nai@zju.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Sciences, Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>Research Center for Advanced Artificial Intelligence Theory, Zhejiang Lab</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Reichenbach</surname><given-names>Tobias</given-names></name><role>Reviewing Editor</role><aff><institution>Imperial College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>20</day><month>04</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e55613</elocation-id><history><date date-type="received" iso-8601-date="2020-01-30"><day>30</day><month>01</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-04-20"><day>20</day><month>04</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Jin et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Jin et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-55613-v2.pdf"/><abstract><p>Chunking is a key mechanism for sequence processing. Studies on speech sequences have suggested low-frequency cortical activity tracks spoken phrases, that is, chunks of words defined by tacit linguistic knowledge. Here, we investigate whether low-frequency cortical activity reflects a general mechanism for sequence chunking and can track chunks defined by temporarily learned artificial rules. The experiment records magnetoencephalographic (MEG) responses to a sequence of spoken words. To dissociate word properties from the chunk structures, two tasks separately require listeners to group pairs of semantically similar or semantically dissimilar words into chunks. In the MEG spectrum, a clear response is observed at the chunk rate. More importantly, the chunk-rate response is task-dependent. It is phase locked to chunk boundaries, instead of the semantic relatedness between words. The results strongly suggest that cortical activity can track chunks constructed based on task-related rules and potentially reflects a general mechanism for chunk-level representations.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>From digital personal assistants like Siri and Alexa to customer service chatbots, computers are slowly learning to talk to us. But as anyone who has interacted with them will appreciate, the results are often imperfect.</p><p>Each time we speak or write, we use grammatical rules to combine words in a specific order. These rules enable us to produce new sentences that we have never seen or heard before, and to understand the sentences of others. But computer scientists adopt a different strategy when training computers to use language. Instead of grammar, they provide the computers with vast numbers of example sentences and phrases. The computers then use this input to calculate how likely for one word to follow another in a given context. &quot;The sky is blue&quot; is more common than &quot;the sky is green&quot;, for example.</p><p>But is it possible that the human brain also uses this approach? When we listen to speech, the brain shows patterns of activity that correspond to units such as sentences. But previous research has been unable to tell whether the brain is using grammatical rules to recognise sentences, or whether it relies on a probability-based approach like a computer.</p><p>Using a simple artificial language, Jin et al. have now managed to tease apart these alternatives. Healthy volunteers listened to lists of words while lying inside a brain scanner. The volunteers had to group the words into pairs, otherwise known as chunks, by following various rules that simulated the grammatical rules present in natural languages. Crucially, the volunteers’ brain activity tracked the chunks – which differed depending on which rule had been applied – rather than the individual words. This suggests that the brain processes speech using abstract rules instead of word probabilities.</p><p>While computers are now much better at processing language, they still perform worse than people. Understanding how the human brain solves this task could ultimately help to improve the performance of personal digital assistants.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>chunking</kwd><kwd>speech sequence</kwd><kwd>magnetoencephalographic</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31771248</award-id><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Major Scientific Research Project of Zhejiang Lab</institution></institution-wrap></funding-source><award-id>2019KB0AC02</award-id><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Fundamental Research Funds for the Central Universities</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Zhejiang Provincial Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>LY20C090008</award-id><principal-award-recipient><name><surname>Jin</surname><given-names>Peiqing</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neuroimaging reveals that the brain response to spoken language can be better explained by rule-based models than statistical models recently developed in artificial intelligence research.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>How the brain processes sequences is a central question in cognitive science and neuroscience, and speech is a classic example of complex and rapid sequences that the human brain can effectively process. In general, speech utterances such as sentences are not memorized sequences, but instead are proposed to reflect compositional processes which allow us to understand and produce countless new sentences never heard before (such as the one you are currently reading). Therefore, to derive the meaning of a speech sequence, the brain has to integrate information across words, the meaning of which are stored in long-term memory. Recent neurophysiological results have shown that when listening to speech, cortical activity is observed on multiple time scales that match the time scales of multiple levels of linguistic units, such as sentences, phrases, words, and syllables (<xref ref-type="bibr" rid="bib7">Brodbeck et al., 2018</xref>; <xref ref-type="bibr" rid="bib8">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Ding et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Keitel et al., 2018</xref>; <xref ref-type="bibr" rid="bib54">Makov et al., 2017</xref>). Critically, cortical responses on the time scales of phrases and sentences are observed even when the phrasal and sentential boundaries are not cued by prosodic features or by the transitional probability between words.</p><p>The neural responses on the time scales of phrases and sentences have been taken as strong evidence that the brain applies grammatical rules to group words into chunks (<xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib56">Martin and Doumas, 2017</xref>). Challenging this rule-based chunking interpretation, it has been argued that neural responses at the phrasal and sentential rates may not reflect neural construction of multi-word chunks based on rules and can instead be explained by neural tracking of properties of individual words alone (<xref ref-type="bibr" rid="bib27">Frank and Yang, 2018</xref>). In the English materials used in <xref ref-type="bibr" rid="bib17">Ding et al. (2017)</xref>, for example, all sentences have the same structure of adjective+noun+verb+noun, for example ‘new plans gave hope’. Therefore, neural activity tracking lexical properties, such as part of speech information or lexical semantic information that distinguishes objects and actions, may appear to track sentences and phrases. For example, neural activity tracking verbs will occur at the sentence rate, since there is one verb per sentence. Consistent with this lexical property model, it has been shown that apparent neural tracking of sentences could be observed if the neural response independently represents each word using multidimensional features learned by statistical analysis of large corpora (<xref ref-type="bibr" rid="bib27">Frank and Yang, 2018</xref>). In other words, neural populations that are tuned to lexical features of individual words may show activity that apparently tracks sentence structures.</p><p>Furthermore, it is well established that the neural response to a word depends on the context and the response amplitude is smaller if the word is semantically related to previous words (<xref ref-type="bibr" rid="bib48">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="bib50">Lau et al., 2008</xref>). In general, words within the same sentence are more related than words from neighboring sentences. Therefore, for a context-dependent neural response, its amplitude is expected to be stronger at the beginning of a sentence, leading to apparent neural tracking of sentences. This model considers semantic relatedness between consecutive words, but it does not consider the sentence structure: Semantic relatedness is evaluated the same way within and across sentence boundaries. Apparent sentence tracking behavior is generated since words within a sentence are more closely related.</p><p>The lexical property model and semantic relatedness model do not assume chunk-level representations and therefore provide different explanations for sentential/phrasal-rate responses than the rule-based chunking model. The rule-based chunking model, however, has additional flexibility, allowing the same sequence of words to be grouped differently based on different sets of rules. This flexibility is most clearly demonstrated when processing structurally ambiguous sequences. For example, ‘sent her kids story books’ can be chunked as ‘sent [her kids] story books’ or ‘sent her [kids story books]’ (<xref ref-type="bibr" rid="bib70">Shultz and Pilon, 1973</xref>). For such structurally ambiguous sentences, the rule-based chunking model, but not the lexical property model or the semantic relatedness model, would predict different phrase-tracking responses when the sentence is chunked differently.</p><p>Notably, the three models introduced here are not exclusive, and neural encoding of lexical properties is a prerequisite to analyze the semantic relations between words or to build multi-word chunks. It is well established that the brain responses reflect encoding of lexical properties and semantic relations, but it remains debated whether the brain represents multi-word chunks. Therefore, the goal of the current study is to test whether neural activity on the time scales commensurate with multi-word chunks indeed indicates chunk-level neural representations or can be explained by the simpler word-level representations.</p><p>Here, we distinguished the rule-based chunking model from the word-based models by asking the listeners to chunk a word sequence based on different rules in two experimental conditions. The word sequence was a sequence of nouns that describe either living (L) things or nonliving (N) things (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The sequence had no syntactically defined chunks. Nevertheless, the participants explicitly learned artificial rules to chunk the sequence, and rules varied between two conditions. While participants performed the rule-based chunking task, their cortical responses were recorded using MEG. The word-level models, that is the lexical property model and the semantic relatedness model, predicted the same neural response when participants listened to the same word sequence regardless of how the sequence was chunked. The rule-based chunking model, however, predicted chunk-dependent neural responses.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimuli and task.</title><p>(<bold>A</bold>) Stimuli consist of isochronously presented nouns, which belong to two semantic categories, that is living (L) or nonliving (N) things. Two nouns construct a chunk and the chunks further construct sequences. In the same-category condition (upper panel), nouns in a chunk always belong to the same semantic category. In the different-category condition (lower panel), nouns in a chunk always belong to different categories. Sequences in each condition further divide into alternating-order (left) and random-order (right) sequences. The alternating-order sequence only differs by a time lag between the same- and different-category conditions. In the illustration, colors are used to distinguish words from different categories and the two words in a chunk but in the speech stimulus all words are synthesized in the same manner. (<bold>B</bold>) The task is to decompose each sequence into two-word chunks and detect invalid chunks. Three trials and the correct responses are shown for each condition (tick and cross for normal and outlier trials, respectively). Red underlines highlight the invalid chunks.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55613-fig1-v2.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Word sequences and model predictions</title><p>Participants were instructed to parse a sequence of words into chunks and each chunk consisted of two words. The words were drawn from two categories, that is living (L) and nonliving (N) things. The experiment contrasted two conditions in which the chunks were constructed based on different rules. In one condition, referred to as the same-category condition, the two words in a chunk belonged to the same semantic category (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, upper panel). In the other condition, referred to as the different-category condition, the two words in a chunk were drawn from different categories (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, lower panel). Based on these rules, there were two valid chunks in the same-category condition, that is LL and NN, and also two valid chunks in the different-category condition, that is NL and LN.</p><p>The same- and different-category conditions were presented in separate blocks. In each condition, participants were asked to parse the sequences into chunks, that is pairs of words, and were instructed about the rules that valid chunks followed. They had to judge if any invalid chunk was presented in a sequence (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The sequences in each condition (<italic>N</italic> = 60) were further divided into two types, that is the alternating-order sequence (<italic>N</italic> = 30) and the random-order sequence (<italic>N</italic> = 30). In the alternating-order sequence, the two valid chunks in each condition were interleaved while in the random-order sequence the two valid chunks were presented in random order. Equal numbers of alternating-order sequences and random-order sequences were intermixed and presented randomly in each block. The behavioral correct rate was 85 ± 2% and 86 ± 2% for the same- and different-category conditions respectively (mean ± SEM across participants). The neural responses to these two types of sequences were separately analyzed. The alternating-order sequences allowed an intuitive comparison between the same- and different-category conditions, which would be detailed in the following. The random-order sequences were designed as fillers to avoid alternative strategies for the task (see Materials and methods for details), but model simulations in the following showed that they could also distinguish the three models.</p><p>Simulations of the neural responses to the alternating- and random-order sequences by the three models are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> for both the same- and different-category conditions. The lexical property model considers two neural populations that idealize tuning to living and nonliving word meanings, respectively. Since the two neural populations are anti-correlated, only the neural population tuned to living things is shown (<xref ref-type="fig" rid="fig2">Figure 2AB</xref>). The simulated neural response analyzed in the frequency domain demonstrates that neither the population shows a spectral peak at 1 Hz, that is the chunk rate, in the spectrum. In contrast to the lexical property model, the semantic relatedness model and the rule-based chunking model predict a 1 Hz response peak (red and green curves respectively in <xref ref-type="fig" rid="fig2">Figure 2AB</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Model simulations.</title><p>(<bold>A</bold>) Simulations for alternating-order sequences. The left panel illustrates the response waveform for example sequences. Neural responses predicted by the lexical property and semantic relatedness models differ by a time lag between same- and different-category conditions. The rule-based chunking model, however, predicts identical responses in both conditions. The middle panel shows the predicted spectrum averaged over all sequences in the experiment. The semantic relatedness model and the rule-based chunking model both predict a significant 1 Hz response, while the lexical property model predicts a significant response at 0.5 Hz and its odd order harmonics. The right panel shows predicted phase difference between same- and different-category conditions at 1 Hz. The phase difference predicted by the lexical property model is uniformly distributed. The semantic relatedness model predicts a 180° phase difference between conditions, while the rule-based chunking model predicts a 0° phase difference. (<bold>B</bold>) Simulations for random-order sequences. The semantic relatedness model and rule-based model predict a significant 1 Hz response. They generate different predictions, however, about the 1 Hz phase difference between conditions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55613-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Model simulation.</title><p>(<bold>A</bold>) Procedures to simulate the lexical property model and the semantic relatedness model. The left panel illustrates the representation of word features. Each feature dimension is represented by a pulse sequence, with a pulse placed at the onset of each word and the amplitude of the pulse modulated by the word feature. Two feature dimensions are illustrated here. The predicted neural response is simply the feature sequence convolving a response function, which is a 500 ms Gaussian window. For the semantic relatedness model, the correlation coefficient between binary vectorial word representations is used to measure semantic similarity between words. In the pule sequence, the pulse at the onset of each word denoted the one minus the correlation between the current word and the previous word. (<bold>B</bold>) Potential waveforms for the chunk response. The rule-based chunking model only assumes a reliable response to each chunk and does not constrain the response waveform. Simulations show that the core model predictions, that is a 1 Hz spectral peak and a 180° phase difference between conditions, are not affected by the response waveform.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55613-fig2-figsupp1-v2.tif"/></fig></fig-group><p>A more fundamental difference between the semantic relatedness model and rule-based chunking model lies in their predictions about the 1 Hz response phase. The semantic relatedness model predicts a 180° phase difference between same- and different-category conditions, while the rule-based chunking model predicts a 0° phase difference between conditions (<xref ref-type="fig" rid="fig2">Figure 2</xref>). For the alternating-order sequence, these predictions are straightforward: These sequences are offset by one word between the same- and different-category conditions. Consequently, neural activity tracking semantic relatedness between words is offset by the duration of a word between conditions. For neural activity at 1 Hz, this time lag lead to a 180° phase difference. For the random-order sequence, although less straightforward, model simulation shows that the 1 Hz response has a 180° phase difference between conditions. For the rule-based model, however, the response is aligned with the chunk boundaries, which are not affected by the conditions and sequence types. Therefore, the rule-based model predicts the same response phase, that is a 0° phase difference, for the same- and different-category conditions.</p><p>In summary, the three models considered in this study lead to different predictions about the neural responses (<xref ref-type="fig" rid="fig2">Figure 2AB</xref>). Details about the model simulations are given in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>. The lexical property model and the semantic relatedness model assume ideal tuning to living/nonliving things. In the following, we turn to the actual neural responses obtained using MEG and evaluate their consistency with the simulations made for the three different models.</p></sec><sec id="s2-2"><title>Rule-dependent neural tracking of Alternating-order sequences</title><p>The MEG responses were separately averaged for the same- and different-category conditions and the mean response was transformed to the frequency domain. We first analyzed the MEG responses to the alternating-sequences. The response spectrum averaged over all MEG gradiometers showed a clear peak at 1 Hz (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, left two plots). The 1 Hz response power was significant in both conditions (F<sub>32,64</sub> = 5.8, p=2.0 × 10<sup>−9</sup> and F<sub>32,64</sub> = 6.5, p=2.1 × 10<sup>−10</sup> for the same- and different-category conditions respectively; F-test, FDR corrected). The 1 Hz spectral peak was consistent with the semantic relatedness model and the rule-based chunking model, but not with the lexical property model (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). On top of the 1 Hz response peak, a 2 Hz response peak was clearly observed and was significant (F<sub>32,64</sub> = 45.8, p=9.7 × 10<sup>−33</sup> and F<sub>32,64</sub> = 35.9, p=1.4 × 10<sup>−29</sup> for the same- and different-category conditions respectively; F-test, FDR corrected). However, no significant peak was observed at 0.5 Hz (F<sub>32,64</sub> = 1.3, p=0.17 and F<sub>32,64</sub> = 1.0, p=0.52 for the same- and different-category conditions respectively; F-test, FDR corrected).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>MEG responses.</title><p>(<bold>A</bold>) The response power spectrum averaged over participants and MEG gradiometers. A 1 Hz response peak and a 2 Hz response peak are observed. The shaded area covered 1 SEM over participants on each side. (<bold>B</bold>) The 1 Hz and 2 Hz response power. There is no significant difference between conditions. (<bold>C, D</bold>) Response topography (gradiometers) and source localization results, averaged over participants. Sensors (shown by black dots) and vertices that have no significant response (p&gt;0.05; F-test; FDR corrected) are not shown in the topography and localization results. The 1 Hz and 2 Hz responses both show bilateral activation. The neural source localization results are shown by the dSPM values and vertices with dSPM smaller than the min value in the color bar are not shown. (<bold>E</bold>) Phase difference between the same-category and different-category conditions at 1 Hz. The topography shows the distribution of phase difference across MEG sensors (one magnetometers and two gradiometers in the same position are circular averaged). The topography is circular averaged over participants. The histogram shows the phase difference distribution for all 306 MEG sensors. The phase difference is closer to 0° (predicted by the rule-based chunking model) than 180° (predicted by the semantic relatedness model). (<bold>F</bold>) Waveform averaged over trials and subjects (the second PC across MEG sensors). The waveform is filtered around 1 Hz and is highly consistent between the same- and the different-category conditions. *p&lt;0.05, **p&lt;0.005</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55613-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Additional MEG results.</title><p>(<bold>A</bold>) Individual response spectrum and phase difference. In the response spectrum, each curve is the result from a participant (averaged over alternating- and random-order sequences with both same- and different-category chunks). A 1 Hz spectral peak can be seen in 14 out of 16 participants. In the phase difference (averaged over alternating- and random-order sequences), each point on the unit circle is the result from a participant. In 14 out of 16 participants, the phase difference is between −45° and 45°. (<bold>B</bold>) Waveform of the first and second PC across all MEG sensors, which is consistent between the same- and different category conditions. (<bold>C</bold>) Region of interest (ROI) analysis of 1 Hz power. The 1 Hz power in two ROIs were extracted (90th percentile). The two ROIs include one in the frontal lobe (shown in red/blue in the left/right hemisphere) and one in the temporal lobe. In the bar graph, each marker denotes the response to each kind of sequence and each bar shows the power averaged over all sequences. *p&lt;0.05, **p&lt;0.005.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55613-fig3-figsupp1-v2.tif"/></fig></fig-group><p>We then analyzed the phase difference between same- and different-category conditions at 1 Hz. In all the 306 MEG sensors, the phase difference averaged over participants was closer to 0° than 180° (<xref ref-type="fig" rid="fig3">Figure 3E</xref>), and in 258 sensors the effect was significant (p&lt;0.01, bootstrap, see Materials and methods, FDR corrected). These results were consistent with the rule-based chunking model (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The phase difference averaged over all MEG sensors was significantly closer to 0° than 180° (p=1 × 10<sup>−4</sup>, bootstrap, see Materials and methods). The mean phase difference was 0.5° and the 99% confidence interval ranged from −22.9° to 18.7°. The main results on response spectrum and response phase difference could be reliably observed in individual participants (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). To further illustrate the response phase difference in the time domain, we analyzed the response waveforms. The Principal Component Analysis (PCA) was employed to extract major response patterns from all MEG sensors. The first PC captured the MEG response to the sound onset (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>) and the second PC captured the 1 Hz response, which oscillate in phase in the same- and different-category conditions (<xref ref-type="fig" rid="fig3">Figure 3F</xref>).</p></sec><sec id="s2-3"><title>Rule-dependent neural tracking of Random-order sequences</title><p>The MEG responses to random-order sequences were analyzed the same way as the responses to alternating-order sequences, and the results were similar: In the response spectrum, a clear peak was observed at 1 Hz (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, right two plots). The 1 Hz response power was significant in both conditions (F<sub>32,64</sub> = 4.5, p=2.6 × 10<sup>−7</sup> and F<sub>32,64</sub> = 5.9, p=1.5 × 10<sup>−9</sup> for the same- and different-category conditions respectively; F-test, FDR corrected). The 1 Hz response power was not significantly different between random- and alternating-order sequences (F<sub>32,32</sub> = 1.0, p=0.99 and F<sub>32,32</sub> = 1.1, p=0.90 for same- and different-category conditions respectively; F-test, FDR corrected; <xref ref-type="fig" rid="fig3">Figure 3B</xref>). The responses to random-order sequences also showed a significant 2 Hz response peak (F<sub>32,64</sub> = 48.5, p=1.7 × 10<sup>−33</sup> and F<sub>32,64</sub> = 44.6, p=2.0 × 10<sup>−32</sup> for the same- and different-category conditions respectively; F-test, FDR corrected). The peak at 0.5 Hz was not significant (F<sub>32,64</sub> = 0.75, p=0.81 and F<sub>32,64</sub> = 0.91, p=0.61 for the same- and different-category conditions respectively; F-test, FDR corrected). At 1 Hz, the phase difference between conditions was closer to 0° than 180° in all MEG sensors (<xref ref-type="fig" rid="fig3">Figure 3E</xref>), and the effect was significant in 196 sensors (p&lt;0.01, bootstrap, see Materials and methods, FDR corrected). The phase difference averaged over all MEG sensors was significantly closer to 0° than 180° (p=4 × 10<sup>−4</sup>, bootstrap, see Materials and methods). The mean phase difference was 20.3° and the 99% confidence interval over participants ranged from −0.1° to 56.6°.</p><p>For both the neural responses to alternating-order and random-order sequences, in the response topography, the 1 Hz and 2 Hz responses showed bilateral activation (<xref ref-type="fig" rid="fig3">Figure 3CD</xref>). Neural source localization results confirmed that both the 1 Hz and 2 Hz responses were mainly generated from bilateral temporal and frontal lobes (<xref ref-type="fig" rid="fig3">Figure 3CD</xref>). An ROI analysis further revealed that the 1 Hz response peak was statistically significant in both temporal and frontal lobes (F<sub>32,64</sub> &gt; 3.3, p&lt;1.9 × 10<sup>−5</sup> for all conditions; F-test, FDR corrected), and the response is stronger in the left superior temporal gyrus than the left inferior frontal gyrus (F<sub>128,128</sub> = 2.0, p=7.1 × 10<sup>−4</sup>; F-test, FDR corrected; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>How chunks are represented during sequence processing is a prominent question in psychology and cognitive neuroscience. Here, we demonstrate that low-frequency neural activity can track multi-word chunks that are mentally constructed based on artificial chunking rules. Critically, the artificial rules here can dissociate the properties of individual words from the chunk structures and therefore provide strong evidence that low-frequency neural activity can encode chunks.</p><p>Whether items in a sequence are mentally represented by hierarchically organized chunks is a heavily debated question in many cognitive domains, including language comprehension, action planning, and event perception (<xref ref-type="bibr" rid="bib21">Everaert et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Frank et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Lashley, 1951</xref>). The multi-item chunks may be called schemas, scripts, or phrases in different fields. For example, for the routine behavior to prepare coffee, one view holds that it constitutes a schema with hierarchical structures: The schema of preparing coffee divides into sub-schemas such as adding sugar and adding milk, while each sub-schema further divides into finer-grained sub-schemas such as opening a package and pouring sugar into coffee (<xref ref-type="bibr" rid="bib11">Cooper and Shallice, 2006</xref>). A contrasting view, however, is that hierarchical schemas are epiphenomenal and the internal cortical states generating basic actions are chained directly without any additional superordinate representation (<xref ref-type="bibr" rid="bib5">Botvinick and Plaut, 2004</xref>). Similarly, for newly learned complex sequential processing tasks, some studies found that the underlying neural activity can be modeled by item-based state-transition models (<xref ref-type="bibr" rid="bib55">Mante et al., 2013</xref>) while others provide evidence for chunk-level neural representations (<xref ref-type="bibr" rid="bib30">Geddes et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Jiang et al., 2018</xref>).</p><p>Chunk-based models and item-based models favor different kinds of neural implementations. Representations of chunks could be implemented by sustained neural activation throughout the duration of a chunk (<xref ref-type="bibr" rid="bib29">Fuster, 2001</xref>). In other words, the neural representation of a chunk is synchronized to the onset and offset of a chunk. Consistent with this idea, both computational models (<xref ref-type="bibr" rid="bib11">Cooper and Shallice, 2006</xref>; <xref ref-type="bibr" rid="bib56">Martin and Doumas, 2017</xref>) and neural responses (<xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib63">Nozaradan et al., 2011</xref>) show activity matching the time scales of hypothetical chunks during routine behavior and language/auditory perception. For item-based models, cortical state is updated item by item, which does not explicitly predict neural synchronization to chunks. Indeed, in some cognitive domains only item-rate neural activity is observed. For example, when a zebra finch sings a multi-syllable song, no slow neural activity matching the time scales of multi-syllabic chunks is observed. Instead, bursts of transient neural responses occur at syllable onsets, which strongly support the item-based state transition model (<xref ref-type="bibr" rid="bib53">Long et al., 2010</xref>). The zebra finch songs, however, are highly stereotyped. Future studies are needed to establish whether the chunk-rate responses observed here is limited to the processing of flexible complex sequences or limited to the primate brain.</p><p>A potential way to link chunk-based and item-based processing mechanisms is that they both exist in the brain but are implemented in different brain areas and applied to different tasks (<xref ref-type="bibr" rid="bib32">Goucha et al., 2017</xref>). During language processing, for example, it has been proposed that item-based simple combination of words is implemented in the anterior temporal lobe (<xref ref-type="bibr" rid="bib4">Bemis and Pylkkanen, 2013</xref>; <xref ref-type="bibr" rid="bib6">Brennan et al., 2012</xref>) while rule-based chunk-level processing is implemented in the inferior frontal lobe (<xref ref-type="bibr" rid="bib34">Grodzinsky and Friederici, 2006</xref>; <xref ref-type="bibr" rid="bib35">Grodzinsky and Santi, 2008</xref>). Furthermore, experience may also affect the processing mechanism. For example, when participants have to perform a task based on rules that are learned explicitly, either rules in an executive control task or grammar in an unknown language, it is barely controversial that rule-based processing occurs in the brain, since there is not enough exposure to build up a state transition model. Nevertheless, for routine behavior or native language processing, extensive exposure allows the learning of statistical relations and it becomes difficult to distinguish rule-based models and item-based state transition models.</p><p>In the current study, listeners explicitly apply artificial rules to group words into chunks, in contrast to previous studies in which the listeners can rely on implicit linguistic knowledge to group words into phrases. It remains elusive whether the brain relies on similar mechanisms for sequence chunking in different tasks. The primary motivation for a domain-general implementation of sequence-chunking tasks is that many sequence-chunking tasks share common computational principles. One such common computation is to find and encode the chunk boundaries. Consistent with a domain-general mechanism to encode chunk boundaries, studies have found similar EEG responses, that is the closure positive shift (CPS), at the prosodic phrasal boundaries in speech (<xref ref-type="bibr" rid="bib52">Li and Yang, 2009</xref>; <xref ref-type="bibr" rid="bib71">Steinhauer et al., 1999</xref>) and music phrasal boundaries (<xref ref-type="bibr" rid="bib81">Zhang et al., 2016</xref>). Similarly, a transient increase in brain activity has been observed at event boundaries in movies (<xref ref-type="bibr" rid="bib80">Zacks et al., 2001</xref>) and in non-speech sound sequences (<xref ref-type="bibr" rid="bib9">Chait et al., 2007</xref>).</p><p>Another common computation in sequence-chunking tasks is to integrate information within a chunk. It has been suggested that low-frequency neural activity may also reflect sequential information integration within spoken phrases (<xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>) and musical meters (<xref ref-type="bibr" rid="bib63">Nozaradan et al., 2011</xref>). During sequential decision making, it has been more quantitatively demonstrated that neural activity reflects accumulation of sensory evidence (<xref ref-type="bibr" rid="bib3">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="bib64">O'Connell et al., 2012</xref>; <xref ref-type="bibr" rid="bib68">Shadlen and Shohamy, 2016</xref>). The task in the current experiment also engages sequential decision making but, unlike most previous sequential decision tasks, it does not allow the decision variable to be updated incrementally. Here, the first word in a chunk, whether an L or N, does not contribute to the decision, and the decision variable should only be updated when the second word is compared with the first word. Since the decision variable is updated every other words, that is every chunk, it could potentially contribute to the chunk-rate response. Nevertheless, the bilateral topography of chunk-rate response is not compatible with the previous finding that decision-related responses concentrate in central MEG channels (<xref ref-type="bibr" rid="bib14">de Lange et al., 2010</xref>). Furthermore, the decision-related centro-parietal positivity (CPP) in EEG is shown to be equivalent to the P300 (<xref ref-type="bibr" rid="bib64">O'Connell et al., 2012</xref>; <xref ref-type="bibr" rid="bib77">Twomey et al., 2015</xref>) while the MEG counterpart of the P300 does not show a bilateral topography either (<xref ref-type="bibr" rid="bib57">Mecklinger et al., 1998</xref>). Therefore, the neural source of the chunk-rate response is not identical to the P300, the dominant decision signal in previous EEG/MEG studies, but it may still reflect sequential decision making since such responses are widely distributed as shown by intracranial neural recordings (<xref ref-type="bibr" rid="bib31">Gold and Shadlen, 2007</xref>).</p><p>The idea that different sequence chunking tasks involve common computational principles does not necessarily imply that these principles are implemented in a common neural network. Nevertheless, there is indeed evidence for domain-general neural networks for sequence chunking. For example, functional MRI studies show that ventrolateral prefrontal cortex, including the Broca’s area, is not only a core area for language processing but also activated by rule-based nonlinguistic sequential processing tasks (<xref ref-type="bibr" rid="bib44">Koechlin and Jubault, 2006</xref>; <xref ref-type="bibr" rid="bib62">Novick et al., 2005</xref>; <xref ref-type="bibr" rid="bib74">Thompson-Schill et al., 2005</xref>). There are also studies, however, arguing for domain-specific sequence processing mechanisms, especially for the processing of language (<xref ref-type="bibr" rid="bib24">Fedorenko et al., 2011</xref>). For example, it has also been shown that, for newly learned rules, rules of different complexity activate different parts of the frontal lobe, forming a posterior-to-anterior gradient (<xref ref-type="bibr" rid="bib1">Badre and Nee, 2018</xref>; <xref ref-type="bibr" rid="bib45">Koechlin and Summerfield, 2007</xref>). During language processing, however, syntactic rules of different complexity all activate Broca’s area (<xref ref-type="bibr" rid="bib39">Jeon and Friederici, 2013</xref>). These findings lead to the hypothesis that automatic processes and more controlled processes rely on distinct neural circuits (<xref ref-type="bibr" rid="bib40">Jeon and Friederici, 2015</xref>). Based on this hypothesis, the chunk-level task in the current study and syntactic analysis may engage different parts of the frontal lobe. With the spatial resolution of MEG, we cannot precisely localize the neural source of the chunk-rate response. However, it is found that the frontal lobe activation is bilateral with no clear lateralization between hemispheres (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>), in contrast to the clearly left lateralized sentence-tracking response (<xref ref-type="bibr" rid="bib16">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Sheng et al., 2019</xref>). Similar to the sentence-tracking response, the chunk-rate response is also stronger in the temporal lobe than in the frontal lobe, suggesting that both sentences and chunks defined by temporary rules can drive large-scale chunk-rate responses in the temporal lobe.</p><p>The results in the current study show that low-frequency neural activity primarily tracks rule-defined chunks instead of semantic relatedness. The semantic relatedness hypothesis, however, is based on solid evidence in the literature. It builds on the priming effect in the psychological literature (<xref ref-type="bibr" rid="bib76">Tulving and Schacter, 1990</xref>) and the neural adaptation effect in the neuroscience literature (<xref ref-type="bibr" rid="bib33">Grill-Spector et al., 2006</xref>), and is related to the predictive coding hypothesis (<xref ref-type="bibr" rid="bib2">Bar, 2007</xref>; <xref ref-type="bibr" rid="bib28">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib75">Tian and Poeppel, 2013</xref>). It is well established that if a word is preceded by a semantically related word, it is processed faster (<xref ref-type="bibr" rid="bib10">Collins and Loftus, 1988</xref>) and its neural response, especially the ERP N400 component and its MEG counterpart, is reduced (<xref ref-type="bibr" rid="bib8">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="bib48">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="bib51">Lau et al., 2009</xref>). In this study, words from the same semantic category are more closely related than words drawn from different categories. Nevertheless, the categories used here are broad categories (e.g. animals or plants). In general, words from a broad category, for example animals, are only weakly related compared with words from a narrower category, for example birds (<xref ref-type="bibr" rid="bib65">Quinn and Kinoshita, 2008</xref>; <xref ref-type="bibr" rid="bib78">Vigliocco et al., 2002</xref>). A weak relationship between words predicts a weak priming effect on the neural response (<xref ref-type="bibr" rid="bib23">Federmeier and Kutas, 1999</xref>), which may underlie why semantic relatedness between words does not drive a strong neural response.</p><p>Furthermore, previous studies have identified two kinds of semantic priming, that is automatic and strategic priming (<xref ref-type="bibr" rid="bib61">Neely, 1977</xref>). Automatic priming can be caused by, for example semantic relatedness between words in long-term memory. Strategic priming, however, can actively predict upcoming words based on temporally learned association rules. Behavioral experiments have demonstrated a cross-category priming effect if the prime word from one category, for example tools, is known to predict target words from a different category, for example animals (<xref ref-type="bibr" rid="bib61">Neely, 1977</xref>). In other words, participants can make use of association rules learned during an experiment to actively predict words that have no long-term semantic relationship with the prime word. Different from automatic priming that can occur with very short stimulus onset asynchrony (SOA) between words, strategic priming occurs when the SOA between the prime and target words is relatively long, e.g.,&gt;400 ms (<xref ref-type="bibr" rid="bib38">Hutchison, 2007</xref>).</p><p>In the current study, the SOA between words is 500 ms, allowing strategic priming to occur. Furthermore, since the chunking rule remains the same in each block, listeners can prepare in advance about how to parse the sequences, making strategic predictions to occur more easily. Based on the knowledge about valid chunks, the semantic category of the second word in each chunk is fully predictable in both the same-category condition and the different-category condition. The first word in each chunk is also predictable in the alternating-order sequences but not predictable in the random-order sequences. Since the alternating-order sequences and the random-order sequences are mixed, predictability is generally lower for the first word than for the second word in each chunk. Therefore, for strategic predictions, the predictability of words correlates with the chunk structure. This kind of strategic predictions, however, is based on rule-based chunking instead of semantic relatedness stored in long-term memory.</p><p>Finally, the chunk-rate response in the current study is consistent with the rule-based chunking model. However, it may reflect the actual chunking process or downstream processes building on the multi-word chunks. After the chunk structure is parsed, the listener could synchronize their attention and predictions to the sequence. Previous studies have suggested that entrained neural oscillations may reflect both sequence parsing (<xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Kösem et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Meyer and Gumbert, 2018</xref>; <xref ref-type="bibr" rid="bib58">Meyer et al., 2016</xref>; <xref ref-type="bibr" rid="bib79">Wang et al., 2017</xref>) and temporal attention/prediction (<xref ref-type="bibr" rid="bib42">Jin et al., 2018</xref>; <xref ref-type="bibr" rid="bib60">Morillon and Baillet, 2017</xref>; <xref ref-type="bibr" rid="bib67">Rimmele et al., 2018</xref>), and could causally modulate speech perception (<xref ref-type="bibr" rid="bib47">Kösem et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Riecke et al., 2018</xref>; <xref ref-type="bibr" rid="bib83">Zoefel et al., 2018</xref>). The current results cannot distinguish which chunk-related process drives the chunk-rate response. What can, however, be concluded here is that the chunk-rate response cannot be fully accounted by neural tracking of individual words. Thus, the current study and previous studies (<xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>) provide strong support to the notion that the brain can construct superordinate linguistic representations based on either long-term syntactic rules or temporary rules learned in an experiment.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Sixteen participants took part in the study (19–27 years old; mean age 22.6; eight female). All participants were right-handed, with no self-reported hearing loss or neurological disorders. The experimental procedures were approved by the Research Ethics Committee of the College of Medicine, Zhejiang University (2019–047) and the Research Ethics Committee of Peking University (2019-02-05). The participants provided written consent and were paid.</p></sec><sec id="s4-2"><title>Words and sentences</title><p>All words were disyllabic words in mandarin Chinese and each syllable was a morpheme. For the noun sequences, each word was selected from a pool of 240 disyllabic concrete nouns. These concrete nouns equally divided into two categories, that is living (L) and nonliving (N) things. Living things further divided into 2 subcategories, that is animals (<italic>N</italic> = 60; e.g., monkey, panda) and plants (<italic>N</italic> = 60; e.g., tulip, strawberry). Nonliving things also divided into two subcategories, that is small manipulatable objects (<italic>N</italic> = 60; e.g., teacup, toothbrush) and large non-manipulatable objects (<italic>N</italic> = 60; e.g., playground, hotel). In each noun sequence, all living things were randomly drawn from a subcategory, that is animals or plants, and all nonliving things were also randomly drawn from a subcategory, that is manipulatable or non-manipulatable objects. Details about how the nouns constructed noun sequences are provided in the <italic>Sequence Structure</italic> section.</p><p>Each disyllabic word was independently synthesized by the iFLYTEK synthesizer (<ext-link ext-link-type="uri" xlink:href="http://peiyin.xunfei.cn/">http://peiyin.xunfei.cn/</ext-link>; female voice, Xiaoying). All disyllabic words were adjusted to the same intensity and the same duration, that is 500 ms, following the procedure in <xref ref-type="bibr" rid="bib17">Ding et al. (2017)</xref>. Within a word, no additional control was applied to the intensity and duration of individual syllables and coarticulation could exist between these syllables. Compared with speech materials in which each syllable was independently synthesized, the disyllabic words synthesized as a whole sounded more natural.</p><p>When constructing sequences, the synthesized disyllabic words were directly concatenated, without any additional pause in between. Therefore, words were isochronously presented at 2 Hz. For speech stimuli generated according to this procedure, each disyllabic word was an acoustically independent unit and larger chunks consisting of multiple words had no acoustically defined boundaries.</p></sec><sec id="s4-3"><title>Sequence structures</title><p>Pairs of nouns constructed chunks and chunks further constructed sequences. The experiment compared two conditions in which the chunks were constructed based on different rules. For the same-category condition, the two nouns in each chunk belonged to the same semantic category. For the different-category condition, however, the two nouns in each chunk were from different semantic categories. Since the study only considered two categories of words, there were two valid chunks in the same-category condition, that is LL and NN, and two valid chunks in the different-category, that is NL and LN. Each chunk was 1 s in duration.</p><p>Each sequence consisted of 12 chunks and therefore was 12 s in duration. In each sequence, the two valid chunks were concatenated in either an alternating order or a random order (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The alternating-order sequence in each condition had a fixed structure, repeating a four-words unit six times, that is NNLL for the same-category condition and NLLN for the different-category condition. The repeating four-words unit led to the 0.5 Hz rhythm in the lexical property model (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In each random-order sequence, every chunk was randomly and independently chosen from the two valid chunks. After the category of each word was determined, the actual words were filled in. Each word was randomly drawn from a pool of 60 words (see <italic>Speech Materials</italic>), with an additional constraint that no word repeated in a sequence.</p><p>The alternating-order sequences had a highly regular structure, which led to a simple relationship between the alternating-order sequences in same- and different-category conditions: Any alternating-order sequence in the different-category condition could be converted to a same-category sequence by removing the first word in the sequence. Attributable to this property, the neural response phase could conveniently distinguish the word- and phrase-based models in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Nevertheless, this property also gave rise to an alternative strategy that could detect invalid chunks based on the same set of rules in both the same- and different-category conditions. For this strategy, the participants ignored the first word of each sequence in the different-category condition and treated the rest of the sequence as a same-category sequence. To eliminate this alternative strategy and to ensure that participants had to apply different rules in the same- and different-category conditions, the random-order sequences were designed as fillers to increase variability.</p></sec><sec id="s4-4"><title>Experimental procedures and tasks</title><p>Participants were familiarized with the synthesized words at the beginning of each experiment. In the familiarization session, after hearing a word, the participants pressed a key to see the word on a screen. Then, the participants could press one key to hear the word again or press another key to hear the next word.</p><p>In the MEG experiment, the same-category condition and the different-category condition were presented in separate blocks and the order of the two blocks was counterbalanced across participants. In each condition, 30 alternating-order sequences and 30 random-order sequences were mixed and presented in a random order. In eight alternating-order sequences and eight random-order sequences, a living noun in one chunk was switched with a nonliving noun in another chunk so that the two chunks were no longer valid (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). These 16 sequences with invalid chunks were called outlier sequences. The outlier sequences (<italic>N</italic> = 16) and normal sequences (<italic>N</italic> = 44) were mixed and presented in a random order. The participants had a rest after listening to 30 sequences.</p><p>Before MEG recording, participants received training. The same-category condition was trained first, followed by the different-category condition. For each condition, participants were explicitly instructed that the sequence was constructed by bi-word chunks and explicitly instructed about how valid chunks were constructed based on living/nonliving things. They were told that any chunk that violated the chunk construction rule was invalid chunks that they had to detect. After receiving instructions, participants were familiarized with two normal sequences, followed by two outlier sequences. When listening to the outlier sequences, they were asked to verbally report the invalid chunks as soon as they heard them. The sequence could be replayed upon request. The participants then went through a practice session, which was the same as the MEG experiment, except that it was carried outside the MEG scanner.</p><p>During the practice session and during the MEG experiment, participants had to distinguish normal and outlier sequences and indicated their decisions by pressing different keys at the end of each sequence. After the key press, the next sequence was presented after a silent interval randomized between 1 s and 2 s (uniform distribution). The practice session ended after the participants made four correct responses in five consecutive sequences. The MEG experiment started after participants finished the practice session for the different-category condition.</p></sec><sec id="s4-5"><title>Data acquisition</title><p>Neuromagnetic responses were recorded using a 306-sensor whole-head MEG system (Elekta-Neuromag, Helsinki, Finland) at Peking University, sampled at 1 kHz. The system had 102 magnetometer and 204 planar gradiometers. Four MEG-compatible electrodes were used to record EOG at 1000 Hz. To remove ocular artifacts in MEG, the horizontal and vertical EOG were regressed out from the recordings using the least-squares method. Four head position indicator (HPI) coils were used to measure the head position inside MEG. The positions of three anatomical landmarks (nasion, left, and right pre-auricular points), the four HPI coils, and at least 200 points on the scalp were also digitized before experiment. For MEG source localization purposes, structural Magnetic Resonance Imaging (MRI) data were collected from all participants using a Siemens Magnetom Prisma 3 T MRI system (Siemens Medical Solutions, Erlangen, Germany) at Peking University. A 3-D magnetization-prepared rapid gradient echo T1-weighted sequence was used to obtain 1 × 1 × 1 mm<sup>3</sup> resolution anatomical images.</p></sec><sec id="s4-6"><title>Data processing</title><p>In each condition, normal and outlier sequences were mixed and presented in a random order. However, only the neural responses to normal sequences were analyzed. Temporal Signal Space Separation (tSSS) was used to remove the external interference from MEG signals (<xref ref-type="bibr" rid="bib73">Taulu and Hari, 2009</xref>). Since the current study only focused on responses at 0.5 Hz, 1 Hz, and 2 Hz, the MEG signals were bandpass filtered between 0.3 and 2.7 Hz using a linear-phase finite impulse response (FIR) filter (−6 dB attenuation at the cut-off frequencies, 10 s Hamming window). The frequency response curve of the FIR filter was compensated in the response spectrum.</p><p>The response during each sequence was extracted, downsampled to 20 Hz sampling rate, and was referred to as a trial.</p><p>The MEG signals were further denoised using a semi-blind source separation technique, the Denoising Source Separation (DSS). The DSS was a linear transform that decomposed multi-sensor MEG signals into components (<xref ref-type="bibr" rid="bib13">de Cheveigné and Parra, 2014</xref>). The bias function of the DSS was chosen as the response averaged over trials within each condition. A common DSS for all conditions was derived based on the response covariance matrices averaged over conditions. The first six DSS components were retained and transformed back to the sensor space for further analysis. This DSS procedure was commonly used to extract cortical responses entrained to speech (<xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib82">Zhang and Ding, 2017</xref>).</p><p>To illustrate the response waveform, the PCA was employed to transform the 306-channel MEG data into components. Responses in the same- and different-category conditions and responses to the alternating- and random-order sequences were pooled in the PCA analysis. The first two PC were shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref> and the 2<sup>nd</sup> PC, which captured the 1 Hz response, was filtered around 1 Hz and shown in <xref ref-type="fig" rid="fig3">Figure 3F</xref> (FIR filter with 2 s Hamming window, cut-off frequency: 0.75 and 1.25 Hz).</p></sec><sec id="s4-7"><title>Frequency-domain analysis</title><p>In the frequency-domain analysis, to avoid the response to the sound onset, the response during the first two seconds of each trial were removed. Consequently, the neural response was 10 s in duration for each trial. The average of all trials was transformed into the frequency domain using the Discrete Fourier Transform (DFT) without any additional smoothing window. The frequency resolution of the DFT analysis was 1/10 Hz. If the complex-valued DFT coefficient at frequency <italic>f</italic> was denoted as <italic>X</italic>(<italic>f</italic>), the response power and phase were |<italic>X</italic>(<italic>f</italic>)|<sup>2</sup> and ∠<italic>X</italic>(<italic>f</italic>), respectively. The DFT was separately applied to each MEG sensor. For the MEG response power analysis, responses from the two collocated gradiometers were always averaged. When showing the spectrum, all MEG gradiometers were averaged. For the phase analysis, all magnetometers and gradiometers were separately analyzed. The circular mean was used to average the neural response phase over participants or sensors.</p></sec><sec id="s4-8"><title>Source localization</title><p>The MEG responses averaged over trials were mapped into source space using cortex constrained minimum norm estimate (MNE) (<xref ref-type="bibr" rid="bib36">Hämäläinen and Ilmoniemi, 1994</xref>), implemented in the Brainstorm software (<xref ref-type="bibr" rid="bib72">Tadel et al., 2011</xref>). The T1-weighted MRI images were used to extract the brain volume, cortex surface, and innermost skull surface using the Freesurfer software (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>). In the MRI images, the three anatomical landmarks (nasion, left, and right pre-auricular points) were marked manually. Both three anatomical landmarks and digitized head points were used to align the MRI images with MEG sensor array. The forward MEG model was derived based on the overlapping sphere model (<xref ref-type="bibr" rid="bib37">Huang et al., 1999</xref>). The identity matrix was used as noise covariance. Source-space activation was measured by the dynamic statistical parametric map (dSPM) (<xref ref-type="bibr" rid="bib12">Dale et al., 2000</xref>) and the value was in arbitrary unit (a.u.). Individual source-space responses, consisting of 15,002 elementary dipoles over the cortex, was rescaled to the ICBM 152 brain template (<xref ref-type="bibr" rid="bib25">Fonov et al., 2011</xref>) for further analyses.</p></sec><sec id="s4-9"><title>Source-space region of interest (ROI) analysis</title><p>Two ROIs were defined in each hemisphere. A frontal-lobe ROI included the pars opercularis and pars triangularis, and a temporal-lobe ROI included the superior temporal area. Anatomical areas are defined according to an automated landmark-based registration algorithm (<xref ref-type="bibr" rid="bib15">Desikan et al., 2006</xref>). In source space, the response of all dipoles were transformed to the frequency domain, and the 90th percentile of response power was calculated for each ROI, at each frequency. When comparing the response between ROIs, the results were averaged over the alternating-order and random-order sequences and over the same-category and different-category conditions.</p></sec><sec id="s4-10"><title>Model simulations</title><p>Pulse sequence: In all three models, the smallest unit being considered was the word, and the model output was updated word by word. Therefore, in the simulations, each model was first simulated using a pulse sequence (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>), in which a pulse was placed at the onset of each word and the pulse amplitude was described in the following. The lexical property model and the semantic relatedness model were simulated based on lexical features. For the model illustrated in <xref ref-type="fig" rid="fig2">Figure 2</xref>, only two features were considered, that is living and nonliving things. Each feature took a binary value, that is one when the feature was present and 0 otherwise, and the pulse amplitude for each word equaled this binary value. The semantic relatedness model built on the lexical property model: The semantic relatedness between the current word and the previous word was characterized by the correlation coefficient between lexical representations (<xref ref-type="bibr" rid="bib8">Broderick et al., 2018</xref>). The correlation coefficient was a scalar. Additionally, since the neural response to a stimulus is usually weaker instead of stronger if the stimulus is preceded by a similar stimulus, we used one minus the correlation coefficient to modulate a pulse sequence. In the rule-based chunking model, pulses of unit amplitude were placed at the chunk onset.</p></sec><sec id="s4-11"><title>Simulate neural response waveform</title><p>The neural responses were smooth waveforms rather than sharp pulses. Therefore, neural response waveforms were further simulated by convolving the pulse sequence with a response function, which was a 500 ms duration Gaussian window. Here, the rule-based chunking model was simulated by a response time locked to the chunk onset. In general, however, the model only assumed a consistent response within the duration of a chunk. Results in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref> confirmed that the key predictions of the model were not affected by the waveforms.</p></sec><sec id="s4-12"><title>Statistical tests</title><sec id="s4-12-1"><title>Spectral peak</title><p>An F-test was used to test if the power at a target frequency <italic>f</italic><sub>T</sub> was significantly higher than the power in neighboring frequency bins (one bin on each side). The power ratio was defined as<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mo>Δ</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>Δ</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>X</italic> was the complex-valued DFT coefficient defined in <italic>Frequency-domain analysis</italic> section, <italic>f</italic><sub>T</sub>±<italic>Δf</italic> denoted the two neighboring frequency bins, and <italic>k</italic> denoted data from the <italic>k</italic><sup>th</sup> participant.</p><p>Under the null hypothesis, that is no difference between the power at the target and neighboring frequency bins, the power ratio was subject to an F(2<italic>N</italic>,4<italic>N</italic>) distribution for a single-sensor recording averaged over <italic>N</italic> independent participants (<xref ref-type="bibr" rid="bib19">Dobie and Wilson, 1996</xref>). Here, response power was calculated over sensors or dipoles and the degree of freedom would further increase if the sensors were not fully correlated. However, since it was difficult to quantify the increase in degree of freedom, we conservatively assumed that the power ratio remained following an F(2<italic>N</italic>,4<italic>N</italic>) distribution. The significance test was applied to the response power at the 0.5 Hz, 1 Hz and 2 Hz. A false discovery rate (FDR) correction was applied to these frequencies.</p></sec><sec id="s4-12-2"><title>Power difference between conditions</title><p>The F-test was used to compare the power at a target frequency between conditions. The response power comparison was performed between two conditions in which the response power was both significant at target frequency. The power ratio between conditions was subject to an F(2<italic>N</italic>,2<italic>N</italic>) distribution, where <italic>N</italic> was the number of participants. Only in the ROI analysis, since the responses were averaged over the alternating-order and random-order sequences, and over the same-category and different-category conditions, the power ratio test was based on an F(8<italic>N</italic>,8<italic>N</italic>) distribution.</p></sec><sec id="s4-12-3"><title>Response phase</title><p>A test based on bias-corrected and accelerated bootstrap (<xref ref-type="bibr" rid="bib20">Efron and Tibshirani, 1994</xref>) was used to test whether the response phase difference was closer to 0° or 180°. In the bootstrap procedure, all the participants were resampled with replacement 100,000 times. The test was two-sided. If the resampled phase difference was closer to 0° for <italic>A</italic> times, the significance level was 2 min(<italic>A</italic> + 1, 100001−<italic>A</italic>)/100001. Furthermore, the 99% confidence interval of the phase difference was also calculated based on the resampled data. It was measured by the smallest angle that could cover 99% of the resampled phase difference.</p></sec><sec id="s4-12-4"><title>Post-hoc effect size calculation</title><p>On top of the showing individual results in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>, an effect size analysis was applied to the 1 Hz spectral peak to validate that the sample size was appropriate. In this analysis, we applied a paired t-test to compare the power at the target frequency and the power averaged over two neighboring frequencies (both in dB scales). Such a t-test had weaker power than the F-test (<xref ref-type="bibr" rid="bib19">Dobie and Wilson, 1996</xref>) but was easy to calculate the effect size using the G*Power Version 3.1 (<xref ref-type="bibr" rid="bib22">Faul et al., 2007</xref>). We calculated d and Power based on the mean and standard deviation (reported in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). For the effect size observed in the data set, the study was powerful with the described sample population and the α level of 0.05.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Cheng Cheng for excellent assistant in data collection, Jiajie Zou for discussion and generation some of the speech materials, Stefan L Frank, Lucia Melloni, Lang Qin, and David Poeppel for thoughtful comments on previous versions of the manuscript. Work supported by National Natural Science Foundation of China 31771248 (ND), Major Scientific Research Project of Zhejiang Lab 2019KB0AC02 (ND), Zhejiang Provincial Natural Science Foundation of China LY20C090008 (PJ), and Fundamental Research Funds for the Central Universities (ND).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Validation, Visualization</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Funding acquisition, Validation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The experimental procedures were approved by the Research Ethics Committee of the College of Medicine, Zhejiang University (2019-047) and the Research Ethics Committee of Peking University (2019-02-05). The participants provided written consent and were paid.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="sdata1"><label>Source data 1.</label><caption><title>Preprocessed MEG data and analysis code.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-55613-data1-v2.zip"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>1 Hz power effect size.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-55613-supp1-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-55613-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The MEG data and analysis code (in MatLab) are available in Source data 1.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname> <given-names>D</given-names></name><name><surname>Nee</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Frontal cortex and the hierarchical control of behavior</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>170</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.11.005</pub-id><pub-id pub-id-type="pmid">29229206</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The proactive brain: using analogies and associations to generate predictions</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>280</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.05.005</pub-id><pub-id pub-id-type="pmid">17548232</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barascud</surname> <given-names>N</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns</article-title><source>PNAS</source><volume>113</volume><fpage>E616</fpage><lpage>E625</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508523113</pub-id><pub-id pub-id-type="pmid">26787854</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bemis</surname> <given-names>DK</given-names></name><name><surname>Pylkkanen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Basic linguistic composition recruits the left anterior temporal lobe and left angular gyrus during both listening and reading</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>1859</fpage><lpage>1873</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs170</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname> <given-names>M</given-names></name><name><surname>Plaut</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Doing without schema hierarchies: a recurrent connectionist approach to normal and impaired routine sequential action</article-title><source>Psychological Review</source><volume>111</volume><fpage>395</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.111.2.395</pub-id><pub-id pub-id-type="pmid">15065915</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brennan</surname> <given-names>J</given-names></name><name><surname>Nir</surname> <given-names>Y</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name><name><surname>Pylkkänen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Syntactic structure building in the anterior temporal lobe during natural story listening</article-title><source>Brain and Language</source><volume>120</volume><fpage>163</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2010.04.002</pub-id><pub-id pub-id-type="pmid">20472279</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname> <given-names>C</given-names></name><name><surname>Hong</surname> <given-names>LE</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rapid transformation from auditory to linguistic representations of continuous speech</article-title><source>Current Biology</source><volume>28</volume><fpage>3976</fpage><lpage>3983</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.10.042</pub-id><pub-id pub-id-type="pmid">30503620</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broderick</surname> <given-names>MP</given-names></name><name><surname>Anderson</surname> <given-names>AJ</given-names></name><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>Crosse</surname> <given-names>MJ</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Electrophysiological correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech</article-title><source>Current Biology</source><volume>28</volume><fpage>803</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.080</pub-id><pub-id pub-id-type="pmid">29478856</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chait</surname> <given-names>M</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>de Cheveigné</surname> <given-names>A</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Processing asymmetry of transitions between order and disorder in human auditory cortex</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>5207</fpage><lpage>5214</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0318-07.2007</pub-id><pub-id pub-id-type="pmid">17494707</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>AM</given-names></name><name><surname>Loftus</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>A Spreading-Activation theory of semantic processing</article-title><source>Readings in Cognitive Science</source><volume>82</volume><fpage>407</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1037//0033-295X.82.6.407</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooper</surname> <given-names>RP</given-names></name><name><surname>Shallice</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Hierarchical schemas and goals in the control of sequential behavior</article-title><source>Psychological Review</source><volume>113</volume><fpage>887</fpage><lpage>916</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.887</pub-id><pub-id pub-id-type="pmid">17014307</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Liu</surname> <given-names>AK</given-names></name><name><surname>Fischl</surname> <given-names>BR</given-names></name><name><surname>Buckner</surname> <given-names>RL</given-names></name><name><surname>Belliveau</surname> <given-names>JW</given-names></name><name><surname>Lewine</surname> <given-names>JD</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamic statistical parametric mapping: combining fMRI and MEG for high-resolution imaging of cortical activity</article-title><source>Neuron</source><volume>26</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)81138-1</pub-id><pub-id pub-id-type="pmid">10798392</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname> <given-names>A</given-names></name><name><surname>Parra</surname> <given-names>LC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Joint decorrelation, a versatile tool for multichannel data analysis</article-title><source>NeuroImage</source><volume>98</volume><fpage>487</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.05.068</pub-id><pub-id pub-id-type="pmid">24990357</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname> <given-names>FP</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Accumulation of evidence during sequential decision making: the importance of top-down factors</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>731</fpage><lpage>738</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4080-09.2010</pub-id><pub-id pub-id-type="pmid">20071538</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname> <given-names>RS</given-names></name><name><surname>Ségonne</surname> <given-names>F</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Quinn</surname> <given-names>BT</given-names></name><name><surname>Dickerson</surname> <given-names>BC</given-names></name><name><surname>Blacker</surname> <given-names>D</given-names></name><name><surname>Buckner</surname> <given-names>RL</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Maguire</surname> <given-names>RP</given-names></name><name><surname>Hyman</surname> <given-names>BT</given-names></name><name><surname>Albert</surname> <given-names>MS</given-names></name><name><surname>Killiany</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><volume>31</volume><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Melloni</surname> <given-names>L</given-names></name><name><surname>Zhang</surname> <given-names>H</given-names></name><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>158</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1038/nn.4186</pub-id><pub-id pub-id-type="pmid">26642090</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Melloni</surname> <given-names>L</given-names></name><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Rule-based and Word-level Statistics-based processing of language: insights from neuroscience</article-title><source>Language, Cognition and Neuroscience</source><volume>32</volume><fpage>570</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1080/23273798.2016.1215477</pub-id><pub-id pub-id-type="pmid">29399592</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Pan</surname> <given-names>X</given-names></name><name><surname>Luo</surname> <given-names>C</given-names></name><name><surname>Su</surname> <given-names>N</given-names></name><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Zhang</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Attention is required for Knowledge-Based sequential grouping: insights from the integration of syllables into words</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>1178</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2606-17.2017</pub-id><pub-id pub-id-type="pmid">29255005</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobie</surname> <given-names>RA</given-names></name><name><surname>Wilson</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A comparison o<italic>f t</italic> tes<italic>t</italic>, <italic>F</italic> tes<italic>t</italic>, and coherence methods o<italic>f</italic> detecting steady‐state auditory‐evoked potentials, distortion‐produc<italic>t</italic> otoacoustic emissions, or other sinusoids</article-title><source>The Journal of the Acoustical Society of America</source><volume>100</volume><fpage>2236</fpage><lpage>2246</lpage><pub-id pub-id-type="doi">10.1121/1.417933</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Efron</surname> <given-names>B</given-names></name><name><surname>Tibshirani</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>An Introduction to the Bootstrap</source><publisher-name>CRC press</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everaert</surname> <given-names>MBH</given-names></name><name><surname>Huybregts</surname> <given-names>MAC</given-names></name><name><surname>Chomsky</surname> <given-names>N</given-names></name><name><surname>Berwick</surname> <given-names>RC</given-names></name><name><surname>Bolhuis</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Structures, not strings: linguistics as part of the cognitive sciences</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>729</fpage><lpage>743</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.09.008</pub-id><pub-id pub-id-type="pmid">26564247</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faul</surname> <given-names>F</given-names></name><name><surname>Erdfelder</surname> <given-names>E</given-names></name><name><surname>Lang</surname> <given-names>AG</given-names></name><name><surname>Buchner</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>G*power 3: a flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title><source>Behavior Research Methods</source><volume>39</volume><fpage>175</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.3758/BF03193146</pub-id><pub-id pub-id-type="pmid">17695343</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federmeier</surname> <given-names>KD</given-names></name><name><surname>Kutas</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A rose by any other name: long-term memory structure and sentence processing</article-title><source>Journal of Memory and Language</source><volume>41</volume><fpage>469</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1006/jmla.1999.2660</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname> <given-names>E</given-names></name><name><surname>Behr</surname> <given-names>MK</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specificity for high-level linguistic processing in the human brain</article-title><source>PNAS</source><volume>108</volume><fpage>16428</fpage><lpage>16433</lpage><pub-id pub-id-type="doi">10.1073/pnas.1112937108</pub-id><pub-id pub-id-type="pmid">21885736</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname> <given-names>V</given-names></name><name><surname>Evans</surname> <given-names>AC</given-names></name><name><surname>Botteron</surname> <given-names>K</given-names></name><name><surname>Almli</surname> <given-names>CR</given-names></name><name><surname>McKinstry</surname> <given-names>RC</given-names></name><name><surname>Collins</surname> <given-names>DL</given-names></name><collab>Brain Development Cooperative Group</collab></person-group><year iso-8601-date="2011">2011</year><article-title>Unbiased average age-appropriate atlases for pediatric studies</article-title><source>NeuroImage</source><volume>54</volume><fpage>313</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.033</pub-id><pub-id pub-id-type="pmid">20656036</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>SL</given-names></name><name><surname>Bod</surname> <given-names>R</given-names></name><name><surname>Christiansen</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How hierarchical is language use?</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>279</volume><fpage>4522</fpage><lpage>4531</lpage><pub-id pub-id-type="doi">10.1098/rspb.2012.1741</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>SL</given-names></name><name><surname>Yang</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Lexical representation explains cortical entrainment during speech comprehension</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0197304</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0197304</pub-id><pub-id pub-id-type="pmid">29771964</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuster</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The prefrontal cortex--an update: time is of the essence</article-title><source>Neuron</source><volume>30</volume><fpage>319</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00285-9</pub-id><pub-id pub-id-type="pmid">11394996</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geddes</surname> <given-names>CE</given-names></name><name><surname>Li</surname> <given-names>H</given-names></name><name><surname>Jin</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Optogenetic editing reveals the hierarchical organization of learned action sequences</article-title><source>Cell</source><volume>174</volume><fpage>32</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.06.012</pub-id><pub-id pub-id-type="pmid">29958111</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname> <given-names>JI</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goucha</surname> <given-names>T</given-names></name><name><surname>Zaccarella</surname> <given-names>E</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A revival of Homo loquens as a builder of labeled structures: neurocognitive considerations</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>81</volume><fpage>213</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.01.036</pub-id><pub-id pub-id-type="pmid">28318539</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname> <given-names>K</given-names></name><name><surname>Henson</surname> <given-names>R</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Repetition and the brain: neural models of stimulus-specific effects</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>14</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.11.006</pub-id><pub-id pub-id-type="pmid">16321563</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grodzinsky</surname> <given-names>Y</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neuroimaging of syntax and syntactic processing</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>240</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.03.007</pub-id><pub-id pub-id-type="pmid">16563739</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grodzinsky</surname> <given-names>Y</given-names></name><name><surname>Santi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The battle for broca's region</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>474</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.09.001</pub-id><pub-id pub-id-type="pmid">18930695</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hämäläinen</surname> <given-names>MS</given-names></name><name><surname>Ilmoniemi</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Interpreting magnetic fields of the brain: minimum norm estimates</article-title><source>Medical &amp; Biological Engineering &amp; Computing</source><volume>32</volume><fpage>35</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1007/BF02512476</pub-id><pub-id pub-id-type="pmid">8182960</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>MX</given-names></name><name><surname>Mosher</surname> <given-names>JC</given-names></name><name><surname>Leahy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A sensor-weighted overlapping-sphere head model and exhaustive head model comparison for MEG</article-title><source>Physics in Medicine and Biology</source><volume>44</volume><fpage>423</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/44/2/010</pub-id><pub-id pub-id-type="pmid">10070792</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutchison</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Attentional control and the relatedness proportion effect in semantic priming</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>33</volume><fpage>645</fpage><lpage>662</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.33.4.645</pub-id><pub-id pub-id-type="pmid">17576145</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeon</surname> <given-names>HA</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Two principles of organization in the prefrontal cortex are cognitive hierarchy and degree of automaticity</article-title><source>Nature Communications</source><volume>4</volume><elocation-id>2041</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms3041</pub-id><pub-id pub-id-type="pmid">23787807</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeon</surname> <given-names>HA</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Degree of automaticity and the prefrontal cortex</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>244</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.03.003</pub-id><pub-id pub-id-type="pmid">25843542</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>X</given-names></name><name><surname>Long</surname> <given-names>T</given-names></name><name><surname>Cao</surname> <given-names>W</given-names></name><name><surname>Li</surname> <given-names>J</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Wang</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Production of Supra-regular spatial sequences by macaque monkeys</article-title><source>Current Biology</source><volume>28</volume><fpage>1851</fpage><lpage>1859</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.047</pub-id><pub-id pub-id-type="pmid">29887304</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname> <given-names>P</given-names></name><name><surname>Zou</surname> <given-names>J</given-names></name><name><surname>Zhou</surname> <given-names>T</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Eye activity tracks task-relevant structures during speech and auditory sequence perception</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>5374</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-07773-y</pub-id><pub-id pub-id-type="pmid">30560906</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname> <given-names>A</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004473</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004473</pub-id><pub-id pub-id-type="pmid">29529019</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname> <given-names>E</given-names></name><name><surname>Jubault</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Broca's area and the hierarchical organization of human behavior</article-title><source>Neuron</source><volume>50</volume><fpage>963</fpage><lpage>974</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.05.017</pub-id><pub-id pub-id-type="pmid">16772176</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname> <given-names>E</given-names></name><name><surname>Summerfield</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>An information theoretical approach to prefrontal executive function</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>229</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.04.005</pub-id><pub-id pub-id-type="pmid">17475536</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kösem</surname> <given-names>A</given-names></name><name><surname>Basirat</surname> <given-names>A</given-names></name><name><surname>Azizi</surname> <given-names>L</given-names></name><name><surname>van Wassenhove</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>High-frequency neural activity predicts word parsing in ambiguous speech streams</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>2497</fpage><lpage>2512</lpage><pub-id pub-id-type="doi">10.1152/jn.00074.2016</pub-id><pub-id pub-id-type="pmid">27605528</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kösem</surname> <given-names>A</given-names></name><name><surname>Bosker</surname> <given-names>HR</given-names></name><name><surname>Takashima</surname> <given-names>A</given-names></name><name><surname>Meyer</surname> <given-names>A</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>Hagoort</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural entrainment determines the words we hear</article-title><source>Current Biology</source><volume>28</volume><fpage>2867</fpage><lpage>2875</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.07.023</pub-id><pub-id pub-id-type="pmid">30197083</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname> <given-names>M</given-names></name><name><surname>Federmeier</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP)</article-title><source>Annual Review of Psychology</source><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id><pub-id pub-id-type="pmid">20809790</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lashley</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="1951">1951</year><chapter-title>the problem of serial order in behavior</chapter-title><source>Cerebral Mechanisms in Behavior, the Hixon Symposium</source><publisher-loc>New York</publisher-loc><publisher-name>Wiley</publisher-name><fpage>112</fpage><lpage>136</lpage></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname> <given-names>EF</given-names></name><name><surname>Phillips</surname> <given-names>C</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A cortical network for semantics: (de)constructing the N400</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>920</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nrn2532</pub-id><pub-id pub-id-type="pmid">19020511</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname> <given-names>E</given-names></name><name><surname>Almeida</surname> <given-names>D</given-names></name><name><surname>Hines</surname> <given-names>PC</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A lexical basis for N400 context effects: evidence from MEG</article-title><source>Brain and Language</source><volume>111</volume><fpage>161</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2009.08.007</pub-id><pub-id pub-id-type="pmid">19815267</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>W</given-names></name><name><surname>Yang</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Perception of prosodic hierarchical boundaries in mandarin chinese sentences</article-title><source>Neuroscience</source><volume>158</volume><fpage>1416</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2008.10.065</pub-id><pub-id pub-id-type="pmid">19111906</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>MA</given-names></name><name><surname>Jin</surname> <given-names>DZ</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Support for a synaptic chain model of neuronal sequence generation</article-title><source>Nature</source><volume>468</volume><fpage>394</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1038/nature09514</pub-id><pub-id pub-id-type="pmid">20972420</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makov</surname> <given-names>S</given-names></name><name><surname>Sharon</surname> <given-names>O</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Ben-Shachar</surname> <given-names>M</given-names></name><name><surname>Nir</surname> <given-names>Y</given-names></name><name><surname>Zion Golumbic</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sleep disrupts High-Level speech parsing despite significant basic auditory processing</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>7772</fpage><lpage>7781</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0168-17.2017</pub-id><pub-id pub-id-type="pmid">28626013</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname> <given-names>V</given-names></name><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>AE</given-names></name><name><surname>Doumas</surname> <given-names>LAA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A mechanism for the cortical computation of hierarchical linguistic structure</article-title><source>PLOS Biology</source><volume>15</volume><elocation-id>e200066</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2000663</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mecklinger</surname> <given-names>A</given-names></name><name><surname>Maess</surname> <given-names>B</given-names></name><name><surname>Opitz</surname> <given-names>B</given-names></name><name><surname>Pfeifer</surname> <given-names>E</given-names></name><name><surname>Cheyne</surname> <given-names>D</given-names></name><name><surname>Weinberg</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A MEG analysis of the P300 in visual discrimination tasks</article-title><source>Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section</source><volume>108</volume><fpage>45</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/S0168-5597(97)00092-0</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>L</given-names></name><name><surname>Henry</surname> <given-names>MJ</given-names></name><name><surname>Gaston</surname> <given-names>P</given-names></name><name><surname>Schmuck</surname> <given-names>N</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Linguistic Bias modulates interpretation of speech via neural Delta-Band oscillations</article-title><source>Cerebral Cortex</source><volume>9</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw228</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>L</given-names></name><name><surname>Gumbert</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Synchronization of electrophysiological responses with speech benefits syntactic information processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1066</fpage><lpage>1074</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01236</pub-id><pub-id pub-id-type="pmid">29324074</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morillon</surname> <given-names>B</given-names></name><name><surname>Baillet</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Motor origin of temporal predictions in auditory attention</article-title><source>PNAS</source><volume>114</volume><fpage>E8913</fpage><lpage>E8921</lpage><pub-id pub-id-type="doi">10.1073/pnas.1705373114</pub-id><pub-id pub-id-type="pmid">28973923</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neely</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Semantic priming and retrieval from lexical memory: roles of inhibitionless spreading activation and limited-capacity attention</article-title><source>Journal of Experimental Psychology: General</source><volume>106</volume><fpage>226</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.106.3.226</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novick</surname> <given-names>JM</given-names></name><name><surname>Trueswell</surname> <given-names>JC</given-names></name><name><surname>Thompson-Schill</surname> <given-names>SL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Cognitive control and parsing: reexamining the role of broca's area in sentence comprehension</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>5</volume><fpage>263</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.3758/CABN.5.3.263</pub-id><pub-id pub-id-type="pmid">16396089</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname> <given-names>S</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name><name><surname>Missal</surname> <given-names>M</given-names></name><name><surname>Mouraux</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Tagging the neuronal entrainment to beat and meter</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>10234</fpage><lpage>10240</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0411-11.2011</pub-id><pub-id pub-id-type="pmid">21753000</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Connell</surname> <given-names>RG</given-names></name><name><surname>Dockree</surname> <given-names>PM</given-names></name><name><surname>Kelly</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A supramodal accumulation-to-bound signal that determines perceptual decisions in humans</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1729</fpage><lpage>1735</lpage><pub-id pub-id-type="doi">10.1038/nn.3248</pub-id><pub-id pub-id-type="pmid">23103963</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname> <given-names>WM</given-names></name><name><surname>Kinoshita</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Congruence effect in semantic categorization with masked primes with narrow and broad categories</article-title><source>Journal of Memory and Language</source><volume>58</volume><fpage>286</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2007.03.004</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riecke</surname> <given-names>L</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name><name><surname>Sorger</surname> <given-names>B</given-names></name><name><surname>Başkent</surname> <given-names>D</given-names></name><name><surname>Gaudrain</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural entrainment to speech modulates speech intelligibility</article-title><source>Current Biology</source><volume>28</volume><fpage>161</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.11.033</pub-id><pub-id pub-id-type="pmid">29290557</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimmele</surname> <given-names>JM</given-names></name><name><surname>Morillon</surname> <given-names>B</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Arnal</surname> <given-names>LH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Proactive sensing of periodic and aperiodic auditory patterns</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>870</fpage><lpage>882</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.08.003</pub-id><pub-id pub-id-type="pmid">30266147</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decision making and sequential sampling from memory</article-title><source>Neuron</source><volume>90</volume><fpage>927</fpage><lpage>939</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.036</pub-id><pub-id pub-id-type="pmid">27253447</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheng</surname> <given-names>J</given-names></name><name><surname>Zheng</surname> <given-names>L</given-names></name><name><surname>Lyu</surname> <given-names>B</given-names></name><name><surname>Cen</surname> <given-names>Z</given-names></name><name><surname>Qin</surname> <given-names>L</given-names></name><name><surname>Tan</surname> <given-names>LH</given-names></name><name><surname>Huang</surname> <given-names>MX</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Gao</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The cortical maps of hierarchical linguistic structures during speech perception</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>3232</fpage><lpage>3240</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy191</pub-id><pub-id pub-id-type="pmid">30137249</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shultz</surname> <given-names>TR</given-names></name><name><surname>Pilon</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Development of the ability to detect linguistic ambiguity</article-title><source>Child Development</source><volume>44</volume><fpage>728</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.2307/1127716</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinhauer</surname> <given-names>K</given-names></name><name><surname>Alter</surname> <given-names>K</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Brain potentials indicate immediate use of prosodic cues in natural speech processing</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>191</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1038/5757</pub-id><pub-id pub-id-type="pmid">10195205</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadel</surname> <given-names>F</given-names></name><name><surname>Baillet</surname> <given-names>S</given-names></name><name><surname>Mosher</surname> <given-names>JC</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Leahy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Brainstorm: a User-Friendly application for MEG/EEG analysis</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1155/2011/879716</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taulu</surname> <given-names>S</given-names></name><name><surname>Hari</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Removal of magnetoencephalographic artifacts with temporal signal-space separation: demonstration with single-trial auditory-evoked responses</article-title><source>Human Brain Mapping</source><volume>30</volume><fpage>1524</fpage><lpage>1534</lpage><pub-id pub-id-type="doi">10.1002/hbm.20627</pub-id><pub-id pub-id-type="pmid">18661502</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson-Schill</surname> <given-names>SL</given-names></name><name><surname>Bedny</surname> <given-names>M</given-names></name><name><surname>Goldberg</surname> <given-names>RF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The frontal lobes and the regulation of mental activity</article-title><source>Current Opinion in Neurobiology</source><volume>15</volume><fpage>219</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2005.03.006</pub-id><pub-id pub-id-type="pmid">15831406</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The effect of imagination on stimulation: the functional specificity of efference copies in speech processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1020</fpage><lpage>1036</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00381</pub-id><pub-id pub-id-type="pmid">23469885</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tulving</surname> <given-names>E</given-names></name><name><surname>Schacter</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Priming and human memory systems</article-title><source>Science</source><volume>247</volume><fpage>301</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1126/science.2296719</pub-id><pub-id pub-id-type="pmid">2296719</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twomey</surname> <given-names>DM</given-names></name><name><surname>Murphy</surname> <given-names>PR</given-names></name><name><surname>Kelly</surname> <given-names>SP</given-names></name><name><surname>O'Connell</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The classic P300 encodes a build-to-threshold decision variable</article-title><source>European Journal of Neuroscience</source><volume>42</volume><fpage>1636</fpage><lpage>1643</lpage><pub-id pub-id-type="doi">10.1111/ejn.12936</pub-id><pub-id pub-id-type="pmid">25925534</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vigliocco</surname> <given-names>G</given-names></name><name><surname>Vinson</surname> <given-names>DP</given-names></name><name><surname>Damian</surname> <given-names>MF</given-names></name><name><surname>Levelt</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Semantic distance effects on object and action naming</article-title><source>Cognition</source><volume>85</volume><fpage>B61</fpage><lpage>B69</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(02)00107-5</pub-id><pub-id pub-id-type="pmid">12169413</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>FH</given-names></name><name><surname>Zevin</surname> <given-names>JD</given-names></name><name><surname>Mintz</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Top-down structure influences learning of nonadjacent dependencies in an artificial language</article-title><source>Journal of Experimental Psychology: General</source><volume>146</volume><fpage>1738</fpage><lpage>1748</lpage><pub-id pub-id-type="doi">10.1037/xge0000384</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zacks</surname> <given-names>JM</given-names></name><name><surname>Braver</surname> <given-names>TS</given-names></name><name><surname>Sheridan</surname> <given-names>MA</given-names></name><name><surname>Donaldson</surname> <given-names>DI</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Ollinger</surname> <given-names>JM</given-names></name><name><surname>Buckner</surname> <given-names>RL</given-names></name><name><surname>Raichle</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Human brain activity time-locked to perceptual event boundaries</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>651</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1038/88486</pub-id><pub-id pub-id-type="pmid">11369948</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>J</given-names></name><name><surname>Jiang</surname> <given-names>C</given-names></name><name><surname>Zhou</surname> <given-names>L</given-names></name><name><surname>Yang</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perception of hierarchical boundaries in music and its modulation by expertise</article-title><source>Neuropsychologia</source><volume>91</volume><fpage>490</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.09.013</pub-id><pub-id pub-id-type="pmid">27659874</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Time-domain analysis of neural tracking of hierarchical linguistic structures</article-title><source>NeuroImage</source><volume>146</volume><fpage>333</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.016</pub-id><pub-id pub-id-type="pmid">27856315</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoefel</surname> <given-names>B</given-names></name><name><surname>Archer-Boyd</surname> <given-names>A</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Phase entrainment of brain oscillations causally modulates neural responses to intelligible speech</article-title><source>Current Biology</source><volume>28</volume><fpage>401</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.11.071</pub-id><pub-id pub-id-type="pmid">29358073</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.55613.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Reichenbach</surname><given-names>Tobias</given-names></name><role>Reviewing Editor</role><aff><institution>Imperial College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Reichenbach</surname><given-names>Tobias</given-names> </name><role>Reviewer</role><aff><institution>Imperial College London</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>We believe that your work makes an important and timely contribution to the current debate regarding cortical responses to word-level features. With a smart experimental design, you clearly show that the mental chunking of word sequences contributes to shaping the neural responses.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Low-frequency neural activity reflects rule-based chunking during speech listening&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Tobias Reichenbach as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The manuscript describes an experiment that assesses the importance of mental “chunking” of groups of words during speech listening for neural responses. This is a very timely investigation, since there is a current debate regarding cortical responses to word-level features, such as syntactic and semantic ones. A difficulty with the observations has been that the data could be explained by different models, such as models based on syntactic features or on word similarity. The authors use a clever design in which participants are asked to chunk the same stimuli in two different ways. They show that the neural responses change depending on how the chunking is done, although the underlying stimulus remains essentially the same.</p><p>Essential revisions:</p><p>1) The authors employ both regularly alternating and irregularly alternating sequences, but the case for considering these two types is not strongly made. The former are said to be &quot;intuitive&quot;, and the second useful as &quot;fillers&quot; which is not strongly convincing. Is there something more to be gained (e.g. as suggested in the final paragraph of the Discussion)? Including both makes the story more complex, and the figures busier, so it is worth making the most of it.</p><p>2) The main results rely on the phase difference of the response at 1 Hz between the two categories “same” and “different”. But the significance of these phase differences is never assessed. They certainly seem to differ strongly from a uniform phase distribution, but this should nonetheless be confirmed through appropriate statistical testing. Moreover, the phase difference in the random-order sequence does seem to be centered slightly away from 0. Is that indeed the case, and if so, how could this slight shift away from zero be explained? Moreover, could one show time-domain plots of the trial-averaged response, in addition to, the frequency-domain analysis, to clearly make the point regarding the different phases?</p><p>3) The chunking, and the associated neural response, is not uniquely tied to speech processing. Instead, the task of the chunking of the word groups is an artificial one that could likely be applied to other auditory stimuli, such as groups of different notes. In particular, more general decision-making signals might play a role in the observed neural response. The subjects have to concentrate in this task and make a covert decision on each pair of words, potentially leading to a neural response that lines up with the chunks. Such a neural response could resemble a P3b (see, e.g., work from O'Connell and Kelly on the centro-parietal positivity decision-making signal and its relationship to the P3b component). Please discuss these issues in greater detail.</p><p>4) Related to the above, the authors discuss previous literature on chunking and its relationship to the frontal lobes. Why does the data presented here not show a contribution from the frontal lobes? It would also be good to compare the neural responses described in the manuscript to those reported in the author's previous publication (Ding et al., 2016b).</p><p>5) The Discussion ends relatively abruptly. Please add a paragraph that gives a summary and/or outlook.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Low-frequency neural activity reflects rule-based chunking during speech listening&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Barbara Shinn-Cunningham (Senior Editor), a Reviewing Editor, and the original reviewers.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>Essential revisions:</p><p>Regarding the analysis of the statistical significance of the phase differences at 1 Hz, the authors now write that the phase differences were closer to 0 than to 180 degrees in all MEG sensors, and significantly so in 205 sensors. But for the test for statistical significance, they write that they have not corrected for multiple comparisons. Given the large number of MEG sensors, any result without correction for multiple comparisons appears meaningless. The authors should carry out an appropriate correction, and only report the results after this correction.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.55613.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The authors employ both regularly alternating and irregularly alternating sequences, but the case for considering these two types is not strongly made. The former are said to be &quot;intuitive&quot;, and the second useful as &quot;fillers&quot; which is not strongly convincing. Is there something more to be gained (e.g. as suggested in the final paragraph of the Discussion)? Including both makes the story more complex, and the figures busier, so it is worth making the most of it.</p></disp-quote><p>The alternating-order sequences facilitate the comparison between same- and different-category conditions, since the alternating-order sequences in the 2 conditions are only offset by 1 word. Nevertheless, the highly regular structure of the alternating-order sequences allows the participants to detect invalid chunks using multiple strategies. Therefore, random-order sequences are introduced to avoid alternative strategies and force the participants to detect invalid chunks using different rules in the same- and different-category conditions.</p><p>We apologize that this critical information was not clearly conveyed in the previous manuscript. We have now added the following to Results:</p><p>“The random-order sequences were designed as fillers to avoid alternative strategies for the task (see Materials and methods for details).”</p><p>and the following to Materials and methods:</p><p>“The alternating-order sequences had a highly regular structure, which led to a simple relationship between the alternating-order sequences in same- and different-category conditions: Any alternating-order sequence in the different-category condition could be converted to a same-category sequence by removing the first word in the sequence. Attributable to this property, the neural response phase could conveniently distinguish the word- and chunk-based models in Figure 2. Nevertheless, this property also gave rise to an alternative strategy that could detect invalid chunks based on the same set of rules in both the same- and different-category conditions. For this strategy, the participants ignored the first word of each sequence in the different-category condition and treated the rest of the sequence as a same-category sequence. To eliminate this alternative strategy and to ensure that participants had to apply different rules in the same- and different-category conditions, the random-order sequences were designed as fillers to increase variability.”</p><disp-quote content-type="editor-comment"><p>2) The main results rely on the phase difference of the response at 1 Hz between the two categories “same” and “different”. But the significance of these phase differences is never assessed. They certainly seem to differ strongly from a uniform phase distribution, but this should nonetheless be confirmed through appropriate statistical testing.</p></disp-quote><p>We have now added a significance test on the phase difference and also reported the 99% confidence interval of the phase difference.</p><p>“In all the 306 MEG sensors, the phase difference averaged over participants was closer to 0° than 180° (Figure 3E), and in 261 sensors the effect was significant (P &lt; 0.01, bootstrap, see Materials and methods, not corrected for multiple comparisons). These results were consistent with the rule-based chunking model (Figure 2A). The phase difference averaged over all MEG sensors was significantly closer to 0° than 180° (P = 1×10<sup>-4</sup>, bootstrap, see Materials and methods). The mean phase difference was 0.5° and the 99% confidence interval ranged from -22.9° to 18.7°.”</p><p>“At 1-Hz, the phase difference between conditions was closer to 0° than 180° in all MEG sensors (Figure 3E), and the effect was significant in 205 sensors (P &lt; 0.01, bootstrap, see Materials and methods, not corrected for multiple comparisons). The phase difference averaged over all MEG sensors was significantly closer to 0° than 180° (P = 4×10<sup>-4</sup>, bootstrap, see Materials and methods). The mean phase difference was 20.3° and the 99% confidence interval over participants ranged from -0.1° to 56.6°.”</p><disp-quote content-type="editor-comment"><p>Moreover, the phase difference in the random-order sequence does seem to be centered slightly away from 0. Is that indeed the case, and if so, how could this slight shift away from zero be explained?</p></disp-quote><p>The phase difference for the random-order sequences slightly deviated from 0° but it was certainly closer to 0° than 180°. We have now reported the mean phase difference and its 99% confidence interval in Results, so that the readers could clearly see that the mean phase difference deviates from 0° but the 99% confidence interval still includes 0°. Furthermore, the new time course analysis revealed no clear phase lag between conditions either (Figure 3F). Therefore, we did not discuss this phase deviation since it was not a strong or reliable effect.</p><disp-quote content-type="editor-comment"><p>Moreover, could one show time-domain plots of the trial-averaged response, in addition to, the frequency-domain analysis, to clearly make the point regarding the different phases?</p></disp-quote><p>We have added time-domain results in Figure 3F and Figure 3—figure supplement 1C. We extracted the first 2 Principal Components (PC) based on the 306 MEG sensors. The 1<sup>st</sup> PC captured the response to sound onset and the 2-Hz response (Figure 3—figure supplement 1C), while the 2<sup>nd</sup> PC captured the 1-Hz response of interest. The 2<sup>nd</sup> PC, when filtered around 1 Hz, clearly showed that the responses in same- and different-category conditions oscillated in phase (Figure 3F).</p><disp-quote content-type="editor-comment"><p>3) The chunking, and the associated neural response, is not uniquely tied to speech processing. Instead, the task of the chunking of the word groups is an artificial one that could likely be applied to other auditory stimuli, such as groups of different notes. In particular, more general decision-making signals might play a role in the observed neural response. The subjects have to concentrate in this task and make a covert decision on each pair of words, potentially leading to a neural response that lines up with the chunks. Such a neural response could resemble a P3b (see, e.g., work from O'Connell and Kelly on the centro-parietal positivity decision-making signal and its relationship to the P3b component). Please discuss these issues in greater detail.</p></disp-quote><p>Indeed, the chunk detection task is a sequential detection task and we have added discussions on how the chunk response may potentially relate to decision-making signals in the brain.</p><p>“During sequential decision making, it has been more quantitatively demonstrated that neural activity reflects accumulation of sensory evidence (Barascud et al., 2016; O'Connell et al., 2012; Shadlen and Shohamy, 2016). […] Therefore, the neural source of the chunk-rate response is not identical to the P300, the dominant decision signal in previous EEG/MEG studies, but it may still reflect sequential decision making since such responses are highly distributive as shown by intracranial neural recordings (Gold and Shalden, 2007).”</p><disp-quote content-type="editor-comment"><p>4) Related to the above, the authors discuss previous literature on chunking and its relationship to the frontal lobes. Why does the data presented here not show a contribution from the frontal lobes? It would also be good to compare the neural responses described in the manuscript to those reported in the author's previous publication (Ding et al., 2016b).</p></disp-quote><p>Thanks for raising this point. There is actually weak activation in the frontal lobe. However, similar to the response to sentences, the chunk-rate response is stronger in the temporal lobe. We have now added a ROI analysis to separately show the activation in temporal and frontal ROIs (Figure 3—figure supplement 1C) and added the following in to Results and Discussion.</p><p>“An ROI analysis further revealed that the 1-Hz response peak was statistically significant in both temporal and frontal lobes (F32,64 &gt; 3.3, P &lt; 1.9×10-5 for all conditions; F-test, FDR corrected), and the response is stronger in the left superior temporal gyrus than the left inferior frontal gyrus (F128,128 = 2.0, P = 7.1×10-4 ; F-test, FDR corrected; Figure 3—figure supplement 1C).”</p><p>“With the spatial resolution of MEG, we cannot precisely localize the neural source of the chunk-rate response. However, it is found that the frontal lobe activation is bilateral with no clear lateralization between hemispheres (Figure 3—figure supplement 1C), in contrast to the clearly left lateralized sentence-tracking response (Ding et al., 2016b; Sheng et al., 2019). Similar to the sentence-tracking response, the chunk-rate response is also stronger in the temporal lobe than in the frontal lobe, suggesting that both sentences and chunks defined by temporary rules can drive large-scale chunk-rate responses in the temporal lobe.”</p><disp-quote content-type="editor-comment"><p>5) The Discussion ends relatively abruptly. Please add a paragraph that gives a summary and/or outlook.</p></disp-quote><p>Thanks for the suggestion and we have now reorganized the Discussion section, which now ends with the following paragraph.</p><p>“Finally, the chunk-rate response in the current study is consistent with the rule-based chunking model. […] Thus, the current study and previous studies (Ding et al., 2016a) provide strong support to the notion that the brain can construct superordinate linguistic representations based on either long-term syntactic rules or temporary rules learned in an experiment.”</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Regarding the analysis of the statistical significance of the phase differences at 1 Hz, the authors now write that the phase differences were closer to 0 than to 180 degrees in all MEG sensors, and significantly so in 205 sensors. But for the test for statistical significance, they write that they have not corrected for multiple comparisons. Given the large number of MEG sensors, any result without correction for multiple comparisons appears meaningless. The authors should carry out an appropriate correction, and only report the results after this correction.</p></disp-quote><p>We have now used the false discovery rate (FDR) correction for multiple comparisons.</p><p>“In all the 306 MEG sensors, the phase difference averaged over participants was closer to 0° than 180° (Figure 3E), and in 258 sensors the effect was significant (P &lt; 0.01, bootstrap, see Materials and methods, FDR corrected).”</p><p>“At 1-Hz, the phase difference between conditions was closer to 0° than 180° in all MEG sensors (Figure 3E), and the effect was significant in 196 sensors (P &lt; 0.01, bootstrap, see Materials and methods, FDR corrected).”</p></body></sub-article></article>