<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">22794</article-id><article-id pub-id-type="doi">10.7554/eLife.22794</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Emergence of transformation-tolerant representations of visual objects in rat lateral extrastriate cortex</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-72893"><name><surname>Tafazoli</surname><given-names>Sina</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1926-0227</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="data-ro1"/><xref ref-type="fn" rid="pa1">‡</xref></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-72628"><name><surname>Safaai</surname><given-names>Houman</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8609-7397</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="data-ro1"/></contrib><contrib contrib-type="author" id="author-72908"><name><surname>De Franceschi</surname><given-names>Gioia</given-names> </name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa2">§</xref><xref ref-type="other" rid="data-ro1"/></contrib><contrib contrib-type="author" id="author-72895"><name><surname>Rosselli</surname><given-names>Federica Bianca</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="data-ro1"/><xref ref-type="fn" rid="pa3">¶</xref></contrib><contrib contrib-type="author" id="author-72896"><name><surname>Vanzella</surname><given-names>Walter</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="data-ro1"/></contrib><contrib contrib-type="author" id="author-72897"><name><surname>Riggi</surname><given-names>Margherita</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="data-ro1"/></contrib><contrib contrib-type="author" id="author-72898"><name><surname>Buffolo</surname><given-names>Federica</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="data-ro1"/><xref ref-type="fn" rid="pa4">**</xref></contrib><contrib contrib-type="author" id="author-40025"><name><surname>Panzeri</surname><given-names>Stefano</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1700-8909</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="data-ro1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-12467"><name><surname>Zoccolan</surname><given-names>Davide</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7221-4188</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="data-ro1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Visual Neuroscience Lab</institution>, <institution>International School for Advanced Studies (SISSA)</institution>, <addr-line><named-content content-type="city">Trieste</named-content></addr-line>, <country>Italy</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Laboratory of Neural Computation, Center for Neuroscience and Cognitive Systems @UniTn</institution>, <institution>Istituto Italiano di Tecnologia</institution>, <addr-line><named-content content-type="city">Rovereto</named-content></addr-line>, <country>Italy</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Neurobiology</institution>, <institution>Harvard Medical School</institution>, <addr-line><named-content content-type="city">Boston</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Tsao</surname><given-names>Doris Y</given-names></name><role>Reviewing editor</role><aff id="aff4"><institution>California Institute of Technology</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>zoccolan@sissa.it</email></corresp><fn fn-type="present-address" id="pa1"><label>‡</label><p>Princeton Neuroscience Institute, Princeton University, NewJersey, United States</p></fn><fn fn-type="present-address" id="pa2"><label>§</label><p>Department of Experimental Psychology, Institute of Behavioural Neuroscience, University College London, London, United Kingdom</p></fn><fn fn-type="present-address" id="pa3"><label>¶</label><p>Department of Behavior and Brain Organization, Center of Advanced European Studies and Research (Caesar), An Institute of the Max-Planck Society, Bonn, Germany</p></fn><fn fn-type="present-address" id="pa4"><label>**</label><p>Neuroscience and Brain Technologies department, Istituto Italiano di Tecnologia, Genova, Italy</p></fn><fn fn-type="con" id="equal-contrib"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="pub" publication-format="electronic"><day>11</day><month>04</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e22794</elocation-id><history><date date-type="received"><day>29</day><month>10</month><year>2016</year></date><date date-type="accepted"><day>26</day><month>02</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Tafazoli et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Tafazoli et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-22794-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="commentary" xlink:href="10.7554/eLife.26401"/><abstract><object-id pub-id-type="doi">10.7554/eLife.22794.001</object-id><p>Rodents are emerging as increasingly popular models of visual functions. Yet, evidence that rodent visual cortex is capable of advanced visual processing, such as object recognition, is limited. Here we investigate how neurons located along the progression of extrastriate areas that, in the rat brain, run laterally to primary visual cortex, encode object information. We found a progressive functional specialization of neural responses along these areas, with: (1) a sharp reduction of the amount of low-level, energy-related visual information encoded by neuronal firing; and (2) a substantial increase in the ability of both single neurons and neuronal populations to support discrimination of visual objects under identity-preserving transformations (e.g., position and size changes). These findings strongly argue for the existence of a rat object-processing pathway, and point to the rodents as promising models to dissect the neuronal circuitry underlying transformation-tolerant recognition of visual objects.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.001">http://dx.doi.org/10.7554/eLife.22794.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.22794.002</object-id><title>eLife digest</title><p>Everyday, we see thousands of different objects with many different shapes, colors, sizes and textures. Even an individual object – for example, a face – can present us with a virtually infinite number of different images, depending on from where we view it. In spite of this extraordinary variability, our brain can recognize objects in a fraction of a second and without any apparent effort.</p><p>Our closest relatives in the animal kingdom, the non-human primates, share our ability to effortlessly recognize objects. For many decades, they have served as invaluable models to investigate the circuits of neurons in the brain that underlie object recognition. In recent years, mice and rats have also emerged as useful models for studying some aspects of vision. However, it was not clear whether these rodents’ brains could also perform complex visual processes like recognizing objects.</p><p>Tafazoli, Safaai et al. have now recorded the responses of visual neurons in rats to a set of objects, each presented across a range of positions, sizes, rotations and brightness levels. Applying computational and mathematical tools to these responses revealed that visual information progresses through a number of brain regions. The identity of the visual objects is gradually extracted as the information travels along this pathway, in a way that becomes more and more robust to changes in how the object appears.</p><p>Overall, Tafazoli, Safaai et al. suggest that rodents share with primates some of the key computations that underlie the recognition of visual objects. Therefore, the powerful sets of experimental approaches that can be used to study rats and mice – for example, genetic and molecular tools – could now be used to study the circuits of neurons that enable object recognition. Gaining a better understanding of such circuits can, in turn, inspire the design of more powerful artificial vision systems and help to develop visual prosthetics. Achieving these goals will require further work to understand how different classes of neurons in different brain regions interact as rodents perform complex visual discrimination tasks.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.002">http://dx.doi.org/10.7554/eLife.22794.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>object recognition</kwd><kwd>visual cortex</kwd><kwd>invariance</kwd><kwd>transformation tolerance</kwd><kwd>neuronal coding</kwd><kwd>information theory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Marie Curie International Reintegration Grant</institution></institution-wrap></funding-source><award-id>PIRG6-GA-2009-256563 IVOR</award-id><principal-award-recipient><name><surname>Zoccolan</surname><given-names>Davide</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGP0015/2013</award-id><principal-award-recipient><name><surname>Zoccolan</surname><given-names>Davide</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>Consolidator Grant 616803-LEARN2SEE</award-id><principal-award-recipient><name><surname>Zoccolan</surname><given-names>Davide</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>ITN Marie Curie Grant</institution></institution-wrap></funding-source><award-id>project ABC FP7-2007-2013/PITN-GA-2011-290011</award-id><principal-award-recipient><name><surname>Panzeri</surname><given-names>Stefano</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution>Autonomous Province of Trento</institution></institution-wrap></funding-source><award-id>Grandi Progetti 2012 &quot;ATTEND&quot;</award-id><principal-award-recipient><name><surname>Panzeri</surname><given-names>Stefano</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neuronal recordings from rat visual cortex reveal an object-processing pathway, along which neuronal representations become increasingly capable of supporting recognition of visual objects in spite of variation in their appearance.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Converging evidence (<xref ref-type="bibr" rid="bib89">Wang and Burkhalter, 2007</xref>; <xref ref-type="bibr" rid="bib90">Wang et al., 2011</xref>, <xref ref-type="bibr" rid="bib91">2012</xref>) indicates that rodent visual cortex is organized in two clusters of strongly reciprocally connected areas, which resemble, anatomically, the primate ventral and dorsal streams (i.e., the cortical pathways specialized for the processing of, respectively, shape and motion information). The first cluster includes most of lateral extrastriate areas, while the second encompasses medial and parietal extrastriate cortex. Solid causal evidence confirms the involvement of these modules in ventral-like and dorsal-like computations – lesioning laterotemporal and posterior parietal cortex strongly impairs, respectively, visual pattern discrimination and visuospatial perception (<xref ref-type="bibr" rid="bib23">Gallardo et al., 1979</xref>; <xref ref-type="bibr" rid="bib45">McDaniel et al., 1982</xref>; <xref ref-type="bibr" rid="bib94">Wörtwein et al., 1993</xref>; <xref ref-type="bibr" rid="bib2">Aggleton et al., 1997</xref>; <xref ref-type="bibr" rid="bib78">Sánchez et al., 1997</xref>; <xref ref-type="bibr" rid="bib80">Tees, 1999</xref>). By comparison, functional understanding of visual processing in rodent extrastriate cortex is still limited. While studies employing parametric visual stimuli (e.g., drifting gratings) support the specialization of dorsal areas for motion processing (<xref ref-type="bibr" rid="bib4">Andermann et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Marshel et al., 2011</xref>; <xref ref-type="bibr" rid="bib36">Juavinett and Callaway, 2015</xref>), the functional signature of ventral-like computations has yet to be found in lateral areas. In fact, parametric stimuli do not allow probing the core property of a ventral-like pathway – i.e., the ability to support recognition of visual objects despite variation in their appearance, resulting from (e.g.) position and size changes. In primates, this function, known as <italic>transformation-tolerant</italic> (or <italic>invariant</italic>) recognition, is mediated by the gradual reformatting of object representations that takes place along the ventral stream (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>). Recently, a number of behavioral studies have shown that rats too are capable of invariant recognition (<xref ref-type="bibr" rid="bib97">Zoccolan et al., 2009</xref>; <xref ref-type="bibr" rid="bib79">Tafazoli et al., 2012</xref>; <xref ref-type="bibr" rid="bib85">Vermaercke and Op de Beeck, 2012</xref>; <xref ref-type="bibr" rid="bib3">Alemi-Neissi et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Vinken et al., 2014</xref>; <xref ref-type="bibr" rid="bib67">Rosselli et al., 2015</xref>), thus arguing for the existence of cortical machinery supporting this function also in rodents. This hypothesis is further supported by the preferential reliance of rodents on vision during spatial navigation (<xref ref-type="bibr" rid="bib16">Cushman et al., 2013</xref>; <xref ref-type="bibr" rid="bib98">Zoccolan, 2015</xref>), and by the strong dependence of head-directional tuning on visual cues in rat hippocampus (<xref ref-type="bibr" rid="bib1">Acharya et al., 2016</xref>). Yet, in spite of a recent attempt at investigating rat visual areas with shape stimuli (<xref ref-type="bibr" rid="bib86">Vermaercke et al., 2014</xref>), functional evidence about how rodent visual cortex may support transformation-tolerant recognition is still sparse.</p><p>In our study, we compared how visual object information is processed along the anatomical progression of extrastriate areas that, in the rat brain, run laterally to V1: lateromedial (LM), laterointermediate (LI) and laterolateral (LL) areas (<xref ref-type="bibr" rid="bib20">Espinoza and Thomas, 1983</xref>; <xref ref-type="bibr" rid="bib47">Montero, 1993</xref>; <xref ref-type="bibr" rid="bib86">Vermaercke et al., 2014</xref>). By applying specially designed information theoretic and decoding analyses, we found a sharp reduction of the amount of low-level information encoded by neuronal firing along this progression, and a concomitant increase in the ability of neuronal representations to support invariant recognition. Taken together, these findings provide compelling evidence about the existence of a ventral-like, object-processing pathway in rat visual cortex.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We used 32-channel silicon probes to record from rat primary visual cortex and the three lateral extrastriate areas LM, LI and LL (<xref ref-type="fig" rid="fig1">Figure 1A</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Our experimental design was inspired by a well-established approach in ventral stream studies, consisting in passively presenting an animal (either awake or anesthetized) with a large number of visual objects in rapid sequence. This allows probing how object information is processed by the initial, largely feedforward cascade of ventral-stream computations underlying rapid recognition of visual objects (<xref ref-type="bibr" rid="bib81">Thorpe et al., 1996</xref>; <xref ref-type="bibr" rid="bib21">Fabre-Thorpe et al., 1998</xref>; <xref ref-type="bibr" rid="bib68">Rousselet et al., 2002</xref>) – a reflexive, stimulus-driven process that is largely independent from top-down signals and whether the animal is engaged in a recognition task (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>).<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.22794.003</object-id><label>Figure 1.</label><caption><title>Experimental design.</title><p>(<bold>A</bold>) Oblique insertion of a single-shank silicon probe in a typical recording session targeting rat lateral extrastriate areas LM, LI and LL, located between V1 and temporal association cortex (TeA). The probe contained 32 recording sites, spanning 1550 µm, from tip (site 1) to base (site 32). The probe location was reconstructed postmortem (left), by superimposing a bright-field image of the Nissl-stained coronal section at the targeted bregma (light gray) with an image (dark gray) showing the staining with the fluorescent dye (red), used to coat the probe before insertion (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). (<bold>B</bold>) The stimulus set, consisting of ten visual objects (top) and their transformations (bottom). (<bold>C</bold>) Firing intensity maps (top) displaying the RFs recorded along the probe shown in (<bold>A</bold>). The numbers identify the sites each unit was recorded from. Tracking the retinotopy of the RF centers (bottom: colored dots) and its reversals (white arrows) allowed identifying the area each unit was recorded from. Details about how the stimuli were presented and the RFs were mapped are provided in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>. The number of neurons per area obtained in each recording session (i.e., from each rat) is reported in <xref ref-type="supplementary-material" rid="SD1-data">Figure 1—source data 1</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.003">http://dx.doi.org/10.7554/eLife.22794.003</ext-link></p><p><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.22794.004</object-id><label>Figure 1—source data 1.</label><caption><title>Number of neurons per area obtained from each rat.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.004">http://dx.doi.org/10.7554/eLife.22794.004</ext-link></p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-22794-fig1-data1-v1.docx"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig1-v1"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.005</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Histological reconstruction of the laminar location of the recording sites.</title><p>Histological reconstruction of the recording sites for three example experimental sessions, two targeting lateral extrastriate areas LM, LI and LL (<bold>A–B</bold>) and one targeting primary visual cortex (<bold>C</bold>). The recordings were performed with different configurations of 32-channel silicon probes. In (<bold>A–B</bold>), a single-shank probe (model A1 × 32-5mm50-413), with recording sites spanning 1550 µm, was inserted diagonally into rat visual cortex, in such a way to target either the deep (<bold>A</bold>) or superficial (<bold>B</bold>) layers of areas LM, LI and LL. Note that the histological section shown in (<bold>A</bold>) refers to the same section shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. In (<bold>C</bold>), an 8-shank probe (model A8 × 4-2mm100-200-177), with recording sites spanning an area of 1400 µm (across shanks) x 300 µm (within shank), was inserted perpendicularly into primary visual cortex (V1), targeting deep layers. Each panel shows a low magnification image of the Nissl-stained coronal section at the targeted bregma, with superimposed higher-magnification Nissl images (either 2.5X or 10X), taken around the insertion track(s) of the probe. These images were stitched together to obtain a high-resolution picture of the probe location, with respect to the cortical layers. The position of the recording sites was reconstructed by tracing the outline of the fluorescent track left by each shank (that had been coated with the fluorescent dye DiI before insertion; e.g., see the red trace in <xref ref-type="fig" rid="fig1">Figure 1A</xref>), and then relying on the known geometry of the probe (see Materials and methods for details). The resulting position of the shank(s) and recording sites are indicated, respectively, by the thick black lines and by the yellow dots. The red lines show the boundaries between different groups of cortical laminae: layers II-IV, layer V and layer VI. These boundaries were estimated from the variation of cell size, morphology and density across the cortical thickness.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.005">http://dx.doi.org/10.7554/eLife.22794.005</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig1-figsupp1-v1"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.006</object-id><label>Figure 1—figure supplement 2.</label><caption><title>Computation of the receptive field size and receptive field luminance, and illustration of the tangent screen projection.</title><p>(<bold>A</bold>) Illustration of the virtual grid of 6 × 11, 10°-wide cells, where the drifting bars used to map the neuronal receptive fields (RFs) were shown. In each cell, the bars were presented at four different orientations (0°, 45°, 90° and 135°), sweeping from one side of the cell to the opposite one and back at the speed of 66°/s. (<bold>B</bold>) Receptive field map (left), resulting from plotting the average number of spikes fired by a neuron, in response to the four oriented bars, across the grid of visual field locations shown in (<bold>A</bold>). In the right plot, the same firing intensity map is rendered in three dimensions (blue dots), along with the two-dimensional Gaussian (mesh grid surface) that best fitted the data points. The ellipse centered on the peak of the Gaussian and defined by its widths along the <italic>x</italic> and <italic>y</italic> axes was taken as the extent of the neuronal RF (black ellipse in the left plot). (<bold>C</bold>) Illustration of the procedure to compute the RF luminance of a visual stimulus. The image of an example visual stimulus (left plot) is superimposed to the grid used to map the RF of a recorded neuron (middle plot; same RF as in B). The two maps (i.e., the stimulus image and the RF) are multiplied in a cell-by-cell fashion, so as to yield an RF-weighted luminance intensity map of the stimulus (right plot). These luminance intensity values are then summed to obtain the RF luminance of the stimulus (this procedure is equivalent to compute the dot product between the stimulus image and the RF map). The standard deviation of the RF-weighted luminance intensity values falling inside the RF was taken as a measure of the contrast impinged by the stimulus on the RF (we called this metric <italic>RF contrast</italic>; see Materials and methods). Note that, in (<bold>A–C</bold>), the lines of the grid are shown only to help visualizing the spatial arrangement and extent of the stimuli and of the RF map, but no grid was actually shown during stimulus presentation. (<bold>D–E</bold>) Illustration of the tangent screen projection used to present the visual stimuli in our experiment. This projection avoided distorting the shape of the objects, when they were translated across the stimulus display, in spite of the close proximity of the eye to the display. See Materials and methods for a detailed explanation of the variables and their relationship. (<bold>F</bold>) The images in the top row are snapshots taken from the stimulus display during the presentation of an example object at four different azimuth locations (−22.5°, −15°, −7.5° and 0°). It can be noticed the distortion applied to the object by the tangent screen projection to compensate for the distortion produced by the perspective projection to the retina. The bottom row shows the resulting projections of the object over the retina. To provide a spatial reference, the grid of visual field locations used to map the neuronal RFs is also shown (pink grid).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.006">http://dx.doi.org/10.7554/eLife.22794.006</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig1-figsupp2-v1"/></fig></fig-group></p><p>During a recording session, a rat was presented with a rich battery of visual stimuli, consisting of 10 visual objects (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, top), each transformed along five variation axes (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, bottom), for a total of 380 stimulus conditions (i.e., object views). These conditions included combinations of object identities (i.e., objects #7–10) and transformations that we have previously shown to be invariantly recognized by rats (<xref ref-type="bibr" rid="bib79">Tafazoli et al., 2012</xref>). In addition, drifting bars were used to map the receptive field (RF) of each recorded unit (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A–B</xref>). To allow for the repeated presentation (average of 26.5 trials per stimulus) of such large number of stimulus conditions, while maintaining a good stability of the recordings, rats were kept in an anesthetized state during each session (see Discussion for possible implications).</p><p>We recorded 771 visually driven and stimulus informative units in 26 rats: 228 from V1, 131 from LM, 260 from LI, and 152 from LL (neurons in each area came from at least eight rats; <xref ref-type="supplementary-material" rid="SD1-data">Figure 1—source data 1</xref>). With ‘units’ we refer here to a combination of both well-isolated single units and multiunit clusters. Hereafter, most results will be presented over the whole population of units, although a number of control analyses were carried out on well-isolated single units only (see Discussion). The cortical area each unit was recorded from was identified by tracking the progression of the RFs recorded along a probe (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), so as to map the reversals of the retinotopy that, in rodent visual cortex, delineate the borders between adjacent areas (<xref ref-type="bibr" rid="bib73">Schuett et al., 2002</xref>; <xref ref-type="bibr" rid="bib89">Wang and Burkhalter, 2007</xref>; <xref ref-type="bibr" rid="bib4">Andermann et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Marshel et al., 2011</xref>; <xref ref-type="bibr" rid="bib59">Polack and Contreras, 2012</xref>; <xref ref-type="bibr" rid="bib86">Vermaercke et al., 2014</xref>). This procedure was combined with the histological reconstruction of the probe insertion track (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) and, when possible, of the laminar location of the individual recording sites (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><sec id="s2-1"><title>The fraction of energy-independent stimulus information sharply increases from V1 to LL</title><p>Under the hypothesis that, along an object-processing pathway, information about low-level image properties should be partially lost (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>), we measured the sensitivity of each recorded neuron to the lowest-level attribute of the visual input – the amount of luminous energy a stimulus impinges on a neuronal receptive field. This was quantified by a metric (<italic>RF luminance</italic>; see Materials and methods), which, for some neuron, seemed to account for the modulation of the firing rate not only across object transformations, but also across object identities. This was the case for the example V1 neuron shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, where the objects eliciting the larger responses along the position axis (red and blue curves) were the ones that consistently covered larger fractions of the neuron’s RF (yellow ellipses), thus yielding higher RF luminance values (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A–B</xref>), as compared to the less effective object (green curve). By contrast, for other units (e.g., the example LL neuron shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref>), RF luminance did not appear to account for the tuning for object identity – similarly bright objects (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C–D</xref>) yielded very different response magnitudes (compare the red with the green and blue curves).<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.22794.007</object-id><label>Figure 2.</label><caption><title>Tuning of an example V1 and LL neuron.</title><p>(<bold>A–B</bold>) The bottom plots show the average firing rates (AFRs) of a V1 (<bold>A</bold>) and a LL (<bold>B</bold>) neuron, evoked by three objects, presented at eight different visual field positions (shown in <xref ref-type="fig" rid="fig1">Figure 1B.1</xref>). The top plots show the peri-stimulus time histograms (PSTHs) obtained at two positions, along with the images of the corresponding stimulus conditions. These images also display the RF profile of each neuron, in the guise of three concentric ellipses, corresponding to 1 SD, 2 SD and 3 SD of the two-dimensional Gaussians that were fitted to the raw RFs. The numbers (white font) show the luminosity that each object condition impinged on the RF (referred to as <italic>RF luminance</italic>; the distributions of RF luminance values produced by the objects across the full set of positions and the full set of transformations are shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The gray patches over the PSTHs show the spike count windows (150 ms) used to compute the AFRs (their onsets were the response latencies of the neurons). Error bars are SEM. (<bold>C–D</bold>) Luminance sensitivity profiles for the two examples neurons shown in (<bold>A</bold>) and (<bold>B</bold>). For each neuron, the stimulus conditions were grouped in 23 RF luminance bins with 10 stimuli each, and the intensity of firing across the resulting 23 × 10 matrix was color-coded. Within each bin, the stimuli were ranked according to the magnitude of the response they evoked.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.007">http://dx.doi.org/10.7554/eLife.22794.007</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig2-v1"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.008</object-id><label>Figure 2—figure supplement 1.</label><caption><title>RF luminance values produced by very effective and poorly effective objects: a comparison between an example V1 and an example LL neuron.</title><p>(<bold>A</bold>) The arrows show the RF luminance values that were measured, across eight positions, for three objects that were differently effective at driving an example V1 neuron (same cell of <xref ref-type="fig" rid="fig2">Figure 2A and C</xref>). The most effective object (#7; see <xref ref-type="fig" rid="fig2">Figure 2A</xref>) consistently yielded larger RF luminance values (red arrows), compared to the least effective one (#1; green arrows). By comparison, another object (#9), which was also quite effective at driving the cell (see <xref ref-type="fig" rid="fig2">Figure 2A</xref>), produced RF luminance values (blue arrows) that substantially overlapped with those of object #7. (<bold>B</bold>) Same comparison as in (<bold>A</bold>), but with the distributions of RF luminance values of the three objects, computed across the full set of 23 transformations each object underwent (same color code as in A). (<bold>C</bold>) The RF luminance values produced by three objects, across eight positions, for an example LL neuron (same cell of <xref ref-type="fig" rid="fig2">Figure 2B and D</xref>). In this case, the most effective object (#6; see <xref ref-type="fig" rid="fig2">Figure 2B</xref>) did not yield the largest RF luminance values (red arrows). The other two objects (#9 and #8), which were both ineffective at driving the cell (see <xref ref-type="fig" rid="fig2">Figure 2B</xref>), produced either very similar (#9; green arrows) or considerably larger (#8; blue arrows) RF luminance values, as compared to object #6. (<bold>D</bold>) Same comparison as in (<bold>C</bold>), but with the distributions of RF luminance values of the three objects, computed across the full set of 23 transformations each object underwent (same color code as in C).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.008">http://dx.doi.org/10.7554/eLife.22794.008</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig2-figsupp1-v1"/></fig></fig-group></p><p>To better appreciate the sensitivity of each neuron to stimulus energy, we considered a subset of the stimulus conditions, consisting of 23 transformations of each object, for a total of 230 stimuli (see Materials and methods). We then grouped these stimuli in 23 equi-populated RF luminance bins and we color-coded the intensity of the neuronal response across the resulting matrix of 10 stimuli per 23 bins (the stimuli within each bin were ranked according to the magnitude of the response they evoked). Many units, as the example V1 neuron of <xref ref-type="fig" rid="fig2">Figure 2C</xref> (same cell of <xref ref-type="fig" rid="fig2">Figure 2A</xref>), showed a gradual increase of activity across consecutive bins of progressively larger luminance, with little variation of firing within each bin. Other units, as the example LL neuron of <xref ref-type="fig" rid="fig2">Figure 2D</xref> (same cell of <xref ref-type="fig" rid="fig2">Figure 2B</xref>), displayed no systematic variation of firing across the luminance axis, but were strongly modulated by the conditions within each luminance bin, thus suggesting a tuning for higher-level stimulus properties.</p><p>Note that, although the example LL neuron of <xref ref-type="fig" rid="fig2">Figure 2D</xref> fired more sparsely than the example V1 neuron of <xref ref-type="fig" rid="fig2">Figure 2C</xref>, the sparseness of neuronal firing across the 230 stimulus conditions, measured as defined in (<xref ref-type="bibr" rid="bib87">Vinje and Gallant, 2000</xref>), was not statistically different between the LL and the V1 populations (p&gt;0.05; Mann-Whitney U-test), with the median sparseness being ~0.13 in both areas. Critically, this does not imply that the two areas do not differ in the way they encode visual objects, because sparseness is a combined measure of object selectivity and tolerance to changes in object appearance, which is positively correlated with the former and negatively correlated with the latter. As such, a concomitant increase of both selectivity and tolerance can lead to no appreciable change of sparseness across an object-processing hierarchy (<xref ref-type="bibr" rid="bib70">Rust and DiCarlo, 2012</xref>). This suggests that other approaches are necessary to compare visual object representations along a putative ventral-like pathway.</p><p>In our study, we first quantified the relative sensitivity of a neuron to stimulus luminance and higher-level features by using information theory, because of two main advantages this approach offers in investigating neuronal coding. First, computing mutual information between a stimulus’ feature and the evoked neuronal response provides an upper bound to how well we can reconstruct the stimulus’ feature from observing the neural response on a single trial, without committing to the choice of any specific decoding algorithm (<xref ref-type="bibr" rid="bib64">Rieke et al., 1997</xref>; <xref ref-type="bibr" rid="bib7">Borst and Theunissen, 1999</xref>; <xref ref-type="bibr" rid="bib60">Quiroga and Panzeri, 2009</xref>; <xref ref-type="bibr" rid="bib66">Rolls and Treves, 2011</xref>). Second, information theory provides a solid mathematical framework to disentangle the ability of a neuron to encode a given stimulus’ feature from its ability to encode another feature, even if these two features are not independently distributed across the stimuli, i.e., even if they are correlated in an arbitrarily complex, non-linear way (<xref ref-type="bibr" rid="bib34">Ince et al., 2012</xref>). This property was crucial to allow estimating the relative contribution of luminance and higher-level features to the tuning of rat visual neurons, given that luminance did co-vary, in general, with other stimulus properties, such as object identity, position, size, etc. (see <xref ref-type="fig" rid="fig2">Figure 2A–B</xref>).</p><p>In our analysis, for each neuron, we first computed Shannon’s mutual information between stimulus identity <italic>S</italic> and neuronal response <italic>R</italic>, formulated as:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <italic>P(s)</italic> is the probability of presentation of stimulus <italic>s</italic>, <italic>P(r|s)</italic> is the probability of observing a response <italic>r</italic> following presentation of stimulus <italic>s</italic>, and <italic>P(r)</italic> is the probability of observing a response <italic>r</italic> across all stimulus presentations. The response <italic>R</italic> was quantified as the number of spikes fired by the neuron in a 150 ms-wide spike count window (e.g., see the gray patches in <xref ref-type="fig" rid="fig2">Figure 2A–B</xref>), while the stimulus conditions <italic>S</italic> included all the 23 transformations of the 10 objects, previously used to produce <xref ref-type="fig" rid="fig2">Figure 2C–D</xref>, for a total of 230 different stimuli (see Materials and methods for details). As graphically illustrated in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> measured the discriminability of these 230 different stimuli, given the spike count distributions they evoked in a single neuron.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.22794.009</object-id><label>Figure 3.</label><caption><title>Information conveyed by the neuronal response about stimulus luminance and luminance-independent visual features.</title><p>(<bold>A</bold>) Illustration of how total stimulus information per neuron was computed. All the views of all the objects (total of 230 stimuli) were considered as different stimulus conditions, each giving rise to its own response distribution (colored curves; only a few examples are shown here). Mutual information between stimulus and response measured the discriminability of the stimuli, given the overlap of their response distributions. (<bold>B</bold>) Mutual information (median over the units recorded in each area ± SE) between stimulus and response in each visual area (full bars; *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected). The white portion of the bars shows the median information that each area carried about stimulus luminance, while the colored portion is the median information about higher-order, luminance-independent visual features. The number of cells in each area is written on the corresponding bar. (<bold>C</bold>) Median fraction of luminance-independent stimulus information (<inline-formula><mml:math id="inf2"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>; see Results) that neurons carried in each area. Error bars and significance levels/test as in (<bold>B</bold>). The mutual information metrics obtained for neurons sampled from cortical layers II-IV and V-VI are reported in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. The <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> values obtained for neuronal subpopulations with matched spike isolation quality are shown in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>. The sensitivity of rat visual neurons to luminance variations of the same object is shown in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>. The information carried by rat visual neurons about stimulus contrast and contrast-independent visual features is reported in <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.009">http://dx.doi.org/10.7554/eLife.22794.009</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig3-v1"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.010</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Information conveyed by the neuronal response about stimulus luminance and luminance-independent visual features: a comparison between superficial and deep layers.</title><p>Same mutual information analysis as the one shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, but considering separately the neuronal populations recorded in cortical layers II-IV (left plots) and V-VI (right plots). Colors, symbols, significance levels and statistical tests as in <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>A</bold>) To check whether the drop of stimulus information (full bars) was similarly sharp in superficial and deep layers, a two-way ANOVA, with <italic>visual area</italic> and <italic>layer</italic> as factors, was carried out. The test yielded a significant main effect for area (p&lt;0.001, <italic>F</italic><sub>3,687</sub> = 15.87) and a significant interaction between area and layer (p&lt;0.001, <italic>F</italic><sub>3,687</sub> = 5.61), but not a main effect for layer alone (p&gt;0.9, <italic>F</italic><sub>1,687</sub> = 0.01). This indicates that the information loss was sharper in deep layers. As for the case of the entire populations (see <xref ref-type="fig" rid="fig3">Figure 3B</xref>), the drop of information was mainly due to a reduction of the information about stimulus luminosity (white bars), while the information about higher-order visual attributes (colored bars) remained more stable across the areas. This was especially noticeable for deep layers, where the information about luminosity in LM and LI became almost half as large as in V1, further dropping in LL to about one third of what observed in V1. (<bold>B</bold>) The fraction of luminance-independent stimulus information (<italic>f</italic><sub>high</sub>) that neurons carried in each area increased more gradually and was, overall, larger in deep than in superficial layers. A two-way ANOVA, with <italic>visual area</italic> and <italic>layer</italic> as factors, confirmed this observation, yielding a significant main effect for both area (p&lt;0.001, <italic>F</italic><sub>3,687</sub> = 63.58) and layer (p&lt;0.001, <italic>F</italic><sub>1,687</sub> = 14.7) and also a significant interaction (p&lt;0.01, <italic>F</italic><sub>3,687</sub> = 4.06). In terms of pairwise comparisons, this resulted in a very large and significant increase of <italic>f</italic><sub>high</sub> in LL, as compared to all other areas, in both superficial and deep layers, and in a significant difference also between LI and V1/LM, but in deep layers only.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.010">http://dx.doi.org/10.7554/eLife.22794.010</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig3-figsupp1-v1"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.011</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Independence of the fraction of luminance-independent stimulus information from the quality of spike isolation.</title><p>(<bold>A–B</bold>) Same mutual information analysis as the one shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, but restricted to units within a given tirtile of the <italic>SNR</italic> and <italic>RV</italic> metrics, used to asses the quality of spike isolation (see Materials and methods). Note that the quality of spike isolation increases as function of <italic>SNR</italic>, while it decreases as a function of <italic>RV</italic>. The number of units in each area per tirtile is superimposed to the corresponding bar. (<bold>C</bold>) Same mutual information analysis, considering only the neuronal subpopulations with good spike isolation quality (i.e., with both <italic>SNR</italic> &gt;10 and <italic>RV</italic> &lt;2%) – these constraints also equated the neuronal subpopulations in terms of firing rate (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3B</xref>). In (<bold>A–C</bold>), symbols, significance levels, and statistical tests are as in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.011">http://dx.doi.org/10.7554/eLife.22794.011</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig3-figsupp2-v1"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.012</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Sensitivity of rat visual neurons to luminance variations of the same object.</title><p>(<bold>A</bold>) The thin black curves show the sensitivity to luminance variations of the same object for the neurons recorded in each area. Each curve was obtained by taking, for each neuron, the responses across the four luminance levels that were tested for each object (i.e., 12.5%, 25%, 50% and full luminance; see the example images reported below the abscissa in the leftmost panel), and then averaging these responses across the 10 object identities. This yielded a curve showing the average response of the neuron to each luminance level. Each curve was then normalized to its maximum, so as to allow a better comparison among the different neurons in a given area. The colored curves show the averages of these luminance-sensitivity curves across all the neurons recorded in each area. Note that the luminance levels on the abscissa are reported on a logarithmic scale. (<bold>B</bold>) For each neuron, the sharpness of its sensitivity to luminance changes of the same object was quantified by computing the sparseness of the black curves shown in (<bold>A</bold>). The bars show how the sparseness decreased across the four visual areas (median over the units recorded in each area ± SE; **p&lt;0.01, 1-tailed U-test, Holm-Bonferroni corrected).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.012">http://dx.doi.org/10.7554/eLife.22794.012</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig3-figsupp3-v1"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.013</object-id><label>Figure 3—figure supplement 4.</label><caption><title>Information conveyed by the neuronal response about stimulus contrast and contrast-independent visual features.</title><p>(<bold>A</bold>) The full bars show the mutual information (median over the units recorded in each area ± SE) between stimulus and response in each visual area. The white portion of the bars shows <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, the information that each area carried about stimulus contrast <italic>C</italic> (as measured by the RF contrast metric; see Materials and methods), while the colored portion shows <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>'</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, the information about stimulus identity (<bold>S’</bold>), as defined by any possible visual attribute with the exception of the RF contrast. The number of cells in each area is written on the corresponding bar. Note these numbers are lower than the total numbers of neurons recorded in each area (e.g., compare to <xref ref-type="fig" rid="fig3">Figure 3B</xref>), because only a fraction of units met the criterion to be included in this analysis (see Materials and methods). Also note that, because only a subset of neurons and object conditions contributed to this analysis (see Materials and methods), the total stimulus information reported here for V1, LM and LI is lower than the total stimulus information shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. In particular, this is due to the fact that only stimuli that covered at least 10% of each RF were included in this analysis (see Materials and methods). This reduced the range of luminance values spanned by the stimulus conditions – a fact that, given the strong luminance sensitivity of V1, LM and LI neurons (see <xref ref-type="fig" rid="fig3">Figure 3B</xref>), produced the drop of stimulus information carried by individual neurons in these areas. No appreciable reduction of total stimulus information was found in LL (compare the fourth full bar in this figure to the matching bar in <xref ref-type="fig" rid="fig3">Figure 3B</xref>), thus confirming once more the much lower sensitive of LL neurons to stimulus luminance. (<bold>B</bold>) Median fraction of contrast-independent stimulus information that neurons carried in each area. Error bars are SEs of the medians (*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.013">http://dx.doi.org/10.7554/eLife.22794.013</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig3-figsupp4-v1"/></fig></fig-group></p><p>We then decomposed this overall stimulus information <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> into the sum of the information about stimulus luminance and the information about luminance-independent, higher-level features, using the following mathematical identity (<xref ref-type="bibr" rid="bib34">Ince et al., 2012</xref>):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mtext>|</mml:mtext><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <italic>L</italic> is the RF luminance of the visual stimuli; <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is the information that <italic>R</italic> conveys about <italic>L</italic>; and <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>'</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> measures how much information <italic>R</italic> carries about a variable <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> that denotes the identity of each stimulus condition <italic>S</italic>, as defined by any possible visual attribute with the exception of the RF luminance <italic>L</italic> (i.e., <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mtext> </mml:mtext><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>The overall amount of visual information decreased gradually and significantly along the areas’ progression (full bars in <xref ref-type="fig" rid="fig3">Figure 3B</xref>), with the median <italic>I</italic>(<italic>R;S</italic>) being about half in LL (~0.06 bits) than in V1 (~0.12 bits; 1-tailed, Mann-Whitney U-test, Holm-Bonferroni corrected for multiple comparisons; hereafter, unless otherwise stated, all the between-area comparisons have been statistically assessed with this test; see Material and methods). This decline was due to a loss of the energy-related information <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> (white portion of the bars), rather than to a drop of the higher-level, energy-independent information <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (colored portion of the bars), which changed little across the areas. As a result, the fraction of total information that neurons carried about higher-level visual attributes, i.e., <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, became about twice as large in LL (median ~0.5) as in V1, LM and LI, and such differences were all highly significant (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). All these trends were largely preserved when neurons in superficial and deep layers were considered separately (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) and did not depend on the quality of spike isolation (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>; see Discussion).</p><p>The decrease of sensitivity to stimulus luminance along the areas’ progression was confirmed by measuring the tuning of rat visual neurons across the four luminance changes each object underwent (i.e., 12.5%, 25%, 50% and 100% luminance; see <xref ref-type="fig" rid="fig1">Figure 1B.5</xref>). In all the areas, the luminance-sensitivity curves showed a tendency of the firing rate to increase as a function of object luminance (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A</xref>). However, such a growth was steeper in V1 and LM, as compared to LI and LL, where several neurons had a relatively flat tuning, with a peak, in some cases, at intermediate luminance levels. These trends were quantified by computing the sparseness of the response of each neuron over the luminance axis, which decreased monotonically along the areas’ progression (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3B</xref>), thus confirming the drop of sensitivity to luminance from V1 to LL.</p><p>To further explore whether rat lateral visual areas were differentially sensitive to other low-level properties, we defined a metric (<italic>RF contrast</italic>; see Materials and methods) that quantified the variability of the luminance pattern impinged by any given stimulus on a neuronal RF. We then measured how much information rat visual neurons carried about RF contrast, and how much information they carried about contrast-independent visual features (i.e., we applied <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, but with RF contrast instead of RF luminance). The information about RF contrast decreased monotonically along the areas’ progression (white portion of the bars in <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4A</xref>), while the contrast-independent information peaked in LL. As a consequence, the fraction of contrast-independent information carried by neuronal firing grew sharply and significantly from V1 to LI (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4B</xref>). Taken together, the results presented in this section show a clear tendency for low-level visual information to be substantially pruned along rat lateral extrastriate areas.</p></sec><sec id="s2-2"><title>The amount of view-invariant object information gradually increases from V1 to LL</title><p>Next, we explored whether neurons along lateral extrastriate areas also become gradually more capable of coding the identity of visual objects in spite of variation in their appearance (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>). To this aim, we relied on both information theoretic and linear decoding analyses. Both approaches have been extensively used to investigate the primate visual system, with mutual information yielding estimates of the capability of single neurons to code both low-level (e.g., contrast and orientation) and higher-level (e.g., faces) visual features at different time resolutions and during different time epochs of the response (<xref ref-type="bibr" rid="bib53">Optican and Richmond, 1987</xref>; <xref ref-type="bibr" rid="bib82">Tovee et al., 1994</xref>; <xref ref-type="bibr" rid="bib65">Rolls and Tovee, 1995</xref>; <xref ref-type="bibr" rid="bib77">Sugase et al., 1999</xref>; <xref ref-type="bibr" rid="bib46">Montemurro et al., 2008</xref>; <xref ref-type="bibr" rid="bib34">Ince et al., 2012</xref>), and linear decoders probing the suitability of neuronal populations to support easy readout of object identity (<xref ref-type="bibr" rid="bib33">Hung et al., 2005</xref>; <xref ref-type="bibr" rid="bib40">Li et al., 2009</xref>; <xref ref-type="bibr" rid="bib69">Rust and Dicarlo, 2010</xref>; <xref ref-type="bibr" rid="bib54">Pagan et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Baldassi et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Hong et al., 2016</xref>). In our study, we used both methods because of their complementary advantages. By computing mutual information, we estimated the overall amount of transformation-invariant information that single neurons conveyed about object identity. By applying linear decoders, we measured what fraction of such invariant information was formatted in a convenient, easy-to-read-out way, both at the level of single neurons and neuronal populations.</p><p>In the information theoretic analysis, we defined every stimulus condition <italic>S</italic> as a combination of object identity <inline-formula><mml:math id="inf14"><mml:mi>O</mml:mi></mml:math></inline-formula> and transformation <inline-formula><mml:math id="inf15"><mml:mi>T</mml:mi></mml:math></inline-formula> (i.e., <italic>S = O and T</italic>) and we expressed the overall stimulus information <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>T</mml:mi><mml:mtext>|</mml:mtext><mml:mi>O</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is the amount of view-invariant object information carried by neuronal firing, i.e., the information that <italic>R</italic> conveys about object identity, when the responses produced by the 23 transformations of an object (across repeated presentations) are considered together, so as to give rise to an overall response distribution (see illustration in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, bottom). The other term, <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>T</mml:mi><mml:mo>|</mml:mo><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, is the information that <italic>R</italic> carries about the specific transformation of an object, once its identity has been fixed.<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.22794.014</object-id><label>Figure 4.</label><caption><title>Comparing total visual information and view-invariant object information per neuron.</title><p>(<bold>A</bold>) Illustration of how total visual information and view-invariant object information per neuron were computed, given an object pair. In the first case, all the views of the two objects were considered as different stimulus conditions, each giving rise to its own response distribution (colored curves). In the second case, the response distributions produced by different views of the same object were merged into a single, overall distribution (shown in blue and red, respectively, for the two objects). (<bold>B</bold>) Total visual information per neuron (median over the units recorded in each area ± SE) as a function of the similarity between the RF luminance of the objects in each pair, as defined by <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (see Results). (<bold>C</bold>) Left: view-invariant object information per neuron (median ± SE) as a function of <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Right: view-invariant object information per neuron (median ± SE) for <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> (*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected). The number of cells in each area is written on the corresponding bar. (<bold>D</bold>) Ratio between view-invariant object information and total information per neuron (median ± SE), for <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>. Significance levels/test as in (<bold>C</bold>). The invariant object information carried by RF luminance as a function of <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is reported in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.014">http://dx.doi.org/10.7554/eLife.22794.014</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig4-v1"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.015</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Invariant object information carried by RF luminance as a function of <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</title><p>For each neuron and object pair, we computed <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the mutual information between object identity (with all the views of each object in the pair taken into account) and RF luminance. The colored curves show the median of <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (± SE) across all the units recorded in each area as a function of the similarity between the RF luminance of the objects in each pair, as defined by <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (see Results).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.015">http://dx.doi.org/10.7554/eLife.22794.015</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig4-figsupp1-v1"/></fig></fig-group></p><p>Given a neuron, we computed these information metrics for all possible pairs of object identities, while, at the same time, measuring the similarity between the objects in each pair in terms of RF luminance. Such similarity was evaluated by computing the ratio between the mean luminance of the dimmer object (across its 23 views) and the mean luminance of the brighter one – the resulting <italic>luminosity ratio</italic> ranged from zero (dimmer object fully dark) to one (both objects with the same luminance). By considering object pairs with a luminosity ratio larger than a given threshold <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and allowing <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to range from zero to one, we could restrict the computation of the information metrics to object pairs that were progressively less discriminable based on luminance differences, thus probing to what extent the ability of a neuron to code invariantly object identity depended on its luminance sensitivity.</p><p>The overall stimulus information per neuron <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> followed the same trend already shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref> (i.e., it decreased gradually along the areas’ progression) and such trend remained largely unchanged as a function of <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). By contrast, the amount of view-invariant object information per neuron <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> strongly depended on <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, left). When all object pairs were considered (<inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), areas LM and LI conveyed the largest amount of invariant information, followed by V1 and LL. This trend remained stable until <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> reached 0.3, after which the amount of invariant information in V1, LM and LI dropped sharply. By contrast, the invariant information in LL remained stable until <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> approached 0.7, after which it started to increase. As a result, when only pairs of objects with very similar luminosity were considered (<inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>), a clear gradient emerged across the four areas, with the invariant information in LI and LL being significantly larger than in V1 and LM (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, right).</p><p>These results indicate that neurons in V1, LM and LI were able to rely on their sharp sensitivity to luminous energy (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) and use luminance as a cue to convey relatively large amount of invariant object information, when such cue was available (i.e., for small values of <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). The example V1 neuron shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref> is one of such units. This cell successfully supported the discrimination of some object pairs only because of its strong sensitivity to stimulus luminance (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), which, for those pairs, happened to co-vary with object identity, in spite of the position changes and the other transformations that the objects underwent (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A–B</xref>). In cases like this, observing a large amount of view-invariant information would be an artifact, because luminance would not at all be diagnostic of object identity, if these neurons were probed with a variety of object appearances as large as the one experienced during natural vision (where each object can project thousands of different images on the retina). It is only because of the limited range of transformations that are testable in a neurophysiology experiment that luminance can possibly serve as a transformation-invariant cue of object identity.</p><p>To verify that luminance could indeed act as a transformation-invariant cue, we measured <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> – the amount of view-invariant object information conveyed by RF luminance alone (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). As expected, when no restriction was applied to the luminance difference of the objects to discriminate (i.e., for small values of <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was very large in all the areas. This confirmed that RF luminance, by itself, was able to convey a substantial amount of invariant object information. When <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> was allowed to increase, <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> dropped sharply, eventually reaching zero in all the areas for <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>. Interestingly, this decrease was the same found for <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> in V1, LM and LI (see <xref ref-type="fig" rid="fig4">Figure 4C</xref>), thus showing that a large fraction of the invariant information observed in these areas (but not in LL) at low <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> was indeed accounted for by luminance differences between the objects in the pairs. Hence, the need of setting <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>, thus considering only pairs of objects with very similar luminance to nullify the luminance confound, when comparing the areas in terms of their ability to support invariant recognition.</p><p>Our analysis shows that, when this restriction was applied, a clear gradient emerged along the areas’ progression, with LL conveying the largest amount of invariant information per neuron, followed by LI and then by V1/LM (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, right). A similar, but sharper trend was observed when the relative contribution of the view-invariant information to the total information was measured, i.e., when the ratio between <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> was computed (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). The fraction of invariant information increased very steeply and significantly along the areas’ progression, being almost four times larger in LL than in V1/LM, and ~1.7 times larger in LL than in LI. Overall, these results indicate that the information that single neurons are able to convey about the identity of visual objects, in spite of variation in their appearance, becomes gradually larger along rat lateral visual areas (see Discussion for further implications of these findings).</p></sec><sec id="s2-3"><title>Object representations become more linearly separable from V1 to LL, and better capable of supporting generalization to novel object views</title><p><inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) provides an upper bound to the amount of transformation-invariant information that single neurons can encode about object identity, but it does not quantifies how easily this information can be read out by simple linear decoders (see illustration in <xref ref-type="fig" rid="fig5">Figure 5A</xref>). Assessing this property, known as <italic>linear separability</italic> of object representations, is crucial, because attaining progressively larger levels of linear separability is considered the key computational goal of the ventral stream (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>). In our study, we first measured linear separability at the single-cell level – i.e., given a neuron and a pair of objects, we tested the ability of a binary linear decoder to correctly label the responses produced by the 23 views of each object (this was done using the cross-validation procedure described in Materials and methods). For consistency with the previous analyses, the discrimination performance of the decoders was computed as the mutual information between the actual and the predicted object labels from the decoding outcomes (<xref ref-type="bibr" rid="bib60">Quiroga and Panzeri, 2009</xref>).<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.22794.016</object-id><label>Figure 5.</label><caption><title>Linear separability of object representations at the single-neuron level.</title><p>(<bold>A</bold>) Illustration of how a similar amount of view-invariant information per neuron can be encoded by object representations with a different degree of linear separability. The overlap between the response distributions produced by two objects across multiple views is similarly small in the top and bottom examples; hence, the view-invariant object information per neuron is similarly large. However, only for the distributions shown at the bottom, a single discrimination boundary can be found (dashed line) that allows discriminating the objects regardless of their specific view. (<bold>B</bold>) Linear separability of object representations at the single-neuron level (median over the units recorded in each area ± SE; *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected). The number of cells in each area is written on the corresponding bar. (<bold>C</bold>) Ratio between linear separability, as computed in (<bold>B</bold>), and view-invariant object information, as computed in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, right (median ± SE). Significance levels/test as in (<bold>B</bold>). In both (<bold>B</bold>) and (<bold>C</bold>), <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.016">http://dx.doi.org/10.7554/eLife.22794.016</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig5-v1"/></fig></p><p>The linear separability of object representations at the single-neuron level increased monotonically and significantly along the areas’ progression (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), being ~4 times larger in LL as in V1 and LM, and reaching an intermediate value in LI. This increase was steeper than the growth of the view-invariant information <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, right), thus suggesting that not only LL neurons encoded more invariant information than neurons in the other areas, but also that a lager fraction of this information was linearly decodable in LL. To quantify this observation we computed, for each cell, the ratio between linear separability (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) and amount of invariant information (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, right). The resulting fraction of invariant information that was linearly decodable increased from ~55% in V1 to ~70% in LL, being significantly larger in LL than in any other area (<xref ref-type="fig" rid="fig5">Figure 5C</xref>).</p><p>An alternative (more stringent) way to assess transformation-tolerance is to measure to what extent a linear decoder, trained with a single view per object, is able to discriminate the other (untrained) views of the same objects (see illustration in <xref ref-type="fig" rid="fig6">Figure 6A</xref>) – a property known as <italic>generalization</italic> across transformations. Since, at the population level, large linear separability does not necessarily imply large generalization performance (<xref ref-type="bibr" rid="bib69">Rust and Dicarlo, 2010</xref>) (see also Figure 8A), it was important to test rat visual neurons with regard to the latter property too (see Materials and methods). Our analysis revealed that, when assessed at the single-cell level, the ability of rat visual neurons to support generalization to novel object views increased as significantly and steeply as linear separability (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Interestingly, this trend was widespread across the whole cortical thickness and equally sharp in superficial and deep layers, although the generalization performances were larger in the latter (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). These conclusions were supported by a two-way ANOVA with <italic>visual area</italic> and <italic>layer</italic> as factors, yielding a significant main effect for both area and layer (p&lt;0.001, <italic>F</italic><sub>3,687</sub> = 9.9 and <italic>F</italic><sub>1,687</sub> = 6.93, respectively) but no significant interaction (p&gt;0.15, <italic>F</italic><sub>3,687</sub> = 1.74).<fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.22794.017</object-id><label>Figure 6.</label><caption><title>Ability of single neurons to support generalization to novel object views.</title><p>(<bold>A</bold>) Illustration of how the ability of single neurons to discriminate novel views of previously trained objects was assessed. The blue and red curves refer to the hypothetical response distributions evoked by different views of two objects. A binary decoder is trained to discriminate two specific views (darker curves, left), and then tested for its ability to correctly recognize two different views (darker curves, right), using the previously learned discrimination boundary (dashed lines). (<bold>B</bold>) Generalization performance achieved by single neurons with novel object views (median over the units recorded in each area ± SE; *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected). The number of cells in each area is written on the corresponding bar. (<bold>C</bold>) Median generalization performances (± SE) achieved by single neurons for the neuronal subpopulations sampled from cortical layers II-IV (left) and V-VI (right). Significance levels/test as in (<bold>B</bold>). Note that the laminar location was not retrieved for all the recorded units (see Materials and methods). (<bold>D</bold>) Median generalization performances (± SE) achieved by single neurons, computed along individual transformation axes. In (<bold>C–D</bold>), significance levels/test are as in (<bold>B</bold>). <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> in (<bold>B–D</bold>). The generalization performances achieved by single neurons across parametric position and size changes are reported in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. The generalization performances obtained for neuronal subpopulations with matched spike isolation quality are shown in <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>. The firing rate magnitude measured in the four areas (before and after matching the neuronal populations in terms of spike isolation quality) is reported in <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.017">http://dx.doi.org/10.7554/eLife.22794.017</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig6-v1"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.018</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Generalization achieved by single neurons across parametric position and size changes.</title><p>(<bold>A</bold>) The curves show the generalization performances of binary decoders (median over the units recorded in each area ± SE) that were trained to discriminate two objects presented at the same position in the visual field, and then were required to discriminate those same objects across increasingly wider positions changes, with respect to the training location (see Materials and methods for details). Differences among the four areas were quantified by a two-way ANOVA with <italic>visual area</italic> and <italic>distance from the training position</italic> as factors. The main effects of area (<italic>F</italic><sub>3,595</sub> = 4.078) and distance (<italic>F</italic><sub>3,1785</sub> = 2.706) were both significant (p&lt;0.01 and p&lt;0.05, respectively), thus confirming the existence of a gradient in the amount of position tolerance along the areas’ progression and along the distance axis. The interaction term was also significant (p&lt;0.001, <italic>F</italic><sub>9,1785</sub> = 3.239), indicating that the four decoding performances dropped at a different pace along the distance axis. (<bold>B</bold>) Discrimination performances of binary decoders (median over the units recorded in each area ± SE) that were required to generalize from a given training size to either a smaller (left) or larger (right) test size (see Materials and methods for details; *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.018">http://dx.doi.org/10.7554/eLife.22794.018</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig6-figsupp1-v1"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.019</object-id><label>Figure 6—figure supplement 2.</label><caption><title>Independence of the generalization performances yielded by single neurons from the quality of spike isolation.</title><p>(<bold>A–B</bold>) Same decoding analysis as the one shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref>, but restricted to units within a given tirtile of the <italic>SNR</italic> and <italic>RV</italic> metrics, used to asses the quality of spike isolation (see Materials and methods and the legend of <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). The number of units in each area per tirtile is the same reported in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A–B</xref>. (<bold>C</bold>) Same decoding analysis, considering only the neuronal subpopulations with good spike isolation quality (i.e., with both <italic>SNR</italic> &gt;10 and <italic>RV</italic> &lt;2%) – these constraints also equated the neuronal subpopulations in terms of firing rate (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3B</xref>). In (<bold>A–C</bold>), symbols, significance levels, and statistical tests are as in <xref ref-type="fig" rid="fig6">Figure 6B</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.019">http://dx.doi.org/10.7554/eLife.22794.019</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig6-figsupp2-v1"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.020</object-id><label>Figure 6—figure supplement 3.</label><caption><title>Magnitude of the firing rates.</title><p>(<bold>A</bold>) Median number of spikes per second fired by the neurons in each visual area, as a response to: (1) the most effective object condition (left); (2) the 10 most effective object conditions (middle); and (3) all the 230 object conditions used in the single-cell information theoretic and decoding analyses. Error bars are SE of the medians. Stars indicate the statistical significance of pairwise comparisons (*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected). The number of cells in each area is superimposed to the corresponding bar. (<bold>B</bold>) Same plots as in (<bold>A</bold>), but after considering only the neuronal subpopulations with good spike isolation quality (i.e., same subpopulations as in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C</xref> and <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2C</xref>). As a result of this selection, the median firing rates became statistically indistinguishable across all the four visual areas.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.020">http://dx.doi.org/10.7554/eLife.22794.020</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig6-figsupp3-v1"/></fig></fig-group></p><p>The growth of transformation tolerance from V1 to LL was also observed when the individual transformation axes were considered separately in the decoding analysis (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). In this case, the training and testing views of the objects to be decoded were all randomly sampled from the same axis of variation: either position, size, rotation (both in-plane and in-depth, pooled together) or luminance. In all four cases, LL yielded the largest decoding performance, typically followed by LI and then by the more medial areas (V1 and LM). The difference between LL and V1/LM was statistically significant for the position, size and rotation changes. Other comparisons were also significant, such as LL vs. LI for the position and size transformations, and LI vs. V1 and LM for the rotations.</p><p>To further compare the position tolerance afforded by the four visual areas, we also measured the ability of single neurons to support generalization across increasingly wider positions changes – a decoder was trained to discriminate two objects presented at the same position in the visual field, and then tested for its ability to discriminate those same objects, when presented at positions that were increasingly distant from the training location (Materials and methods). The neurons in LL consistently yielded the largest generalization performance, which remained very stable (invariant) as a function of the distance from the training location (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>). A much lower performance was observed for LI and LM, while V1 displayed the steepest decrease along the distance axis (these observations were all statistically significant, as assessed by a two-way ANOVA with <italic>visual area</italic> and <italic>distance from the training position</italic> as factors; see the legend of <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref> for details). A similar analysis was carried out to quantify the tolerance to size changes. In this case, we trained binary decoders to discriminate two objects at a given size, and then tested how well they generalized when those same objects were shown at either smaller or larger sizes (Materials and methods). The resulting patterns of generalization performances (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref>) confirmed once more the significantly larger tolerance afforded by LL neurons, as compared to the other visual areas. Taken together, the results of <xref ref-type="fig" rid="fig6">Figure 6D</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> indicate that the growth of tolerance across the areas’ progression was widespread across all tested transformations axes, ultimately yielding to the emergence, in LL, of a general-purpose representation that tolerated a wide spectrum of image-level variation (see Discussion).</p><p>Inspired by previous studies of the ventral stream (<xref ref-type="bibr" rid="bib40">Li et al., 2009</xref>; <xref ref-type="bibr" rid="bib69">Rust and Dicarlo, 2010</xref>), we also tested to what extent RF size played a role in determining the growth of invariance across rat lateral visual areas (intuitively, neurons with large RFs should respond to their preferred visual features over a wide span of the visual field, thus displaying high position and size tolerance). Earlier investigations of rat visual cortex have shown that RF size increases along the V1-LM-LI-LL progression (<xref ref-type="bibr" rid="bib20">Espinoza and Thomas, 1983</xref>; <xref ref-type="bibr" rid="bib86">Vermaercke et al., 2014</xref>). Our recordings confirmed and expanded these previous observations, showing that RF size (defined in Materials and methods) grew significantly at each step along the areas’ progression (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), with the median in LL (~30° of visual angle) being about twice as large as in V1. At the same time, RF size varied widely within each area, resulting in a large overlap among the distributions obtained for the four populations. This allowed sampling the largest possible V1, LM, LI and LL subpopulations with matched RF size ranges (gray patches in <xref ref-type="fig" rid="fig7">Figure 7B</xref>), and computing the generalization performances yielded by these subpopulations (<xref ref-type="fig" rid="fig7">Figure 7C</xref>) – again, LL and LI afforded significantly larger tolerance than V1 and LM. A similar result was found for the fraction of energy-independent stimulus information <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> carried by these RF-matched subpopulations (<xref ref-type="fig" rid="fig7">Figure 7D</xref>) – <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> followed the same trend observed for the whole populations (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), with a sharp, significant increase from the more medial areas to LL.<fig-group><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.22794.021</object-id><label>Figure 7.</label><caption><title>Single-neuron decoding and mutual information metrics, compared across neuronal subpopulations with matched RF size.</title><p>(<bold>A</bold>) Spreads (left) and medians ± SE (right) of the RF sizes measured in each visual area (*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected). The number of cells in each area for which RF size could be estimated is reported on the corresponding bar of the right chart. Spreads and medians of the response latencies are reported in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>. (<bold>B</bold>) For each visual area, the generalization performances achieved by single neurons (same data of <xref ref-type="fig" rid="fig6">Figure 6B</xref>) are plotted against the RF sizes (same data of panel A). In the case of LI and LL, these two metrics were significantly anti-correlated (**p&lt;0.01; 2-tailed t-test). (<bold>C</bold>) Median generalization performances (± SE) achieved by single neurons, as in <xref ref-type="fig" rid="fig6">Figure 6B</xref>, but considering only neuronal subpopulations with matched RF size ranges, indicated by the gray patches in (<bold>B</bold>) (*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected). The number of neurons in each area fulfilling this constraint is reported on the corresponding bar. (<bold>D</bold>) Median fraction of luminance-independent stimulus information (± SE) conveyed by neuronal firing as in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, but including only the RF-matched subpopulations used in (<bold>C</bold>). Significance levels/test as in (<bold>C</bold>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.021">http://dx.doi.org/10.7554/eLife.22794.021</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig7-v1"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.022</object-id><label>Figure 7—figure supplement 1.</label><caption><title>Response latencies.</title><p>Spreads (left) and medians ± SE (right) of the response latencies measured in each visual area (*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected). The number of cells in each area is reported on the corresponding bar of the right chart.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.022">http://dx.doi.org/10.7554/eLife.22794.022</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig7-figsupp1-v1"/></fig></fig-group></p><p>Interestingly, the decoding performances obtained for the RF-matched subpopulations were larger than those obtained for the whole populations, especially in the more lateral areas (compare <xref ref-type="fig" rid="fig7">Figure 7C</xref> to <xref ref-type="fig" rid="fig6">Figure 6B</xref>). This means that the range of RF sizes that was common to the four populations contained the neurons that, in each area, yielded the largest decoding performances. This was the result of the specific relationship that was observed, within each neuronal population, between performance and RF size (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). While in V1 performance slightly increased as a function of RF size (although not significantly; p&gt;0.05; two-tailed t-test), performance and RF size were significantly anti-correlated in LI and LL (p&lt;0.01). As explained in the Discussion, these findings suggest a striking similarity with the tuning properties of neurons at the highest stages of the monkey ventral stream (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>).</p></sec><sec id="s2-4"><title>Linear readout of population activity confirms the steep increase of transformation-tolerance along rat lateral visual areas</title><p>The growth of transformation tolerance reported in the previous section was highly significant and quite substantial, in relative terms. In LL, the discrimination performances were typically 2–4 times larger than in V1, yet, in absolute terms, their magnitude was in the order of a few thousandths of a bit. This raised the question of whether such single-unit variations would translate into macroscopic differences among the areas, at the neuronal population level.</p><p>To address this issue, we performed a population decoding analysis, in which we trained binary linear classifiers to read out visual object identity from the activity of neuronal populations of increasing size in V1, LM, LI and LL. Random subpopulations of <italic>N</italic> units, with <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mn>12</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mn>24</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mn>48</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mn>96</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, were sampled from the full sets of neurons in each area. Given a subpopulation, the response axes of the sampled units formed a vector space, where each object condition was represented by the cloud of population response vectors produced by the repeated presentation of the condition across multiple trials (see illustration in <xref ref-type="fig" rid="fig8">Figure 8A</xref>). The binary classifiers were required to correctly label the population vectors produced by different pairs of objects across transformations, thus testing to what extent the underlying object representations were linearly separable (<xref ref-type="fig" rid="fig8">Figure 8A</xref>, left) and generalizable (<xref ref-type="fig" rid="fig8">Figure 8A</xref>, right). To avoid the luminance tuning confound, a pair of objects was included in the analysis, only if at least 96 neurons in each area could be found for which the luminance ratio of the pair was larger than a given threshold <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. We set <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to the largest value (0.8) that yielded at least three object pairs (<xref ref-type="fig" rid="fig8">Figure 8B</xref> show the pairs that met this criterion, while <xref ref-type="fig" rid="fig8">Figure 8C and E</xref> show the mean classification performances over these three pairs).<fig-group><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.22794.023</object-id><label>Figure 8.</label><caption><title>Linear separability and generalization of object representations, tested at the level of neuronal populations.</title><p>(<bold>A</bold>) Illustration of the population decoding analyses used to test linear separability and generalization. The clouds of dots show the sets of response population vectors produced by different views of two objects. Left: for the test of linear separability, a binary linear decoder is trained with a fraction of the response vectors (filled dots) to all the views of both objects, and then tested with the left-out response vectors (empty dots), using the previously learned discrimination boundary (dashed line). The cartoon depicts the ideal case of two object representations that are perfectly separable. Right: for the test of generalization, a binary linear decoder is trained with all the response vectors (filled dots) produced by a single view per object, and then tested for its ability to correctly discriminate the response vectors (empty dots) produced by the other views, using the previously learned discrimination boundary (dashed line). As illustrated here, perfect linearly separability does not guarantee perfect generalization to untrained object views (see the black-filled, mislabeled response vectors in the right panel). (<bold>B</bold>) The three pairs of visual objects that were selected for the population decoding analyses shown in C-F, based on the fact that their luminance ratio fulfilled the constraint of being larger than <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for at least 96 neurons in each area. (<bold>C</bold>) Classification performance of the binary linear decoders in the test for linear separability, as a function of the number of neurons <italic>N</italic> used to build the population vector space. Performances were computed for the three pairs of objects shown in (<bold>B</bold>). Each dot shows the mean of the performances obtained for the three pairs (± SE). The performances are reported as the mutual information between the actual and the predicted object labels (left). In addition, for <italic>N</italic> = 96, they are also shown in terms of classification accuracy (right). The dashed lines (left) and the horizontal marks (right) show the linear separability of arbitrary groups of views of two objects (same three pairs used in the main analysis; see Results). (<bold>D</bold>) The statistical significance of each pairwise area comparison, in terms of linear separability, is reported for each individual object pair (1-tailed U-test, Holm-Bonferroni corrected). In the pie charts, a black slice indicates that the test was significant (p&lt;0.001) for the corresponding pairs of objects and areas (e.g., LL &gt; LI). (<bold>E</bold>) Classification performance of the binary linear decoders in the test for generalization across transformations. Same description as in (<bold>C</bold>). (<bold>F</bold>) Statistical significance of each pairwise area comparison, in terms of generalization across transformations. Same description as in (<bold>D</bold>). The same analyses, performed over a larger set of object pairs, after setting <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula>, are shown in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>. The dependence of linear separability and generalization from <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is shown in <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>. The statistical comparison between the performances achieved by a population of 48 LL neurons and V1, LM and LI populations of 96 neurons is reported in <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.023">http://dx.doi.org/10.7554/eLife.22794.023</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig8-v1"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.024</object-id><label>Figure 8—figure supplement 1.</label><caption><title>Linear separability and generalization of object representations, tested at the level of neuronal populations, using a larger set of object pairs.</title><p>(<bold>A</bold>) Classification performance of binary linear decoders in the test for linear separability (see <xref ref-type="fig" rid="fig8">Figure 8A</xref>, left), as a function of the size of the neuronal subpopulations used to build the population vector space. This plot is equivalent to the one shown in <xref ref-type="fig" rid="fig8">Figure 8C</xref>, with the difference that, here, the objects that the decoders had to discriminate were allowed to differ more in terms of luminosity (this was achieved by setting <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula>). As a result, much more object pairs (23) could be tested, compared to the analysis shown in <xref ref-type="fig" rid="fig8">Figure 8C</xref>. Each dot shows the mean of the 23 performances obtained for these object pairs and the error bar shows its SE. The larger number of object pairs allowed applying a 1-tailed, paired t-test (with Holm-Bonferroni correction) to assess whether the differences among the average performances in the four areas were statistically significant (*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001). The performances are reported both as mutual information between actual and predicted object labels (left) and as classification accuracy (i.e., as the percentage of correctly labeled response vectors; right). (<bold>B</bold>) Classification performance of binary linear decoders in the test for generalization across transformations (see <xref ref-type="fig" rid="fig8">Figure 8A</xref>, right). Same description as in (<bold>A</bold>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.024">http://dx.doi.org/10.7554/eLife.22794.024</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig8-figsupp1-v1"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.025</object-id><label>Figure 8—figure supplement 2.</label><caption><title>Dependence of linear separability and generalization, measured at the neuronal population level, from the luminance difference of the objects to discriminate.</title><p>(<bold>A</bold>) Classification performance of binary linear decoders in the test for linear separability (see <xref ref-type="fig" rid="fig8">Figure 8A</xref>, left) as a function of the similarity between the RF luminance of the objects to discriminate, as defined by <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (see Results). The curves, which were produced using populations of 96 neurons, report the median performance in each visual area (± SE) over all the object pairs obtained for a given value of <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Note that, for <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula>, the performances are equivalent to those already shown in the left panels <xref ref-type="fig" rid="fig8">Figure 8C</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1A</xref> (rightmost points). Also note that, for <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, all the available object pairs contributed to the analysis. As such, the corresponding performances are those yielded by the four visual areas when no restriction was applied to the luminosity of the objects to discriminate. (<bold>B</bold>) Same analysis as in (<bold>A</bold>), but for the test of generalization across transformations (see <xref ref-type="fig" rid="fig8">Figure 8A</xref>, right).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.025">http://dx.doi.org/10.7554/eLife.22794.025</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig8-figsupp2-v1"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22794.026</object-id><label>Figure 8—figure supplement 3.</label><caption><title>Statistical comparison between the performance achieved by a population of 48 LL neurons and the performances yielded by populations of 96 neurons in V1, LM and LI.</title><p>(<bold>A</bold>) For each of the three object pairs tested in <xref ref-type="fig" rid="fig8">Figure 8</xref> (shown in <xref ref-type="fig" rid="fig8">Figure 8B</xref>), we checked whether a population of 48 LL neurons yielded a significantly larger performance than V1, LM and LI populations of twice the number of neurons (i.e., 96 units) in the linear discriminability task (see <xref ref-type="fig" rid="fig8">Figure 8A</xref>, left). The resulting pie chart shown here should be compared to the rightmost column of the pie chart in <xref ref-type="fig" rid="fig8">Figure 8D</xref>, with a black slice indicating that the comparison was significant (p&lt;0.001; 1-tailed U-test, Holm-Bonferroni corrected) for the corresponding pairs of objects and areas – e.g., LL (48 units) &gt; LI (96 units). (<bold>B</bold>) Same analysis as in (<bold>A</bold>), but for the test of generalization across transformations (see <xref ref-type="fig" rid="fig8">Figure 8A</xref>, right). The pie chart shown here should be compared to the rightmost column of the pie chart in <xref ref-type="fig" rid="fig8">Figure 8F</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22794.026">http://dx.doi.org/10.7554/eLife.22794.026</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22794-fig8-figsupp3-v1"/></fig></fig-group></p><p>The ability of the classifiers to linearly discriminate the object pairs increased sharply as a function of <italic>N</italic> (<xref ref-type="fig" rid="fig8">Figure 8C</xref>, left). In LL, the performance grew of ~500%, when <italic>N</italic> increased from 6 to 96, reaching ~0.22 bits, which was nearly twice as large as the performance obtained in LI. More in general, linear separability grew considerably along the areas’ progression, following the same trend observed at the single-cell level (see <xref ref-type="fig" rid="fig5">Figure 5B</xref>), but with performances that were about two orders of magnitude larger (for <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>96</mml:mn></mml:mrow></mml:math></inline-formula>). As a result, the differences among the areas became macroscopic – when measured in terms of classification accuracy (<xref ref-type="fig" rid="fig8">Figure 8C</xref>, right), LL performance (~76% correct discrimination) was about eight percentage points above LI, 10 above V1 and 17 above LM. Given the small number of object pairs that could be tested in this analysis (3), the significance of each pairwise area comparison was assessed at the level of every single pair – e.g., we tested if the objects belonging to a pair were better separable in the LL than in the LI representation (this test was performed for <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>96</mml:mn></mml:mrow></mml:math></inline-formula>, using, as a source of variability for the performances, the 50 resampling iterations carried out for each object pair; see Materials and methods). For all object pairs, the performances yielded by LL were significantly larger than in any other area (last column of <xref ref-type="fig" rid="fig8">Figure 8D</xref>). In the case of LI, the performances were significantly larger than in LM and V1, respectively, for all and two out of three pairs (middle column of <xref ref-type="fig" rid="fig8">Figure 8D</xref>). Following the same rationale of a recent primate study (<xref ref-type="bibr" rid="bib69">Rust and Dicarlo, 2010</xref>), we also checked how well binary linear classifiers could separate the representations of two arbitrary groups of object views – i.e., with each group containing half randomly-chosen views of one of the objects in the pair, and half randomly-chosen views of the other object (Materials and methods). For all the areas, the resulting discrimination performances were barely above the chance level (i.e., 0 bits and 50% correct discrimination; see dashed lines in <xref ref-type="fig" rid="fig8">Figure 8C</xref>). This means that rat lateral visual areas progressively reformat object representations, so as to make them more suitable to support <italic>specifically</italic> the discrimination of visual objects across view changes, and not <italic>generically</italic> the discrimination of arbitrary image collections.</p><p>In the test of generalization to novel object views, the classification performances (<xref ref-type="fig" rid="fig8">Figure 8E</xref>, left) were about one order of magnitude smaller than those obtained in the test of linear separability (<xref ref-type="fig" rid="fig8">Figure 8C</xref>, left). Still, for <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>96</mml:mn></mml:mrow></mml:math></inline-formula>, they were about one order of magnitude larger than those obtained in the generalization task for single neurons (compare to <xref ref-type="fig" rid="fig6">Figure 6B</xref>). In LL, the performance increased very steeply as a function of <italic>N</italic>, reaching ~0.032 bits for <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>96</mml:mn></mml:mrow></mml:math></inline-formula>, which was more than three times larger than what obtained in LI. This implies a macroscopic advantage of LL, over the other areas, in terms of generalization ability – when measured in terms of accuracy (<xref ref-type="fig" rid="fig8">Figure 8E</xref>, right), the performances in V1, LM and LI were barely above the 50% chance level, while, in LL, they approached 60% correct discrimination. This achievement is far from trivial, given how challenging the discrimination task was, requiring generalization from a single view per object to many other views, spread across five different variation axes. Again, the statistical significance of each pairwise area comparison was assessed at the level of the individual object pairs. For all the pairs, the performances yielded by LL were significantly larger than in any other area (last column of <xref ref-type="fig" rid="fig8">Figure 8F</xref>), while, for LI, the performances were significantly larger than in LM and V1 for two out of three pairs (middle column of <xref ref-type="fig" rid="fig8">Figure 8F</xref>).</p><p>To check the generality of our conclusions, we repeated these decoding analyses after loosening the constraint on the luminance ratio, i.e., after lowering <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to 0.6, which yielded a larger number of object pairs (23). Linear separability and generalization still largely followed the trends shown in <xref ref-type="fig" rid="fig8">Figure 8C and E</xref>, with LL yielding the largest performances, and LM the lowest ones (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). The main difference was that the performances in V1 were larger, reaching the same level of LI. This was expected, since lowering <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> made it easier for V1, given its sharp tuning for luminosity, to discriminate the object pairs based on their luminance difference. In fact, it should be noticed that whatever cue is available to single neurons to succeed in an invariant discrimination task, that same cue will also be effective at the population level, because the population will inherit the sensitivity to the cue of its constituent neurons. This was the case of the luminance difference between the objects in a pair, which, unless constrained to be minimal, acted as a transformation-invariant cue for the V1, LM and LI populations. This is shown by the dependence of linear separability and generalization, in these areas, on <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, while, for the LL population, both metrics remained virtually unchanged as a function of <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). Hence, the need of matching as closely as possible the luminance of the objects, while assessing transformation tolerance, also at the population level.</p><p>Finally, we asked to what extent the decoding performances observed at the population level could be explained by the single-neuron properties illustrated in the previous sections. One possibility was that the superior tolerance achieved by the LL population resulted from the larger view-invariant information carried by the individual LL neurons, as reported in <xref ref-type="fig" rid="fig4">Figure 4C</xref> (bar plot). To test whether this was the case, we matched the LL population and the other populations in terms of the total information they carried, based on the per-neuron invariant information observed in the four areas. Specifically, since in LL the invariant information per neuron was about twice as large as in V1 and LM and about 50% larger than in LI, we compared an LL population of <italic>N</italic> units to V1, LM and LI populations of 2<italic>N</italic> units. This ensured that the latter would approximately have an overall view-invariant information that was either equal to or larger than the one of the LL population. We carried out this comparison by counting how many object pairs were still better discriminated by a population of 48 LL neurons, as compared to V1, LM and LI populations of 96 units (this is equivalent to compare the second-last red point to the last green, cyan and violet points in <xref ref-type="fig" rid="fig8">Figure 8C and E</xref>). We found that, consistently with what reported when comparing populations of equal size (<xref ref-type="fig" rid="fig8">Figure 8C and E</xref>), also in this case LL yielded significantly larger performances than the other areas in all comparisons but one (see <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>). This indicates that the larger view-invariant information per neuron observed in LL is not sufficient, by itself, to explain the extent by which the LL population surpasses the other populations in the linear discriminability and generalization tasks – the better format of the LL representation plays a key role in determining its tolerance.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we investigated whether a functional specialization for the processing of object information is implemented along the anatomical progression of extrastriate areas (LM, LI and LL) that, in the rat brain, run laterally to V1. Our experiments revealed that many neuronal processing properties followed a gradual, largely monotonic trend of variation along the V1-LI-LL progression. Specifically, we observed: (1) a pruning of low-level information about stimulus luminosity, essentially without loss of higher-level visual information (<xref ref-type="fig" rid="fig3">Figure 3</xref>); (2) an increase of the view-invariant object information conveyed by neuronal firing (<xref ref-type="fig" rid="fig4">Figure 4</xref>); and (3) a growth in the ability of both single neurons (<xref ref-type="fig" rid="fig5">Figures 5</xref>–<xref ref-type="fig" rid="fig6">6</xref>) and neuronal populations (<xref ref-type="fig" rid="fig8">Figure 8</xref>) to support discrimination of visual objects in spite of transformation in their appearance.</p><p>All these trends match very closely the key computations that are expected to take place along a feed-forward object-processing hierarchy (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>). Additionally, the tuning properties underlying the invariance attained by LI and LL are remarkably consistent with those found at the highest stages of the monkey ventral stream (<xref ref-type="bibr" rid="bib40">Li et al., 2009</xref>; <xref ref-type="bibr" rid="bib69">Rust and Dicarlo, 2010</xref>). For both areas, the larger separability of object representations across view changes (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), and not the shear increase of RF size (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), was the key factor at the root of their superior ability to code invariantly object identity (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). The same applies to the trade-off between RF size and object discriminability that was found for LI and LL (<xref ref-type="fig" rid="fig7">Figure 7B</xref>), which is reminiscent of the negative relationship between tolerance and selectivity observed in primate inferotemporal cortex (IT) and area V4 (<xref ref-type="bibr" rid="bib96">Zoccolan et al., 2007</xref>; <xref ref-type="bibr" rid="bib48">Nandy et al., 2013</xref>; <xref ref-type="bibr" rid="bib75">Sharpee et al., 2013</xref>). Critically, these findings do not imply an exact one-to-one correspondence between areas LI and LL in the rat and areas V4 and IT in the monkey. Establishing such a parallel would require a quantitative comparison between rat and monkey visual areas in terms of (e.g.) the magnitude of the decoding performances they are able to attain, which is beyond the scope of this study. In fact, achieving such comparison would require testing all the areas in both species under the same exact experimental conditions (e.g., same combination of object identities and transformations and same presentation protocol). Instead, the strength of our conclusions rests on the fact that they are based on a qualitative agreement between the trends observed in our study across the V1-LI-LL progression and those reported in the literature for the monkey ventral stream (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>).</p><p>Because of this agreement, our results provide the strongest and most systematic functional evidence, to our knowledge, that V1, LI and LL belong to a cortical object-processing pathway, with V1 sitting at the bottom of the functional hierarchy, LL at the top, and LI acting as an intermediate processing stage. With regard to LM, our data do not support a clear assignment to the ventral stream, because this area never displayed any significant advantage, over V1, in terms of higher-order processing of visual objects. Yet, the significant increase of RF size (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) and response latency (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>) from V1 to LM is consistent with the role that mouse anatomical and functional studies have suggested for this area – one step beyond V1 in the processing hierarchy, routing both ventral and dorsal information, similar to area V2 in primates (<xref ref-type="bibr" rid="bib89">Wang and Burkhalter, 2007</xref>; <xref ref-type="bibr" rid="bib91">Wang et al., 2012</xref>; <xref ref-type="bibr" rid="bib26">Glickfeld et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Juavinett and Callaway, 2015</xref>).</p><p>As mentioned in the Introduction, these findings are in agreement with recent anatomical studies of mouse visual cortex and earlier lesion studies in the rat. At the functional level, our results strongly support the involvement of LI and LL in ventral-like computations, whose neuronal correlates have remained quite elusive to date (<xref ref-type="bibr" rid="bib32">Huberman and Niell, 2011</xref>; <xref ref-type="bibr" rid="bib51">Niell, 2011</xref>; <xref ref-type="bibr" rid="bib26">Glickfeld et al., 2014</xref>). In fact, in the mouse, LM displays properties that are more consistent with dorsal processing, such as preference for high temporal frequencies (TFs) and low spatial frequencies (SFs) and tuning for global motion, while LI, in spite of its preference for high SFs, prefers high TFs too and only shows a marginally larger orientation selectivity than V1 (<xref ref-type="bibr" rid="bib4">Andermann et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Marshel et al., 2011</xref>; <xref ref-type="bibr" rid="bib36">Juavinett and Callaway, 2015</xref>). Evidence of ventral-stream processing is similarly limited in the rat, despite a recent attempt at investigating LM, LI and LL, as well as the visual part of posterior temporal association cortex (named TO by the authors), with both parametric stimuli (gratings) and visual shapes (<xref ref-type="bibr" rid="bib86">Vermaercke et al., 2014</xref>). Orientation and direction tuning were found to increase along the V1-LM-LI-LL-TO progression. However, when linear decoders were used to probe the ability of these areas to support shape discrimination across a position shift, only TO displayed an advantage over the other areas, and just in relative terms – i.e., the drop of discrimination performance from the train to the test position was the smallest in TO, but, in absolute terms, neither TO nor any of the lateral areas afforded a better generalization than V1 across the position change. Such weaker evidence of ventral stream processing, compared to what found in our study (e.g., see <xref ref-type="fig" rid="fig8">Figure 8</xref>), is likely attributable to the much lower number of transformations tested by (<xref ref-type="bibr" rid="bib86">Vermaercke et al., 2014</xref>) (only one position change), and to the fact that sensitivity to stimulus luminance was not taken into account when comparing the areas in terms of position tolerance.</p><sec id="s3-1"><title>Validity and implications of our findings</title><p>A number of control analyses were performed to verify the solidity of our conclusions. The loss of energy-related stimulus information and the increase of transformation tolerance across the V1-LI-LL progression were found: (1) across the whole cortical thickness (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> and <xref ref-type="fig" rid="fig6">Figure 6C</xref>); (2) after matching the RF sizes of the four populations (<xref ref-type="fig" rid="fig7">Figure 7C–D</xref>); (3) across the whole spectrum of spike isolation quality in our recordings (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A–B</xref> and <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2A–B</xref>); and (4) when considering only neuronal subpopulations with the best spike isolation quality (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C</xref> and <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2C</xref>), which also equated them in terms of firing rate (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3B</xref>). This means that no inhomogeneity in the sampling from the cortical laminae, in the amount of visual field coverage, in the quality of spike isolation, or in the magnitude of the firing rate could possibly account for our findings.</p><p>Another issue deserving a discussion is the potential impact of an inaccurate estimate of the neuronal RFs on the calculation of the RF luminance, a metric that played a key role in all our analyses. Two orders of problems emerge when estimating the RF of a neuron. First, the structure and size of the RF depend, in general, on the shape and size of the stimuli used to map it. In our experiments, we used very simple stimuli (high-contrast drifting bars) presented over a dense grid of visual field locations. The rationale was to rely on the luminance-driven component of the neuronal response to simply estimate what portion of the visual field each neuron was sensitive too. This approach was very effective, because all the recorded neurons retained some amount of sensitivity to luminance, even those that were tuned to more complex visual features than luminance alone, as the LL neurons (see the white portion of the bars in <xref ref-type="fig" rid="fig3">Figure 3B</xref>). As a result, very sharp RF maps were obtained in all the areas (see examples in <xref ref-type="fig" rid="fig1">Figure 1C</xref>). Another problem, when estimating RFs, is that they do not always have an elliptical shape, although, in many instances, they are very well approximated by 2-dimensional Gaussians (<xref ref-type="bibr" rid="bib52">Op De Beeck and Vogels, 2000</xref>; <xref ref-type="bibr" rid="bib8">Brincat and Connor, 2004</xref>; <xref ref-type="bibr" rid="bib49">Niell and Stryker, 2008</xref>; <xref ref-type="bibr" rid="bib69">Rust and Dicarlo, 2010</xref>). In our study, we took two measures to prevent poor elliptical fits from possibly affecting our conclusions. In the RF size analysis (<xref ref-type="fig" rid="fig7">Figure 7</xref>), we only included data from neurons with RFs that were well fitted by 2-dimensional Gaussians (see Materials and methods). More importantly, for the computation of the RF luminance, we did not use the fitted RFs, but we directly used the raw RF maps (see Materials and methods and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2C</xref>). This allowed weighting the luminance of the stimulus images using the real shapes of the RFs, thus reliably computing the RF luminance for all the recorded neurons.</p><p>Finally, it is worth considering the implications of having studied object representations in anesthetized rats, passively exposed to visual stimuli. Two motivations are at the base of this choice. First, the need of probing visual neurons with the repeated presentation (tens of trials; see Materials and methods) of hundreds of different stimulus conditions, which are essential to properly investigate invariant object representations. The second motivation was the need of excluding potential effects of top-down signals, task- and state-dependence, learning and memory (<xref ref-type="bibr" rid="bib24">Gavornik and Bear, 2014</xref>; <xref ref-type="bibr" rid="bib13">Cooke and Bear, 2015</xref>; <xref ref-type="bibr" rid="bib9">Burgess et al., 2016</xref>), which are all detrimental when the goal is to understand the initial, largely reflexive, feed-forward sweep of activation through a visual processing hierarchy (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>). For these reasons, many primate studies have investigated ventral stream functions in anesthetized monkeys [e.g., see (<xref ref-type="bibr" rid="bib39">Kobatake and Tanaka, 1994</xref>; <xref ref-type="bibr" rid="bib35">Ito et al., 1995</xref>; <xref ref-type="bibr" rid="bib41">Logothetis et al., 1999</xref>; <xref ref-type="bibr" rid="bib83">Tsunoda et al., 2001</xref>; <xref ref-type="bibr" rid="bib72">Sato et al., 2013</xref>; <xref ref-type="bibr" rid="bib11">Chen et al., 2015</xref>)] or, if awake animals were used, under passive viewing conditions [e.g., see (<xref ref-type="bibr" rid="bib57">Pasupathy and Connor, 2002</xref>; <xref ref-type="bibr" rid="bib8">Brincat and Connor, 2004</xref>; <xref ref-type="bibr" rid="bib33">Hung et al., 2005</xref>; <xref ref-type="bibr" rid="bib38">Kiani et al., 2007</xref>; <xref ref-type="bibr" rid="bib93">Willmore et al., 2010</xref>; <xref ref-type="bibr" rid="bib69">Rust and Dicarlo, 2010</xref>; <xref ref-type="bibr" rid="bib31">Hong et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">El-Shamayleh and Pasupathy, 2016</xref>)].</p><p>In the face of these advantages, anesthesia has several drawbacks. It can depress cortical activity, especially in high-order areas (<xref ref-type="bibr" rid="bib29">Heinke and Schwarzbauer, 2001</xref>), and put the cortex in a highly synchronized state (<xref ref-type="bibr" rid="bib76">Steriade et al., 1993</xref>). In our study, we took inspiration from previous work on the visual cortex of anesthetized rodents (<xref ref-type="bibr" rid="bib95">Zhu and Yao, 2013</xref>; <xref ref-type="bibr" rid="bib22">Froudarakis et al., 2014</xref>; <xref ref-type="bibr" rid="bib58">Pecka et al., 2014</xref>), cats (<xref ref-type="bibr" rid="bib10">Busse et al., 2009</xref>) and monkeys (<xref ref-type="bibr" rid="bib41">Logothetis et al., 1999</xref>; <xref ref-type="bibr" rid="bib72">Sato et al., 2013</xref>), and we limited the impact of these issues by combining a light anesthetic with fentanyl-based sedation (Materials and methods). This yielded robust visually-evoked responses both in V1 and extrastriate areas (see PSTHs in <xref ref-type="fig" rid="fig2">Figure 2A–B</xref>, top). Still, we observed a gradual reduction of firing rate intensity along the areas’ progression (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3A</xref>), but such decrease, as mentioned above, did not account for our findings (see <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3B</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C</xref> and <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2C</xref>). Obviously, this does exclude the impact of anesthesia on more subtle aspects of neuronal processing. For instance, isoflurane and urethane anesthesia have been reported to alter the excitation-inhibition balance that is typical of wakefulness, thus resulting in very time persistent stimulus-evoked responses, broad RFs and reduced strength of surround suppression (<xref ref-type="bibr" rid="bib27">Haider et al., 2013</xref>; <xref ref-type="bibr" rid="bib84">Vaiceliunaite et al., 2013</xref>). However, under fentanyl anesthesia, surround suppression was found to be very robust in mouse V1 (<xref ref-type="bibr" rid="bib58">Pecka et al., 2014</xref>), and our own recordings show that responses and RFs were far from sluggish and very similar to those obtained, from the same cortical areas, in awake rats (<xref ref-type="bibr" rid="bib86">Vermaercke et al., 2014</xref>) – sharp tuning was observed in both the time and space domains, with transient responses, rarely lasting longer than 150 ms (see examples in <xref ref-type="fig" rid="fig2">Figure 2A–B</xref>, top), and well-defined RFs (see examples in <xref ref-type="fig" rid="fig1">Figure 1C</xref>), some as small as 5° of visual angle (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). Finally, neuronal activity in mouse visual cortex during active wakefulness has been shown to be very similar to that in the anesthetized state, with regard to a number of key tuning properties. These include the sharpness of orientation tuning in V1 (<xref ref-type="bibr" rid="bib49">Niell and Stryker, 2008</xref>, <xref ref-type="bibr" rid="bib50">2010</xref>), the sparseness and discriminability of natural scene representations in V1, LM and anterolateral area AL (<xref ref-type="bibr" rid="bib22">Froudarakis et al., 2014</xref>), and the integration of global motion signals in rostrolateral area RL (<xref ref-type="bibr" rid="bib36">Juavinett and Callaway, 2015</xref>).</p><p>Taken together, the evidence reviewed above is highly reassuring with regard to the validity and generality of our findings. Obviously, being our data collected in passively viewing rats, the increase of transformation tolerance observed along the V1-LI-LL progression is not the result of a supervised learning process. Rather, our findings suggest that lateral extrastriate areas act as banks of general-purpose feature detectors, each endowed with an intrinsic degree of transformation tolerance. By virtue of their larger invariance, the detectors at the highest stages are able to automatically support transformation-tolerant recognition, without the need of explicitly learning the associative relations among all the views of an object. These conclusions are in full agreement with a recent behavioral study, showing that rats are capable of spontaneously generalize their recognition to previously unseen views of an object, without the need of any training (<xref ref-type="bibr" rid="bib79">Tafazoli et al., 2012</xref>).</p><p>Another important implication of our study concerns the increase of the invariant object information <italic>per neuron</italic> found across rat lateral visual areas (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, bar plot). Critically, this finding does not imply that the overall invariant information <italic>per area</italic> also increases from V1 to LL. In fact, if the areas’ progression acted as a purely feed-forward processing chain, a <italic>per area</italic> increase would mean that new object information is created from one processing step to the next, a fact that would violate the data processing inequality (<xref ref-type="bibr" rid="bib14">Cover and Thomas, 2006</xref>). But in rat visual cortex, there is a strong reduction of the number of neurons from V1 through the succession of lateral extrastriate areas. LM, LI and LL occupy a cortical surface that is, respectively, 31%, 3.5% and 2.1% of the surface of V1 (<xref ref-type="bibr" rid="bib20">Espinoza and Thomas, 1983</xref>). Therefore, the object information <italic>per neuron</italic> can increase along the areas’ progression, without this implying that the total object information <italic>per area</italic> also increases. In addition, the connectivity among rat lateral visual areas is far from being strictly feed-forward. In both rats (<xref ref-type="bibr" rid="bib71">Sanderson et al., 1991</xref>; <xref ref-type="bibr" rid="bib47">Montero, 1993</xref>; <xref ref-type="bibr" rid="bib12">Coogan and Burkhalter, 1993</xref>) and mice (<xref ref-type="bibr" rid="bib91">Wang et al., 2012</xref>), many corticortical and thalamocortical ‘bypass’ routes reach higher-level visual areas, in addition to the main anatomical route that connects consecutive processing stages step-by-step (e.g., V1 directly projects to LI and LL, and LL receives direct projections from the thalamus). Thus, the concentration of more object information in each individual neuron, while the visual representation is reformatted across consecutive processing stages, should not be interpreted as an indication that total object information increases across areas. Rather, our interpretation is that the increase of invariant object information per neuron is likely an essential step to make information about object identity gradually more explicit, and more easily readable by downstream neurons that only have access to a limited number of presynaptic units (as confirmed by the linear decoding analyses shown in <xref ref-type="fig" rid="fig5">Figures 5</xref>–<xref ref-type="fig" rid="fig8">8</xref>).</p><p>To conclude, we believe that these results pave the way for the exploration of the neuronal mechanisms underlying invariant object representations in an animal model that is amenable to a large variety of experimental approaches (<xref ref-type="bibr" rid="bib98">Zoccolan, 2015</xref>). In addition, the remarkable similarity between the anatomical organization of rat and mouse visual cortex suggests that mice too can serve as powerful models to dissect ventral stream computations, given the battery of genetic and molecular tools that this species affords (<xref ref-type="bibr" rid="bib42">Luo et al., 2008</xref>; <xref ref-type="bibr" rid="bib32">Huberman and Niell, 2011</xref>; <xref ref-type="bibr" rid="bib37">Katzner and Weigelt, 2013</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animal preparation and surgery</title><p>All animal procedures were in agreement with international and institutional standards for the care and use of animals in research and were approved by the Italian Ministry of Health: project N. DGSAF 22791-A (submitted on Sep. 7, 2015) was approved on Dec. 10, 2015 (approval N. 1254/2015-PR); project N. 5388-III/14 (submitted on Aug. 23, 2012) and project N. 3612-III/12 (submitted on Sep. 15, 2009) were approved according to the legislative decree 116/92, article 7. We used 26 naïve Long-Evans male rats (Charles River Laboratories), with age 3–12 months and weight 300–600 grams. The rats were anesthetized with an intraperitoneal (IP) injection of a solution of fentanyl (Fentanest: 0,3 mg/kg; Pfizer) and medetomidin (Domitor: 0,3 mg/kg; Orion Pharma). Body temperature was maintained at 37.5°C by a feedback-controlled heating pad (Panlab, Harvard Apparatus). Heart rate and oxygen level were monitored through a pulse oximeter (Pulsesense-VET, Nonin), and a constant flow of oxygen was delivered to the rat to prevent hypoxia.</p><p>The anesthetized animal was placed in a stereotaxic apparatus (Narishige, SR-5R). Following a scalp incision, a craniotomy was performed over the left hemisphere (~1.5 mm wide in diameter) and the dura was removed to allow the insertion of the electrode array. Stereotaxic coordinates for V1 recordings ranged from −5.16 to −7.56 mm anteroposterior (AP), with reference to bregma; for extrastriate areas (LM, LI and LL), they ranged from −6.42 to −7.68 mm AP. The exposed brain surface was covered with saline to prevent drying. The eyes were protected from direct light and prevented from drying by application of the ophthalmic solution Epigel (Ceva Vetem).</p><p>Once the surgical procedure was completed, the rat was maintained in the anesthetized state by continuous IP infusion of the fentanyl/medetomidin solution (0,1 mg/kg/h). The level of anesthesia was periodically monitored by checking the absence of tail, ear and paw reflex. The right eye of the animal was immobilized using an eye-ring anchored to the stereotaxic apparatus (the left eye was covered with black tape), with the pupil’s orientation set at 0° elevation and 65° azimuth. The stereotax was positioned, so as to align the eye with the center of the stimulus display, and was rotated leftward of 45°, so as to bring the binocular field of the right eye to cover the left side of the display.</p></sec><sec id="s4-2"><title>Neuronal recordings</title><p>Recordings were performed with different configurations of 32-channel silicon probes (NeuroNexus Technologies). To maximize the coverage of the monitor by V1 RFs, neurons in this area were recorded using 8-shank arrays with either 177 µm<sup>2</sup> site area and 100 µm spacing (model A8 × 4-2mm100-200-177) or 413 µm<sup>2</sup> site area and 50 µm spacing (model A8 × 4-2mm50-200-413), and 4-shank arrays with 177 µm<sup>2</sup> site area and 100 µm spacing (model A4 × 8–5 mm-100-200-177). To map the retinotopy along extrastriate areas (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), recordings from LM, LI and LL were performed using single-shank probes with either 177 µm<sup>2</sup> site area and 25 µm spacing (model A1 × 32-5mm25-177) or 413 µm<sup>2</sup> site area and 50 µm spacing (model A1 × 32-5mm50-413). For V1, the probe was inserted perpendicularly to the cortex (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>), while, for the lateral areas, it was tilted with an angle of ~30° (<xref ref-type="fig" rid="fig1">Figure 1A</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A–B</xref>). For the probes with 25 µm spacing, only half of the channels (i.e., either odd or even) were used, to avoid considering as different units the same neuron recorded by adjacent sites. To allow the histological reconstruction of the electrode insertion track, the electrode was coated, before insertion, with Vybrant DiI cell-labeling solution (Life Technologies), and, at the end of the recording session, an electrolytic lesion was performed, by passing a 5 µA current for 2 s through the last 2 (multi-shank probes) or 4 (single-shank probe) channels at the tip of each shank (see below for a detailed description of the histological procedures).</p><p>To decide how many neurons to record in each area, we took inspiration from previous population coding studies that have compared different ventral stream areas in terms of their ability to support object recognition (<xref ref-type="bibr" rid="bib69">Rust and Dicarlo, 2010</xref>; <xref ref-type="bibr" rid="bib54">Pagan et al., 2013</xref>). These studies show that pairwise area comparisons become macroscopic when the size of the neuronal population in each area approaches 100 units. Therefore, in our experiments, we aimed at recording more than 100 units for each of the four visual areas under investigation. The final number of units obtained per area depended on the yield of each individual recording session (reported in <xref ref-type="supplementary-material" rid="SD1-data">Figure 1—source data 1</xref>) and on how accessible to recording any given area was – e.g., recordings from the deepest area (LL) were the most challenging and, since LI and LL were typically recorded simultaneously (see <xref ref-type="supplementary-material" rid="SD1-data">Figure 1—source data 1</xref>), we collected a large number of units from LI in the attempt of adequately sampling LL. The final number of units recorded in each area ranged from 131 (LM) to 260 (LI) (note that these numbers refer to the visually driven and stimulus informative units; see below for an explanation)</p><p>Extracellular signals were acquired using a system three workstation (Tucker-Davis Technologies) with a sampling rate of 25 kHz and were filtered from 0.3 to 5 kHz. Action potentials (spikes) were detected and sorted for each recording site separately, using Wave Clus (<xref ref-type="bibr" rid="bib61">Quiroga et al., 2004</xref>) in MATLAB (The MathWorks). Spike isolation quality was assessed using two apposite metrics (see below). Neuronal responses were quantified by counting spikes in neuron-specific spike-count windows (e.g., see gray patches in <xref ref-type="fig" rid="fig2">Figure 2A–B</xref>), with a fixed duration of 150 ms and an onset that was equal to the latency of the neuronal response, so as to capture most of the stimulus-evoked activity. The latency was estimated as the time, relative to the stimulus onset, when the response reached 20% of its peak value [for a full description see (<xref ref-type="bibr" rid="bib96">Zoccolan et al., 2007</xref>)].</p></sec><sec id="s4-3"><title>Histology</title><p>At the end of the recording session, each animal was deeply anesthetized with an overdose of urethane (1.5 gr/kg) and perfused transcardially with phosphate buffer saline (PBS) 0.1 M, followed by 4% paraformaldehyde (PFA) in PBS 0.1 M, pH 7.4. The brain was extracted from the skull and postfixed overnight in 4% PFA at 4°C. After postfixation, the tissue was cryoprotected by immersion in 15% w/v sucrose in PBS 0.1 M for at least 24 hr at 4°C, and then kept in 30% w/v sucrose in PBS 0.1 M, until it was sectioned coronally at either 20 or 30 µm thickness on a freezing microtome (Leica SM2000R, Nussloch, Germany). Sections were mounted immediately on Superfrost Plus slides and let dry at room temperature overnight. A brief wash in distilled water was performed, to remove the excess of crystal salt sedimented on the slices, before inspecting them at the microscope.</p><p>For each slice, we acquired three different kinds of images, using a digital camera adapted to a Leica microscope (Leica DM6000B-CTR6000, Nussloch, Germany). First, various bright field photographs were taken at 2.5X and 10X magnification, so as to fully tile the region of the slice (left hemisphere) where the visual cortical areas were located. Second, for each of such bright field pictures, a matching fluorescence image was also acquired (using a red filter with emission at 700 nm), to visualize the red fluorescence track left by the insertion of the probe (that had been coated with Vybrant DiI cell-labeling solution before the insertion) (<xref ref-type="bibr" rid="bib17">DiCarlo et al., 1996</xref>; <xref ref-type="bibr" rid="bib6">Blanche et al., 2005</xref>). Following the acquisition of this set of images, the slices were further stained for Nissl substance (using the Cresyl Violet method) and pictures were taken at 2.5X and 10X magnification. In addition, to better visualize the anatomic structures within the slice, a lower magnification picture of the entire left hemisphere was also taken, using a Canon 6D digital camera with a Tamron 90 mm f/2.8 macro lens.</p><p>The fluorescence, bright field, and Nissl-stained images were processed using Inkscape (an open source SVG editor; <ext-link ext-link-type="uri" xlink:href="http://www.inkscape.org">http://www.inkscape.org</ext-link>), so as to reconstruct the anteroposterior coordinate of the probe insertion track and, when possible, the laminar location of the recording sites along the probe. This was achieved in three steps. First, each pair of matching fluorescence and bright field images were superimposed to produce a single picture, showing both the insertion track of the probe and the anatomical structure of the section. Then, these pictures and the Nissl-stained images were aligned by matching anatomical landmarks (e.g., the margins of the section). Finally, for each section, the mosaic of matched fluorescence, bright field and Nissl images were stitched together (by relying, again, on anatomical landmark), and then superimposed to the low-magnification image taken with the Canon digital camera. In the resulting image (see examples in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), the position of the shank(s) of the probe was drawn (thick black lines in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), by tracing the outline of the fluorescent track, and taking into account, when available, the location of the electrolytic lesion performed at the end of the recording session. Based on the known geometry of the silicon probes, the location of each recording site was also drawn over the shank(s) (yellow dots over the black lines in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). In the final image, the boundaries between the cortical layers (red lines in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) were identified, based on the difference in size, morphology and density of the Nissl-labeled cells across the cortical thickness. This allowed estimating the laminar location of the recording sites. Since the number of neurons sampled from each individual cortical layer was not very large, in our analyses, we grouped the units recorded from layers II/III and IV and those recorded from layers V and VI (see <xref ref-type="fig" rid="fig6">Figure 6C</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>In some sessions, multiple recordings blocks were performed at different depths along the same probe insertion. In these cases, it was not always possible to histologically reconstruct the probe location within each block, given that a single fluorescent track was often observed, without clean-cut interruptions that could serve as landmarks for the individual blocks. In all such cases, the laminar position of the recorded units was not assigned. This explains why, in each area, the sum of the layer-labeled cells reported in our analyses is not equal to the total number of recorded units (e.g., compare the numbers of neurons reported in <xref ref-type="fig" rid="fig6">Figure 6B</xref> to those reported in <xref ref-type="fig" rid="fig6">Figure 6C</xref>). Since, in multi-block recording sessions, the probe reconstruction was especially difficult for the initial (more superficial) block, LM (the first area to be recorded during oblique penetrations) was the area in which, for a relative large fraction of neurons, we did not assign a laminar position.</p></sec><sec id="s4-4"><title>Visual stimuli</title><p>Our stimulus set consisted of 380 visual object conditions, obtained by producing 38 distinct views (or transformations) of 10 different objects, using the ray tracer POV-Ray (<ext-link ext-link-type="uri" xlink:href="http://www.povray.org/">http://www.povray.org/</ext-link>). The objects were chosen, so as to span a range of shape features and low-level properties (e.g., luminance). Six of them (#1–6) were computer-graphics reproductions of real-world objects, while the remaining four (#7–10) were artificial shapes, originally designed for a behavioral study assessing invariant visual object recognition in rats (<xref ref-type="bibr" rid="bib79">Tafazoli et al., 2012</xref>). <xref ref-type="fig" rid="fig1">Figure 1B</xref> (top) shows these objects, as they appeared in the <italic>pose</italic> that we defined as <italic>default</italic>, with regard to the rotation parameters (i.e., 0° in-plane and in-depth rotation), and at their <italic>default luminosity</italic> (i.e., 100% luminance). Other default parameters were the <italic>default size</italic> (35° of visual angle) and <italic>two default azimuth positions</italic> (−15° and +15° of visual angle) over the stimulus display. Each of these default parameters was held constant, when some other parameter was changed to generate the object transformations, as detailed below. The elevation of the stimuli was fixed at 0° of visual angle and never varied. Size was computed as the diameter of an object’s bounding circle. The transformations that each object underwent were the following.</p><p><italic>Positions changes:</italic> each object was horizontally shifted across the stimulus display, from −22.5° to +30° of visual angle (azimuth), in steps of 7.5° (<xref ref-type="fig" rid="fig1">Figure 1B.1</xref>). These numbers refer to the center of the display (0°), which was aligned to the position of the right eye of the rat (i.e., a hypothetical straight line passing through the eye and perpendicular to the monitor would hit it exactly in the middle). The pose, luminance and size of the objects were the default ones.</p><p><italic>Size changes:</italic> each object was scaled from 15° to 55° of visual angle, in steps of 10° (<xref ref-type="fig" rid="fig1">Figure 1B.2</xref>). The pose, luminance and positions (−15° and +15°) of the objects were the default ones.</p><p><italic>In-plane rotations:</italic> each object was in-plane rotated from −40° to +40°, in steps of 20° (<xref ref-type="fig" rid="fig1">Figure 1B.3</xref>). The in-depth rotation, luminance, size and positions of the objects were the default ones.</p><p><italic>In-depth rotations:</italic> each object was presented at five different in-depth (azimuth) rotation angles: −60°, −40°, 0°, + 40°, and +60° (<xref ref-type="fig" rid="fig1">Figure 1B.4</xref>). The in-plane rotation, luminance, size and positions of the objects were the default ones.</p><p><italic>Luminance changes:</italic> each object was presented at four different luminance levels: 100% (i.e., default luminance), 50%, 25% and 12.5% (<xref ref-type="fig" rid="fig1">Figure 1B.5</xref>). The pose, size and positions of the objects were the default ones.</p><p>The object conditions were presented in rapid sequence (250 ms stimulus on, followed by 250 ms of blank screen), randomly interleaved with the drifting bars used to map the neuronal RFs (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A–B</xref>). During a recording session, the presentation of each condition was repeated a large number of times. For V1 neurons, each conditions was presented, on average, 26 ± 2 times; for LM neurons, 25 ± 4 times; for LI neurons, 27 ± 3 times; and for LL, neurons 28 ± 2 times. This allowed obtaining a good estimate of the conditional probability <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo><mml:mi>O</mml:mi><mml:mo> </mml:mo><mml:mo>&amp;</mml:mo><mml:mo> </mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of measuring a given response <inline-formula><mml:math id="inf78"><mml:mi>R</mml:mi></mml:math></inline-formula> to any combination of object identity <inline-formula><mml:math id="inf79"><mml:mi>O</mml:mi></mml:math></inline-formula> and transformation <inline-formula><mml:math id="inf80"><mml:mi>T</mml:mi></mml:math></inline-formula>.</p><p>The stimuli were displayed on a 47 inch LCD monitor (Sharp PN-E471R), with 1920 × 1080 pixel resolution, 60 Hz refresh rate, 9 ms response time, 700 cd/m2 maximum brightness, 1.200:1 contrast ratio, positioned at a distance of 30 cm from the right eye, spanning a visual field of 120° azimuth and 90° elevation. To avoid distortions of the objects’ shape, the stimuli were presented under a tangent screen projection (see next section and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2D–F</xref>).</p></sec><sec id="s4-5"><title>Tangent screen projection</title><p>As explained above, the stimulus display was positioned at a distance of 30 cm from the rat eye. This number was chosen, because the optimal viewing distance for rats has been reported to range between 20 and 30 cm (<xref ref-type="bibr" rid="bib92">Wiesenfeld and Branchek, 1976</xref>). In addition, earlier behavioral studies from our group have shown that rats are capable of discriminating complex visual objects under a variety of identity-preserving transformations, when viewing the objects at a distance of 30 cm (<xref ref-type="bibr" rid="bib97">Zoccolan et al., 2009</xref>; <xref ref-type="bibr" rid="bib79">Tafazoli et al., 2012</xref>; <xref ref-type="bibr" rid="bib3">Alemi-Neissi et al., 2013</xref>; <xref ref-type="bibr" rid="bib67">Rosselli et al., 2015</xref>). However, because of such a short viewing distance, objects will appear distorted, when they undergo large translations over the stimulus display. Because of this, position changes will also result in size changes and distortions of the objects’ shape, unless appropriate corrections are applied. To address this issue, we displayed the stimuli under a tangent screen projection. This projection allows presenting the stimuli as they would appear, if they were shown on virtual screens that are tangent to a circle centered on the rat’s eye, with a radius equal to the distance from the eye to the point on the display just in front of it (i.e., 30 cm). Thanks to this projection, the shape, size and aspect ratio of each stimulus were preserved across all the eight azimuth positions tested in our experiment (see previous section).</p><p>The tangent projection is explained in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2D–F</xref>. Panels D and E show, respectively, a top view and a side view of the rat eye and the stimulus display (<italic>O</italic><sub>1</sub>). <italic>R</italic> is the distance between the eye and the center of the display. <italic>O</italic><sub>2</sub> is an example (virtual) tangent screen, where an object would be shown, if its center was translated of an azimuth angle <italic>θ</italic> to the left of the center of the stimulus display (while maintaining the default elevation of 0°). The coordinate <italic>x</italic><sub>0</sub> indicates the projection of the object’s center over <italic>O</italic><sub>1</sub>, following this azimuth shift <italic>θ</italic>. The Cartesian coordinates (<italic>x</italic><sub>2</sub>, <italic>y</italic><sub>2</sub>) indicate the position of a pixel of the stimulus image, relative to the object’s center, over the virtual screen <italic>O</italic><sub>2</sub>, while (<italic>x</italic><sub>1</sub>, <italic>y</italic><sub>1</sub>) indicate the projection of this point over the display <italic>O</italic><sub>1</sub>, i.e., its coordinates, relative to the center of the object (<italic>x</italic><sub>0</sub>, 0) in <italic>O</italic><sub>1</sub>. The red line drawn over <italic>O</italic><sub>2</sub> shows the distance of this pixel from the object’s center over the tangent screen, while the red line drawn over <italic>O</italic><sub>1</sub> is the projection of this distance over the stimulus display.</p><p>The projection of any point (<italic>x</italic><sub>2</sub>, <italic>y</italic><sub>2</sub>) in the virtual tangent screen <italic>O</italic><sub>2</sub> to a point (<italic>x</italic><sub>1</sub>, <italic>y</italic><sub>1</sub>) in the stimulus display <italic>O</italic><sub>1</sub> can be computed using simple trigonometric relationships. <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2D</xref> shows how <italic>x</italic><sub>1</sub> can be expressed as a function of <italic>x</italic><sub>2</sub> and <italic>θ</italic>:<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mi>tan</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϑ</mml:mi><mml:mo>+</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where:<disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mi>tan</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="1em"/><mml:mi>φ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>tan</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that <italic>φ</italic> is the azimuth position of the point (<italic>x</italic><sub>2</sub>, <italic>y</italic><sub>2</sub>), when expressed in spherical coordinates. Similarly, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2E</xref> shows how <italic>y</italic><sub>1</sub> can be expressed as a function of <italic>y</italic><sub>2</sub> and <italic>θ</italic>:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where:<disp-formula id="equ7"><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>φ</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="1em"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>+</mml:mo><mml:mi>ϑ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mtext> </mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To better illustrate the effect of the tangent screen projection, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2D</xref> shows how two points at the same distance from (but on opposite sides of) the object’s center in the tangent screen (see, respectively, the red and green lines over <italic>O</italic><sub>2</sub>) would be projected over the stimulus display (see, respectively, the red and green lines over <italic>O</italic><sub>1</sub>). As shown by the drawing, these two points would not be equidistant any longer from the object’s center, in the projection over <italic>O</italic><sub>1</sub>. This can be better appreciated by looking, in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2F</xref> (top), at the images of an example object (object #8) shown at positions −22.5°, −15°, −7.5° and 0° of visual angle (azimuth) over the stimulus display (these are a subset of the eight different positions tested for each object in our experiment; see above). The distortion applied to the object by the tangent screen projection becomes progressively larger and more asymmetrical (with respect to the vertical axis of the object), the larger is the distance of the object’s center from the center of the display (0° azimuth). Critically, this distortion was designed to exactly compensate the one produced by the perspective projection to the retina, so that, regardless of the magnitude of the azimuth displacement, the resulting projection of an object over the retina will have the same shape, size and aspect ratio as the ones produced by the object shown at center of the display, right in front of the rat’s eye (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2F</xref>, bottom).</p></sec><sec id="s4-6"><title>Selection of the neuronal populations included in the analyses</title><p>Throughout our study, data analysis was restricted to those neurons that met two criteria: (1) being visually driven; and (2) being stimulus informative. The first criterion was implemented as following. Given a neuron, its response to each of the 380 object conditions and its background firing rate were computed. A 2-tailed t-test was then applied to check if at least one of these conditions evoked a response that was significantly different (either larger or lower) than background (p&lt;0.05 with Bonferroni correction for multiple comparison). The second criterion was based on the computation of the mutual information between stimulus and response <italic>I</italic>(<italic>R;S</italic>) (defined below). Note that, in this case, only 230 object conditions <italic>S</italic> were used (see next section). To be included in the analysis, a neuron had to carry a significant amount of information <italic>I</italic>(<italic>R;S</italic>) (p&lt;0.05; permutation test; described below). The number of neurons that met these two criteria from each area per rat is reported in <xref ref-type="supplementary-material" rid="SD1-data">Figure 1—source data 1</xref>.</p></sec><sec id="s4-7"><title>Selection of the object conditions to be included in the mutual information and single-cell decoding analyses</title><p>As explained above, 38 different transformations of each object were presented during the experiment. These included eight positions across the horizontal extent (azimuth) of the stimulus display, plus four sizes, four in-plane rotations, 4-in-depth rotations and three luminance levels, presented at two of the eight positions (i.e., −15° and +15° azimuth). For any given neuron, all the eight positions were included in the single-cell mutual information and decoding analyses (<xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig7">7</xref>), while, of the remaining transformations, only those shown at the position that was closer to the neuron’s RF center (i.e., either −15° or +15°) were used. This yielded 23 different transformations per object, for a total of 230 stimuli <italic>S</italic>. The motivation of choosing this subset of transformations was to maximize the coverage of the object conditions by the RFs of the recorded neurons. In the case of the population decoding analyses (<xref ref-type="fig" rid="fig8">Figure 8</xref>), 19 of these 23 transformations per object were used (see below).</p></sec><sec id="s4-8"><title>Computation of the receptive field size</title><p>Given a neuron, we obtained an estimate of its receptive field (RF) profile, using a procedure adapted from <xref ref-type="bibr" rid="bib49">Niell and Stryker (2008)</xref>. We measured the neuronal response to 10° long drifting bars with four different orientations (0°, 45°, 90°, and 135°), presented over a grid of 6 × 11 cells on the stimulus display (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). The responses to the four orientations in each cell were averaged to obtain a two-dimensional map, showing the firing intensity at each location over the display (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>, left). This raw RF map was fitted with a two-dimensional Gaussian, with independent widths (SDs) <inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf82"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>, right). The RF size (diameter) was computed as the average of <inline-formula><mml:math id="inf83"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf84"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (see the black ellipse in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>, left). The goodness of the fit was measured by the <italic>coefficient of determination R</italic><sup>2</sup>, defined as:<disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the sum of the squared residuals of the fit and <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the sum of the squared differences from the mean of the raw RF values. A fit was considered acceptable if <italic>R</italic><sup>2</sup> was larger than 0.5.</p></sec><sec id="s4-9"><title>Computation of the receptive field (RF) luminance and RF contrast</title><p>To estimate how much luminous energy any given stimulus (i.e., object condition) impinged on a neuronal RF, we defined a metric (which we called <italic>RF luminance</italic>), resulting from computing the dot product between the raw RF map of the neuron (normalized to its maximal value, so as to range between 0 and 1) and the luminance profile of the stimulus over the image plane. This is equivalent to weight the stimulus image by the RF profile of the neuron (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2C</xref> provides a graphical description of this procedure). This quantity was then normalized by the maximal luminance that could possibly fall into the neuron’s RF (corresponding to the case of a full-field stimulus at maximal brightness), hence the percentage values reported in <xref ref-type="fig" rid="fig2">Figure 2A–B</xref> (white font) and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p><p>A similar approach was used to measure the variability of the pattern of luminance impinged by a stimulus on a neuronal RF, using a metric that we called <italic>RF contrast</italic>. This metric was defined as the standard deviation of the RF-weighted luminance values produced by the stimulus that were contained within the RF itself (see the rightmost plot in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2C</xref>). Note that, in general, since the neuronal RFs did not fully cover all the object conditions (see examples in <xref ref-type="fig" rid="fig2">Figure 2A–B</xref>), also the background contributed to the RF contrast metric. Therefore, in the analysis in which we assessed the amount of information carried by single neurons about RF contrast (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>), we only considered object conditions that covered at least 10% of the RFs. This restriction was applied because we wanted to measure how neurons coded the contrast over a large enough surface of the visual objects. In addition, since the number of stimuli that met this criterion was different for the various neurons, we included in the analysis only neurons for which at least 23 object conditions fulfilled this constraint (this explains why the number of neurons reported in <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4A</xref> is lower than the total number of units recorded in each area; e.g., compare to <xref ref-type="fig" rid="fig3">Figure 3B</xref>). This insured that the information about RF contrast was estimated with enough visual stimuli.</p></sec><sec id="s4-10"><title>Quality metrics of spike isolation</title><p>The quality of spike isolation was assessed through two widely used benchmarks: signal-to-noise ratio (<italic>SNR</italic>) and refractory violations (<italic>RV</italic>) (<xref ref-type="bibr" rid="bib62">Quiroga et al., 2005</xref>, <xref ref-type="bibr" rid="bib63">2008</xref>; <xref ref-type="bibr" rid="bib25">Gelbard-Sagiv et al., 2008</xref>; <xref ref-type="bibr" rid="bib30">Hill et al., 2011</xref>). These metrics were defined as following.<disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the average peak-to-peak amplitude of all the spikes detected as a single cluster by the sorting algorithm, and <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is an estimate of the variability of the background noise computed from the rest of the filtered signal (in our application, this was the median of the absolute value of the filtered signal, divided by 0.6745 [<xref ref-type="bibr" rid="bib61">Quiroga et al., 2004</xref>]). <italic>RV</italic> was defined as the fraction of inter spike intervals (ISI) that were shorter than 2 <italic>ms</italic>, the rationale being that a large <italic>RV</italic> indicates a substantial violation of the neuron’s refractory period and, therefore, a contamination from other units (<xref ref-type="bibr" rid="bib30">Hill et al., 2011</xref>).</p><p>These metrics were used in two ways. First, the neurons recorded from the four areas were pooled together and the resulting distributions of <italic>SNR</italic> and <italic>RV</italic> values were divided in three equi-populated bins (tertiles). Within each tertile, the mutual information and generalization metrics were re-computed, so as to compare neuronal subpopulations within the same range of isolation quality (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A–B</xref> and <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2A–B</xref>). Second, we selected from the four areas only neurons with good isolation quality, as defined by imposing both <italic>SNR</italic> &gt;10 and <italic>RV</italic> &lt;2%, and, again, we restricted the computation of the mutual information and generalization metrics to these subpopulations (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C</xref> and <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2C</xref>). These constraints also equated the neuronal subpopulations in terms of firing rate (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3B</xref>), which, otherwise, would significantly decrease from V1 to the more lateral areas (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3A</xref>).</p></sec><sec id="s4-11"><title>Statistical tests</title><p>All the single-cell properties measured for the visual areas have been reported in terms of medians ± Standard Error (SE) of the median (estimated by bootstrap). In all these cases, the statistical significance of each pairwise, between-area comparison was assessed with a 1-tailed, Mann-Whitney U-test, with Holm-Bonferroni correction for multiple comparisons. The choice of the 1-tailed test was motivated by the fact that, in any comparison, we had a clear hypothesis about the rank of each visual area, with respect to the measured property, based on previous anatomical, lesion and functional studies (see Introduction and Discussion).</p></sec><sec id="s4-12"><title>Mutual information analysis</title><p>The mutual information <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between stimulus <italic>S</italic> and response <italic>R</italic> was computed according to <xref ref-type="disp-formula" rid="equ1">eq. 1</xref> (see Results), and its meaning is graphically illustrated in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. Here, we report some more technical details about how the calculation of the information metrics used through the study (i.e., see <xref ref-type="disp-formula" rid="equ1 equ2 equ3">Equation 1, 2 and 3</xref>) was carried out.</p><p>In all the information theoretic analyses, the response <italic>R</italic> was quantified as the number of spikes fired by a neuron in a 150 ms-wide spike count window (see above), discretized into three equi-populated bins (whose boundaries were computed, for each neuron independently, on the whole set of responses collected across repeated presentations of all the stimuli). Given that we recorded an average of 26.5 trials per stimulus (see above), this discretization yielded about nine trials per stimulus per response bin. Earlier studies (<xref ref-type="bibr" rid="bib55">Panzeri and Treves, 1996</xref>; <xref ref-type="bibr" rid="bib56">Panzeri et al., 2007</xref>) showed that the limited sampling bias in information calculations can be effectively corrected out – leading to precise information estimations – if there are at least four repetitions per stimulus and per response bin. Quantizing responses in three bins was thus appropriate to obtain highly conservative and unbiased information estimates. Yet, we explored a range of different bin numbers within the extent for which the bias could reliably be corrected, and we found similar patterns of mutual information values across the four visual areas.</p><p>As shown in the Results (see <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and <xref ref-type="fig" rid="fig3">Figure 3B</xref>), <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be rewritten, using conditional mutual information (<xref ref-type="bibr" rid="bib34">Ince et al., 2012</xref>), as the sum of the luminance information <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and the higher-level, luminance-independent information <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Since, for each object and neuron, the maximum number of possible luminance values <italic>L</italic> is equal to the number of unique transformations the objet underwent (i.e., 23), <italic>L</italic> was discretized in 23 bins (see <xref ref-type="fig" rid="fig2">Figure 2C–D</xref>). This number of luminance bins fully covered the luminance variation for each object, without loss of luminance information, and it yielded, on average, 100 trials per response and stimulus bin, which was comfortably sufficient to control for the sampling bias in the computation of the mutual information (<xref ref-type="bibr" rid="bib56">Panzeri et al., 2007</xref>). The same arguments apply to the analysis in which <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was decomposed as the sum of the contrast information <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and the contrast-independent information <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>).</p><p>To evaluate the statistical significance of mutual information values we used a random permutation method (<xref ref-type="bibr" rid="bib34">Ince et al., 2012</xref>). In this method, separately for each considered information calculation and for each neuron, the association between stimulus and response was randomly permuted 100 times, to generate a null-hypothesis distribution of information values in the absence of any association between stimulus and response. The significance levels (<italic>p</italic> values) of the mutual information values were obtained from this null-hypothesis distribution, as explained in (<xref ref-type="bibr" rid="bib34">Ince et al., 2012</xref>).</p><p>All information measures were computed using the Information Breakdown Toolbox (<xref ref-type="bibr" rid="bib43">Magri et al., 2009</xref>) and were corrected for limited sampling bias using the Panzeri-Treves method (<xref ref-type="bibr" rid="bib55">Panzeri and Treves, 1996</xref>; <xref ref-type="bibr" rid="bib56">Panzeri et al., 2007</xref>). This method uses an asymptotic large-N expansion (where N is the total number of trials available across all stimuli) to estimate and then subtract out the limited sampling bias from the raw information estimate. The asymptotic estimation of the bias has the following expression: <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:mi>A</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <italic>R<sub>s</sub></italic> and <italic>R</italic> are the number of bins with non-zero stimulus-specific and stimulus-unspecific response probabilities, respectively. These numbers of bins are estimated from the data with a Bayesian procedure, as described in <xref ref-type="bibr" rid="bib55">Panzeri and Treves (1996)</xref>.</p></sec><sec id="s4-13"><title>Single cell decoding analysis</title><p>This analysis was meant to assess the ability of single neuronal responses to support the discrimination of a pair of objects in spite of changes in their appearance. Given a neuron, we defined the following variables. <inline-formula><mml:math id="inf97"><mml:mi>R</mml:mi></mml:math></inline-formula> is the neuron’s response, measured by counting the number of spikes fired in a given spike count window, following stimulus presentation (see above for the choice of the spike count window). <inline-formula><mml:math id="inf98"><mml:mi>O</mml:mi></mml:math></inline-formula> denotes the identity of a visual object, among the set of 10 available objects, i.e., <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="inf100"><mml:mi>T</mml:mi></mml:math></inline-formula> denotes the specific transformation an objet underwent, among the set of 23 transformations (i.e., views) available for that neuron (see above for an explanation of how these conditions were chosen for each neuron), i.e., <inline-formula><mml:math id="inf101"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For each neuron, we selected all possible pairs of objects <inline-formula><mml:math id="inf102"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf103"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and, for each pair, we performed two kinds of analysis: (1) a test of linear separability of object representations across transformations (<xref ref-type="fig" rid="fig5">Figure 5</xref>); and (2) various tests of generalization of the discrimination of the two objects across transformations (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The results obtained for each object pair were then averaged, to estimate the linear separability and generalizability afforded by each neuron. Both procedures are detailed below.</p></sec><sec id="s4-14"><title>Test of linear separability</title><p>This analysis was carried out following a 5-fold cross-validation procedure. Given a neuron and a pair of objects <inline-formula><mml:math id="inf104"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we randomly partitioned the set of responses of the neuron to the repeated presentation of each object (across all 23 tested views) in five subsets. In each cross-validation loop, only four of these subsets were used as the training data to build the decoder, while the remaining subset was used as the test data to measure the decoder’s performance at discriminating the two objects. More formally, we considered the conditional probabilities of observing a response <inline-formula><mml:math id="inf106"><mml:mi>R</mml:mi></mml:math></inline-formula> to the presentation of any view of each object, i.e., <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mtext> </mml:mtext><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mtext> </mml:mtext><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the ‘train’ subscript indicates that only 4/5 of the responses (those belonging to the training data set) were used to compute the conditional probabilities. We then used these probabilities to find a boundary, along the spike count axis, that allowed the discrimination of all the views of an object from all the views of the other object. To avoid overfitting the decoder to the training data, we minimized the assumptions on the shape of the conditional probabilities, and we simply parameterized them by computing their means <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and variances <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (with <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). We then used these parameters to find the discrimination boundary, by applying quadratic discriminant analysis (<xref ref-type="bibr" rid="bib28">Hastie et al., 2009</xref>). That is, the discrimination boundary was defined as the threshold response <inline-formula><mml:math id="inf115"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, obtained by setting to zero the log-ratio of the posterior probabilities, i.e.:<disp-formula id="equ10"><label>(4)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mtext> </mml:mtext><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mtext> </mml:mtext><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Having fixed this decision boundary, the decoder was then tested for its ability to correctly classify the remaining 1/5 of the responses belonging to the test set, according to the following binary classification rule:<disp-formula id="equ11"><label>(5)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>R</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>R</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This procedure was repeated for all possible combinations of the five subsets of responses in four train sets and one test set, with each combination yielding a distinct cross-validation loop. The outcomes of the classification, obtained across the resulting five cross-validation loops, were collected in a confusion matrix, which was then used to compute the performance of the decoder with that specific object pair. As explained in the Results, the performance was computed as the mutual information between the actual and the predicted object labels from the decoding outcomes [i.e., as the mutual information between the rows and columns of the confusion matrix (<xref ref-type="bibr" rid="bib60">Quiroga and Panzeri, 2009)</xref>]. This computation was performed for all possible object pairs. The resulting performances were then averaged to obtain the linear separability afforded by the neuron. Note that, to prevent large decoding performances from being trivially achieved only based on luminance differences, only those pairs with objects having a luminosity ratio larger than a given threshold <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (see Results) were included in the final average.</p></sec><sec id="s4-15"><title>Test of generalization</title><p>Given a neuron and a pair of objects <inline-formula><mml:math id="inf117"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf118"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we first trained a binary classifier to partition the spike count axis with a boundary that allowed the discrimination of two specific transformations <inline-formula><mml:math id="inf119"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf120"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the two objects. To this aim, we considered the conditional probabilities of observing a response <inline-formula><mml:math id="inf121"><mml:mi>R</mml:mi></mml:math></inline-formula> to the presentation of the two object views, i.e., <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>|</mml:mo><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo> </mml:mo><mml:mo>&amp;</mml:mo><mml:mo> </mml:mo><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf123"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>|</mml:mo><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo> </mml:mo><mml:mo>&amp;</mml:mo><mml:mo> </mml:mo><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. Note that these conditional probabilities were obtained by taking into account all the responses of the neuron to the repeated presentation of that specific combination of object identity <inline-formula><mml:math id="inf124"><mml:mi>O</mml:mi></mml:math></inline-formula> and transformation <inline-formula><mml:math id="inf125"><mml:mi>T</mml:mi></mml:math></inline-formula>. Similarly to what done in the linear separability analysis, the conditional probabilities were parameterized by computing their means <inline-formula><mml:math id="inf126"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and variances <inline-formula><mml:math id="inf128"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf129"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (with <inline-formula><mml:math id="inf130"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf131"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), and these parameters were used to find the discrimination boundary, by applying quadratic discriminant analysis (<xref ref-type="bibr" rid="bib28">Hastie et al., 2009</xref>). That is, the discrimination boundary was defined as the threshold response <inline-formula><mml:math id="inf132"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, obtained by setting to zero the log-ratio of the posterior probabilities, i.e.:<disp-formula id="equ12"><label>(6)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mtext> </mml:mtext><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mtext> </mml:mtext><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Having fixed this decision boundary, the decoder was then tested for its ability to correctly classify the responses of the neuron to different transformations (i.e., <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>T</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>T</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) of the same two objects <inline-formula><mml:math id="inf135"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf136"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, according to the following binary classification rule:<disp-formula id="equ13"><label>(7)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>R</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>R</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Following this general scheme, the cross-validation procedure worked as following. Given a neuron, and given a pair of objects, we randomly sampled, independently for each object, one of the 23 transformations available for that neuron. The spike count distributions produced by these transformations were used to build the decision boundary <inline-formula><mml:math id="inf137"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, as defined in <xref ref-type="disp-formula" rid="equ12">Equation (6)</xref>. We then used the responses to the repeated presentation of the remaining 22 transformations of each object as the test set to measure the generalization performance of the decoder, according to the classification rule defined in <xref ref-type="disp-formula" rid="equ13">Equation (7)</xref>. For each object pair, this procedure was repeated 1000 times. In each of these runs, the training transformations of the two objects were randomly sampled, so as to span a wide range of training and testing object views. This yielded an average generalization performance per object pair. This computation was performed for all possible pairs. The resulting performances were then averaged to obtain the generalization performance afforded by the neuron. As for the linear separability analysis, the pairs included in the final average were only those with objects having a luminosity ratio larger than a given threshold <inline-formula><mml:math id="inf138"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Two different versions of this analysis were carried out. In the first one, the training and testing views were randomly sampled from the full set of 23 transformations available for each neuron (<xref ref-type="fig" rid="fig6">Figure 6B–C</xref>). In the second one, the training and testing views were randomly sampled from the individual variation axes: position, size, in-plane and in-depth rotations and luminance (<xref ref-type="fig" rid="fig6">Figure 6D</xref>).</p></sec><sec id="s4-16"><title>Test of generalization along parametric position and size changes</title><p>In the case of the transformations along the position axis, a second kind of analysis was also performed. This analysis was meant to assess how sharply the generalization performance of the binary decoders decayed as a function of increasingly large position changes, thus providing an estimate of the spatial extent of the invariance afforded by a neuron over the visual field (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>). Given a neuron, and given a pair of objects, we selected one of the eight azimuth positions tested during the experiment (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, bottom). Note that here, differently from the cross-validation procedure described above, the same transformation (i.e., the same azimuth position) was selected for both objects in the pair. We used the spike count distributions produced by the two objects at this specific position to build the decision boundary <inline-formula><mml:math id="inf139"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, as defined in <xref ref-type="disp-formula" rid="equ12">Equation 6</xref>. We then tested the ability of this decision boundary to correctly discriminate the two objects presented at testing positions that were increasingly further apart from the training location. That is, for each testing position, we took the responses of the neuron to the repeated presentation of the two objects at that position, and we measured the generalization performance of the decoder, according to the classification rule defined in <xref ref-type="disp-formula" rid="equ13">Equation 7</xref>. The testing positions were chosen either to the left or to the right of the selected training position, depending what direction allowed spanning at least 30° of the visual field. Specifically, for training positions between −22.5° and 0° of visual angle, four testing positions were taken to the right of the training location; vice versa, for training positions between +7.5° and +30° of visual angle, four testing positions were taken to the left of the training location. For any training position, this procedure yielded a curve, showing how stable the discriminability of the objects in the pair was across a span of 30° over the visual field. For any given object pair, this procedure was repeated across all possible training positions, starting from the leftmost one (at −22.5°), up to the rightmost one (+30°). The resulting generalization performances were averaged to yield the mean generalization curve over the position axis for that specific object pair. This computation was then repeated for all possible pairs, and the resulting performances were averaged to obtain the generalization curve over the position axis afforded by the neuron. The pairs included in this final average were those with objects having a luminosity ratio larger than 0.9 (i.e., with <inline-formula><mml:math id="inf140"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>). In addition, to obtain a cleaner estimate of the dependence of position tolerance from the magnitude of the translation, the analysis was restricted to the neurons with unimodal and elliptical RF profiles, which were well fitted by 2-dimensional Gaussians (i.e., those neurons contributing to the RF statistics shown in <xref ref-type="fig" rid="fig7">Figure 7A</xref>). The resulting median generalization curves obtained for the four visual areas are shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>.</p><p>A similar kind of analysis was performed for the transformations along the size axis (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref>). The cross-validation procedure was similar to the one described in the previous paragraph for the position changes. However, since the size conditions were fewer than the position ones (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, bottom), and lent themselves to be analyzed in terms of generalization from small to larger sizes (and vice versa), we grouped them into three classes: (1) the default size (i.e., 35°); (2) the two sizes than were smaller than 35° (i.e., 15° and 25°); and 3) the two sizes than were larger than 35° (i.e., 45° and 55°). Given an object pair, we tested all possible generalizations from the default size to the smaller and larger sizes and, vice versa. In the first case, we used the spike count distributions produced by presenting the two objects at size 35° to build the decision boundary <inline-formula><mml:math id="inf141"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, as defined in <xref ref-type="disp-formula" rid="equ12">Equation 6</xref>. We then tested the ability of this decision boundary to correctly discriminate the two objects presented at each of the other sizes. That is, for each testing size, we took the responses of the neuron to the repeated presentation of the two objects at that size, and we measured the generalization performance of the decoder, according to the classification rule defined in <xref ref-type="disp-formula" rid="equ13">Equation 7</xref>. This procedure yielded four generalization performances: two from size 35° to larger sizes, and two from size 35° to smaller sizes. A similar approach was used to compute the generalization from the other sizes to the default one. Also in this case, four generalization performances were obtained: two from smaller sizes to size 35°, and two from larger sizes to size 35°. Overall, the full procedure yielded eight generalization performances per object pair. These performances were divided into two groups. The first group included the generalization from size 35° to smaller sizes and from larger sizes to size 35°. The second group included the generalization from size 35° to larger sizes and from smaller sizes to size 35°. The four performances within each group were averaged to yield the final large→small and small→large generalization performances for the object pair. This procedure was repeated for each pair and the resulting performances were averaged to estimate the generalization afforded by the neuron, when object size changed from large to small and vice versa. The pairs included in these final averages were those with objects having a luminosity ratio larger than 0.9 (i.e., with <inline-formula><mml:math id="inf142"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>). The resulting median generalization performances obtained for the four visual areas are shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref>.</p></sec><sec id="s4-17"><title>Computation of the chance level and assessment of the statistical significance of the classification performances</title><p>In the cross-validation procedures described above (for both the tests of linear separability and generalization), each step yielding a classification performance also yielded a chance generalization performance. This was computed by randomly permuting the object identity labels associated to the responses of the neuron before building the decoder. While carrying out this permutation, care was taken to randomly shuffle only the label <inline-formula><mml:math id="inf143"><mml:mi>O</mml:mi></mml:math></inline-formula>, which specifies object identity, and not the label <inline-formula><mml:math id="inf144"><mml:mi>T</mml:mi></mml:math></inline-formula>, which specifies the transformation the objet underwent. This means that, given a pair of objects <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the identity labels <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf148"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> were randomly shuffled among the responses produced by matching views of the two objects (i.e., the shuffling was performed, separately, for each of the 23 transformations the two objects underwent). This was done to isolate the contribution of the only variable of interest (i.e., object identity) to the performances attained by the classifiers.</p><p>For any given neuron, chance performances were obtained for all object pairs, and were then averaged, according to the same procedures described above, to yield the final chance decoding performance of the neuron. These chance performances were used in two ways. First, to obtain conservative estimates of the ability of the decoders to generalize to new object views, the chance performances were subtracted from the actual (i.e., un-shuffled) classification performances. This allowed measuring and reporting, in all the single-cell decoding analyses (<xref ref-type="fig" rid="fig5">Figures 5</xref>–<xref ref-type="fig" rid="fig7">7</xref>), the net amount of transformation-tolerance afforded by single neurons in the four visual areas (in general, these chance performances obtained by shuffling were all very close to the theoretical 0 bit chance level of the binary classification task). Second, the distribution of classification performances obtained for a neuronal population was compared to the matching distribution of chance performances, to assess whether the former was significantly larger than the latter (according to a 1-tailed, Mann-Whitney U-test). This was the case for all the classification performances reported in <xref ref-type="fig" rid="fig5">Figures 5</xref>–<xref ref-type="fig" rid="fig7">7</xref>.</p></sec><sec id="s4-18"><title>Population decoding analysis</title><p>The population decoding analysis followed the same rationale of the single-cell decoding analysis. Binary linear decoders were built to assess the ability of a population of neurons in a visual area to support the discrimination of a pair of objects in spite of changes in their appearance. Again, two kinds of analyses were preformed – a test of linear separability of the object representations, and a test of generalization of the discrimination of the two objects across transformations.</p></sec><sec id="s4-19"><title>Construction of the population vector representational space</title><p>Following the design of previous population decoding studies of the monkey ventral stream (<xref ref-type="bibr" rid="bib33">Hung et al., 2005</xref>; <xref ref-type="bibr" rid="bib69">Rust and Dicarlo, 2010</xref>; <xref ref-type="bibr" rid="bib54">Pagan et al., 2013</xref>), we measured whether (and how sharply) the decoders’ performance grew as a function of the size of the neuronal populations, by randomly sampling subpopulations of 6, 12, 24, 48 and 96 units from the full sets of neurons recorded in each area. More specifically, for a given subpopulation size <italic>N</italic>, with <inline-formula><mml:math id="inf149"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>12</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>24</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>48</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>96</mml:mn><mml:mtext> </mml:mtext></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we run 50 subpopulation resampling iterations. In each iteration, <italic>N</italic> different units were randomly chosen from the full population, and, for each stimulus condition <italic>S</italic> (i.e., each combination of object identity and transformation <italic>S</italic> = <italic>O&amp;T</italic>), <italic>M</italic> pseudo-population response vectors were built, drawing from the responses of the <italic>N</italic> units to the repeated presentation of <italic>S</italic> (in the cartoons of <xref ref-type="fig" rid="fig8">Figure 8A</xref>, a set of pseudo-population vectors corresponding to a given stimulus <italic>S</italic> is graphically illustrated as a cloud of dots with a specific color). Since the neurons belonging to each area were recorded across different sessions (and, therefore, not all the neurons in a given area were recorded simultaneously; see <xref ref-type="supplementary-material" rid="SD1-data">Figure 1—source data 1</xref>), the pseudo-population response vectors to stimulus <italic>S</italic> were built by assigning to each component of the vector the response of the corresponding neuron in a randomly sampled presentation of <italic>S</italic>. For each component, <italic>M</italic> of such responses were sampled without replacement, to obtain the final set of <italic>M</italic> pseudo-population vectors to be used in a given subpopulation resampling iteration. In each resampling iteration, linear separability and generalization were computed as detailed below.</p></sec><sec id="s4-20"><title>Test of linear separability</title><p>As in the case of the single-cell analysis, we applied a 5-fold cross-validation procedure. Given a neuronal subpopulation of size <italic>N</italic> (obtained in a specific resampling iteration) and a pair of objects <inline-formula><mml:math id="inf150"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf151"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we randomly partitioned the set of pseudo-population response vectors associated to the presentation of each object (across all 23 tested views) in five subsets. In each cross-validation loop, only four of these subsets were used as the train data to build the decoder, while the remaining subset was used as the test data to measure the decoder’s performance at discriminating the two objects. Based on the train data, a linear decoder learned to find a hyperplane that partitioned the population vector space into two semi-spaces, each corresponding to a specific object label (i.e., either <inline-formula><mml:math id="inf152"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf153"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>,).</p><p>As a decoder, we used a Support Vector Machine (SVM) (<xref ref-type="bibr" rid="bib15">Cristianini and Shawe-Taylor, 2000</xref>) in its Matlab implementation, with a linear kernel. The specific method used to find the hyperplane was Sequential Minimal Optimization (SMO) (<xref ref-type="bibr" rid="bib74">Schölkopf and Smola, 2001</xref>). The soft-margin parameter <italic>C</italic> was set to its default value of 1 for all the trainings. Having found the hyperplane using the train data, the decoder was tested for its ability to correctly classify the remaining 1/5 of the response vectors (belonging to the test set), depending on the semi-space in which each vector was located.</p><p>This procedure was repeated for all possible combinations of the five subsets of response vectors in four train sets and one test set, with each combination yielding a distinct cross-validation loop. The outcomes of the classification, obtained across the resulting five cross-validation loops, were collected in a confusion matrix, which was then used to compute the performance of the decoder with that specific object pair in that specific subpopulation resampling iteration. The performance was computed both as the mutual information between the actual and the predicted object labels from the decoding outcomes and as the classification accuracy. As mentioned above, 50 resampling iterations were run for each object pair. The resulting 50 decoding performances were then averaged to obtain the linear separability of the object pair. Finally, the performances obtained for different pairs were averaged to estimate the linear separability afforded by neuronal subpopulation.</p><p>As in the case of the single-cell analysis, only those pairs with objects having a luminosity ratio larger than a given threshold <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (see Results) were included in the final average. Note that, in the case of the population analysis, this constraints had to be satisfied by all the neurons included in a given subpopulation, for all the areas. Obviously, the larger the subpopulation was (i.e., the larger <italic>N</italic>), the harder was to find pairs of objects that satisfied the constraint. For this reason, it was impossible to set <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>, as done in the single-cell analysis, because this choice would have yielded only a single object pair for the largest subpopulation (i.e., for <italic>N</italic> = 96). Therefore, for the analysis shown in the main text (<xref ref-type="fig" rid="fig8">Figure 8C–F</xref>), we set <inline-formula><mml:math id="inf156"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>, which yielded three object pairs for <italic>N</italic> = 96, and still allowed assessing linear separability in a regime where the luminance confound was kept under control. In addition, to check the generality of our conclusions, we also recomputed our analysis for <inline-formula><mml:math id="inf157"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula>, which yielded 23 object pairs (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1A</xref>).</p></sec><sec id="s4-21"><title>Test of generalization</title><p>The overall scheme of the decoding procedure was similar to the one described above (again, binary SVMs with linear kernels were used), with the key difference that, in any given subpopulation resampling iteration, only the response vectors of two randomly chosen transformations <inline-formula><mml:math id="inf158"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf159"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the two objects <inline-formula><mml:math id="inf160"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf161"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> were used to train the decoder. Following this training (i.e., once found the hyperplane), the decoder was tested for its ability to correctly classify (i.e., generalize to) the response vectors of all the remaining transformations (i.e., <inline-formula><mml:math id="inf162"><mml:mrow><mml:mi>T</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi>T</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) of the same two objects, and the outcomes of this classification were collected in a confusion matrix. This procedure was repeated in such a way to choose exhaustively all possible combinations of train views <inline-formula><mml:math id="inf164"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf165"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Since, in the population decoding analysis, 19 views per object were used (i.e., for each object, <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>19</mml:mn></mml:mrow></mml:msub></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; see below for an explanation), this yielded 19<sup>2</sup> confusion matrixes. The data from these matrixes were merged into a global confusion matrix, which was then used to compute the performance of the decoder with that specific object pair in that specific subpopulation resampling iteration. Again, the performance was computed both as the mutual information between the actual and the predicted object labels from the decoding outcomes and as the classification accuracy. The final generalization performance obtained for a given object pair was computed as the mean of the performances resulting from the 50 resampling iterations. Finally, the performances obtained for different pairs were averaged to estimate the generalization afforded by neuronal subpopulation.</p><p>As in the case of the linear separability analysis (see previous section), only those pairs with objects having a luminosity ratio larger than a given threshold <inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (see Results) were included in the final average. <inline-formula><mml:math id="inf168"><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> was set to 0.8 (for the results shown in <xref ref-type="fig" rid="fig8">Figure 8E–F</xref>) and to 0.6 (for the results shown in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1B</xref>), following the same rationale explained in the previous section.</p></sec><sec id="s4-22"><title>Computation of the chance level performance and of the linear separability of arbitrary groups of object views</title><p>As in the case of the single-cell analyses, we also computed chance classification performances (for both the tests of linear separability and generalization). These were obtained by running the same decoding analyses described above, but with the key difference of randomly shuffling first the association between object identity labels and pseudo-population response vectors. This was done following the same rationale of the single-cell analysis (see description above), i.e., by randomly shuffling only the label <inline-formula><mml:math id="inf169"><mml:mi>O</mml:mi></mml:math></inline-formula>, which specifies object identity, and not the label <inline-formula><mml:math id="inf170"><mml:mi>T</mml:mi></mml:math></inline-formula>, which specifies the transformation the objet underwent.</p><p>For any given neuronal subpopulation and any given object pair, chance performances were subtracted from the actual (i.e., un-shuffled) classification performances. This allowed measuring and reporting the net amount of transformation-tolerance afforded by the neuronal populations in the four visual areas. This subtraction was done only for the performances computed as the mutual information between the actual and the predicted object labels from the decoding outcomes (i.e., results shown in <xref ref-type="fig" rid="fig8">Figure 8C and E</xref>, left and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1A and B</xref>, left). For the results reported in terms of classification accuracy (<xref ref-type="fig" rid="fig8">Figure 8C and E</xref>, right and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1A and B</xref>, right), the chance performances were not subtracted from the actual performances, so as to better highlight that the theoretical chance level was 50% correct discrimination. These chance performances were all between 49.4% and 50.1% correct discrimination for the linear separability test, and between 49.98% and 50.02% correct discrimination for the generalization test.</p><p>Finally, for each pair of objects <inline-formula><mml:math id="inf171"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf172"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the linear separability analysis was also repeated for arbitrary groups of views of the two objects. To perform this test, in any subpopulation resampling iteration, out of the 19 views of object <inline-formula><mml:math id="inf173"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, nine were randomly chosen and assigned to the first group, while the remaining 10 were assigned to the second group. Similarly, 10 randomly sampled views of object <inline-formula><mml:math id="inf174"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo> </mml:mo></mml:mrow></mml:math></inline-formula>were assigned to the first group and the remaining nine to the second group. The decoding procedure was then performed as previously described. Only, in this case, the linear SVM decoders were trained to find a hyperplane in the response vector space that discriminated the set of views of the first group from the set of views of the second one. Fifty resampling iterations were carried out, each yielding a classification performance. The resulting 50 decoding performances were then averaged to obtain the linear separability of two arbitrary groups of object views, taken from that specific object pair. Finally, the performances obtained for different pairs were averaged to estimate the linear separability of arbitrary groups of object views afforded by neuronal subpopulation (shown as dashed lines in <xref ref-type="fig" rid="fig8">Figure 8C</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1A</xref>).</p></sec><sec id="s4-23"><title>Choice of the object transformations used in the population decoding analyses</title><p>As explained in Materials and methods, out of the 38 transformations tested for each object, 23 were chosen, for each neuron, to be used in the single-cell decoding analyses. These included all the eight position changes, plus the remaining transformations (size and luminance changes, as well as in-plane and in-depth rotations) shown at the position that was closer to the neuron’s RF center (i.e., either −15° or +15° of visual angle). Out of these 23 transformations, 19 were used in the population decoding analysis. In fact, for the neurons with RF center that was closer to −15°, the rightmost four positions (i.e., from +7.5° to +30° of visual angle) were excluded. Similarly, for the neurons with RF center that was closer to +15°, the leftmost four positions (i.e., from −22.5° to 0° of visual angle) were excluded. As a result, for the group of neurons with RFs closer to the −15° position, the 19 chosen transformations were the complementary set of the 19 transformations chosen for the group of neurons with RFs closer to the +15° position. This allowed aligning the two groups of neuronal RFs, by mapping the [−22.5°, 0°] range of positions of the first group onto the [ + 7.5°,+30°] range of positions of the second one, thus obtaining a single neuronal population, from which the neurons in each resampling iteration could be drawn.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Alessio Ansuini for his help in implementing the spike sorting procedure, Rosilari Bellacosa Marotti for her help with some of the statistical analyses and Alberto Petrocelli for his help in defining the protocols for anesthesia and surgery.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>ST, Conceptualization, Resources, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>HS, Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>GDF, Resources, Data curation, Formal analysis, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con4"><p>FBR, Resources, Data curation, Investigation</p></fn><fn fn-type="con" id="con5"><p>WV, Conceptualization, Software, Formal analysis, Visualization, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>MR, Resources, Formal analysis, Investigation, Visualization</p></fn><fn fn-type="con" id="con7"><p>FB, Resources, Formal analysis, Visualization</p></fn><fn fn-type="con" id="con8"><p>SP, Conceptualization, Formal analysis, Supervision, Funding acquisition, Investigation, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con9"><p>DZ, Conceptualization, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing—original draft, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Animal experimentation: All animal procedures were in agreement with international and institutional standards for the care and use of animals in research and were approved by the Italian Ministry of Health: project N. DGSAF 22791-A (submitted on Sep. 7, 2015) was approved on Dec. 10, 2015 (approval N. 1254/2015-PR); project N. 5388-III/14 (submitted on Aug. 23, 2012) and project N. 3612-III/12 (submitted on Sep. 15, 2009) were approved according to the legislative decree 116/92, article 7. All surgical procedures were performed under anesthesia, and every effort was made to minimize suffering (details explain din Materials and Methods).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><sec id="s7" sec-type="datasets"><title>Major datasets</title><p>The following dataset was generated:</p><p><related-object content-type="generated-dataset" id="data-ro1" source-id="http://dx.doi.org/10.5061/dryad.vd8tf" source-id-type="uri"><collab>Sina Tafazoli</collab><x>,</x> <collab>Houman Safaai</collab><x>,</x> <collab>Gioia De Franceschi</collab><x>,</x> <collab>Federica Bianca Rosselli</collab><x>,</x> <collab>Walter Vanzella</collab><x>,</x> <collab>Margherita Riggi</collab><x>,</x> <collab>Federica Buffolo</collab><x>,</x> <collab>Stefano Panzeri</collab><x>,</x> <collab>Davide Zoccolan</collab><x>,</x> <year>2016</year><x>,</x><source>Source data file for the article authored by Tafazoli and colleagues on invariant object representations in rat visual cortex</source><x>,</x> <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.vd8tf">http://dx.doi.org/10.5061/dryad.vd8tf</ext-link><x>,</x> <comment>Available at Dryad Digital Repository under a CC0 Public Domain Dedication</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>L</given-names></name><name><surname>Aghajan</surname><given-names>ZM</given-names></name><name><surname>Vuong</surname><given-names>C</given-names></name><name><surname>Moore</surname><given-names>JJ</given-names></name><name><surname>Mehta</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Causal influence of visual cues on hippocampal directional selectivity</article-title><source>Cell</source><volume>164</volume><fpage>197</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.12.015</pub-id><pub-id pub-id-type="pmid">26709045</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aggleton</surname><given-names>JP</given-names></name><name><surname>Keen</surname><given-names>S</given-names></name><name><surname>Warburton</surname><given-names>EC</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Extensive cytotoxic lesions involving both the rhinal cortices and area TE impair recognition but spare spatial alternation in the rat</article-title><source>Brain Research Bulletin</source><volume>43</volume><fpage>279</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1016/S0361-9230(97)00007-5</pub-id><pub-id pub-id-type="pmid">9227838</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alemi-Neissi</surname><given-names>A</given-names></name><name><surname>Rosselli</surname><given-names>FB</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multifeatural shape processing in rats engaged in invariant visual object recognition</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>5939</fpage><lpage>5956</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3629-12.2013</pub-id><pub-id pub-id-type="pmid">23554476</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andermann</surname><given-names>ML</given-names></name><name><surname>Kerlin</surname><given-names>AM</given-names></name><name><surname>Roumis</surname><given-names>DK</given-names></name><name><surname>Glickfeld</surname><given-names>LL</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specialization of mouse higher visual cortical areas</article-title><source>Neuron</source><volume>72</volume><fpage>1025</fpage><lpage>1039</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.11.013</pub-id><pub-id pub-id-type="pmid">22196337</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassi</surname><given-names>C</given-names></name><name><surname>Alemi-Neissi</surname><given-names>A</given-names></name><name><surname>Pagan</surname><given-names>M</given-names></name><name><surname>Dicarlo</surname><given-names>JJ</given-names></name><name><surname>Zecchina</surname><given-names>R</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Shape similarity, better than semantic membership, accounts for the structure of visual object representations in a population of monkey inferotemporal neurons</article-title><source>PLoS Computational Biology</source><volume>9</volume><elocation-id>e1003167</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003167</pub-id><pub-id pub-id-type="pmid">23950700</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanche</surname><given-names>TJ</given-names></name><name><surname>Spacek</surname><given-names>MA</given-names></name><name><surname>Hetke</surname><given-names>JF</given-names></name><name><surname>Swindale</surname><given-names>NV</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Polytrodes: high-density silicon electrode arrays for large-scale multiunit recording</article-title><source>Journal of Neurophysiology</source><volume>93</volume><fpage>2987</fpage><lpage>3000</lpage><pub-id pub-id-type="doi">10.1152/jn.01023.2004</pub-id><pub-id pub-id-type="pmid">15548620</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Information theory and neural coding</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>947</fpage><lpage>957</lpage><pub-id pub-id-type="doi">10.1038/14731</pub-id><pub-id pub-id-type="pmid">10526332</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brincat</surname><given-names>SL</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Underlying principles of visual shape selectivity in posterior inferotemporal cortex</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>880</fpage><lpage>886</lpage><pub-id pub-id-type="doi">10.1038/nn1278</pub-id><pub-id pub-id-type="pmid">15235606</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname><given-names>CR</given-names></name><name><surname>Ramesh</surname><given-names>RN</given-names></name><name><surname>Sugden</surname><given-names>AU</given-names></name><name><surname>Levandowski</surname><given-names>KM</given-names></name><name><surname>Minnig</surname><given-names>MA</given-names></name><name><surname>Fenselau</surname><given-names>H</given-names></name><name><surname>Lowell</surname><given-names>BB</given-names></name><name><surname>Andermann</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Hunger-Dependent enhancement of food cue responses in mouse postrhinal cortex and lateral amygdala</article-title><source>Neuron</source><volume>91</volume><fpage>1154</fpage><lpage>1169</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.07.032</pub-id><pub-id pub-id-type="pmid">27523426</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busse</surname><given-names>L</given-names></name><name><surname>Wade</surname><given-names>AR</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Representation of concurrent stimuli by population activity in visual cortex</article-title><source>Neuron</source><volume>64</volume><fpage>931</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.11.004</pub-id><pub-id pub-id-type="pmid">20064398</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>YP</given-names></name><name><surname>Lin</surname><given-names>CP</given-names></name><name><surname>Hsu</surname><given-names>YC</given-names></name><name><surname>Hung</surname><given-names>CP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Network anisotropy trumps noise for efficient object coding in macaque inferior temporal cortex</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>9889</fpage><lpage>9899</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4595-14.2015</pub-id><pub-id pub-id-type="pmid">26156990</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coogan</surname><given-names>TA</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Hierarchical organization of areas in rat visual cortex</article-title><source>Journal of Neuroscience</source><volume>13</volume><fpage>3749</fpage><lpage>3772</lpage><pub-id pub-id-type="pmid">7690066</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooke</surname><given-names>SF</given-names></name><name><surname>Bear</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual recognition memory: a view from V1</article-title><source>Current Opinion in Neurobiology</source><volume>35</volume><fpage>57</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.06.008</pub-id><pub-id pub-id-type="pmid">26151761</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cover</surname><given-names>TM</given-names></name></person-group><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><edition>2nd Edn</edition><source>Elements of Information Theory</source><publisher-loc>Hoboken, N.J</publisher-loc><publisher-name>Wiley-Interscience</publisher-name></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cristianini</surname><given-names>N</given-names></name><name><surname>Shawe-Taylor</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>An Introduction to Support Vector Machines: And Other Kernel-Based Learning Methods</source><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cushman</surname><given-names>JD</given-names></name><name><surname>Aharoni</surname><given-names>DB</given-names></name><name><surname>Willers</surname><given-names>B</given-names></name><name><surname>Ravassard</surname><given-names>P</given-names></name><name><surname>Kees</surname><given-names>A</given-names></name><name><surname>Vuong</surname><given-names>C</given-names></name><name><surname>Popeney</surname><given-names>B</given-names></name><name><surname>Arisaka</surname><given-names>K</given-names></name><name><surname>Mehta</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multisensory control of multimodal behavior: do the legs know what the tongue is doing?</article-title><source>PLoS One</source><volume>8</volume><elocation-id>e80465</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0080465</pub-id><pub-id pub-id-type="pmid">24224054</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Lane</surname><given-names>JW</given-names></name><name><surname>Hsiao</surname><given-names>SS</given-names></name><name><surname>Johnson</surname><given-names>KO</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Marking microelectrode penetrations with fluorescent dyes</article-title><source>Journal of Neuroscience Methods</source><volume>64</volume><fpage>75</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/0165-0270(95)00113-1</pub-id><pub-id pub-id-type="pmid">8869487</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Shamayleh</surname><given-names>Y</given-names></name><name><surname>Pasupathy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Contour curvature as an invariant code for objects in visual area V4</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>5532</fpage><lpage>5543</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4139-15.2016</pub-id><pub-id pub-id-type="pmid">27194333</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Espinoza</surname><given-names>SG</given-names></name><name><surname>Thomas</surname><given-names>HC</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Retinotopic organization of striate and extrastriate visual cortex in the hooded rat</article-title><source>Brain Research</source><volume>272</volume><fpage>137</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(83)90370-0</pub-id><pub-id pub-id-type="pmid">6616189</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fabre-Thorpe</surname><given-names>M</given-names></name><name><surname>Richard</surname><given-names>G</given-names></name><name><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Rapid categorization of natural images by rhesus monkeys</article-title><source>NeuroReport</source><volume>9</volume><fpage>303</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1097/00001756-199801260-00023</pub-id><pub-id pub-id-type="pmid">9507973</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Cotton</surname><given-names>RJ</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Yatsenko</surname><given-names>D</given-names></name><name><surname>Saggau</surname><given-names>P</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Population code in mouse V1 facilitates readout of natural scenes through increased sparseness</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>851</fpage><lpage>857</lpage><pub-id pub-id-type="doi">10.1038/nn.3707</pub-id><pub-id pub-id-type="pmid">24747577</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallardo</surname><given-names>L</given-names></name><name><surname>Mottles</surname><given-names>M</given-names></name><name><surname>Vera</surname><given-names>L</given-names></name><name><surname>Carrasco</surname><given-names>MA</given-names></name><name><surname>Torrealba</surname><given-names>F</given-names></name><name><surname>Montero</surname><given-names>VM</given-names></name><name><surname>Pinto-Hamuy</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Failure by rats to learn a visual conditional discrimination after lateral peristriate cortical lesions</article-title><source>Physiological Psychology</source><volume>7</volume><fpage>173</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.3758/BF03332905</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gavornik</surname><given-names>JP</given-names></name><name><surname>Bear</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Higher brain functions served by the lowly rodent primary visual cortex</article-title><source>Learning &amp; Memory</source><volume>21</volume><fpage>527</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1101/lm.034355.114</pub-id><pub-id pub-id-type="pmid">25225298</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelbard-Sagiv</surname><given-names>H</given-names></name><name><surname>Mukamel</surname><given-names>R</given-names></name><name><surname>Harel</surname><given-names>M</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Internally generated reactivation of single neurons in human Hippocampus during free recall</article-title><source>Science</source><volume>322</volume><fpage>96</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1126/science.1164685</pub-id><pub-id pub-id-type="pmid">18772395</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glickfeld</surname><given-names>LL</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Andermann</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A mouse model of higher visual cortical function</article-title><source>Current Opinion in Neurobiology</source><volume>24</volume><fpage>28</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.08.009</pub-id><pub-id pub-id-type="pmid">24492075</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haider</surname><given-names>B</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inhibition dominates sensory responses in the awake cortex</article-title><source>Nature</source><volume>493</volume><fpage>97</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1038/nature11665</pub-id><pub-id pub-id-type="pmid">23172139</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>The Elements of Statistical Learning: Data Mining, Inference, and Prediction 2 Edizione</source><publisher-name>Springer-Verlag</publisher-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinke</surname><given-names>W</given-names></name><name><surname>Schwarzbauer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Subanesthetic isoflurane affects task-induced brain activation in a highly specific manner: a functional magnetic resonance imaging study</article-title><source>Anesthesiology</source><volume>94</volume><fpage>973</fpage><lpage>981</lpage><pub-id pub-id-type="doi">10.1097/00000542-200106000-00010</pub-id><pub-id pub-id-type="pmid">11465623</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>DN</given-names></name><name><surname>Mehta</surname><given-names>SB</given-names></name><name><surname>Kleinfeld</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Quality metrics to accompany spike sorting of extracellular signals</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>8699</fpage><lpage>8705</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0971-11.2011</pub-id><pub-id pub-id-type="pmid">21677152</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Explicit information for category-orthogonal object properties increases along the ventral stream</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>613</fpage><lpage>622</lpage><pub-id pub-id-type="doi">10.1038/nn.4247</pub-id><pub-id pub-id-type="pmid">26900926</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huberman</surname><given-names>AD</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What can mice tell Us about how vision works?</article-title><source>Trends in Neurosciences</source><volume>34</volume><fpage>464</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2011.07.002</pub-id><pub-id pub-id-type="pmid">21840069</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>CP</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Fast readout of object identity from macaque inferior temporal cortex</article-title><source>Science</source><volume>310</volume><fpage>863</fpage><lpage>866</lpage><pub-id pub-id-type="doi">10.1126/science.1117593</pub-id><pub-id pub-id-type="pmid">16272124</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Mazzoni</surname><given-names>A</given-names></name><name><surname>Bartels</surname><given-names>A</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A novel test to determine the significance of neural selectivity to single and multiple potentially correlated stimulus features</article-title><source>Journal of Neuroscience Methods</source><volume>210</volume><fpage>49</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2011.11.013</pub-id><pub-id pub-id-type="pmid">22142889</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Tamura</surname><given-names>H</given-names></name><name><surname>Fujita</surname><given-names>I</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Size and position invariance of neuronal responses in monkey inferotemporal cortex</article-title><source>Journal of Neurophysiology</source><volume>73</volume><fpage>218</fpage><lpage>226</lpage><pub-id pub-id-type="pmid">7714567</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Pattern and component motion responses in mouse visual cortical areas</article-title><source>Current Biology : CB</source><volume>25</volume><fpage>1759</fpage><lpage>1764</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.05.028</pub-id><pub-id pub-id-type="pmid">26073133</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katzner</surname><given-names>S</given-names></name><name><surname>Weigelt</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visual cortical networks: of mice and men</article-title><source>Current Opinion in Neurobiology</source><volume>23</volume><fpage>202</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.01.019</pub-id><pub-id pub-id-type="pmid">23415830</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Mirpour</surname><given-names>K</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Object category structure in response patterns of neuronal population in monkey inferior temporal cortex</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>4296</fpage><lpage>4309</lpage><pub-id pub-id-type="doi">10.1152/jn.00024.2007</pub-id><pub-id pub-id-type="pmid">17428910</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobatake</surname><given-names>E</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex</article-title><source>Journal of Neurophysiology</source><volume>71</volume><fpage>856</fpage><lpage>867</lpage><pub-id pub-id-type="pmid">8201425</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>What response properties do individual neurons need to underlie position and clutter &quot;invariant&quot; object recognition?</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>360</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1152/jn.90745.2008</pub-id><pub-id pub-id-type="pmid">19439676</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Guggenberger</surname><given-names>H</given-names></name><name><surname>Peled</surname><given-names>S</given-names></name><name><surname>Pauls</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Functional imaging of the monkey brain</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>555</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1038/9210</pub-id><pub-id pub-id-type="pmid">10448221</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>L</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Genetic dissection of neural circuits</article-title><source>Neuron</source><volume>57</volume><fpage>634</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.01.002</pub-id><pub-id pub-id-type="pmid">18341986</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magri</surname><given-names>C</given-names></name><name><surname>Whittingstall</surname><given-names>K</given-names></name><name><surname>Singh</surname><given-names>V</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A toolbox for the fast information analysis of multiple-site LFP, EEG and spike train recordings</article-title><source>BMC Neuroscience</source><volume>10</volume><elocation-id>81</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-10-81</pub-id><pub-id pub-id-type="pmid">19607698</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specialization of seven mouse visual cortical areas</article-title><source>Neuron</source><volume>72</volume><fpage>1040</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.004</pub-id><pub-id pub-id-type="pmid">22196338</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDaniel</surname><given-names>WF</given-names></name><name><surname>Coleman</surname><given-names>J</given-names></name><name><surname>Lindsay</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>A comparison of lateral peristriate and striate neocortical ablations in the rat</article-title><source>Behavioural Brain Research</source><volume>6</volume><fpage>249</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(82)90027-4</pub-id><pub-id pub-id-type="pmid">7171386</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montemurro</surname><given-names>MA</given-names></name><name><surname>Rasch</surname><given-names>MJ</given-names></name><name><surname>Murayama</surname><given-names>Y</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Phase-of-firing coding of natural visual stimuli in primary visual cortex</article-title><source>Current Biology : CB</source><volume>18</volume><fpage>375</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.02.023</pub-id><pub-id pub-id-type="pmid">18328702</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montero</surname><given-names>VM</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Retinotopy of cortical connections between the striate cortex and extrastriate visual areas in the rat</article-title><source>Experimental Brain Research</source><volume>94</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1007/BF00230466</pub-id><pub-id pub-id-type="pmid">8335065</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nandy</surname><given-names>AS</given-names></name><name><surname>Sharpee</surname><given-names>TO</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Mitchell</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The fine structure of shape tuning in area V4</article-title><source>Neuron</source><volume>78</volume><fpage>1102</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.016</pub-id><pub-id pub-id-type="pmid">23791199</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Highly selective receptive fields in mouse visual cortex</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>7520</fpage><lpage>7536</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0623-08.2008</pub-id><pub-id pub-id-type="pmid">18650330</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Exploring the next frontier of mouse vision</article-title><source>Neuron</source><volume>72</volume><fpage>889</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.011</pub-id><pub-id pub-id-type="pmid">22196324</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op De Beeck</surname><given-names>H</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Spatial sensitivity of macaque inferior temporal neurons</article-title><source>The Journal of Comparative Neurology</source><volume>426</volume><fpage>505</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1002/1096-9861(20001030)426:4&lt;505::AID-CNE1&gt;3.0.CO;2-M</pub-id><pub-id pub-id-type="pmid">11027395</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Optican</surname><given-names>LM</given-names></name><name><surname>Richmond</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Temporal encoding of two-dimensional patterns by single units in primate inferior temporal cortex. III. information theoretic analysis</article-title><source>Journal of Neurophysiology</source><volume>57</volume><fpage>162</fpage><lpage>178</lpage><pub-id pub-id-type="pmid">3559670</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagan</surname><given-names>M</given-names></name><name><surname>Urban</surname><given-names>LS</given-names></name><name><surname>Wohl</surname><given-names>MP</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Signals in inferotemporal and perirhinal cortex suggest an untangling of visual target information</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1132</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1038/nn.3433</pub-id><pub-id pub-id-type="pmid">23792943</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Treves</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Analytical estimates of limited sampling biases in different information measures</article-title><source>Network</source><volume>7</volume><fpage>87</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1088/0954-898X/7/1/006</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Senatore</surname><given-names>R</given-names></name><name><surname>Montemurro</surname><given-names>MA</given-names></name><name><surname>Petersen</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Correcting for the sampling Bias problem in spike train information measures</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>1064</fpage><lpage>1072</lpage><pub-id pub-id-type="doi">10.1152/jn.00559.2007</pub-id><pub-id pub-id-type="pmid">17615128</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname><given-names>A</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Population coding of shape in area V4</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>1332</fpage><lpage>1338</lpage><pub-id pub-id-type="doi">10.1038/nn972</pub-id><pub-id pub-id-type="pmid">12426571</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pecka</surname><given-names>M</given-names></name><name><surname>Han</surname><given-names>Y</given-names></name><name><surname>Sader</surname><given-names>E</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Experience-dependent specialization of receptive field surround for selective coding of natural scenes</article-title><source>Neuron</source><volume>84</volume><fpage>457</fpage><lpage>469</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.010</pub-id><pub-id pub-id-type="pmid">25263755</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polack</surname><given-names>PO</given-names></name><name><surname>Contreras</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Long-range parallel processing and local recurrent activity in the visual cortex of the mouse</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>11120</fpage><lpage>11131</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6304-11.2012</pub-id><pub-id pub-id-type="pmid">22875943</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quian Quiroga</surname><given-names>R</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Extracting information from neuronal populations: information theory and decoding approaches</article-title><source>Nature Reviews. Neuroscience</source><volume>10</volume><fpage>173</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1038/nrn2578</pub-id><pub-id pub-id-type="pmid">19229240</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>RQ</given-names></name><name><surname>Nadasdy</surname><given-names>Z</given-names></name><name><surname>Ben-Shaul</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Unsupervised spike detection and sorting with wavelets and superparamagnetic clustering</article-title><source>Neural Computation</source><volume>16</volume><fpage>1661</fpage><lpage>1687</lpage><pub-id pub-id-type="doi">10.1162/089976604774201631</pub-id><pub-id pub-id-type="pmid">15228749</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>RQ</given-names></name><name><surname>Reddy</surname><given-names>L</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Invariant visual representation by single neurons in the human brain</article-title><source>Nature</source><volume>435</volume><fpage>1102</fpage><lpage>1107</lpage><pub-id pub-id-type="doi">10.1038/nature03687</pub-id><pub-id pub-id-type="pmid">15973409</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>RQ</given-names></name><name><surname>Mukamel</surname><given-names>R</given-names></name><name><surname>Isham</surname><given-names>EA</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Human single-neuron responses at the threshold of conscious recognition</article-title><source>PNAS</source><volume>105</volume><fpage>3599</fpage><lpage>3604</lpage><pub-id pub-id-type="doi">10.1073/pnas.0707043105</pub-id><pub-id pub-id-type="pmid">18299568</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rieke</surname><given-names>F</given-names></name><name><surname>Warland</surname><given-names>D</given-names></name><name><surname>Ruyter van Steveninck</surname><given-names>RR</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>Spikes: Exploring the Neural Code</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Tovee</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Sparseness of the neuronal representation of stimuli in the primate temporal visual cortex</article-title><source>Journal of Neurophysiology</source><volume>73</volume><fpage>713</fpage><lpage>726</lpage><pub-id pub-id-type="pmid">7760130</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Treves</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The neuronal encoding of information in the brain</article-title><source>Progress in Neurobiology</source><volume>95</volume><fpage>448</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2011.08.002</pub-id><pub-id pub-id-type="pmid">21907758</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosselli</surname><given-names>FB</given-names></name><name><surname>Alemi</surname><given-names>A</given-names></name><name><surname>Ansuini</surname><given-names>A</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Object similarity affects the perceptual strategy underlying invariant visual object recognition in rats</article-title><source>Frontiers in Neural Circuits</source><volume>9</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2015.00010</pub-id><pub-id pub-id-type="pmid">25814936</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Fabre-Thorpe</surname><given-names>M</given-names></name><name><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Parallel processing in high-level categorization of natural images</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>629</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1038/nn866</pub-id><pub-id pub-id-type="pmid">12032544</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Dicarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Selectivity and tolerance (&quot;invariance&quot;) both increase as visual information propagates from cortical area V4 to IT</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>12978</fpage><lpage>12995</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0179-10.2010</pub-id><pub-id pub-id-type="pmid">20881116</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Balanced increases in selectivity and tolerance produce constant sparseness along the ventral visual stream</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>10170</fpage><lpage>10182</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6125-11.2012</pub-id><pub-id pub-id-type="pmid">22836252</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanderson</surname><given-names>KJ</given-names></name><name><surname>Dreher</surname><given-names>B</given-names></name><name><surname>Gayer</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Prosencephalic connections of striate and extrastriate areas of rat visual cortex</article-title><source>Experimental Brain Research</source><volume>85</volume><fpage>324</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1007/BF00229410</pub-id><pub-id pub-id-type="pmid">1716594</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>T</given-names></name><name><surname>Uchida</surname><given-names>G</given-names></name><name><surname>Lescroart</surname><given-names>MD</given-names></name><name><surname>Kitazono</surname><given-names>J</given-names></name><name><surname>Okada</surname><given-names>M</given-names></name><name><surname>Tanifuji</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Object representation in inferior temporal cortex is organized hierarchically in a mosaic-like structure</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>16642</fpage><lpage>16656</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5557-12.2013</pub-id><pub-id pub-id-type="pmid">24133267</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuett</surname><given-names>S</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Mapping retinotopic structure in mouse visual cortex with optical imaging</article-title><source>Journal of Neuroscience</source><volume>22</volume><fpage>6549</fpage><lpage>6559</lpage><pub-id pub-id-type="pmid">12151534</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Smola</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Learning with Kernels</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpee</surname><given-names>TO</given-names></name><name><surname>Kouh</surname><given-names>M</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Trade-off between curvature tuning and position invariance in visual area V4</article-title><source>PNAS</source><volume>110</volume><fpage>11618</fpage><lpage>11623</lpage><pub-id pub-id-type="doi">10.1073/pnas.1217479110</pub-id><pub-id pub-id-type="pmid">23798444</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steriade</surname><given-names>M</given-names></name><name><surname>Nuñez</surname><given-names>A</given-names></name><name><surname>Amzica</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>A novel slow (&lt; 1 hz) oscillation of neocortical neurons in vivo: depolarizing and hyperpolarizing components</article-title><source>Journal of Neuroscience</source><volume>13</volume><fpage>3252</fpage><lpage>3265</lpage><pub-id pub-id-type="pmid">8340806</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugase</surname><given-names>Y</given-names></name><name><surname>Yamane</surname><given-names>S</given-names></name><name><surname>Ueno</surname><given-names>S</given-names></name><name><surname>Kawano</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Global and fine information coded by single neurons in the temporal visual cortex</article-title><source>Nature</source><volume>400</volume><fpage>869</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1038/23703</pub-id><pub-id pub-id-type="pmid">10476965</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sánchez</surname><given-names>RF</given-names></name><name><surname>Montero</surname><given-names>VM</given-names></name><name><surname>Espinoza</surname><given-names>SG</given-names></name><name><surname>Díaz</surname><given-names>E</given-names></name><name><surname>Canitrot</surname><given-names>M</given-names></name><name><surname>Pinto-Hamuy</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Visuospatial discrimination deficit in rats after ibotenate lesions in anteromedial visual cortex</article-title><source>Physiology &amp; Behavior</source><volume>62</volume><fpage>989</fpage><lpage>994</lpage><pub-id pub-id-type="doi">10.1016/S0031-9384(97)00201-1</pub-id><pub-id pub-id-type="pmid">9333191</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tafazoli</surname><given-names>S</given-names></name><name><surname>Di Filippo</surname><given-names>A</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Transformation-tolerant object recognition in rats revealed by visual priming</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>21</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3932-11.2012</pub-id><pub-id pub-id-type="pmid">22219267</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tees</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The effects of posterior parietal and posterior temporal cortical lesions on multimodal spatial and nonspatial competencies in rats</article-title><source>Behavioural Brain Research</source><volume>106</volume><fpage>55</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/S0166-4328(99)00092-3</pub-id><pub-id pub-id-type="pmid">10595422</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname><given-names>S</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Marlot</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><volume>381</volume><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/381520a0</pub-id><pub-id pub-id-type="pmid">8632824</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tovee</surname><given-names>MJ</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Azzopardi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Translation invariance in the responses to faces of single neurons in the temporal visual cortical areas of the alert macaque</article-title><source>Journal of Neurophysiology</source><volume>72</volume><fpage>1049</fpage><lpage>1060</lpage><pub-id pub-id-type="pmid">7807195</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsunoda</surname><given-names>K</given-names></name><name><surname>Yamane</surname><given-names>Y</given-names></name><name><surname>Nishizaki</surname><given-names>M</given-names></name><name><surname>Tanifuji</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Complex objects are represented in macaque inferotemporal cortex by the combination of feature columns</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>832</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1038/90547</pub-id><pub-id pub-id-type="pmid">11477430</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaiceliunaite</surname><given-names>A</given-names></name><name><surname>Erisken</surname><given-names>S</given-names></name><name><surname>Franzen</surname><given-names>F</given-names></name><name><surname>Katzner</surname><given-names>S</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spatial integration in mouse primary visual cortex</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>964</fpage><lpage>972</lpage><pub-id pub-id-type="doi">10.1152/jn.00138.2013</pub-id><pub-id pub-id-type="pmid">23719206</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A multivariate approach reveals the behavioral templates underlying visual discrimination in rats</article-title><source>Current Biology : CB</source><volume>22</volume><fpage>50</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.11.041</pub-id><pub-id pub-id-type="pmid">22209530</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Gerich</surname><given-names>FJ</given-names></name><name><surname>Ytebrouck</surname><given-names>E</given-names></name><name><surname>Arckens</surname><given-names>L</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Van den Bergh</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functional specialization in rat occipital and temporal visual cortex</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>1963</fpage><lpage>1983</lpage><pub-id pub-id-type="doi">10.1152/jn.00737.2013</pub-id><pub-id pub-id-type="pmid">24990566</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinje</surname><given-names>WE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Sparse coding and decorrelation in primary visual cortex during natural vision</article-title><source>Science</source><volume>287</volume><fpage>1273</fpage><lpage>1276</lpage><pub-id pub-id-type="doi">10.1126/science.287.5456.1273</pub-id><pub-id pub-id-type="pmid">10678835</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visual categorization of natural movies by rats</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>10645</fpage><lpage>10658</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3663-13.2014</pub-id><pub-id pub-id-type="pmid">25100598</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Area map of mouse visual cortex</article-title><source>The Journal of Comparative Neurology</source><volume>502</volume><fpage>339</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1002/cne.21286</pub-id><pub-id pub-id-type="pmid">17366604</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Gao</surname><given-names>E</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Gateways of ventral and dorsal streams in mouse visual cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>1905</fpage><lpage>1918</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3488-10.2011</pub-id><pub-id pub-id-type="pmid">21289200</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Network analysis of corticocortical connections reveals ventral and dorsal processing streams in mouse visual cortex</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>4386</fpage><lpage>4399</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6063-11.2012</pub-id><pub-id pub-id-type="pmid">22457489</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiesenfeld</surname><given-names>Z</given-names></name><name><surname>Branchek</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Refractive state and visual acuity in the hooded rat</article-title><source>Vision Research</source><volume>16</volume><fpage>823</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(76)90142-5</pub-id><pub-id pub-id-type="pmid">960609</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>BD</given-names></name><name><surname>Prenger</surname><given-names>RJ</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural representation of natural images in visual area V2</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>2102</fpage><lpage>2114</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4099-09.2010</pub-id><pub-id pub-id-type="pmid">20147538</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wörtwein</surname><given-names>G</given-names></name><name><surname>Mogensen</surname><given-names>J</given-names></name><name><surname>Williams</surname><given-names>G</given-names></name><name><surname>Carlos</surname><given-names>JH</given-names></name><name><surname>Divac</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Cortical area in the rat that mediates visual pattern discrimination</article-title><source>Acta Neurobiologiae Experimentalis</source><volume>54</volume><fpage>365</fpage><lpage>376</lpage><pub-id pub-id-type="pmid">7887187</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Yao</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Modification of visual cortical receptive field induced by natural stimuli</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>1923</fpage><lpage>1932</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs178</pub-id><pub-id pub-id-type="pmid">22735159</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Kouh</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Trade-off between object selectivity and tolerance in monkey inferotemporal cortex</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>12292</fpage><lpage>12307</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1897-07.2007</pub-id><pub-id pub-id-type="pmid">17989294</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Oertelt</surname><given-names>N</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A rodent model for the study of invariant visual object recognition</article-title><source>PNAS</source><volume>106</volume><fpage>8748</fpage><lpage>8753</lpage><pub-id pub-id-type="doi">10.1073/pnas.0811583106</pub-id><pub-id pub-id-type="pmid">19429704</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Invariant visual object recognition and shape processing in rats</article-title><source>Behavioural Brain Research</source><volume>285</volume><fpage>10</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2014.12.053</pub-id><pub-id pub-id-type="pmid">25561421</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.22794.029</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Tsao</surname><given-names>Doris Y</given-names></name><role>Reviewing editor</role><aff id="aff5"><institution>California Institute of Technology</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Emergence of transformation-tolerant representations of visual objects in rat lateral extrastriate cortex&quot; for consideration by <italic>eLife</italic>. Your article has been favorably evaluated by David Van Essen (Senior Editor) and three reviewers, one of whom is a member of our Board of Reviewing Editors. The following individual involved in review of your submission has agreed to reveal her identity: Nicole Rust (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>In this manuscript, the authors present a comparison of 4 visual areas in the rat with regard to object-based representations. The authors present a battery of stimuli consisting of various objects and identity-preserving transformations (size, position, view, etc.) of these objects and quantify using either mutual information or linear decoding analysis the extent to which single neurons and populations of neurons represent the visual stimulus in a transformation- invariant fashion. They find that while V1 is largely driven by low-level features (namely luminance in RF), neurons in higher visual areas LI and LL carry more information about high-level visual features (e.g. beyond total luminance). The work is systematic and carefully considered and the results can be compared with a better-understood species (the macaque). As such, the manuscript has the potential to become foundational for the field. The reviewers appreciate the care with which the authors have considered how simple low-level (luminance) modulations spuriously impact quantifying the responses of neurons at different stages of the pathway. Overall, the reviewers agree this is a high-quality study with interesting results, providing compelling evidence for object recognition function in lateral higher visual areas in rats.</p><p>The reviewers feel that the mutual information analysis should be included, but it needs to be better motivated. Furthermore, the authors need to take into account different total information about object identity (regardless of whether this is transformation-tolerant) across areas, in order to justify their claim that LL has a more linearly separable representation of object identity as a result of a transformation in information format from lower to higher stages.</p><p>Essential revisions:</p><p>1) The mutual information analysis is confusing to wade through. I appreciate that it represents upper bounds on the information present, however I think a more common method of making arguments about the representation of information in specific visual areas lies in decoding analysis or linear separability which is presented in later Figures (5 and 6). The problem with mutual information calculation is that I don't have a good metric with which to compare it to unlike decoding analysis where I know what chance decoding performance should be. The authors should explain the motivation for the mutual information analysis better, and perhaps compare to results of similar analyses in primates.</p><p>2) I have concerns related to the analyses that lead up to the main claim that area LL has a more robust view-invariant object representation than V1 (or LI). <xref ref-type="fig" rid="fig4">Figure 4</xref> presents an analysis that deconstructs total information into 1) &quot;view-invariant information&quot;, which is a quantification of how non-overlapping the responses to two objects are across views (regardless of whether that information is linearly separable) and 2) a component that accounts for the remaining non-overlap, which must be attributed to modulations by view. The main result that the authors highlight from <xref ref-type="fig" rid="fig4">Figure 4C</xref> is that for neurons that are selective for object features (not just luminance modulations), object information increases from V1 – LI – LL. Based on the claims outlined later in the paper, either the authors or I are misinterpreting what their &quot;view-invariant information&quot; measure reflects. As I understand it, 1) this is a &quot;per neuron&quot; (rather than total) measure of the amount of object-based information at each stage, and it is likely that different stages have different numbers of neurons, 2) it is a measure of the total information about object-identity, and 3) information about object-identity cannot be created in feed-forward pathway. The issue at hand is that – in the last paragraph of the subsection “The amount of view-invariant object information gradually increases from V1 to LL”, it implies that LL somehow has more total information for discriminating object identity than V1 (and LI) once luminance modulations are disregarded. This can only be true if 1) all the object-based information present in LL arrives there via a route that goes through V1 (and then LI) AND V1 (and LI) have more neurons than LL or 2) object-based information arrives in LL via some path that does not also pass through V1. Similarly, the linear separability analyses presented later in the paper (e.g. <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig8">8</xref>) are interesting only after first equating total information for object identity at each stage by (e.g.) allowing V1 (and LI) to have more neurons than LL. Once that is done properly, the main result – that LL is better at object discrimination with a linear read-out – may still hold (e.g. as a back-of-the-envelope calculation, LL has ~2x more object information per neuron (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) but comparing 96 V1 neurons to 48 LL neurons reveals that LL is still better (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). But this issue should be carefully considered for all analyses throughout the paper.</p><p>3) The major results were generated by computing the mutual information and training a decoder. These are good analytical tools, but do not offer intuitive interpretations for a broad audience (e.g., bit values &lt;&lt; 1). To address this issue, it would be nice to provide some basic statistics about stimulus representation. From the example neuron in <xref ref-type="fig" rid="fig2">Figure 2</xref>, it seems the response of the LL neuron is sparser than the V1 neuron. Is this true at the population level? Since the information and decoding analysis indicate that LL is transformation tolerant, can the authors infer what neurons might be &quot;encoding&quot;/&quot;tuned for&quot;/&quot;computing&quot;? Did the authors see clues in firing rate variation over different transformations? A large transformation space was used, but we did not learn much about the representation of these transformations. <xref ref-type="fig" rid="fig4">Figure 4B and C</xref> imply that information about transformations decreased along the ventral stream. How was the transformation-tolerance generated along the ventral stream? Was the information about transformation along different axes reduced simultaneously or sequentially? Neuronal responses from four visual areas along the ventral pathway was collected. Other than V1 and LL, the author did not assign potential functions to LM and LI. Do they serve roles for transformation representations along particular dimensions?</p><p>4) One of the careful steps of the analysis was to control for luminance. The total information about the object stimuli declined along the ventral stream. It was when discriminating stimuli matched for RF and luminance, view-invariant information and linear discrimination accuracy increased along the ventral stream. It is understandable that at single neuron level the information about the stimuli is largely affected by the relative location of RFs and stimulus. But at the population level, it is not clear why it is necessary to limit the analysis to luminance-matched pairs. At the population level, each individual stimulus would be represented at various RF/luminance levels in different neurons. Thus, if the representation space was invariant to transformation, luminance control might not be needed. Can the authors obtain similar results at the population level without controlling so carefully for luminance?</p><p>5) The authors do a suitable job of controlling for overall luminance across the different object conditions, however neurons in V1 are likely not only responding to contrast defined by the object edge which is effectively what is being quantified here, but also the contrast on the object surface. It seems at first glance to be similar across the objects, but it would be nice to see a quantification of the contrast present on the object surfaces as this could also be driving firing rate differences across the object conditions.</p><p>6) To motivate luminance-matched analysis, the author showed that the information of luminance was reducing from V1 to LL. Is LL luminance insensitive to the same object? I.e., firing in LL should change less than in other areas to transformation along the luminance axis. This can be directly tested using the existing data set.</p><p>7) The calculation of RF luminance is affected by the shape of the stimulus and RF. Was the increase of f_high purely a result of larger RFs in LL? RFs in higher visual areas cannot be modeled simply by a single Gaussian profile. And RF shape can be affected by the stimuli (even in V1, e.g., Yeh, Xing, et al. 2009 PNAS). Is the calculation of RF luminance robust enough? Can the uncertainty affect the information and decoding analysis?</p><p>8) In <xref ref-type="fig" rid="fig4">Figure 4C</xref>, we can see that V1, LM and LI actually have higher view-invariant object information when the luminance ratio is &lt; ~0.5. Although it is arguable that the object pair may be discriminated by the difference in luminance, it is possible that luminance difference across transformation of the same object is larger than the luminance difference between the object pair. The later suggests transformation-tolerant object representation in these areas. Please explain/clarify.</p><p>9) The authors emphasize the similarities between the hierarchical organization of macaque and rat ventral temporal cortex, in that greater linear separability of different object classes, invariant to view, is seen in higher areas. However, I don't see any direct quantification of performance between the two species. The decoding performance in the rat seems significantly worse than that in monkey ITC (60% on a binary choice in the generalization task, where macaque ITC decoding performance reaches human levels). Please comment on how the values of discriminability and generalizability presented in <xref ref-type="fig" rid="fig7">Figure 7</xref> compare to such analysis in the primate ventral stream.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.22794.030</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>1) The mutual information analysis is confusing to wade through. I appreciate that it represents upper bounds on the information present, however I think a more common method of making arguments about the representation of information in specific visual areas lies in decoding analysis or linear separability which is presented in later Figures (5 and 6). The problem with mutual information calculation is that I don't have a good metric with which to compare it to unlike decoding analysis where I know what chance decoding performance should be. The authors should explain the motivation for the mutual information analysis better, and perhaps compare to results of similar analyses in primates.</italic> </p><p>Following the reviewers’ suggestion, we have added two introductory paragraphs (subsection “The fraction of energy-independent stimulus information sharply increases from V1 to LL”, fourth paragraph and subsection “The amount of view-invariant object information gradually increases from V1 to LL”, first paragraph) to extensively explain the motivation of the mutual information analysis, its relationship with linear decoding analysis and the complementary nature of the two approaches. We have also consolidated the description of the information theoretic analysis, moving the definition of mutual information from the Methods to the Results (see eq. 1 and subsection “The fraction of energy-independent stimulus information sharply increases from V1 to LL”, fifth paragraph). Finally, we have also added a cartoon to <xref ref-type="fig" rid="fig3">Figure 3</xref>, to make it easier to interpret the meaning of mutual information (see <xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><p>In general, while we appreciate that the use of information theoretic metrics is not very common in the study of visual object recognition, as compared to other approaches based on pattern classification, the information metrics are the most widely used in many other fields of sensory coding, following the influential work of Bialek, Richmond, Victor and many others. The use of information theory in this paper, besides being specifically suited to the particular nature of the problem we investigate, may help both this paper and the problem of object recognition to get more attention from a wider section of the computational neuroscience community, which, in turn, can be stimulated to bring more techniques and ideas into this research field.</p><p><italic>2) I have concerns related to the analyses that lead up to the main claim that area LL has a more robust view-invariant object representation than V1 (or LI). <xref ref-type="fig" rid="fig4">Figure 4</xref> presents an analysis that deconstructs total information into 1) &quot;view-invariant information&quot;, which is a quantification of how non-overlapping the responses to two objects are across views (regardless of whether that information is linearly separable) and 2) a component that accounts for the remaining non-overlap, which must be attributed to modulations by view. The main result that the authors highlight from <xref ref-type="fig" rid="fig4">Figure 4C</xref> is that for neurons that are selective for object features (not just luminance modulations), object information increases from V1 – LI – LL. Based on the claims outlined later in the paper, either the authors or I are misinterpreting what their &quot;view-invariant information&quot; measure reflects. As I understand it, 1) this is a &quot;per neuron&quot; (rather than total) measure of the amount of object-based information at each stage, and it is likely that different stages have different numbers of neurons, 2) it is a measure of the total information about object-identity, and 3) information about object-identity cannot be created in feed-forward pathway. The issue at hand is that in the last paragraph of the subsection “The amount of view-invariant object information gradually increases from V1 to LL”, it implies that LL somehow has more total information for discriminating object identity than V1 (and LI) once luminance modulations are disregarded. This can only be true if 1) all the object-based information present in LL arrives there via a route that goes through V1 (and then LI) AND V1 (and LI) have more neurons than LL or 2) object-based information arrives in LL via some path that does not also pass through V1.</italic> </p><p>We thank the reviewers for pointing out the importance of being very clear about the fact that results such as those shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref> should not be interpreted as implying that the total information about object identity present in each individual area increases along the visual hierarchy. We agree with the reviewers that, to address this point, the best way is to stress that the measure reported in <xref ref-type="fig" rid="fig4">Figure 4C</xref> is a <italic>per neuron</italic> measure, and not a <italic>per area</italic> measure. And, as correctly suggested by the reviewers, information per neuron can increase along the processing hierarchy, provided that the number of neurons decreases from one area to the next or that the hierarchy is not strictly feedforward. As it happens, this is exactly the case of rat visual cortical areas, whose size undergoes a dramatic and progressive shrinkage from V1 to LL, and whose connectivity is far from being purely feed-forward. In summary, the results of <xref ref-type="fig" rid="fig4">Figure 4C</xref> do not violate the data processing inequality. Rather, they indicate that rat lateral visual areas, while progressively losing neurons, concentrate more information in individual units, as a part of the process of making object information more explicit and more easily readable by downstream neurons that only have access to a limited number of presynaptic units. In our revised manuscript, we have added a paragraph to the Discussion to extensively comment on this issue (see subsection “Validity and implications of our findings”, sixth paragraph). In addition, we have also specified very clearly the “per-neuron” nature of our metrics in the text and in the axis labels of the figures, whenever appropriate (see our revised <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig7">7</xref>).</p><p><italic>Similarly, the linear separability analyses presented later in the paper (e.g. <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig8">8</xref>) are interesting only after first equating total information for object identity at each stage by (e.g.) allowing V1 (and LI) to have more neurons than LL. Once that is done properly, the main result – that LL is better at object discrimination with a linear read-out – may still hold (e.g. as a back-of-the-envelope calculation, LL has ~2x more object information per neuron (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) but comparing 96 V1 neurons to 48 LL neurons reveals that LL is still better (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). But this issue should be carefully considered for all analyses throughout the paper.</italic> </p><p>We agree that it is interesting to check if LL is still superior to the other areas (in terms of population decoding), after having approximately equated the populations under comparison, based on the amount of view-invariant information coded by the individual neurons in each population. To address this issue, we have carried out the analysis suggested by the reviewers, i.e., we have compared an LL population of 48 units to V1, LM and LI populations of 96 units, showing that LL is, indeed, still better than the other areas. We have added a new supplementary figure (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>) to report this comparison and we have carefully explained the results of this new analysis and its motivation in the last paragraph of the subsection “Linear readout of population activity confirms the steep increase of transformation-tolerance along rat lateral visual areas”.</p><p>At the same time, we do not think that the population decoding results are interesting <italic>only</italic> after matching the total information across the areas. We believe that there is a value also in comparing populations of equal size, as done in most monkey ventral stream studies. In fact, the increase of invariant information per neuron along the areas’ progression is a property that any downstream neuron would necessarily exploit, while trying to read out object identity. Therefore, when comparing different areas in terms of their ability to support invariant recognition, it is important to take into account also differences among the invariant object information carried by individual neurons. This requires comparing the four areas at the population level by taking neuronal ensembles of the same size (as shown in <xref ref-type="fig" rid="fig8">Figure 8C-F</xref>). We thus decided to present both the same-size comparisons already present in the previous version of the manuscript and the new information-matched analysis suggested by the reviewers.</p><p><italic>3) The major results were generated by computing the mutual information and training a decoder. These are good analytical tools, but do not offer intuitive interpretations for a broad audience (e.g., bit values &lt;&lt; 1). To address this issue, it would be nice to provide some basic statistics about stimulus representation. From the example neuron in <xref ref-type="fig" rid="fig2">Figure 2</xref>, it seems the response of the LL neuron is sparser than the V1 neuron. Is this true at the population level?</italic> </p><p>We agree with the reviewers that the information and decoding metrics may be somewhat hard to digest for a broader audience. On the one hand, we have tried to illustrate theseanalyses as clearly as possible, with the aid of a few single cell examples (<xref ref-type="fig" rid="fig2">Figure 2</xref>) and several cartoons (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig5">Figure 5A</xref>, <xref ref-type="fig" rid="fig6">Figure 6A</xref> and <xref ref-type="fig" rid="fig8">Figure 8A</xref>). We have also added a new cartoon to <xref ref-type="fig" rid="fig3">Figure 3</xref> (see the new <xref ref-type="fig" rid="fig3">Figure 3A</xref>) to better explain the meaning of the mutual information analysis. We believe that these cartoons substantially help making our analyses and results intelligible. On the other hand, given that our manuscript is focused on neuronal coding of high-level object information, resorting to simpler (more intuitive) metrics or analyses would not be appropriate – it would not allow capturing the essence of the information rat visual areas encode, and could potentially lead to misleading conclusions.</p><p>A nice example of the limitation of more intuitive metrics is exactly the sparseness measure that the reviewers mentioned. It is true that, for the example neurons shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, the LL unit fires more sparsely than the V1 unit. However, at the population level, LL neurons did not fire more sparsely than V1 neurons, but this result should not be taken as an indication of V1 and LL processing the visual input in a similar way. In fact, sparseness is a metric that is affected by both object selectivity (which tends to make sparseness higher) and transformation tolerance (which tends to make sparseness lower). As shown by Rust and DiCarlo (2012), although these properties both increase across the monkey ventral stream (i.e., from V4 to IT), sparseness remains unchanged. Therefore, sparseness cannot be taken as a very informative measure to compare visual object representations along a putative ventral- like pathway. We have added a paragraph to our revised manuscript, where we explain this and we report the sparseness comparison between V1 and LL at the population level (see subsection “The fraction of energy-independent stimulus information sharply increases from V1 to LL”, third paragraph).</p><p><italic>Since the information and decoding analysis indicate that LL is transformation tolerant, can the authors infer what neurons might be &quot;encoding&quot;/&quot;tuned for&quot;/&quot;computing&quot;?</italic> </p><p>We agree with the reviewers that understanding what combination of visual features any given neuron is tuned to is a very interesting topic. In primate studies, this endeavor has a very long tradition, starting from the reduction method of Tanaka and colleagues till the more refined and quantitative efforts of Connor’s group. All these studies rely on a very specific design, selection and parameterization of the stimuli (Connor), and make use of model fitting procedures (Connor) that can only be applied to data recorded using large, parametric shape spaces. Our stimulus set was not designed for applying such approaches and, as a result, a parametric analysis of what visual features each of our neurons was tuned to is not possible. The reason is that, by design, our work belongs to a different family of studies, whose goal is to measure the ability of single neurons and neuronal populations to code invariantly object identity, and not to infer shape tuning itself. Nevertheless, our study does provide some very careful quantification of the extent to which two low-level visual properties are coded across rat lateral visual areas: stimulus luminance and, in our revised manuscript, also stimulus contrast (which was computed to address the reviewers’ essential revision #5; see the new <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>).</p><p><italic>Did the authors see clues in firing rate variation over different transformations? A large transformation space was used, but we did not learn much about the representation of these transformations. <xref ref-type="fig" rid="fig4">Figure 4B and C</xref> imply that information about transformations decreased along the ventral stream. How was the transformation-tolerance generated along the ventral stream? Was the information about transformation along different axes reduced simultaneously or sequentially?</italic> </p><p>In our manuscript, we have carried out most of our analyses by including all the transformations at once (along all variation axes). The motivation was to probe invariance under the most challenging conditions, to allow the differences among the visual areas to emerge more clearly. However, we agree with the reviewers that presenting the transformation-tolerance attained by the various areas with regard to specific axes is also important. This is why, in <xref ref-type="fig" rid="fig6">Figure 6D</xref> we have presented the results of the most important decoding analysis (where generalization across transformation was tested) across all four variation axis: position, size, luminosity and rotation. We believe that this figure already answers many of the questions of the reviewers. In fact, it can be seen how LL is always the area attaining the largest transformation tolerance, typically followed by LI and then by LM and V1. In other words, the same growth of invariance observed when all transformations are considered together (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), is visible also for the individual variation axes (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). This means that tolerance was built up gradually along the V1, LI, LL pathway and “simultaneously” along all tested transformation axes. We apologize if this analysis – already present in the first submission – might have gone unnoticed by the reviewers, as it was described very briefly. In our current revision, we have expanded this section and better emphasized this result (see subsection “Object representations become more linearly separable from V1 to LL, and better capable of supporting generalization to novel object views”, fourth paragraph).</p><p>In addition, we have carried out two new analyses, in which we have measured position tolerance and size tolerance in the four areas across parametric changes. These new analyses confirmed the superior tolerance achieved by LL, thus fully addressing, we believe, the reviewers’ concern (these new analyses are shown in the new <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> and are described in the fifth paragraph of the aforementioned subsection, with a detailed explanation provided in the Methods subsection “Test of generalization along parametric position and size changes”).</p><p><italic>Neuronal responses from four visual areas along the ventral pathway was collected. Other than V1 and LL, the author did not assign potential functions to LM and LI. Do they serve roles for transformation representations along particular dimensions?</italic> </p><p>In all our analyses, we have always compared all four areas. In our discussion of the results, we often focused on LL, and its difference from V1, partly because these two areas are at the extremes of the anatomical progression, and partly (and more importantly) because our data show that they also behave as opposite poles of the functional hierarchy. However, we have not overlooked the implications of our findings with regard to LM and LI. Most of our analyses show that LI behaves as an intermediate processing stage in the functional hierarchy (for instance, this is clearly stated in the conclusions we draw at the beginning of the Discussion (third paragraph), yielding view-invariant information and decoding performances that are significantly larger than V1 and LM (<xref ref-type="fig" rid="fig4">Figures 4</xref>–<xref ref-type="fig" rid="fig8">8</xref>), although often significantly lower than LL (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig8">8</xref>). So, we did assign a specific function to LI as an intermediate object processing stage, between V1 and LL. With regard to LM, we have commented extensively in the Discussion that its functional properties (in terms of invariant coding) do make it indistinguishable from V1. We have also made reference to the anatomical and functional literature of the mouse visual system, showing that this area has mixed ventral-like and dorsal-like properties, thus suggesting that LM can be homologous to area V2 in primates (which belongs to both pathways). We believe that our data support this interpretation (this is clearly stated in the third paragraph of the Discussion).</p><p><italic>4) One of the careful steps of the analysis was to control for luminance. The total information about the object stimuli declined along the ventral stream. It was when discriminating stimuli matched for RF and luminance, view-invariant information and linear discrimination accuracy increased along the ventral stream. It is understandable that at single neuron level the information about the stimuli is largely affected by the relative location of RFs and stimulus. But at the population level, it is not clear why it is necessary to limit the analysis to luminance-matched pairs. At the population level, each individual stimulus would be represented at various RF/luminance levels in different neurons. Thus, if the representation space was invariant to transformation, luminance control might not be needed. Can the authors obtain similar results at the population level without controlling so carefully for luminance?</italic> </p><p>We thank the reviewers for this comment, which has helped us clarifying why applying the constraint over the luminance of the objects to be decoded is essential also at the population level.The fact is that, in general, a neuronal population will inherit its discrimination power from its constituent neurons. And this applies also to the coding of luminance. If luminance differences can be used as a cue, by single neurons, to solve the invariant task, because of the sharp tuning of single neurons for luminance, then the neuronal population will retain this ability. To show that this was the case, we have run the population analyses of <xref ref-type="fig" rid="fig8">Figure 8</xref> for the full spectrum of Th_LumRatio. The resulting performance curves show how the performance itself is extremely sensitive to the difference of luminance that the objects are allowed to have, displaying a trend that is similar to what already reported for single neurons in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. Hence, the need to control for the luminance confound also in the population analysis. We have now better explained this in the fifth paragraph of the subsection “Linear readout of population activity confirms the steep increase of transformation-tolerance along rat lateral visual areas” and we have reported the result of this new analysis in the new <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>.</p><p><italic>5) The authors do a suitable job of controlling for overall luminance across the different object conditions, however neurons in V1 are likely not only responding to contrast defined by the object edge which is effectively what is being quantified here, but also the contrast on the object surface. It seems at first glance to be similar across the objects, but it would be nice to see a quantification of the contrast present on the object surfaces as this could also be driving firing rate differences across the object conditions.</italic> </p><p>We thank the reviewers for this very useful suggestion. We have carried the analysis they suggested. To this aim, we have defined a new metric, which we called RF contrast, that quantifies the variation of luminance produced by any given stimulus within the RF of a neuron. We have then applied the same analysis based on mutual information (i.e., same analysis of eq. 2) to quantify how much information neurons in the four areas carried about RF contrast and the complementary information that was not about RF contrast. The results of this new analysis are shown in a new supplementary figure (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>) and are described in the last paragraph of the subsection “The fraction of energy-independent stimulus information sharply increases from V1 to LL” (see also the Methods subsection “Computation of the Receptive Field (RF) luminance and RF contrast”, last paragraph). We have found, for stimulus contrast, a trend that was very similar to the one found for stimulus luminance, with the information about RF contrast decreasing monotonically along the areas progression. As a consequence, the fraction of contrast-independent information increased monotonically and significantly from V1 to LL. Overall, this new analysis confirms the pruning of low-level information that takes place along rat lateral visual areas.</p><p><italic>6) To motivate luminance-matched analysis, the author showed that the information of luminance was reducing from V1 to LL. Is LL luminance insensitive to the same object? I.e., firing in LL should change less than in other areas to transformation along the luminance axis. This can be directly tested using the existing data set.</italic> </p><p>The intuition of the reviewers is correct. To show this, we have performed a new analysis, in which we report the tuning to luminance variation of the same object for the neurons in the four visual areas. This tuning is sharper in the more medial areas (V1 and LM) than in the more lateral ones (LI and LL). We have quantified this trend by computing the sparseness of the luminance tuning curves, and we have found that sparseness decreases progressively and monotonically from V1 to LL. The results of this new analysis are shown in a new supplementary figure (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>) and are described in the eighth paragraph of the subsection “The fraction of energy-independent stimulus information sharply increases from V1 128 to LL”.</p><p><italic>7) The calculation of RF luminance is affected by the shape of the stimulus and RF. Was the increase of f_high purely a result of larger RFs in LL?</italic> </p><p>We have controlled for this, by running the analysis shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>, where we have taken subpopulations in all 4 areas with matched RF sizes. <xref ref-type="fig" rid="fig7">Figure 7D</xref> shows that the growth of f_high is as strong as for the full populations. So, the increase of f_high does not depend on RF size.</p><p><italic>RFs in higher visual areas cannot be modeled simply by a single Gaussian profile. And RF shape can be affected by the stimuli (even in V1, e.g., Yeh, Xing, et al. 2009 PNAS). Is the calculation of RF luminance robust enough? Can the uncertainty affect the information and decoding analysis?</italic> </p><p>This is a very interesting topic, which definitely deserves some in-depth discussion. We have now added a full paragraph to deal with this issue (subsection “Validity and implications of our findings”, second paragraph).</p><p>Briefly, with regard to the dependence of the RFs on the type of stimuli used to map them, in our experiments, we used very simple stimuli (high contrast drifting bars), with the goal of measuring the luminance-driven component of the neuronal response, so as to estimate what portion of the visual field each neuron was sensitive to. This is a very simple but effective procedure, because all the recorded neurons retained some amount of sensitivity to luminance, even in LL (see <xref ref-type="fig" rid="fig3">Figure 3A</xref>). As a result, it was possible to obtain very clean RF maps in all the areas (as shown by the examples in <xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>With regard to the accuracy of fitting RF maps with 2-d Gaussians, we agree that, in general, such fits may not be appropriate for all visual neurons, but we do not believe that this procedure cannot be applied in higher-level areas. In fact, fitting 2-d Gaussians is commonly accepted as a valid procedure to get a first approximation of the RF extension/shape, even in high-level visual areas, such as monkey IT (e.g., see Op De Beeck and Vogels, 2000; Brincat and Connor, 2004; Rust and DiCarlo, 2010). In any event, in our study, we took two measures to avoid possible mistakes/biases, produced by inaccurate Gaussian fits. In the RF size analysis (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), we only included data from neurons whose RFs were very well fitted by Gaussians. More importantly, for the computation of the RF luminance, we did not use the fitted RFs, but we directly used the raw RF maps. This allowed weighting the luminance of the stimulus images using the real shapes of the RFs, and allowed computing the RF luminance for all the neurons, also those with RFs that were not well fitted by Gaussians.</p><p><italic>8) In <xref ref-type="fig" rid="fig4">Figure 4C</xref>, we can see that V1, LM and LI actually have higher view-invariant object information when the luminance ratio is &lt; ~0.5. Although it is arguable that the object pair may be discriminated by the difference in luminance, it is possible that luminance difference across transformation of the same object is larger than the luminance difference between the object pair. The later suggests transformation-tolerant object representation in these areas. Please explain/clarify.</italic> </p><p>To address the concern of the reviewer, we have carried out a new analysis, whose result is reported in a new supplementary figure (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). In this analysis, we have computed the information that RF luminance is able to convey about object identity in spite of transformations: I(L;O). This information is very large in all the areas, unless the objects in the pairs are forced to have similar luminance (by requiring the luminance ratio to approach 1), in which case I(L;O) drops sharply to zero. The crucial fact is that, for V1, LM and LI, the drop of I(L;O) as a function of the luminance ratio follows the same exact trend as the drop of the invariant object information I(R;O) shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. This implies that, in these areas, a large fraction of the invariant information is indeed accounted for by the luminance differences of the object pairs. In turn, this proves the need of restricting the computation of I(R;O) to the largest possible value of the luminance ratio. Our new analysis shows that setting the constraint on the luminance ratio to 0.9 effectively get rid of the luminance confound in all the areas, thus guaranteeing a fair comparison. This new analysis is described in the sixth paragraph of the subsection “The amount of view-invariant object information gradually increases from V1 to LL”.</p><p><italic>9) The authors emphasize the similarities between the hierarchical organization of macaque and rat ventral temporal cortex, in that greater linear separability of different object classes, invariant to view, is seen in higher areas. However, I don't see any direct quantification of performance between the two species. The decoding performance in the rat seems significantly worse than that in monkey ITC (60% on a binary choice in the generalization task, where macaque ITC decoding performance reaches human levels). Please comment on how the values of discriminability and generalizability presented in <xref ref-type="fig" rid="fig7">Figure 7</xref> compare to such analysis in the primate ventral stream.</italic> </p><p>We thank the reviewer for this suggestion. Although a direct quantitative comparison of decoding performances between monkeys and rats would be of great interest, we feel that a quantitative comparison between published experiments in primates and our new experiment in rodents would be difficult to interpret, because of the large differences in presentation protocols and combinations of object identity and transformations across these studies. In fact, even comparing object representations in different areas of the same species (the macaque) has required recording those areas (e.g., V4 and IT) under the same exact experimental conditions (e.g., see Rust and DiCarlo, 2010). In the case of our study, the reviewers point to the fact that a 60% generalization performance is not very high, but this performance was achieved in a test of generalization from a single object view to 19 other different views, spread across four different translation axes, while most monkey IT studies have tested generalization to single steps along individual transformation axes.</p><p>However, thanks to the reviewers’ comment, we have realized that, in our previous version of the manuscript, it was not sufficiently clear that one of our primary goals was achieving a qualitative comparison among rats and monkeys in terms of the existence of a progression of visual cortical representations to support invariant recognition. These qualitative comparisons are still possible, and lend themselves to useful interpretations, even in the face of the differences in the experimental protocols applied so far across species. We have now amended the text, adding a new section to the Discussion to clearly explain this (second paragraph).</p></body></sub-article></article>