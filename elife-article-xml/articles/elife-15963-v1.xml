<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="article-commentary" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">15963</article-id><article-id pub-id-type="doi">10.7554/eLife.15963</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Prediction error</subject></subj-group><subj-group subj-group-type="display-channel"><subject>Insight</subject></subj-group></article-categories><title-group><article-title>The expanding role of dopamine</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-50374"><name><surname>Doll</surname><given-names>Bradley B</given-names></name><x>is in the</x><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-41719"><name><surname>Daw</surname><given-names>Nathaniel D</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5029-1430</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/><x>is at the</x></contrib><aff id="aff1"><label>1</label><institution>Center for Neural Science, New York University</institution>, <addr-line><named-content content-type="city">New York</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychology and Princeton Neuroscience Institute</institution>, <institution>Princeton University</institution>, <addr-line><named-content content-type="city">Princeton</named-content></addr-line>, <country>United States</country><email>ndaw@princeton.edu</email></aff></contrib-group><pub-date date-type="pub" publication-format="electronic"><day>21</day><month>04</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e15963</elocation-id><history><date date-type="received"><day>12</day><month>04</month><year>2016</year></date><date date-type="accepted"><day>12</day><month>04</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Doll et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Doll et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-15963-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="commentary-article" xlink:href="10.7554/eLife.13665"/><abstract><p>Evidence increasingly suggests that dopaminergic neurons play a more sophisticated role in predicting rewards than previously thought.</p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>dopamine</kwd><kwd>prediction error</kwd><kwd>single unit</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Rat</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta></custom-meta-group></article-meta></front><body><boxed-text><p><bold>Related research article</bold> Sadacca BF, Jones JL, Schoenbaum G. 2016. Midbrain dopamine neurons compute inferred and cached value prediction errors in a common framework. <italic>eLife</italic> <bold>5</bold>:e13665. doi: <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.7554/eLife.13665">10.7554/eLife.13665</ext-link></p><p><bold>Image</bold> Rats can learn to associate a clicker with a reward without experiencing the two at the same time</p><p><inline-graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-15963-inf1-v1"/></p></boxed-text><p>As any kindergarten instructor will tell you, reward is one of the most powerful teachers. Some of the earliest and most iconic examples of behavioral psychology concern how animals learn, from experience, which stimuli or actions accompany reward (<xref ref-type="bibr" rid="bib10">Thorndike, 1898</xref>; <xref ref-type="bibr" rid="bib7">Pavlov, 1927</xref>). A century later, computational neuroscientists have described neural circuits that underpin such learning. These are based on the mutual interactions between neurons that contain the neuromodulator dopamine and other neurons they connect with, particularly those in a brain region called the striatum. The dopaminergic neurons receive information about predicted rewards, and report back the mismatch between those expectations and the rewards actually received. These “reward prediction errors,” in turn, allow the predictions to be updated, a computation known as model-free learning.</p><p>The problem with this well-studied framework is that humans and rodents can learn about rewards in many ways other than by direct experience (<xref ref-type="bibr" rid="bib11">Tolman, 1948</xref>). Computationally, these capabilities have been understood in terms of “model-based” learning methods, which draw on knowledge of task structure to anticipate possible rewards that have never been directly experienced. Due in part to the support for a tidy, closed-loop picture of dopamine’s involvement in reward prediction, researchers have tended to assume that such capabilities arise from some separate, more sophisticated brain system. Now, in eLife, Brian Sadacca, Joshua Jones and Geoffrey Schoenbaum indicate that these more sophisticated learning capabilities instead arise within – or at least impinge upon – the dopaminergic learning circuit itself (<xref ref-type="bibr" rid="bib8">Sadacca et al., 2016</xref>).</p><p>Sadacca et al. – who are based at the National Institute on Drug Abuse, University of Maryland School of Medicine and Johns Hopkins School of Medicine – recorded spiking from dopaminergic neurons while rats performed a task designed to defeat simple model-free learning. The task, called sensory preconditioning, assesses the rats’ ability to associate a stimulus (for example, a clicker) with a reward without ever experiencing the two together (<xref ref-type="fig" rid="fig1">Figure 1</xref>). To do so, a rat needs to integrate experiences from two separate training phases. First, in a “pre-conditioning” phase, the clicker was paired with another neutral stimulus (for example, a tone); then, in a “conditioning” phase, the tone (but not the clicker) was paired with a reward. Not only was the clicker never paired with the reward, it was not even paired with a reward-predicting stimulus, since the tone’s relationship with reward was established at a later stage.<fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The activity of dopaminergic neurons helps rats to integrate separate experiences to predict when a reward will be given.</title><p>(<bold>A</bold>) Schematic of the task used by Sadacca et al. In the pre-conditioning phase, rats learn to associate a clicker with a tone. In a subsequent conditioning phase, the rats learn to link the tone with a food reward. In the final test phase, the rats hear the clicker, and behave as if they expect a reward. (<bold>B</bold>) Three potential associative retrieval mechanisms that might support integrative inference about the stimulus. Left: during the conditioning phase, presenting the tone could call the clicker to mind, allowing both stimuli to be linked to a reward. Middle: after conditioning, the mental replay of experiences may permit the relationships between separate sets of stimuli to be learned. Right: in the test phase, the rats may make new inferences that cause the rats to expect a reward when they hear the clicker.</p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-15963-fig1-v1"/></fig></p><p>Nevertheless, in a test phase, rats demonstrated (by visiting a food cup where a reward had previously been delivered) that they associated the clicker with reward. This capability is well known; more surprising was that these reward predictions could also be seen in the responses of dopaminergic neurons to the clicker. This result complicates dual-system explanations, which state that model-based inference occurs separately from the dopaminergic reward learning system. It also speaks against the traditional closed-loop account of dopaminergic learning, in which the circuit should only know about reward predictions it has taught itself via direct pairings.</p><p>This result adds to a series of studies suggesting that responses in dopaminergic neurons and associated areas report more sophisticated reward predictions than theory suggests (<xref ref-type="bibr" rid="bib1">Bromberg-Martin et al., 2010</xref>; <xref ref-type="bibr" rid="bib2">Daw et al., 2011</xref>). Key challenges going forward are to understand how this information gets into the circuit, and what neural computations produce the information in the first place.</p><p>These questions might have the same answer if at least some of an animal’s sophisticated learning capacities actually build upon a dopaminergic foundation. This perspective is supported by work in humans that suggests that the dopamine system also helps to produce similar integrative inferences about rewards (<xref ref-type="bibr" rid="bib3">Deserno et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Sharp et al., 2015</xref>; <xref ref-type="bibr" rid="bib5">Doll et al., 2016</xref>). There are several (not mutually exclusive) possibilities for how dopaminergic learning might contribute to these integrative predictions.</p><p>One possibility is that inferences made during the test phase cause the rats to expect a reward when they hear the clicker. Model-based learning theories envision that the brain retrieves successor stimuli (here, the clicker would evoke the tone) as a sort of mental simulation that helps to predict reward. Evidence of such retrieval has been demonstrated using fMRI in humans (<xref ref-type="bibr" rid="bib4">Doll et al., 2015</xref>). Though this mechanism need not involve dopamine, it could: the usual dopaminergic learning circuit could map the evoked representations to a reward.</p><p>The association of the clicker with the reward could also have already been made earlier in the experiment. Though Sadacca et al. believe this is unlikely in their study, two broadly applicable mechanisms for this process have been suggested. One possibility, supported by a human fMRI study of a similar sensory preconditioning task (<xref ref-type="bibr" rid="bib12">Wimmer and Shohamy, 2012</xref>), is that associations between the clicker and reward could already form in the conditioning phase (during which the rats learn to associate a tone with a reward). If presenting the tone called to mind the clicker, which preceded the tone in the first training phase, dopaminergic learning could associate the reward that followed with both stimuli.</p><p>Mentally rehearsing clicker-tone and tone-reward sequences after the conditioning phase (but before testing) could also allow the brain to learn the relationship between the clicker and reward. Replay of previously experienced events has been observed in neural recordings during sleep and quiet rest. If the brain treated such mock experiences like real ones – allowing them to drive dopaminergic responding and learning – this too could drive the integrative association (<xref ref-type="bibr" rid="bib6">Gershman et al., 2014</xref>).</p><p>In all, the venerable framework of dopaminergic reward learning may have more explanatory power than originally thought, as the results of Sadacca et al. suggest that it might point toward explanations even for cases that were thought to challenge it.</p></body><back><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bromberg-Martin</surname><given-names>ES</given-names></name><name><surname>Matsumoto</surname><given-names>M</given-names></name><name><surname>Hong</surname><given-names>S</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A pallidus-habenula-dopamine pathway signals inferred stimulus values</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>1068</fpage><lpage>1076</lpage><pub-id pub-id-type="doi">10.1152/jn.00158.2010</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Model-based influences on humans' choices and striatal prediction errors</article-title><source>Neuron</source><volume>69</volume><fpage>1204</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deserno</surname><given-names>L</given-names></name><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Boehme</surname><given-names>R</given-names></name><name><surname>Buchert</surname><given-names>R</given-names></name><name><surname>Heinze</surname><given-names>H-J</given-names></name><name><surname>Grace</surname><given-names>AA</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Heinz</surname><given-names>A</given-names></name><name><surname>Schlagenhauf</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Ventral striatal dopamine reflects behavioral and neural signatures of model-based control during sequential decision making</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>112</volume><fpage>1595</fpage><lpage>1600</lpage><pub-id pub-id-type="doi">10.1073/pnas.1417219112</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doll</surname><given-names>BB</given-names></name><name><surname>Duncan</surname><given-names>KD</given-names></name><name><surname>Simon</surname><given-names>DA</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Model-based choices involve prospective neural activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>767</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1038/nn.3981</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doll</surname><given-names>BB</given-names></name><name><surname>Bath</surname><given-names>KG</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Variability in dopamine genes dissociates model-based and model-free reinforcement learning</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>1211</fpage><lpage>1222</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1901-15.2016</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Markman</surname><given-names>AB</given-names></name><name><surname>Otto</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Retrospective revaluation in sequential decision making: A tale of two systems</article-title><source>Journal of Experimental Psychology. General</source><volume>143</volume><fpage>182</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1037/a0030844</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pavlov</surname><given-names>IP</given-names></name></person-group><year iso-8601-date="1927">1927</year><source>Conditioned Reflexes</source><publisher-loc>New York</publisher-loc><publisher-name>Limelight</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadacca</surname><given-names>BF</given-names></name><name><surname>Jones</surname><given-names>JL</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Midbrain dopamine neurons compute inferred and cached value prediction errors in a common framework</article-title><source>eLife</source><volume>5</volume><elocation-id>e13665</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.13665</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharp</surname><given-names>ME</given-names></name><name><surname>Foerde</surname><given-names>K</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dopamine selectively remediates 'model-based' reward learning: A computational approach</article-title><source>Brain</source><volume>139</volume><fpage>355</fpage><lpage>364</lpage><pub-id pub-id-type="doi">10.1093/brain/awv347</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorndike</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="1898">1898</year><article-title>Animal intelligence: An experimental study of the associative processes in animals</article-title><source>The Psychological Review: Monograph Supplements</source><volume>2</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1037/10780-000</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>Cognitive maps in rats and men</article-title><source>Psychological Review</source><volume>55</volume><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1037/h0061626</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>GE</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Preference by association: How memory mechanisms in the hippocampus bias decisions</article-title><source>Science</source><volume>338</volume><fpage>270</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1126/science.1223252</pub-id></element-citation></ref></ref-list></back></article>