<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">31557</article-id><article-id pub-id-type="doi">10.7554/eLife.31557</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Sensory cortex is optimised for prediction of future input</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-98202"><name><surname>Singer</surname><given-names>Yosef</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-68508"><name><surname>Teramoto</surname><given-names>Yayoi</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3419-0351</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-38220"><name><surname>Willmore</surname><given-names>Ben DB</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-6"/><xref ref-type="other" rid="par-7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-14601"><name><surname>King</surname><given-names>Andrew J</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5180-7179</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-8"/><xref ref-type="other" rid="par-9"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-98209"><name><surname>Schnupp</surname><given-names>Jan W H</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-19854"><name><surname>Harper</surname><given-names>Nicol S</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7851-4840</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-10"/><xref ref-type="other" rid="par-11"/><xref ref-type="other" rid="par-12"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Physiology, Anatomy and Genetics</institution>, <institution>University of Oxford</institution>, <addr-line><named-content content-type="city">Oxford</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff2"><institution content-type="dept">Department of Biomedical Sciences</institution>, <institution>City University of Hong Kong</institution>, <addr-line><named-content content-type="city">Hong Kong</named-content></addr-line>, <country>Hong Kong</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-12394"><name><surname>Gallant</surname><given-names>Jack L</given-names></name><role>Reviewing editor</role><aff><institution>University of California, Berkeley</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>nicol.harper@dpag.ox.ac.uk</email> (NH);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>18</day><month>06</month><year>2018</year></pub-date><volume>7</volume><elocation-id>e31557</elocation-id><history><date date-type="received"><day>25</day><month>08</month><year>2017</year></date><date date-type="accepted"><day>16</day><month>06</month><year>2018</year></date></history><permissions><copyright-statement>Â© 2018, Singer et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Singer et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-31557-v2.pdf"/><abstract><p>Neurons in sensory cortex are tuned to diverse features in natural scenes. But what determines which features neurons become selective to? Here we explore the idea that neuronal selectivity is optimised to represent features in the recent sensory past that best predict immediate future inputs. We tested this hypothesis using simple feedforward neural networks, which were trained to predict the next few video or audio frames in clips of natural scenes. The networks developed receptive fields that closely matched those of real cortical neurons in different mammalian species, including the oriented spatial tuning of primary visual cortex, the frequency selectivity of primary auditory cortex and, most notably, their temporal tuning properties. Furthermore, the better a network predicted future inputs the more closely its receptive fields resembled those in the brain. This suggests that sensory processing is optimised to extract those features with the most capacity to predict future input.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Clarendon Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Singer</surname><given-names>Yosef</given-names></name><name><surname>Teramoto</surname><given-names>Yayoi</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000769</institution-id><institution>University Of Oxford</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000703</institution-id><institution>Action on Hearing Loss</institution></institution-wrap></funding-source><award-id>PA07</award-id><principal-award-recipient><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/H008608/1</award-id><principal-award-recipient><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT10525/Z/14/Z</award-id><principal-award-recipient><name><surname>Teramoto</surname><given-names>Yayoi</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT076508AIA</award-id><principal-award-recipient><name><surname>Willmore</surname><given-names>Ben DB</given-names></name></principal-award-recipient></award-group><award-group id="par-7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT108369/Z/2015/Z</award-id><principal-award-recipient><name><surname>Willmore</surname><given-names>Ben DB</given-names></name></principal-award-recipient></award-group><award-group id="par-8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT076508AIA</award-id><principal-award-recipient><name><surname>King</surname><given-names>Andrew J</given-names></name></principal-award-recipient></award-group><award-group id="par-9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT108369/Z/2015/Z</award-id><principal-award-recipient><name><surname>King</surname><given-names>Andrew J</given-names></name></principal-award-recipient></award-group><award-group id="par-10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT082692</award-id><principal-award-recipient><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="par-11"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT076508AIA</award-id><principal-award-recipient><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="par-12"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT108369/Z/2015/Z</award-id><principal-award-recipient><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf2"><p>Andrew J King, Senior Editor, <italic>eLife</italic>.</p></fn><fn fn-type="conflict" id="conf1"><p>The other authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: Auditory RFs of neurons were recorded in the primary auditory cortex (A1) and anterior auditory field (AAF) of 5 pigmented ferrets of both sexes (all &gt; 6 months of age) and used as a basis for comparison with the RFs of model units trained on auditory stimuli. These recordings were performed under license from the UK Home Office and were approved by the University of Oxford Committee on Animal Care and Ethical Review. Full details of the recording methods are described in earlier studies [45,90]. Briefly, we induced general anaesthesia with a single intramuscular dose of medetomidine (0.022 mg Â· kgâ1 Â· hâ1) and ketamine (5 mg Â· kgâ1 Â· hâ1), which was then maintained with a continuous intravenous infusion of medetomidine and ketamine in saline. Oxygen was supplemented with a ventilator, and we monitored vital signs (body temperature, end-tidal CO2, and the electrocardiogram) throughout the experiment. The temporal muscles were retracted, a head holder was secured to the skull surface, and a craniotomy and a durotomy were made over the auditory cortex. Extracellular recordings were made using silicon probe electrodes (Neuronexus Technologies) and acoustic stimuli were presented via Panasonic RPHV27 earphones, which were coupled to otoscope specula that were inserted into each ear canal, and driven by Tucker-Davis Technologies System III hardware (48 kHz sample rate).</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>All custom code used in this study was implemented in MATLAB and Python. We have uploaded the code to a public Github repository. The raw auditory experimental data is available atÂ Â https://osf.io/ayw2p/. The movies and sounds used for training the models are all publicly available at the websites detailed in the Methods.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Jan Schnupp</collab></person-group><year iso-8601-date="2016">2016</year><source>NetworkReceptiveFields</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/ayw2p/">https://osf.io/ayw2p/</ext-link><comment>Available at the Open Science Framework</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-31557-supp-v2.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>