<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">45645</article-id><article-id pub-id-type="doi">10.7554/eLife.45645</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Monkey EEG links neuronal color and motion information across species and scales</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-130752"><name><surname>Sandhaeger</surname><given-names>Florian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9633-9556</contrib-id><email>florian.sandhaeger@uni-tuebingen.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-131550"><name><surname>von Nicolai</surname><given-names>Constantin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8928-5860</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-29191"><name><surname>Miller</surname><given-names>Earl K</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-23659"><name><surname>Siegel</surname><given-names>Markus</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5115-936X</contrib-id><email>markus.siegel@uni-tuebingen.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Centre for Integrative Neuroscience</institution><institution>University of Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Hertie Institute for Clinical Brain Research</institution><institution>University of Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">MEG Center</institution><institution>University of Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">IMPRS for Cognitive and Systems Neuroscience</institution><institution>University of Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">The Picower Institute for Learning and Memory and Department of Brain and Cognitive Sciences</institution><institution>Massachusetts Institute of Technology</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Pasternak</surname><given-names>Tatiana</given-names></name><role>Reviewing Editor</role><aff><institution>University of Rochester</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>09</day><month>07</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e45645</elocation-id><history><date date-type="received" iso-8601-date="2019-01-30"><day>30</day><month>01</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-06-15"><day>15</day><month>06</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Sandhaeger et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Sandhaeger et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-45645-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.45645.001</object-id><p>It remains challenging to relate EEG and MEG to underlying circuit processes and comparable experiments on both spatial scales are rare. To close this gap between invasive and non-invasive electrophysiology we developed and recorded human-comparable EEG in macaque monkeys during visual stimulation with colored dynamic random dot patterns. Furthermore, we performed simultaneous microelectrode recordings from 6 areas of macaque cortex and human MEG. Motion direction and color information were accessible in all signals. Tuning of the non-invasive signals was similar to V4 and IT, but not to dorsal and frontal areas. Thus, MEG and EEG were dominated by early visual and ventral stream sources. Source level analysis revealed corresponding information and latency gradients across cortex. We show how information-based methods and monkey EEG can identify analogous properties of visual processing in signals spanning spatial scales from single units to MEG – a valuable framework for relating human and animal studies.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.45645.002</object-id><title>eLife digest</title><p>Neurons carry information in the form of electrical signals, which we can listen to by applying sensors to the scalp: the resulting recordings are called an EEG. Electrical activity within the brain also generates a weak magnetic field above the scalp, which can be measured using a technique known as MEG. Both EEG and MEG only require a few dozen sensors, placed centimeters away from the brain itself, but they can reveal the precise timing and rough location of changes in neural activity.</p><p>However, the brain consists of billions of neurons interconnected to form complex circuits, and EEG or MEG cannot reveal changes in activity of these networks in fine detail. In animals, and in patients undergoing brain surgery, scientists can use hair-thin microelectrodes to directly record the activity of individual neurons. Yet, it is difficult to know how activity measured inside the brain relates to that measured outside.</p><p>To find out, Sandhaeger et al. had monkeys and healthy human volunteers perform the same task, where they had to watch a series of colored dots moving across a screen. The brain of the human participants was monitored using MEG; in the monkeys, EEG provided an indirect measure of brain activity, while microelectrodes directly revealed the activity of thousands of individual neurons.</p><p>All three recordings contained information about movement and color. Moreover, the monkey EEG bridged the gap between direct and indirect recordings. Sandhaeger et al. identified signals in the monkey EEG that corresponded to the microelectrode recordings. They also spotted signals in the human MEG that matched the monkey EEG. Linking non-invasive measures of brain activity with underlying neural circuits could help to better understand the human brain. This approach may also allow clinicians to interpret EEG and MEG recordings in patients with brain disorders more easily.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>monkey EEG</kwd><kwd>human MEG</kwd><kwd>electrophysiology</kwd><kwd>color vision</kwd><kwd>motion perception</kwd><kwd>decoding</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R37MH087027</award-id><principal-award-recipient><name><surname>Miller</surname><given-names>Earl K</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>StG335880</award-id><principal-award-recipient><name><surname>Siegel</surname><given-names>Markus</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>276693517 (SFB 1233)</award-id><principal-award-recipient><name><surname>Siegel</surname><given-names>Markus</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SI1332-3/1</award-id><principal-award-recipient><name><surname>Siegel</surname><given-names>Markus</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Centre for Integrative Neuroscience</institution></institution-wrap></funding-source><award-id>EXC 307 (DFG)</award-id><principal-award-recipient><name><surname>Siegel</surname><given-names>Markus</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Using Monkey EEG as a bridge technology, color and motion representations in human MEG are linked to microcircuit activity in ventral stream areas.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>How do results from human magnetoencephalography (MEG) and electroencephalography (EEG) experiments relate to those obtained from animals in invasive electrophysiology? It is generally well understood how potential changes in large populations of neurons can propagate through tissue types and lead to detectable electric potentials and associated magnetic fields outside the head (<xref ref-type="bibr" rid="bib37">Pesaran et al., 2018</xref>). Yet, in typical MEG and EEG experiments, we have little clue which specific cellular and circuit mechanisms contribute to the recorded signals (<xref ref-type="bibr" rid="bib15">Cohen, 2017</xref>).</p><p>This can be attributed to several factors. First, the reconstruction of cortical sources from non-invasive signals is generally limited and based on assumptions (<xref ref-type="bibr" rid="bib17">Darvas et al., 2004</xref>). Second, invasive and non-invasive electrophysiology are largely separate research fields. Comparable experiments performed on both levels and in the same species are rare, with few recent exceptions (<xref ref-type="bibr" rid="bib6">Bimbi et al., 2018</xref>; <xref ref-type="bibr" rid="bib22">Godlove et al., 2011</xref>; <xref ref-type="bibr" rid="bib38">Reinhart et al., 2012</xref>; <xref ref-type="bibr" rid="bib43">Shin et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Snyder et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Snyder et al., 2018</xref>). Third, studies employing invasive and non-invasive methods in parallel suffer from sparse sampling of recording sites. Massively parallel invasive recordings in multiple brain regions have only recently become viable (<xref ref-type="bibr" rid="bib19">Dotson et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Siegel et al., 2015</xref>), and EEG recordings in awake behaving animals have so far been limited to relatively few electrodes. This sparsity limits specificity when drawing conclusions from one level to the other. In summary, the mapping between measurement scales is severely underconstrained, both theoretically when trying to infer cortical sources of non-invasively measured activity, and experimentally by the lack of sufficiently comparable data.</p><p>Thus, key for linking different scales are comparable large-scale recordings on all levels to provide high specificity and eventually trace the origins of large-scale phenomena back to their underlying cellular mechanisms. Importantly, this includes non-invasive recordings in animals. These allow to bridge the gap between invasive animal electrophysiology and non-invasive human experiments by permitting to disentangle similarities and differences due to species membership from those due to measurement technique. An especially suitable candidate for this is monkey EEG, making use of evolutionary proximity and promising to better connect the rich literature in non-human primate neurophysiology with human studies.</p><p>A powerful tool to link data from different measurement scales is the abstraction from measured activity itself to its information content, as enabled by multivariate decoding methods. Representational similarity analysis (RSA) compares the representational structure of signals (<xref ref-type="bibr" rid="bib11">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib27">Kriegeskorte et al., 2008a</xref>). However, as decoding approaches have inherent difficulties to identify the sources of decodable information (<xref ref-type="bibr" rid="bib10">Carlson et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Liu et al., 2018</xref>), it is necessary to employ thoughtful control analyses or experiments (<xref ref-type="bibr" rid="bib12">Cichy et al., 2015</xref>) to disambiguate different possible mechanisms underlying large-scale information structure. This crucially relies on empirical knowledge about processes on the circuit-scale.</p><p>To bridge the gap between invasive and non-invasive electrophysiology, in the present study, we developed and employed fully human-comparable high-density monkey EEG. We presented identical color and motion stimuli to both human participants and macaque monkeys and combined large-scale recordings on multiple scales, including invasive electrophysiology from six areas across the macaque brain, monkey EEG and human MEG with multivariate decoding and representational similarity analysis. We found color and motion direction information not only in invasive signals, but also in EEG and MEG. We show how motion and color tuning in human MEG can be traced back to the properties of individual units. Our results establish a proof of principle for using large-scale electrophysiology across species and measurement scales to link non-invasive recordings to circuit-level activity.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To compare information about color and motion direction in invasive and non-invasive signals, we presented rapid streams of dynamic random dot kinematograms (RDKs) with varying color and motion direction to macaque monkeys and humans (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We measured single-unit activity, analog multi-unit activity and local field potentials from multiple microelectrodes in six areas of two macaques, and MEG in eleven human volunteers. In order to establish a link between these data that differed both in species model and measurement technique, we developed non-invasive, human-comparable macaque EEG (for details, see methods). We used custom-made 65-channel caps with miniaturized EEG electrodes to measure scalp-EEG in two animals. This data, matching the invasive recordings in terms of species and the human MEG in terms of signal scale, allowed to relate circuit-level activity to large-scale measurements in humans. After preprocessing, we treated all data types identically and submitted them to the same multivariate pattern analysis (MVPA) of visual information. We used multi-class LDA (<xref ref-type="bibr" rid="bib23">Hastie et al., 2009</xref>) and a cross-validation scheme to derive time-resolved confusion matrices. For each combination of two stimulus classes A and B, the confusion matrices indicate the average probability of the LDA to assign a trial of class A to class B. From this, we extracted information time courses, latencies, and tuning properties.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.45645.003</object-id><label>Figure 1.</label><caption><title>Experimental paradigm, recording and analyses.</title><p>(<bold>A</bold>) We presented a stream of random dot patterns with rapidly changing colors and motion directions. After successfully maintaining fixation for a specified time, a liquid reward or auditory reward cue was delivered. (<bold>B</bold>) Colors and motion directions were sampled from geometrically identical circular spaces. Colors were uniformly distributed on a circle in L*C*h-space, such that they were equal in luminance and chromaticity, and only varied in hue. (<bold>C</bold>) We performed simultaneous microelectrode recordings from up to six cortical areas. We used custom 65 channel EEG-caps to record human-comparable EEG in macaque monkeys. MEG was recorded in human participants. (<bold>D</bold>) We used the same multivariate analysis approach on all signal types: Multi-class LDA applied to multiple recording channels resulted in time-resolved confusion matrices, from which we extracted classifier accuracy time courses and tuning profiles.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45645-fig1-v1.tif"/></fig><sec id="s2-1"><title>Color and motion direction information in invasive and noninvasive signals</title><p>We found that information about both motion direction and color was present in all signal types (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In LFP, multi-unit and single-unit data, motion and color information were strongest in areas MT and V4, respectively, in line with their established functional roles. Nonetheless, both features were represented ubiquitously (p&lt;0.05, cluster permutation, for most areas apart from motion in IT LFP). Importantly, monkey EEG (<xref ref-type="fig" rid="fig2">Figure 2B and E</xref>) and human MEG (<xref ref-type="fig" rid="fig2">Figure 2C and F</xref>) also contained information about motion direction and color (p&lt;0.05 for both features in both species, cluster-permutation).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.45645.004</object-id><label>Figure 2.</label><caption><title>Color and motion direction information across areas and measurement scales.</title><p>All panels show classifier accuracy, quantified as the single trial prediction probability for the correct stimulus. Error bars indicate standard error over recording sessions (in macaques) or participants (in humans), horizontal lines show periods of significant information (cluster permutation with p&lt;0.05, corrected for number of regions). (<bold>A</bold>) Color and (<bold>D</bold>) motion information is available in most areas in multi-unit, single-unit and LFP data. Information decreases along the cortical hierarchy. Note that MUA color information has very similar timecourses in PFC and FEF, and thus, FEF is barely visible. (<bold>B</bold>) Color and (<bold>E</bold>) motion information is available in monkey EEG. Insets: distribution of information in monkey EEG, estimated using source-level searchlight decoding. Information peaks in occipital areas. (<bold>C</bold>) Color and (<bold>F</bold>) motion information is available in human MEG. Insets: distribution of information in human MEG, estimated using source-level searchlight decoding. Information peaks in occipital areas.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45645-fig2-v1.tif"/></fig><p>Our analysis of microelectrode recordings showed decreasing information strength along the cortical hierarchy. To test whether this phenomenon was also detectable non-invasively, we performed source-reconstruction of monkey EEG and human MEG data using detailed physical headmodels (see methods, ‘Source reconstruction and searchlight analysis’). We then repeated the MVPA in a searchlight fashion across the cortex. Indeed, for both monkey EEG and human MEG, this revealed gradients of information with strongest information in early visual areas (<xref ref-type="fig" rid="fig2">Figure 2B,C,E,F</xref>; insets).</p><p>To compare the dynamics of feature information, we estimated information latencies as the time point the decoding performance reached half its maximum (<xref ref-type="fig" rid="fig3">Figure 3</xref>). For the invasive recordings, latencies were in accordance with the visual processing hierarchy, with information rising earliest in MT for motion direction (SUA: 78 ms, MUA: 81 ms, LFP: 98 ms), earliest in V4 for color (SUA: 82 ms, MUA: 86 ms, LFP: 91 ms), and last in frontal areas. Generally, color information was available earlier than motion direction information in most areas where latencies could be estimated reliably for SUA (V4: p=0.001; IT: p=0.13; MT: p=0.39, random permutation), MUA (V4: p&lt;0.001; IT: p=0.12; MT: p=0.26, random permutation) and LFP (V4: p=0.006; IT: p=0.75; MT: p=0.37, random permutation), consistent with previous results from the same animals in a different task (<xref ref-type="bibr" rid="bib45">Siegel et al., 2015</xref>). These results translated to the noninvasive signals: Both for monkey EEG (color: 91 ms, motion: 103 ms, p=0.03, random permutation) and human MEG (color: 70 ms, motion: 97 ms, p&lt;0.001, random permutation), color information rose earlier, and the latencies were comparable with those found invasively. Using the searchlight decoding analysis, we again found gradients consistent with the cortical hierarchy, with lowest latencies in occipital and highest latencies in more frontal regions (<xref ref-type="fig" rid="fig3">Figure 3B,C,E,F</xref>; insets), as confirmed by correlating source position and estimated latencies (MEG color: p=10<sup>−15</sup>, MEG motion direction: p=10<sup>−4</sup>, monkey EEG color: p=0.017, monkey EEG motion direction: p=10<sup>−20</sup>).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.45645.005</object-id><label>Figure 3.</label><caption><title>Color and motion direction information latencies across areas and measurement scales.</title><p>All panels show normalized classifier accuracy, and latency estimates as well as confidence intervals (bootstrap, 95%). (<bold>A</bold>) Color and (<bold>D</bold>) motion information rises first in early visual areas, and last in frontal areas. (<bold>B</bold>) Color and (<bold>E</bold>) motion latencies in monkey EEG are comparable to those in early visual areas. Insets: distribution of latencies in monkey EEG, estimated using source-level searchlight decoding. Information rises later in frontal sources than in occipital sources. Marked positions indicate sources for which time courses are shown. (<bold>C</bold>) Color and (<bold>F</bold>) motion latencies in human MEG are comparable to those of early visual areas in the macaque brain. Insets: distribution of latencies in human MEG, estimated using source-level searchlight decoding. Information rises later in frontal sources than in occipital sources.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45645-fig3-v1.tif"/></fig></sec><sec id="s2-2"><title>MEG color information cannot be explained by luminance confounds</title><p>Is it plausible that the contents of sensory representations are accessible to noninvasive electrophysiology? It has been shown that, in general, features represented at the level of cortical columns can propagate to decodable MEG and EEG signals (<xref ref-type="bibr" rid="bib12">Cichy et al., 2015</xref>). Recently, it was reported that information about the motion direction of random dot stimuli can be extracted from EEG signals (<xref ref-type="bibr" rid="bib5">Bae and Luck, 2019</xref>). This study is, however, to our knowledge the first direct report of color decoding from MEG or EEG. It is conceivable that luminance confounds introduced by imperfections in the color calibration or individual variation in retinal color processing could explain color decoding. To exclude this possibility, we performed a control experiment in a single human subject, in which we manipulated luminance such that each stimulus was presented in a darker and a brighter version. We then used a cross-classification approach to test whether true color information dominated the artificially introduced luminance effect. To this end, we grouped trials such that, for each color, one luminance level was used for training and the other for evaluating the decoder, effectively creating a mismatch of information between test and training data. The color decoder could now, in principle, pick up three sources of information: true color differences, unknown, confounding luminance differences, and experimentally introduced luminance differences. In isolation, these luminance differences should lead to below-chance accuracy. Therefore, any remaining above-chance effect would either indicate that the luminance confound was even stronger than the control manipulation, or that true color information was present. Indeed, we found that classifier accuracy was still significantly above chance (p&lt;0.05, cluster permutation), and undiminished by the luminance manipulation (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Furthermore, we compared the confusion matrices of classifiers trained and tested on dark or bright stimuli, trained on dark and tested on bright stimuli, or vice versa (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). All confusion matrices were highly similar, indicating that the representational structure was comparable for low- and high luminance colors. Taken together, this suggests that in our main experiment, equiluminant and equisaturated color stimuli lead to discriminable MEG signatures, and luminance confounds had only a small, if any, effect.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.45645.006</object-id><label>Figure 4.</label><caption><title>Control experiments.</title><p>(<bold>A</bold>) Time-resolved classifier accuracy. Accuracy is highest when trained and tested on high-luminance stimuli (L1), and lower when trained and tested on low-luminance stimuli (L2). Training on half of the color space in low luminance, and half of the color space in high luminance, and testing on the remainder (G1 x G2), results in accuracy comparable to the low-luminance stimuli alone. (<bold>B</bold>) Confusion matrices for low-luminance and high luminance stimuli, as well as classifiers trained on low-luminance and tested on high-luminance stimuli and vice versa (L1 x L2, L2 x L1). (<bold>C</bold>) Maximum color and motion classifier accuracies for all individual sessions. Color is better classified in monkey EEG, motion is better classified in human MEG and EEG. Simultaneously recorded human MEG and EEG results in overall higher accuracy in MEG, but more motion than color information in both.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45645-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.45645.007</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Perceptual equiluminance control.</title><p>(<bold>A</bold>) To account for the possibility of luminance differences between stimuli due to different color sensitivities in macaques and humans, we performed a psychophysical experiment using a no-report minimum motion technique based on eye-movements in a third macaque monkey. We presented moving gratings consisting of alternating frames with luminance and color contrasts, respectively, with each frame being shifted by a quarter cycle for 500 ms. Color contrasts consisted of a reference gray and a color to be probed. If the probed color was darker than the reference gray, motion in one direction was perceived, if it was brighter, in the opposite direction. A color equiluminant to the reference gray should not evoke any stable motion percept. Stimuli were presented in two conditions: either a brighter probe color elicited upwards motion, or downwards motion. (<bold>B</bold>) This technique was applied to colors of 8 different hues, at 19 luminance levels. (<bold>C</bold>) We computed a luminance difference index based on eye position measurements, by calculating the difference in vertical eye trace curvature between conditions. For a color equiluminant to the reference gray, this measure was expected to be 0. The left plots show the luminance difference measure resolved in time, the right plots show the average over time. For all colors, 0 was crossed within the range of luminances probed, suggesting that this range included colors equiluminant to the reference gray. There was a significantly positive slope (pearson correlation, p&lt;0.05) for 6 out of 8 colors, validating our approach. (<bold>D</bold>) Estimates of the zero-crossing for all eight hues are close to the L value of the reference gray, suggesting that psychophysically equiluminant colors in the macaque are close to those derived from human L*C*h - space. Shaded area shows 95% bootstrap confidence interval.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45645-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Our stimuli were generated in L*C*h-space, which is designed based on perceptual uniformity in humans. However, it has been shown that color sensitivities in macaque monkeys are highly similar, but not identical to humans (<xref ref-type="bibr" rid="bib21">Gagin et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Lindbloom-Brown et al., 2014</xref>). To ensure that color decoding in the monkey data was not driven by luminance differences, we performed a psychophysical control experiment in a third macaque monkey. Using a minimum-motion technique and eye movements as readout (<xref ref-type="bibr" rid="bib32">Logothetis and Charles, 1990</xref>), we found that equiluminant colors generated in L*C*h-space were also close to perceptually equiluminant for this monkey (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p></sec><sec id="s2-3"><title>Information contained in human EEG is comparable to MEG</title><p>While in human MEG data, there was more information about motion direction than about color, monkey EEG data showed the opposite effect (<xref ref-type="fig" rid="fig2">Figure 2B,C,E,F</xref>). In principle, this could be due to differences in species, measurement modality (EEG or MEG), or differences in the visual stimulation that were beyond our control due to the separate recording environments. To exclude measurement modality as the relevant factor, we acquired simultaneous MEG and EEG data in one of the human participants and compared the amount of motion direction and color information across MEG and EEG data. All monkey EEG recording sessions contained more information about color, and all human MEG recordings contained more information about motion direction. Notably, the human EEG session was consistent with the MEG results. While information was generally lower for EEG than for simultaneous MEG, EEG showed the same dominance of motion information (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). This suggests that the differences of information dominance between human MEG and monkey EEG were not due to the recording modality.</p></sec><sec id="s2-4"><title>Representational similarity analysis</title><p>Having established the presence of information in all signal types, we next asked how the representational structure of motion direction and color varied across brain areas, species, and measurement scales. To address this, we performed representational similarity analysis (<xref ref-type="bibr" rid="bib27">Kriegeskorte et al., 2008a</xref>) (RSA) on the LDA confusion matrices averaged over a time window during which visual information was present (50–250 ms). In short, we used RSA to compare patterns of similarity between stimulus classes, as given by the confusion matrices, across areas and signal types. First, we sought to characterize the diversity of representations across the six areas measured invasively (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). For color information, we found that representations were highly similar between SUA, MUA and LFP, as well as between all six cortical areas (p&lt;0.05 for most pairs of areas and measures, uncorrected), indicating that a single representational structure was dominant across the brain. In the case of motion direction, areas were split into ventral stream visual areas (IT and V4) and frontal and dorsal visual stream areas (MT, LIP, FEF, PFC). Within each of these two groups, there were again high correlations between areas and measures, but we found no significant similarity between the groups.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.45645.008</object-id><label>Figure 5.</label><caption><title>Representational similarity between areas and measurement scales.</title><p>(<bold>A</bold>) Similarity between SUA, MUA and LFP color (top) and motion direction representations (bottom) in six areas of the macaque brain, masked at p&lt;0.05 (uncorrected). Color representations are highly similar between all areas; motion representations are split between frontal/dorsal and ventral areas. (<bold>B</bold>) and (<bold>C</bold>) Similarity between monkey EEG and human MEG color and motion representations and those in SUA, MUA and LFP in six areas. Non-invasive color representations are similar to all areas, motion representations are similar to IT and V4 representations (p&lt;0.05, random permutation test, corrected for number of areas). (<bold>D</bold>) Color and motion representations are similar between human MEG and monkey EEG (both p&lt;0.001, random permutation test).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45645-fig5-v1.tif"/></fig><p>How does information contained in locally recorded neuronal activity relate to information in large-scale EEG signals? We found that the color representation in macaque EEG was highly similar to those of SUA, MUA and LFP in all six areas, while the EEG motion direction representation reflected only the ventral stream areas V4 and IT (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, p&lt;0.05 for IT SUA and MUA, V4 SUA, MUA and LFP, random permutation, corrected for number of areas). Notably, we found no motion direction similarity between area MT and EEG (SUA: p=0.84; MUA: p=0.85; LFP: p=0.82, uncorrected). This implies that, although MT contained a large amount of motion direction information, EEG signals were dominated by activity from areas with V4- or IT-like motion direction tuning. We found similar results when comparing invasive data to human MEG; again, there were strong similarities between color representations in all areas and human MEG, as well as between motion direction representations in V4 and IT and human MEG (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, p&lt;0.05). Furthermore, both color and motion direction representations were highly similar between monkey EEG and human MEG (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, color: r = 0.83, p=0.0002; motion: r = 0.69, p=0.0003).</p></sec><sec id="s2-5"><title>Similarity is explained by tuning properties</title><p>Color representations were similar across the brain, while motion direction representations were divided into two categories, only one of which translated to non-invasive signals. To investigate what led to these effects, we examined the underlying representations more closely. <xref ref-type="fig" rid="fig6">Figure 6</xref> shows the color and motion direction confusion matrices for MT and V4 multi-unit activity as well as for monkey EEG and human MEG. All color confusion matrices displayed a simple pattern decreasing with distance from the diagonal. This implies that neural activity distances in all areas, signals and both species approximately matched perceptual distances in color space. We found a similar representation of motion direction in area MT.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.45645.009</object-id><label>Figure 6.</label><caption><title>Color and motion direction tuning.</title><p>Color and motion direction tuning in (<bold>A</bold>) MT (left), V4 (right), (<bold>B</bold>) monkey EEG and (<bold>C</bold>) human MEG. Shown is, for each area or signal type, first: the temporally resolved prediction probability as a function of the distance between true and predicted stimulus. The dark blue line indicates the probability of a stimulus being predicted correctly (classifier accuracy), the green (color) and orange (motion) lines the probability of a stimulus being predicted as the opposite in the circular stimulus space. Second: the confusion matrix, indicating prediction probabilities for all stimulus combinations. Third: A representation tuning curve, indicating prediction probabilities as a function of distance between true and predicted stimulus at the time of peak accuracy. For color, tuning is always unimodal, with opposite-classifications having the lowest probability. For motion direction, V4, EEG and MEG, but not MT tuning is bimodal, with opposite-classifications having a higher probability than some intermediate stimulus distances.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45645-fig6-v1.tif"/></fig><p>However, motion direction representations in V4, monkey EEG and human MEG displayed a distinct peak in similarity on the off-diagonal opposite to the true motion direction, indicating that these signals were, to some extent, invariant to motion in opposite directions. To assess the temporal dynamics of this effect, we collapsed the confusion matrices over stimuli, which results in prediction probabilities as a function of the angular difference between true and predicted stimuli (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Here, the off-diagonal elements in the confusion matrices translated to an increased probability of a stimulus to be predicted as the one opposite in stimulus space. At all timepoints, color stimuli were least likely to be classified as the opposite color, whereas there was an increased probability for motion directions to be identified as the opposite. In terms of population tuning, this corresponds to bimodal tuning curves (<xref ref-type="fig" rid="fig6">Figure 6</xref>). We quantified the presence of such bimodal tuning across areas and measurement scales by calculating the slope in prediction probability between opposite (180-degree difference) and next-to-opposite (135- and 225-degree difference) stimuli, normalized by the range of prediction probabilities (<xref ref-type="fig" rid="fig7">Figure 7</xref>). This revealed that motion direction tuning was indeed significantly bimodal in V4 and IT as well as monkey EEG and human MEG, but not for any of the more dorsal or frontal areas. There was no significant bimodal color tuning for any area or measurement scale.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.45645.010</object-id><label>Figure 7.</label><caption><title>Motion direction tuning bimodality across areas and measurement scales explains representational similarity.</title><p>Representations of color (<bold>A</bold>) do not show bimodality; representations of motion (<bold>B</bold>) do in IT, V4 and non-invasive data, but not in frontal and dorsal areas. (<bold>C, E</bold>). Correlation of representational similarity between SUA, MUA and LFP in all areas with differences in bimodality. In case of color (<bold>C</bold>), there is no strong correlation, in case of motion (<bold>E</bold>), there is a strong anticorrelation. (<bold>D, F</bold>). Average tuning of individual single units, multi units or LFP channels in MT and V4. Color tuning is unimodal in both areas, motion tuning is bimodal in area V4.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45645-fig7-v1.tif"/></fig><p>We used linear regression to estimate the contribution of bimodality differences to the pattern of similarity between invasively measured areas and signal types (<xref ref-type="fig" rid="fig7">Figure 7C and E</xref>). To this end, we computed differences in bimodality between each combination of SUA, MUA and LFP, and all areas. We then assessed to what extent these differences in bimodality accounted for the variance in representational similarity. Importantly, in the case of motion direction, bimodality could largely explain the pattern of representational similarity between areas and measures (R<sup>2</sup> = 0.28, p=0). This was not the case for the small bimodality differences in color tuning, which did not affect representational similarity (R<sup>2</sup> = 0, p=0.99). Thus, similar motion direction bimodality led to V4 and IT showing similar motion representations, which were also similar to those in monkey EEG and human MEG.</p></sec><sec id="s2-6"><title>Motion direction bimodality is present in individual SUA, MUA and LFP channels</title><p>Finally, we asked on which level the motion direction bimodality arose. The presence of a bimodality effect in MEG, EEG, LFP, multi-unit and sorted unit data suggests that it was not caused by anisotropies in the large-scale organization of motion direction tuning, but rather by properties of individual units: if individual single or multi-units, or LFP channels, were able to distinguish between opposite motion directions, a multivariate analysis of several channels would be expected to also reflect this separability. We therefore expected bimodal motion direction tuning curves to be prominent in those areas which exhibited a multivariate bimodality effect. To test this, we aligned all tuning curves in V4 and MT according to their preferred direction and calculated, for each area, their average. Indeed, direction tuning curves in areas V4 (SUA: p=1*10<sup>−9</sup>, MUA: p=8*10<sup>−11</sup>, LFP: p=0.03) were bimodal, whereas direction tuning curves in area MT (SUA: p=0.66, MUA: p=0.31, LFP: p=0.87) or color tuning curves in either area (all p&gt;0.42) were not (<xref ref-type="fig" rid="fig7">Figure 7D and F</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We found that information about motion direction and color was present in invasively recorded signals in many cortical areas in macaque monkeys as well as in non-invasive electrophysiological data from macaques and humans. Dissecting the information structure revealed representations according to perceptual similarity for color in all areas, and for motion direction in dorsal and frontal areas. Contrary to that, V4 and IT motion direction representations were bimodal, indicative of orientation rather than direction selectivity. We found the same bimodal pattern in monkey EEG and human MEG, as confirmed by representational similarity analysis. Together with converging evidence from latency and information distributions this pointed to early visual and ventral stream areas such as V4 as the main drivers of EEG and MEG representations, while dorsal areas like MT did not appear to strongly contribute to non-invasive signals.</p><sec id="s3-1"><title>Widespread representations of visual features across cortex</title><p>Consistent with earlier reports (<xref ref-type="bibr" rid="bib3">An et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Mendoza-Halliday et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Siegel et al., 2015</xref>), we found color and motion information in all areas we measured, rather than in a small amount of specialized areas. Nonetheless, the amount of information strongly depended on the area. Interestingly, the motion direction decoding accuracies we found were lower than previously reported in both area MT and prefrontal cortex (<xref ref-type="bibr" rid="bib33">Mendoza-Halliday et al., 2014</xref>). This can largely be attributed to differences in the paradigm and analysis strategy: First, rather than decoding from large pseudo-populations, we used small, simultaneously recorded populations. Second, we report averaged single trial probabilities, which tended to be smaller but more robust than the corresponding discrete classification results. Third, the rapid succession of very short stimuli likely limited cortical processing of each stimulus. Fourth, our paradigm only involved passive fixation. Especially in higher-order areas we would expect representations to be strengthened, and altered, according to task demands in a more engaging cognitive task.</p></sec><sec id="s3-2"><title>Early ventral stream areas as sources of non-invasive information</title><p>Stimulus features showing a spatial organization at the scale of cortical columns, such as orientation, can in principle be decoded from EEG and MEG (<xref ref-type="bibr" rid="bib12">Cichy et al., 2015</xref>). This implies that other, similarly topographical representations should be equally accessible. Indeed, a clustering of both color (<xref ref-type="bibr" rid="bib16">Conway and Tsao, 2009</xref>; <xref ref-type="bibr" rid="bib39">Roe et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Tanigawa et al., 2010</xref>) and motion direction (<xref ref-type="bibr" rid="bib2">Albright et al., 1984</xref>; <xref ref-type="bibr" rid="bib29">Li et al., 2013</xref>; <xref ref-type="bibr" rid="bib49">Tanigawa et al., 2010</xref>) has been reported in several areas of the visual system. This suggests that our successful decoding of stimulus color and motion direction was not attributable to confounding factors, but likely stemmed from true feature-related signals.</p><p>Crucially, even though we recorded invasively in many areas, our results do not unequivocally identify the sources of visual information in MEG and EEG. First, neither color nor motion direction representations are limited to the areas we recorded from. Secondly, partially due to the simple feature spaces used for our stimuli, many areas are expected to show highly similar tuning properties. Based on RSA, we can therefore only conclude that the non-invasively measured information stems from areas with tuning similar to V4 or IT. It is reasonable to assume that earlier visual areas strongly contributed to this, which is corroborated by our source level searchlight analysis revealing strong information peaks in occipital cortex. Furthermore, it has been shown that for example area V1 exhibits a more bimodal motion direction tuning (i.e. orientation or axis tuning) than area MT (<xref ref-type="bibr" rid="bib1">Albright, 1984</xref>), matching the results found here in V4. There is, however, previous evidence that the structure of color representations decodable from area V1 using fMRI is not in agreement with perceptual similarity (<xref ref-type="bibr" rid="bib9">Brouwer and Heeger, 2013</xref>), contrary to area V4, and contrary to the representations we found in MEG and EEG, suggesting that these color representations might not be explained by V1 activity alone.</p><p>Notably, in area MT cortical columns with opposite preferred motion directions along the same axis lie spatially close to each other (<xref ref-type="bibr" rid="bib2">Albright et al., 1984</xref>; <xref ref-type="bibr" rid="bib7">Born and Bradley, 2005</xref>). This could, in principle, lead to a diminished decodability of opposite motion directions from mass signals such as EEG, MEG or fMRI. In such a scenario, the source of bimodal motion direction tuning might still lie in area MT. However, this would require columns with opposite preferred motion directions to be close to uniformly distributed at the relevant spatial scale. While several recent fMRI studies have focused on motion axis decoding (<xref ref-type="bibr" rid="bib41">Schneider et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Zimmermann et al., 2011</xref>), motion direction has been successfully decoded from BOLD signals in area MT (<xref ref-type="bibr" rid="bib26">Kamitani and Tong, 2006</xref>). Given that motion representations are prevalent across visual cortex (<xref ref-type="bibr" rid="bib3">An et al., 2012</xref>), we consider it unlikely that MT was a dominant source of the bimodally tuned motion signals we measured in EEG and MEG.</p><p>In sum, this suggests that the information decoded from non-invasive signals originated in a mixture of several early visual areas. Recordings from additional visual areas using the same paradigm are required to further clarify this. Future studies may also expand the stimulus space - a limitation of the present proof-of-principle study. Manipulating other stimulus features in order to maximize differences between areas will allow to further dissociate representations in specific parts of the visual system.</p></sec><sec id="s3-3"><title>Monkey EEG as a bridge technology</title><p>We utilized human-comparable monkey EEG as a bridge technology to link invasive animal electrophysiology to human MEG. High electrode density and methods identical to those used in human M/EEG enabled us to perform source reconstruction and directly relate measures across species. The few available previous studies measuring EEG in nonhuman primates were typically restricted to only a few electrodes (<xref ref-type="bibr" rid="bib6">Bimbi et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Snyder et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Snyder et al., 2018</xref>) and used diverging methods such as skull-screw electrodes, or both (<xref ref-type="bibr" rid="bib22">Godlove et al., 2011</xref>; <xref ref-type="bibr" rid="bib34">Musall et al., 2014</xref>; <xref ref-type="bibr" rid="bib38">Reinhart et al., 2012</xref>; <xref ref-type="bibr" rid="bib52">Whittingstall and Logothetis, 2009</xref>; <xref ref-type="bibr" rid="bib53">Woodman et al., 2007</xref>). We show how monkey EEG can serve as a missing link to enable the disentangling of species differences from differences in measurement modality. In isolation, our observation of bimodal motion direction tuning in human MEG could not directly inform conclusions about the relative contributions of dorsal and ventral stream areas. Finding the same result in monkey EEG allowed us to infer that it was not due to a decreased influence of MT-like areas in the human, but rather a sign of a general dominance of V4-like tuning in non-invasive signals.</p><p>State-of-the-art animal electrophysiology requires large technical efforts and comes at a significant ethical cost. When applied in addition to ongoing invasive experiments, the marginal cost of monkey EEG is comparably small. It is non-invasive, depends mostly on standard human neuroscience tools, and does not necessitate further animal training. This is far outweighed by the potential benefits of establishing a database for linking invasive and non-invasive electrophysiology and for enhancing comparability between the fields. Notably, another possibility to achieve this goal is given by invasive electrophysiological recordings in human patients, that are however severely constrained by the requirement for medical necessity.</p></sec><sec id="s3-4"><title>A framework for linking measurement scales</title><p>In the current work, we used an information-based approach to compare brain areas, measurement scales, and species. Such analyses are powerful tools to relate very different signals based on their information contents. This may not only include data from different measurement techniques, such as MEG and fMRI (<xref ref-type="bibr" rid="bib11">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">Cichy et al., 2016a</xref>), or species (<xref ref-type="bibr" rid="bib11">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Kriegeskorte et al., 2008b</xref>), but also cognitive or computational models (<xref ref-type="bibr" rid="bib14">Cichy et al., 2016b</xref>; <xref ref-type="bibr" rid="bib51">Wardle et al., 2016</xref>). Furthermore, instead of comparing representations of simple sensory stimuli, the same framework can be applied to complex task conditions (<xref ref-type="bibr" rid="bib24">Hebart et al., 2018</xref>).</p><p>We would like to highlight that our framework of cross-species and cross-scale comparisons is not limited to information-based analyses. For example we anticipate that it will be highly interesting to compare and pinpoint specific spectral signatures of circuit activity in signals on all scales (<xref ref-type="bibr" rid="bib18">Donner and Siegel, 2011</xref>; <xref ref-type="bibr" rid="bib44">Siegel et al., 2012</xref>). This has been successful in some cases (<xref ref-type="bibr" rid="bib42">Sherman et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Shin et al., 2017</xref>), but could significantly benefit from the present large scale approach to gain further specificity. In the long term, with sufficient knowledge about mechanistic signatures on all scales, this could facilitate the establishment of transfer functions between circuit activity and non-invasive human electrophysiology (<xref ref-type="bibr" rid="bib15">Cohen, 2017</xref>; <xref ref-type="bibr" rid="bib18">Donner and Siegel, 2011</xref>; <xref ref-type="bibr" rid="bib44">Siegel et al., 2012</xref>). It is important to note that such transfer can only be possible based on knowledge on all scales. As has been noted before (<xref ref-type="bibr" rid="bib48">Sprague et al., 2018</xref>), macro-scale signals alone always suffer from an ill-posed inverse problem when trying to infer micro-scale properties. The approach of dense recordings on all scales, as outlined here, allows to bridge this gap by constraining inferences. Such developments would allow quick and inexpensive access to circuit function in the human brain, both for basic research and in clinical practice (<xref ref-type="bibr" rid="bib44">Siegel et al., 2012</xref>).</p></sec><sec id="s3-5"><title>Summary and conclusion</title><p>In sum, we show that color and motion direction can be decoded from non-invasive electrophysiology in humans and monkeys. Our results suggest that such simple stimulus representations are dominated by signals from early ventral stream areas. This inference serves as a proof-of-principle for, and was enabled by, using high-density monkey EEG as a bridge technology to link scales and species.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Macaque microelectrode recordings</title><sec id="s4-1-1"><title>Subjects</title><p>Microelectrode recordings were performed in two adult rhesus macaques, one male (monkey R) and one female (monkey P). Each monkey was implanted with a titanium headpost to immobilize the head. Following behavioral training, three titanium recording chambers were stereotactically implanted over frontal, parietal, and occipitotemporal cortices in the left hemisphere. All procedures followed the guidelines of the Massachusetts Institute of Technology Committee on Animal Care and the National Institutes of Health.</p></sec><sec id="s4-1-2"><title>Stimuli and apparatus</title><p>We presented rapid streams of colored random dot kinematograms with 100% motion and color coherence. Colors and motion directions changed randomly from stimulus to stimulus. We sampled dot colors from a circle in CIEL*C*h color space such that they had equal luminance and saturation. The background color was always a uniform black. Therefore, individual stimuli contained both luminance and chromaticity contrasts between background and dots, whereas the only features varying over stimuli were color hue and motion direction. Sequences of stimuli were presented before each trial of an unrelated delayed saccade task and separated by short inter-stimulus intervals, while fixation had to be maintained. Stimuli had a diameter of 3.2 degrees of visual angle, featuring 400 dots with a diameter of 0.08 degrees. Two variants of this paradigm were used: in stimulus configuration A, we showed sequences of 6 stimuli lasting 150 ms with an ITI of 50 ms. In this case, 12 uniformly distributed colors and motion directions (0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330 degrees) were used and dots moved at a speed of 10 degrees per second. In stimulus configuration B, sequences of 8 stimuli were shown. In this case, stimuli lasted 100 ms with ISIs of 20 ms, were sampled from eight colors and motion directions (0, 45, 90, 135, 180, 225, 270, 315 degrees), and dots moved at a speed of 1.33 degrees per second. Liquid rewards were administered when the monkeys succeeded in both maintaining fixation on the stimulus streams and in completing the subsequent unrelated trial. Stimuli were generated offline using MATLAB, and presented using the MonkeyLogic toolbox (<xref ref-type="bibr" rid="bib4">Asaad et al., 2013</xref>).</p></sec><sec id="s4-1-3"><title>Microelectrode recordings</title><p>Microelectrode activity was recorded in a total of 71 recording sessions, 47 in monkey P and 24 in monkey R. 31 of the sessions in monkey P used stimulus configuration A, 16 used stimulus configuration B. 18 of the sessions in monkey R used stimulus configuration A, six used stimulus configuration B. Combined over all sessions of both monkeys, 58,056 stimuli were presented. In each recording session, we acutely lowered Epoxy-coated tungsten electrodes in up to six areas out of the lateral prefrontal cortex, frontal eye fields (FEF), lateral intraparietal cortex (LIP), inferotemporal cortex (TEO), visual area V4, and the middle temporal area (MT). Neuronal activity was recorded across a maximum of 108 electrodes simultaneously. All signals were recorded broad-band at 40 kHz referenced to the titanium headpost. Monkeys sat in a primate chair, while stimuli were presented on a CRT monitor with a refresh rate of 100 Hz.</p></sec><sec id="s4-1-4"><title>Preprocessing</title><p>We analyzed data from a total of 4505 cortical recording sites (V4: 372, IT: 148, MT: 272, LIP: 897, FEF: 1067, PFC: 1749). From the broad-band data, analog multi-unit activity (MUA) was extracted by high- and low-pass filtering at 500 and 6000 Hz, respectively (2<sup>nd</sup>-order zero-phase forward-reverse Butterworth filters), rectification, low-pass filtering at 250 Hz (2<sup>nd</sup>-order zero-phase forward-reverse Butterworth filter), and resampling at 1 kHz. Local field potentials (LFP) were extracted by low-pass filtering of broad-band data at 500 Hz and later re-referenced to a local bipolar reference. Single unit activity (SUA) was obtained through spike sorting (Plexon Offline Sorter) of the high- (500 Hz) and low- (6000 Hz) pass filtered broad-band data thresholded at four times the noise threshold. Single-unit isolation was assessed by an expert user (CvN) and judged according to a quality index ranging from 1 (clearly distinguishable, putative single unit) to 4 (clearly multi-unit). We used principal components (PC) 1 and 2 of the spike waveform as well as the nonlinear energy function of the spike as axes in 3D sorting space. A putative single unit had to exhibit clear separability of its cluster in this 3D feature space, as well as a clean stack of individual waveforms in its overlay plot. Units of all quality types were included in the analysis. All signal types were then band-pass-filtered between 0.1 and 10 Hz (Butterworth, 2-pass, 4<sup>th</sup> order). This transformed single unit spikes into an approximation of instantaneous firing rate and ensured comparability of all signal types with EEG and MEG data.</p></sec></sec><sec id="s4-2"><title>Macaque EEG</title><sec id="s4-2-1"><title>Subjects</title><p>We measured scalp EEG in two male adult rhesus monkeys. All procedures were approved by local authorities (Regierungspräsidium Tübingen).</p></sec><sec id="s4-2-2"><title>Stimuli and apparatus</title><p>Stimuli were created as described above for the macaque microelectrode recordings. However, we only used eight colors and motion directions, and no additional, unrelated task was performed. Initially, a central spot had to be fixated for 500 ms, after which stimuli started to appear for 100 ms each and without an ISI. Monkey V received a liquid reward as well as auditory feedback after 2 s of successful fixation on the stimulus sequence, after which the trial ended. For monkey E, we presented a continuous stimulus sequence as long as fixation was maintained. After each multiple of 5 s of successful fixation, reward and auditory feedback were administered. As soon as fixation was broken, the stimulus sequence stopped.</p><p>To maximize signal-to-noise ratio, we chose larger stimuli for most recording sessions: In all 3 sessions of monkey E, and 4 out of 8 sessions of monkey V, stimuli had a diameter of 6 degrees of visual angle, with a 0.75-degree central annulus. They consisted of 1600 dots with 0.2-degree diameter moving at 10 degrees per second. In the remaining 4 sessions of monkey V, stimuli had a diameter of 3.2 degrees, and consisted of 400 dots with 0.08-degree radius, therefore matching those used in the microelectrode recordings. Stimuli were generated offline using MATLAB and presented using Psychtoolbox (<xref ref-type="bibr" rid="bib8">Brainard, 1997</xref>).</p></sec><sec id="s4-2-3"><title>EEG recordings</title><p>EEG was recorded using 65 Ag/AgCl electrodes and a NeurOne recording system (Bittium, Oulu, Finland) in 11 recording sessions, during which a total of 167,762 stimuli were presented. All channels were referenced to a central electrode, recorded with a sampling rate of 5 kHz and low-pass filtered online at 1250 Hz. An additional ground electrode was placed at the headpost. Electrodes were placed on the scalp using a custom-built 66-channel EEG cap (Easycap, Herrsching, Germany) covering the entire scalp. To leave room for the headpost, one of the 66 electrode positions was not used. Based on anatomical MRIs and 3D-printed head models, EEG caps were fabricated to match the individual animal’s head shape. To achieve low impedances, we shaved and cleaned the monkeys’ heads with saline and alcohol before each experimental session. Electrodes were filled in advance with a sticky conductive gel (Ten20, Weaver and Company, Aurora, Colorado, USA). After placing the cap on the head, we applied a second, abrasive conductive gel (Abralyt 2000, Easycap, Herrsching, Germany) through the opening of the electrodes, yielding stable impedances below 10 kΩ. Before each session, we 3D-localized electrode positions relative to the head using a Polaris Vicra optical tracking system (NDI, Waterloo, Ontario, Canada). Monkeys sat in a primate chair in a dark recording chamber while stimuli were presented on a CRT monitor with a refresh rate of 100 Hz. Infrared eye-tracking was performed at a sampling frequency of 1000 Hz using an Eyelink 1000 system (SR Research, Ottawa, Ontario, Canada).</p></sec><sec id="s4-2-4"><title>Preprocessing</title><p>EEG data was down-sampled to 300 Hz, re-referenced to an average reference and band-pass-filtered between 0.1 and 10 Hz (4<sup>th</sup> order, forward-reverse Butterworth filter).</p></sec></sec><sec id="s4-3"><title>Human MEG</title><sec id="s4-3-1"><title>Subjects</title><p>11 healthy volunteers (three female, 28.6 + −4.8 years) with normal or corrected-to-normal vision participated in this study. They received monetary rewards for participation that were in part dependent on their performance on the task. The study was conducted in accordance with the Declaration of Helsinki and was approved by the ethics committee of the University of Tübingen. All participants gave written informed consent before participating.</p></sec><sec id="s4-3-2"><title>Stimuli and apparatus</title><p>Stimuli were created and presented as described above for the monkey EEG recordings. Random dot kinematograms had a diameter of 6 degrees, with a central annulus of 0.75 degrees, and were presented in a continuous stream that ended when fixation was broken. After each multiple of 5 s of successful fixation, participants received auditory feedback associated with a monetary reward. Stimuli were generated offline using MATLAB, and presented using Psychtoolbox (<xref ref-type="bibr" rid="bib8">Brainard, 1997</xref>).</p></sec><sec id="s4-3-3"><title>MEG recordings</title><p>We recorded MEG (Omega 2000, CTF Systems, Inc, Port Coquitlam, Canada) with 275 channels at a sampling rate of 2,343.75 Hz in a magnetically shielded chamber. The eleven participants completed one recording session each, resulting in a total of 237,348 stimuli being presented. Participants sat upright in a dark room, while stimuli were projected onto a screen at a viewing distance of 55 cm using an LCD projector (Sanyo PLC-XP41, Moriguchi, Japan) at 60 Hz refresh rate.</p></sec><sec id="s4-3-4"><title>Preprocessing</title><p>MEG data was downsampled to 300 Hz and band-pass-filtered between 0.1 and 10 Hz (4<sup>th</sup> order, forward-reverse Butterworth filter).</p></sec></sec><sec id="s4-4"><title>Structural MRI</title><p>To enable source reconstruction, we acquired anatomical MRI scans from both macaques and humans. T1-weighted images were obtained for each human participant and the two monkeys used for EEG recordings.</p></sec><sec id="s4-5"><title>Multivariate classification</title><p>We used linear discriminant analysis (LDA) to extract the content and structure of information about stimulus features from all signal types. Trials were stratified such that each combination of color and motion direction occurred equally often, grouped according to one of the two stimulus features and split into training and test sets. For each time-point, we trained multi-class LDA on the training set, and predicted stimulus probabilities in the test set, using the activity in single or multi-units, EEG electrodes or MEG sensors as classification features. From the predicted stimulus probabilities, we created confusion matrices indicating the probability of stimuli being labeled as any other stimulus by the classifier. We evaluated classifier performance as the hit rate, calculated as the mean of the diagonal of the confusion matrix.</p><p>For EEG and MEG, we repeated this analysis in a 10-fold cross-validation scheme for each session, using all available sensors. For SUA, MUA and LFP, we used 2-fold instead of 10-fold cross-validation. Here, stimuli were presented in sequences of six or eight stimuli, and the occurrence of individual stimuli at each sequence position was not fully balanced. To prevent a potential confound of stimulus information with sequence position, we chose a stratification approach that kept the number of occurrences of each stimulus at each sequence position identical by oversampling the under-represented stimuli within each cross-validation fold. Due to the relatively low number of stimuli per recording session, 10-fold cross-validation would not have resulted in sufficient trials per fold for this approach. We therefore chose 2-fold cross-validation instead and performed classification independently for each of the six areas recorded. We restricted the analysis to five units per area at a time and repeated it for all or maximally 40 random combinations of the available units, to enable a comparison of information content in different areas. Results from these repetitions were averaged before statistical analysis. This analysis was performed for each time point from 250 ms before to 500 ms after stimulus onset, in steps of 10 ms, resulting in confusion matrices and classifier performances at 76 time points. In most of our recordings we presented eight different colors or motion directions. However, in the invasive recordings in stimulus configuration A there were 12 colors and directions. Therefore, we interpolated the confusion matrices of these recordings from a 12 × 12 to an 8 × 8 space.</p><p>We assessed the presence of significant information using a cluster sign permutation procedure (similar to <xref ref-type="bibr" rid="bib11">Cichy et al., 2014</xref>). After subtracting chance performance (0.125), we determined temporally contiguous clusters during which information was higher than 0 (one-tailed t-test over recording sessions, p&lt;0.01). We then randomly multiplied the information time-course of each recording session 10,000 times with either 1 or −1, resulting in an expected value of 0. In each random permutation, we re-computed information clusters and determined the cluster-mass of the strongest cluster. Each original cluster was assigned a p-value by comparing its size to the distribution of sizes of the random permutation’s strongest clusters.</p></sec><sec id="s4-6"><title>Latencies</title><p>Information latency was computed as the time point classifier performance reached half its peak. The peak was estimated as the first local maximum in performance that reached at least 75% of the global performance maximum. To avoid latencies being dominated by those recording sessions containing the most information, we normalized each session’s classifier performance and used only those sessions where the post-stimulus performance peak was at least 1.5 times higher than the largest deviation during pre-stimulus baseline.</p><p>We estimated 95%-confidence intervals using bootstrapping. To statistically assess latency differences between color and motion direction, we used a random permutation procedure. True differences were compared to a distribution of latency differences generated from 10,000 random permutations of the group labels. To test whether latencies in the source-reconstructed monkey EEG and MEG systematically varied along the occipito-frontal gradient, we selected all sources containing significant information (cluster permutation, p&lt;0.05). We then computed Pearson correlation coefficients between the physical location of those sources along the occipito-frontal gradient and the estimated latencies.</p></sec><sec id="s4-7"><title>Luminance control</title><p>To control for possible effects of luminance on color classification, we measured MEG as described above in one human participant during an additional control experiment. For this experiment, we used the same stimulus space as for the main experiment, but additionally included each color at a lower luminance level, such that the luminance contrast between colored dots and background was 20% lower. We then employed the same multivariate classification approach, but split training and test data according to their luminance levels. First, we used only either low-luminance or high-luminance trials for both training and testing. Second, we repeatedly split the color space into two halves, along each possible axis, trained on high-luminance stimuli from the one half and low-luminance stimuli from the other, and tested on the remaining stimuli. We then averaged confusion matrices over all axes, before extracting classification accuracies. To assess statistical significance, we repeated the analysis 100 times after shuffling the stimulus labels; the distribution of accuracies from shuffled data was used to compute p-values for the unshuffled data.</p></sec><sec id="s4-8"><title>Macaque equiluminance control</title><p>As color vision in macaques and humans is slightly different, we performed a psychophysical control experiment in a third macaque monkey to assess if our stimuli were in fact perceptually equiluminant to macaque monkeys. To this end, we used an adapted minimum motion technique using eye-movements as a readout (<xref ref-type="bibr" rid="bib32">Logothetis and Charles, 1990</xref>). We measured small eye movements while the monkey was required to hold fixation on sequentially presented grating stimuli. Each stimulus lasted 500 ms and consisted of a repeating sequence of 4 frames, where frames 1 and 4 contained luminance contrast gratings, whereas frames 2 and 3 contained a contrast between a reference gray of a defined luminance and the probe color we wanted to assess. The phase of each grating proceeded by a quarter cycle with respect to the previous one, such that a probe color of higher luminance than the reference gray would elicit a motion percept in one direction, whereas a probe color of lower luminance would elicit a motion percept in the opposite direction. A probe color of the same luminance as the reference gray should not elicit any consistent perceived motion. Each stimulus was presented in two conditions: In the first condition, a color of higher luminance would elicit upwards motion, in the second one it would elicit downwards motion. We showed stimuli in trials of four, where subsequent stimuli always belonged to the opposite condition. We computed the difference in eye trace curvature – the second derivative of the vertical eye position over time - between conditions as a measure of perceived luminance deviation from the reference gray. We used this procedure for colors of eight hues in L*C*h-space, as in the main experiment. Stimuli of each color were generated at 19 L values, centered around the L value of the reference gray. The reference gray was chosen as the center of the largest possible equiluminant circle in L*C*h-space, such that it was comparable in luminance to the stimuli used in the main experiment. Using linear regression, we assessed at which L value the luminance difference measure crossed 0, which established the point of perceptual equiluminance.</p></sec><sec id="s4-9"><title>Human EEG control</title><p>To assess whether the inverted relationship between color and motion information in monkeys and humans was due to differences between EEG and MEG, we simultaneously measured EEG and MEG in one of the eleven human participants. Identical analyses were performed on the human EEG data, and we compared maximal accuracies for color and motion decoding in all monkey EEG and human MEG sessions as well as the human EEG session.</p></sec><sec id="s4-10"><title>Source reconstruction and searchlight analysis</title><p>To assess the distribution of information in human and macaque brains, we performed source reconstruction on monkey EEG and human MEG data and repeated the multivariate classification in a searchlight fashion. We used structural MRI scans to create individual realistic head models. For MEG source reconstruction, we generated single-shell head models (<xref ref-type="bibr" rid="bib35">Nolte, 2003</xref>). In the case of EEG source reconstruction, we manually extracted the skull and then segmented the brain into gray matter (GM), white matter (WM) and cerebrospinal fluid (CSF) using SPM and fieldtrip toolboxes in combination with probabilistic tissue maps for the macaque brain (<xref ref-type="bibr" rid="bib40">Rohlfing et al., 2012</xref>). We determined the position of the titanium headposts with respect to the head surface using an optical tracking system, and incorporated 3D models of the headposts into our segmentation. These overall six tissue types (WM, GM, CSF, skull, scalp, titanium) were then used to generate detailed finite element head models (FEM) using the SimBio toolbox (<xref ref-type="bibr" rid="bib20">Fingberg et al., 2003</xref>) as implemented in Fieldtrip. We estimated human MEG source activity at 457 and monkey EEG source activity at 517 equally spaced locations on the cortical surface, using linear spatial filtering (<xref ref-type="bibr" rid="bib50">Van Veen et al., 1997</xref>).</p></sec><sec id="s4-11"><title>Representational similarity analysis</title><p>We compared representational structure between brain areas, measurement methods and species using representational similarity analysis. To this end, we computed the temporal average of the confusion matrices over a time period in which stimulus information was available (50–250 ms). Each entry in the resulting matrix gave an approximation of the similarity between stimulus representations. We then performed RSA by correlating matrices, after removing the diagonal. To assess significant similarity, we used a permutation procedure in which we randomly reassigned stimulus labels to the rows and columns of the confusion matrices 10,000 times. P-values were computed as the probability that the similarity between shuffled matrices deviated from zero at least as strongly as the true similarity.</p></sec><sec id="s4-12"><title>Population tuning properties</title><p>From the time-averaged confusion matrices, we extracted several tuning parameters to identify the factors contributing to similarity across scales and species. First, we collapsed confusion matrices across stimuli to obtain tuning curves denoting classifier prediction probability as a function of distance between stimuli. In these tuning curves, a peak at zero indicates a high probability of a stimulus being correctly identified by the classifier, and a peak at 180 degrees indicates an elevated probability of a stimulus being identified as its opposite. We estimated population tuning bimodality by computing the difference between opposite (180 degrees) and next-to-opposite (135, 225 degrees) stimuli normalized by the difference between maximal and minimal prediction probabilities. This bimodality-index is positive in case of a second peak at 180 degrees and zero or negative in case of a unimodal tuning curve. We used t-tests over sessions or subjects to test statistical significance of the bimodality (bimodality-index&gt;0). To estimate the importance of bimodality for representational similarity, we computed the differences in bimodality between all invasively measured areas and signal types. We then used linear regression to determine the amount of variance in the representational similarities explained by these bimodality differences.</p></sec><sec id="s4-13"><title>Single channel tuning</title><p>To estimate average tuning curves of single units, multi-units and LFP channels in each cortical area, we performed one-way ANOVAs on each channel to select those containing information about color or motion direction, respectively, with a statistical threshold of p&lt;0.05. We then computed single-channel tuning curves and aligned them according to their preferred stimulus, determined as the stimulus for which firing rate, or LFP power, was highest. Finally, we computed the mean of all aligned tuning curves within one area, for each signal type. To assess single-unit bimodality, in a given area, we used one-sided t-tests to assess if the above described bimodality index was larger than 0.</p></sec><sec id="s4-14"><title>Software</title><p>All analyses were performed in MATLAB, using custom code as well as the Fieldtrip (<xref ref-type="bibr" rid="bib36">Oostenveld et al., 2011</xref>) and SPM toolboxes.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors thank Nima Noury for help with the monkey EEG recordings.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con2"><p>Investigation, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Funding acquisition</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Methodology, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was conducted in accordance with the Declaration of Helsinki and was approved by the ethics committee of the University of Tuebingen (615/2017BO2). All participants gave written informed consent before participating.</p></fn><fn fn-type="other"><p>Animal experimentation: Microelectrode recordings were performed in two adult rhesus macaques. All procedures followed the guidelines of the Massachusetts Institute of Technology Committee on Animal Care and the National Institutes of Health. Scalp EEG was measured in two male adult rhesus monkeys. All procedures were approved by local authorities (Regierungspräsidium Tübingen, CIN 3/14).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.45645.011</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-45645-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data and MATLAB code required to reproduce all figures are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/tuhsk/">https://osf.io/tuhsk/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Florian</surname><given-names>Sandhaeger</given-names></name><name><surname>Constantin</surname><given-names>von Nicolai</given-names></name><name><surname>Earl</surname><given-names>K Miller</given-names></name><name><surname>Markus</surname><given-names>Siegel</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Monkey EEG links neuronal color and motion information across species and scales</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/OSF.IO/TUHSK</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albright</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Direction and orientation selectivity of neurons in visual area MT of the macaque</article-title><source>Journal of Neurophysiology</source><volume>52</volume><fpage>1106</fpage><lpage>1130</lpage><pub-id pub-id-type="doi">10.1152/jn.1984.52.6.1106</pub-id><pub-id pub-id-type="pmid">6520628</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albright</surname> <given-names>TD</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name><name><surname>Gross</surname> <given-names>CG</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Columnar organization of directionally selective cells in visual area MT of the macaque</article-title><source>Journal of Neurophysiology</source><volume>51</volume><fpage>16</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1152/jn.1984.51.1.16</pub-id><pub-id pub-id-type="pmid">6693933</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>An</surname> <given-names>X</given-names></name><name><surname>Gong</surname> <given-names>H</given-names></name><name><surname>Qian</surname> <given-names>L</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Pan</surname> <given-names>Y</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Yang</surname> <given-names>Y</given-names></name><name><surname>Wang</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Distinct functional organizations for processing different motion signals in V1, V2, and V4 of macaque</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>13363</fpage><lpage>13379</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1900-12.2012</pub-id><pub-id pub-id-type="pmid">23015427</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaad</surname> <given-names>WF</given-names></name><name><surname>Santhanam</surname> <given-names>N</given-names></name><name><surname>McClellan</surname> <given-names>S</given-names></name><name><surname>Freedman</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>High-performance execution of psychophysical tasks with complex visual stimuli in MATLAB</article-title><source>Journal of Neurophysiology</source><volume>109</volume><fpage>249</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1152/jn.00527.2012</pub-id><pub-id pub-id-type="pmid">23034363</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bae</surname> <given-names>GY</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Decoding motion direction using the topography of sustained ERPs and alpha oscillations</article-title><source>NeuroImage</source><volume>184</volume><fpage>242</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.09.029</pub-id><pub-id pub-id-type="pmid">30223063</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bimbi</surname> <given-names>M</given-names></name><name><surname>Festante</surname> <given-names>F</given-names></name><name><surname>Coudé</surname> <given-names>G</given-names></name><name><surname>Vanderwert</surname> <given-names>RE</given-names></name><name><surname>Fox</surname> <given-names>NA</given-names></name><name><surname>Ferrari</surname> <given-names>PF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Simultaneous scalp recorded EEG and local field potentials from monkey ventral premotor cortex during action observation and execution reveals the contribution of mirror and motor neurons to the mu-rhythm</article-title><source>NeuroImage</source><volume>175</volume><fpage>22</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.03.037</pub-id><pub-id pub-id-type="pmid">29571717</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Born</surname> <given-names>RT</given-names></name><name><surname>Bradley</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Structure and function of visual area MT</article-title><source>Annual Review of Neuroscience</source><volume>28</volume><fpage>157</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.26.041002.131052</pub-id><pub-id pub-id-type="pmid">16022593</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname> <given-names>GJ</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Categorical clustering of the neural representation of color</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>15454</fpage><lpage>15465</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2472-13.2013</pub-id><pub-id pub-id-type="pmid">24068814</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname> <given-names>T</given-names></name><name><surname>Goddard</surname> <given-names>E</given-names></name><name><surname>Kaplan</surname> <given-names>DM</given-names></name><name><surname>Klein</surname> <given-names>C</given-names></name><name><surname>Ritchie</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Ghosts in machine learning for cognitive neuroscience: moving from data to theory</article-title><source>NeuroImage</source><volume>180</volume><fpage>88</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.019</pub-id><pub-id pub-id-type="pmid">28793239</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Ramirez</surname> <given-names>FM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Can visual information encoded in cortical columns be decoded from magnetoencephalography data in humans?</article-title><source>NeuroImage</source><volume>121</volume><fpage>193</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.07.011</pub-id><pub-id pub-id-type="pmid">26162550</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Similarity-Based fusion of MEG and fMRI reveals Spatio-Temporal dynamics in human cortex during visual object recognition</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3563</fpage><lpage>3579</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw135</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Deep neural networks predict hierarchical Spatio-temporal cortical dynamics of human visual object recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1601.02970">https://arxiv.org/abs/1601.02970</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MX</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Where does EEG come from and what does it mean?</article-title><source>Trends in Neurosciences</source><volume>40</volume><fpage>208</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2017.02.004</pub-id><pub-id pub-id-type="pmid">28314445</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conway</surname> <given-names>BR</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Color-tuned neurons are spatially clustered according to color preference within alert macaque posterior inferior temporal cortex</article-title><source>PNAS</source><volume>106</volume><fpage>18034</fpage><lpage>18039</lpage><pub-id pub-id-type="doi">10.1073/pnas.0810943106</pub-id><pub-id pub-id-type="pmid">19805195</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darvas</surname> <given-names>F</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Kucukaltun-Yildirim</surname> <given-names>E</given-names></name><name><surname>Leahy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Mapping human brain function with MEG and EEG: methods and validation</article-title><source>NeuroImage</source><volume>23 Suppl 1</volume><fpage>S289</fpage><lpage>S299</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.014</pub-id><pub-id pub-id-type="pmid">15501098</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donner</surname> <given-names>TH</given-names></name><name><surname>Siegel</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A framework for local cortical oscillation patterns</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>191</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.03.007</pub-id><pub-id pub-id-type="pmid">21481630</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dotson</surname> <given-names>NM</given-names></name><name><surname>Hoffman</surname> <given-names>SJ</given-names></name><name><surname>Goodell</surname> <given-names>B</given-names></name><name><surname>Gray</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Large-Scale Semi-Chronic Microdrive Recording System for Non-Human Primates</article-title><source>Neuron</source><volume>96</volume><fpage>769</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.050</pub-id><pub-id pub-id-type="pmid">29107523</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fingberg</surname> <given-names>J</given-names></name><name><surname>Berti</surname> <given-names>G</given-names></name><name><surname>Hartmann</surname> <given-names>U</given-names></name><name><surname>Basermann</surname> <given-names>A</given-names></name><name><surname>Wolters</surname> <given-names>CH</given-names></name><name><surname>Anwander</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Bio-numerical simulations with SimBio</article-title><source>NEC Research and Development</source><volume>44</volume><fpage>140</fpage><lpage>145</lpage></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gagin</surname> <given-names>G</given-names></name><name><surname>Bohon</surname> <given-names>KS</given-names></name><name><surname>Butensky</surname> <given-names>A</given-names></name><name><surname>Gates</surname> <given-names>MA</given-names></name><name><surname>Hu</surname> <given-names>JY</given-names></name><name><surname>Lafer-Sousa</surname> <given-names>R</given-names></name><name><surname>Pulumo</surname> <given-names>RL</given-names></name><name><surname>Qu</surname> <given-names>J</given-names></name><name><surname>Stoughton</surname> <given-names>CM</given-names></name><name><surname>Swanbeck</surname> <given-names>SN</given-names></name><name><surname>Conway</surname> <given-names>BR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Color-detection thresholds in rhesus macaque monkeys and humans</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.1167/14.8.12</pub-id><pub-id pub-id-type="pmid">25027164</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Godlove</surname> <given-names>DC</given-names></name><name><surname>Emeric</surname> <given-names>EE</given-names></name><name><surname>Segovis</surname> <given-names>CM</given-names></name><name><surname>Young</surname> <given-names>MS</given-names></name><name><surname>Schall</surname> <given-names>JD</given-names></name><name><surname>Woodman</surname> <given-names>GF</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Event-related potentials elicited by errors during the stop-signal task. I. macaque monkeys</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>15640</fpage><lpage>15649</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3349-11.2011</pub-id><pub-id pub-id-type="pmid">22049407</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname> <given-names>T</given-names></name><name><surname>Tibshirani</surname> <given-names>R</given-names></name><name><surname>Friedman</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>The Elements of Statistical Learning</source><publisher-loc>New York, NY</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Bankson</surname> <given-names>BB</given-names></name><name><surname>Harel</surname> <given-names>A</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The representational dynamics of task and object processing in humans</article-title><source>eLife</source><volume>7</volume><elocation-id>e32816</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id><pub-id pub-id-type="pmid">29384473</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname> <given-names>JJ</given-names></name><name><surname>Steinmetz</surname> <given-names>NA</given-names></name><name><surname>Siegle</surname> <given-names>JH</given-names></name><name><surname>Denman</surname> <given-names>DJ</given-names></name><name><surname>Bauza</surname> <given-names>M</given-names></name><name><surname>Barbarits</surname> <given-names>B</given-names></name><name><surname>Lee</surname> <given-names>AK</given-names></name><name><surname>Anastassiou</surname> <given-names>CA</given-names></name><name><surname>Andrei</surname> <given-names>A</given-names></name><name><surname>Aydın</surname> <given-names>Ç</given-names></name><name><surname>Barbic</surname> <given-names>M</given-names></name><name><surname>Blanche</surname> <given-names>TJ</given-names></name><name><surname>Bonin</surname> <given-names>V</given-names></name><name><surname>Couto</surname> <given-names>J</given-names></name><name><surname>Dutta</surname> <given-names>B</given-names></name><name><surname>Gratiy</surname> <given-names>SL</given-names></name><name><surname>Gutnisky</surname> <given-names>DA</given-names></name><name><surname>Häusser</surname> <given-names>M</given-names></name><name><surname>Karsh</surname> <given-names>B</given-names></name><name><surname>Ledochowitsch</surname> <given-names>P</given-names></name><name><surname>Lopez</surname> <given-names>CM</given-names></name><name><surname>Mitelut</surname> <given-names>C</given-names></name><name><surname>Musa</surname> <given-names>S</given-names></name><name><surname>Okun</surname> <given-names>M</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Putzeys</surname> <given-names>J</given-names></name><name><surname>Rich</surname> <given-names>PD</given-names></name><name><surname>Rossant</surname> <given-names>C</given-names></name><name><surname>Sun</surname> <given-names>WL</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamitani</surname> <given-names>Y</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Decoding seen and attended motion directions from activity in the human visual cortex</article-title><source>Current Biology</source><volume>16</volume><fpage>1096</fpage><lpage>1102</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.04.003</pub-id><pub-id pub-id-type="pmid">16753563</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Ruff</surname> <given-names>DA</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Bodurka</surname> <given-names>J</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>P</given-names></name><name><surname>Zhu</surname> <given-names>S</given-names></name><name><surname>Chen</surname> <given-names>M</given-names></name><name><surname>Han</surname> <given-names>C</given-names></name><name><surname>Xu</surname> <given-names>H</given-names></name><name><surname>Hu</surname> <given-names>J</given-names></name><name><surname>Fang</surname> <given-names>Y</given-names></name><name><surname>Lu</surname> <given-names>HD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A motion direction preference map in monkey V4</article-title><source>Neuron</source><volume>78</volume><fpage>376</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.02.024</pub-id><pub-id pub-id-type="pmid">23622068</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindbloom-Brown</surname> <given-names>Z</given-names></name><name><surname>Tait</surname> <given-names>LJ</given-names></name><name><surname>Horwitz</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spectral sensitivity differences between rhesus monkeys and humans: implications for neurophysiology</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>3164</fpage><lpage>3172</lpage><pub-id pub-id-type="doi">10.1152/jn.00356.2014</pub-id><pub-id pub-id-type="pmid">25253473</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>T</given-names></name><name><surname>Cable</surname> <given-names>D</given-names></name><name><surname>Gardner</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inverted Encoding Models of Human Population Response Conflate Noise and Neural Tuning Width</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>398</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2453-17.2017</pub-id><pub-id pub-id-type="pmid">29167406</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Charles</surname> <given-names>ER</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>The minimum motion technique applied to determine isoluminance in psychophysical experiments with monkeys</article-title><source>Vision Research</source><volume>30</volume><fpage>829</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(90)90052-M</pub-id><pub-id pub-id-type="pmid">2385924</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mendoza-Halliday</surname> <given-names>D</given-names></name><name><surname>Torres</surname> <given-names>S</given-names></name><name><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sharp emergence of feature-selective sustained activity along the dorsal visual pathway</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1255</fpage><lpage>1262</lpage><pub-id pub-id-type="doi">10.1038/nn.3785</pub-id><pub-id pub-id-type="pmid">25108910</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname> <given-names>S</given-names></name><name><surname>von Pföstl</surname> <given-names>V</given-names></name><name><surname>Rauch</surname> <given-names>A</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Whittingstall</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Effects of neural synchrony on surface EEG</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>1045</fpage><lpage>1053</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs389</pub-id><pub-id pub-id-type="pmid">23236202</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolte</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title><source>Physics in Medicine and Biology</source><volume>48</volume><fpage>3637</fpage><lpage>3652</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/48/22/002</pub-id><pub-id pub-id-type="pmid">14680264</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesaran</surname> <given-names>B</given-names></name><name><surname>Vinck</surname> <given-names>M</given-names></name><name><surname>Einevoll</surname> <given-names>GT</given-names></name><name><surname>Sirota</surname> <given-names>A</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Siegel</surname> <given-names>M</given-names></name><name><surname>Truccolo</surname> <given-names>W</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Srinivasan</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Investigating large-scale brain dynamics using field potential recordings: analysis and interpretation</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>903</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0171-8</pub-id><pub-id pub-id-type="pmid">29942039</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinhart</surname> <given-names>RM</given-names></name><name><surname>Heitz</surname> <given-names>RP</given-names></name><name><surname>Purcell</surname> <given-names>BA</given-names></name><name><surname>Weigand</surname> <given-names>PK</given-names></name><name><surname>Schall</surname> <given-names>JD</given-names></name><name><surname>Woodman</surname> <given-names>GF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Homologous mechanisms of visuospatial working memory maintenance in macaque and human: properties and sources</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>7711</fpage><lpage>7722</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0215-12.2012</pub-id><pub-id pub-id-type="pmid">22649249</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roe</surname> <given-names>AW</given-names></name><name><surname>Chelazzi</surname> <given-names>L</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name><name><surname>Conway</surname> <given-names>BR</given-names></name><name><surname>Fujita</surname> <given-names>I</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name><name><surname>Lu</surname> <given-names>H</given-names></name><name><surname>Vanduffel</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Toward a unified theory of visual area V4</article-title><source>Neuron</source><volume>74</volume><fpage>12</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.011</pub-id><pub-id pub-id-type="pmid">22500626</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohlfing</surname> <given-names>T</given-names></name><name><surname>Kroenke</surname> <given-names>CD</given-names></name><name><surname>Sullivan</surname> <given-names>EV</given-names></name><name><surname>Dubach</surname> <given-names>MF</given-names></name><name><surname>Bowden</surname> <given-names>DM</given-names></name><name><surname>Grant</surname> <given-names>KA</given-names></name><name><surname>Pfefferbaum</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The INIA19 template and NeuroMaps atlas for primate brain image parcellation and spatial normalization</article-title><source>Frontiers in Neuroinformatics</source><volume>6</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2012.00027</pub-id><pub-id pub-id-type="pmid">23230398</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname> <given-names>M</given-names></name><name><surname>Kemper</surname> <given-names>VG</given-names></name><name><surname>Emmerling</surname> <given-names>TC</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Columnar clusters in the human motion complex reflect consciously perceived motion axis</article-title><source>PNAS</source><volume>116</volume><fpage>5096</fpage><lpage>5101</lpage><pub-id pub-id-type="doi">10.1073/pnas.1814504116</pub-id><pub-id pub-id-type="pmid">30808809</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherman</surname> <given-names>MA</given-names></name><name><surname>Lee</surname> <given-names>S</given-names></name><name><surname>Law</surname> <given-names>R</given-names></name><name><surname>Haegens</surname> <given-names>S</given-names></name><name><surname>Thorn</surname> <given-names>CA</given-names></name><name><surname>Hämäläinen</surname> <given-names>MS</given-names></name><name><surname>Moore</surname> <given-names>CI</given-names></name><name><surname>Jones</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural mechanisms of transient neocortical beta rhythms: converging evidence from humans, computational modeling, monkeys, and mice</article-title><source>PNAS</source><volume>113</volume><fpage>E4885</fpage><lpage>E4894</lpage><pub-id pub-id-type="doi">10.1073/pnas.1604135113</pub-id><pub-id pub-id-type="pmid">27469163</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname> <given-names>H</given-names></name><name><surname>Law</surname> <given-names>R</given-names></name><name><surname>Tsutsui</surname> <given-names>S</given-names></name><name><surname>Moore</surname> <given-names>CI</given-names></name><name><surname>Jones</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The rate of transient beta frequency events predicts behavior across tasks and species</article-title><source>eLife</source><volume>6</volume><elocation-id>e29086</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.29086</pub-id><pub-id pub-id-type="pmid">29106374</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname> <given-names>M</given-names></name><name><surname>Donner</surname> <given-names>TH</given-names></name><name><surname>Engel</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spectral fingerprints of large-scale neuronal interactions</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>121</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1038/nrn3137</pub-id><pub-id pub-id-type="pmid">22233726</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname> <given-names>M</given-names></name><name><surname>Buschman</surname> <given-names>TJ</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical information flow during flexible sensorimotor decisions</article-title><source>Science</source><volume>348</volume><fpage>1352</fpage><lpage>1355</lpage><pub-id pub-id-type="doi">10.1126/science.aab0551</pub-id><pub-id pub-id-type="pmid">26089513</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname> <given-names>AC</given-names></name><name><surname>Morais</surname> <given-names>MJ</given-names></name><name><surname>Willis</surname> <given-names>CM</given-names></name><name><surname>Smith</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Global network influences on local functional connectivity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>736</fpage><lpage>743</lpage><pub-id pub-id-type="doi">10.1038/nn.3979</pub-id><pub-id pub-id-type="pmid">25799040</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname> <given-names>AC</given-names></name><name><surname>Issar</surname> <given-names>D</given-names></name><name><surname>Smith</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What does scalp electroencephalogram coherence tell Us about long-range cortical networks?</article-title><source>European Journal of Neuroscience</source><volume>48</volume><fpage>2466</fpage><lpage>2481</lpage><pub-id pub-id-type="doi">10.1111/ejn.13840</pub-id><pub-id pub-id-type="pmid">29363843</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname> <given-names>TC</given-names></name><name><surname>Adam</surname> <given-names>KCS</given-names></name><name><surname>Foster</surname> <given-names>JJ</given-names></name><name><surname>Rahmati</surname> <given-names>M</given-names></name><name><surname>Sutterer</surname> <given-names>DW</given-names></name><name><surname>Vo</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inverted encoding models assay Population-Level stimulus representations, not Single-Unit neural tuning</article-title><source>Eneuro</source><volume>5</volume><elocation-id>ENEURO.0098-18.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0098-18.2018</pub-id><pub-id pub-id-type="pmid">29876523</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanigawa</surname> <given-names>H</given-names></name><name><surname>Lu</surname> <given-names>HD</given-names></name><name><surname>Roe</surname> <given-names>AW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional organization for color and orientation in macaque V4</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1542</fpage><lpage>1548</lpage><pub-id pub-id-type="doi">10.1038/nn.2676</pub-id><pub-id pub-id-type="pmid">21076422</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname> <given-names>BD</given-names></name><name><surname>van Drongelen</surname> <given-names>W</given-names></name><name><surname>Yuchtman</surname> <given-names>M</given-names></name><name><surname>Suzuki</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>44</volume><fpage>867</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1109/10.623056</pub-id><pub-id pub-id-type="pmid">9282479</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wardle</surname> <given-names>SG</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Grootswagers</surname> <given-names>T</given-names></name><name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual similarity of visual patterns predicts dynamic neural activation patterns measured with MEG</article-title><source>NeuroImage</source><volume>132</volume><fpage>59</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.02.019</pub-id><pub-id pub-id-type="pmid">26899210</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittingstall</surname> <given-names>K</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Frequency-band coupling in surface EEG reflects spiking activity in monkey visual cortex</article-title><source>Neuron</source><volume>64</volume><fpage>281</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.08.016</pub-id><pub-id pub-id-type="pmid">19874794</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodman</surname> <given-names>GF</given-names></name><name><surname>Kang</surname> <given-names>MS</given-names></name><name><surname>Rossi</surname> <given-names>AF</given-names></name><name><surname>Schall</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonhuman primate event-related potentials indexing covert shifts of attention</article-title><source>PNAS</source><volume>104</volume><fpage>15111</fpage><lpage>15116</lpage><pub-id pub-id-type="doi">10.1073/pnas.0703477104</pub-id><pub-id pub-id-type="pmid">17848520</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmermann</surname> <given-names>J</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Feinberg</surname> <given-names>D</given-names></name><name><surname>Adriany</surname> <given-names>G</given-names></name><name><surname>Chaimow</surname> <given-names>D</given-names></name><name><surname>Shmuel</surname> <given-names>A</given-names></name><name><surname>Uğurbil</surname> <given-names>K</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mapping the organization of axis of motion selective features in human area MT using high-field fMRI</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e28716</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0028716</pub-id><pub-id pub-id-type="pmid">22163328</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.45645.015</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Pasternak</surname><given-names>Tatiana</given-names></name><role>Reviewing Editor</role><aff><institution>University of Rochester</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Wimmer</surname><given-names>Klaus</given-names> </name><role>Reviewer</role><aff><institution>Centre de Recerca Matemàtica</institution><country>Spain</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Tootell</surname><given-names>Roger</given-names> </name><role>Reviewer</role><aff><institution>Harvard Medical School</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Monkey EEG links neuronal color and motion information across species and scales&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Klaus Wimmer (Reviewer #1); Roger Tootell (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>Your manuscript was well received by both reviewers. They each commented on the pioneering nature of the work comparing neural signals recorded at different spatial scales in humans and macaques and on its relevance to the wider neuroscience community. However, they raised a number of issues related to analysis and decoding of neuronal activity recorded in monkeys. In addition, there were reservations concerning the interpretation of the underlying neural selectivity and its sources. These issues must be addressed before the paper will be considered for publication in <italic>eLife</italic> and are listed below.</p><p>Essential revisions:</p><p>1) Please address the reasons for the low decoding accuracy of MT activity compared to the published reports</p><p>2) Please comment on the significance levels for the low accuracy decoding results. Please perform comparable cross validation for the neuronal recordings and for the EEG and MEG, as suggested by reviewer 1.</p><p>3) Please address the possibility that stimulus representation in higher brain regions may be weakened in passively fixating subjects.</p><p>4) Reviewer 2 pointed out that direction-selective activity is not limited to neurons residing in area MT, which in rhesus monkeys is a small and deep structure, compared to other possible sources of direction selective activity in visual cortex. Please address the possibility of other sources of direction selectivity in the EEG activity recorded in monkeys and MEG in humans.</p><p>5) Please correct the placement of the sampling site of area V4 in Figure 1C and the point that color selective signals are not confined to area V4.</p><p>6) Please provide a detailed description of the color stimulus.</p><p>7) In the Discussion, please address reviewer 2's comment on bimodality. This reviewer highlights a possibility that direction columns have a spacing much smaller than that for axis of motion, a possibility raised by the early columnar models in macaque (Albright et al., 1984; Born and Bradley, 2005). A possibility supported by high resolution fMRI data that the &quot;axis of motion&quot; columns in human MT/V5 may include neurons of opposite direction selectivity (Zimmerman et al., 2011; Schneider et al., 2019). If the direction columns are significantly smaller than those of axis of motion, the source localization and measurements of bimodality may differ from each other in MT and other areas. In this context, the work of An et al., 2012 pointing to sources of motion signals in cortical areas outside MT should also be considered.</p><p>8) In Figure 2, please change blue and green colors for different cortical areas, to make the easier to distinguish and improve the visibility of the two lines in Figure 2A.</p><p>9) Provide the missing details highlighted by reviewer 1.</p><p>Complete reviews for your information:</p><p><italic>Reviewer #1:</italic></p><p>This manuscript investigates the representation of color and motion direction in human and macaque cerebral cortex, using: (i) simultaneous microelectrode recordings from 6 cortical areas in the macaque, (ii) high-density EEG again in the macaque, and (iii) MEG and EEG in humans. It is argued that information about motion and color is present in all signal types and both species. The pattern of stimulus encoding in EEG and MEG signals was similar to microelectrode recordings in ventral visual areas but not dorsal or frontal areas, clarifying thus the origin of color and motion information in the non-invasive recordings.</p><p>The combination of invasive and non-invasive recordings in humans and in macaque is an experimental tour de force and relating these signals obtained at different spatial scales is of broad interest to the systems neuroscience community. This is pioneering work. I have read the manuscript with great interest. Relating the EEG and MEG signals to tuning properties of neurons in V4 and MT (direction vs. orientation-tuned) is a convincing example of the usefulness of this approach.</p><p>However, I think there are some limitations of the study which should be discussed, and I have also identified some issues which need clarification.</p><p>1) The decoding accuracy for single-unit and multi-unit activity is very low and this puzzles me. Let's take MT as an example: coding accuracy is well below 0.2 in all cases (chance level is at 0.125). We know that the majority of neurons in MT is tuned to motion direction and 100% coherent random dot stimuli are perhaps the stimuli that best drives these neurons. Thus, I expected a decoding accuracy for 1 out of 8 directions close to 1.0 in MT. In fact, Mendoza-Halliday et al., 2014 reported an accuracy &gt; 0.5 even in PFC. The single unit tuning curves in Figure 7F also show high selectivity. What explains the low accuracy in the manuscript (e.g. Figure 2)? Is it because of the bandpass filtering of spike trains? I think in order to properly relate invasive and non-invasive recording it must be assured that both measurements are consistent with previous literature.</p><p>2) Statistical significance of decoding results. Most of the decoding accuracies shown in Figure 2 are really low, sometimes ~0.1255 to 0.126 when the chance level is 0.125. Frankly, I don't understand how such small effects can be significant given the number of subjects and trials. It is crucial to confirm that some of the tiny effects are not caused by confounds such as any small imbalance in the data that could be detected by the decoder (unequal trial numbers, differences between individuals, etc.). At the very least I would suggest running the whole decoding analysis on data with shuffled stimulus class labels. This should not yield significant decoding apart from a few false positives. More detailed questions concerning the statistical procedures: Why only a 2-fold cross validation for SUA and MUA and not 10-fold as for EEG and MEG? Cluster permutation procedure (subsection “Multivariate classification”, last paragraph): I do not understand the rationale of multiplying with +1 or -1. This should be explained better and a reference should be given. Finally, is the use of t-tests appropriate here?</p><p>3) One limitation of the study is that subjects view the random dot patterns just passively. This may reduce the stimulus representation in higher brain areas (see e.g. work of Romo and Pasternak with active and passive tasks). This should be discussed.</p><p>4) Data availability. The authors have put the data points that are necessary to plot the figures in a repository, but this is of limited use for the community. I would strongly suggest making the full data available for further analysis. I hope this request is consistent with the spirit of <italic>eLife</italic>. The main advance of this manuscript is methodological, and I believe that providing this data as a resource to the community would be a further argument for publishing it in <italic>eLife</italic>.</p><p><italic>Reviewer #2:</italic></p><p>General:</p><p>The overall goal of synthesizing work from humans and macaques, and across different functional tools (e.g. EEG, electrophysiology and MEG), and across spatial scales, is very important. In that sense alone, this study is valuable and should have impact. Also, the analysis is relatively sophisticated and quantitative. However, the biological interpretation of the underlying neural selectivity is a little naïve, which weakens support for the overall conclusions. Presumably, the authors could mitigate the latter concern by revising the text accordingly.</p><p>Specific:</p><p>Based on the wealth of prior research in both macaques and humans, it is well accepted that most/many neurons in area MT/V5 respond to stimuli in a direction-selective manner. However it is important to remember that area MT/V5 is a relatively small area, and not located on the cortical surface (thus arguably less amenable to accurate source analysis). More importantly, many other cortical areas (in areas V1, V2, V3, V3A, MSTd/l, etc.) also have many direction-selective neurons (or direction-activated) neurons, and most of those areas are much larger compared to MT/V5. Thus it seems likely that such areas contribute significantly to the 'motion' contrast tested here; i.e. it was not produced entirely (or perhaps even preferentially) in area MT/V5.</p><p>The attribution of color-selective responses to 'V4' is less certain. For one thing, the oval indicating the 'V4' sampling site in macaque (Figure 1C) is overlaid on foveal V1/V2, not V4. The confusion is partly due to the presence of a very small gyrus (between the operculum and lunate gyrus) that connects the foveal representations of V1, V2, V3 and V4. A second concern is that color selective neurons and columns/patches/responses are not confined to (nor prominently present in) area V4; that is a persistent myth based on a few recordings made in the 1970s. Subsequent research has demonstrated many color responsive columns/patches/response in many other areas, (although not in MT). Thirdly, the color stimulus in 1A appears to be a mix of black dots and colored dots – i.e. a stimulus which varies in both luminance and color. [However I could not find a direct description of the color stimulus – so I am not certain about the stimulus]. Fourth, the color selectivity in humans (e.g. the L*C*h* space) differs from that in macaques, partly because the ratio of long- to medium-wavelength cones varies by a factor of two between these two species.</p><p>Thus, I conclude that the measured responses to these different stimuli may well differ as reported in the recording sites, consistent with the authors interpretation. However, the simple attribution of color-vs.-motion selectivity to MT and V4 needs to be significantly qualified. Presumably if the authors had been able to directly map the cortical sites that respond selectively to color or motion everywhere in visual cortex, the source localization would have been more certain.</p><p>The source localization of MEG and EEG information is well known to be roughly inverted, differentially maximal from sulci vs. gyri in the two measurements. This issue is crucial in the determination of source localization in brain. The authors might explore and discuss this in more detail; here I instead got the impression that the EEG and MEG signals were similar.</p><p>The extensive discussion of bi-modality (vs. lack thereof) in color vs. motion may change during the revision. However, it may be relevant that more recent attempts to map 'direction' columns (i.e. a dimension of 360 degrees) in cortex have yielded only axis-of-motion columns (a 180 degree dimension). Also, as briefly discussed by the authors, attempts to model color-selective cortical responses as psychophysically-color-opponent signals have not been entirely successful. These backgrounds may clarify the bi-modality discussion.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.45645.016</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Please address the reasons for the low decoding accuracy of MT activity compared to the published reports.</p></disp-quote><p>We thank the reviewers for bringing this to our attention. We added a paragraph discussing the availability and magnitude of information across cortical areas, with a focus on MT and PFC, to the Discussion section (subsection “Widespread representations of visual features across cortex”).</p><p>Briefly, there are several methodological differences that we believe account for the lower decoding accuracies in MT and PFC than in Mendoza-Halliday et. Al., 2014. While they decode from larger pseudo-populations of 30 not simultaneously recorded neurons, we decode from populations of 5 simultaneously recorded signals. Furthermore, our paradigm consisted of a rapid succession of very short, only passively fixated stimuli, which arguably does not maximize decodability. Lastly, we report the average of single-trial LDA probabilities instead of discrete classification results. When using discrete classifier outputs, the information in each trial is reduced to which of the class distributions it lies closest to, whereas LDA probabilities retain continuous information about the relative distances to all classes. This made our information estimates more robust, but decreased decoding accuracies.</p><disp-quote content-type="editor-comment"><p>2) Please comment on the significance levels for the low accuracy decoding results. Please perform comparable cross validation for the neuronal recordings and for the EEG and MEG, as suggested by reviewer 1.</p></disp-quote><p>We thank the reviewers for this comment, which made us realize that we did not sufficiently describe our statistical procedures. As suggested by reviewer 1, we expanded on our explanation of the sign permutation test in the Materials and methods section and added a reference for a paper using a similar test in a similar context.</p><p>Such a test (performed on the session level) can result in statistical significance for low decoding accuracies, if these are consistently above chance across recording sessions. To double-check the results, we performed the identical analysis on trial-shuffled data, as suggested by reviewer 1, which revealed – as expected – no significant clusters for any combination of visual feature, area, and measurement method. Thus, there are no false positive results. The results of this control analysis are shown in <xref ref-type="fig" rid="respfig1">Author response image 1</xref>.</p><fig id="respfig1"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45645-resp-fig1-v1.tif"/></fig><p>Furthermore, we added an explanation of the use of the 2-fold cross-validation procedure in the invasive data to the Materials and methods section (subsection “Multivariate classification”, second paragraph). Briefly, this was done to enable the control of a potential confounding factor: Stimuli were presented in groups of 6 or 8 at the beginning of each trial of another task, but different stimuli were not fully balanced over sequential positions within these groups. As this could have led to a confound of stimulus information with sequence position, we chose to use a stratification approach in which the number of occurrences of each individual stimulus was kept the same for all sequence positions in each cross-validation fold. Due to the overall low number of trials, 10-fold cross-validation did not result in sufficient trials per fold for this approach. However, due to the larger number of recording sessions as compared to the non-invasive data, we could still arrive at robust information estimates with 2-fold cross-validation, and the essential logic of independent training and test sets was not affected by using 2 instead of 10 folds.</p><p>We agree with the reviewer that to achieve maximal comparability, the identical analysis as performed in the non-invasive data is of interest. We therefore repeated the decoding of aMUA, SUA and LFP data using a 10-fold cross-validation, without the serial position stratification, with highly similar results in terms of overall decoding accuracy and statistical significance, with the exception of occasional minimal amounts of baseline information (see e.g. V4 LFP) which we assume to be the result of the confound described above. The results of this control analysis are attached as <xref ref-type="fig" rid="respfig2">Author response image 2</xref>.</p><fig id="respfig2"><label>Author response image 2.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45645-resp-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>3) Please address the possibility that stimulus representation in higher brain regions may be weakened in passively fixating subjects.</p></disp-quote><p>We thank the reviewers for this comment, that we fully agree with. We have incorporated a short consideration into the same paragraph discussing decoding strength in different cortical areas (see 2., and subsection “Widespread representations of visual features across cortex”).</p><disp-quote content-type="editor-comment"><p>4) Reviewer 2 pointed out that direction-selective activity is not limited to neurons residing in area MT, which in rhesus monkeys is a small and deep structure, compared to other possible sources of direction selective activity in visual cortex. Please address the possibility of other sources of direction selectivity in the EEG activity recorded in monkeys and MEG in humans.</p></disp-quote><p>We fully agree with the reviewer that, also because of the size and positioning of MT, the motion direction information in EEG/MEG was likely also influenced by other areas than MT. To clarify this, we have adapted parts of the Discussion section: first, we state more clearly now that not only the areas that we recorded from, but also many other areas are selective for both motion and color and could therefore contribute to EEG/MEG decoding. Secondly, (mainly in response to point 7 below), we have expanded on the consequences of anatomical organization within MT (subsection “Early ventral stream areas as sources of non-invasive information”).</p><disp-quote content-type="editor-comment"><p>5) Please correct the placement of the sampling site of area V4 in Figure 1C and the point that color selective signals are not confined to area V4.</p></disp-quote><p>We have corrected the placement of V4 in the schematic and clarified in the Discussion that both color and motion information was found in all areas we recorded from, but is also expected in many other areas (see also response to point 4 above).</p><disp-quote content-type="editor-comment"><p>6) Please provide a detailed description of the color stimulus.</p></disp-quote><p>We added the missing information about the stimulus background color to our description of the stimuli in the Materials and methods section / Macaque microelectrode recordings / Stimuli and Apparatus. As the reviewer correctly points out, each stimulus contained both luminance and chromaticity contrasts. However, both of these were kept constant over stimuli, while the only manipulated features were hue and motion direction, which we subsequently decoded from the neural data.</p><disp-quote content-type="editor-comment"><p>7) In the Discussion, please address reviewer 2's comment on bimodality. This reviewer highlights a possibility that direction columns have a spacing much smaller than that for axis of motion, a possibility raised by the early columnar models in macaque (Albright et al., 1984; Born and Bradley, 2005). A possibility supported by high resolution fMRI data that the &quot;axis of motion&quot; columns in human MT/V5 may include neurons of opposite direction selectivity (Zimmerman et al., 2011; Schneider et al., 2019). If the direction columns are significantly smaller than those of axis of motion, the source localization and measurements of bimodality may differ from each other in MT and other areas. In this context, the work of An et al., 2012 pointing to sources of motion signals in cortical areas outside MT should also be considered.</p></disp-quote><p>We thank the reviewer for bringing up this important possibility. We added a paragraph to the Discussion section exploring the consequences of columns with opposite motion direction selectivity being in close proximity in MT (subsection “Early ventral stream areas as sources of non-invasive information”, third paragraph). Briefly, we consider it unlikely that the bimodally tuned motion information measures in EEG/MEG was mainly driven by MT activity, but we point out that our data does not support a definitive answer to this question.</p><disp-quote content-type="editor-comment"><p>8) In Figure 2, please change blue and green colors for different cortical areas, to make the easier to distinguish and improve the visibility of the two lines in Figure 2A.</p></disp-quote><p>We exchanged the colors to enhance the differences between cortical areas, while maintaining our overall goals of a consistent color scheme and of a continuous color gradient. Additionally, we moved the least visible line into the foreground, making it somewhat more visible. However, as the two lines are very similar, full visibility is challenging. We therefore added a note to the figure legend.</p><disp-quote content-type="editor-comment"><p>9) Provide the missing details highlighted by reviewer 1.</p></disp-quote><p>We added the missing details, specifically we 1) explained the distinction between error bars over sessions in invasive macaque recordings and monkey EEG recordings, and error bars over participants in human MEG, 2) added information about the unrelated delayed saccade task (Materials and methods / Macaque microelectrode recordings / Stimuli and Apparatus), 3) made the requested changes to the color scheme / line visibility.</p><disp-quote content-type="editor-comment"><p>Complete reviews for your information:</p><p>Reviewer #1:</p><p>[…] I think there are some limitations of the study which should be discussed, and I have also identified some issues which need clarification.</p><p>1) The decoding accuracy for single-unit and multi-unit activity is very low and this puzzles me. Let's take MT as an example: coding accuracy is well below 0.2 in all cases (chance level is at 0.125). We know that the majority of neurons in MT is tuned to motion direction and 100% coherent random dot stimuli are perhaps the stimuli that best drives these neurons. Thus, I expected a decoding accuracy for 1 out of 8 directions close to 1.0 in MT. In fact, Mendoza-Halliday et al., 2014 reported an accuracy &gt; 0.5 even in PFC. The single unit tuning curves in Figure 7F also show high selectivity. What explains the low accuracy in the manuscript (e.g. Figure 2)? Is it because of the bandpass filtering of spike trains? I think in order to properly relate invasive and non-invasive recording it must be assured that both measurements are consistent with previous literature.</p></disp-quote><p>See above (Essential revisions, point 1).</p><disp-quote content-type="editor-comment"><p>2) Statistical significance of decoding results. Most of the decoding accuracies shown in Figure 2 are really low, sometimes ~0.1255 to 0.126 when the chance level is 0.125. Frankly, I don't understand how such small effects can be significant given the number of subjects and trials. It is crucial to confirm that some of the tiny effects are not caused by confounds such as any small imbalance in the data that could be detected by the decoder (unequal trial numbers, differences between individuals, etc.). At the very least I would suggest running the whole decoding analysis on data with shuffled stimulus class labels. This should not yield significant decoding apart from a few false positives. More detailed questions concerning the statistical procedures: Why only a 2-fold cross validation for SUA and MUA and not 10-fold as for EEG and MEG? Cluster permutation procedure (subsection “Multivariate classification”, last paragraph): I do not understand the rationale of multiplying with +1 or -1. This should be explained better and a reference should be given. Finally, is the use of t-tests appropriate here?</p></disp-quote><p>See above (Essential revisions, point 2).</p><disp-quote content-type="editor-comment"><p>3) One limitation of the study is that subjects view the random dot patterns just passively. This may reduce the stimulus representation in higher brain areas (see e.g. work of Romo and Pasternak with active and passive tasks). This should be discussed.</p></disp-quote><p>See above (Essential revisions, point 3).</p><disp-quote content-type="editor-comment"><p>4) Data availability. The authors have put the data points that are necessary to plot the figures in a repository, but this is of limited use for the community. I would strongly suggest making the full data available for further analysis. I hope this request is consistent with the spirit of eLife. The main advance of this manuscript is methodological, and I believe that providing this data as a resource to the community would be a further argument for publishing it in eLife.</p></disp-quote><p>We thank the reviewer for the request and we agree that open data is desirable. Of course, we are happy to provide raw data upon reasonable request. However, due to the effort involved in collecting data of this kind and other ongoing projects, we refrain from publishing the full raw data without restriction at this time</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>General:</p><p>The overall goal of synthesizing work from humans and macaques, and across different functional tools (e.g. EEG, electrophysiology and MEG), and across spatial scales, is very important. In that sense alone, this study is valuable and should have impact. Also, the analysis is relatively sophisticated and quantitative. However, the biological interpretation of the underlying neural selectivity is a little naïve, which weakens support for the overall conclusions. Presumably, the authors could mitigate the latter concern by revising the text accordingly.</p><p>Specific:</p><p>Based on the wealth of prior research in both macaques and humans, it is well accepted that most/many neurons in area MT/V5 respond to stimuli in a direction-selective manner. However it is important to remember that area MT/V5 is a relatively small area, and not located on the cortical surface (thus arguably less amenable to accurate source analysis). More importantly, many other cortical areas (in areas V1, V2, V3, V3A, MSTd/l, etc.) also have many direction-selective neurons (or direction-activated) neurons, and most of those areas are much larger compared to MT/V5. Thus it seems likely that such areas contribute significantly to the 'motion' contrast tested here; i.e. it was not produced entirely (or perhaps even preferentially) in area MT/V5.</p></disp-quote><p>See above (Essential revisions, point 4).</p><disp-quote content-type="editor-comment"><p>The attribution of color-selective responses to 'V4' is less certain. For one thing, the oval indicating the 'V4' sampling site in macaque (Figure 1C) is overlaid on foveal V1/V2, not V4. The confusion is partly due to the presence of a very small gyrus (between the operculum and lunate gyrus) that connects the foveal representations of V1, V2, V3 and V4. A second concern is that color selective neurons and columns/patches/responses are not confined to (nor prominently present in) area V4; that is a persistent myth based on a few recordings made in the 1970s. Subsequent research has demonstrated many color responsive columns/patches/response in many other areas, (although not in MT). Thirdly, the color stimulus in 1A appears to be a mix of black dots and colored dots – i.e. a stimulus which varies in both luminance and color. [However I could not find a direct description of the color stimulus – so I am not certain about the stimulus]. Fourth, the color selectivity in humans (e.g. the L*C*h* space) differs from that in macaques, partly because the ratio of long- to medium-wavelength cones varies by a factor of two between these two species.</p><p>Thus, I conclude that the measured responses to these different stimuli may well differ as reported in the recording sites, consistent with the authors interpretation. However, the simple attribution of color-vs.-motion selectivity to MT and V4 needs to be significantly qualified. Presumably if the authors had been able to directly map the cortical sites that respond selectively to color or motion everywhere in visual cortex, the source localization would have been more certain.</p></disp-quote><p>For the concerns regarding area V4, see above (Essential revisions, point 5). We thank the reviewer for bringing up the concerns regarding color selectivity differences between species. While stimuli are often designed based on human color spaces for macaque experiments, and while color vision is quite similar in both species, we are aware of the differences mentioned by the reviewer. For the purposes of this study, we do not consider a slightly distorted color space very problematic, however the potential confound of color hue and luminance is an issue we would like to address. We therefore decided to include a supplementary figure (Figure 4—figure supplement 1) providing evidence that equiluminant colors from a human L*C*h – space also appear close to equiluminant to macaque monkeys.</p><disp-quote content-type="editor-comment"><p>The source localization of MEG and EEG information is well known to be roughly inverted, differentially maximal from sulci vs. gyri in the two measurements. This issue is crucial in the determination of source localization in brain. The authors might explore and discuss this in more detail; here I instead got the impression that the EEG and MEG signals were similar.</p><p>The extensive discussion of bi-modality (vs. lack thereof) in color vs. motion may change during the revision. However, it may be relevant that more recent attempts to map 'direction' columns (i.e. a dimension of 360 degrees) in cortex have yielded only axis-of-motion columns (a 180 degree dimension). Also, as briefly discussed by the authors, attempts to model color-selective cortical responses as psychophysically-color-opponent signals have not been entirely successful. These backgrounds may clarify the bi-modality discussion.</p></disp-quote><p>See above (Essential revisions, point 7).</p></body></sub-article></article>