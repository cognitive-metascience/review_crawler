<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">36329</article-id><article-id pub-id-type="doi">10.7554/eLife.36329</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Ultra-Rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-107831"><name><surname>Mohsenzadeh</surname><given-names>Yalda</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8525-957X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-109532"><name><surname>Qin</surname><given-names>Sheng</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-109533"><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-39948"><name><surname>Pantazis</surname><given-names>Dimitrios</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8246-8878</contrib-id><email>pantazis@mit.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">McGovern Institute for Brain Research</institution><institution>Massachusetts Institute of Technology</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Education and Psychology</institution><institution>Freie Universität Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-28130"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Reviewing Editor</role><aff id="aff3"><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>21</day><month>06</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e36329</elocation-id><history><date date-type="received" iso-8601-date="2018-03-01"><day>01</day><month>03</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2018-06-16"><day>16</day><month>06</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Mohsenzadeh et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Mohsenzadeh et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-36329-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.36329.001</object-id><p>Human visual recognition activates a dense network of overlapping feedforward and recurrent neuronal processes, making it hard to disentangle processing in the feedforward from the feedback direction. Here, we used ultra-rapid serial visual presentation to suppress sustained activity that blurs the boundaries of processing steps, enabling us to resolve two distinct stages of processing with MEG multivariate pattern classification. The first processing stage was the rapid activation cascade of the bottom-up sweep, which terminated early as visual stimuli were presented at progressively faster rates. The second stage was the emergence of categorical information with peak latency that shifted later in time with progressively faster stimulus presentations, indexing time-consuming recurrent processing. Using MEG-fMRI fusion with representational similarity, we localized recurrent signals in early visual cortex. Together, our findings segregated an initial bottom-up sweep from subsequent feedback processing, and revealed the neural signature of increased recurrent processing demands for challenging viewing conditions.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.36329.002</object-id><title>eLife digest</title><p>The human brain can interpret the visual world in less than the blink of an eye. Specialized brain regions process different aspects of visual objects. These regions form a hierarchy. Areas at the base of the hierarchy process simple features such as lines and angles. They then pass this information onto areas above them, which process more complex features, such as shapes. Eventually the area at the top of the hierarchy identifies the object. But information does not only flow from the bottom of the hierarchy to the top. It also flows from top to bottom. The latter is referred to as feedback activity, but its exact role remains unclear.</p><p>Mohsenzadeh et al. used two types of imaging to map brain activity in space and time in healthy volunteers performing a visual task. The volunteers had to decide whether a series of images that flashed up briefly on a screen included a face or not. The results showed that the brain adapts its visual processing strategy to suit the viewing conditions. They also revealed three key principles for how the brain recognizes visual objects.</p><p>First, if early visual information is incomplete – for example, because the images appeared only briefly – higher regions of the hierarchy spend more time processing the images. Second, when visual information is incomplete, higher regions of the hierarchy send more feedback down to lower regions. This leads to delays in identifying the object. And third, lower regions in the hierarchy – known collectively as early visual cortex – process the feedback signals. This processing takes place at the same time as the higher levels identify the object.</p><p>Knowing the role of feedback is critical to understanding how the visual system works. The next step is to develop computer models of visual processing. The current findings on the role of feedback should prove useful in designing such models. These might ultimately pave the way to developing treatments for visual impairments caused by damage to visual areas of the brain.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual pathway</kwd><kwd>feedback</kwd><kwd>multivariate pattern classification</kwd><kwd>MEG</kwd><kwd>fMRI</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>McGovern Institute</institution></institution-wrap></funding-source><award-id>Neurotechnology Program</award-id><principal-award-recipient><name><surname>Pantazis</surname><given-names>Dimitrios</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Emmy Noether Award</institution></institution-wrap></funding-source><award-id>CI241/1-1</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Reducing visibility with higher image presentation rates increases recurrent processing demands along the visual processing pathway to resolve object recognition.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The human visual system interprets the external world through a cascade of visual processes that overlap in space and time. Visual information is transformed not only feedforward, as it propagates through ascending connections, but also from higher to lower hierarchy areas through descending feedback connections and within the same areas through lateral connections (<xref ref-type="bibr" rid="bib1">Ahissar et al., 2009</xref>; <xref ref-type="bibr" rid="bib7">Bullier, 2001</xref>; <xref ref-type="bibr" rid="bib20">Enns and Di Lollo V, 2000</xref>; <xref ref-type="bibr" rid="bib44">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib45">Lamme et al., 1998</xref>). This concurrent activation of a dense network of anatomical connections poses a critical obstacle to the reliable measurement of recurrent signals and their segregation from feedforward activity. As a result, our knowledge on the role of recurrent processes and how they interact with feedforward processes to solve visual recognition is still incomplete.</p><p>Here we used an ultra-rapid serial visual presentation (ultra-RSVP) of real-world images to segregate early bottom-up from recurrent signals in the ventral pathway. We postulated that, under such rapid stimulus presentations, visual processes will degrade substantially by suppressing sustained neural signals that typically last hundreds of milliseconds. As a result, neural signals would become transient, reducing the overlap of processes in space and time and enabling us to disentangle distinct processing steps. Recent behavioral evidence exemplified the remarkable robustness of the human visual system to capture conceptual information in stimuli presented at similar rates (<xref ref-type="bibr" rid="bib6">Broers et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Evans et al., 2011</xref>; <xref ref-type="bibr" rid="bib61">Potter et al., 2014</xref>). Thus the underlying neural signals, while deprived, would still represent brain activity required to accomplish visual object recognition.</p><p>We recorded human MEG data while participants viewed ultra-RSVP sequences with rates 17 or 34 ms per picture. Confirming our hypothesis, the rapid presentation of images segregated the activation cascade of the ventral visual pathway into two temporally dissociable processing stages, disentangling the initial bottom-up sweep from subsequent processing in high-level visual cortex. Capitalizing on this dissociation, we used multivariate pattern classification of MEG data to characterize the activation dynamics of the ventral pathway and address the following three challenges: we investigated how the evolution of the bottom-up sweep predicts the formation of high-level visual representations; we sought evidence for rapid recurrent activity that facilitates visual recognition; and we explored whether reducing visibility with higher stimulus presentation rates increases recurrent processing demands. Finally, to resolve the locus of feedforward and feedback visual signals, we tracked the spatiotemporal dynamics with a MEG-fMRI fusion approach based on representational similarity (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>, <xref ref-type="bibr" rid="bib13">2016a</xref>; <xref ref-type="bibr" rid="bib42">Kriegeskorte et al., 2008</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We collected MEG data while human participants (n = 17) viewed rapid sequences of 11 real-world images presented at 17 or 34 ms per picture. The middle image (sixth image, named target) was randomly sampled from a set of 12 face images or 12 object images, while the remaining images (1–5 and 7–11, named masks) comprised different categories of objects (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Participants performed a two-alternative forced choice task reporting whether a face was present in the sequence or not.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.36329.003</object-id><label>Figure 1.</label><caption><title>Rapid serial visual presentation (RSVP) task.</title><p>(<bold>a</bold>) Experimental procedure. The stimulus set comprised 12 face targets, 12 object targets, and 45 masks of various objects. Participants viewed a RSVP sequence of 11 images, with the middle image (target) either a face or an object. The images were presented at a rate of 17 ms per picture or 34 ms per picture in separate trials. Following a delay of 0.7 - 1 s to prevent motor artifacts, subjects were prompted to respond by pressing a button whether they have seen a face or not. (Images shown are not examples of the original stimulus set due to copyright; the exact stimulus set is visualized at <ext-link ext-link-type="uri" xlink:href="https://megrsvp.github.io">https://megrsvp.github.io</ext-link>. Images shown are in public domain and available at pexels.com under a Creative Commons Zero (CC0) license.) (<bold>b</bold>) Behavioral performance in the two RSVP conditions. Bars indicate d’ performance and error bars indicate SEM. Stars above each bar and between bars indicate significant performance and significant differences between the two conditions, respectively (<italic>n</italic>=17; two-sided signed-rank test; <italic>P</italic><inline-formula><mml:math id="inf1"><mml:mo>≪</mml:mo></mml:math></inline-formula>0.001).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36329-fig1-v2"/></fig><p>Even though the image presentation was extremely rapid, participants performed the face detection task consistently above chance in the 17 ms per picture RSVP condition (sensitivity index d’ ± SEM = 1.95 ± 0.11), and with high accuracy in the 34 ms per picture RSVP condition (d’ ± SEM = 3.58 ± 0.16) (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Behavioral performance was significantly different between the two conditions (<italic>n</italic>=17; two-sided signed-rank test; <italic>P</italic><inline-formula><mml:math id="inf2"><mml:mo>≪</mml:mo></mml:math></inline-formula>0.001).</p><p>To track how neural representations resolved stimulus information in time, we used time-resolved multivariate pattern classification on the MEG data (<xref ref-type="bibr" rid="bib14">Cichy and Pantazis, 2017</xref>; <xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">King and Dehaene, 2014</xref>). We extracted peri-stimulus MEG signals from −300 ms to 900 ms (1 ms resolution) with respect to the target image onset. For each time point separately, we used the MEG data to classify pairwise (50% chance level) all 24 target images. The results of the classification (% decoding accuracy) were used to populate a time-resolved 24 × 24 decoding matrix indexed by the target images (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). To demonstrate the advantage of RSVP in dissociating visual processes against other experimental paradigms, we further computed decoding matrices in a slow visual presentation at 500 ms per picture, where the same 24 target images were presented in isolation for 500 ms with an ISI of 1 s. The entire procedure yielded three time-resolved decoding matrices, one for the 17, 34, and 500 ms per picture conditions respectively.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.36329.004</object-id><label>Figure 2.</label><caption><title>Decoding of target images from MEG signals.</title><p>(<bold>a</bold>) Multivariate pattern analysis of MEG signals. A support vector machine (SVM) classifier learned to discriminate pairs of target images using MEG data at time point <italic>t</italic>. The decoding accuracies populated a 24 × 24 decoding matrix at each time point <italic>t</italic>. (Images shown are not examples of the original stimulus set due to copyright; see <xref ref-type="fig" rid="fig1">Figure 1</xref> caption for details) (<bold>b</bold>) Time course of grand total target image decoding for the 500, 34, and 17 ms per picture conditions. Pairwise decoding accuracies were averaged across all elements of the decoding matrix. Time is relative to target image onset. Color coded lines above plots indicate significant times. Color coded interval lines below plots indicate stimuli presentation times, with thick and thin lines indicating target and mask presentations. Arrows indicate peak latencies. (<bold>c, d</bold>) Time course of within category target image decoding for faces and objects. The decoding matrix was divided into 2 segments for pairs of within-face and within-object comparisons, and the corresponding decoding accuracies were averaged. (<bold>e–g</bold>) Peak latency times for the above target decoding time courses and corresponding 95% confidence intervals are depicted with bar plots and error bars, respectively. (<bold>h–j</bold>) Onset latency times for the above decoding time courses and 95% confidence intervals. Stars above bars indicate significant differences between conditions. (<italic>n</italic>=16 for 500 ms per picture and <italic>n</italic>=17 for RSVP conditions; time courses were evaluated with one-sided sign permutation tests, cluster defining threshold <italic>P</italic><inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05, and corrected significance level <italic>P</italic><inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05; bar plots were evaluated with bootstrap tests for 95% confidence intervals and two-sided hypothesis tests; false discovery rate corrected at <italic>P</italic><inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36329-fig2-v2"/></fig><sec id="s2-1"><title>Rapid serial visual presentation disrupted the early sweep of visual activity</title><p>To determine the time series with which individual images were discriminated by neural representations, we averaged all elements of the decoding matrix, resulting in a grand total decoding time series (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). First, we found that neural responses were resolved at the level of individual images in all 3 viewing conditions. Second, decoding accuracies decreased with faster stimulus presentation rates, reflecting the challenging nature of the RSVP task with stimuli presented for very short times. Third, peak latencies shifted earlier with faster stimulus presentation rates (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). That is, the 500 ms per picture condition reached a peak at 121 ms (95% confidence interval: 102-126 ms), preceded by the 34 ms per picture RSVP condition at 100 ms (94-107 ms), and finally the 17 ms per picture RSVP condition at 96 ms (93-99 ms) (all statistically different; <italic>P</italic><inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05; two-sided sign permutation tests). Fourth, onset latencies shifted later with faster stimulus presentation rates (<xref ref-type="fig" rid="fig2">Figure 2h</xref>). That is, the 500 ms per picture condition had onset at 28 ms (9-53 ms), followed by the 34 ms per picture RSVP condition at 64 ms (58-69 ms), and finally the 17 ms per picture RSVP condition at 70 ms (63-76 ms) (all statistically different; <italic>P</italic><inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05; two-sided sign permutation tests).</p><p>The decreased decoding accuracy combined with the increasingly early peak latency and increasingly late onset latency for the RSVP conditions indicate that visual activity was disrupted over the first 100 ms. Even though the highest levels of the visual processing hierarchy in humans are reached with the first 100 ms, there is little time for feedback connections from these areas to exert an effect (<xref ref-type="bibr" rid="bib44">Lamme and Roelfsema, 2000</xref>). Thus, neural activity during the first 100 ms has been linked to the engagement of feedforward and local recurrent connections, rather than long-range feedback connections. In line with these arguments, the early peaks at 100 and 96 ms for the 34 and 17 ms per picture RSVP conditions, respectively, explicitly delineate the first sweep of visual activity, differentiating it from later neural activity that includes feedback influences from the top of the visual hierarchy. Further, if early decoding would only reflect feedforward activity, we would not expect to see onset latency differences, but we do. The fact that different stimulus durations have different onsets suggests that interactions with recurrent activity are already incorporated when the first decoding onsets emerge, arguing against the view that the early part of the decoding time course can be uniquely tied to feedforward alone (<xref ref-type="bibr" rid="bib23">Fahrenfort et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib65">Ringach et al., 1997</xref>).</p><p>Next, to investigate the generalization of our findings to any pair of images, even when they share categorical content, we evaluated whether our results hold to within-category image classification. For this, we subdivided the decoding matrix into two partitions, corresponding to within-face comparisons, and within-object comparisons. Averaging the elements of each partition separately determined the time series with which individual images were resolved within the subdivision of faces (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) and objects (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). We confirmed the generalization and reliability of our findings, as our results were similar to the grand total decoding time series: individual images were discriminated by neural responses; decoding accuracies were weaker for rapid stimulus presentations; and peak and onset latencies had the same ordinal relationship as in the grand total analysis (<xref ref-type="fig" rid="fig2">Figure 2fg</xref>). Peak and onset latencies for the grand total and within category comparisons are shown in <xref ref-type="table" rid="table1">Table 1</xref>.</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.36329.005</object-id><label>Table 1.</label><caption><title>Peak and onset latency of the time series for single image decoding (<xref ref-type="fig" rid="fig2">Figure 2</xref>) and categorical division decoding (<xref ref-type="fig" rid="fig3">Figure 3</xref>), with 95% confidence intervals in brackets.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top">Presentation rate</th><th valign="top">Peak latency (ms)</th><th valign="top">Onset latency (ms)</th></tr></thead><tbody><tr><td rowspan="3" valign="top">Grand total</td><td valign="top">500 ms per picture</td><td valign="top">121 (102–126)</td><td valign="top">28 (9–53)</td></tr><tr><td valign="top">34 ms per picture</td><td valign="top">100 (96–107)</td><td valign="top">64 (58–69)</td></tr><tr><td valign="top">17 ms per picture</td><td valign="top">96 (93–99)</td><td valign="top">70 (63–76)</td></tr><tr><td rowspan="3" valign="top">Within-faces</td><td valign="top">500 ms per picture</td><td valign="top">113 (104–119)</td><td valign="top">59 (30–73)</td></tr><tr><td valign="top">34 ms per picture</td><td valign="top">104 (96–109)</td><td valign="top">74 (62–81)</td></tr><tr><td valign="top">17 ms per picture</td><td valign="top">98 (96–104)</td><td valign="top">86 (78–117)</td></tr><tr><td rowspan="3" valign="top">Within-objects</td><td valign="top">500 ms per picture</td><td valign="top">102 (93–102)</td><td valign="top">48 (10–55)</td></tr><tr><td valign="top">34 ms per picture</td><td valign="top">97 (90–97)</td><td valign="top">60 (27–67)</td></tr><tr><td valign="top">17 ms per picture</td><td valign="top">94 (87–95)</td><td valign="top">70 (64–74)</td></tr><tr><td rowspan="3" valign="top">Between minus <break/>within</td><td valign="top">500 ms per picture</td><td valign="top">136 (130–139)</td><td valign="top">46 (15–51)</td></tr><tr><td valign="top">34 ms per picture</td><td valign="top">169 (165–177)</td><td valign="top">73 (67–78)</td></tr><tr><td valign="top">17 ms per picture</td><td valign="top">197 (175–218)</td><td valign="top">139 (67–155)</td></tr></tbody></table></table-wrap><p>In sum, decoding accuracies decreased with progressively shorter stimulus presentation times, indicating that neuronal signals encoded less stimulus information at rapid presentation rates. Onset latencies shifted late with shorter presentation times, indicating that recurrent activity exerts its influence even as the first decoding onsets emerged. Importantly, the progressively earlier peak with shorter presentation times indicated disruption of the first sweep of visual activity, thus indexing feedforward and local recurrent processing and segregating it in time from subsequent processing that includes feedback influences from high-level visual cortex.</p></sec><sec id="s2-2"><title>Rapid serial visual presentation delayed the emergence of categorical information</title><p>How did the disruption of the early sweep of visual activity, reported in the previous section, affect the emergence of categorical information in the RSVP conditions? A prevalent theory posits that core object recognition is largely solved in a feedforward manner (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="bib48">Liu et al., 2002</xref>; <xref ref-type="bibr" rid="bib69">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib78">Thorpe et al., 1996</xref>). If this holds under rapid presentation conditions, then categorical signals would be expected to emerge with comparable dynamics regardless of stimulus presentation rates. However, opposing theories concur that feedback activity is critical for visual awareness and consciousness (<xref ref-type="bibr" rid="bib44">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib1">Ahissar et al., 2009</xref>; <xref ref-type="bibr" rid="bib24">Fahrenfort et al., 2017</xref>, <xref ref-type="bibr" rid="bib23">2012</xref>). According to these theories, presenting stimuli at rapid presentation rates would (i) afford less time for initial stimulus evidence accumulation (a process that in all likelihood already incorporates some local recurrent processing, as suggested by variable onset latencies reported in the previous section) and (ii) lead to disruption of recurrent signals of the target stimulus due to the masking stimuli of the RSVP paradigm. These would be consistent with slowing down the speed and extent with which category information can be resolved using recurrence (<xref ref-type="bibr" rid="bib5">Brincat and Connor, 2006</xref>; <xref ref-type="bibr" rid="bib74">Tang and Kreiman, 2017</xref>).</p><p>To differentiate between those competing theories, we computed categorical division time series. We divided the decoding matrix into partitions corresponding to within-category (face or object) and between-category stimulus comparisons separately for each of the three viewing conditions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). The difference of between-category minus within-category average decoding accuracies served as a measure of clustering by category membership.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.36329.006</object-id><label>Figure 3.</label><caption><title>Categorical information encoded in MEG signals.</title><p>(<bold>a</bold>) Time course of categorical division depending on presentation rate. For each condition, the MEG decoding matrix was divided into 3 segments for pairs of within-face, within-object, and between-face/object comparisons, and a categorical effect was estimated by contrasting the averaged decoding accuracies of the within from the between segments. Time is relative to target image onset. Color coded lines above plots, interval bars below plots, and arrows same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. (<bold>b</bold>) Peak latency times for the categorical information time courses and 95% confidence intervals are depicted with bar plots and error bars, respectively. Stars above bars indicate significant differences between conditions. (<bold>c</bold>) The first two dimensions of multidimensional scaling (MDS) of the MEG decoding matrices are shown for the times of peak categorical information for the 3 conditions. (<italic>n</italic>=16 for 500 ms per picture and <italic>n</italic>=17 for RSVP conditions; time courses were evaluated with one-sided sign permutation tests, cluster defining threshold <italic>P</italic><inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05, and corrected significance level <italic>P</italic><inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05; bar plots were evaluated with bootstrap tests for 95% confidence intervals and two-sided hypothesis tests; false discovery rate corrected at <italic>P</italic><inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36329-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36329.007</object-id><label>Figure 3—Figure supplement 1</label><caption><title>Linear decoding of faces vs. objects category.</title><p>For each condition and at each time point a SVM classifier was trained to decode object versus face trials with a leave-one-out procedure. The shape and peaks of the time series largely match the results in <xref ref-type="fig" rid="fig3">Figure 3a</xref>. This indicates that binary image classification followed by comparison of inter- vs intra-class averaging (as performed in <xref ref-type="fig" rid="fig3">Figure 3a</xref>) is consistent with a direct classification of faces vs. objects (shown in supplement). The color coded lines above the curves indicate significant time points (one-sided sign permutation tests, cluster defining threshold p&lt;0.05, and corrected significance level p&lt;0.05).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36329-fig3-figsupp1-v2"/></fig></fig-group><p>We found that the categorical division time series resolved face versus object information in all three conditions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). Consistent with the grand total decoding results, categorical neural representations were stronger in the 500 and 34 ms per picture conditions than the 17 ms per picture condition. Multidimensional scaling (MDS) plots (<xref ref-type="bibr" rid="bib43">Kruskal and Wish, 1978</xref>) at peak latencies for the three conditions, offering an intuitive visualization of the stimulus relationships, are shown in <xref ref-type="fig" rid="fig3">Figure 3c</xref>. These plots revealed strong categorical division for the 500 and 34 ms per picture conditions, followed by weaker but still distinct categorical division in the 17 ms per picture condition.</p><p>The peak of the categorical division time series revealed the time points at which categorical information was most explicitly encoded in the neural representations (<xref ref-type="bibr" rid="bib17">DiCarlo and Cox, 2007</xref>). The peak latency increased as presentation rates became progressively faster. That is, the time series for the 500 ms per picture condition peaked at 136 ms (130-139 ms), followed by the 34 ms per picture RSVP at 169 ms (165-177 ms), and the 17 ms per picture RSVP at 197 ms (184-218 ms) (all statistically different; <italic>P</italic><inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05; two-sided sign permutation tests) (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). This relationship is reverse from the peak latency of the first sweep of visual activity reported in the previous section, further stressing the existence of variable dynamics in the ventral pathway. This suggests that categorical information did not arise directly in a purely feedforward mode of processing, as this would predict comparable temporal dynamics in all conditions. Instead, it is consistent with the idea that recurrent interactions within the ventral stream facilitate the emergence of categorical information by enhancing stimulus information in challenging visual tasks (<xref ref-type="bibr" rid="bib5">Brincat and Connor, 2006</xref>; <xref ref-type="bibr" rid="bib28">Hochstein and Ahissar, 2002</xref>; <xref ref-type="bibr" rid="bib63">Rajaei et al., 2018</xref>; <xref ref-type="bibr" rid="bib74">Tang and Kreiman, 2017</xref>; <xref ref-type="bibr" rid="bib75">Tapia and Beck, 2014</xref>).</p><p>Taken together, our results revealed variable temporal neural dynamics for viewing conditions differing in presentation time. Even though the peak latency of the first sweep of visual activity shifted earlier with higher presentation rates, as reported in the previous section, the peak latency of categorical information shifted later, stretching the time between the abrupt end of the initial visual sweep and the emergence of categorical information. This inverse relationship in peak latencies discounts a feedforward cascade as the sole explanation for categorical representations.</p></sec><sec id="s2-3"><title>Neuronal representations became increasingly transient at rapid stimulus presentation rates</title><p>As neuronal signals propagate along the ventral pathway, neural activity can either change rapidly at subsequent time points, or persist for extended times. Transient activity reflects processing of different stimulus properties over time in either a feedforward manner, as computations become more abstract, or a recurrent manner as neurons tune their responses. On the other hand, persistent activity could maintain results of a particular neural processing stage for later use.</p><p>Our premise in introducing the ultra-RSVP task was to suppress the persistent neural activity, and in doing so better capture the transient neural dynamics that reflect distinct neural processing steps. To experimentally confirm that persistent neural activity was indeed suppressed with rapid presentation rates, we extended the SVM classification procedure with a temporal generalization approach (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">King and Dehaene, 2014</xref>). In particular, we used a classifier trained on data at a time point <italic>t</italic> to evaluate discrimination at all other time points <italic>t’</italic>. Intuitively, if neural representations are sustained across time, the classifier should generalize well across other time points.</p><p>Temporal generalization matrices were computed by averaging decoding across all pairwise image conditions and all subjects, thus extending over time the results presented in <xref ref-type="fig" rid="fig2">Figure 2b</xref>. Our temporal generalization analysis confirmed that neural activity became increasingly transient at rapid presentation rates (<xref ref-type="fig" rid="fig4">Figure 4</xref>). While the 500 ms per picture condition had maps with broad off-diagonal significant elements characteristic of sustained representations, the RSVP conditions had narrow diagonal maps indicating transient neural patterns, with the 17 ms per picture RSVP narrower than the 34 ms per picture RSVP.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.36329.008</object-id><label>Figure 4.</label><caption><title>Temporal generalization of target image decoding for the 500, 34, and 17 ms per picture conditions.</title><p>The SVM classifier was trained with MEG data from a given time point <italic>t</italic> (training time) and tested on all other time points (testing time). The temporal generalization decoding matrix was averaged over all image pairs and all subjects, thus corresponding to the temporal generalization of the grand total decoding time series in <xref ref-type="fig" rid="fig2">Figure 2b</xref>. The black line marks the target image onset time. The gray lines mark the image offset in the 500 ms per picture condition and the RSVP sequence onset/offset times in the rapid presentation conditions. The white contour indicates significant decoding values (<italic>n</italic>=16 for 500 ms per picture and <italic>n</italic>=17 for RSVP conditions; one-sided sign permutation tests, cluster defining threshold <italic>P</italic><inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05, and corrected significance level <italic>P</italic><inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36329-fig4-v2"/></fig><p>The increasingly transient activity in the RSVP conditions shows that neural activity continuously transformed stimulus information in a feedforward and feedback manner, will less neural resources used to maintain information. Thus, the results confirmed our hypothesis that the ultra-RSVP task would suppress persistent neural activity.</p></sec><sec id="s2-4"><title>Unfolding the dynamics of feedforward and feedback processes in space and time</title><p>The analyses presented thus far segregated the temporal dynamics of the initial bottom-up sweep from subsequent signals incorporating recurrent activity in the ventral pathway. Furthermore, peak latencies for early and late visual signals varied inversely, consistent with feedback processing. Here we mapped visual signals on the cortex to identify where in the brain feedforward and feedback signal interact.</p><p>To map the spatiotemporal dynamics of the visual processes we used a MEG-fMRI fusion method based on representational similarity (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>, <xref ref-type="bibr" rid="bib13">2016a</xref>). For this, we first localized the MEG signals on the cortex and derived the time series from all source elements within two regions-of-interest (ROIs): early visual cortex (EVC) and inferior temporal cortex (IT). We selected EVC as the first region of the cortical feedforward sweep, and IT as the end point where neural patterns have been found to indicate object category (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>). We then performed time-resolved multivariate pattern classification on the MEG data following the same procedure described earlier, only now we created pattern vectors by concatenating activation values from sources within a given ROI, instead of concatenating the whole-head sensor measurements. This procedure resulted in one MEG RDM for each ROI and time point.</p><p>We compared the representational similarity between the time-resolved MEG RDMs for the two cortical regions (EVC and IT) and the fMRI RDMs for the same regions (<xref ref-type="fig" rid="fig5">Figure a and b</xref>). This yielded two time series of MEG-fMRI representational similarity, one for EVC and one for IT. In all conditions, consistent with the view of visual processing as a spatiotemporal cascade (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>), the time series peaked earlier for EVC than IT (<xref ref-type="fig" rid="fig5">Figure 5c–h</xref>). The peak-to-peak latency between EVC and IT increased as viewing conditions became increasingly challenging with faster presentation rates: Δ=27 ms for the 500 ms per picture condition; Δ=79 ms for the 34 ms per picture RSVP; and Δ=115 ms for the 17 ms per picture RSVP (all statistically different; two-sided bootstrap hypothesis tests; <italic>P</italic><inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05). This latency difference was the compounded effect of two factors. First, the EVC peak had progressively shorter latencies (104 vs. 87 vs. 80 ms for the 3 conditions), and second the IT peak had progressively longer latencies (131 vs. 166 vs. 195 ms for the three conditions). This inverse relationship between the EVC and IT peaks corroborated the findings of the previous sections, namely that a disrupted first sweep of visual activity was associated with a delayed emergence of categorical division information. It further bound the processing stages in time to the V1 and IT locations in space.</p><p>Importantly, while EVC had a single peak at 104 ms and persistent representations over hundreds of milliseconds for the 500 ms per picture condition, its dynamics were transient and bimodal for the RSVP conditions. For the 34 ms per picture RSVP condition, an early peak at 87 ms was immediately followed by weak MEG-fMRI representational similarities, and then a second peak at 169 ms. For the 17 ms per picture RSVP condition, we observed similar dynamics with an early peak at 80 ms and a second peak at 202 ms, though in this case the second peak was not strictly defined because the time course did not reach significance, possibly due to compromised neural representations at such fast stimulus presentation rates. The second peak in EVC occurred at similar times as the peak in IT (Δ=3 ms for the 34 ms per picture RSVP condition, <italic>p=</italic>0.06; and Δ=7 ms for the 17 ms per picture RSVP condition, <italic>p</italic><inline-formula><mml:math id="inf15"><mml:mo>≪</mml:mo></mml:math></inline-formula>0.001; two-sided bootstrap hypothesis tests). This is consistent with feedback activity in EVC at the same time as IT solves visual object recognition. <xref ref-type="table" rid="table2">Table 2</xref> summarizes latencies and 95% confidence intervals for all conditions. We note here that resolving feedback activity in EVC was possible with MEG-fMRI fusion because MEG activation patterns disentangled slow fMRI hemodynamic responses in EVC that correspond to the combined contributions of feedforward and feedback visual activity.</p><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.36329.009</object-id><label>Table 2.</label><caption><title>Peak and onset latency of the time series for MEG-fMRI fusion at EVC and IT, with 95% confidence intervals in brackets.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top"/><th valign="top">Peak latency (ms)</th><th valign="top">Onset latency (ms)</th></tr></thead><tbody><tr><td rowspan="2" valign="top">500 ms per picture</td><td valign="top">IT</td><td valign="top">131 (127–135)</td><td valign="top">63 (50–75)</td></tr><tr><td valign="top">EVC</td><td valign="top">104 (93–120)</td><td valign="top">53 (50–57)</td></tr><tr><td rowspan="2" valign="top">34 ms per picture</td><td valign="top">IT</td><td valign="top">166 (162–173)</td><td valign="top">70 (50–88)</td></tr><tr><td valign="top">EVC</td><td valign="top">87 (83–97) and <break/>169 (164-176)*</td><td valign="top">71 (61–81)</td></tr><tr><td rowspan="2" valign="top">17 ms per picture</td><td valign="top">IT</td><td valign="top">195 (170–203)</td><td valign="top">162 (55–183)</td></tr><tr><td valign="top">EVC</td><td valign="top">80 (75–100) and <break/>202 (190-219)*</td><td valign="top">71 (50–76)</td></tr></tbody></table><table-wrap-foot><fn><p>*Time series had two early peaks.</p></fn></table-wrap-foot></table-wrap><p>In sum, the combination of the RSVP paradigm with MEG-fMRI representational similarity resolved bimodal dynamics for EVC. The first EVC peak offered evidence that disruption of early visual activity resulted in delayed categorical division information in IT. The second EVC peak occurred at approximately the same time as the peak in IT and is consistent with feedback activity from IT to EVC.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Using an ultra-RSVP task, we dissected the ventral pathway activation cascade into two temporally distinct processing stages: the initial bottom-up sweep and the subsequent emergence of categorical information. For the first stage, we found a progressively earlier peak with decreasing viewing times, indicating an early disruption of visual activity (<xref ref-type="fig" rid="fig2">Figure 2</xref>). For the second stage, we found a progressively later peak with decreasing viewing times, indicating a delayed emergence of categorical information at high-level visual cortex (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This reverse relation of the peak latencies between the two processing stages has two critical implications: first, the extent of disruption of the initial sweep is related to longer processing times at subsequent stages to solve visual recognition; and second, such variable temporal dynamics index the existence of recurrent activity in the ventral pathway that takes up additional processing time to solve visual recognition when initial signals are limited in time. Finally, using MEG-fMRI fusion we pinpointed the locus where recurrent activity becomes effective to EVC (<xref ref-type="fig" rid="fig5">Figure 5</xref>), with temporal onset overlapping with the dynamics of the emergence of categorical information in high-level cortex.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.36329.010</object-id><label>Figure 5.</label><caption><title>Representational similarity of MEG to fMRI signals at EVC and IT.</title><p>(<bold>a</bold>) For every time point <italic>t</italic>, the EVC-specific MEG RDM was compared (Spearman’s rho) with the EVC-specific fMRI RDM, yielding a time series of MEG-fMRI representational similarity at EVC. (<bold>b</bold>) Same as in (<bold>a</bold>) but for IT. (<bold>c–e</bold>) Time series of MEG-fMRI representational similarity at EVC and IT for the 3 conditions. Time is shown relative to target image onset. Color coded lines below plots indicate significant times. Peak latencies are indicated with arrows. While the 202 ms peak of the EVC time series in (<bold>e</bold>) is not significant, it is marked to indicate comparable temporal dynamics with the 34 ms per picture condition. (<bold>f–h</bold>) Peak latency times for the representational similarity time courses and 95% confidence intervals are depicted with bar plots and error bars, respectively. Stars above bars indicate significant differences between conditions. (<italic>n</italic>=16 for 500 ms per picture and <italic>n</italic>=17 for RSVP conditions; time courses were evaluated with one-sided sign permutation tests, cluster defining threshold <italic>P</italic><inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05, and corrected significance level <italic>P</italic><inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05; bar plots were evaluated with bootstrap tests for 95% confidence intervals and two-sided hypothesis tests; false discovery rate corrected at <italic>P</italic><inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>0.05).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-36329-fig5-v2"/></fig><p>While a large body of literature has investigated recurrent processing in the visual cortex, our work goes beyond and complements prior studies in several dimensions. First, consistent with behavioral and transcranial magnetic stimulation experiments that postulated the locus of recurrent activity signals in V1/V2 (<xref ref-type="bibr" rid="bib8">Camprodon et al., 2010</xref>; <xref ref-type="bibr" rid="bib19">Drewes et al., 2016</xref>; <xref ref-type="bibr" rid="bib39">Koivisto et al., 2011</xref>; <xref ref-type="bibr" rid="bib82">Wokke et al., 2013</xref>), we mapped the locus of rapid feedback activity in early visual cortex. Second, our study elucidates the functional nature of recurrent connections in vision. Recurrent connections have often been thought to subserve slower top-down attentional modulations from frontoparietal sites once recognition has been solved (<xref ref-type="bibr" rid="bib2">Bar et al., 2006</xref>; <xref ref-type="bibr" rid="bib30">Hopf et al., 2006</xref>; <xref ref-type="bibr" rid="bib68">Sehatpour et al., 2008</xref>). In contrast, our study corroborates growing experimental evidence suggesting that recurrent signals also originate from within the ventral pathway as recognition unfolds, and can exert their influence before attentional modulations (<xref ref-type="bibr" rid="bib4">Boehler et al., 2008</xref>; <xref ref-type="bibr" rid="bib83">Wyatte et al., 2014</xref>). Third, while extensive evidence suggests the existence of rapid feedback signals using highly controlled artificial stimuli, such as Kanizsa-type illusory figures, motion of random dots, and oriented bars (<xref ref-type="bibr" rid="bib26">Halgren et al., 2003</xref>; <xref ref-type="bibr" rid="bib33">Hupé et al., 1998</xref>; <xref ref-type="bibr" rid="bib46">Lamme et al., 2002</xref>; <xref ref-type="bibr" rid="bib54">Murray et al., 2002</xref>; <xref ref-type="bibr" rid="bib65">Ringach et al., 1997</xref>; <xref ref-type="bibr" rid="bib81">Wokke et al., 2012</xref>, <xref ref-type="bibr" rid="bib82">2013</xref>; <xref ref-type="bibr" rid="bib85">Yoshino et al., 2006</xref>), it is not clear whether these results generalize to real-world situations. Since our target stimuli were natural images of objects and faces, we demonstrated the existence of feedback activity in early visual cortex under more ecologically valid conditions. Fourth, even though temporal delays in processing stimulus information have been observed when visibility conditions were impoverished (<xref ref-type="bibr" rid="bib74">Tang and Kreiman, 2017</xref>), here we explicitly related bottom-up and feedback dynamics to each other and offered well-defined temporal signatures for both mechanisms. Last, rather than using unimodal data to study recurrent activity that reveals either spatial or temporal aspects of brain activity, we applied a novel way to investigate ventral neural dynamics by coupling MEG and fMRI data to holistically capture the spatiotemporal dynamics of brain activation (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>, <xref ref-type="bibr" rid="bib13">2016a</xref>).</p><p>Our results thus allow the identification of three fundamental processing principles of object recognition: (i) the brain compensates for early disruptions of the initial sweep through longer processing times at subsequent stages; (ii) delays in solving object recognition are an index of increased recurrent processing; and (iii) such recurrent processing takes place in early visual cortex and coincides in time with the emergence of categorical information in inferior-temporal cortex.</p><sec id="s3-1"><title>Sequencing the processing cascade in the ventral visual pathway</title><p>Here we discuss the nature of visual processing and the role of recurrent dynamics for each of the two temporally distinct stages of visual processing revealed by the ultra-RSVP task.</p><sec id="s3-1-1"><title>First stage - feedforward activity</title><p>The first wave of visual responses we observed peaked at approximately 100 ms for the RSVP conditions (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). This latency falls within the time range during which the feedforward sweep is expected to reach the top of the visual hierarchy, but leaves little time for substantial contributions from high-level feedback connections (<xref ref-type="bibr" rid="bib44">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib75">Tapia and Beck, 2014</xref>; <xref ref-type="bibr" rid="bib83">Wyatte et al., 2014</xref>). Therefore, responses up to 100 ms tracked a feedforward process with contributions from local recurrent circuits, but separate from later feedback signals emerging from high-level visual cortex.</p></sec><sec id="s3-1-2"><title>First stage - early recurrent activity</title><p>The first sweep of visual activity was characterized by a ramping process with a peak latency approximately 100 ms for the RSVP conditions (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Such extended latencies, though too short to include substantial feedback signals from the top of the visual hierarchy, permit incorporation of information from local recurrent connections. In typical viewing conditions, such as the 500 ms per picture condition used here, neurons are known to remain active even after their initial contribution in the feedforward sweep, with their responses modulated with contextual information both within and outside their receptive fields (<xref ref-type="bibr" rid="bib44">Lamme and Roelfsema, 2000</xref>). Examples include the dynamic orientation tuning of V1 cell responses, which is shaped by horizontal intracortical connections (<xref ref-type="bibr" rid="bib65">Ringach et al., 1997</xref>), and the segregation of texture, which is shaped by feedback signals from extrastriate areas (<xref ref-type="bibr" rid="bib33">Hupé et al., 1998</xref>; <xref ref-type="bibr" rid="bib45">Lamme et al., 1998</xref>). Experiments using transcranial magnetic stimulation (<xref ref-type="bibr" rid="bib8">Camprodon et al., 2010</xref>; <xref ref-type="bibr" rid="bib39">Koivisto et al., 2011</xref>), backward masking (<xref ref-type="bibr" rid="bib4">Boehler et al., 2008</xref>; <xref ref-type="bibr" rid="bib22">Fahrenfort et al., 2007</xref>; <xref ref-type="bibr" rid="bib40">Kovács et al., 1995</xref>; <xref ref-type="bibr" rid="bib46">Lamme et al., 2002</xref>), and reversible cooling (<xref ref-type="bibr" rid="bib33">Hupé et al., 1998</xref>) offer evidence that rapid feedback circuits in the ventral pathway are engaged within the first 80–150 ms of vision, with local feedback signals from extrastriate areas to V1 emerging within 80–110 ms (<xref ref-type="bibr" rid="bib83">Wyatte et al., 2014</xref>).</p><p>Was the first wave in the ultra-RSVP conditions shaped by local recurrent connections? While our results cannot discount such influences, it is reasonable to construe the first wave in the RSVP conditions as disproportionately characterized by feedforward propagating signals rather than local recurrent activity. This is because EVC time series had strong MEG-fMRI representational similarities over hundreds of milliseconds for the 500 ms per picture condition, whereas weak dynamics with early peak latencies (80 and 87 ms) for the RSVP conditions (<xref ref-type="fig" rid="fig5">Figure 5c–e</xref>). This suggests any possible recurrent processes in EVC for the 500 ms per picture condition were suppressed in the RSVP conditions immediately after 80 or 87 ms.</p><p>Reduced early recurrent activity in the RSVP conditions could be due to two reasons. First, shorter stimulus presentation times resulted in limited stimulus evidence accumulation. This may be reflected in the progressively later onset latencies with decreasing viewing times in <xref ref-type="fig" rid="fig2">Figure 2h–j</xref>, which suggest that local recurrent interactions have a rapid influence in the ventral stream dynamics. And second, the mask stimuli disrupted recurrent processing of the target in the RSVP conditions. This may be reflected in the more transient dynamics for the RSVP conditions in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></sec><sec id="s3-1-3"><title>Second stage - delayed emergence of categorical information</title><p>The first sweep of visual activity was temporally dissociated from a second processing stage at which categorical information emerged. The peak latency and strength of the early visual sweep were inversely related with the timing of categorical representations. Thus, compromised visual signals early in the ventral stream resulted in delays in the emergence of categorical information. This is consistent with recurrent visual processes requiring additional time to interpret the face target images within the RSVP sequences. Such delays in neural responses have been used as an indicator of recurrent computations in previous studies, including object selective responses in human fusiform gyri for partially visible images (<xref ref-type="bibr" rid="bib74">Tang and Kreiman, 2017</xref>) and the integration of object parts into a coherent representation in macaque posterior IT (<xref ref-type="bibr" rid="bib5">Brincat and Connor, 2006</xref>).</p><p>An alternative explanation of the variable peak latencies due to delayed propagation of feedforward signals is unlikely. Instead of observing a graded increase in latencies, the peaks in the feedforward sweep were inversely related to the peaks in categorical information, stretching the time difference between the two processing stage and discounting a purely feedforward explanation.</p></sec><sec id="s3-1-4"><title>Second stage - recurrent activity in EVC</title><p>Categorical signals for the two RSVP conditions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>) were associated with the onset of IT responses and the synchronous reengagement of EVC (<xref ref-type="fig" rid="fig5">Figure 5c–e</xref>). This reengagement of EVC at the same time categorical information emerged in high-level visual cortex suggests recurrent processes as its basis.</p><p>Reengagement of EVC is consistent with theories positing that feedback signals serve to add details in an initially established scene (<xref ref-type="bibr" rid="bib28">Hochstein and Ahissar, 2002</xref>; <xref ref-type="bibr" rid="bib59">Peyrin et al., 2010</xref>; <xref ref-type="bibr" rid="bib75">Tapia and Beck, 2014</xref>). According to these theories, when stimuli are degraded, partial, or otherwise ambiguous, recurrent processes fill-in stimulus information by propagating partial responses back to EVC and activating neurons that would normally respond to the complete stimulus (<xref ref-type="bibr" rid="bib53">Muckli et al., 2015</xref>; <xref ref-type="bibr" rid="bib56">O'Reilly et al., 2013</xref>; <xref ref-type="bibr" rid="bib74">Tang and Kreiman, 2017</xref>; <xref ref-type="bibr" rid="bib83">Wyatte et al., 2014</xref>). Thus, here recurrent activity may have filled-in missing visual information necessary to recognize the face target images. Such information is probably behaviorally relevant, as even relatively modest shifts in the latency of categorical information encoded rapidly in the ventral stream have been linked to corresponding shifts in behavioral response delays (<xref ref-type="bibr" rid="bib9">Cauchoix et al., 2016</xref>). Future work could benefit from our neuroimaging methodology to investigate the precise role of recurrent activity in EVC.</p><p>While the recurrent signals reported here could also reflect attentional modulations (<xref ref-type="bibr" rid="bib30">Hopf et al., 2006</xref>; <xref ref-type="bibr" rid="bib80">Tsotsos et al., 1995</xref>), this explanation is less likely. Rapid recurrent signals originating within the ventral pathway are involuntary (<xref ref-type="bibr" rid="bib66">Roland et al., 2006</xref>) and temporally dissociable from top-down attentional signals that are typically reported at later times (<xref ref-type="bibr" rid="bib4">Boehler et al., 2008</xref>; <xref ref-type="bibr" rid="bib83">Wyatte et al., 2014</xref>).</p><p>Our study used a fixed button mapping for the yes/no responses on faces in the RSVP task, which in principle may introduce motor plan-related signals in the MEG responses. However we believe it is unlikely our results are confounded by motor-related signals for the following reasons: First, our experimental design used a delayed response design for the RSVP conditions (response was prompted 0.7–1 s after the offset of the stimulus), and no response for the 500 ms per picture condition. Thus any motor artifacts, including motor preparation signals, should have been delayed considerably for the RSVP conditions, and are completely absent in the 500 ms per picture condition. Second, we believe that our analysis to localize these processes with fMRI-MEG fusion supports the source of categorical information in IT. Third, previous studies have shown that motor-preparation signals occurs considerably later (300-400 ms after stimulus onset) (<xref ref-type="bibr" rid="bib78">Thorpe et al., 1996</xref>). Thus it is unlikely the effects before 200 ms from stimulus onset reported in our data are related to motor preparation.</p><p>Taken together, the ultra-RSVP task enabled us to demarcate two different processing stages in the ventral stream, segregate the initial bottom-up sweep from late categorical signals, characterize the delay of the emergence of categorical information as an index of increased recurrent processing demands, and localize feedback activity in EVC. Our findings can motivate future experiments investigating the functional role of visual processes in the ventral pathway, capitalizing from the segregation of early and late processes achieved from the ultra-RSVP paradigm.</p></sec></sec><sec id="s3-2"><title>Statiotemporal bounds for computational models of vision</title><p>Most popular computer vision models, such as deep neural networks (<xref ref-type="bibr" rid="bib47">LeCun et al., 2015</xref>) and HMAX (<xref ref-type="bibr" rid="bib64">Riesenhuber and Poggio, 1999</xref>), have adopted a feedforward architecture to sequentially transform visual signals into complex representations, akin to the human ventral stream. Recent work has shown that these models not only achieve accuracy in par with human performance in many tasks (<xref ref-type="bibr" rid="bib27">He et al., 2015</xref>), but also share a hierarchical correspondence with neural object representations (<xref ref-type="bibr" rid="bib11">Cichy et al., 2016b</xref>; <xref ref-type="bibr" rid="bib52">Martin Cichy et al., 2017</xref>; <xref ref-type="bibr" rid="bib84">Yamins et al., 2014</xref>).</p><p>Even though models with purely feedforward architecture can easily recognize whole objects (<xref ref-type="bibr" rid="bib69">Serre et al., 2007</xref>), they often mislabel objects in challenging conditions, such as incongruent object-background pairings, or ambiguous and partially occluded inputs (<xref ref-type="bibr" rid="bib35">Johnson and Olshausen, 2005</xref>; <xref ref-type="bibr" rid="bib56">O'Reilly et al., 2013</xref>). Instead, models that incorporate recurrent connections are robust to partially occluded objects (<xref ref-type="bibr" rid="bib56">O'Reilly et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Rajaei et al., 2018</xref>), suggesting the importance of recurrent processing for object recognition.</p><p>Unlike other studies that use stimuli that are occluded or camouflaged (<xref ref-type="bibr" rid="bib72">Spoerer et al., 2017</xref>; <xref ref-type="bibr" rid="bib74">Tang and Kreiman, 2017</xref>), our RSVP task offers no obvious computation that can be embued to feedback processes when presentation times are shortened. That is, our study does not inform on the precise nature of computations needed for stimulus evidence accumulation when presentation times are extremely short. Despite this fact, our findings on the duration and sequencing of ventral stream processes can still offer insights for developing computational models with recursive architecture. First, such models should solve object categorization within the first couple hundred milliseconds, even when the feedforward and feedback pathways are compromised as in the ultra-RSVP task. Second, the timing of recurrent processes should not be predetermined and fixed, but vary depending on viewing conditions, as in the case of onset and peak latency shifts in the RSVP decoding time series. Here viewing conditions related to the speed of the RSVP task, but it is reasonable to expect other challenging conditions, such as ambiguous or partial stimuli, to exert time delays, though possibly longer (<xref ref-type="bibr" rid="bib74">Tang and Kreiman, 2017</xref>). Third, the timing and strength of the early visual signals should inversely determine the timing of categorical representations. And fourth, feedback processes in deep models should activate early layers of the model at the same time object representations are emerging at the last layers. Despite these insights, future research is needed to understand what recurrent computations are exactly carried out by the brain to solve visual recognition under RSVP-like presentation conditions.</p></sec><sec id="s3-3"><title>Ultra-fast rapid serial visual presentation as an experimental model to study recurrent neural processing</title><p>Since its conception (<xref ref-type="bibr" rid="bib60">Potter and Levy, 1969</xref>), the RSVP paradigm has been implemented with stimulus rates about 100 ms per item or slower. Inherent to the design, RSVP experiments have revealed the temporal limitations of human perception (<xref ref-type="bibr" rid="bib71">Spence, 2002</xref>), attention (<xref ref-type="bibr" rid="bib55">Nieuwenstein and Potter, 2006</xref>), and memory (<xref ref-type="bibr" rid="bib62">Potter, 1993</xref>). Recently however, behavioral investigations have been exploring even faster presentation rates in ultra-RSVP tasks (<xref ref-type="bibr" rid="bib6">Broers et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Evans et al., 2011</xref>; <xref ref-type="bibr" rid="bib61">Potter et al., 2014</xref>). These experiments found that observers can detect target images at rates 13 to 20 ms per picture.</p><p>Due to its effectiveness in masking stimuli by combining forward and backward masking, the ultra-RSVP paradigm could be used to address the question whether recurrent processing is necessary for recognition of objects. One view posits that a purely feedforward mode of processing is sufficient to extract meaning from complex natural scenes (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>). High behavioral performance in the ultra-RSVP task has been used as an argument to support this view. Specifically, such rapid presentations of stimuli have been presumed to block recurrent activity, since low level visual representations are immediately overwritten by subsequent images and time is too short to allow multiple synaptic transmissions (<xref ref-type="bibr" rid="bib79">Tovée, 1994</xref>).</p><p>However, this interpretation has been challenged both here, with the reengagement of EVC late in the processing stream, and by the finding that the ability to detect and categorize images at such speeds depends on the efficacy of the images to mask one another (<xref ref-type="bibr" rid="bib49">Maguire and Howe, 2016</xref>). Thus, it still remains an open question whether recurrent activity is necessary to extract conceptual meaning (<xref ref-type="bibr" rid="bib31">Howe, 2017</xref>).</p><p>Though our study did not address whether such recurrent activity can arise in more effective masking conditions that suppress visibility (<xref ref-type="bibr" rid="bib49">Maguire and Howe, 2016</xref>), it paves the way for future studies to explore the link between stimulus visibility and recurrent neuronal processes. Such studies could vary the effectiveness of forward and backward masking to segregate the early from late visual signals, as accomplished here, and investigate under what conditions (e.g. ambiguous or occluded input) stimulus visibility (<xref ref-type="bibr" rid="bib37">King et al., 2016</xref>; <xref ref-type="bibr" rid="bib67">Salti et al., 2015</xref>) is associated with feedback activity. As ultra-RSVP reduces visibility, future studies could also investigate whether recurrent activity is an integral component of the neural correlates of consciousness, defined as the minimum neuronal mechanisms jointly sufficient for a conscious percept (<xref ref-type="bibr" rid="bib38">Koch et al., 2016</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Seventeen healthy subjects (12 female; 16 right-handed and one left-handed; age mean ± s.d. 27.2 ± 5.7 years) with normal or corrected to normal vision participated in the RSVP experiment. They all signed an informed consent form and were compensated for their participation. The study was approved by the Institutional Review Board of the Massachusetts Institute of Technology and followed the principles of the Declaration of Helsinki.</p></sec><sec id="s4-2"><title>Experimental design and stimulus set</title><sec id="s4-2-1"><title>RSVP experiment</title><p>The stimulus set comprised 24 target images (12 faces and 12 objects) and 45 mask images of various object categories (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Images shown are not examples of the original stimulus set due to copyright; the exact stimulus set is visualized at <ext-link ext-link-type="uri" xlink:href="https://megrsvp.github.io.">https://megrsvp.github.io.</ext-link> We chose this stimulus set because it enabled comparison with MEG and fMRI data of a previous study (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>).</p><p>Participants viewed RSVP sequences of 11 images presented at a rate of 17 ms per picture or 34 ms per picture in separate trials. The middle image was randomly sampled from the set of 24 target images, and the surrounding images from the set of mask images. The stimuli were presented at the center of the screen against a gray background and subtended 2.9° of visual angle.</p><p>Each trial included a 0.5 s baseline time preceding the 17 ms per picture or 34 ms per picture RSVP sequence. At the end of the sequence a blank screen was presented for 0.7–1 s (uniformly distributed), which served to prevent motor artifacts, and then subjects were prompted to perform a two-alternative forced choice task reporting whether a face image was present in the sequence. That is, participants performed a yes/no task on faces without being informed on the alternative object (fruit/vegetable) target images. Responses were given with the right index finger using a MEG-compatible response box and a fixed button mapping for the face present and non-present response.</p><p>Trials were presented in random order in 12 blocks of 120 trials each, comprising both the 17 ms per picture and 34 ms per picture speed conditions interleaved randomly. In total, we collected 30 trials for each of the 24 target images and each of the two RSVP rates. The whole experiment lasted around 70 min. To avoid eye movement artifacts, the subjects were asked to fixate on a black cross presented at the center of the screen and blink only when pressing a button and not during the RSVP sequences.</p><p>Since the RSVP task was extremely challenging, before the experiment we trained participants for 5 min using a slower rate of 50 ms per picture. This assured the participants understood the task and could perform well during the higher presentation speeds.</p></sec><sec id="s4-2-2"><title>500 ms per picture experiment</title><p>In a separate MEG experiment, a different cohort of 16 healthy participants view the same 24 target images in isolation. Images were presented in random order for 500 ms with an ISI of 1 s. Participants were instructed to press a button and blink their eyes in response to a paper clip that was shown randomly every 3 to 5 trials. Participants completed 10–15 runs, with each image presented twice per run. We thus collected a total of 20–30 trials per target image. Note that the MEG data acquired for the 500 ms per picture experiment were part of a larger 92-image data set that has been previously published (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>).</p></sec><sec id="s4-2-3"><title>fMRI experiment</title><p>The same 24 target images were also presented to a different cohort of 15 healthy participants in an fMRI experiment. In particular, images were presented in random order for 500 ms with an ISI of 2.5 s. Participants completed 2 sessions of 10 to 14 runs each, and each image was presented once per run. Thirty null trials with no stimulus presentation were randomly interspersed, during which the fixation cross turned darker for 100 ms and participants reported the change with a button press. This resulted in 20–28 trials per target image. The fMRI data set was also part of a larger 92-image data set that has been previously published (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>).</p></sec></sec><sec id="s4-3"><title>MEG acquisition and preprocessing</title><p>We collected MEG data using a 306-channel Elekta Triux system with a 1000 Hz sampling rate. The data was band-pass filtered with cut-off frequencies of 0.03 and 330 Hz. The MEG system contained 102 triple sensor elements (2 gradiometers and one magnetometer each) organized on a helmet shaped array. The location of the head was measured continuously during MEG recording by activating a set of 5 head position indicator coils placed over the head.</p><p>The raw MEG data was preprocessed with the Maxfilter software (Elekta, Stockholm) to compensate for head movements and denoise the data using spatiotemporal filters (<xref ref-type="bibr" rid="bib77">Taulu and Simola, 2006</xref>; <xref ref-type="bibr" rid="bib76">Taulu et al., 2004</xref>). The Brainstorm software (<xref ref-type="bibr" rid="bib73">Tadel et al., 2011</xref>) was then used to extract trials from −300 ms to 900 ms with respect to target onset. Every trial was baseline-corrected to remove the mean from each channel during the baseline period, defined as the time before the onset of the first mask stimulus for the RSVP task, or the target image for the 500 ms per picture condition. A 6000 fT peak-to-peak rejection threshold was set to discard bad trials, and the remaining trials were smoothed with a 20 Hz low-pass filter. Eye blink artifacts were automatically detected from frontal sensor MEG data, and then principal component analysis was used to remove these artifacts.</p></sec><sec id="s4-4"><title>MEG multivariate pattern analysis</title><sec id="s4-4-1"><title>Sensor space</title><p>To extract information about visual stimuli from the MEG data, we used multivariate pattern analysis. The procedure was based on linear support vector machines (SVM) using the libsvm software implementation (<xref ref-type="bibr" rid="bib10">Chang and Lin, 2011</xref>) with a fixed regularization parameter C = 1. Before classification, the MEG trials for each target image were sub-averaged in groups of 3 with random assignment to reduce computational load, yielding <italic>M</italic> trials per target image (<italic>M</italic> was about 9–10 for the RVSP experiment when considering bad trials, and varied between 6 to 10 per subject for the 500 ms per picture condition).</p><p>The SVM analysis was performed separately for each subject in a time-resolved manner. Specifically, for each time point t (from −300 ms to 900 ms in 1 ms steps) the MEG sensor data were arranged in 306-dimensional pattern vectors, yielding <italic>M</italic> pattern vectors per time point and target image. Then for each time point and pair of images separately, we measured the performance of the classifier to discriminate between images using leave-one-out cross-validation: <italic>M</italic>-1 vectors were randomly assigned to the training set, and the left-out vector to the training set to evaluate the classifier decoding accuracy. By repeating the classification procedure 100 times, each with random trial sub-averaging, and averaging decoding accuracies over repetitions, we populated a time-resolved 24 × 24 decoding matrix, indexed in rows and columns by the classified target images (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). This decoding matrix, termed representational dissimilarity matrix (RDM), is symmetric and has an undefined diagonal (no classification within image). The entire procedure created one MEG RDM for each time point and subject.</p><p>Categorical division time series were constructed by dividing the MEG RDM matrix into partitions corresponding to within-category (face or object) and between-category stimulus comparisons. The difference of between-category minus within-category average decoding accuracies served as a measure of clustering by category membership. An alternate approach to computing categorical information time series is to directly train a classifier to discriminate face vs. object stimuli. While such methodological approach may be sensitive to different aspects of categorical stimulus information in general, it yielded consistent results in our data (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></sec><sec id="s4-4-2"><title>Source space</title><p>To perform multivariate pattern analysis on the cortex and localize representational information on regions of interest (ROIs), we mapped MEG signals on source space. Source activation maps were computed on cortical surfaces derived from Freesurfer automatic segmentation (<xref ref-type="bibr" rid="bib25">Fischl et al., 2004</xref>) of the Colin27 default anatomy (<xref ref-type="bibr" rid="bib29">Holmes et al., 1998</xref>). The forward model was calculated using an overlapping spheres model (<xref ref-type="bibr" rid="bib32">Huang et al., 1999</xref>). MEG signals were then mapped on a grid of ~15000 cortical sources using a dynamic statistical parametric mapping approach (dSPM) (<xref ref-type="bibr" rid="bib15">Dale et al., 2000</xref>) and time series were derived from sources within early visual cortex (EVC) and inferior temporal cortex (IT) (<xref ref-type="bibr" rid="bib16">Desikan et al., 2006</xref>).</p><p>Source-based multivariate pattern analysis for the two cortical ROIs, EVC and IT, was performed exactly as in sensor space, however time-resolved pattern vectors were created by concatenating activation values from cortical sources within a given ROI, rather than concatenating the whole-head sensor measurements. This procedure resulted in one MEG RDM for each time point, ROI, and subject.</p></sec></sec><sec id="s4-5"><title>Multidimensional scaling</title><p>To visualize the complex patterns of the 24 × 24 MEG RDMs, which capture the relations across the neural patterns elicited by the 24 target images, we used the first two dimensions of multidimensional scaling (MDS) (<xref ref-type="bibr" rid="bib43">Kruskal and Wish, 1978</xref>; <xref ref-type="bibr" rid="bib70">Shepard, 1980</xref>). MDS is an unsupervised method to visualize the level of similarity between different images contained in a distance matrix. Intuitively, MDS plotted the data in two dimensions where similar images were grouped together and different images far apart. fMRI data acquisition and analysis</p><p>The fMRI data was collected using a 3T Trio Siemens Scanner and 32-channel head coil. The structural images were acquired in the beginning of each session using T1-weighted sequences with TR = 1900 ms, TE = 2.52 ms, flip angle = 9°, FOV = 256 mm<sup>2</sup>, and 192 sagittal slices. Functional data was acquired with high spatial resolution but partial coverage of the brain covering occipital and temporal lobe using gradient-echo EPI sequence with TR = 2000 ms, TE = 31 ms, flip angle = 80°, FOV read = 192 mm, FOV phase = 100%, ascending acquisition, gap = 10%, resolution = 2 mm isotropic, and slices = 25.</p><p>The details of fMRI analysis can be found in (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>) and here we explain it briefly. SPM8 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>) was used to analyze the fMRI data. The data was realigned, re-sliced, and co-registered with the structural images for each subject and session separately. Then a general linear model analysis was used to estimate t-value maps for each of the 24 target images. We further defined two volumetric ROIs for fMRI data analysis, V1 and IT. V1 was defined separately for each participant using an anatomical eccentricity template (<xref ref-type="bibr" rid="bib3">Benson et al., 2012</xref>), and corresponded to a 0–6° visual angle. IT was defined using a mask comprising bilateral fusiform and inferior temporal cortex (<xref ref-type="bibr" rid="bib50">Maldjian et al., 2003</xref>), keeping the most strongly 361 activated voxels from a cross-validated dataset to match the size of IT to the average size of V1.</p></sec><sec id="s4-6"><title>fMRI multivariate pattern analysis</title><p>To assess the relations between brain fMRI responses across the 24 target images, we constructed space-resolved fMRI RDMs using a correlation-based method. We conducted two types of analyses: (1) ROI-based and (2) spatially unbiased using a searchlight approach.</p><p>For the ROI-based analysis, we extracted and concatenated the V1 or IT voxel t-values to form ROI-specific fMRI pattern vectors. For each pair of images, we then calculated the dissimilarity (one minus Pearson’s rho) between the fMRI pattern vectors, resulting in a 24 × 24 fMRI RDM indexed by the compared images. This procedure resulted in one fMRI RDM for each ROI and subject.</p><p>For the searchlight-based analysis (<xref ref-type="bibr" rid="bib41">Kriegeskorte et al., 2006</xref>), we constructed fMRI RDMs for each voxel in the brain. In particular, for each voxel v we extracted fMRI activation values in a sphere centered at v with a radius of 4 voxels (searchlight at v) and arranged them into fMRI pattern vectors. For each pair of images, we then calculated the pairwise dissimilarity (one minus Pearson’s rho) between fMRI pattern vectors, resulting in a 24 × 24 fMRI RDM indexed by the compared images. This procedure yielded one fMRI RDM for each voxel in the brain and subject.</p></sec><sec id="s4-7"><title>fMRI-MEG fusion using representational similarity analysis</title><p>To assess the spatiotemporal dynamics of EVC and IT, we applied a fMRI-MEG fusion approach based on representational similarity analysis (RSA) (<xref ref-type="bibr" rid="bib42">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>). The basic idea is that if two stimuli are similarly represented in MEG patterns, they should also be similarly represented in fMRI patterns, a correspondence that can be directly evaluated using the RDMs. Thus, we computed the similarity (Spearman’s rho) between time-resolved MEG RDMs and space-resolved fMRI RDMs.</p><p>For ROI-based fMRI-MEG fusion, we used fMRI RDMs and MEG RDMs from the corresponding ROIs. In particular, for each time point we computed the similarity (Spearman’s rho) between the subject-averaged MEG RDM and the subject-specific fMRI RDM. This procedure yielded <italic>n</italic> = 14 time courses of MEG-fMRI representational similarity for each ROI and subject.</p></sec><sec id="s4-8"><title>Temporal generalization of multivariate pattern analysis</title><p>To investigate whether maintenance of stimulus information was compromised in the RSVP relative to the 500 ms per picture condition, we extended the SVM classification procedure using a temporal generalization approach (<xref ref-type="bibr" rid="bib12">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">King and Dehaene, 2014</xref>; <xref ref-type="bibr" rid="bib57">Pantazis et al., 2017</xref>). This method involved training the SVM classifier at a given time point <italic>t</italic>, as before, but testing across all other time points. Intuitively, if representations are stable over time, the classifier should successfully discriminate signals not only at the trained time <italic>t</italic>, but also over extended periods of time that share the same neuronal representations. We repeated this temporal generalization analysis for every pair of stimuli, and the results were averaged across compared images and subjects, yielding 2-dimensional temporal generalization matrices with the x-axis denoting training time and the y-axis testing time.</p></sec><sec id="s4-9"><title>Peak latency analysis</title><p>For statistical assessment of peak and onset latency of the time series, we performed bootstrap tests. The subject-specific time series were bootstrapped 1000 times and the empirical distribution of the peak latency of the subject-averaged time series was used to define 95% confidence intervals. A similar procedure was used to define 95% confidence intervals for onset latency. For peak-to-peak latency differences, we obtained 1000 bootstrapped samples of the difference between the two peaks, which resulted in an empirical distribution of peak-to-peak latency differences. We then used the tail of this empirical distribution to evaluate the number of bootstrap samples that crossed 0, which allowed us to compute a p-value for the peak-to-peak latency difference. Finally, the p-values were corrected for multiple comparisons using false discovery rate at a 0.05 level. A similar procedure was used for onset-to-onset differences.</p><p>We had one cohort of subjects for the RSVP conditions, and another for the 500 ms per picture condition. For consistency, we performed between-subject comparisons in all comparisons across the 17, 34, and 500 ms per picture conditions.</p></sec><sec id="s4-10"><title>Statistical inference</title><p>Statistical inference relied on non-parametric statistical tests that do not make assumptions on the distributions of the data (<xref ref-type="bibr" rid="bib51">Maris and Oostenveld, 2007</xref>; <xref ref-type="bibr" rid="bib58">Pantazis et al., 2005</xref>). Specifically, for the statistical assessment of classification time series, temporal generalization matrices, and MEG-fMRI representational similarities we performed permutation-based cluster-size inference. The null hypothesis was equal to 50% chance level for decoding results, and 0 for decoding differences or correlation values. In all cases we could permute the condition labels of the MEG data, which was equivalent to a sign permutation test that randomly multiplied subject responses by +1 or −1. We used 1000 permutations, 0.05 cluster defining threshold and 0.05 cluster threshold for time series and temporal generalization maps.</p></sec><sec id="s4-11"><title>Source data and code</title><p>Data and Matlab code used for statistical analyses and producing results in main <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig5">5</xref>, are available as a Source data one file with this article.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We are thankful to Aude Oliva and Molly Potter for discussions on the interpretations of results. This work was funded by the McGovern Institute Neurotechnology Program to DP and an Emmy Noether Award (CI241/1-1) to RMC. Data collection was conducted at the Athinoula A Martinos Imaging Center at the McGovern Institute for Brain Research, MIT.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Wrote the paper. Assisted with designing the experiments, Conducting the experiments, Analyzing the data, Preparing the manuscript</p></fn><fn fn-type="con" id="con2"><p>Assisted with designing the experiments, Conducting the experiments, Analyzing the data</p></fn><fn fn-type="con" id="con3"><p>Assisted with designing the experiments, And preparing the manuscript</p></fn><fn fn-type="con" id="con4"><p>Designed the experiment. Wrote the paper. Assisted with conducting the experiments, Analyzing the data, Preparing the manuscript</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was approved by the Institutional Review Board of the Massachusetts Institute of Technology and followed the principles of the Declaration of Helsinki. All subjects signed an informed consent form and were compensated for their participation.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="sdata1"><object-id pub-id-type="doi">10.7554/eLife.36329.011</object-id><label>Source data 1.</label><caption><title><xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig5">5</xref> source data and code.</title><p>Decoding target images; resolving categorical information; and computing MEG-fMRI representational similarities.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-36329-data1-v2.zip"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.36329.012</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-36329-transrepform-v2.pdf"/></supplementary-material><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study to support the main findings are included in the manuscript and supporting files. Source data files have been provided for Figures 2, 3 and 5.</p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahissar</surname> <given-names>M</given-names></name><name><surname>Nahum</surname> <given-names>M</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name><name><surname>Hochstein</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reverse hierarchies and sensory learning</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>364</volume><fpage>285</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0253</pub-id><pub-id pub-id-type="pmid">18986968</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name><name><surname>Kassam</surname> <given-names>KS</given-names></name><name><surname>Ghuman</surname> <given-names>AS</given-names></name><name><surname>Boshyan</surname> <given-names>J</given-names></name><name><surname>Schmid</surname> <given-names>AM</given-names></name><name><surname>Schmidt</surname> <given-names>AM</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Hämäläinen</surname> <given-names>MS</given-names></name><name><surname>Marinkovic</surname> <given-names>K</given-names></name><name><surname>Schacter</surname> <given-names>DL</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Top-down facilitation of visual recognition</article-title><source>PNAS</source><volume>103</volume><fpage>449</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507062103</pub-id><pub-id pub-id-type="pmid">16407167</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname> <given-names>NC</given-names></name><name><surname>Butt</surname> <given-names>OH</given-names></name><name><surname>Datta</surname> <given-names>R</given-names></name><name><surname>Radoeva</surname> <given-names>PD</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name><name><surname>Aguirre</surname> <given-names>GK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The retinotopic organization of striate cortex is well predicted by surface topology</article-title><source>Current Biology</source><volume>22</volume><fpage>2081</fpage><lpage>2085</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.09.014</pub-id><pub-id pub-id-type="pmid">23041195</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehler</surname> <given-names>CN</given-names></name><name><surname>Schoenfeld</surname> <given-names>MA</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Hopf</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Rapid recurrent processing gates awareness in primary visual cortex</article-title><source>PNAS</source><volume>105</volume><fpage>8742</fpage><lpage>8747</lpage><pub-id pub-id-type="doi">10.1073/pnas.0801999105</pub-id><pub-id pub-id-type="pmid">18550840</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brincat</surname> <given-names>SL</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dynamic shape synthesis in posterior inferotemporal cortex</article-title><source>Neuron</source><volume>49</volume><fpage>17</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.11.026</pub-id><pub-id pub-id-type="pmid">16387636</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broers</surname> <given-names>N</given-names></name><name><surname>Potter</surname> <given-names>MC</given-names></name><name><surname>Nieuwenstein</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Enhanced recognition of memorable pictures in ultra-fast RSVP</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>1080</fpage><lpage>1086</lpage><pub-id pub-id-type="doi">10.3758/s13423-017-1295-7</pub-id><pub-id pub-id-type="pmid">28484948</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bullier</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Integrated model of visual processing</article-title><source>Brain Research Reviews</source><volume>36</volume><fpage>96</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/S0165-0173(01)00085-6</pub-id><pub-id pub-id-type="pmid">11690606</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camprodon</surname> <given-names>JA</given-names></name><name><surname>Zohary</surname> <given-names>E</given-names></name><name><surname>Brodbeck</surname> <given-names>V</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Two phases of V1 activity for visual recognition of natural images</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>1262</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21253</pub-id><pub-id pub-id-type="pmid">19413482</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cauchoix</surname> <given-names>M</given-names></name><name><surname>Crouzet</surname> <given-names>SM</given-names></name><name><surname>Fize</surname> <given-names>D</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast ventral stream neural activity enables rapid visual categorization</article-title><source>NeuroImage</source><volume>125</volume><fpage>280</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.10.012</pub-id><pub-id pub-id-type="pmid">26477655</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>C-C</given-names></name><name><surname>Lin</surname> <given-names>C-J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIBSVM: a library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><fpage>1</fpage><lpage>27</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id><pub-id pub-id-type="pmid">27282108</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Similarity-Based fusion of MEG and fMRI reveals Spatio-Temporal dynamics in human cortex during visual object recognition</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3563</fpage><lpage>3579</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw135</pub-id><pub-id pub-id-type="pmid">27235099</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multivariate pattern analysis of MEG and EEG: a comparison of representational structure in time and space</article-title><source>NeuroImage</source><volume>158</volume><fpage>441</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.023</pub-id><pub-id pub-id-type="pmid">28716718</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Liu</surname> <given-names>AK</given-names></name><name><surname>Fischl</surname> <given-names>BR</given-names></name><name><surname>Buckner</surname> <given-names>RL</given-names></name><name><surname>Belliveau</surname> <given-names>JW</given-names></name><name><surname>Lewine</surname> <given-names>JD</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamic statistical parametric mapping: combining fMRI and MEG for high-resolution imaging of cortical activity</article-title><source>Neuron</source><volume>26</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)81138-1</pub-id><pub-id pub-id-type="pmid">10798392</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname> <given-names>RS</given-names></name><name><surname>Ségonne</surname> <given-names>F</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Quinn</surname> <given-names>BT</given-names></name><name><surname>Dickerson</surname> <given-names>BC</given-names></name><name><surname>Blacker</surname> <given-names>D</given-names></name><name><surname>Buckner</surname> <given-names>RL</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Maguire</surname> <given-names>RP</given-names></name><name><surname>Hyman</surname> <given-names>BT</given-names></name><name><surname>Albert</surname> <given-names>MS</given-names></name><name><surname>Killiany</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><volume>31</volume><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name><name><surname>Cox</surname> <given-names>DD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id><pub-id pub-id-type="pmid">17631409</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name><name><surname>Zoccolan</surname> <given-names>D</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drewes</surname> <given-names>J</given-names></name><name><surname>Goren</surname> <given-names>G</given-names></name><name><surname>Zhu</surname> <given-names>W</given-names></name><name><surname>Elder</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Recurrent processing in the formation of shape percepts</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>185</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2347-15.2016</pub-id><pub-id pub-id-type="pmid">26740660</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enns</surname> <given-names>JT</given-names></name><name><surname>Di Lollo V</surname></name></person-group><year iso-8601-date="2000">2000</year><article-title>What's new in visual masking?</article-title><source>Trends in Cognitive Sciences</source><volume>4</volume><fpage>345</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01520-5</pub-id><pub-id pub-id-type="pmid">10962616</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname> <given-names>KK</given-names></name><name><surname>Horowitz</surname> <given-names>TS</given-names></name><name><surname>Wolfe</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>When categories collide: accumulation of information about multiple categories in rapid scene perception</article-title><source>Psychological Science</source><volume>22</volume><fpage>739</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1177/0956797611407930</pub-id><pub-id pub-id-type="pmid">21555522</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Masking disrupts reentrant processing in human visual cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>1488</fpage><lpage>1497</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.9.1488</pub-id><pub-id pub-id-type="pmid">17714010</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name><name><surname>Snijders</surname> <given-names>TM</given-names></name><name><surname>Heinen</surname> <given-names>K</given-names></name><name><surname>van Gaal</surname> <given-names>S</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuronal integration in visual cortex elevates face category tuning to conscious face perception</article-title><source>PNAS</source><volume>109</volume><fpage>21504</fpage><lpage>21509</lpage><pub-id pub-id-type="doi">10.1073/pnas.1207414110</pub-id><pub-id pub-id-type="pmid">23236162</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name><name><surname>van Leeuwen</surname> <given-names>J</given-names></name><name><surname>Olivers</surname> <given-names>CN</given-names></name><name><surname>Hogendoorn</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Perceptual integration without conscious access</article-title><source>PNAS</source><volume>114</volume><fpage>3744</fpage><lpage>3749</lpage><pub-id pub-id-type="doi">10.1073/pnas.1617268114</pub-id><pub-id pub-id-type="pmid">28325878</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Salat</surname> <given-names>DH</given-names></name><name><surname>van der Kouwe</surname> <given-names>AJ</given-names></name><name><surname>Makris</surname> <given-names>N</given-names></name><name><surname>Ségonne</surname> <given-names>F</given-names></name><name><surname>Quinn</surname> <given-names>BT</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Sequence-independent segmentation of magnetic resonance images</article-title><source>NeuroImage</source><volume>23</volume><fpage>S69</fpage><lpage>S84</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.016</pub-id><pub-id pub-id-type="pmid">15501102</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halgren</surname> <given-names>E</given-names></name><name><surname>Mendola</surname> <given-names>J</given-names></name><name><surname>Chong</surname> <given-names>CD</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cortical activation to illusory shapes as measured with magnetoencephalography</article-title><source>NeuroImage</source><volume>18</volume><fpage>1001</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00045-4</pub-id><pub-id pub-id-type="pmid">12725774</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</chapter-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><fpage>1026</fpage><lpage>1034</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochstein</surname> <given-names>S</given-names></name><name><surname>Ahissar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>View from the top: hierarchies and reverse hierarchies in the visual system</article-title><source>Neuron</source><volume>36</volume><fpage>791</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01091-7</pub-id><pub-id pub-id-type="pmid">12467584</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname> <given-names>CJ</given-names></name><name><surname>Hoge</surname> <given-names>R</given-names></name><name><surname>Collins</surname> <given-names>L</given-names></name><name><surname>Woods</surname> <given-names>R</given-names></name><name><surname>Toga</surname> <given-names>AW</given-names></name><name><surname>Evans</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Enhancement of MR images using registration for signal averaging</article-title><source>Journal of Computer Assisted Tomography</source><volume>22</volume><fpage>324</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1097/00004728-199803000-00032</pub-id><pub-id pub-id-type="pmid">9530404</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopf</surname> <given-names>JM</given-names></name><name><surname>Boehler</surname> <given-names>CN</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name><name><surname>Tsotsos</surname> <given-names>JK</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Schoenfeld</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Direct neurophysiological evidence for spatial suppression surrounding the focus of attention in vision</article-title><source>PNAS</source><volume>103</volume><fpage>1053</fpage><lpage>1058</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507746103</pub-id><pub-id pub-id-type="pmid">16410356</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howe</surname> <given-names>PDL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Natural scenes can be identified as rapidly as individual features</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>79</volume><fpage>1674</fpage><lpage>1681</lpage><pub-id pub-id-type="doi">10.3758/s13414-017-1349-y</pub-id><pub-id pub-id-type="pmid">28584956</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>MX</given-names></name><name><surname>Mosher</surname> <given-names>JC</given-names></name><name><surname>Leahy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A sensor-weighted overlapping-sphere head model and exhaustive head model comparison for MEG</article-title><source>Physics in Medicine and Biology</source><volume>44</volume><elocation-id>423</elocation-id><lpage>440</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/44/2/010</pub-id><pub-id pub-id-type="pmid">10070792</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hupé</surname> <given-names>JM</given-names></name><name><surname>James</surname> <given-names>AC</given-names></name><name><surname>Payne</surname> <given-names>BR</given-names></name><name><surname>Lomber</surname> <given-names>SG</given-names></name><name><surname>Girard</surname> <given-names>P</given-names></name><name><surname>Bullier</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical feedback improves discrimination between figure and background by V1, V2 and V3 neurons</article-title><source>Nature</source><volume>394</volume><fpage>784</fpage><lpage>787</lpage><pub-id pub-id-type="doi">10.1038/29537</pub-id><pub-id pub-id-type="pmid">9723617</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname> <given-names>L</given-names></name><name><surname>Meyers</surname> <given-names>EM</given-names></name><name><surname>Leibo</surname> <given-names>JZ</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The dynamics of invariant object recognition in the human visual system</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1152/jn.00394.2013</pub-id><pub-id pub-id-type="pmid">24089402</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>JS</given-names></name><name><surname>Olshausen</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The recognition of partially visible natural objects in the presence and absence of their occluders</article-title><source>Vision Research</source><volume>45</volume><fpage>3262</fpage><lpage>3276</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.06.007</pub-id><pub-id pub-id-type="pmid">16043208</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id><pub-id pub-id-type="pmid">24593982</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Pescetelli</surname> <given-names>N</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain Mechanisms Underlying the Brief Maintenance of Seen and Unseen Sensory Information</article-title><source>Neuron</source><volume>92</volume><fpage>1122</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.051</pub-id><pub-id pub-id-type="pmid">27930903</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Massimini</surname> <given-names>M</given-names></name><name><surname>Boly</surname> <given-names>M</given-names></name><name><surname>Tononi</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural correlates of consciousness: progress and problems</article-title><source>Nature Reviews Neuroscience</source><volume>17</volume><fpage>307</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.22</pub-id><pub-id pub-id-type="pmid">27094080</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koivisto</surname> <given-names>M</given-names></name><name><surname>Railo</surname> <given-names>H</given-names></name><name><surname>Revonsuo</surname> <given-names>A</given-names></name><name><surname>Vanni</surname> <given-names>S</given-names></name><name><surname>Salminen-Vaparanta</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Recurrent processing in V1/V2 contributes to categorization of natural scenes</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>2488</fpage><lpage>2492</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3074-10.2011</pub-id><pub-id pub-id-type="pmid">21325516</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kovács</surname> <given-names>G</given-names></name><name><surname>Vogels</surname> <given-names>R</given-names></name><name><surname>Orban</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Cortical correlate of pattern backward masking</article-title><source>PNAS</source><volume>92</volume><fpage>5587</fpage><lpage>5591</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.12.5587</pub-id><pub-id pub-id-type="pmid">7777553</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Information-based functional brain mapping</article-title><source>PNAS</source><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id><pub-id pub-id-type="pmid">16537458</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kruskal</surname> <given-names>JB</given-names></name><name><surname>Wish</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>Multidimensional Scaling</source><publisher-name>Sage</publisher-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title><source>Trends in Neurosciences</source><volume>23</volume><fpage>571</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01657-X</pub-id><pub-id pub-id-type="pmid">11074267</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Supèr</surname> <given-names>H</given-names></name><name><surname>Spekreijse</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Feedforward, horizontal, and feedback processing in the visual cortex</article-title><source>Current Opinion in Neurobiology</source><volume>8</volume><fpage>529</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(98)80042-1</pub-id><pub-id pub-id-type="pmid">9751656</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Zipser</surname> <given-names>K</given-names></name><name><surname>Spekreijse</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Masking interrupts figure-ground signals in V1</article-title><source>Journal of Cognitive Neuroscience</source><volume>14</volume><fpage>1044</fpage><lpage>1053</lpage><pub-id pub-id-type="doi">10.1162/089892902320474490</pub-id><pub-id pub-id-type="pmid">12419127</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>A</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Stages of processing in face perception: an MEG study</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>910</fpage><lpage>916</lpage><pub-id pub-id-type="doi">10.1038/nn909</pub-id><pub-id pub-id-type="pmid">12195430</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maguire</surname> <given-names>JF</given-names></name><name><surname>Howe</surname> <given-names>PD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Failure to detect meaning in RSVP at 27 ms per picture</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>78</volume><fpage>1405</fpage><lpage>1413</lpage><pub-id pub-id-type="doi">10.3758/s13414-016-1096-5</pub-id><pub-id pub-id-type="pmid">27048442</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maldjian</surname> <given-names>JA</given-names></name><name><surname>Laurienti</surname> <given-names>PJ</given-names></name><name><surname>Kraft</surname> <given-names>RA</given-names></name><name><surname>Burdette</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>An automated method for neuroanatomic and cytoarchitectonic atlas-based interrogation of fMRI data sets</article-title><source>NeuroImage</source><volume>19</volume><fpage>1233</fpage><lpage>1239</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00169-1</pub-id><pub-id pub-id-type="pmid">12880848</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin Cichy</surname> <given-names>R</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamics of scene representations in the human brain revealed by magnetoencephalography and deep neural networks</article-title><source>NeuroImage</source><volume>153</volume><fpage>346</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.03.063</pub-id><pub-id pub-id-type="pmid">27039703</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muckli</surname> <given-names>L</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Vizioli</surname> <given-names>L</given-names></name><name><surname>Petro</surname> <given-names>LS</given-names></name><name><surname>Smith</surname> <given-names>FW</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Contextual feedback to superficial layers of V1</article-title><source>Current Biology</source><volume>25</volume><fpage>2690</fpage><lpage>2695</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.057</pub-id><pub-id pub-id-type="pmid">26441356</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>MM</given-names></name><name><surname>Wylie</surname> <given-names>GR</given-names></name><name><surname>Higgins</surname> <given-names>BA</given-names></name><name><surname>Javitt</surname> <given-names>DC</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The spatiotemporal dynamics of illusory contour processing: combined high-density electrical mapping, source analysis, and functional magnetic resonance imaging</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>5055</fpage><lpage>5073</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-12-05055.2002</pub-id><pub-id pub-id-type="pmid">12077201</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieuwenstein</surname> <given-names>MR</given-names></name><name><surname>Potter</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Temporal limits of selection and memory encoding: a comparison of whole versus partial report in rapid serial visual presentation</article-title><source>Psychological Science</source><volume>17</volume><fpage>471</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2006.01730.x</pub-id><pub-id pub-id-type="pmid">16771795</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Reilly</surname> <given-names>RC</given-names></name><name><surname>Wyatte</surname> <given-names>D</given-names></name><name><surname>Herd</surname> <given-names>S</given-names></name><name><surname>Mingus</surname> <given-names>B</given-names></name><name><surname>Jilk</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Recurrent processing during object recognition</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>124</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00124</pub-id><pub-id pub-id-type="pmid">23554596</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Fang</surname> <given-names>M</given-names></name><name><surname>Qin</surname> <given-names>S</given-names></name><name><surname>Mohsenzadeh</surname> <given-names>Y</given-names></name><name><surname>Li</surname> <given-names>Q</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decoding the orientation of contrast edges from MEG evoked and induced responses</article-title><source>NeuroImage</source><elocation-id>S1053-8119(17)30590-6</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.022</pub-id><pub-id pub-id-type="pmid">28712993</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name><name><surname>Baillet</surname> <given-names>S</given-names></name><name><surname>Leahy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A comparison of random field theory and permutation methods for the statistical analysis of MEG data</article-title><source>NeuroImage</source><volume>25</volume><fpage>383</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.09.040</pub-id><pub-id pub-id-type="pmid">15784416</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peyrin</surname> <given-names>C</given-names></name><name><surname>Michel</surname> <given-names>CM</given-names></name><name><surname>Schwartz</surname> <given-names>S</given-names></name><name><surname>Thut</surname> <given-names>G</given-names></name><name><surname>Seghier</surname> <given-names>M</given-names></name><name><surname>Landis</surname> <given-names>T</given-names></name><name><surname>Marendaz</surname> <given-names>C</given-names></name><name><surname>Vuilleumier</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The neural substrates and timing of top-down processes during coarse-to-fine categorization of visual scenes: a combined fMRI and ERP study</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>2768</fpage><lpage>2780</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21424</pub-id><pub-id pub-id-type="pmid">20044901</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potter</surname> <given-names>MC</given-names></name><name><surname>Levy</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Recognition memory for a rapid sequence of pictures</article-title><source>Journal of Experimental Psychology</source><volume>81</volume><elocation-id>10</elocation-id><lpage>15</lpage><pub-id pub-id-type="doi">10.1037/h0027470</pub-id><pub-id pub-id-type="pmid">5812164</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potter</surname> <given-names>MC</given-names></name><name><surname>Wyble</surname> <given-names>B</given-names></name><name><surname>Hagmann</surname> <given-names>CE</given-names></name><name><surname>McCourt</surname> <given-names>ES</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Detecting meaning in RSVP at 13 ms per picture</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>76</volume><fpage>270</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.3758/s13414-013-0605-z</pub-id><pub-id pub-id-type="pmid">24374558</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potter</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Very short-term conceptual memory</article-title><source>Memory &amp; Cognition</source><volume>21</volume><fpage>156</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.3758/BF03202727</pub-id><pub-id pub-id-type="pmid">8469123</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rajaei</surname> <given-names>K</given-names></name><name><surname>Mohsenzadeh</surname> <given-names>Y</given-names></name><name><surname>Ebrahimpour</surname> <given-names>R</given-names></name><name><surname>Khaligh-Razavi</surname> <given-names>S-M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Beyond core object recognition: recurrent processes account for object recognition under occlusion</article-title><source>Biorxiv</source><pub-id pub-id-type="doi">10.1101/302034</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id><pub-id pub-id-type="pmid">10526343</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ringach</surname> <given-names>DL</given-names></name><name><surname>Hawken</surname> <given-names>MJ</given-names></name><name><surname>Shapley</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Dynamics of orientation tuning in macaque primary visual cortex</article-title><source>Nature</source><volume>387</volume><fpage>281</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1038/387281a0</pub-id><pub-id pub-id-type="pmid">9153392</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roland</surname> <given-names>PE</given-names></name><name><surname>Hanazawa</surname> <given-names>A</given-names></name><name><surname>Undeman</surname> <given-names>C</given-names></name><name><surname>Eriksson</surname> <given-names>D</given-names></name><name><surname>Tompa</surname> <given-names>T</given-names></name><name><surname>Nakamura</surname> <given-names>H</given-names></name><name><surname>Valentiniene</surname> <given-names>S</given-names></name><name><surname>Ahmed</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical feedback depolarization waves: a mechanism of top-down influence on early visual areas</article-title><source>PNAS</source><volume>103</volume><fpage>12586</fpage><lpage>12591</lpage><pub-id pub-id-type="doi">10.1073/pnas.0604925103</pub-id><pub-id pub-id-type="pmid">16891418</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salti</surname> <given-names>M</given-names></name><name><surname>Monto</surname> <given-names>S</given-names></name><name><surname>Charles</surname> <given-names>L</given-names></name><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Parkkonen</surname> <given-names>L</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct cortical codes and temporal dynamics for conscious and unconscious percepts</article-title><source>eLife</source><volume>4</volume><elocation-id>e05652</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05652</pub-id><pub-id pub-id-type="pmid">25997100</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sehatpour</surname> <given-names>P</given-names></name><name><surname>Molholm</surname> <given-names>S</given-names></name><name><surname>Schwartz</surname> <given-names>TH</given-names></name><name><surname>Mahoney</surname> <given-names>JR</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Javitt</surname> <given-names>DC</given-names></name><name><surname>Stanton</surname> <given-names>PK</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A human intracranial study of long-range oscillatory coherence across a frontal-occipital-hippocampal brain network during visual object processing</article-title><source>PNAS</source><volume>105</volume><fpage>4399</fpage><lpage>4404</lpage><pub-id pub-id-type="doi">10.1073/pnas.0708418105</pub-id><pub-id pub-id-type="pmid">18334648</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A feedforward architecture accounts for rapid categorization</article-title><source>PNAS</source><volume>104</volume><fpage>6424</fpage><lpage>6429</lpage><pub-id pub-id-type="doi">10.1073/pnas.0700622104</pub-id><pub-id pub-id-type="pmid">17404214</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Multidimensional scaling, tree-fitting, and clustering</article-title><source>Science</source><volume>210</volume><fpage>390</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1126/science.210.4468.390</pub-id><pub-id pub-id-type="pmid">17837406</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Rapid, serial and visual: a presentation technique with potential</article-title><source>Information Visualization</source><volume>1</volume><fpage>13</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1057/palgrave.ivs.9500008</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spoerer</surname> <given-names>CJ</given-names></name><name><surname>McClure</surname> <given-names>P</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Recurrent convolutional neural networks: a better model of biological object recognition</article-title><source>Frontiers in Psychology</source><volume>8</volume><elocation-id>1551</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2017.01551</pub-id><pub-id pub-id-type="pmid">28955272</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadel</surname> <given-names>F</given-names></name><name><surname>Baillet</surname> <given-names>S</given-names></name><name><surname>Mosher</surname> <given-names>JC</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Leahy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Brainstorm: a user-friendly application for MEG/EEG analysis</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1155/2011/879716</pub-id><pub-id pub-id-type="pmid">21584256</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tang</surname> <given-names>H</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Recognition of Occluded Objects</chapter-title><person-group person-group-type="editor"><name><surname>Zhao</surname> <given-names>Q</given-names></name></person-group><source>Computational and Cognitive Neuroscience of Vision</source><publisher-loc>Singapore</publisher-loc><publisher-name>Springer Singapore</publisher-name><fpage>41</fpage><lpage>58</lpage></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tapia</surname> <given-names>E</given-names></name><name><surname>Beck</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Probing feedforward and feedback contributions to awareness with visual masking and transcranial magnetic stimulation</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>1173</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.01173</pub-id><pub-id pub-id-type="pmid">25374548</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taulu</surname> <given-names>S</given-names></name><name><surname>Kajola</surname> <given-names>M</given-names></name><name><surname>Simola</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Suppression of interference and artifacts by the signal space separation method</article-title><source>Brain Topography</source><volume>16</volume><fpage>269</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1023/B:BRAT.0000032864.93890.f9</pub-id><pub-id pub-id-type="pmid">15379226</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taulu</surname> <given-names>S</given-names></name><name><surname>Simola</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spatiotemporal signal space separation method for rejecting nearby interference in MEG measurements</article-title><source>Physics in Medicine and Biology</source><volume>51</volume><fpage>1759</fpage><lpage>1768</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/51/7/008</pub-id><pub-id pub-id-type="pmid">16552102</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname> <given-names>S</given-names></name><name><surname>Fize</surname> <given-names>D</given-names></name><name><surname>Marlot</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><volume>381</volume><elocation-id>520</elocation-id><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/381520a0</pub-id><pub-id pub-id-type="pmid">8632824</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tovée</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Neuronal processing. How fast is the speed of thought?</article-title><source>Current Biology</source><volume>4</volume><fpage>1125</fpage><lpage>1127</lpage><pub-id pub-id-type="doi">10.1016/S0960-9822(00)00253-0</pub-id><pub-id pub-id-type="pmid">7704578</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsotsos</surname> <given-names>JK</given-names></name><name><surname>Culhane</surname> <given-names>SM</given-names></name><name><surname>Kei Wai</surname> <given-names>WY</given-names></name><name><surname>Lai</surname> <given-names>Y</given-names></name><name><surname>Davis</surname> <given-names>N</given-names></name><name><surname>Nuflo</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Modeling visual attention via selective tuning</article-title><source>Artificial Intelligence</source><volume>78</volume><fpage>507</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(95)00025-9</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wokke</surname> <given-names>ME</given-names></name><name><surname>Sligte</surname> <given-names>IG</given-names></name><name><surname>Steven Scholte</surname> <given-names>H</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Two critical periods in early visual cortex during figure-ground segregation</article-title><source>Brain and Behavior</source><volume>2</volume><fpage>763</fpage><lpage>777</lpage><pub-id pub-id-type="doi">10.1002/brb3.91</pub-id><pub-id pub-id-type="pmid">23170239</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wokke</surname> <given-names>ME</given-names></name><name><surname>Vandenbroucke</surname> <given-names>AR</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Confuse your illusion: feedback to early visual cortex contributes to perceptual completion</article-title><source>Psychological Science</source><volume>24</volume><fpage>63</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1177/0956797612449175</pub-id><pub-id pub-id-type="pmid">23228938</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyatte</surname> <given-names>D</given-names></name><name><surname>Jilk</surname> <given-names>DJ</given-names></name><name><surname>O'Reilly</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Early recurrent feedback facilitates visual object recognition under challenging conditions</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>674</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00674</pub-id><pub-id pub-id-type="pmid">25071647</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Cadieu</surname> <given-names>CF</given-names></name><name><surname>Solomon</surname> <given-names>EA</given-names></name><name><surname>Seibert</surname> <given-names>D</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshino</surname> <given-names>A</given-names></name><name><surname>Kawamoto</surname> <given-names>M</given-names></name><name><surname>Yoshida</surname> <given-names>T</given-names></name><name><surname>Kobayashi</surname> <given-names>N</given-names></name><name><surname>Shigemura</surname> <given-names>J</given-names></name><name><surname>Takahashi</surname> <given-names>Y</given-names></name><name><surname>Nomura</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Activation time course of responses to illusory contours and salient region: a high-density electrical mapping comparison</article-title><source>Brain Research</source><volume>1071</volume><fpage>137</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2005.11.089</pub-id><pub-id pub-id-type="pmid">16413507</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.36329.015</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Reviewing Editor</role><aff id="aff4"><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Ultra-rapid serial visual presentation disentangles feedforward and feedback processes in the ventral visual pathway&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Floris P de Lange as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Michael Frank as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Johannes Jacobus Fahrenfort (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The goal of this study is to try to dissociate between feedforward and feedback processes during visual recognition. Human observers were engaged in a face vs. object categorization task using an RSVP paradigm. MEG decoding for all pairs of target images was attempted as a function of time. This lead to a first estimate of information flow interpreted as a feedforward sweep. Decoding was then attempted for face vs. object categorization and the associated information flow was delayed in time and interpreted as feedback processes.</p><p>The reviewers found this to be an original study using sound analyses and the main conclusions of the paper are well supported by the data. That said, there were concerns with respect to the analysis choice: why not directly try to classify faces vs. objects and instead perform a binary image classification followed by an inter- vs. intra-class averaging? There were different viewpoints on this issue by the different reviewers, so it would be required that you clarify the rationale for the analysis choice, and/or include the more direct inter-class classification analysis. Furthermore, there were concerns with respect to the relevance of the work for the development of computational models, and the interpretation of the results.</p><p>Essential revisions:</p><p>1) The analysis focuses on decoding image identity for pairs of images (either averaged over all pairs or looking at the difference when the pairs fall on the same side or on opposite side of the categorization boundary.) This is different from the task that the participants are engaged in (face vs. objects). Why did the authors not directly try to classify faces vs. objects, i.e. attempt to match the readout with the behavioral task? More generally, it is not completely clear how the various signals that are read out from MEG relate to behavioral decisions.</p><p>2) It is not clear how the various readouts that are performed relate to task-relevant neural computations. The authors discuss the relevance of their results for the development of computational models but this is not all that convincing because 1) there is no obvious computation that can be embued to the feedback when presentation times are shortened (unlike in some of the studies that are cited which use stimuli that are occluded or camouflaged); 2) the present study does not contribute significantly new knowledge that is directly relevant to constraining computational models (except for the fact that feedback should follow the feedforward pass in a way that is not &quot;predetermined or fixed&quot;).</p><p>3) Figure 2E: Looks at peak latency to discuss limiting evidence accumulation. Eyeballing, the <italic>onset</italic> latencies in this graph are reliably faster for longer duration presentations (also consistently between within-face and within-object). I have observed similar effects in my own data (i.e. faster onsets for longer stimulus durations). This is potentially interesting to look at and discuss, because this not only speaks (1) to the emergence of feedforward activity but in addition (2) shows its early interaction with recurrent activity. It also goes against the argument that early decoding delinates feedforward activity alone, and paints a picture that is slightly more subtle. If early decoding would only reflect feedforward, we would not expect to see latency onset differences, but we do. The fact that different stimulus durations have different onsets suggests that feedback is already incorporated when the first decoding onsets emerge, going against the oversimplification that the early part of the decoding time course can be uniquely tied to feedforward alone. I think the manuscript would benefit from making this intricacy explict, the reader should not be left with the impression that we can fully separate feedforward and feedback. Figure 4A also suggests that feedback is already incorporated at a very early stage during long lasting presentations (recurrence mediated temporal generalization), and so do the onset latencies in Table 2.</p><p>4) &quot;How did the disruption of the first sweep of visual activity, reported in the previous section, affect the emergence of categorical information in the RSVP conditions?&quot; and &quot;According to these theories, presenting stimuli at rapid presentation rates would compromise feedforward signals thus increase the need for recurrent processes to fill-in missing stimulus information.&quot;</p><p>Consider rephrasing this paragraph to more exactly reflect what is going on. Two things occur in the current paradigm / analyses:</p><p>1) From Figure 2: Short SOAs have less time to continue initial evidence accumulation (a process that in all likelihood already incorporates some local feedback, see previous comment).</p><p>2) From Figure 3 (and also according to the referenced theories) feedback signals are interrupted/hampered by the incoming feedforward signals of the mask at short SOAs (i.e. the feedforward signal of the ensuing mask interrupts the feedback signal of the target stimulus, see also Fahrenfort et al., 2007 and references below). This apparently slows down the speed and extent with which category information can be resolved using recurrence.</p><p>These two things could be better described/disentangled in this paragraph.</p><p>The current phrasing suggest that feedforward activation (rather than recurrence) is interrupted. Moreover, the current phrasing makes it sound as information is actually missing, as in Muckli et al., et al.2015.</p><p>This is not the case, there is not enough time to accumulate information in time, hence the contribution of recurrent processing to resolve image category.</p><p>Other relevant references in this context: Kovacs, Vogels and Orban, 1995;.O'Reilly et al., 2013; Fahrenfort et al., 2012.</p><p>5) Comparison of different conditions:</p><p>One set of participants engaged in the RSVP task (17 ms and 34 ms). A different set of participants engaged in the 1500 ms condition (recycled data from a previous experiment). This makes me wonder how the data are analyzed: in a within-subject, or between-subject fashion? What was the task for the 1500 ms condition? This should be added to the Materials and methods section.</p><p>6) Response mapping:</p><p>Was there a fixed button mapping, i.e. did subjects always press a particular button for a particular choice? If so, can the authors rule out that the 'categorical information' classified in Figure 3 pertains to the motor plan, rather than a perceptual categorization? Presumably the motor plan is also delayed for the more difficult conditions.</p><p>7) Peak latency analysis:</p><p>I didn't find any detail on how peak latency was established. Were they determined within every subject, or on the grand average (using jackknifing or boot-strapping to obtain the confidence intervals)? What was the exact procedure used to identify the peak? Given that most results hinge on the latency analyses, it is imperative that they are described in detail.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.36329.016</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>[…] The reviewers found this to be an original study using sound analyses and the main conclusions of the paper are well supported by the data. That said, there were concerns with respect to the analysis choice: why not directly try to classify faces vs. objects and instead perform a binary image classification followed by an inter- vs. intra-class averaging? There were different viewpoints on this issue by the different reviewers, so it would be required that you clarify the rationale for the analysis choice, and/or include the more direct inter-class classification analysis. Furthermore, there were concerns with respect to the relevance of the work for the development of computational models, and the interpretation of the results.</p></disp-quote><p>We thank the reviewers for highlighting these two critical points: a) the methodological choice of binary image classification followed by inter- vs. intra-class averaging; and b) the relevance of the work for the development of computational models.</p><p>Regarding point (a) – classifier choice:</p><p>We acknowledge that the two different methodological approaches – i) direct classification of faces vs. objects, and ii) binary image classification followed by inter- vs. intra-class averaging – may be sensitive to different aspects of categorical information in principle. However, both are valid and will often yield convergent results. Following the reviewers’ request, we performed direct categorical decoding on the data and the result is presented in Figure 3—figure supplement 1. The results, now added as supplemental material, largely replicate the results in Figure 3A (which uses binary classification followed by inter- vs. intra-class averaging).</p><p>The rationale behind using a binary classification in the main text is that we are also interested in comparing data from two different modalities (MEG and fMRI) using representational similarity analysis. Binary classification here creates the entries of RDM matrices from MEG data by detailing pair-wise classification results. We eventually compare the MEG RDMs and fMRI RDMs in Figure 5. Furthermore, in the context of this study the pairwise decoding approach is a more versatile approach, enabling us to study in one go within category decoding as well as categorical effect (between – within contrasts), and also construct multidimensional scaling visualizations as in Figure 3.</p><p>We now clarify our methodological choice to use binary classification MEG RDM matrices to construct categorical division time series in the Materials and methods section:</p><p>“Categorical division time series were constructed by dividing the MEG RDM matrix into partitions corresponding to within-category (face or object) and between-category stimulus comparisons. […] While such methodological approach may be sensitive to different aspects of categorical stimulus information in general, it yielded consistent results in our data (Figure 3—figure supplement 1).”</p><p>Regarding point (b) – relevance to computer models of vision:</p><p>We agree with the reviewers that there are limitations in how our work may contribute to the development of computational models. In particular, we acknowledge that unlike occlusion or camouflaged stimuli, shortening the presentation time of stimuli as in our RSVP task does not directly suggest an obvious computation for feedback processes. However, we do believe our work can still offer valuable insights in computational models with recurrent architectures. We have thus modified the Discussion to tone down our claims and clarify the above limitations.</p><p>“Unlike other studies that use stimuli that are occluded or camouflaged (Spoerer et al., 2017; Tang and Kreiman, 2017), our RSVP task offers no obvious computation that can be embued to feedback processes when presentation times are shortened. […] Despite these insights, future research is needed to understand what recurrent computations are exactly carried out by the brain to solve visual recognition under RSVP-like presentation conditions.”</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The analysis focuses on decoding image identity for pairs of images (either averaged over all pairs or looking at the difference when the pairs fall on the same side or on opposite side of the categorization boundary.) This is different from the task that the participants are engaged in (face vs. objects). Why did the authors not directly try to classify faces vs. objects, i.e. attempt to match the readout with the behavioral task? More generally, it is not completely clear how the various signals that are read out from MEG relate to behavioral decisions.</p></disp-quote><p>We thank the reviewers for highlighting this point. Please see our detailed response to the summary above, which clarifies the merits of performing binary classification (i.e. allows us to i) compare data from two different modalities – MEG and fMRI; ii) perform multidimensional scaling; and iii) characterize both intra- and inter-class decoding). We also demonstrate that this method yields largely equivalent results to direct classification of faces vs. objects. These results are now added as Figure 3—figure supplement 1.</p><p>Regarding the behavioral data, since this was a delayed response task, only accuracy and not reaction times are relevant. Performance on the 34ms per picture condition was almost perfect (i.e. nearly all trials had correct ‘seen’ responses), and thus only the 17ms per picture condition, with performance consistently above chance (Figure 1C), is interesting for behavioral analysis. But unfortunately, in our paradigm correct and incorrect ‘seen’ trials greatly varied across stimuli and participants, and often individual stimuli were consistently seen or unseen. This confounds any analysis of behavioral decisions with individual stimuli. Future work conducting experiments which adjust visibility per stimulus and participant are needed to better clarify the relation of behavior to neural signals. Nevertheless, following the reviewers’ comment, we performed an analysis to decode seen versus unseen <italic>face</italic> trials in the 17ms per picture RSVP condition (presented in Figure R1). The results show we can decode the seen vs. unseen condition (subject to the confound above). Such analysis is not possible for the 34ms per picture condition because nearly all trials had correct ‘seen’ responses. Since this is not the focus of the current study, and given these results are confounded by stimulus identity, we opted not to include these results in the manuscript.</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.36329.014</object-id><label>Author response image 1.</label><caption><title>Decoding seen versus unseen faces in the 17ms per picture condition.</title></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-36329-resp-fig1-v2"/></fig><disp-quote content-type="editor-comment"><p>2) It is not clear how the various readouts that are performed relate to task-relevant neural computations. The authors discuss the relevance of their results for the development of computational models but this is not all that convincing because 1) there is no obvious computation that can be embued to the feedback when presentation times are shortened (unlike in some of the studies that are cited which use stimuli that are occluded or camouflaged); 2) the present study does not contribute significantly new knowledge that is directly relevant to constraining computational models (except for the fact that feedback should follow the feedforward pass in a way that is not &quot;predetermined or fixed&quot;).</p></disp-quote><p>We agree with the reviewer that, unlike occlusion or camouflaged stimuli, shortening the presentation time of stimuli as in our RSVP task does not offer an obvious computation for feedback processes. We thus acknowledge the limitations of our work in informing the design of computational models. However we do believe our work does offer valuable insight for computational models with recurrent architectures. For example, we believe the onset and peak latency shifts suggest explicit temporal constraints for the representational dynamics of recurrent networks; and the inverse timing of early signals with respect to late categorical representations suggest biologically plausible interactions that support object recognition in computation networks. We have thus modified the Discussion to tone down our claims, clarify the above limitations, and better specify how this work can contribute to the design of computational models.</p><p>“Unlike other studies that use stimuli that are occluded or camouflaged (Spoerer et al., 2017; Tang and Kreiman, 2017), our RSVP task offers no obvious computation that can be embued to feedback processes when presentation times are shortened. […] Despite these insights, future research is needed to understand what recurrent computations are necessary to solve visual recognition under shortened stimulus presentations related to our RSVP task.”</p><disp-quote content-type="editor-comment"><p>3) Figure 2E: Looks at peak latency to discuss limiting evidence accumulation. Eyeballing, the onset latencies in this graph are reliably faster for longer duration presentations (also consistently between within-face and within-object). I have observed similar effects in my own data (i.e. faster onsets for longer stimulus durations). This is potentially interesting to look at and discuss, because this not only speaks (1) to the emergence of feedforward activity but in addition (2) shows its early interaction with recurrent activity. It also goes against the argument that early decoding delinates feedforward activity alone, and paints a picture that is slightly more subtle. If early decoding would only reflect feedforward, we would not expect to see latency onset differences, but we do. The fact that different stimulus durations have different onsets suggests that feedback is already incorporated when the first decoding onsets emerge, going against the oversimplification that the early part of the decoding time course can be uniquely tied to feedforward alone. I think the manuscript would benefit from making this intricacy explict, the reader should not be left with the impression that we can fully separate feedforward and feedback. Figure 4A also suggests that feedback is already incorporated at a very early stage during long lasting presentations (recurrence mediated temporal generalization), and so do the onset latencies in Table 2.</p></disp-quote><p>We thank the reviewer for directing our attention to onset latencies. The reviewer offers great insights in the role of variable onset latencies as an index of rapid recurrent activity. Convinced by the reviewer’s argument, we modified Figure 2H, I, J and the Results and Discussion sections to include these results.</p><p>The reviewer mentioned has observed similar results in his/her data; we were unable to find a related publication, but we are of course happy to cite this work if the reviewer shares the related information.</p><p>“Fourth, onset latencies shifter later with faster stimulus presentation rates (Figure 2H). That is, the 0.5 s per picture condition had onset at 28 ms (9-53 ms), followed by the 34 ms per picture RSVP condition at 64 ms (58-69 ms), and finally the 17 ms per picture RSVP condition at 70 ms (65-77 ms) (all statistically different; <italic>P</italic>&lt; 0.05; two-sided sign permutation tests).”</p><p>“The decreased decoding accuracy combined with the increasingly early peak latency and increasingly late onset latency for the RSVP conditions indicate that visual activity was disrupted over the first 100 ms. […] The fact that different stimulus durations have different onsets suggests that interactions with recurrent activity are already incorporated when the first decoding onsets emerge, arguing against the view that the early part of the decoding time course can be uniquely tied to feedforward alone (Fahrenfort et al., 2012; Lamme and Roelfsema, 2000; Ringach et al., 1997).</p><p>“In sum, decoding accuracies decreased with progressively shorter stimulus presentation times, indicating that neuronal signals encoded less stimulus information at rapid presentation rates. […] Importantly, the progressively earlier peak with shorter presentation times indicated disruption of the first sweep of visual activity, thus indexing feedforward and local recurrent processing and segregating it in time from subsequent processing that includes feedback influences from high-level visual cortex.”</p><p>“Reduced early recurrent activity in the RSVP conditions could be due to two reasons. […] This may be reflected in the progressively later onset latencies with decreasing viewing times in Figure 2H, I, J which suggest that local recurrent interactions have a rapid influence in the ventral stream dynamics.”</p><disp-quote content-type="editor-comment"><p>4) &quot;How did the disruption of the first sweep of visual activity, reported in the previous section, affect the emergence of categorical information in the RSVP conditions?&quot; and &quot;According to these theories, presenting stimuli at rapid presentation rates would compromise feedforward signals thus increase the need for recurrent processes to fill-in missing stimulus information.&quot;</p><p>Consider rephrasing this paragraph to more exactly reflect what is going on. Two things occur in the current paradigm / analyses:</p><p>1) From Figure 2: Short SOAs have less time to continue initial evidence accumulation (a process that in all likelihood already incorporates some local feedback, see previous comment).</p><p>2) From Figure 3 (and also according to the referenced theories) feedback signals are interrupted/hampered by the incoming feedforward signals of the mask at short SOAs (i.e. the feedforward signal of the ensuing mask interrupts the feedback signal of the target stimulus, see also Fahrenfort et al., 2007 and references below). This apparently slows down the speed and extent with which category information can be resolved using recurrence.</p><p>These two things could be better described/disentangled in this paragraph.</p><p>The current phrasing suggest that feedforward activation (rather than recurrence) is interrupted. Moreover, the current phrasing makes it sound as information is actually missing, as in Muckli et al., 2015.</p><p>This is not the case, there is not enough time to accumulate information in time, hence the contribution of recurrent processing to resolve image category.</p><p>Other relevant references in this context: Kovacs, Vogels and Orban, 1995; O'Reilly et al., 2013; Fahrenfort et al., 2012.</p></disp-quote><p>We agree with the reviewer and rephrased the paragraph as requested. Also, we now cite the above references in several places in the manuscript, including below.</p><p>“However, opposing theories concur that feedback activity is critical for visual awareness and consciousness (Lamme and Roelfsema,</p><p>2000; Ahissar et al., 2009; Fahrenfort et al., 2017, 2012). […] These would be consistent with slowing down the speed and extent with which category information can be resolved using recurrence</p><p>(Brincat and Connor, 2006; Tang and Kreiman, 2017).”</p><disp-quote content-type="editor-comment"><p>5) Comparison of different conditions:</p><p>One set of participants engaged in the RSVP task (17 ms and 34 ms). A different set of participants engaged in the 1500 ms condition (recycled data from a previous experiment). This makes me wonder how the data are analyzed: in a within-subject, or between-subject fashion? What was the task for the 1500 ms condition? This should be added to the Materials and methods section.</p></disp-quote><p>Indeed, we have one cohort subjects for the RSVP conditions and another for the 500ms per picture condition (called 1500ms in the previous version of the manuscript). To be consistent, we performed between-subject comparisons in all cases. This is now clarified in the method section of the revised manuscript.</p><p>“We had one cohort of subjects for the RSVP conditions, and another for the 500 ms per picture condition. For consistency, we performed between-subject comparisons in all comparisons across the 17 ms, 34 ms, and 0.5 ms per picture conditions.”</p><p>For the 500 ms per picture condition, participants performed simple detection tasks to keep them engaged, which were adapted to the specific requirements of each acquisition technique. Specifically, for the MEG experiment, participants were instructed to press a button and blink their eyes in response to a paper clip that was shown randomly every 3 to 5 trials. For the fMRI experiment, thirty null trials with no stimulus presentation were randomly interspersed, during which the fixation cross turned darker for 100 ms and participants reported the change with a button press. The description of the task in this condition is now clarified in the Materials and methods section of the revised manuscript:</p><p>“Participants were instructed to press a button and blink their eyes in response to a paper clip that was shown randomly every 3 to 5 trials”</p><p>“Thirty null trials with no stimulus presentation were randomly interspersed, during which the fixation cross turned darker for 100 ms and participants reported the change with a button press.”</p><disp-quote content-type="editor-comment"><p>6) Response mapping:</p><p>Was there a fixed button mapping, i.e. did subjects always press a particular button for a particular choice? If so, can the authors rule out that the 'categorical information' classified in Figure 3 pertains to the motor plan, rather than a perceptual categorization? Presumably the motor plan is also delayed for the more difficult conditions.</p></disp-quote><p>We used a fixed button mapping, and we revised the manuscript to clarify this point. We acknowledge the reviewer’s concern regarding the potential confounding nature of the motor plan signals. However we believe it is unlikely our results are motor-related for the following reasons: First, our experimental design uses a delayed response design for the RSVP conditions (response was prompted 0.7-1 s after the offset of the stimulus), and no response for the 500 ms per picture condition. Thus any motor artifacts, including motor preparation signals, should have been delayed considerably for the RSVP conditions, and are completely absent in the 500 ms per picture condition. Second, we believe that our analysis to localize these processes with fMRI-MEG fusion which shows again progressively later peak latencies in fMRIMEG fusion time series in IT supports the source of this categorical information and its pattern in IT. Third, previous studies, such as Thorpe et al., 1996, have shown that motor-preparation signals occurs considerably later (300ms-400ms after stimulus onset). Thus it is unlikely the effect reported in our data (which is before 200ms after stimulus onset) to be related to motor-preparation.</p><p>“Our study used a fixed button mapping for the yes/no responses on faces in the RSVP task, which in principle may introduce motor plan-related signals in the MEG responses. […] Thus it is unlikely the effects before 200ms from stimulus onset reported in our data are related to motor preparation.”</p><disp-quote content-type="editor-comment"><p>7) Peak latency analysis:</p><p>I didn't find any detail on how peak latency was established. Were they determined within every subject, or on the grand average (using jackknifing or boot-strapping to obtain the confidence intervals)? What was the exact procedure used to identify the peak? Given that most results hinge on the latency analyses, it is imperative that they are described in detail.</p></disp-quote><p>Peak and onset latency 95% confidence intervals, and peak-to-peak and onset-toonset latency differences were evaluated by bootstrapping across participants and computing subject-averaged time series. Thus, the peaks and onsets were defined on (original or bootstrapped) subject-averaged time series.</p><p>The exact procedure was originally described in the ‘Statistical inference’ subsection of the Materials and methods section. We now moved the text to a separate section ‘Peak latency analysis’, which we revised for further clarification.</p><p>“Peak latency analysis</p><p>For statistical assessment of peak and onset latency of the time series, we performed bootstrap tests. […] We had one cohort subjects for the RSVP conditions, and another for the 0.5 ms per picture condition. To be consistent, we performed between-subject comparisons in all comparisons across the 17 ms,</p><p>34 ms, and 0.5 ms per picture conditions.”</p></body></sub-article></article>