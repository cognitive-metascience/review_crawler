<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="review-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">49547</article-id><article-id pub-id-type="doi">10.7554/eLife.49547</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Review Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Ten simple rules for the computational modeling of behavioral data</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-141553"><name><surname>Wilson</surname><given-names>Robert C</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2963-2971</contrib-id><email>bob@email.arizona.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-104362"><name><surname>Collins</surname><given-names>Anne GE</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3751-3662</contrib-id><email>annecollins@berkeley.edu</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Psychology</institution><institution>University of Arizona</institution><addr-line><named-content content-type="city">Tucson</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Cognitive Science Program</institution><institution>University of Arizona</institution><addr-line><named-content content-type="city">Tucson</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Psychology</institution><institution>University of California, Berkeley</institution><addr-line><named-content content-type="city">Berkeley</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Helen Wills Neuroscience Institute</institution><institution>University of California, Berkeley</institution><addr-line><named-content content-type="city">Berkeley</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Reviewing Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>26</day><month>11</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e49547</elocation-id><history><date date-type="received" iso-8601-date="2019-06-26"><day>26</day><month>06</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-10-09"><day>09</day><month>10</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Wilson and Collins</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Wilson and Collins</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-49547-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="commentary" xlink:href="10.7554/eLife.48175"/><abstract><p>Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computational modeling</kwd><kwd>model fitting</kwd><kwd>validation</kwd><kwd>reproducibility</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000049</institution-id><institution>National Institute on Aging</institution></institution-wrap></funding-source><award-id>R56 AG061888</award-id><principal-award-recipient><name><surname>Wilson</surname><given-names>Robert C</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1640885</award-id><principal-award-recipient><name><surname>Collins</surname><given-names>Anne GE</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01 MH118279</award-id><principal-award-recipient><name><surname>Collins</surname><given-names>Anne GE</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Brain imaging reveals frequency-dependent lateralized rhythmic finger tapping control by the auditory cortex with left-lateralized control of relatively fast and right-lateralized control of relatively slow rhythms.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><title>What is computational modeling of behavioral data?</title><p>The goal of computational modeling in behavioral science is to use precise mathematical models to make better sense of behavioral data. The behavioral data most often come in the form of choices, but can also be reaction times, eye movements, or other easily observable behaviors, and even neural data. The models come in the form of mathematical equations that link the experimentally observable variables (e.g. stimuli, outcomes, past experiences) to behavior in the immediate future. In this sense, computational models instantiate different ‘algorithmic hypotheses’ about how behavior is generated.</p><p>Exactly what it means to ‘make sense’ of behavioral data is, to some extent, a matter of taste that will vary according to the researcher’s goals (<xref ref-type="bibr" rid="bib48">Kording et al., 2018</xref>). In some cases, a simple model that can explain broad qualitative features of the data is enough. In other cases, more detailed models that make quantitative predictions are required (<xref ref-type="bibr" rid="bib8">Breiman, 2001</xref>). The exact form of the models, and exactly what we do with them, is limited only by our imaginations, but four uses dominate the literature: simulation, parameter estimation, model comparison, and latent variable inference.</p><list list-type="simple"><list-item><p><bold>Simulation</bold> involves running the model with particular parameter settings to generate ‘fake’ behavioral data. These simulated data can then be analyzed in much the same way as one would analyze real data, to make precise, falsifiable predictions about qualitative and quantitative patterns in the data. Simulation is a way to make theoretical predictions more precise and testable. (Some examples include <xref ref-type="bibr" rid="bib13">Cohen et al., 1990</xref>; <xref ref-type="bibr" rid="bib18">Collins and Frank, 2014</xref>; <xref ref-type="bibr" rid="bib72">Rescorla and Wagner, 1972</xref>; <xref ref-type="bibr" rid="bib31">Farashahi et al., 2017</xref>; <xref ref-type="bibr" rid="bib57">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="bib1">Abbott et al., 2015</xref>; <xref ref-type="bibr" rid="bib52">Lee and Webb, 2005</xref>).</p></list-item><list-item><p><bold>Parameter estimation</bold> involves finding the set of parameter values that best account for real behavioral data for a given model. These parameters can be used as a succinct summary of a given data set (<xref ref-type="bibr" rid="bib70">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib94">Wilson et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib27">Donkin et al., 2016</xref>), for investigating individual differences (<xref ref-type="bibr" rid="bib36">Frank et al., 2007</xref>; <xref ref-type="bibr" rid="bib79">Starns and Ratcliff, 2010</xref>; <xref ref-type="bibr" rid="bib16">Collins and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib40">Gillan et al., 2016</xref>; <xref ref-type="bibr" rid="bib78">Somerville et al., 2017</xref>; <xref ref-type="bibr" rid="bib64">Nilsson et al., 2011</xref>) and for quantifying the effects of interventions such as drugs, lesions, illness, or experimental conditions (<xref ref-type="bibr" rid="bib35">Frank et al., 2004</xref>; <xref ref-type="bibr" rid="bib55">Lorains et al., 2014</xref>; <xref ref-type="bibr" rid="bib29">Dowd et al., 2016</xref>; <xref ref-type="bibr" rid="bib98">Zajkowski et al., 2017</xref>; <xref ref-type="bibr" rid="bib91">Warren et al., 2017</xref>; <xref ref-type="bibr" rid="bib97">Wimmer et al., 2018</xref>; <xref ref-type="bibr" rid="bib86">van Ravenzwaaij et al., 2011</xref>).</p></list-item><list-item><p><bold>Model comparison</bold> involves trying to compute which of a set of possible models best describes the behavioral data, as a way to understand which mechanisms are more likely to underlie behavior. This is especially useful when the different models make similar qualitative predictions but differ quantitatively (<xref ref-type="bibr" rid="bib95">Wilson and Niv, 2011</xref>; <xref ref-type="bibr" rid="bib23">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib16">Collins and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib16">Collins and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib34">Fischer and Ullsperger, 2013</xref>; <xref ref-type="bibr" rid="bib80">Steyvers et al., 2009</xref>; <xref ref-type="bibr" rid="bib41">Haaf and Rouder, 2017</xref>; <xref ref-type="bibr" rid="bib26">Donkin et al., 2014</xref>).</p></list-item><list-item><p><bold>Latent variable inference</bold> involves using the model to compute the values of hidden variables (for example values of different choices) that are not immediately observable in the behavioral data, but which the theory assumes are important for the computations occurring in the brain. Latent variable inference is especially useful in neuroimaging where it is used to help search for the neural correlates of the model (<xref ref-type="bibr" rid="bib65">O'Doherty et al., 2007</xref>; <xref ref-type="bibr" rid="bib96">Wilson and Niv, 2015</xref>; <xref ref-type="bibr" rid="bib28">Donoso et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Cohen et al., 2017</xref>), but also for electroencephalogram (EEG), electrocorticography (ECOG), electrophysiology and pupillometry among many other data sources (<xref ref-type="bibr" rid="bib66">O'Reilly et al., 2013</xref>; <xref ref-type="bibr" rid="bib19">Collins and Frank, 2018</xref>; <xref ref-type="bibr" rid="bib75">Samejima et al., 2005</xref>; <xref ref-type="bibr" rid="bib12">Cavanagh et al., 2014</xref>; <xref ref-type="bibr" rid="bib60">Nassar et al., 2012</xref>).</p></list-item></list><p>Each of these uses has its strengths and weaknesses, and each of them can be mishandled in a number of ways, causing us to draw wrong and misleading conclusions (<xref ref-type="bibr" rid="bib62">Nassar and Frank, 2016</xref>; <xref ref-type="bibr" rid="bib68">Palminteri et al., 2017</xref>). Here we present a beginner-friendly, pragmatic, practical and details-oriented introduction (complete with example code available at [code]) on how to relate models to data and how to avoid many potential modeling mistakes. Our goal for this paper is to go beyond the mere mechanics of implementing models — as important as those mechanics are — and instead focus on the harder question of how to figure out what, exactly, a model is telling us about the mind. For this reason, we focus primarily on the simplest modeling techniques most accessible to beginning modelers, but almost all of our points apply more generally and readers interested in more advanced modeling techniques should consult the many excellent tutorials, didactic examples, and books on the topic (<xref ref-type="bibr" rid="bib10">Busemeyer and Diederich, 2010</xref>; <xref ref-type="bibr" rid="bib22">Daw, 2011</xref>; <xref ref-type="bibr" rid="bib25">Daw and Tobler, 2014</xref>; <xref ref-type="bibr" rid="bib42">Heathcote et al., 2015</xref>; <xref ref-type="bibr" rid="bib44">Huys, 2017</xref>; <xref ref-type="bibr" rid="bib83">Turner et al., 2013</xref>; <xref ref-type="bibr" rid="bib87">Vandekerckhove et al., 2015</xref>; <xref ref-type="bibr" rid="bib90">Wagenmakers and Farrell, 2004</xref>; <xref ref-type="bibr" rid="bib73">Rigoux et al., 2014</xref>; <xref ref-type="bibr" rid="bib64">Nilsson et al., 2011</xref>; <xref ref-type="bibr" rid="bib32">Farrell and Lewandowsky, 2018</xref>; <xref ref-type="bibr" rid="bib50">Lee et al., 2019</xref>).</p><p>For clarity of exposure, we chose to make all of the examples in this paper reflect a single narrow domain - reinforcement learning models applied to choice data (<xref ref-type="bibr" rid="bib82">Sutton and Barto, 2018</xref>). We chose this domain for a few reasons. (1) Modeling is particularly popular in the field of learning. Indeed, this field benefits from modeling particularly because of the nature of the behavioral data: trials are dependent on all past history and thus unique, making classic data analysis with aggregation across conditions less successful. (2) The sequential dependency of trials in learning contexts can lead to technical challenges when fitting models that are absent in non-learning contexts. However, the same techniques are widely and successfully applied to other observable behavior, such as reaction times (<xref ref-type="bibr" rid="bib71">Ratcliff and Rouder, 1998</xref>; <xref ref-type="bibr" rid="bib88">Viejo et al., 2015</xref>; <xref ref-type="bibr" rid="bib5">Ballard and McClure, 2019</xref>; <xref ref-type="bibr" rid="bib93">Wiecki et al., 2013</xref>), and to other domains, including but not limited to perception (<xref ref-type="bibr" rid="bib77">Sims, 2018</xref>), perceptual decision-making (<xref ref-type="bibr" rid="bib71">Ratcliff and Rouder, 1998</xref>; <xref ref-type="bibr" rid="bib30">Drugowitsch et al., 2016</xref>; <xref ref-type="bibr" rid="bib33">Findling et al., 2018</xref>), economic decision-making (<xref ref-type="bibr" rid="bib86">van Ravenzwaaij et al., 2011</xref>; <xref ref-type="bibr" rid="bib64">Nilsson et al., 2011</xref>), visual short-term memory (<xref ref-type="bibr" rid="bib27">Donkin et al., 2016</xref>; <xref ref-type="bibr" rid="bib26">Donkin et al., 2014</xref>; <xref ref-type="bibr" rid="bib61">Nassar et al., 2018</xref>), long-term memory (<xref ref-type="bibr" rid="bib6">Batchelder and Riefer, 1990</xref>), category learning (<xref ref-type="bibr" rid="bib52">Lee and Webb, 2005</xref>), executive functions (<xref ref-type="bibr" rid="bib41">Haaf and Rouder, 2017</xref>; <xref ref-type="bibr" rid="bib45">Jahfari et al., 2019</xref>), and so on. Thus, our hope is that, regardless of the techniques you use or the domain you model, by following these 10 simple steps (<xref ref-type="fig" rid="fig1">Figure 1</xref>), you will be able to minimize your modeling mishaps and unleash the power of computational modeling on your own behavioral data!</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic of the 10 rules and how they translate into a process for using computational modeling to better understand behavior.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-49547-fig1-v1.tif"/></fig></sec><sec id="s2"><title>Design a good experiment!</title><p>Computational modeling is a powerful technique, but it can never replace good experimental design. Modeling attempts to capture how information is manipulated behind the scenes to produce the behavior; thus it is fundamentally limited by the behavioral data, which is itself fundamentally limited by the experimental protocol. A researcher studying face perception would not attempt to fit Prospect Theory to a face perception task; and a researcher studying the differential effects of gain and loss would not do it in a gambling task with only gains. Although obvious in these simple cases, the question becomes more difficult as the complexity of the model increases: is a given learning protocol rich enough to allow the identification of dynamic changes in learning rate, of working memory or episodic memory contributions to learning, or of reward range adaptation? Often, the answer to these questions will be ‘no’ unless the protocol has been deliberately designed to provide this power.</p><p>So, how should you go about designing a good experiment with computational modeling in mind? While this process will always be something of an art form, we suggest that you ask yourself the following questions in order to optimize your experimental design:</p><sec id="s2-1"><title>What scientific question are you asking?</title><p>Although this sounds obvious, it is easy to get sucked into an experimental design without ever asking the most basic questions about your goals. What cognitive process are you targeting? What aspect of behavior are you trying to capture? What hypotheses are you trying to pick apart? For example, you may be trying to identify how working memory contributes to learning or how behavioral variability can be used to explore. Keeping your scientific goals in mind when you design the task can save much time later on.</p></sec><sec id="s2-2"><title>Does your experiment engage the targeted processes?</title><p>This may be a difficult question to answer, and it may require expert knowledge or piloting. However, you need to know that the experimental design actually engages the processes that you are trying to model.</p></sec><sec id="s2-3"><title>Will signatures of the targeted processes be evident from the simple statistics of the data?</title><p>In addition to engaging the processes of interest, the best experiments make these processes identifiable in classical analyses of the behavioral data (<xref ref-type="bibr" rid="bib68">Palminteri et al., 2017</xref>). For example, if you are investigating working memory contributions to learning, you may look for a signature of load on behavior by constructing an experimental design that varies load, to increase chances of probing working memory’s role in learning. Seeing signs of the computations of interest in simple analyses of behavior builds confidence that the modeling process will actually work. In our experience, computational modeling is rarely informative when there is no evidence of an effect in model-independent analyses of behavior.</p><p>To answer these questions, it is important to have a clear theoretical hypothesis of what phenomenon is to be modeled. In fact, although designing a good experiment is the first step, it goes hand-in-hand with designing a good model, and the two steps should ideally be done in parallel.</p></sec><sec id="s2-4"><title>But what if I’m not an experimentalist?</title><p>Computational modeling is hard and many of the best modelers are specialists who never run experiments of their own. Instead these researchers test their models against published findings, publicly available datasets, or even, if they are lucky, unpublished data from their experimental colleagues. Such specialist modelers might feel that they can safely ignore this first point about experimental design and instead focus on explaining the data they can get. We strongly urge them not to. Instead we urge these specialist modelers to always be considering better ways in which their models could be tested. Such experimental thinking helps you to be more concrete in your ideas and to think about how your model might apply outside of the context for which it was designed. In addition, thinking experimentally — and even better talking with experimentalists — forces you to engage with behavior as it actually is rather than as you would like it to be, which in turn can lead to new insights. Finally, by proposing concrete experimental designs, it is easier to convince your experimental colleagues to actually test your ideas, which is surely the goal if we are to move the field forward.</p></sec></sec><sec id="s3"><title>An illustrative example: the multi-armed bandit task</title><p>The ten rules in this paper are quite general, but we will illustrate many of our points using simple examples from our own field of reinforcement learning. Code for implementing all of these examples is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/AnneCollins/TenSimpleRulesModeling">https://github.com/AnneCollins/TenSimpleRulesModeling)</ext-link> (<xref ref-type="bibr" rid="bib20">Collins and Wilson, 2019</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/TenSimpleRulesModeling">https://github.com/elifesciences-publications/TenSimpleRulesModeling</ext-link>). The goal of these example studies is to understand how people learn to maximize their rewards in a case where the most rewarding choice is initially unknown.</p><p>More specifically, we consider the case in which a participant makes a series of <inline-formula><mml:math id="inf1"><mml:mi>T</mml:mi></mml:math></inline-formula> choices between <inline-formula><mml:math id="inf2"><mml:mi>K</mml:mi></mml:math></inline-formula> slot machines, or ‘one-armed bandits’, to try to maximize their earnings. If played on trial <inline-formula><mml:math id="inf3"><mml:mi>t</mml:mi></mml:math></inline-formula>, each slot machine, <inline-formula><mml:math id="inf4"><mml:mi>k</mml:mi></mml:math></inline-formula>, pays out a reward, <inline-formula><mml:math id="inf5"><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, which is one with reward probability, <inline-formula><mml:math id="inf6"><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula>, and otherwise 0. The reward probabilities are different for each slot machine and are initially unknown to the subject. In the simplest version of the task, the reward probabilities are fixed over time.</p><p>The three <bold>experimental parameters</bold> of this task are: the number of trials, <inline-formula><mml:math id="inf7"><mml:mi>T</mml:mi></mml:math></inline-formula>, the number of slot machines, <inline-formula><mml:math id="inf8"><mml:mi>K</mml:mi></mml:math></inline-formula>, and the reward probabilities of the different options, <inline-formula><mml:math id="inf9"><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula>, which may or may not change over time. The settings of these parameters will be important for determining exactly what information we can extract from the experiment. In this example, we will assume that <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, and that the reward probabilities are <inline-formula><mml:math id="inf12"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> for slot machine 1 and <inline-formula><mml:math id="inf13"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula> for slot machine 2.</p></sec><sec id="s4"><title>Design good models</title><p>Just as bad experiments can limit our ability to test different hypotheses, bad models – quite literally the mathematical embodiment of our hypotheses – can further limit the conclusions we can draw (<xref ref-type="bibr" rid="bib26">Donkin et al., 2014</xref>). This point is especially important if we are designing new models, but even well-established computational models can be problematic in some cases (<xref ref-type="bibr" rid="bib9">Broomell and Bhatia, 2014</xref>; <xref ref-type="bibr" rid="bib64">Nilsson et al., 2011</xref>).</p><p>Critical to the design of the model is a clear understanding of your reason for modeling. Are you interested in a descriptive model that succinctly summarizes, but perhaps does not explain, behavioral data? A mechanistic model to tie behavior to the brain? Or an elegant mathematical model to illustrate a concept? As shown in an excellent article by Kording and colleagues (<xref ref-type="bibr" rid="bib48">Kording et al., 2018</xref>), computational modelers have a wide variety of goals for their models, and understanding your own motivations is a great place to start.</p><p>More pragmatically, there are a number of different approaches for designing models that have been successfully used in the literature. Perhaps the simplest approach is to use heuristics to find a ‘reasonable’ way to handle information to produce the target behavior. This approach was how the delta rule (see Model three below) was first invented (<xref ref-type="bibr" rid="bib72">Rescorla and Wagner, 1972</xref>). Another approach is to scour the artificial intelligence, computer science, and applied mathematics literature for algorithms that have been used to solve similar problems for artificial agents. This approach has been fruitfully applied in the field of reinforcement learning (<xref ref-type="bibr" rid="bib82">Sutton and Barto, 2018</xref>), where algorithms such as Q-learning and temporal difference learning have been related to human and animal behavior and brain function (<xref ref-type="bibr" rid="bib92">Watkins and Dayan, 1992</xref>; <xref ref-type="bibr" rid="bib57">Montague et al., 1996</xref>). Another approach is to take a Bayes-optimal perspective, to design algorithms that perform optimally given a model of the environment and the task. Ideal observer models in vision are one example in which this approach has been applied successfully (<xref ref-type="bibr" rid="bib37">Geisler, 2011</xref>). More generally, Bayes-optimal models can be further pursued by investigating simpler algorithms that approximate the ideal strategy, or by imposing bounded rationality constraints, such as limited computational resources, on ideal observer agents (<xref ref-type="bibr" rid="bib21">Courville and Daw, 2008</xref>; <xref ref-type="bibr" rid="bib59">Nassar et al., 2010</xref>; <xref ref-type="bibr" rid="bib16">Collins and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib24">Daw and Courville, 2007</xref>; <xref ref-type="bibr" rid="bib54">Lieder et al., 2018</xref>).</p><p>Regardless of the approach (or, better yet, approaches) that you take to design your models, it is important to keep the following points in mind:</p><sec id="s4-1"><title>A computational model should be as simple as possible, but no simpler</title><p>Einstein’s old edict applies equally to models of the mind as it does to models of physical systems. Simpler, more parsimonious models are easier to fit and easier to interpret and should always be included in the set of models under consideration. Indeed, formal model comparison techniques (described in detail in Appendix 2) include a penalty for overly complex models, which are more likely to overfit the data and generalize poorly, and favor simpler models so long as they can account for the data.</p></sec><sec id="s4-2"><title>A computational model should be interpretable (as much as possible)</title><p>In the process of developing models that can account for the behavioral data, researchers run the risk of adding components to a model that are not interpretable as a sensible manipulation of information. For example, a negative learning rate is difficult to interpret in the framework of reinforcement learning. Although such uninterpretable models may sometimes improve fits, nonsensical parameter values may indicate that something important is missing from your model, or that a different cognitive process altogether is at play.</p></sec><sec id="s4-3"><title>The models should capture <italic>all</italic> the hypotheses that you plan to test</title><p>While it is obviously important to design models that can capture your main hypothesis, it is even more important to design models that capture competing hypotheses. Crucially, <italic>competing models should not be strawmen</italic> — they should have a genuine chance of relating to behavior in the task environment, and they should embody a number of reasonable, graded hypotheses. You should of course put equal effort into fitting these models as you do your favored hypothesis. Better yet, you shouldn’t have a favored hypothesis at all — let the data determine which model is the best fit, not your a priori commitment to one model or another.</p><boxed-text id="box1"><label>Box 1.</label><caption><p>Example: Modeling behavior in the multi-armed bandit task.</p></caption><p>We consider five different models of how participants could behave in the multi-armed bandit task.</p><p> <bold>Model 1: Random responding</bold></p><p>In the first model, we assume that participants do not engage with the task at all and simply press buttons at random, perhaps with a bias for one option over the other. Such random behavior is not uncommon in behavioral experiments, especially when participants have no external incentives for performing well. Modeling such behavior can be important if we wish to identify such ‘checked out’ individuals in a quantitative and reproducible manner, either for exclusion or to study the checked-out behavior itself. To model this behavior, we assume that participants choose between the two options randomly, perhaps with some overall bias for one option over the other. This bias is captured with a parameter <inline-formula><mml:math id="inf14"><mml:mi>b</mml:mi></mml:math></inline-formula> (which is between 0 and 1), such that the probability of choosing the two options is<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mtext>and</mml:mtext></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Thus, for two bandits, the random responding model has just one free parameter, controlling the overall bias for option 1 over option 2, <inline-formula><mml:math id="inf15"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p> <bold>Model 2: Noisy win-stay-lose-shift</bold></p><p>The win-stay-lose-shift model is one of the simplest models that adapts its behavior according to feedback. Consistent with the name, the model repeats rewarded actions and switches away from unrewarded actions. In the noisy version of the model, the win-stay-lose-shift rule is applied probabilistically, such that the model applies the win-stay-lose-shift rule with probability <inline-formula><mml:math id="inf16"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>, and chooses randomly with probability <inline-formula><mml:math id="inf17"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>. In the two-bandit case, the probability of choosing option <inline-formula><mml:math id="inf18"><mml:mi>k</mml:mi></mml:math></inline-formula> is<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mtext>if </mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi> <mml:mtext>and </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mtext>OR </mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:mi>k</mml:mi> <mml:mtext>and </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mtext>if </mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:mi>k</mml:mi> <mml:mtext>and </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mtext>OR </mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi> <mml:mtext>and </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is the choice at trial <inline-formula><mml:math id="inf20"><mml:mi>t</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf21"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> the reward at trial <inline-formula><mml:math id="inf22"><mml:mi>t</mml:mi></mml:math></inline-formula>. Although more complex to implement, this model still only has one free parameter, the overall level of randomness, <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p><bold>Model 3: Rescorla Wagner</bold></p><p>In this model, participants first <italic>learn</italic> the expected value of each slot machine based on the history of previous outcomes and then use these values to make a <italic>decision</italic> about what to do next. A simple model of learning is the Rescorla-Wagner learning rule (<xref ref-type="bibr" rid="bib72">Rescorla and Wagner, 1972</xref>), whereby the value of option <inline-formula><mml:math id="inf24"><mml:mi>k</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf25"><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> is updated in response to reward <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> according to:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf27"><mml:mi>α</mml:mi></mml:math></inline-formula> is the learning rate, which takes a value between 0 and 1 and captures the extent to which the prediction error, <inline-formula><mml:math id="inf28"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, updates the value. For simplicity, we assume that the initial value, <inline-formula><mml:math id="inf29"><mml:msubsup><mml:mi>Q</mml:mi><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula>, is zero, although it is possible to treat the <inline-formula><mml:math id="inf30"><mml:msubsup><mml:mi>Q</mml:mi><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> as a free parameter of the model.</p><p>A simple model of decision making is to assume that participants use the options’ values to guide their decisions, choosing the most valuable option most frequently, but occasionally making ‘mistakes’ (or exploring) by choosing a low-value option. One choice rule with these properties is known as the ‘softmax’ choice rule, which chooses option <inline-formula><mml:math id="inf31"><mml:mi>k</mml:mi></mml:math></inline-formula> with probability<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf32"><mml:mi>β</mml:mi></mml:math></inline-formula> is the ‘inverse temperature’ parameter that controls the level of stochasticity in the choice, ranging from <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for completely random responding and <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> for deterministically choosing the highest value option.</p><p>Combining the learning (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) and decision rules (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) gives a simple model of decision-making in this task with two free parameters: the learning rate, <inline-formula><mml:math id="inf35"><mml:mi>α</mml:mi></mml:math></inline-formula>, and the inverse temperature, <inline-formula><mml:math id="inf36"><mml:mi>β</mml:mi></mml:math></inline-formula>. That is, in our general notation, for this model <inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p><bold>Model 4: Choice kernel</bold></p><p>This model tries to capture the tendency for people to repeat their previous actions. In particular, we assume that participants compute a ‘choice kernel,’ <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, for each action, which keeps track of how frequently they have chosen that option in the recent past. This choice kernel updates in much the same way as the values in the Rescorla-Wagner rule, i.e. according to<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf39"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if option <inline-formula><mml:math id="inf40"><mml:mi>k</mml:mi></mml:math></inline-formula> is played on trial <inline-formula><mml:math id="inf41"><mml:mi>t</mml:mi></mml:math></inline-formula>, otherwise <inline-formula><mml:math id="inf42"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf43"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the choice-kernel learning rate. For simplicity, we assume that the initial value of the choice kernel is always zero, although, like the initial <inline-formula><mml:math id="inf44"><mml:mi>Q</mml:mi></mml:math></inline-formula>-value in the Rescorla-Wagner model, this could be a parameter of the model. Note that with <inline-formula><mml:math id="inf45"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, this model is very similar to model 2 (win-stay-lose-shift). From there, we assume that each option is chosen according to<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf46"><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the inverse temperature associated with the choice kernel.</p><p>Combining the choice kernel (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) with the decision rule (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) gives a simple model of decision-making in this task with two free parameters: the choice-kernel learning rate, <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, and the choice-kernel inverse temperature <inline-formula><mml:math id="inf48"><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>. That is, in our general notation, for this model <inline-formula><mml:math id="inf49"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p> <bold>Model 5: Rescorla Wagner + choice kernel</bold></p><p>Finally, our most complex model mixes the reinforcement learning model with the choice kernel model. In this model, the values update according to <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, while the choice kernel updates according to <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>. The terms are then combined to compute the choice probabilities as<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>This most complex model has four free parameters, i.e. <inline-formula><mml:math id="inf50"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></boxed-text></sec></sec><sec id="s5"><title>Simulate, simulate, simulate!</title><p>Once you have an experimental design and a set of computational models, a really important step is to create <italic>fake</italic>, or <italic>surrogate</italic> data (<xref ref-type="bibr" rid="bib68">Palminteri et al., 2017</xref>). That is, you should use the models to simulate the behavior of participants in the experiment, and to observe how behavior changes with different models, different model parameters, and different variants of the experiment. This step will allow you to refine the first two steps: confirming that the experimental design elicits the behaviors assumed to be captured by the computational model. To do this, here are some important steps.</p><sec id="s5-1"><title>Define model-independent measures that capture key aspects of the processes you are trying to model</title><p>Finding qualitative signatures (and there will often be more than one) of the model is crucial. By studying these measures with simulated data, you will have greater intuition about what is going on when you use the same model-independent measures to analyze real behavior (<xref ref-type="bibr" rid="bib23">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib16">Collins and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib17">Collins and Frank, 2013</xref>; <xref ref-type="bibr" rid="bib61">Nassar et al., 2018</xref>; <xref ref-type="bibr" rid="bib52">Lee and Webb, 2005</xref>).</p></sec><sec id="s5-2"><title>Simulate the model across the range of parameter values</title><p>Then, visualize behavior as a function of the parameters. Almost all models have free parameters. Understanding how changes to these parameters affect behavior will help you to better interpret your data and to understand individual differences in fit parameters. For example, in probabilistic reinforcement learning tasks modeled with a simple delta-rule model (Model 3; <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>), the learning rate parameter, <inline-formula><mml:math id="inf51"><mml:mi>α</mml:mi></mml:math></inline-formula>, can relate to both the speed of learning and noisiness in asymptotic behavior, as can the inverse temperature parameter, <inline-formula><mml:math id="inf52"><mml:mi>β</mml:mi></mml:math></inline-formula> (in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>), as seen in <xref ref-type="fig" rid="box2fig1">Box 2—figure 1B</xref>.</p></sec><sec id="s5-3"><title>Visualize the simulated behavior of different models</title><p>This will allow you to verify that behavior is qualitatively different for different models, making their predictions in the experimental setup different (<xref ref-type="fig" rid="box2fig1">Box 2—figure 1A</xref>). If the behavior of different models is <italic>not</italic> qualitatively different, this is a sign that you should try to design a better experiment. Although not always possible, distinguishing between models on the basis of qualitative patterns in the data is always preferable to quantitative model comparison (<xref ref-type="bibr" rid="bib63">Navarro, 2019</xref>; <xref ref-type="bibr" rid="bib68">Palminteri et al., 2017</xref>).</p><p>More generally, the goal of the simulation process is to clarify how the models and experimental design satisfy your goal of identifying a cognitive process in behavior. If the answer is positive — i.e. the experiment is rich enough to capture the expected behavior, the model’s parameters are interpretable, and competing models make dissociable predictions — you can move on to the next step. Otherwise, you should loop back through these first three sections to make sure that your experimental design and models work well together, and that the model parameters have identifiable effects on the behavior, which is a prerequisite for the fourth step, fitting the parameters (c.f. <xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><boxed-text id="box2"><label>Box 2.</label><caption><p>Example: simulating behavior in the bandit task.</p></caption><p>To simulate behavior, we first need to define the parameters of the task. These include the total number of trials, <inline-formula><mml:math id="inf53"><mml:mi>T</mml:mi></mml:math></inline-formula> (=1000 in the example), as well as the number of bandits, <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>K</mml:mi><mml:mspace width="veryverythickmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and the reward probability for each bandit, <inline-formula><mml:math id="inf55"><mml:msup><mml:mi>μ</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula> (0.2 and 0.8 for bandits 1 and 2, respectively). The experiment parameters, as used in the simulation, should match the actual parameters used in the experiment.</p><p>Next we define the parameters of the model. One way to do this is to sample these parameters randomly from prior distributions over each parameter, the exact form of which will vary from model to model. These prior distributions should generally be as broad as possible, but if something is known about the distribution of possible parameter values for a particular model, this is one place to include it.</p><p>With the free parameters set, we then proceed with the simulation. First, we simulate the choice on the first trial, <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, by assuming that the model chooses option <inline-formula><mml:math id="inf57"><mml:mi>k</mml:mi></mml:math></inline-formula> with probability, <inline-formula><mml:math id="inf58"><mml:msubsup><mml:mi>p</mml:mi><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula>. Next we simulate the outcome, <inline-formula><mml:math id="inf59"><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, of this choice. In Models 2–5, we use the action and/or outcome to update the choice probabilities for the next trial. Repeating this process for all trials up to <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> completes one simulation. The simulations can then be analyzed in the same way as participants’ data is, ideally with the same code taking different inputs. This process should be repeated several times, with different parameter settings, to get a handle on how the model behaves as a function of its parameters.</p><p>To illustrate how one might visualize the simulated results, we look at two model-independent measures that should capture fundamental aspects of learning: the probability of repeating an action, <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>stay</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (should I change my behavior in response to feedback?), and the probability of choosing the correct option, <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>correct</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (have I learned?). In <xref ref-type="fig" rid="box2fig1">Box 2—figure 1A</xref> below, we plot <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>stay</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as a function of the reward on the last trial for each of the models with a particular set of parameters (M1: <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, M2: <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, M3: <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, M4: <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, M5: <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf72"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). For some models (in particular the win-stay-lose-shift model (Model 2), we expect a strong dependence on past reward, but for others, such as the random responder (Model 1), we expect no dependence. Of course, the exact behavior of each model depends crucially on the parameters used in the simulations and care should be taken to ensure that these simulation parameters are reasonable, perhaps by matching to typical parameter values used in the literature or by constraining to human-like overall performance. Better yet is to simulate behavior across a range of parameter settings to determine how the model-independent measures change with different parameters.</p><p>A more thorough exploration of the parameter space for Model 3 is shown in <xref ref-type="fig" rid="box2fig1">Box 2—figure 1B</xref>, where we plot the <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>correct</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the first and last 10 trials as a function of the learning rate, <inline-formula><mml:math id="inf75"><mml:mi>α</mml:mi></mml:math></inline-formula>, and softmax parameter, <inline-formula><mml:math id="inf76"><mml:mi>β</mml:mi></mml:math></inline-formula>. Note that the ‘optimal’ learning rate, i.e. the value of <inline-formula><mml:math id="inf77"><mml:mi>α</mml:mi></mml:math></inline-formula> that maximizes <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"/><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>correct</mml:mtext></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, varies between early and late trials and as a function of the softmax parameter <inline-formula><mml:math id="inf79"><mml:mi>β</mml:mi></mml:math></inline-formula>, where for early trials higher <inline-formula><mml:math id="inf80"><mml:mi>β</mml:mi></mml:math></inline-formula> implies a lower optimal <inline-formula><mml:math id="inf81"><mml:mi>α</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib23">Daw et al., 2011</xref>).</p><p>The question of how to choose the model-independent measures of behavior has no easy answer and calls to the domain knowledge of the specific scientific question that the modeler is attempting to answer. As a rule of thumb, the measures should capture global characteristics (e.g. overall performance) and diagnostic measures that relate to the question of interest, and may visualize different qualitative predictions of different models.</p><fig id="box2fig1" position="anchor"><label>Box 2—figure 1.</label><caption><title>Simulating behavior in the two-armed bandit task.</title><p>(<bold>A</bold>) Win-stay-lose-shift behavior varies widely between models. (<bold>B</bold>) Model 3 simulations (100 per parameter setting) show how the learning rate and softmax parameters influence two aspects of behavior: early performance (first 10 trials), and late performance (last 10 trials). The left graph shows that learning rate is positively correlated with early performance improvement only for low <inline-formula><mml:math id="inf82"><mml:mi>β</mml:mi></mml:math></inline-formula> values or for very low <inline-formula><mml:math id="inf83"><mml:mi>α</mml:mi></mml:math></inline-formula> values. For high <inline-formula><mml:math id="inf84"><mml:mi>β</mml:mi></mml:math></inline-formula> values, there is a U-shape relationship between learning rate and early speed of learning. The right graph shows that with high <inline-formula><mml:math id="inf85"><mml:mi>β</mml:mi></mml:math></inline-formula> values, high learning rates negatively influence asymptotic behavior. Thus, both parameters interact to influence both the speed of learning and asymptotic performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-49547-box2-fig1-v1.tif"/></fig></boxed-text></sec></sec><sec id="s6"><title>Fit the parameters</title><p>A key component of computational modeling is estimating the values of the parameters that best describe your behavioral data. There are a number of different ways of estimating parameters, but here we focus on the maximum-likelihood approach, although almost all of our points apply to other methods such as Markov Chain Monte Carlo approaches (<xref ref-type="bibr" rid="bib51">Lee and Wagenmakers, 2014</xref>). Mathematical details, as well as additional discussion of other approaches to model fitting can be found in Appendix 1.</p><p>In the maximum likelihood approach to model fitting, our goal is to find the parameter values of model <inline-formula><mml:math id="inf86"><mml:mi>m</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf87"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, that maximize the likelihood of the data, <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, given the parameters, <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Maximizing the likelihood is equivalent to maximizing the log of the likelihood, <inline-formula><mml:math id="inf90"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which is numerically more tractable. (The likelihood is a product of many numbers smaller than 1, which can be rounded to 0 with limited precision computing. By contrast, the log-likelihood is a sum of negative numbers, which is usually tractable and will not be rounded to 0.) A simple mathematical derivation shows that this log-likelihood can be written in terms of the choice probabilities of the individual model as<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability of each individual choice given the parameters of the model and the information available up to that choice, which is at the heart of the definition of each model (for example in <xref ref-type="disp-formula" rid="equ1 equ2 equ3 equ4 equ5 equ6 equ7">Equations 1-7)</xref>.</p><p>In principle, finding the maximum likelihood parameters is as ‘simple’ as maximizing <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula>. In practice, of course, finding the maximum of a function is not a trivial process. The simplest approach, a brute force search of the entire parameter space, is occasionally useful, and may help you to understand how different parameters interact (see <xref ref-type="fig" rid="box3fig1">Box 3—figure 1</xref>). However, this approach is unfeasible outside of the simplest cases (e.g. one or two parameters with tight bounds) because of the high computational costs of evaluating the likelihood function at a large number of points.</p><p>Fortunately, a number of tools exist for finding local maxima (and minima) of functions quickly using variations on gradient ascent (or descent). For example, Matlab’s <monospace>fmincon</monospace> function can use a variety of sophisticated optimization algorithms (e.g. <xref ref-type="bibr" rid="bib58">Moré and Sorensen, 1983</xref>; <xref ref-type="bibr" rid="bib11">Byrd et al., 2000</xref>) to find the minimum of a function (and other factors such as the Hessian that can be useful in some situations [<xref ref-type="bibr" rid="bib22">Daw, 2011</xref>]). So long as one remembers to feed fmincon the <italic>negative</italic> log-likelihood (whose minimum is at the same parameter values as the maximum of the positive log-likelihood), using tools such as fmincon can greatly speed up model fitting. Even here, though, a number of problems can arise when trying to maximize <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> that can be reduced by using the tips and tricks described below. Most of the tips come from understanding that optimization algorithms are not foolproof and in particular are subject to numerical constraints. They generalize to other black box optimization functions in other languages, for example the Python scipy.optimize package or the optim function in R.</p><sec id="s6-1"><title>Be sure that your initial conditions give finite log-likelihoods</title><p>Optimizers such as fmincon require you to specify initial parameter values from which to start the search. Perhaps the simplest way in which the search process can fail is if these initial parameters give log-likelihoods that are not finite numbers (e.g. infinities or NaNs, not a number in Matlab speak). If your fitting procedure fails, this can often be the cause.</p></sec><sec id="s6-2"><title>Beware rounding errors, zeros and infinities</title><p>More generally, the fitting procedure can go wrong if it encounters infinities or NaNs during the parameter search. This can occur if a choice probability is rounded down to zero, thus making the log of the choice probability <inline-formula><mml:math id="inf94"><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>. Likewise, if your model involves exponentials (e.g. the softmax choice rule in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>), this can lead to errors whereby the exponential of a very large number is ‘rounded up’ to infinity. One way to avoid these issues is by constraining parameter values to always give finite choice probabilities and log-likelihoods at the boundaries. One way to diagnose these issues is to include checks in the code for valid log-likelihoods.</p></sec><sec id="s6-3"><title>Be careful with constraints on parameters</title><p>If the constraints are ill chosen, it is possible that the solution will be at the bounds, which is often, but not always, a red flag.</p><p>Only include parameters that have an influence on the likelihood. If only two parameters impact the likelihood, but the optimizer attempts to fit three, it will usually find the optimum for the two relevant parameters and a random value for the third; however, it will lead to slower and less efficient fitting.</p></sec><sec id="s6-4"><title>Beware local minima!</title><p>Finally, a key limitation of optimization algorithms is that they are only guaranteed to find <italic>local</italic> minima, which are not guaranteed to be the <italic>global</italic> minima corresponding to the best fitting parameters. One way to mitigate this issue is to run the fitting procedure multiple times with random initial conditions, recording the best fitting log-likelihood for each run. The best fitting parameters are then the parameters corresponding to the run with the highest log-likelihood. There is no hard-and-fast rule for knowing how many starting points to use in a given situation, besides the fact that more complex models will require more starting points. Thus, this number must be determined empirically in each case. One way to validate the number of starting points is by plotting the best likelihood score as a function of the number of starting points. As the number of initial conditions increases, the best-fitting likelihood (and corresponding the parameters) will improve up to an asymptote close to the true maximum of the function (e.g. <xref ref-type="fig" rid="box3fig1">Box 3—figure 1</xref>).</p><boxed-text id="box3"><label>Box 3.</label><caption><p>Example: contending with multiple local maxima.</p></caption><p>As a real example with local maxima, we consider the case of a simplified version of the mixed reinforcement learning and working memory model from <xref ref-type="bibr" rid="bib16">Collins and Frank, 2012</xref>. For simplicity, we relegate the details of this model to Appendix 4. To appreciate the example, all one really needs to know is that in its simplest version, this model has two parameters: <inline-formula><mml:math id="inf95"><mml:mi>ρ</mml:mi></mml:math></inline-formula>, which captures the effect of working memory, and <inline-formula><mml:math id="inf96"><mml:mi>α</mml:mi></mml:math></inline-formula>, which captures the learning rate of reinforcement learning. As is seen in <xref ref-type="fig" rid="box3fig1">Box 3—figure 1</xref> below, this model (combined with an appropriate experiment) gives rise to a log-likelihood surface with multiple local maxima. Depending on the starting point, the optimization procedure can converge to any one of these local maxima, meaning that the ‘maximum’ likelihood fits may not reflect the global maximum likelihood.</p><p>To mitigate this concern, a simple and effective approach is to repeat the optimization procedure many times, keeping track of the best fitting log-likelihood and parameters in each case. An approximation to the global maximum is to take the best log-likelihood from this list of fits. The results of this multiple iteration procedure can be summarized by plotting the best log-likelihood as a function of the number of starting points, or similarly, by plotting the distance from the so-far best parameters to the final best parameters as a function of the number of starting points (<xref ref-type="fig" rid="box3fig1">Box 3—figure 1B</xref>). As the number of starting points increases, the best-fitting log-likelihood and parameters will converge to the global maximum. This plot also allows us to judge when we have used enough starting points. Specifically, if the best fitting parameters appear to have reached asymptote, that gives us a good indication that the fit is the best we can do.</p><fig id="box3fig1" position="anchor"><label>Box 3—figure 1.</label><caption><title>An example with multiple local minima.</title><p>(<bold>Left</bold>) Log-likelihood surface for a working memory reinforcement learning model with two parameters. In this case, there are several local minima, all of which can be found by the optimization procedure depending on the starting point. Red x, generative parameters; black circle, optimum with brute search method; black *, optimum with fmincon and multiple starting points. (<bold>Right</bold>) Plotting the distance from the best fitting parameters after <inline-formula><mml:math id="inf97"><mml:mi>n</mml:mi></mml:math></inline-formula> iterations to the best fitting parameters after all iterations as a function of the number of starting points <inline-formula><mml:math id="inf98"><mml:mi>n</mml:mi></mml:math></inline-formula> gives a good sense of when the procedure has found the global optimum. The inset shows the same plot on a logarithmic scale for distance, illustrating that there are still very small improvements to be made after the third iteration.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-49547-box3-fig1-v1.tif"/></fig></boxed-text></sec></sec><sec id="s7"><title>Check that you can recover your parameters</title><p>Before reading too much into the best-fitting parameter values, <inline-formula><mml:math id="inf99"><mml:msubsup><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, it is important to check whether the fitting procedure gives meaningful parameter values in the best case scenario, -that is, when fitting fake data where the ‘true’ parameter values are known (<xref ref-type="bibr" rid="bib64">Nilsson et al., 2011</xref>). Such a procedure is known as ‘Parameter Recovery’, and is a crucial part of any model-based analysis.</p><p>In principle, the recipe for parameter recovery is quite simple. First, simulate fake data with known parameter values. Next, fit the model to these fake data to try to ‘recover’ the parameters. Finally, compare the recovered parameters to their true values. In a perfect world, the simulated and recovered parameters will be tightly correlated, with no bias. If there is only a weak correlation between the simulated and recovered parameters and/or a significant bias, then this is an indication that there is either a bug in your code (which from our own experience is fairly likely) or the experiment is underpowered to assess this model.</p><p>To make the most of your parameter recovery analysis, we suggest the following tips:</p><sec id="s7-1"><title>Make sure your simulation parameters are in the right range</title><p>An important choice for parameter recovery is the range of simulation parameters that you wish to recover. Some models/experiments only give good parameter recovery for parameters in a particular range — if the simulation parameters are too big or too small, they can be hard to recover. An illustration of this is the softmax parameter, <inline-formula><mml:math id="inf100"><mml:mi>β</mml:mi></mml:math></inline-formula>, where very large <inline-formula><mml:math id="inf101"><mml:mi>β</mml:mi></mml:math></inline-formula> values lead to almost identical behavior in most experiments. Thus parameter recovery may fail for large <inline-formula><mml:math id="inf102"><mml:mi>β</mml:mi></mml:math></inline-formula> values but work well for small <inline-formula><mml:math id="inf103"><mml:mi>β</mml:mi></mml:math></inline-formula> values. Of course, selecting only the range of parameters that <italic>can</italic> be recovered by your model is not necessarily the right choice, especially if the parameter values you obtain when fitting real data are outside of this range! For this reason, we have the following recommendations for choosing simulation parameter values:</p><list list-type="order"><list-item><p>If you have already fit your data, we recommend matching the range of your simulation parameters to the range of values obtained by your fit.</p></list-item><list-item><p>If you have not fit your data but you are using a model that has already been published, match the range of parameters to the range seen in previous studies.</p></list-item><list-item><p>Finally, if the model is completely new and the ‘true’ parameter values are unknown, we recommend simulating over as wide a range as possible to get a sense of whether and where parameters can be recovered. You can rely on your exploration of how model parameters affect simulated behavior to predict a range beyond which parameters will not affect behavior much.</p></list-item></list><p>Note that it is not necessarily problematic if a model’s parameters are not recoverable in a full parameter space, as long as they are recoverable in the range that matters for real data.</p></sec><sec id="s7-2"><title>Plot the correlations between simulated and recovered parameters</title><p>While the correlation coefficient between simulated and recovered parameters is a useful number for summarizing parameter recovery, we also strongly recommend that you actually plot the simulated vs recovered parameters. This makes the correlation clear, and also reveals whether the correlation holds in some parameter regimes but not others. It also reveals any existing bias (for example, a tendency to recover higher or lower values in average).</p></sec><sec id="s7-3"><title>Make sure the recovery process does not introduce correlations between parameters</title><p>In addition to looking at the correlations between simulated and recovered parameters, we also recommend looking at the correlation between the recovered parameters themselves. If the simulation parameters are uncorrelated with one another, correlation between the recovered parameters is an indication that the parameters in the model are trading off against one another (<xref ref-type="bibr" rid="bib22">Daw, 2011</xref>). Such trade-offs can sometimes be avoided by reparameterizing the model (e.g. <xref ref-type="bibr" rid="bib67">Otto et al., 2013</xref>) or redesigning the experiment. Sometimes, however, such trade-offs are unavoidable. In these cases, it is crucial to report the trade-off in parameters so that a ‘correlation’ between fit parameter values is not over-interpreted in real data.</p><p>A note about parameter differences between different populations or conditions: a growing use of model fitting is to compare parameter values between populations (e.g. schizophrenia patients vs healthy controls [<xref ref-type="bibr" rid="bib15">Collins et al., 2014</xref>]) or conditions (e.g., transcranial magnetic stimulation to one area or another [<xref ref-type="bibr" rid="bib98">Zajkowski et al., 2017</xref>]). If your primary interest is a difference like this, then parameter recovery can be used to give an estimate of statistical power. In particular, for a proposed effect size (e.g., on the average difference in one parameter between groups or conditions) you can simulate and recover parameters for the groups or conditions and then perform statistical tests to detect group differences in this simulated data set. The power for this effect size is then the frequency with which the statistical tests detect no effect given that the effect is there.</p></sec><sec id="s7-4"><title>Remember that even successful parameter recovery represents a best-case scenario!</title><p>What does successful parameter recovery tell you? That data generated by a known model with given parameters can be fit to recover those parameters. This is the best case you could possibly hope for in the model-based analysis and it is unlikely to ever occur as the ‘true’ generative process for behavior — that is, the inner workings of the mind and brain — is likely much more complex than any model you could conceive. There’s no easy answer to this problem. We only advise that you remember to be humble when you present your results!</p><boxed-text id="box4"><label>Box 4.</label><caption><p>Example: parameter recovery in the reinforcement learning model.</p></caption><p>We performed parameter recovery with Model 3, the Rescorla Wagner model, on the two-armed bandit task. As before, we set the means of each bandit at <inline-formula><mml:math id="inf104"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula> and the number of trials at <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>. We then simulated the actions of the model according to <xref ref-type="disp-formula" rid="equ3 equ4">Equations 3 and 4</xref>, with learning rate, <inline-formula><mml:math id="inf107"><mml:mi>α</mml:mi></mml:math></inline-formula>, and softmax temperature, <inline-formula><mml:math id="inf108"><mml:mi>β</mml:mi></mml:math></inline-formula>, set according to<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mtext>and</mml:mtext></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Exp</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>After simulating the model, we fit the parameters using a maximum likelihood approach to get fit values of learning rate, <inline-formula><mml:math id="inf109"><mml:mi>α</mml:mi></mml:math></inline-formula>, and softmax parameter, <inline-formula><mml:math id="inf110"><mml:mi>β</mml:mi></mml:math></inline-formula>. We then repeated this process 1000 times using new values of <inline-formula><mml:math id="inf111"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf112"><mml:mi>β</mml:mi></mml:math></inline-formula> each time. The results are plotted in <xref ref-type="fig" rid="box4fig1">Box 4—figure 1</xref> below. As is clear from this plot, there is fairly good agreement between the simulated and fit parameter values. In addition, we can see that the fit for <inline-formula><mml:math id="inf113"><mml:mi>β</mml:mi></mml:math></inline-formula> is best with a range, <inline-formula><mml:math id="inf114"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>β</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, and that outside this range, the correspondence between simulation and fit is not as good. If we further select points where parameter recovery for <inline-formula><mml:math id="inf115"><mml:mi>α</mml:mi></mml:math></inline-formula> is bad (i.e., when <inline-formula><mml:math id="inf116"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula>, grey dots in <xref ref-type="fig" rid="box4fig1">Box 4—figure 1</xref>), we find that parameter recovery for <inline-formula><mml:math id="inf117"><mml:mi>α</mml:mi></mml:math></inline-formula> is worse when <inline-formula><mml:math id="inf118"><mml:mi>β</mml:mi></mml:math></inline-formula> is outside of the range. Depending on the values of <inline-formula><mml:math id="inf119"><mml:mi>β</mml:mi></mml:math></inline-formula> that we obtain by fitting human behavior, this worse correspondence at small and large <inline-formula><mml:math id="inf120"><mml:mi>β</mml:mi></mml:math></inline-formula> values may or may not be problematic. It may be a good idea to use the range of parameters obtained from fitting the real data to test the quality of recovery within the range that matters.</p><fig id="box4fig1" position="anchor"><label>Box 4—figure 1.</label><caption><title>Parameter recovery for the Rescorla Wagner model (model 3) in the bandit task with 1000 trials.</title><p>Grey dots in both panels correspond to points where parameter recovery for <inline-formula><mml:math id="inf121"><mml:mi>α</mml:mi></mml:math></inline-formula> is bad.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-49547-box4-fig1-v1.tif"/></fig></boxed-text></sec></sec><sec id="s8"><title>Can you arbitrate between different models?</title><p>In model comparison, our goal is to determine which model, out of a set of possible models, is most likely to have generated the data. There are a number of different ways to make this comparison (summarized in more detail in Appendix 2) that involve different approximations to the Bayesian evidence for each model (e.g., <xref ref-type="bibr" rid="bib22">Daw, 2011</xref>; <xref ref-type="bibr" rid="bib73">Rigoux et al., 2014</xref>). Here, we focus on the most common method which is related to the log-likelihood computed in 'Fit the parameters'.</p><p>A simplistic approach to model comparison would be to compare the log-likelihoods of each model at the best fitting parameter settings, <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. However, if the data, <inline-formula><mml:math id="inf123"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, used to evaluate the log-likelihood are the same as those used to fit the parameters, then this approach will lead to overfitting, as the model with the most free parameters will almost always fit this ‘training’ data best. As an extreme example, consider the case of a model with one ‘parameter’ per choice, which is the identity of the choice the person actually made. Such a ‘model’ would fit the data perfectly, but would of course tell us nothing about how the choices were actually determined and would make no predictions about what choices would be made in a different setting. Overfitting is a problem in that it decreases the generalizability of the model: it makes it less likely that the conclusions drawn would apply to a different sample.</p><p>One way to avoid overfitting is to perform cross-validation: by measuring fit on held-out data, we directly test generalizability. However, this is not always possible for practical reasons (number of samples) or more fundamental ones (dependence between data points). Thus, other methods mitigate the risk of overfitting by approximately accounting for the degrees of freedom in the model. There are several methods for doing this (including penalties for free parameters), which are discussed in more detail in the Appendices. There is a rich theoretical literature debating which method is best (<xref ref-type="bibr" rid="bib90">Wagenmakers and Farrell, 2004</xref>; <xref ref-type="bibr" rid="bib87">Vandekerckhove et al., 2015</xref>). Here, we do not position ourselves in this theoretical debate, and instead focus on one of the simplest methods, the Bayes Information Criterion, <inline-formula><mml:math id="inf124"><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula>, which has an explicit penalty for free parameters.<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf125"><mml:mover accent="true"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> is the log-likelihood value at the best fitting parameter settings, and <inline-formula><mml:math id="inf126"><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> is the number of parameters in model <inline-formula><mml:math id="inf127"><mml:mi>m</mml:mi></mml:math></inline-formula>. The model with the smallest <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> score is the model that best fits the data. Thus, the positive effect of <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> in the last term corresponds to a penalty for models with large numbers of parameters.</p><p>While <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> is simple enough to apply in order to find the model that, apparently, best fits your data, it is important to check that your model comparison process gives sensible results for simulated data. Just as parameter fitting should be validated by parameter recovery on simulated data, so model comparison should be validated by model recovery on simulated data.</p><p>More specifically, model recovery involves simulating data from all models (with a range of parameter values carefully selected as in the case of parameter recovery) and then fitting that data with all models to determine the extent to which fake data generated from model <inline-formula><mml:math id="inf130"><mml:mi>A</mml:mi></mml:math></inline-formula> is best fit by model <inline-formula><mml:math id="inf131"><mml:mi>A</mml:mi></mml:math></inline-formula> as opposed to model <inline-formula><mml:math id="inf132"><mml:mi>B</mml:mi></mml:math></inline-formula>. This process can be summarized in a confusion matrix (see <xref ref-type="fig" rid="box5fig1">Box 5—figure 1</xref> below for an example) that quantifies the probability that each model is the best fit to data generated from the other models, that is, <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>fit model</mml:mtext><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mtext>simulated model</mml:mtext><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In a perfect world, the confusion matrix will be the identity matrix, but in practice, this is not always the case (e.g., <xref ref-type="bibr" rid="bib95">Wilson and Niv, 2011</xref>).</p><p>When computing and interpreting a confusion matrix it is important to keep the following points in mind:</p><sec id="s8-1"><title>Compare different methods of model comparison</title><p>If the confusion matrix has large off-diagonal components, then you have a problem with model recovery. There are a number of factors that could cause this problem, ranging from a bug in the code to an underpowered experimental design. However, one cause that is worth investigating is whether you are using the wrong method for penalizing free parameters. In particular, different measures penalize parameters in different ways that are ‘correct’ under different assumptions. If your confusion matrix is not diagonal, it may be that the assumptions underlying your measures (e.g. BIC) do not hold for your models, in which case it might be worth trying another metric for model comparison (e.g., AIC [<xref ref-type="bibr" rid="bib90">Wagenmakers and Farrell, 2004</xref>]; see Appendix 2).</p></sec><sec id="s8-2"><title>Be careful with the choice of parameters when computing the confusion matrix</title><p>Just as parameter recovery may only be successful in certain parameter regimes, so too can model recovery depend critically on the parameters chosen to simulate the models. In some parameter regimes, two models may lead to very different behavior, but they may be indistinguishable in other parameter regimes (see <xref ref-type="fig" rid="box5fig1">Box 5—figure 1</xref> below). As with parameter recovery, we believe that the best approach is to match the range of the parameters to the range seen in your data, or to the range that you expect from prior work.</p></sec><sec id="s8-3"><title>A note on interpreting the confusion matrix</title><p>As described above, and in keeping with standard practice from statistics, the confusion matrix is defined as the probability that data simulated by one model is best fit by another, that is, <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>fit model</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>simulated model</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. However, when we fit a model to real data, we are usually more interested in making the reverse inference — that is, given that model <inline-formula><mml:math id="inf135"><mml:mi>B</mml:mi></mml:math></inline-formula> fits our data best, which model is most likely to have generated the data? This is equivalent to computing <inline-formula><mml:math id="inf136"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>simulated model</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>fit model</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Note that this measure, which we term the ‘inversion matrix’ to distinguish it from the confusion matrix, is not the same as the confusion matrix unless model recovery is perfect. Of course, the inversion matrix can be computed from the confusion matrix using Bayes rule (see Appendix 3) and it may be useful to report it in cases where the confusion matrix is not diagonal.</p></sec><sec id="s8-4"><title>The elephant in the room with model comparison</title><p>As wonderful as it is to find that your model ‘best’ fits the behavioral data, the elephant in the room (or perhaps more correctly <italic>not</italic> in the room) with all model comparison is that it only tells you which of the models you considered fits the data best. In and of itself, this is rather limited information as there are infinitely many other models that you did not consider. This makes it imperative to start with a good set of models that rigorously capture the competing hypotheses (that is, think hard in Step 2). In addition, it will be essential to validate (at least) your winning model (see Step 9) to show how simulating its behavior can generate the patterns seen in the data that you did not explicitly fit, and thus obtain an <italic>absolute</italic> measure of how well your model relates to your data.</p><boxed-text id="box5"><label>Box 5.</label><caption><p>Example: confusion matrices in the bandit task.</p></caption><p>To illustrate model recovery, we simulated the behavior of the five models on the two-armed bandit task. As before, the means were set at <inline-formula><mml:math id="inf137"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf138"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>, and the number of trials was set at <inline-formula><mml:math id="inf139"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>. For each simulation, model parameters were sampled randomly for each model. Each simulated data set was then fit to each of the given models to determine which model fit best (according to BIC). This process was repeated 100 times to compute the confusion matrices which are plotted below in <xref ref-type="fig" rid="box5fig1">Box 5—figure 1A and B</xref>.</p><p>The difference between these two confusion matrices is in the priors from which the simulation parameters were sampled. In panel A, parameters were sampled from the following priors:</p><p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><tbody><tr valign="top"><td><bold>Model</bold></td><td><bold>Priors</bold></td></tr><tr valign="top"><td>Model 1</td><td><inline-formula><mml:math id="inf140"><mml:mrow><mml:mi>b</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr valign="top"><td>Model 2</td><td><inline-formula><mml:math id="inf141"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr valign="top"><td>Model 3</td><td><inline-formula><mml:math id="inf142"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>β</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Exp</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr valign="top"><td>Model 4</td><td><inline-formula><mml:math id="inf144"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Exp</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr valign="top"><td>Model 5</td><td><inline-formula><mml:math id="inf146"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf147"><mml:mrow><mml:mi>β</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Exp</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf148"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Exp</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap></p><p>In panel B, all of the softmax parameters <inline-formula><mml:math id="inf150"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf151"><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> were increased by 1. This has the effect of reducing the amount of noise in the behavior, which makes the models more easily identifiable and the corresponding confusion matrix more diagonal. The fact that the confusion matrix can be so dependent on the simulating parameter values means that it is crucial to match the simulation parameters to the actual fit parameters as best as possible. Models that are identifiable in one parameter regime may be impossible to distinguish in another!</p><p>In addition to the confusion matrices, we also plot the inversion matrices in <xref ref-type="fig" rid="box5fig1">Box 5—figure 1C and D</xref>. These are computed from the confusion matrices using Bayes rule assuming a uniform prior on models (see Appendix 3). These matrices more directly address the question of how to interpret a model comparison result where one model fits a particular subject best.</p><fig id="box5fig1" position="anchor"><label>Box 5—figure 1.</label><caption><title>Confusion matrices in the bandit task showing the effect of prior parameter distributions on model recovery.</title><p>Numbers denote the probability that data generated with model X are best fit by model Y, thus the confusion matrix represents <inline-formula><mml:math id="inf152"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>fit model</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>simulated model</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>A</bold>) When there are relatively large amounts of noise in the models (possibility of small values for <inline-formula><mml:math id="inf153"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>), models 3–5 are hard to distinguish from one another. (<bold>B</bold>) When there is less noise in the models (i.e. minimum value of <inline-formula><mml:math id="inf155"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf156"><mml:msub><mml:mi>β</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is 1), the models are much easier to identify. (<bold>C</bold>) The inversion matrix provides easier interpretation of fitting results when the true model is unknown. For example, the confusion matrix indicates that M1 is always perfectly recovered, while M5 is only recovered 30% of the time. By contrast, the inversion matrix shows that if M1 is the best fitting model, our confidence that it generated the data is low (54%), but if M5 is the best fitting model, our confidence that it did generate the data is high (97%). (<bold>D</bold>) Similar results with less noise in simulations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-49547-box5-fig1-v1.tif"/></fig></boxed-text></sec></sec><sec id="s9"><title>Run the experiment and analyze the actual data</title><p>Once all the previous steps have been completed, you can finally move on to modeling your empirical data. The first step to complete is of course to analyze the data <italic>without</italic> the model, in the same way that we recommended for model simulations in section 'Simulate, simulate, simulate!' This model-independent analysis is extremely important: you designed the experiment to test specific hypotheses, and constructed models to reflect them. Simulations showed expected patterns of behaviors given those hypotheses. If the model-independent analyses do not show evidence of the expected results, there is almost no point in fitting the model. Instead, you should go back to the beginning, either re-thinking the computational models if the analyses show interesting patterns of behavior, or re-thinking the experimental design or even the scientific question you are trying to answer. In our experience, if there is no model-independent evidence that the processes of interest are engaged, then a model-based analysis is unlikely to uncover evidence for the processes either.</p><p>If, however, the behavioral results are promising, the next step is to fit the models developed previously and to perform model comparison. After this step, you should check that the parameter range obtained with the fitting is within a range where parameter and model recovery were good. If the range is outside what you explored with simulations, you should go back over the parameter and model recovery steps to match the empirical parameter range, and thus ensure that the model fitting and model comparison procedures lead to interpretable results.</p><p>An important point to remember is that human behavior is always messier than the model, and it is unlikely that the class of models you explored actually contains the ‘real’ model thatgenerated human behavior. At this point, you should consider looping back to Steps 2–5 to improve the models, guided by in depth model-independent analysis of the data.</p><p>For example, you may consider modeling ‘unimportant parameters’, representing mechanisms that are of no interest to your scientific question but that might still affect your measures. Modeling these unimportant parameters usually captures variance in the behavior that would otherwise be attributed to noise, and as such, makes for a better estimation of ‘important’ parameters. For example, capturing pre-existing biases (e.g. a preference for left/right choices) in a decision or learning task provides better estimation of the inverse temperature, by avoiding attributing systematic biases to noise, which then affords better estimation of other parameters like the learning rate (this is evident in <xref ref-type="fig" rid="box6fig1">Box 6—figure 1</xref>).</p><boxed-text id="box6"><label>Box 6.</label><caption><p>Example: improving parameter recovery by modeling unimportant parameters.</p></caption><p>To illustrate the effect that ‘unimportant’ parameters (i.e., parameters that represent mechanisms that are of no interest to your scientific question, but may still affect your measures) can have on fitting results, we model the effect of a side bias on parameter recovery in Model 3. In particular, we assume that, in addition to choosing based on learned value, the model also had a side bias, <inline-formula><mml:math id="inf157"><mml:mi>B</mml:mi></mml:math></inline-formula>, that effectively changes the value of the left bandit. That is, in the two-bandit case, the choice probabilities are given by<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>We then simulated behavior with this model for a range of parameter values and fit the model with the original version of model 3, without the bias, and the modified version of model 3, with the bias. In this simulation, agents learn for 10 independent two-armed bandits in successive 50-trial blocks, with <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.8</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.8</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in different blocks. For simplicity, we assumed that the agent treats each block as independent, and started from the same initial values of <inline-formula><mml:math id="inf160"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>As can be seen below, including the ‘unimportant’ bias in the fit greatly improves the extent to which both the learning rate, <inline-formula><mml:math id="inf161"><mml:mi>α</mml:mi></mml:math></inline-formula>, and softmax parameter, <inline-formula><mml:math id="inf162"><mml:mi>β</mml:mi></mml:math></inline-formula>, can be recovered.</p><fig id="box6fig1" position="anchor"><label>Box 6—figure 1.</label><caption><title>Modeling unimportant parameters provides better estimation of important parameters.</title><p>The top row shows parameter recovery of the model without the bias term. The bottom row shows much more accurate parameter recovery, for all parameters, when the bias parameter is included in the model fits.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-49547-box6-fig1-v1.tif"/></fig></boxed-text></sec><sec id="s10"><title>Validate (at least) the winning model</title><p>All the previous steps measure a <italic>relative</italic> goodness of fit. Does model A fit better than model B? However, before interpreting any results from a model, it is essential to ensure that the model actually usefully captures the data in an <italic>absolute</italic> sense. This step is called model validation, and should never be skipped: it is possible to fit a model, get high fit measures, and nevertheless completely miss the essence of the behavior.</p><p>One method for model validation is computing the average trial likelihood as an absolute measure of fit. Although this measure has some nice properties — for example, the best possible value is one when the model predicts behavior perfectly — it offers limited value when choices are actually stochastic (which may be the case in many situations; <xref ref-type="bibr" rid="bib30">Drugowitsch et al., 2016</xref>) or the environment is complex. In these cases, the best possible likelihood per trial is less than 1, but it is not known what the best possible likelihood per trial could be. For this reason, although the likelihood per trial can be a useful tool for model validation (<xref ref-type="bibr" rid="bib53">Leong et al., 2017</xref>), interpreting it as an absolute measure of model fit is of limited value.</p><p>A better method to validate a model is to simulate it with the fit parameter values (<xref ref-type="bibr" rid="bib68">Palminteri et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Nassar and Frank, 2016</xref>; <xref ref-type="bibr" rid="bib63">Navarro, 2019</xref>), a procedure long performed by statisticians as part of the ‘posterior predictive check’ (<xref ref-type="bibr" rid="bib74">Roecker, 1991</xref>; <xref ref-type="bibr" rid="bib38">Gelman et al., 1996</xref>). You should then analyze the simulated data in the same way that you analyzed the empirical data, to verify that all important behavioral effects are qualitatively and quantitatively captured by the simulations with the fit parameters. For example, if you observe a qualitative difference between two conditions empirically, the model should reproduce it. Likewise, if a learning curve reaches a quantitative asymptote of 0.7, simulations shouldn’t reach a vastly different one.</p><p>Some researchers analyze the posterior prediction of the model conditioned on the past history, instead of simulated data. In our previous notation, they evaluate the likelihood of choice <inline-formula><mml:math id="inf163"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> given past data, <inline-formula><mml:math id="inf164"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, where the past data includes choices made by the subject, <italic>not</italic> choices made by the model, <inline-formula><mml:math id="inf165"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In some cases, this approach leads to very similar results to simulations, because simulations sample choices on the basis of a very similar probability, where the past data, <inline-formula><mml:math id="inf166"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, include choices made by the <italic>model</italic>. However, it can also be dramatically different if the path of actions sampled by the participant is widely different from the paths likely to be selected by the model (leading to very different past histories).</p><p>Palminteri and colleagues (<xref ref-type="bibr" rid="bib68">Palminteri et al., 2017</xref>) offer a striking example of this effect, where Model A fits better than Model B by any quantitative measure of model comparison, but is completely unable to capture the essence of the behavior. In their example, data are generated with a reinforcement learning agent (which takes the place of the subject) on a reversal learning task (where a choice that was previously good becomes bad, and reciprocally). These data are then fit with either a win-stay lose-shift model (model B), or a simplistic choice kernel model, which assumes that previous choices tend to be repeated (model A). Because of the autocorrelation in the choices made by the reinforcement learning agent, model A, which tends to repeat previous actions, fits better than model B, whose win-stay-lose-shift choices only depend on the action and outcome from the last trial. However, model A is completely insensitive to reward, and thus is unable to generate a reversal behavior when it is simulated with the fit model parameters. Thus, in this case, model A should be discarded, despite a greater quantitative fit. Nevertheless, the fact that the best validating model B captures less variance than model A should serve as a warning that model B is missing crucial components of the data and that a better model probably exists. This should incite the researcher to go back to the drawing board to develop a better model, for example one that combines elements of both models or a different model entirely, and perhaps a better experiment to test it.</p><p>More generally, if your validation step fails, you should go back to the drawing board! This may involve looking for a better model, as well as redesigning the task. Be careful interpreting results from a model that is not well validated! Of course, exactly what it means for a model to ‘fail’ the validation step is not well defined: no model is perfect, and there is no rule of thumb to tell us when a model is <italic>good enough</italic>. The most important aspect of validation is for you (and your readers) to be aware of its limitations, and in which ways they may influence any downstream results.</p><boxed-text id="box7"><label>Box 7.</label><caption><p>Example: model validation where the fit model performs too well.</p></caption><p>Most examples of model validation involve a case where a model that fits well performs poorly on the task in simulation. For example, in the <xref ref-type="bibr" rid="bib68">Palminteri et al. (2017)</xref> example, the choice kernel model cannot perform the task at all because its behavior is completely independent of reward. Here, we offer a different example of failed model validation in which the model performs <italic>better</italic> in simulation than the predicted and observed artificial agent’s behavior. Moreover, this model appears to fit data generated from a different model better than it fits data generated from itself! In this example, we imagine a deterministic stimulus-action learning task in which agents are presented with one of three stimuli (<inline-formula><mml:math id="inf167"><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf168"><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf169"><mml:msub><mml:mi>s</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>), which instruct them which of three actions (<inline-formula><mml:math id="inf170"><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf171"><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf172"><mml:msub><mml:mi>a</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>) will be rewarded when chosen. <inline-formula><mml:math id="inf173"><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is the correct choice for both stimuli <inline-formula><mml:math id="inf174"><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf175"><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf176"><mml:msub><mml:mi>a</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> for <inline-formula><mml:math id="inf177"><mml:msub><mml:mi>s</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is incorrect for all stimuli.</p><p>The two models that we consider are both reinforcement learning agents. The first, a ‘blind’ agent does not see the stimulus at all and learns only about the value of the three different actions, that is <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, regardless of the stimulus. The second, a ‘state-based’ agent, observes the stimulus and learns a value for each action that can be different for each stimulus, that is <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Parameters in the models are set such that the learning curves for the two agents are approximately equal (<xref ref-type="fig" rid="box7fig1">Box 7—figure 1A</xref>). See appendices for details of the models.</p><p>We then consider how both models fit behavior simulated by either of these models. In <xref ref-type="fig" rid="box7fig1">Box 7—figure 1B</xref>, we plot the average likelihood with which the state-based model predicts the actual choices of the blind and state-based agents, that is the average <inline-formula><mml:math id="inf181"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mtext>state-based</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. As is clear from this figure, the state-based model predicts choices from the blind agent with <italic>higher</italic> likelihood than choices from the state-based agent! While counter intuitive, this result does not imply that the state-based model is unable to fit its own behavior. Instead, this result reflects the difference in noise (softmax parameters) between the two agents. The blind RL agent has a low noise parameter, allowing the state-based model to fit it quite well. Conversely, the state-based RL agent has a high noise parameter, meaning that the behavior is harder to predict even when it is fit with the correct model.</p><p>That the state-based model captures state-based behavior better than it fits blind behavior is illustrated in <xref ref-type="fig" rid="box7fig1">Box 7—figure 1C</xref>. Here, we plot the simulated learning curves of the state-based model using the parameter values that were fit to either the state-based agent or the blind agent. Although the parameters of the state-based model obtained through the fit to the state-based agent generate a learning curve that is quite similar to that of the agent (compare blue lines in <xref ref-type="fig" rid="box7fig1">Box 7—figure 1A and C</xref>), the state-based fit to the blind agent performs too well (compare yellow lines in <xref ref-type="fig" rid="box7fig1">Box 7—figure 1A and C</xref>).</p><p>Thus the model validation step provides support for the state-based model when it is the correct model of behavior, but rules out the state-based model when the generating model was different. The take-away from this example should be that measures of model-fit and model comparison cannot replace a thorough validation step, which can contradict them.</p><fig id="box7fig1" position="anchor"><label>Box 7—figure 1.</label><caption><title>An example of successful and unsuccessful model validation.</title><p>(<bold>A</bold>) Behavior is simulated by one of two reinforcement learning models (a blind agent and a state-based agent) performing the same learning task. Generative parameters of the two models were set so that the learning curves of the models were approximately equal. (<bold>B</bold>) Likelihood per trial seems to indicate a worse fit for the state-based-simulated data than the blind-simulated data. (<bold>C</bold>) However, validation by model simulations with fit parameters shows that the state-based model captures the data from the state-based agent (compare dark learning curves in panels A and C), but not from the the blind agent (yellow learning curves in panels A and C).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-49547-box7-fig1-v1.tif"/></fig></boxed-text></sec><sec id="s11"><title>Analyze the winning model</title><p>To minimize risks of p-hacking, model-dependent analyses should only be performed on the winning model, after researchers are satisfied that the model captures the behavior. One particularly powerful application of model-based analysis of behavior involves estimating the <italic>latent</italic> variables in the model. Latent variables are the hidden components of the algorithms underlying the behavior that are not directly observable from the behavior itself. These latent variables shed light on the internal workings of the model and, if we are to take the model seriously, should have some representation in the subjects’ mind and brain (<xref ref-type="bibr" rid="bib14">Cohen et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">O'Doherty et al., 2007</xref>).</p><p>Extracting latent variables from the model is as simple as simulating the model and recording how the latent variables evolve over time. The parameters of the simulation should be the fit parameters for each subject. In most cases, it is useful to yoke the choices of the model to the choices the participants actually made, thus the latent variables evolve according to the experience participants actually had. This is especially true if the choices can influence what participants see in the future.</p><p>Once estimated, the latent variables can be used in much the same way as any other observable variable in the analysis of data. Perhaps the most powerful application comes when combined with physiological data such as pupil dilation, EEG, and fMRI. The simplest of these approaches uses linear regression to test whether physiological variables correlate with the latent variables of interest. Such an approach has led to a number of insights into the neural mechanisms underlying behavior (<xref ref-type="bibr" rid="bib60">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="bib23">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib28">Donoso et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Collins and Frank, 2018</xref>; <xref ref-type="bibr" rid="bib34">Fischer and Ullsperger, 2013</xref>), although, as with any modeling exercise, latent variable analysis should be done with care (<xref ref-type="bibr" rid="bib96">Wilson and Niv, 2015</xref>).</p><p>Other model-dependent analyzes include studying individual differences as captured by fit parameters. Fit parameters can be treated as a dependent variable in continuous analyses (e.g. correlating with age, symptom scales, and so on [<xref ref-type="bibr" rid="bib40">Gillan et al., 2016</xref>]) or group comparisons (e.g. patients vs. matched controls [<xref ref-type="bibr" rid="bib15">Collins et al., 2014</xref>]).</p></sec><sec id="s12"><title>Reporting model-based analyses</title><p>Congratulations! You have developed, simulated, and fit your model (and maybe several other competing models) to your data. You have estimated parameters, computed model comparison scores, and validated whether your model can generate realistic-looking behavior. It’s time to start writing! But what exactly should you report in your paper? And how should you report it?</p><sec id="s12-1"><title>Model selection</title><p>In many modeling papers, a key conclusion from the work is that one model fits the data better than other competing models. To make this point convincingly, we recommend including the following things in your paper, either as main results or in the supplementary material.</p><sec id="s12-1-1"><title>Model recovery analysis</title><sec id="s12-1-1-1"><title>Confusion matrix</title><p>Before anyone should believe your model comparison results, you need to demonstrate the ability of your analysis/experiment to distinguish between models under ideal conditions of simulated data. The best way to visualize these results is with a confusion matrix, as outlined in section 'Can you arbitrate between different models'? If the model comparison result is central to your paper, we recommend including the confusion matrix as a figure in the main text. If model comparison is less important, we recommend including it in the supplementary materials.</p></sec><sec id="s12-1-1-2"><title>Number of subjects best fit by each model</title><p>The simplest way to visualize how well the winning model fits the data is with a histogram showing the number of subjects best fit by each model. Obviously if all subjects are best fit with one model, the story is simple. The more likely scenario is that some subjects will be best fit by other models. Such a result is important to acknowledge in the paper as it may reflect the use of different strategies by different people or that the ‘correct’ model lies somewhere in between the models you have considered.</p></sec></sec><sec id="s12-1-2"><title>Group level statistics</title><sec id="s12-1-2-1"><title>Exceedance probabilities</title><p>A more sophisticated and less biased (<xref ref-type="bibr" rid="bib69">Piray et al., 2018</xref>) way to report model comparison results is by computing the probability that a single model best describes all the data. This is clearly an assumption whose merits should be discussed in your paper. In cases where it is valid, the method of <xref ref-type="bibr" rid="bib73">Rigoux et al. (2014)</xref> computes these ‘Exceedance Probabilities’, the probability that each model generated all the data. These probabilities can also be reported in histogram or table form.</p><p>Model-independent measures of simulated data. The cleanest way to demonstrate the superiority of one model is if that model can account for qualitative patterns in the data that are not captured by other models (see section 'Validate (at least) the winning model').</p></sec></sec></sec><sec id="s12-2"><title>Parameter fits</title><p>Many modeling papers involve fitting parameters to behavioral data. In some cases this is the main point of the paper, for example to show that parameter values differ between groups or treatments, in other cases parameter fitting is secondary to model comparison. In all cases, we recommend reporting the fit parameter values in as transparent a way as possible (i.e. more than just the means and standard errors).</p><sec id="s12-2-1"><title>Report distributions of parameter values</title><p>The simplest way to report parameter fits is to plot a distribution of all fit parameter values, for example in the form of a histogram (e.g. Figure S1 in <xref ref-type="bibr" rid="bib94">Wilson et al., 2013</xref> and <xref ref-type="bibr" rid="bib61">Nassar et al., 2018</xref>) or a cloud of points (e.g. Figure 5 in <xref ref-type="bibr" rid="bib43">Huys et al., 2011</xref>). This gives a great sense of the variability in each parameter across the population and can also illustrate problems with fitting. For example, if a large number of fit parameters are clustered around the upper and lower bounds, this may indicate a problem with the model.</p></sec><sec id="s12-2-2"><title>Plot pairwise correlations between fit parameter values</title><p>A deeper understanding of the relationships <italic>between</italic> fit parameters can be obtained by making scatter plots of the pairwise correlations between parameters. As with histograms of individual parameters, this approach gives a sense of the distribution of parameters, and can provide evidence of problems with the model; for example, if two parameters trade off against one another, it is a sign that these parameters may be unidentifiable in the experiment.</p></sec><sec id="s12-2-3"><title>Report parameter recovery</title><p>Finally, all parameter fit analyses should sit on the shoulders of a comprehensive parameter recovery analysis with simulated data. If parameters cannot be recovered in the ideal case of simulated data, there is little that they can tell us about real behavior.</p></sec></sec><sec id="s12-3"><title>Share your data and code!</title><p>The most direct way to communicate your results is to share the data and code. This approach encourages transparency and ensures that others can see <italic>exactly</italic> what you did. Sharing data and code also allows others to extend your analyses easily, by applying it to their own data or adding new models into the mix.</p><p>Ideally the data you share should be the raw data for the experiment, with minimal or no preprocessing (apart from the removal of identifying information). The code you share should reproduce all steps in your analysis, including any preprocessing/outlier exclusion you may have performed and generating all of the main and supplementary figures in the paper. In a perfect world, both data and code would be shared publicly on sites such as GitHub, DataVerse and so on. However, this is not always possible, for example, if the data come from collaborators who do not agree to data sharing, or if further analyses are planned using the same data set. In this case, we recommend having a clean set of ‘shareable’ code (and hopefully data too) that can be sent via email upon request.</p></sec><sec id="s12-4"><title>Should you always report all of your modeling results?</title><p>Finally, if you are using an established model, it can be tempting to skip many of the steps outlined above and report only the most exciting results. This temptation can be even greater if you are using code developed by someone else that, perhaps, you do not fully understand. In our opinion, taking shortcuts like this is dangerous. For one thing, your experiment or population may be different and the model may perform differently in this regime. For another, quite often ‘established’ models (in the sense that they have been published before), have not been validated in a systematic way. More generally, as with any research technique, when using computational modeling you need to demonstrate that you are applying the method correctly, and the that steps we outline here can help. In conclusion, even if developing the model is not the central point of your paper, you should report all of your modeling results.</p></sec></sec><sec id="s13"><title>What now?</title><sec id="s13-1"><title>Looping back</title><p>A modeler’s work is never done. To paraphrase George Box, there are no correct models, there are only useful models (<xref ref-type="bibr" rid="bib7">Box, 1979</xref>). To make your model more useful, there are a number of next steps to consider to test whether your model really does describe a process in the mind.</p><sec id="s13-1-1"><title>Improve the model to account for discrepancies with your existing data set</title><p>Model fits are never perfect and, even in the best cases, there are often small discrepancies with actual data. The simplest next step is to try to address these discrepancies by improving the model, either by including additional factors (such as side bias or lapse rates) or by devising new models entirely.</p></sec><sec id="s13-1-2"><title>Use your model to make predictions</title><p>The best models don’t just explain data in one experiment, they <italic>predict</italic> data in completely new situations. If your model does not easily generalize to new situations, try to understand why that is and how it could be adjusted to be more general. If your model does generalize, test its predictions against new data — either data you collect yourself from a new experiment or data from other studies that (hopefully) have been shared online.</p></sec></sec><sec id="s13-2"><title>Using advanced techniques</title><p>Another potential next step is to use more powerful modeling techniques. We focused here on the simplest techniques (maximum likelihood estimation and model comparison by BIC) because of their accessibility to beginners, and because most of the advice we give here generalizes to more advanced techniques. In particular, no matter how advanced the modeling technique used, validation is essential (<xref ref-type="bibr" rid="bib68">Palminteri et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Nassar and Frank, 2016</xref>; <xref ref-type="bibr" rid="bib44">Huys, 2017</xref>). Nevertheless, the simple methods described here have known limitations. More advanced techniques attempt to remedy them, but come with their own pitfalls. A complete review of these advanced techniques is beyond the scope of this paper; instead we provide pointers to a few of the most interesting techniques for the ambitious reader to pursue.</p><sec id="s13-2-1"><title>Compute maximum a posteriori (MAP) parameter values</title><p>Perhaps the simplest step for improving parameter estimates is to include prior information about parameter values. When combined with the likelihood, these priors allow us to compute the posterior, which we can use to find the maximum a posteriori (MAP) parameter values. Although they are still point estimates, with good priors, MAP parameters can be more accurate than parameters estimated with maximum likelihood approaches (<xref ref-type="bibr" rid="bib39">Gershman, 2016</xref>; <xref ref-type="bibr" rid="bib22">Daw, 2011</xref>), although when the priors are bad, this method has problems of its own (<xref ref-type="bibr" rid="bib47">Katahira, 2016</xref>).</p></sec><sec id="s13-2-2"><title>Approximate the full posterior by sampling</title><p>Point estimates of model parameters, such as those obtained with MLE or MAP, lose interesting information about uncertainty over the parameter distribution. Sampling approaches (such as Markov Chain Monte Carlo or MCMC) provide this richer information; furthermore, they allow modelers to investigate more complex assumptions. For example, hierarchical Bayesian approaches make it possible to fit all participants simultaneously, integrating assumptions about their dependence (e.g. one single group, multiple groups, effects of covariates of interest such as age and so on; <xref ref-type="bibr" rid="bib49">Lee, 2011</xref>; <xref ref-type="bibr" rid="bib51">Lee and Wagenmakers, 2014</xref>; <xref ref-type="bibr" rid="bib93">Wiecki et al., 2013</xref>).</p></sec><sec id="s13-2-3"><title>Advanced optimizers and approximate likelihood</title><p>Some models have intractable likelihoods, for example if the choice state has too many dimensions, as in continuous movements, or if the model included unobservable choices. There exist methods to approximate likelihoods to relate them quantitatively to data, such as the ABC method (<xref ref-type="bibr" rid="bib85">Turner and Sederberg, 2012</xref>; <xref ref-type="bibr" rid="bib81">Sunnåker et al., 2013</xref>). There are also advanced methods for finding best fit parameters in a sample-efficient manner when computing the likelihood is expensive (<xref ref-type="bibr" rid="bib3">Acerbi and Ji, 2017</xref>; <xref ref-type="bibr" rid="bib2">Acerbi, 2018</xref>).</p></sec><sec id="s13-2-4"><title>Model selection</title><p>Bayesian model selection provides less biased, statistically more accurate ways of identifying which model is best at the group level (<xref ref-type="bibr" rid="bib73">Rigoux et al., 2014</xref>). This may be particularly important when comparing model selection between groups, for example between patients and controls (<xref ref-type="bibr" rid="bib69">Piray et al., 2018</xref>).</p></sec><sec id="s13-2-5"><title>Incorporating other types of data</title><p>We focused on modeling a single type of observable data, choices. However, there is a rich literature on fitting models to other measurements, such as reaction times (<xref ref-type="bibr" rid="bib70">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib71">Ratcliff and Rouder, 1998</xref>), but also to eye movements and neural data (<xref ref-type="bibr" rid="bib84">Turner et al., 2016</xref>). Furthermore, fitting more than one measurement at a time provides additional constraints to the model, and as such may provide better fit (<xref ref-type="bibr" rid="bib5">Ballard and McClure, 2019</xref>). However, fitting additional data can increase the complexity of the model-fitting process and additional care must be taken to determine exactly how different types of data should be combined (<xref ref-type="bibr" rid="bib88">Viejo et al., 2015</xref>).</p></sec></sec></sec><sec id="s14"><title>Epilogue</title><p>Our goal for this paper was to offer practical advice, for beginners as well as seasoned researchers, on the computational modeling of behavioral data. To this end, we offered guidance on how to generate models, simulate models, fit models, compare models, validate models, and extract latent variables from models to compare with physiological data. We have talked about how to avoid common pitfalls and misinterpretations that can arise with computational modeling, and lingered, quite deliberately, on the importance of good experimental design. Many of these lessons were lessons we learned the hard way, by actually making these mistakes for ourselves over a combined 20+ years in the field. By following these steps, we hope that you will avoid some of the errors that slowed our own research, and that the overall quality of computational modeling in behavioral science will improve.</p></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We are grateful to all our lab members who provided feedback on this paper, in particular Beth Baribault, Waitsang Keung, Sarah Master, Sam McDougle, and William Ryan. We are grateful for useful reviewers’ and editors’ feedback, including that from Tim Behrens, Mehdi Khamassi, Ken Norman, Valentin Wyart, and other anonymous reviewers. We also gratefully acknowledge the contribution of many others in our previous labs and collaborations, with whom we learned many of the techniques, tips and tricks presented here. This work was supported by NIA Grant R56 AG061888 to RCW and NSF Grant 1640885 and NIH Grant R01 MH118279 to AGEC.</p></ack><sec id="s15" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname> <given-names>JT</given-names></name><name><surname>Austerweil</surname> <given-names>JL</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Random walks on semantic networks can resemble optimal foraging</article-title><source>Psychological Review</source><volume>122</volume><fpage>558</fpage><lpage>569</lpage><pub-id pub-id-type="doi">10.1037/a0038693</pub-id><pub-id pub-id-type="pmid">25642588</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Acerbi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Variational bayesian monte carlo</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>8213</fpage><lpage>8223</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/8043-variational-bayesian-monte-carlo">https://papers.nips.cc/paper/8043-variational-bayesian-monte-carlo</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Acerbi</surname> <given-names>L</given-names></name><name><surname>Ji</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical bayesian optimization for model fitting with bayesian adaptive direct search</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1836</fpage><lpage>1846</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/6780-practical-bayesian-optimization-for-model-fitting-with-bayesian-adaptive-direct-search">https://papers.nips.cc/paper/6780-practical-bayesian-optimization-for-model-fitting-with-bayesian-adaptive-direct-search</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akaike</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>A new look at the statistical model identification</article-title><source>IEEE Transactions on Automatic Control</source><volume>19</volume><fpage>716</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1109/TAC.1974.1100705</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ballard</surname> <given-names>IC</given-names></name><name><surname>McClure</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Joint modeling of reaction times and choice improves parameter identifiability in reinforcement learning models</article-title><source>Journal of Neuroscience Methods</source><volume>317</volume><fpage>37</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.01.006</pub-id><pub-id pub-id-type="pmid">30664916</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Batchelder</surname> <given-names>WH</given-names></name><name><surname>Riefer</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Multinomial processing models of source monitoring</article-title><source>Psychological Review</source><volume>97</volume><fpage>548</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.97.4.548</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Box</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="1979">1979</year><chapter-title>Robustness in the strategy of scientific model building</chapter-title><source>Robustness in Statistics</source><publisher-name>Elsevier</publisher-name><fpage>201</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1016/b978-0-12-438150-6.50018-2</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Statistical modeling: the two cultures (with comments and a rejoinder by the author)</article-title><source>Statistical Science</source><volume>16</volume><fpage>199</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1214/ss/1009213726</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broomell</surname> <given-names>SB</given-names></name><name><surname>Bhatia</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Parameter recovery for decision modeling using choice data</article-title><source>Decision</source><volume>1</volume><fpage>252</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1037/dec0000020</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Busemeyer</surname> <given-names>JR</given-names></name><name><surname>Diederich</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Cognitive Modeling</source><publisher-name>Sage</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Byrd</surname> <given-names>RH</given-names></name><name><surname>Gilbert</surname> <given-names>JC</given-names></name><name><surname>Nocedal</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A trust region method based on interior point techniques for nonlinear programming</article-title><source>Mathematical Programming</source><volume>89</volume><fpage>149</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1007/PL00011391</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>JF</given-names></name><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Kochar</surname> <given-names>A</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Eye tracking and pupillometry are indicators of dissociable latent decision processes</article-title><source>Journal of Experimental Psychology: General</source><volume>143</volume><fpage>1476</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1037/a0035813</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Dunbar</surname> <given-names>K</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>On the control of automatic processes: a parallel distributed processing account of the stroop effect</article-title><source>Psychological Review</source><volume>97</volume><fpage>332</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.97.3.332</pub-id><pub-id pub-id-type="pmid">2200075</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Daw</surname> <given-names>N</given-names></name><name><surname>Engelhardt</surname> <given-names>B</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Li</surname> <given-names>K</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Pillow</surname> <given-names>J</given-names></name><name><surname>Ramadge</surname> <given-names>PJ</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name><name><surname>Willke</surname> <given-names>TL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Computational approaches to fMRI analysis</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>304</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1038/nn.4499</pub-id><pub-id pub-id-type="pmid">28230848</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>AG</given-names></name><name><surname>Brown</surname> <given-names>JK</given-names></name><name><surname>Gold</surname> <given-names>JM</given-names></name><name><surname>Waltz</surname> <given-names>JA</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Working memory contributions to reinforcement learning impairments in schizophrenia</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>13747</fpage><lpage>13756</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0989-14.2014</pub-id><pub-id pub-id-type="pmid">25297101</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>AG</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis</article-title><source>European Journal of Neuroscience</source><volume>35</volume><fpage>1024</fpage><lpage>1035</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07980.x</pub-id><pub-id pub-id-type="pmid">22487033</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>AG</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cognitive control over learning: creating, clustering, and generalizing task-set structure</article-title><source>Psychological Review</source><volume>120</volume><fpage>190</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1037/a0030852</pub-id><pub-id pub-id-type="pmid">23356780</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>AG</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Opponent actor learning (OpAL): modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive</article-title><source>Psychological Review</source><volume>121</volume><fpage>337</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1037/a0037015</pub-id><pub-id pub-id-type="pmid">25090423</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>AG</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Within-and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory</article-title><source>PNAS</source><elocation-id>201720963</elocation-id></element-citation></ref><ref id="bib20"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>AG</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>TenSimpleRulesModeling</data-title><version designator="3a01850">3a01850</version><publisher-name>Github</publisher-name><ext-link ext-link-type="uri" xlink:href="https://github.com/AnneCollins/TenSimpleRulesModeling">https://github.com/AnneCollins/TenSimpleRulesModeling</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Courville</surname> <given-names>AC</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The rat as particle filter</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>369</fpage><lpage>376</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/3205-the-rat-as-particle-filter">https://papers.nips.cc/paper/3205-the-rat-as-particle-filter</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Trial-by-trial data analysis using computational models</article-title><source>Decision Making, Affect, and Learning: Attention and Performance XXIII</source><volume>23</volume><fpage>3</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1093/acprof:oso/9780199600434.003.0001</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Seymour</surname> <given-names>B</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Model-based influences on humans' choices and striatal prediction errors</article-title><source>Neuron</source><volume>69</volume><fpage>1204</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id><pub-id pub-id-type="pmid">21435563</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Courville</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The pigeon as particle filter</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Tobler</surname> <given-names>PN</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Value learning through reinforcement: the basics of dopamine and reinforcement learning</chapter-title><source>Neuroeconomics</source><edition>Second Edition</edition><publisher-name>Elsevier</publisher-name><fpage>283</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1016/b978-0-12-416008-8.00015-2</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donkin</surname> <given-names>C</given-names></name><name><surname>Tran</surname> <given-names>SC</given-names></name><name><surname>Nosofsky</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Landscaping analyses of the ROC predictions of discrete-slots and signal-detection models of visual working memory</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>76</volume><fpage>2103</fpage><lpage>2116</lpage><pub-id pub-id-type="doi">10.3758/s13414-013-0561-7</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donkin</surname> <given-names>C</given-names></name><name><surname>Kary</surname> <given-names>A</given-names></name><name><surname>Tahir</surname> <given-names>F</given-names></name><name><surname>Taylor</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Resources masquerading as slots: Flexible allocation of visual working memory</article-title><source>Cognitive Psychology</source><volume>85</volume><fpage>30</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2016.01.002</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoso</surname> <given-names>M</given-names></name><name><surname>Collins</surname> <given-names>AGE</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Foundations of human reasoning in the prefrontal cortex</article-title><source>Science</source><volume>344</volume><fpage>1481</fpage><lpage>1486</lpage><pub-id pub-id-type="doi">10.1126/science.1252254</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dowd</surname> <given-names>EC</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Collins</surname> <given-names>A</given-names></name><name><surname>Gold</surname> <given-names>JM</given-names></name><name><surname>Barch</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Probabilistic reinforcement learning in patients with schizophrenia: Relationships to anhedonia and avolition</article-title><source>Biological Psychiatry: Cognitive Neuroscience and Neuroimaging</source><volume>1</volume><fpage>460</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1016/j.bpsc.2016.05.005</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>Wyart</surname> <given-names>V</given-names></name><name><surname>Devauchelle</surname> <given-names>AD</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational precision of mental inference as critical source of human choice suboptimality</article-title><source>Neuron</source><volume>92</volume><fpage>1398</fpage><lpage>1411</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.11.005</pub-id><pub-id pub-id-type="pmid">27916454</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farashahi</surname> <given-names>S</given-names></name><name><surname>Rowe</surname> <given-names>K</given-names></name><name><surname>Aslami</surname> <given-names>Z</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Soltani</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Feature-based learning improves adaptability without compromising precision</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>1768</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01874-w</pub-id><pub-id pub-id-type="pmid">29170381</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Farrell</surname> <given-names>S</given-names></name><name><surname>Lewandowsky</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Computational Modeling of Cognition and Behavior</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9781316272503</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Findling</surname> <given-names>C</given-names></name><name><surname>Skvortsova</surname> <given-names>V</given-names></name><name><surname>Dromnelle</surname> <given-names>R</given-names></name><name><surname>Palminteri</surname> <given-names>S</given-names></name><name><surname>Wyart</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Computational noise in reward-guided learning drives behavioral variability in volatile environments</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/439885</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname> <given-names>AG</given-names></name><name><surname>Ullsperger</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Real and fictive outcomes are processed differently but converge on a common adaptive mechanism</article-title><source>Neuron</source><volume>79</volume><fpage>1243</fpage><lpage>1255</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.006</pub-id><pub-id pub-id-type="pmid">24050408</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Seeberger</surname> <given-names>LC</given-names></name><name><surname>O'reilly</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>By carrot or by stick: cognitive reinforcement learning in parkinsonism</article-title><source>Science</source><volume>306</volume><fpage>1940</fpage><lpage>1943</lpage><pub-id pub-id-type="doi">10.1126/science.1102941</pub-id><pub-id pub-id-type="pmid">15528409</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Moustafa</surname> <given-names>AA</given-names></name><name><surname>Haughey</surname> <given-names>HM</given-names></name><name><surname>Curran</surname> <given-names>T</given-names></name><name><surname>Hutchison</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Genetic triple dissociation reveals multiple roles for dopamine in reinforcement learning</article-title><source>PNAS</source><volume>104</volume><fpage>16311</fpage><lpage>16316</lpage><pub-id pub-id-type="doi">10.1073/pnas.0706111104</pub-id><pub-id pub-id-type="pmid">17913879</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contributions of ideal observer theory to vision research</article-title><source>Vision Research</source><volume>51</volume><fpage>771</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.09.027</pub-id><pub-id pub-id-type="pmid">20920517</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Meng</surname> <given-names>XL</given-names></name><name><surname>Stern</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Posterior predictive assessment of model fitness via realized discrepancies</article-title><source>Statistica Sinica</source><volume>6</volume><fpage>733</fpage><lpage>760</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Empirical priors for reinforcement learning models</article-title><source>Journal of Mathematical Psychology</source><volume>71</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.01.006</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillan</surname> <given-names>CM</given-names></name><name><surname>Kosinski</surname> <given-names>M</given-names></name><name><surname>Whelan</surname> <given-names>R</given-names></name><name><surname>Phelps</surname> <given-names>EA</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Characterizing a psychiatric symptom dimension related to deficits in goal-directed control</article-title><source>eLife</source><volume>5</volume><elocation-id>e11305</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11305</pub-id><pub-id pub-id-type="pmid">26928075</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haaf</surname> <given-names>JM</given-names></name><name><surname>Rouder</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Developing constraint in bayesian mixed models</article-title><source>Psychological Methods</source><volume>22</volume><fpage>779</fpage><lpage>798</lpage><pub-id pub-id-type="doi">10.1037/met0000156</pub-id><pub-id pub-id-type="pmid">29265850</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>An introduction to good practices in cognitive modeling</chapter-title><source>An Introduction to Model-Based Cognitive Neuroscience</source><publisher-name>Springer</publisher-name><fpage>25</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1007/978-1-4939-2236-9_2</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname> <given-names>QJM</given-names></name><name><surname>Cools</surname> <given-names>R</given-names></name><name><surname>Gölzer</surname> <given-names>M</given-names></name><name><surname>Friedel</surname> <given-names>E</given-names></name><name><surname>Heinz</surname> <given-names>A</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Disentangling the roles of approach, activation and Valence in instrumental and pavlovian responding</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002028</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002028</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huys</surname> <given-names>QJM</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Bayesian Approaches to Learning and Decision-Making</chapter-title><source>Computational Psychiatry: Mathematical Modeling of Mental Illness </source><publisher-name>Academic Press</publisher-name><fpage>247</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1016/b978-0-12-809825-7.00010-9</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jahfari</surname> <given-names>S</given-names></name><name><surname>Ridderinkhof</surname> <given-names>KR</given-names></name><name><surname>Collins</surname> <given-names>AGE</given-names></name><name><surname>Knapen</surname> <given-names>T</given-names></name><name><surname>Waldorp</surname> <given-names>LJ</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cross-Task contributions of frontobasal ganglia circuitry in response inhibition and Conflict-Induced slowing</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>1969</fpage><lpage>1983</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy076</pub-id><pub-id pub-id-type="pmid">29912363</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kass</surname> <given-names>RE</given-names></name><name><surname>Raftery</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Bayes factors</article-title><source>Journal of the American Statistical Association</source><volume>90</volume><fpage>773</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1080/01621459.1995.10476572</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katahira</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How hierarchical models improve point estimates of model parameters at the individual level</article-title><source>Journal of Mathematical Psychology</source><volume>73</volume><fpage>37</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.03.007</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kording</surname> <given-names>K</given-names></name><name><surname>Blohm</surname> <given-names>G</given-names></name><name><surname>Schrater</surname> <given-names>P</given-names></name><name><surname>Kay</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Appreciating diversity of goals in computational neuroscience</article-title><source>OSF Preprints</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/3vy69/">https://osf.io/3vy69/</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How cognitive modeling can benefit from hierarchical bayesian models</article-title><source>Journal of Mathematical Psychology</source><volume>55</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2010.08.013</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>MD</given-names></name><name><surname>Criss</surname> <given-names>AH</given-names></name><name><surname>Devezer</surname> <given-names>B</given-names></name><name><surname>Donkin</surname> <given-names>C</given-names></name><name><surname>Etz</surname> <given-names>A</given-names></name><name><surname>Leite</surname> <given-names>FP</given-names></name><name><surname>Matzke</surname> <given-names>D</given-names></name><name><surname>Rouder</surname> <given-names>JN</given-names></name><name><surname>Trueblood</surname> <given-names>J</given-names></name><name><surname>White</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Robust modeling in cognitive science</article-title><source>PsyArXiv</source><ext-link ext-link-type="uri" xlink:href="https://psyarxiv.com/dmfhk/">https://psyarxiv.com/dmfhk/</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>MD</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Bayesian Cognitive Modeling: A Practical Course</source><publisher-name>Cambridge university press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9781139087759</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>MD</given-names></name><name><surname>Webb</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Modeling individual differences in cognition</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>12</volume><fpage>605</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.3758/BF03196751</pub-id><pub-id pub-id-type="pmid">16447375</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname> <given-names>YC</given-names></name><name><surname>Radulescu</surname> <given-names>A</given-names></name><name><surname>Daniel</surname> <given-names>R</given-names></name><name><surname>DeWoskin</surname> <given-names>V</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic interaction between reinforcement learning and attention in multidimensional environments</article-title><source>Neuron</source><volume>93</volume><fpage>451</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.040</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieder</surname> <given-names>F</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>M Huys</surname> <given-names>QJ</given-names></name><name><surname>Goodman</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Empirical evidence for resource-rational anchoring and adjustment</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>775</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.3758/s13423-017-1288-6</pub-id><pub-id pub-id-type="pmid">28484951</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorains</surname> <given-names>FK</given-names></name><name><surname>Dowling</surname> <given-names>NA</given-names></name><name><surname>Enticott</surname> <given-names>PG</given-names></name><name><surname>Bradshaw</surname> <given-names>JL</given-names></name><name><surname>Trueblood</surname> <given-names>JS</given-names></name><name><surname>Stout</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Strategic and non-strategic problem gamblers differ on decision-making under risk and ambiguity</article-title><source>Addiction</source><volume>109</volume><fpage>1128</fpage><lpage>1137</lpage><pub-id pub-id-type="doi">10.1111/add.12494</pub-id><pub-id pub-id-type="pmid">24450756</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacKay</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Information Theory, Inference and Learning Algorithms</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montague</surname> <given-names>PR</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A framework for mesencephalic dopamine systems based on predictive hebbian learning</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>1936</fpage><lpage>1947</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-05-01936.1996</pub-id><pub-id pub-id-type="pmid">8774460</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moré</surname> <given-names>JJ</given-names></name><name><surname>Sorensen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Computing a trust region step</article-title><source>SIAM Journal on Scientific and Statistical Computing</source><volume>4</volume><fpage>553</fpage><lpage>572</lpage><pub-id pub-id-type="doi">10.1137/0904038</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Heasly</surname> <given-names>B</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An approximately bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>12366</fpage><lpage>12378</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0822-10.2010</pub-id><pub-id pub-id-type="pmid">20844132</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Rumsey</surname> <given-names>KM</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Parikh</surname> <given-names>K</given-names></name><name><surname>Heasly</surname> <given-names>B</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1040</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1038/nn.3130</pub-id><pub-id pub-id-type="pmid">22660479</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Helmers</surname> <given-names>JC</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Chunking as a rational strategy for lossy data compression in visual working memory</article-title><source>Psychological Review</source><volume>125</volume><fpage>486</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1037/rev0000101</pub-id><pub-id pub-id-type="pmid">29952621</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Taming the beast: extracting generalizable knowledge from computational models of cognition</article-title><source>Current Opinion in Behavioral Sciences</source><volume>11</volume><fpage>49</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2016.04.003</pub-id><pub-id pub-id-type="pmid">27574699</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarro</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Between the Devil and the deep blue sea: tensions between scientific judgement and statistical model selection</article-title><source>Computational Brain &amp; Behavior</source><volume>2</volume><fpage>28</fpage><lpage>34</lpage></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nilsson</surname> <given-names>H</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hierarchical Bayesian parameter estimation for cumulative prospect theory</article-title><source>Journal of Mathematical Psychology</source><volume>55</volume><fpage>84</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2010.08.006</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Doherty</surname> <given-names>JP</given-names></name><name><surname>Hampton</surname> <given-names>A</given-names></name><name><surname>Kim</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Model-based fMRI and its application to reward learning and decision making</article-title><source>Annals of the New York Academy of Sciences</source><volume>1104</volume><fpage>35</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1196/annals.1390.022</pub-id><pub-id pub-id-type="pmid">17416921</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Reilly</surname> <given-names>JX</given-names></name><name><surname>Schuffelgen</surname> <given-names>U</given-names></name><name><surname>Cuell</surname> <given-names>SF</given-names></name><name><surname>Behrens</surname> <given-names>TEJ</given-names></name><name><surname>Mars</surname> <given-names>RB</given-names></name><name><surname>Rushworth</surname> <given-names>MFS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dissociable effects of surprise and model update in parietal and anterior cingulate cortex</article-title><source>PNAS</source><volume>110</volume><fpage>E3660</fpage><lpage>E3669</lpage><pub-id pub-id-type="doi">10.1073/pnas.1305373110</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otto</surname> <given-names>AR</given-names></name><name><surname>Raio</surname> <given-names>CM</given-names></name><name><surname>Chiang</surname> <given-names>A</given-names></name><name><surname>Phelps</surname> <given-names>EA</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Working-memory capacity protects model-based learning from stress</article-title><source>PNAS</source><volume>110</volume><fpage>20941</fpage><lpage>20946</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312011110</pub-id><pub-id pub-id-type="pmid">24324166</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname> <given-names>S</given-names></name><name><surname>Wyart</surname> <given-names>V</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The importance of falsification in computational cognitive modeling</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>425</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.03.011</pub-id><pub-id pub-id-type="pmid">28476348</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Piray</surname> <given-names>P</given-names></name><name><surname>Dezfouli</surname> <given-names>A</given-names></name><name><surname>Heskes</surname> <given-names>T</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Hierarchical bayesian inference for concurrent model fitting and comparison for group studies</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/393561</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Rouder</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Modeling response times for Two-Choice decisions</article-title><source>Psychological Science</source><volume>9</volume><fpage>347</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00067</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rescorla</surname> <given-names>RA</given-names></name><name><surname>Wagner</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>A theory of pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement</article-title><source>Classical Conditioning II: Current Research and Theory</source><volume>2</volume><fpage>64</fpage><lpage>99</lpage></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoux</surname> <given-names>L</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bayesian model selection for group studies - revisited</article-title><source>NeuroImage</source><volume>84</volume><fpage>971</fpage><lpage>985</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.065</pub-id><pub-id pub-id-type="pmid">24018303</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roecker</surname> <given-names>EB</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Prediction error and its estimation for Subset-Selected models</article-title><source>Technometrics</source><volume>33</volume><fpage>459</fpage><lpage>468</lpage><pub-id pub-id-type="doi">10.1080/00401706.1991.10484873</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samejima</surname> <given-names>K</given-names></name><name><surname>Ueda</surname> <given-names>Y</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name><name><surname>Kimura</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Representation of action-specific reward values in the striatum</article-title><source>Science</source><volume>310</volume><fpage>1337</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1126/science.1115270</pub-id><pub-id pub-id-type="pmid">16311337</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Estimating the dimension of a model</article-title><source>The Annals of Statistics</source><volume>6</volume><fpage>461</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1214/aos/1176344136</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname> <given-names>CR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient coding explains the universal law of generalization in human perception</article-title><source>Science</source><volume>360</volume><fpage>652</fpage><lpage>656</lpage><pub-id pub-id-type="doi">10.1126/science.aaq1118</pub-id><pub-id pub-id-type="pmid">29748284</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Somerville</surname> <given-names>LH</given-names></name><name><surname>Sasse</surname> <given-names>SF</given-names></name><name><surname>Garrad</surname> <given-names>MC</given-names></name><name><surname>Drysdale</surname> <given-names>AT</given-names></name><name><surname>Abi Akar</surname> <given-names>N</given-names></name><name><surname>Insel</surname> <given-names>C</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Charting the expansion of strategic exploratory behavior during adolescence</article-title><source>Journal of Experimental Psychology: General</source><volume>146</volume><fpage>155</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1037/xge0000250</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Starns</surname> <given-names>JJ</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The effects of aging on the speed-accuracy compromise: boundary optimality in the diffusion model</article-title><source>Psychology and Aging</source><volume>25</volume><fpage>377</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1037/a0018022</pub-id><pub-id pub-id-type="pmid">20545422</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steyvers</surname> <given-names>M</given-names></name><name><surname>Lee</surname> <given-names>MD</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A bayesian analysis of human decision-making on bandit problems</article-title><source>Journal of Mathematical Psychology</source><volume>53</volume><fpage>168</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2008.11.002</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sunnåker</surname> <given-names>M</given-names></name><name><surname>Busetto</surname> <given-names>AG</given-names></name><name><surname>Numminen</surname> <given-names>E</given-names></name><name><surname>Corander</surname> <given-names>J</given-names></name><name><surname>Foll</surname> <given-names>M</given-names></name><name><surname>Dessimoz</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Approximate bayesian computation</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1002803</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002803</pub-id><pub-id pub-id-type="pmid">23341757</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Sederberg</surname> <given-names>PB</given-names></name><name><surname>Steyvers</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A bayesian framework for simultaneously modeling neural and behavioral data</article-title><source>NeuroImage</source><volume>72</volume><fpage>193</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.01.048</pub-id><pub-id pub-id-type="pmid">23370060</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Rodriguez</surname> <given-names>CA</given-names></name><name><surname>Norcia</surname> <given-names>TM</given-names></name><name><surname>McClure</surname> <given-names>SM</given-names></name><name><surname>Steyvers</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Why more is better: simultaneous modeling of EEG, fMRI, and behavioral data</article-title><source>NeuroImage</source><volume>128</volume><fpage>96</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.030</pub-id><pub-id pub-id-type="pmid">26723544</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Sederberg</surname> <given-names>PB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Approximate Bayesian computation with differential evolution</article-title><source>Journal of Mathematical Psychology</source><volume>56</volume><fpage>375</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2012.06.004</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ravenzwaaij</surname> <given-names>D</given-names></name><name><surname>Dutilh</surname> <given-names>G</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cognitive model decomposition of the BART: assessment and application</article-title><source>Journal of Mathematical Psychology</source><volume>55</volume><fpage>94</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2010.08.010</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vandekerckhove</surname> <given-names>J</given-names></name><name><surname>Matzke</surname> <given-names>D</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>Model Comparison and the Principle of Parsimony</chapter-title><person-group person-group-type="editor"><name><surname>Busemeyer</surname> <given-names>J. R</given-names></name><name><surname>Wang</surname> <given-names>Z</given-names></name><name><surname>Townsend</surname> <given-names>J. T</given-names></name><name><surname>Eidels</surname> <given-names>A</given-names></name></person-group><source>The Oxford Handbook of Computational and Mathematical Psychology</source><volume>300</volume><publisher-name>Oxford University Press</publisher-name><fpage>300319</fpage><pub-id pub-id-type="doi">10.1093/oxfordhb/9780199957996.013.14</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viejo</surname> <given-names>G</given-names></name><name><surname>Khamassi</surname> <given-names>M</given-names></name><name><surname>Brovelli</surname> <given-names>A</given-names></name><name><surname>Girard</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Modeling choice and reaction time during arbitrary visuomotor learning through the coordination of adaptive working memory and reinforcement learning</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>9</volume><elocation-id>225</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2015.00225</pub-id><pub-id pub-id-type="pmid">26379518</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name><name><surname>Lodewyckx</surname> <given-names>T</given-names></name><name><surname>Kuriyal</surname> <given-names>H</given-names></name><name><surname>Grasman</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Bayesian hypothesis testing for psychologists: a tutorial on the Savage–Dickey method</article-title><source>Cognitive Psychology</source><volume>60</volume><fpage>158</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2009.12.001</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Farrell</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>AIC model selection using akaike weights</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>11</volume><fpage>192</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.3758/BF03206482</pub-id><pub-id pub-id-type="pmid">15117008</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname> <given-names>CM</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>van der Wee</surname> <given-names>NJ</given-names></name><name><surname>Giltay</surname> <given-names>EJ</given-names></name><name><surname>van Noorden</surname> <given-names>MS</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Nieuwenhuis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The effect of atomoxetine on random and directed exploration in humans</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0176034</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0176034</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watkins</surname> <given-names>CJCH</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Q-learning</article-title><source>Machine Learning</source><volume>8</volume><fpage>279</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1007/BF00992698</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Sofer</surname> <given-names>I</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>HDDM: hierarchical bayesian estimation of the Drift-Diffusion model in Python</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00014</pub-id><pub-id pub-id-type="pmid">23935581</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A mixture of delta-rules approximation to bayesian inference in change-point problems</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003150</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003150</pub-id><pub-id pub-id-type="pmid">23935472</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Inferring relevance in a changing world</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><elocation-id>189</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2011.00189</pub-id><pub-id pub-id-type="pmid">22291631</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Is model fitting necessary for Model-Based fMRI?</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004237</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004237</pub-id><pub-id pub-id-type="pmid">26086934</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname> <given-names>GE</given-names></name><name><surname>Li</surname> <given-names>JK</given-names></name><name><surname>Gorgolewski</surname> <given-names>KJ</given-names></name><name><surname>Poldrack</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reward learning over weeks versus minutes increases the neural representation of value in the human brain</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7649</fpage><lpage>7666</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0075-18.2018</pub-id><pub-id pub-id-type="pmid">30061189</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zajkowski</surname> <given-names>WK</given-names></name><name><surname>Kossut</surname> <given-names>M</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A causal role for right frontopolar cortex in directed, but not random, exploration</article-title><source>eLife</source><volume>6</volume><elocation-id>e27430</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.27430</pub-id><pub-id pub-id-type="pmid">28914605</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s16" sec-type="appendix"><title>The theory of model fitting</title><p>Formally, the goal of model fitting is to estimate the parameters, <inline-formula><mml:math id="inf182"><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>, for each model, <inline-formula><mml:math id="inf183"><mml:mi>m</mml:mi></mml:math></inline-formula>, that best fit the behavioral data. To do this, we take a Bayesian approach and aim to compute (or at least approximate) the posterior distribution over the parameters given the data, <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. By Bayes’ rule we can write this as<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the prior on the parameters, <inline-formula><mml:math id="inf186"><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>; <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the likelihood of the data given the parameters; and the normalization constant, <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is the probability of the data given the model (which is also known as the marginal likelihood [<xref ref-type="bibr" rid="bib51">Lee and Wagenmakers, 2014</xref>], more on this below). Because the probabilities tend to be small, it is often easier to work with the log of these quantities<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo movablelimits="false">⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo movablelimits="false">:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" movablelimits="false" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo movablelimits="false">,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mtext>log likelihood</mml:mtext></mml:munder><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The log-likelihood often gets its own symbol, <inline-formula><mml:math id="inf189"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and can be written<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf190"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability of each individual choice given the parameters of the model, which is at the heart of the definition of each model (for example in <xref ref-type="disp-formula" rid="equ1 equ2 equ3 equ4 equ5 equ6 equ7">Equations 1–7</xref>).</p><p>In a perfect world, we would evaluate the log-posterior, <inline-formula><mml:math id="inf191"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, exactly, but this can be difficult to compute and unwieldy to report. Instead, we must approximate it. This can be done using sampling approaches such as Markov Chain Monte Carlo approaches (<xref ref-type="bibr" rid="bib51">Lee and Wagenmakers, 2014</xref>), which approximate the full posterior with a set of samples. Another approach is to report a point estimate for the parameters such as the maximum of the log-posterior (the maximum a posteriori [MAP] estimate), or the maximum of the log-likelihood (the maximum likelihood estimate [MLE]). (Note that the log transformation does not change the location of the maximum, so the maximum of the log-likelihood occurs at the same value of <inline-formula><mml:math id="inf192"><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> as the maximum of the likelihood.)<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>arg</mml:mi></mml:mpadded><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>max</mml:mi></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:munder><mml:mo>⁡</mml:mo><mml:mi>log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>arg</mml:mi></mml:mpadded><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>max</mml:mi></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:munder><mml:mo>⁡</mml:mo><mml:mi>log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Note that with a uniform prior on <inline-formula><mml:math id="inf193"><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>, these two estimates coincide.</p><p>These approaches for estimating parameter values each have different strengths and weaknesses. The MCMC approach is the most principled as, with enough samples, it gives a good approximation of the posterior distribution over each parameter value. This approach also gracefully handles small data sets and allows us to combine data from different subjects in a rigorous manner. Despite these advantages, the MCMC approach is more complex (especially for beginners) and can be slow to implement. On the other hand, point estimates such as the MAP and MLE parameter values are much quicker to compute and often give similar answers to the MCMC approach when the amount of data is large (which is often the case when dealing with young and healthy populations). For this reason, we focus our discussion on the point estimate approaches, focusing in particular on maximum likelihood estimation.</p></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><sec id="s17" sec-type="appendix"><title>The theory of model comparison</title><p>In model comparison, our goal is to figure out which model of a set of possible models is most likely to have generated the data. To do this, we compute (or at least try to estimate) the probability that model <inline-formula><mml:math id="inf194"><mml:mi>m</mml:mi></mml:math></inline-formula> generated the data, <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Note that this is the normalization constant from <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>. As with parameter recovery, this probability is difficult to compute directly and so we turn to Bayes’ rule and write<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>∝</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo rspace="0pt">𝑑</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula><mml:math id="inf196"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the prior probability that model <inline-formula><mml:math id="inf197"><mml:mi>m</mml:mi></mml:math></inline-formula> is the correct model and <inline-formula><mml:math id="inf198"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the likelihood of the data given the model. In most cases, <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is assumed to be constant and so we can focus entirely on the likelihood, <inline-formula><mml:math id="inf200"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. As before, it is easier to handle the log of this quantity which is known as the marginal likelihood (<xref ref-type="bibr" rid="bib51">Lee and Wagenmakers, 2014</xref>) or Bayesian evidence, <inline-formula><mml:math id="inf201"><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib46">Kass and Raftery, 1995</xref>), for model <inline-formula><mml:math id="inf202"><mml:mi>m</mml:mi></mml:math></inline-formula>. More explicitly<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo rspace="0pt">𝑑</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If we can compute <inline-formula><mml:math id="inf203"><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> for each model, then the model with the largest evidence is most likely to have generated the data.</p><p>Note that by integrating over the parameter space, the Bayesian evidence implicitly penalizes free parameters. This is because, the more free parameters, the larger the size of the space over which we integrate and, consequently, the smaller <inline-formula><mml:math id="inf204"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is for any given parameter setting. Thus, unless the model predicts the data well for all parameter settings, it pays a price for each additional free parameter. This idea, that simpler models should be favored over more complex models if they both explain the data equally well, is known as Occam’s razor (see Chapter 28 in <xref ref-type="bibr" rid="bib56">MacKay, 2003</xref>).</p><p>Unfortunately, because it involves computing an integral over all possible parameter settings, computing the marginal likelihood exactly is usually impossible. There are several methods for approximating the integral based on either replacing it with a sum over a subset of points (<xref ref-type="bibr" rid="bib89">Wagenmakers et al., 2010</xref>; <xref ref-type="bibr" rid="bib51">Lee and Wagenmakers, 2014</xref>) or replacing it with an approximation around either the MAP or MLE estimates of the parameters. The latter approach is the most common and three particular forms are used: the Bayes Information Criterion (BIC) (<xref ref-type="bibr" rid="bib76">Schwarz, 1978</xref>), Akaike information criterion (AIC) (<xref ref-type="bibr" rid="bib4">Akaike, 1974</xref>) and the Laplace approximation (<xref ref-type="bibr" rid="bib46">Kass and Raftery, 1995</xref>). Here, we will focus on BIC which is an estimate based around the maximum likelihood estimate of the parameters, <inline-formula><mml:math id="inf205"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>,<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mover accent="true"><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf206"><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> is the number of parameters in model <inline-formula><mml:math id="inf207"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf208"><mml:mover accent="true"><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> is the value of the log-likelihood at <inline-formula><mml:math id="inf209"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>.</p><p>Finally, we have found it useful to report the results of model comparison in terms of the likelihood-per-trial <inline-formula><mml:math id="inf210"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, which can be thought of as the ‘average’ probability with which the model predicts each choice,<disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mi>T</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></boxed-text></app><app id="appendix-3"><title>Appendix 3</title><boxed-text><sec id="s18" sec-type="appendix"><title>Computing the inversion matrix from the confusion matrix</title><p>In section 'Can you arbitrate between different models?' we introduced the inversion matrix, <inline-formula><mml:math id="inf211"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>simulated model</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>fit model</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as the probability that data best fit by one model were actually generated from another model. As shown below, this can be readily computed from the confusion matrix, <inline-formula><mml:math id="inf212"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>fit model</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>simulated model</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, by Bayes rule. Abbreviating ‘simulated model’ with ‘sim’ and ‘fit model’ with ‘fit’ we have<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>sim</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>fit</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>fit</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>sim</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>sim</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mtext>sim</mml:mtext></mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>fit</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>sim</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>sim</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>For a uniform prior on models, computing the inversion matrix amounts to renormalizing the confusion matrix over the simulated models.</p></sec></boxed-text></app><app id="appendix-4"><title>Appendix 4</title><boxed-text><sec id="s19" sec-type="appendix"><title>Working memory model used for local minima example</title><p>The model and experimental designs used in <xref ref-type="fig" rid="box3fig1">Box 3—figure 1</xref> are a simplified version of those in <xref ref-type="bibr" rid="bib16">Collins and Frank (2012)</xref>. In short, the experiment attempts to parse out working memory contributions to reinforcement learning by having participants and agents learn stimulus-action contingencies from deterministic feedback, with a different number of stimuli <inline-formula><mml:math id="inf213"><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> being learned in parallel in different blocks. This manipulation targets WM load and isolates WM contributions; see <xref ref-type="bibr" rid="bib16">Collins and Frank (2012)</xref> for details.</p><p>The simplified model assumes a mixture of a classic RL component (with parameters <inline-formula><mml:math id="inf214"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf215"><mml:mi>β</mml:mi></mml:math></inline-formula>) and a working memory component with perfect one-shot learning. The mixture is controled by parameter <inline-formula><mml:math id="inf216"><mml:mi>ρ</mml:mi></mml:math></inline-formula>, capturing the prior willingness to use working memory vs. RL, and capacity parameter <inline-formula><mml:math id="inf217"><mml:mi>K</mml:mi></mml:math></inline-formula>, which scales the mixture weight in proportion to the proportion of stimuli that may be held in working memory: <inline-formula><mml:math id="inf218"><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The original model assumes additional dynamics for the working memory policy and working memory vs. RL weights that render the model more identifiable (<xref ref-type="bibr" rid="bib16">Collins and Frank, 2012</xref>).</p></sec></boxed-text></app><app id="appendix-5"><title>Appendix 5</title><boxed-text><sec id="s20" sec-type="appendix"><title>Model validation example</title><p>In this example, we imagine a deterministic stimulus-action learning task in which subjects are presented with one of three stimuli (<inline-formula><mml:math id="inf219"><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf220"><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf221"><mml:msub><mml:mi>s</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>), which instruct the subject which of three actions (<inline-formula><mml:math id="inf222"><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf223"><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf224"><mml:msub><mml:mi>a</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>) will be rewarded when chosen.</p><p>The two models that we consider are both reinforcement learning agents. The first, a ‘blind’ agent, does not see the stimulus at all and learns only about the value of the three different actions, i.e. <inline-formula><mml:math id="inf225"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, regardless of the stimulus. The second, a ‘state-based’ agent, observes the stimulus and learns a value for each action that can be different for each stimulus, i.e. <inline-formula><mml:math id="inf226"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Learning in both models occurs via a Rescorla-Wagner rule with different learning rates for positive and negative prediction errors. Thus for the blind agent, the values update according to<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>←</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:mrow></mml:math></disp-formula>while for the state-based agent, values update according to<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>←</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In both models, these values guide decisions via softmax decision rule with inverse temperature parameter, <inline-formula><mml:math id="inf227"><mml:mi>β</mml:mi></mml:math></inline-formula>.</p><p>We begin by simulating two different agents: one using the blind algorithm and the other using the state-based approach. Parameters in the models are set such that the learning curves for the two agents are approximately equal (<xref ref-type="fig" rid="box7fig1">Box 7—figure 1A</xref>, blind model: <inline-formula><mml:math id="inf228"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf229"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf230"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>6.5</mml:mn></mml:mrow></mml:math></inline-formula> state-based model: <inline-formula><mml:math id="inf231"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.65</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf232"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf233"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>). In both cases, the agents start from an accuracy of 1/3 and an asymptote at an accuracy of around 2/3 — the blind agent because this is the best it can do, the state-based agent because the softmax parameter is relatively small and hence performance is limited by noise.</p><p>Next we consider how the state-based model fits behavior from these two different agents. In <xref ref-type="fig" rid="box7fig1">Box 7—figure 1B</xref>, we plot the average likelihood with which the state-based model predicts the actual choices of the blind and state-based agents, that is the average <inline-formula><mml:math id="inf234"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mtext>state-based</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. As is clear from this figure, the state-based model predicts choices from the blind agent with <italic>higher</italic> likelihood than choices from the state-based agent! Although counter intuitive, this result does not imply that the state-based model is unable to fit its own behavior. Instead, this result reflects the difference in noise (softmax parameters) between the two subjects. The blind RL subject has a high <inline-formula><mml:math id="inf235"><mml:mi>β</mml:mi></mml:math></inline-formula>, implying less noise, allowing the state-based model to fit it quite well. Conversely, the state-based RL subject has a low <inline-formula><mml:math id="inf236"><mml:mi>β</mml:mi></mml:math></inline-formula>, implying more noise, meaning that the behavior is harder to predict even when it is fit with the correct model.</p><p>That the state-based model fits state-based behavior better than it fits blind behavior is illustrated in <xref ref-type="fig" rid="box7fig1">Box 7—figure 1C</xref>. Here we plot the simulated learning curves of the state-based model using the parameter values that were fit to either the state-based agent or the blind agent. While the fit to the state-based agent generates a learning curve quite similar to that of the subject (compare blue lines in <xref ref-type="fig" rid="box7fig1">Box 7—figure 1A and C</xref>), the state-based fit to the blind agent performs too well (compare yellow lines in panels <xref ref-type="fig" rid="box7fig1">Box 7—figure 1A and C</xref>). Thus the model validation step provides support for the state-based model when it is the correct model of behavior, but rules out the state-based model when the generating model was different.</p></sec></boxed-text></app></app-group></back></article>