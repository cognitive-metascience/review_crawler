<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">00425</article-id><article-id pub-id-type="doi">10.7554/eLife.00425</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Decoding the neural mechanisms of human tool use</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-2880"><name><surname>Gallivan</surname><given-names>Jason P</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="aff" rid="aff2"/><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-3270"><name><surname>McLean</surname><given-names>D Adam</given-names></name><xref ref-type="aff" rid="aff3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-3271"><name><surname>Valyear</surname><given-names>Kenneth F</given-names></name><xref ref-type="aff" rid="aff4"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-1066"><name><surname>Culham</surname><given-names>Jody C</given-names></name><xref ref-type="aff" rid="aff3"/><xref ref-type="aff" rid="aff5"/><xref ref-type="aff" rid="aff6"/><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Psychology</institution>, <institution>Queen’s University</institution>, <addr-line><named-content content-type="city">Kingston</named-content></addr-line>, <country>Canada</country></aff><aff id="aff2"><institution content-type="dept">Centre for Neuroscience Studies</institution>, <institution>Queen’s University</institution>, <addr-line><named-content content-type="city">Kingston</named-content></addr-line>, <country>Canada</country></aff><aff id="aff3"><institution content-type="dept">Brain and Mind Institute, Natural Sciences Centre</institution>, <institution>University of Western Ontario</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>Canada</country></aff><aff id="aff4"><institution content-type="dept">Department of Psychological Sciences</institution>, <institution>Brain Imaging Center, University of Missouri</institution>, <addr-line><named-content content-type="city">Columbia</named-content></addr-line>, <country>United States</country></aff><aff id="aff5"><institution content-type="dept">Department of Psychology</institution>, <institution>University of Western Ontario</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>Canada</country></aff><aff id="aff6"><institution content-type="dept">Neuroscience Program</institution>, <institution>University of Western Ontario</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Angelaki</surname><given-names>Dora</given-names></name><role>Reviewing editor</role><aff><institution>Baylor College of Medicine</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>jasongallivan@gmail.com</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>28</day><month>05</month><year>2013</year></pub-date><pub-date pub-type="collection"><year>2013</year></pub-date><volume>2</volume><elocation-id>e00425</elocation-id><history><date date-type="received"><day>29</day><month>11</month><year>2012</year></date><date date-type="accepted"><day>15</day><month>04</month><year>2013</year></date></history><permissions><copyright-statement>© 2013, Gallivan et al</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Gallivan et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-00425-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="commentary" xlink:href="10.7554/eLife.00866"/><abstract><object-id pub-id-type="doi">10.7554/eLife.00425.001</object-id><p>Sophisticated tool use is a defining characteristic of the primate species but how is it supported by the brain, particularly the human brain? Here we show, using functional MRI and pattern classification methods, that tool use is subserved by multiple distributed action-centred neural representations that are both shared with and distinct from those of the hand. In areas of frontoparietal cortex we found a common representation for planned hand- and tool-related actions. In contrast, in parietal and occipitotemporal regions implicated in hand actions and body perception we found that coding remained selectively linked to upcoming actions of the hand whereas in parietal and occipitotemporal regions implicated in tool-related processing the coding remained selectively linked to upcoming actions of the tool. The highly specialized and hierarchical nature of this coding suggests that hand- and tool-related actions are represented separately at earlier levels of sensorimotor processing before becoming integrated in frontoparietal cortex.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.001">http://dx.doi.org/10.7554/eLife.00425.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.00425.002</object-id><title>eLife digest</title><p>The use of tools is a key characteristic of primates. Chimpanzees—our closest living relatives—use sticks to probe for termites as well as stones to crack open nuts, and have even been seen using specially sharpened sticks as spear-like tools for hunting. However, despite its importance in human evolution, relatively little is known about how tool use is supported by the brain.</p><p>One possibility is that the brain areas involved in controlling hand movements may also begin to incorporate the use of tools. Another is that distinct brain areas evolved to enable tool use. To test these ideas, Gallivan et al. scanned the brains of human subjects as they reached towards and grasped an object using either their right hand or a set of tongs. The tongs had been designed so that they opened whenever the subjects closed their grip, thereby requiring subjects to perform a different set of movements to use the tongs as opposed to their hand alone.</p><p>Three distinct patterns of brain activity were observed. First, areas previously linked to the processing of hand movements and the human body were found to represent actions of the hand alone (and not those of the tool), whereas areas previously linked to the processing of tools and tool-related actions represented actions of the tool alone (and not those of the hand). Second, areas of motor cortex implicated in the generation of movement represented actions performed with both the hand and the tool, but showed distinct activity patterns according to which of these was to be used.</p><p>Lastly, areas associated with high-level cognitive and action-related processing showed similar patterns of activity regardless of whether the subjects were about to use the tongs or just their hand. Given that use of the hand and tool required distinct patterns of muscle contractions, this suggests that these higher-level brain regions must be encoding the action itself rather than the movements needed to achieve it.</p><p>This study is one of the first to use functional neuroimaging to examine real as opposed to simulated tool use, and increases our understanding of the neural basis of tool use in humans. This knowledge could ultimately have applications for the development of brain-machine interfaces, in which electrodes implanted in motor regions of the brain are used to control prosthetic limbs.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.002">http://dx.doi.org/10.7554/eLife.00425.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>fMRI</kwd><kwd>tool use</kwd><kwd>intention</kwd><kwd>action</kwd><kwd>perception</kwd><kwd>motor</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Canadian Institute of Health Research</institution></institution-wrap></funding-source><award-id>MOP84293</award-id><principal-award-recipient><name><surname>Culham</surname><given-names>Jody C</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>Banting Fellowship</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gallivan</surname><given-names>Jason P</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>Natural Sciences and Engineering Research Council Postdoctral fellowship award</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Valyear</surname><given-names>Kenneth F</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>Ontario Ministry of Research and Innovation Postdoctral fellowship award</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gallivan</surname><given-names>Jason P</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Imaging experiments reveal that some brain regions do not distinguish between actions performed using tools and those performed using the hands, while others represent these two types of action separately.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Tool use, whether using a stone, stick, rake, or pliers, provides an extension of the body (<xref ref-type="bibr" rid="bib105">Van Lawick-Goodall, 1970</xref>) and involves, among other things, the transfer of a proximal movement goal for the hand into a more distal goal for the tool (<xref ref-type="bibr" rid="bib57">Johnson and Grafton, 2003</xref>; <xref ref-type="bibr" rid="bib3">Arbib et al., 2009</xref>). A compelling demonstration that this transfer might actually occur at the cortical level comes from neural recordings of grasping neurons in the ventral premotor cortex (PMv) and motor cortex (M1) of macaque monkeys trained to use pliers (<xref ref-type="bibr" rid="bib101">Umilta et al., 2008</xref>). In both these areas, many neurons that encoded the specifics of hand grasping subsequently encoded tool grasping, even when use of the specific tool (reverse pliers that close as the hand grip opens) required hand kinematics opposite to those required when grasping with the hand alone. These findings suggest that tool use is supported by an effector-independent level of representation, in which the overall goal of the motor act is coded separately from the precise hand kinematics required to operate the tool. In further support of this notion, findings from human neuropsychology (<xref ref-type="bibr" rid="bib8">Berti and Frassinetti, 2000</xref>; <xref ref-type="bibr" rid="bib70">Maravita and Iriki, 2004</xref>), human behavior (<xref ref-type="bibr" rid="bib42">Gentilucci et al., 2004</xref>; <xref ref-type="bibr" rid="bib12">Cardinali et al., 2009</xref>, <xref ref-type="bibr" rid="bib13">2012</xref>), and macaque monkey neurophysiology (<xref ref-type="bibr" rid="bib53">Iriki et al., 1996</xref>) suggest that following training, a tool may actually become incorporated into the body schema of the actor and coded as an extension of the hand/limb. While provocative, how well does this single mechanism explain the neural substrates of tool use in humans, particularly within established networks that have been identified for tools (<xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>), hand actions (<xref ref-type="bibr" rid="bib23">Culham et al., 2006</xref>), and body perception (<xref ref-type="bibr" rid="bib82">Peelen and Downing, 2007</xref>)?</p><p>Although considerable research has been done on the brain networks specialized for visual processing of tools and bodies and the visual-motor processing of hand actions, these topics have largely been studied in isolation. Increasing evidence from functional magnetic resonance imaging (fMRI) suggests that human frontoparietal and occipitotemporal cortex contain specialized regions that selectively represent tools and bodies (<xref ref-type="bibr" rid="bib29">Downing et al., 2001</xref>; <xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>; <xref ref-type="bibr" rid="bib37">Frey, 2007</xref>; <xref ref-type="bibr" rid="bib82">Peelen and Downing, 2007</xref>; <xref ref-type="bibr" rid="bib83">Peeters et al., 2009</xref>; <xref ref-type="bibr" rid="bib102">Valyear and Culham, 2010</xref>; <xref ref-type="bibr" rid="bib11">Bracci et al., 2012</xref>). For instance, when individuals view, imagine, or pantomime tool use actions, the supramarginal gyrus (SMG), posterior middle temporal gyrus (pMTG), and dorsal premotor cortex (PMd)—areas that have shown some of the greatest evolutionary expansion in humans (<xref ref-type="bibr" rid="bib104">Van Essen and Dierker, 2007</xref>)—are often co-activated (<xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>; <xref ref-type="bibr" rid="bib37">Frey, 2007</xref>). fMRI studies further suggest that human occipitotemporal cortex also contains body-selective regions for perception, such as the extrastriate body area (EBA), which preferentially respond to viewing of the body and its parts (<xref ref-type="bibr" rid="bib4">Astafiev et al., 2004</xref>; <xref ref-type="bibr" rid="bib27">David et al., 2007</xref>; <xref ref-type="bibr" rid="bib82">Peelen and Downing, 2007</xref>). The frontoparietal regions activated by tools are spatially close to (and perhaps overlapping with) brain areas implicated in hand actions, particularly the grip component of reach-to-grasp actions. Specifically, SMG lies very near the grasp-selective anterior intraparietal sulcus (aIPS, <xref ref-type="bibr" rid="bib16">Chao and Martin, 2000</xref>; <xref ref-type="bibr" rid="bib103">Valyear et al., 2007</xref>) and PMd shows grasp-selective as well as tool-selective responses (<xref ref-type="bibr" rid="bib45">Grezes and Decety, 2002</xref>; <xref ref-type="bibr" rid="bib39">Gallivan et al., 2011</xref>). In addition, real hand actions activate other frontoparietal regions including the superior parieto-occipital cortex (SPOC) region, PMd, and additional areas along the IPS (<xref ref-type="bibr" rid="bib23">Culham et al., 2006</xref>; <xref ref-type="bibr" rid="bib34">Filimon, 2010</xref>), but the specific role of these areas in tool use remains unexplored. Moreover, almost all of the human neuroimaging studies of tools to date have used proxies for real tool use (reviewed in <xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>), including visual stimuli such as images or movies (e.g., <xref ref-type="bibr" rid="bib6">Beauchamp et al., 2002</xref>), semantic tasks (e.g., <xref ref-type="bibr" rid="bib71">Martin et al., 1995</xref>), or simulated tool actions like pantomiming, imitating or imagining tool use (e.g., <xref ref-type="bibr" rid="bib58">Johnson-Frey et al., 2005</xref>; <xref ref-type="bibr" rid="bib94">Rumiati et al., 2004</xref>) or making perceptual judgments about how one would use a tool (e.g., <xref ref-type="bibr" rid="bib55">Jacobs et al., 2010</xref>). It remains unclear whether the highly specialized brain areas within these tool-, body-, and action-related networks in humans also play important roles in planning real movements with a tool or with the body (hand) alone.</p><p>The purpose of the current study was to examine exactly how and where in the human brain tool-specific, hand-specific, and effector-independent (shared hand and tool) representations are coded. To this aim we used fMRI to examine neural activity while human subjects performed a delayed-movement task that required grasp or reach actions towards a single target object. Critically, subjects performed these two different movements using either their hand or reverse tongs, which required opposite operating kinematics compared to when the hand was used alone. This manipulation allowed us to maintain a common set of actions throughout the experiment (grasping vs reaching) while at the same time varying the movement kinematics required to achieve those actions (i.e., depending on whether the hand vs tool effector was used). Using multi-voxel pattern analysis (MVPA) to decode preparatory (pre-movement) signals, we then probed exactly where in frontoparietal cortex and in tool- and body-selective areas in occipitotemporal cortex movement plans (grasping vs reaching) for the hand and tool were distinct (effector-specific) vs where signals related to upcoming actions of the hand could be used to predict the same actions performed with the tool (effector-independent).</p><p>Consistent with an effector-specific coding of hand- and tool-related movements we found that preparatory signals in SPOC and EBA differentiated upcoming movements of the hand only (i.e., hand-specific) whereas in SMG and pMTG they discriminated upcoming movements of the tool only (i.e., tool-specific). In addition, in anterior parietal regions (e.g., aIPS) and motor cortex we found that pre-movement activity patterns discriminated planned actions of ‘both’ the hand and tool but, importantly, could not be used to predict upcoming actions of the other effector. Instead, we found that this effector-independent type of coding was constrained to the preparatory signals of a subset of frontoparietal areas (posterior IPS and premotor cortex), suggesting that in these regions neural representations are more tightly linked to the goal of the action (grasping vs reaching) rather than the specific hand movements required to implement those goals.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>fMRI (3 Tesla) was used to measure the blood oxygenation level-dependent (BOLD) signal in the brains of 13 right-handed subjects (7 females; mean age: 25.7 years) during a slow event-related design with a delay interval. Subjects used either the right hand or a tool (controlled by the right hand) to execute a precision reach-to-grasp (Grasp) or reach-to-touch (Reach) movement towards a single centrally located real three-dimensional (3D) target object made of Lego blocks (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The tool used was a set of reverse tongs; when the hand closed on the grips, the ends of the tongs would open and vice versa. As such, different hand kinematics were required to operate the tool compared to when the hand was used alone. Use of the hand and tool were alternated across experimental runs. The position of the target object was changed between hand and tool experimental runs in order for the grasps and reaches to be performed at a comfortable distance for each effector (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). On each trial, subjects were first cued to the action to be carried out (grasp or reach). Then, following a delay period, they performed the instructed action (with the hand or tool, depending on the experimental run). The delay timing of the paradigm allowed us to divide the trial into discrete time epochs and isolate the sustained plan-related neural responses that evolve prior to movement from the transient visual response (Preview phase) and the movement execution response (Execute phase; <xref ref-type="fig" rid="fig1">Figure 1C,D</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.00425.003</object-id><label>Figure 1.</label><caption><title>Experimental methods and evoked neural activity.</title><p>(<bold>A</bold>) Subject setup shown from side view. (<bold>B</bold>) (Left) experimental apparatus and target object shown from the subject’s point of view for experimental runs where either the hand (top) or reverse tool (bottom) were used. The location of the target object (white block) was switched between run types but did not change its position from trial-to-trial within a imaging run. Dashed line represents the participant’s arc of reachability for each run type. In both cases (left panels), the hand is shown at its starting location. Green star with dark shadow represents the fixation LED and its location in depth. (Right) Hand and tool positions during movements performed by the subject. (<bold>C</bold>) Timing of each event-related trial. Trials began with the 3D object being illuminated while the subject maintained fixation (Preview phase; 6 s). Subjects were then instructed via headphones to perform one of two movements: Grasp the object (‘Grasp’) without lifting it or Touch the object (‘Touch’), initiating the Plan phase portion of the trial. Following a fixed delay interval (12 s), subjects were cued (by an auditory ‘beep’) to perform the instructed movement (initiating the Execute phase) and then return to the starting location. 2 s after the Go cue, vision of the workspace was extinguished and participants waited for the following trial to begin (14-s intertrial interval, ITI). (<bold>D</bold>) Averaged fMRI activity from left dorsal premotor (PMd) cortex, time-locked to trial length. MVPA was performed using single fMRI trials in two ways: 1) based on the % signal change (SC) BOLD activation evoked for each single time point in the trial (time-resolved decoding), allowing us to pinpoint when predictive movement information was available and 2) based on a windowed average of the % SC BOLD activation in the 4 s (2 imaging volumes) prior to movement initiation (denoted by the gray shaded bar).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.003">http://dx.doi.org/10.7554/eLife.00425.003</ext-link></p></caption><graphic xlink:href="elife-00425-fig1-v1.tif"/></fig></p><p>We implemented MVPA in specific frontoparietal and occipitotemporal cortex regions-of-interest (ROIs) for each time-point within a trial and examined, during movement planning (Plan Phase): 1) whether we could predict upcoming grasps (G) vs reaches (R) with either the hand (i.e., Hand-G vs Hand-R) or tool (i.e., Tool-G vs Tool-R) or both and 2) where in the network of areas preparatory patterns of activity for the hand could be used to predict preparatory patterns of activity for the tool and vice versa (e.g., where Hand-G predicts Tool-G activity, and vice versa). With respect to this second aim, it is important to note that based on differences between hand and tool experimental runs, a brain area showing effector-independent preparatory activity patterns cannot be attributable to low-level similarities in motor kinematics (i.e., because the hand and tool required opposite operating mechanics) or sensory input across trial types (i.e., because the object’s visual position with respect to fixation changed between hand and tool runs).</p><p>We first localized a common set of action-related ROIs within each individual subject for subsequent MVPA. These ROIs were defined by performing a whole-brain voxel-wise search contrasting the activity evoked during movement generation (i.e., movement planning [Plan phase] and execution [Execute phase]) vs the activity evoked during simple visual object presentation (Preview phase; when subjects had vision of the target object yet were unaware of which action [Grasp vs Reach] to perform). This [Plan &amp; Execute &gt; 2*Preview] contrast revealed activity throughout a well-documented frontoparietal network of areas (<xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="table" rid="tbl1">Table 1</xref> for coordinates). Within this network, we focused MVPA on 10 commonly described neuroanatomical ROIs in the left hemisphere (contralateral to the arm used), each previously implicated in either hand-related and/or tool-related processing. We localized superior parieto-occipital cortex (SPOC), posterior intraparietal sulcus (pIPS), middle IPS (midIPS), motor cortex, and dorsal premotor (PMd) cortex—a group of well-known parietal and frontal areas generally implicated in hand-related movement planning processes in human and/or macaque cortex (<xref ref-type="bibr" rid="bib23">Culham et al., 2006</xref>; <xref ref-type="bibr" rid="bib2">Andersen and Cui, 2009</xref>; <xref ref-type="bibr" rid="bib19">Cisek and Kalaska, 2010</xref>). In addition, we defined the anterior intraparietal sulcus (aIPS), a region just posterior to aIPS (post. aIPS), supramarginal gyrus (SMG), and ventral premotor (PMv) cortex—a group of parietal and frontal areas generally implicated in hand preshaping and tool-related processes (<xref ref-type="bibr" rid="bib92">Rizzolatti and Luppino, 2001</xref>; <xref ref-type="bibr" rid="bib23">Culham et al., 2006</xref>; <xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>; <xref ref-type="bibr" rid="bib101">Umilta et al., 2008</xref>). One additional area, left somatosensory (SS-cortex), was selected as a sensory control region, not expected to accurately decode movements until stimulation of the hand’s mechanoreceptors at movement onset (i.e., at the Execute phase).<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.00425.004</object-id><label>Figure 2.</label><caption><title>Frontoparietal brain areas selected for movement plan decoding.</title><p>Cortical areas that exhibited larger responses during movement preparation and/or execution than the preceding visual phase [(Plan + Execute) &gt; 2*(Preview)] are shown as orange/yellow activation. Results calculated across all subjects (Random Effects GLM) are displayed on one representative subject’s inflated cortical hemispheres. The general locations of the selected ROIs are outlined in circles (actual ROIs were anatomically defined separately in each subject). Linked to each ROI is the corresponding % SC BOLD activity averaged across voxels, trials, and subjects within each ROI and plotted according to trial length. This time course activity clearly delineates the sustained preparatory responses that form prior to movement onset in each area. Vertical lines correspond to the onset of the Plan and Execute phases of each trial (from left to right). Sulcal landmarks are denoted by white lines (stylized according to the corresponding legend). ROI acronyms are spelled out in main text.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.004">http://dx.doi.org/10.7554/eLife.00425.004</ext-link></p></caption><graphic xlink:href="elife-00425-fig2-v1.tif"/></fig><table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.00425.005</object-id><label>Table 1.</label><caption><p>ROIs with corresponding Talairach coordinates (mean x, y, and z centre of mass and standard deviations) and sizes</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.005">http://dx.doi.org/10.7554/eLife.00425.005</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">ROI name</th><th colspan="6">Tailarach coordinates</th><th colspan="2">ROI size</th></tr><tr><th>x</th><th>y</th><th>z</th><th>std x</th><th>std y</th><th>std z</th><th>mm<sup>3</sup></th><th>Nr voxels</th></tr></thead><tbody><tr><td>Parieto-frontal ROIs</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td> L Superior parieto-occipital cortex (SPOC)</td><td align="char" char=".">−8</td><td align="char" char=".">−75</td><td align="char" char=".">29</td><td align="char" char=".">2.4</td><td align="char" char=".">2.9</td><td align="char" char=".">3.6</td><td align="char" char=".">1469</td><td align="char" char=".">54</td></tr><tr><td> L Posterior intraparietal sulcus (pIPS)</td><td align="char" char=".">−22</td><td align="char" char=".">−68</td><td align="char" char=".">45</td><td align="char" char=".">3.4</td><td align="char" char=".">3.7</td><td align="char" char=".">4.1</td><td align="char" char=".">1640</td><td align="char" char=".">61</td></tr><tr><td> L Middle intraparietal sulcus (midIPS)</td><td align="char" char=".">−32</td><td align="char" char=".">−56</td><td align="char" char=".">46</td><td align="char" char=".">4.4</td><td align="char" char=".">4</td><td align="char" char=".">3.9</td><td align="char" char=".">1943</td><td align="char" char=".">72</td></tr><tr><td> L Posterior anterior intraparietal sulcus (post. aIPS)</td><td align="char" char=".">−42</td><td align="char" char=".">−49</td><td align="char" char=".">43</td><td align="char" char=".">3.6</td><td align="char" char=".">4.3</td><td align="char" char=".">3.9</td><td align="char" char=".">2290</td><td align="char" char=".">85</td></tr><tr><td> L Anterior intraparietal sulcus (aIPS)</td><td align="char" char=".">−42</td><td align="char" char=".">−40</td><td align="char" char=".">42</td><td align="char" char=".">4.2</td><td align="char" char=".">4.3</td><td align="char" char=".">4.2</td><td align="char" char=".">2067</td><td align="char" char=".">77</td></tr><tr><td> L Supramarginal gyrus (SMG)</td><td align="char" char=".">−56</td><td align="char" char=".">−35</td><td align="char" char=".">33</td><td align="char" char=".">3.7</td><td align="char" char=".">3.7</td><td align="char" char=".">4</td><td align="char" char=".">1479</td><td align="char" char=".">55</td></tr><tr><td> L Motor cortex</td><td align="char" char=".">−38</td><td align="char" char=".">−29</td><td align="char" char=".">48</td><td align="char" char=".">4.3</td><td align="char" char=".">4.3</td><td align="char" char=".">4.3</td><td align="char" char=".">2407</td><td align="char" char=".">89</td></tr><tr><td> L Dorsal premotor (PMd) cortex</td><td align="char" char=".">−26</td><td align="char" char=".">−14</td><td align="char" char=".">52</td><td align="char" char=".">4.2</td><td align="char" char=".">3.8</td><td align="char" char=".">4.1</td><td align="char" char=".">2135</td><td align="char" char=".">79</td></tr><tr><td> L Ventral premotor (PMv) cortex</td><td align="char" char=".">−52</td><td align="char" char=".">3</td><td align="char" char=".">15</td><td align="char" char=".">3.3</td><td align="char" char=".">3.8</td><td align="char" char=".">3.1</td><td align="char" char=".">1460</td><td align="char" char=".">54</td></tr><tr><td> L Somatosensory (SS) cortex</td><td align="char" char=".">−39</td><td align="char" char=".">−40</td><td align="char" char=".">48</td><td align="char" char=".">2.8</td><td align="char" char=".">2.8</td><td align="char" char=".">2.9</td><td align="char" char=".">1592</td><td align="char" char=".">59</td></tr><tr><td>Localizer-defined ROIs</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td> L tool-anterior intraparietal sulcus (t-aIPS)</td><td align="char" char=".">−41</td><td align="char" char=".">−42</td><td align="char" char=".">46</td><td align="char" char=".">3.3</td><td align="char" char=".">3.4</td><td align="char" char=".">4.2</td><td align="char" char=".">1038</td><td align="char" char=".">38</td></tr><tr><td> L Posterior middle temporal gyrus (pMTG)</td><td align="char" char=".">−53</td><td align="char" char=".">−57</td><td align="char" char=".">−3</td><td align="char" char=".">4.1</td><td align="char" char=".">3.1</td><td align="char" char=".">3.3</td><td align="char" char=".">621</td><td align="char" char=".">23</td></tr><tr><td> L Extrastriate body area (EBA)</td><td align="char" char=".">−49</td><td align="char" char=".">−72</td><td align="char" char=".">1</td><td align="char" char=".">2.8</td><td align="char" char=".">3.6</td><td align="char" char=".">3.7</td><td align="char" char=".">851</td><td align="char" char=".">32</td></tr></tbody></table><table-wrap-foot><fn><p>Mean ROI sizes across subjects from ACPC data (in mm<sup>3</sup> and functional voxels). Std = standard deviation.</p></fn></table-wrap-foot></table-wrap></p><p>A subset of participants from the motor experiment (n = 8) were recruited for a second localizer experiment. The purpose of this second scan session was to localize well-documented visual-perceptual ROIs involved in body- and tool-selective processing and then examine, using the motor experiment data from these same subjects, whether hand and tool movement plans could be decoded from these separately identified object-selective areas. One area of interest, EBA, has been widely implicated in coding perceptual functions related to the body, including the identification of other individuals, the processing of others’ emotional states and movements, as well as perception of the self and representations of the body schema (<xref ref-type="bibr" rid="bib28">Downing and Peelen, 2011</xref>). Likewise, pMTG has been implicated in coding numerous perceptual functions related to tools, including tool motion, identification, naming, auditory processing, and the retrieval of semantic information concerning their use (e.g., <xref ref-type="bibr" rid="bib72">Martin et al., 1996</xref>; <xref ref-type="bibr" rid="bib5">Beauchamp and Martin, 2007</xref>; <xref ref-type="bibr" rid="bib64">Lewis et al., 2005</xref>). Interestingly, both ROIs are also activated by self-generated movements (unseen hand actions in the case of EBA and pantomimed tool actions in the case of pMTG; <xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>; <xref ref-type="bibr" rid="bib37">Frey, 2007</xref>; <xref ref-type="bibr" rid="bib28">Downing and Peelen, 2011</xref>). The aim here was to clarify the nature and specificity of these sensory-motor responses in the context of planning object-directed hand and tool movements.</p><p>To localize these visual-perceptual ROIs we used a standard block-design localizer task in which participants were required to view static color photos of familiar tools, headless bodies, non-tool objects, and scrambled up versions of these same stimuli (<xref ref-type="fig" rid="fig5">Figure 5A</xref> for timing and protocol; see also <xref ref-type="bibr" rid="bib102">Valyear and Culham, 2010</xref>). To identify the brain areas selectively involved in tool-related visual processing, in each subject we searched for regions showing heightened activation for tools compared to headless bodies, non-tool objects, and scrambled stimuli (conjunction contrast: [(Tools &gt; Bodies) AND (Tools &gt; Objects) AND (Tools &gt; Scrambled)], t = 3, p&lt;0.005, corrected). Across subjects, this contrast revealed consistent activity in two brain regions: 1) pMTG and 2) an area located anteriorly along the IPS (tool-aIPS, t-aIPS). The anatomical locations of these tool-specific activations were highly consistent with previous investigations (<xref ref-type="fig" rid="fig5 fig6">Figures 5 and 6</xref>; <xref ref-type="bibr" rid="bib69">Mahon et al., 2007</xref>; <xref ref-type="bibr" rid="bib102">Valyear and Culham, 2010</xref>). We next searched for body-selective regions by contrasting activity for bodies compared to tools, non-tool objects, and scrambled stimuli (conjunction contrast: [(Bodies &gt; Tools) AND (Bodies &gt; Objects) AND (Bodies &gt; Scrambled)], t = 3, p&lt;0.005, corrected). Across subjects, this contrast revealed consistent activity in EBA, which responds selectively to human bodies and body parts when compared with objects and other control stimuli (<xref ref-type="bibr" rid="bib29">Downing et al., 2001</xref>; <xref ref-type="fig" rid="fig6">Figure 6</xref>). Notably, in each subject we found a highly consistent spatial relationship between EBA and pMTG: pMTG was characteristically positioned lateral, ventral, and anterior to EBA (consistent with <xref ref-type="bibr" rid="bib102">Valyear and Culham, 2010</xref>).</p><sec id="s2-1"><title>Movement plan decoding</title><p>To pinpoint when predictive movement information was available in the spatial voxel patterns, we ran a single-trial decoding analysis for each point in time over the course of a trial (<xref ref-type="bibr" rid="bib99">Soon et al., 2008</xref>; <xref ref-type="bibr" rid="bib47">Harrison and Tong, 2009</xref>). This time-resolved decoding analysis revealed a full range of decoding profiles during movement planning across the network of specified regions. For instance, preparatory voxel patterns in SPOC and EBA accurately predicted upcoming grasping vs reaching actions with the hand only whereas preparatory voxel patterns in SMG and pMTG successfully predicted grasping vs reaching actions with the tool only (<xref ref-type="fig" rid="fig3 fig6">Figures 3 and 6</xref>, red and blue decoding traces). Notably, in nearly all the remaining regions, we were able to successfully use the activity patterns to predict the action performed (grasping vs reaching) for both the hand and tool effector (<xref ref-type="fig" rid="fig3 fig4 fig5">Figures 3–5</xref>, red and blue decoding traces; purple traces will be discussed in the next section entitled ‘Separate and shared representations for the hand and tool’). For instance, in parietal cortex, preparatory activity in pIPS, midIPS, post. aIPS, aIPS, and t-aIPS could be used to accurately discriminate which object-directed hand or tool movement was to be performed moments later (for overlap between t-aIPS and the post. aIPS and aIPS regions, see <xref ref-type="fig" rid="fig5">Figure 5</xref>). Likewise, in frontal cortex, predictive movement activity for hand and tool actions was also found in motor cortex, PMd, and PMv. Importantly, consistent with expectations and previous investigations (<xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>, <xref ref-type="bibr" rid="bib40">2011b</xref>), our sensory control region, SS-cortex, failed to decode any planned movements and only discriminated the different actions upon execution (<xref ref-type="fig" rid="fig3">Figure 3</xref>). To verify the observations obtained from the time-resolved decoding analysis, we also averaged the spatial activity patterns generated over a 4-s (2-volume) window of time immediately prior to the cue for subjects to perform the movement (denoted by gray shaded bars in <xref ref-type="fig" rid="fig3 fig4 fig5 fig6">Figures 3–6</xref>). In line with our previous fMRI investigations (<xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>, <xref ref-type="bibr" rid="bib40">2011b</xref>), this plan-epoch decoding approach supports the notion that discriminatory predictive signals for movement can arise moments prior to action execution.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.00425.006</object-id><label>Figure 3.</label><caption><title>Separate movement plans for the hand and tool decoded from frontoparietal cortex.</title><p>Decoding accuracies are shown for each time point in the trial (time-resolved decoding) and for the Plan-epoch only, the latter based on a windowed average of the spatial activity patterns denoted by the gray shaded bars in the time-resolved decoding plots. In the time-resolved decoding plots, vertical lines correspond to the onset of the Plan and Execute phases of each trial (from left to right). For decoding accuracies discriminating grasp vs reach actions with the Hand (in red) and Tool (in blue) classifier training and testing was done using a single trial N-1 cross-validation approach. Across-effector decoding accuracies (in purple) were computed using all the available data and from training classifiers on Hand-G vs Hand-R trials and testing on Tool-G vs Tool-R trials and then averaging these values with the opposite train-and-test ordering, within each subject. (<bold>A</bold>) Areas of frontoparietal cortex that could decode movement plans with the hand and/or with the tool but not between hand and tool (i.e., no Across-effector decoding). (<bold>B</bold>) Decoding accuracies from the sensory control region, SS-cortex. Note that SS-cortex significantly decodes movements only following action onset (and not during planning). Error bars represent standard error of the mean (SEM) across subjects. Solid black horizontal lines are chance accuracy level (50%). Asterisks assess statistical significance with two-tailed <italic>t</italic>-tests across subjects with respect to 50%. Four-pointed stars assess statistical significance based on a false discovery rate (FDR) correction of q ≤ 0.05. Note also that in the time-resolved decoding plots, the color of each asterisk/star denotes which specific pair-wise discrimination is significant at each point in time. G: grasp; R: reach.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.006">http://dx.doi.org/10.7554/eLife.00425.006</ext-link></p></caption><graphic xlink:href="elife-00425-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.00425.007</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Classifier decoding accuracies in non-brain control regions.</title><p>(Left) Non-brain control ROIs defined in each subject (denoted in light yellow; example subject shown). (Right) Linked to each ROI is the % SC time-course activity and the time-resolved and plan-epoch decoding accuracies (computed and plotted the same as in <xref ref-type="fig" rid="fig3">Figure 3</xref>). Error bars represent standard error of the mean (SEM) across subjects. Solid black lines are chance accuracy level (50%). Note that no significant differences at any point in the trial were found with respect to 50% chance.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.007">http://dx.doi.org/10.7554/eLife.00425.007</ext-link></p></caption><graphic xlink:href="elife-00425-fig3-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.00425.008</object-id><label>Figure 4.</label><caption><title>Shared movement plans for the hand and tool decoded from frontoparietal cortex.</title><p>Decoding accuracies are plotted and computed the same as in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Significant across-effector decoding (purple traces) shows where and when the movement action (Grasp vs Reach) is being represented with some invariance to the acting effector (Hand vs Tool). See <xref ref-type="fig" rid="fig3">Figure 3</xref> caption for format.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.008">http://dx.doi.org/10.7554/eLife.00425.008</ext-link></p></caption><graphic xlink:href="elife-00425-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.00425.009</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Time-resolved and plan-epoch decoding accuracies for across-effector classification, separated according to the direction of classifier training and testing.</title><p>Pink traces and bars denote accuracies that were computed by training classifiers to discriminate hand trials (Hand-G vs Hand-R) and testing on tool trials (Tool-G vs Tool-R). Light blue traces and bars denote accuracies that were computed by training classifiers to discriminate tool trials and testing on hand trials. As in <xref ref-type="fig" rid="fig3 fig4 fig5 fig6">Figures 3–6</xref>, across-effector accuracies were computed using all the available data. Error bars represent standard error of the mean (SEM) across subjects. Solid black horizontal lines are chance accuracy level (50%). Asterisks assess statistical significance with two-tailed <italic>t</italic>-tests across subjects with respect to 50%. Four-pointed stars assess statistical significance based on a FDR correction of q ≤ 0.05.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.009">http://dx.doi.org/10.7554/eLife.00425.009</ext-link></p></caption><graphic xlink:href="elife-00425-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.00425.010</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Voxel weight analyses for the plan-epoch activity in the cross-decoding ROIs (L-pIPS, L-midIPS, L-PMd, and L-PMv), shown for two representative subjects (in A and B).</title><p>(Top of <bold>A</bold> and <bold>B</bold>) ROIs (in yellow) overlayed on the transverse slices of subjects 1 (<bold>A</bold>) and 2 (<bold>B</bold>). (Below) Voxel weights for the trained SVM classifier for the plan-epoch (2 imaging volumes prior to movement initiation). Voxel weights are based on the single-shot train iterations using all the available data. Each column of boxes corresponds to one of the two pair-wise comparisons and each row of boxes corresponds to a transverse slice (3 mm thickness) through the ROI (see expanded box at bottom in (<bold>C</bold>) for legend; voxel size = 3 mm × 3 mm × 3 mm). The color of each voxel in each box denotes its relationship (weight) with the class label (as determined by the trained SVM discriminant function; see scale at bottom in (<bold>C</bold>) for voxel weight color coding). Positive and negative values (red and blue colors, respectively) denote a stronger weighting of a particular voxel towards one planned action vs the other (red = grasp-selective voxels, blue = reach-selective voxels). Gray patches denote the borders of the ROI. Accuracies below each column denotes the test accuracy for that specific pair-wise comparison in the subject (when averaged across the N train-and-test iterations) and shown at the very bottom, the test accuracies for the specific cross-decoding case (based on all the available data). The spatial arrangement of grasp- and reach-selective voxels indicates considerable local variability. (Bottom of <bold>A</bold> and <bold>B</bold>) Voxel weight fingerprints for the 10 most discriminative voxels within a ROI. For each ROI, the raw voxel weights across the two pair-wise comparisons were ordered and the top 10 voxels were selected (i.e., the same 10 discriminative voxels are shown in each plot for each ROI). Within each pair-wise comparison, voxel weights for the common voxel set were normalized to 1 and their magnitudes were plotted around the polar axis (each axis of the polar plot represents a single voxel). The direction of the voxel weights is encoded by the line color: Positive (grasp-specific) voxel weights are plotted in red and the negative (reach-specific) weights are plotted in blue, congruent with the ROI voxel weight maps. L: left; R: right; A: anterior; P: posterior; S: superior; I=inferior.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.010">http://dx.doi.org/10.7554/eLife.00425.010</ext-link></p></caption><graphic xlink:href="elife-00425-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.00425.011</object-id><label>Figure 4—figure supplement 3.</label><caption><title>Movement instructions decoded from transient (but not sustained) responses in auditory cortex.</title><p>(Top) Left auditory cortex activity localized by the same contrast used to identify the frontoparietal sensorimotor network [(Plan + Execute) &gt; 2*(Preview)]. Results calculated across all subjects (Random Effects GLM) are displayed on one representative subject's inflated left hemisphere. The general location of Heschl's gyrus is outlined in a blue circle (actual ROIs were anatomically defined separately in each subject according to stringent anatomical criteria, see main manuscript text). (Below) % SC time-course activity and the time-resolved and plan-epoch decoding accuracies from left auditory cortex. Error bars represent standard error of the mean (SEM) across subjects. Solid black lines are chance accuracy level (50%). Asterisks assess statistical significance with two-tailed <italic>t</italic>-tests across subjects with respect to 50%. Four-pointed stars assess statistical significance based on a FDR correction of q ≤ 0.05. Note that above chance auditory cue decoding transiently arises halfway through the Plan-phase (consistent with a discrimination of the ‘Grasp’ and ‘Touch’ auditory commands delivered to subjects via headphones at the onset of the Plan-phase) but, importantly, not during the pre-defined plan-epoch (denoted by gray bar).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.011">http://dx.doi.org/10.7554/eLife.00425.011</ext-link></p></caption><graphic xlink:href="elife-00425-fig4-figsupp3-v1.tif"/></fig></fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.00425.012</object-id><label>Figure 5.</label><caption><title>Hand and Tool movement plans decoded from the localizer-defined t-aIPS.</title><p>(<bold>A</bold>) Block-design protocol and experimental timing of the Bodies, Objects, and Tools (BOT) localizer. (<bold>B</bold>) Overlay of tool and anterior parietal ROIs. The Motor experiment-defined anterior parietal ROIs (post. aIPS and aIPS; defined by the [(Plan + Execute) &gt; 2*(Preview)] contrast) and the Localizer experiment-defined anterior parietal ROI (t-aIPS; defined by the [(Tools &gt; Scrambled) AND (Tools &gt; Bodies) AND (Tools &gt; Objects)] conjunction contrast) are superimposed on the transverse anatomical slices of three representative subjects. Across all subjects we found a reasonable degree of overlap between the Motor and Localizer experiment-defined anterior parietal ROIs. (<bold>C</bold>) % SC time-course activity and time-resolved and plan-epoch decoding accuracies from t-aIPS. See <xref ref-type="fig" rid="fig3">Figure 3</xref> caption for format.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.012">http://dx.doi.org/10.7554/eLife.00425.012</ext-link></p></caption><graphic xlink:href="elife-00425-fig5-v1.tif"/></fig><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.00425.013</object-id><label>Figure 6.</label><caption><title>Tool and hand movement plans decoded from the localizer-defined pMTG and EBA, respectively.</title><p>(Top) The pMTG (in red) and EBA (in green) are shown in the same three representative subjects as in <xref ref-type="fig" rid="fig5">Figure 5</xref>. pMTG was defined using the conjunction contrast of [(Tools &gt; Scrambled) AND (Tools &gt; Bodies) AND (Tools &gt; Objects)] in each subject. EBA was defined using the conjunction contrast of [(Bodies &gt; Scrambled) AND (Bodies &gt; Tools) AND (Bodies &gt; Objects)]. (Below) % SC time-course activity and time-resolved and plan-epoch decoding accuracies shown for pMTG (bordered in red) and EBA (bordered in green). See <xref ref-type="fig" rid="fig3">Figure 3</xref> caption for format.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.013">http://dx.doi.org/10.7554/eLife.00425.013</ext-link></p></caption><graphic xlink:href="elife-00425-fig6-v1.tif"/></fig></p><p>To ensure our decoding accuracies could not result from spurious factors (e.g., task-correlated head or arm movements), we ran the same classification analyses in two non-brain ROIs where decoding should not be expected: the right ventricle and outside the brain. Consistent with our previous work (<xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>, <xref ref-type="bibr" rid="bib40">2011b</xref>; <xref ref-type="bibr" rid="bib41">Gallivan et al., 2013</xref>), MVPA in these two areas showed no accurate decoding for any phase of the trial (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>Three general observations can be made based on the results of these decoding analyses. First, predictive movement information, if it is to emerge, generally arises in the two time points prior to initiation of the movement (although note that in a few brain areas, such as L-pIPS and L-PMd, this information is also available prior to these two time points). Second, in support of the notion that this predictive motor information is directly related to the ‘intention’ to make a movement, accurate classification never arises prior to the subject being aware of which action to execute (i.e., prior to the auditory instruction delivered at the initiation of the Plan phase). Finally, decoding related to the planning of a movement can be fully disentangled from decoding related to movement execution, which generally arises several imaging volumes later.</p></sec><sec id="s2-2"><title>Separate and shared representations for the hand and tool</title><p>Expanding on these MVPA results—and perhaps more important to the overall interpretations of our findings—we next examined in which brain areas the final action (grasping vs reaching) was being represented with some invariance to the effector to be used. To do this, we trained pattern classifiers to discriminate Hand-G vs Hand-R trials and then tested their performance in discriminating Tool-G vs Tool-R trials (the opposite train-and-test process—train set: Tool-G vs Tool-R → test set: Hand-G vs Hand-R—was also performed, and then we averaged the accuracies from both approaches) (for this technique, see also <xref ref-type="bibr" rid="bib36">Formisano et al., 2008</xref>; <xref ref-type="bibr" rid="bib47">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>). If successful, this cross-classification would suggest that the object-directed action plans being decoded are to some extent independent of the acting effector (at least to the extent that accurate across-effector classification can be achieved). When we performed this analysis, we found accurate across-effector classification in four regions during planning: two areas in posterior parietal cortex (PPC), pIPS and midIPS, and two areas in premotor cortex, PMd and PMv (see purple decoding traces and bars in <xref ref-type="fig" rid="fig4">Figure 4</xref>). (Note that separating these tests, Train set: Hand →Test set: Tool and Train set: Tool → Test set: Hand, revealed no major asymmetries in classification, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Importantly, recall that because the object location was changed (with respect to fixation) between hand and tool experimental runs coupled with the fact that the reverse tool required operating mechanics opposite from those required when the hand was used alone, accurate across-effector classification cannot be attributed to low-level visual, haptic, or kinematic similarities between hand and tool trials. Furthermore, note that accurate across-effector classification does not simply arise in ‘any’ area where the pattern classifiers are able to successfully discriminate grasp vs reach movements for both the hand and tool. Indeed, although several other areas accurately differentiated the two upcoming movements for both effectors (e.g., post. aIPS, aIPS, t-aIPS, and motor cortex), the preparatory spatial patterns of activity in these areas did not allow for accurate cross-classification. This finding is in itself notable, as it suggests that these latter areas may contain separate coding schemes for the hand and tool. One obvious interpretation of this result is that these latter areas separately code the kinematics used to operate the hand vs tool, providing a neural instantiation of the effector-specific representations thought to be critical for complex tool use. These findings are summarized in <xref ref-type="fig" rid="fig7">Figure 7</xref>.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.00425.014</object-id><label>Figure 7.</label><caption><title>Summary of action plan decoding in the human brain for hand and tool movements.</title><p>Pattern classification revealed a wide range of activity profiles across motor and sensory cortices within networks implicated in hand actions, tool understanding, and perception. Some regions (SPOC and EBA) coded planned actions with the hand but not the tool (areas in red). Some regions (SMG and MTG) coded planned actions with the tool but not the hand (areas in blue). Other regions (aIPS and M1) coded planned actions with both effectors (areas in pink) but did so using different neural representations. A final set of brain areas (pIPS, PMd and PMv) instead coded the final type of action to be performed with invariance as to whether the hand or tool was to be used (areas in purple).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00425.014">http://dx.doi.org/10.7554/eLife.00425.014</ext-link></p></caption><graphic xlink:href="elife-00425-fig7-v1.tif"/></fig></p><p>We examined whether the qualitative differences in decoding accuracies between the three pairwise comparisons within each region (i.e., within-hand decoding, within-tool decoding and across-effector decoding) reached statistical significance. We reasoned that a brain area involved in coding the hand, for example, might show significantly higher decoding accuracies for actions planned with the hand vs tool. A 13 (number of ROIs) × 3 (number of pairwise comparisons per ROI) repeated-measures ANOVA (rmANOVA) of the plan-epoch decoding accuracies revealed a strong trend towards a significant interaction even in a relatively low-powered omnibus test (<italic>F</italic><sub>5.701</sub> = 2.170, p=0.069, Greenhouse-Geisser [GG] corrected), suggesting differences in the patterns of decoding across regions. Further investigation of the decoding accuracies within each area (using GG-corrected rmANOVAs and False Discovery Rate [FDR]-corrected follow-up paired sample <italic>t</italic>-tests) revealed only a few significant effects: in L-SPOC, decoding accuracies for the hand were significantly higher than for the tool and for across-effector decoding (both at p&lt;0.05; <italic>F</italic><sub>1.538</sub> = 6.084, p=0.014); in L-SMG, decoding accuracies for the tool were significantly higher than for the hand and for across-effector decoding (both at p&lt;0.05; <italic>F</italic><sub>1.959</sub> = 10.016, p&lt;0.001); in L-motor cortex, decoding accuracies for the hand were significantly higher than across-effector decoding (p&lt;0.05; <italic>F</italic><sub>1.398</sub> = 6.239, p=0.016); and lastly, in L-pMTG, decoding accuracies for the tool were significantly higher than for the hand (p=0.049; <italic>F</italic><sub>1.968</sub> = 4.171, p=0.037) (note that in L-EBA, although decoding accuracies for the hand showed a trend to be higher than for the tool, this did not reach significance; p=0.106; <italic>F</italic><sub>1.370</sub> = 3.635, p=0.078). Taken together, these analyses suggest tool-specific decoding in SMG and pMTG and hand-specific decoding in SPOC and EBA.</p></sec><sec id="s2-3"><title>Voxel weight analyses</title><p>To further examine the underlying patterns of activity that led to accurate decoding and cross-decoding we investigated the voxel weights assigned by the classifier (where the direction of the weight indicates the relationship of the voxel with the class label, as learned by the classifier; see also the caption for <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). In particular, we looked for correspondence in the voxel weights across pair-wise comparisons within single subjects as a potential explanation for why the spatial activity patterns in certain areas might show across-effector decoding (the data from two representative subjects is shown in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>; see also <xref ref-type="bibr" rid="bib36">Formisano et al., 2008</xref> for a similar approach). That is, if the exact same population of voxels were responsible for driving the observed across-effector classification effects than this same voxel set might be consistently biased towards coding one type of action vs the other (i.e., grasping or reaching) for both effectors (hand and tool). (Note that because our pattern classification analysis was performed on non-Talairached data [MVPA was in fact performed on single-subject ACPC-aligned data], comparing the weights across subjects on a single cortical surface was not feasible).</p><p>Visual inspection of the voxel weightings failed to reveal any structured or consistent topography within or across subjects (for similar results, see also <xref ref-type="bibr" rid="bib47">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>). That is, while the weightings of some voxels appeared to be consistent across pair-wise comparisons within a subject, others appeared to shift their weighting depending on the effector to be used in the movement. (Note that the only consistency observed was that voxels coding for one particular type of action [as indicated by the positive or negative direction of the weight] tended to spatially cluster [which is sensible given the spatial blurring of the hemodynamic response; see <xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref> for a further discussion of this issue]). One possible explanation for the anisotropies observed in the voxel weight distributions across pair-wise comparisons is that they relate to the fact that the decoding accuracies reported here, while statistically significant, are generally quite low (means across participants ∼55%). This indicates some appreciable level of noise in the measured planning-related signals, which, given the highly cognitive nature of planning and related processes, likely reflects a wide range of endogenous factors that can vary throughout the course of an entire experiment (e.g., focus, motivation, mood, etc.). Indeed, even when considering the planning-related activity of several frontoparietal structures at the single-neuron level, responses from trial to trial can show considerable variability (e.g., <xref ref-type="bibr" rid="bib98">Snyder et al., 1997</xref>; <xref ref-type="bibr" rid="bib51">Hoshi and Tanji, 2006</xref>). When extrapolating these neurophysiological characteristics to the far coarser spatial resolution measured with fMRI, it is therefore perhaps to be expected that this type of variability should also be reflected in the decoding accuracies generated from single-trial classification. With regards to the resulting voxel weights assigned by the trained SVM pattern classifiers, it should be noted that even in cases where brain decoding is quite robust (e.g., ∼80% for orientation gratings in V1–V4), the spatial arrangement of voxel weights still tends to show considerable local variability both within and across subjects (e.g., <xref ref-type="bibr" rid="bib60">Kamitani and Tong, 2005</xref>; <xref ref-type="bibr" rid="bib47">Harrison and Tong, 2009</xref>).</p></sec><sec id="s2-4"><title>Control findings in auditory cortex</title><p>One alternative explanation to account for the accurate across-effector classification findings reported may be that our frontoparietal cortex results arise not because of the coding of effector-invariant movement goals (grasp vs reach actions) but instead simply because grasp vs reach movements for both hand and tool trials are cued according to the same ‘Grasp’ and ‘Reach’ auditory instructions. In other words, the cross-decoding observed in PPC and premotor cortex regions might only reflect the selective processing of the auditory commands common to Hand-G and Tool-G (‘Grasp’) and Hand-R and Tool-R (‘Touch’) trials and actually have nothing to do with the mutual upcoming goals of the object-directed movement. If this were the case, then we would expect to observe significant across-effector classification in primary auditory cortex (Heschl’s gyrus) for the same time-points as that found for PPC (pIPS and midIPS) and premotor (PMd and PMv) cortex. We directly tested for this possibility in our data by separately localizing left Heschl’s gyrus in each subject with the same contrast used to define the sensorimotor frontoparietal network, [Plan &amp; Execute &gt; 2*Preview] (recall that auditory cues initiate the onset of the Plan and Execute phases of the trial and so this was a robust contrast for localizing primary auditory cortex). We found that although accurate across-effector classification does indeed arise in Heschl’s gyrus during the trial, it does so distinctly earlier in the Plan-phase compared to that of the frontoparietal areas (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). This observation is consistent with the noticeably transient percentage signal change response that accompanies the auditory instructions delivered to participants at the beginning of the Plan-phase (see time-course in <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>), as compared to the more sustained planning-related responses that emerge throughout the entire frontoparietal network (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The temporal disconnect between the cross-decoding found in Heschl’s gyrus (which emerges in the fourth volume of the Plan-phase) and frontoparietal cortex (which generally emerges in the fifth-sixth volumes of the Plan-phase) makes it unlikely that the effector-invariant nature of the responses revealed in PPC and premotor cortex can be fully attributable to simple auditory commonalities in the planning cues.</p></sec><sec id="s2-5"><title>Limitations of interpretation</title><p>It is worth emphasizing that while accurate decoding in a region points to underlying differences in the neural representations associated with different experimental conditions (e.g., for reviews see <xref ref-type="bibr" rid="bib49">Haynes and Rees, 2006</xref>; <xref ref-type="bibr" rid="bib62">Kriegeskorte, 2011</xref>; <xref ref-type="bibr" rid="bib77">Naselaris et al., 2011</xref>; <xref ref-type="bibr" rid="bib78">Norman et al., 2006</xref>), a lack of decoding or ‘null effect’ (i.e., 50% chance classification) can either reflect that the region 1) is not recruited for the conditions being compared, 2) contains neural/pattern differences between the conditions but which cannot be discriminated by the pattern classification algorithm employed (i.e., a limit of methodology, see <xref ref-type="bibr" rid="bib85">Pereira et al., 2009</xref>; <xref ref-type="bibr" rid="bib84">Pereira and Botvinick, 2011</xref>), or 3) is similarly (but non-discriminately) engaged in those conditions. With respect to the first possibility, given that we selected frontoparietal cortex ROIs based on their involvement in the motor task at the single-subject level (using the contrast of [Plan &amp; Execute &gt; Preview] across all conditions), it is reasonable to assume that all the localized areas are in some way engaged in movement generation. (Note that this general assumption is confirmed by the higher-than-baseline levels of activity observed in the signal amplitude responses during the Plan- and Execute-phases of the trial in areas of frontoparietal cortex [<xref ref-type="fig" rid="fig2 fig5">Figures 2 and 5</xref>] and that this even appears to be the case in the independently localizer-defined lateral occipitotemporal areas, EBA and pMTG [<xref ref-type="fig" rid="fig6">Figure 6</xref>]). Although it is understandably difficult to rule out the second possibility (i.e., that voxel pattern differences exist but are not detected with the SVM classifiers), it is worth noting that we do in fact observe null-effects with the classifiers in several regions where they are to be expected. For instance, SS-cortex is widely considered to be a lower-level sensory structure and thus anticipated to only show discrimination related to the motor task once the hand’s mechanoreceptors have been stimulated at object contact (either through the hand directly or through the tool, indirectly). Accordingly, here we find that SS-cortex activity only discriminates between grasp vs reach movements following movement onset (i.e., during the Execute phase of the trial). Likewise, in motor cortex we show decoding for upcoming hand- and tool-related actions but, importantly, find no resulting across-effector classification. This latter result is highly consistent with the coding of differences in the hand kinematics required to operate the tool vs hand alone and accords with the presumed role of motor cortex in generating muscle-related activity (<xref ref-type="bibr" rid="bib59">Kalaska, 2009</xref>; <xref ref-type="bibr" rid="bib18">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib66">Lillicrap and Scott, 2013</xref>). These findings in SS-cortex and motor cortex, when combined with the wide-range of decoding profiles found in other areas (i.e., from the hand-selective activity patterns in SPOC and EBA at one extreme, to the tool-selective activity patterns in SMG and pMTG at the other, for summary see <xref ref-type="fig" rid="fig7">Figure 7</xref>), suggest that the failure of some areas to decode information related to either hand- or tool-related trials (but not those of the other effector) is closely linked to an invariance in the representations of those particular conditions. (To the extent that in cases where the activity of an area fails to discriminate between experimental conditions it can be said that the area is therefore not involved in coding [or invariant to] those particular conditions, we further expand upon interpretations related to these types of null effects in the ‘Discussion’ section.)</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Behavioral, neuropsychological and neurophysiological evidence demonstrates that a central and governing feature of movement planning, and indeed of higher-level cognition in general, is the linking together of overarching action goals with the precise underlying kinematics required by the body to achieve those goals (<xref ref-type="bibr" rid="bib46">Haaland et al., 2000</xref>; <xref ref-type="bibr" rid="bib1">Andersen and Buneo, 2002</xref>; <xref ref-type="bibr" rid="bib35">Fogassi et al., 2005</xref>; <xref ref-type="bibr" rid="bib44">Grafton and Hamilton, 2007</xref>; <xref ref-type="bibr" rid="bib101">Umilta et al., 2008</xref>). Exactly how the human brain supports this cognitive capacity, particularly in the everyday example of tool-use, remains poorly understood. Here we manipulated the type of object-directed hand action that was planned (grasping vs reaching) as well as the effector (hand vs tool) used to implement that action. We then employed fMRI MVPA in order to examine whether planned object-directed hand actions were represented in an effector-specific or effector-independent manner in human frontoparietal and occipitotemporal cortex. At the effector-specific level, we found that SPOC and EBA discriminated upcoming hand movements only whereas SMG and pMTG discriminated upcoming tool movements only. Furthermore, anterior parietal (post. aIPS, aIPS, t-aIPS) and motor cortex areas discriminated planned actions for both the hand and tool, but did not cross-decode between the two effectors. At the effector-independent level, in posterior parietal (pIPS and midIPS) and premotor (PMd and PMv) cortex areas, we found that the pre-movement patterns predictive of grasp vs reach actions for the hand also predicted grasp vs reach actions with the tool. Notably, because the tool-effector required very different hand kinematics than when the hand was used alone, this suggests that these brain areas encoded the action performed rather than the specific muscle movements needed to achieve it. Consistent with the transfer of goals for the hand to those of the tool, this finding resonates with embodied theories of tool use whereby through use, tools become incorporated as part of the body schema. Notably, however, in the majority of regions tested we find that neural representations remain linked to either the hand or tool.</p><sec id="s3-1"><title>Representation of the cortical motor hierarchy</title><p>Hierarchical theories of motor control have existed for more than a century (<xref ref-type="bibr" rid="bib54">Jackson, 1889</xref>; <xref ref-type="bibr" rid="bib96">Sherrington, 1906</xref>; <xref ref-type="bibr" rid="bib50">Hebb, 1949</xref>), distinguishing between the various levels of abstraction required for action planning—for example, at the level of muscles, joints, motor kinematics, and movement goals. The present findings provide insights into where different brain regions might be situated within such a hierarchy. For instance, at some lower level along this hierarchy we likely have hand-selective regions like SPOC and EBA and tool-selective regions like SMG and pMTG. Although typically associated with visual-perceptual processing, EBA, like SPOC, has been implicated in coding movements of the hand/arm (<xref ref-type="bibr" rid="bib4">Astafiev et al., 2004</xref>; <xref ref-type="bibr" rid="bib80">Orlov et al., 2010</xref>, although see <xref ref-type="bibr" rid="bib81">Peelen and Downing, 2005</xref>) and the fact that we were unable to decode tool movement plans from these regions suggests that they fail to incorporate tools into the body schema (see also <xref ref-type="bibr" rid="bib38">Gallivan et al., 2009</xref>). SMG and pMTG, in contrast, are typically activated when human subjects view (<xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>; <xref ref-type="bibr" rid="bib83">Peeters et al., 2009</xref>) or pantomime (<xref ref-type="bibr" rid="bib58">Johnson-Frey et al., 2005</xref>) tool-related actions, and damage to these areas creates difficulty in pantomiming or performing tool use actions (<xref ref-type="bibr" rid="bib46">Haaland et al., 2000</xref>). That planning-related signals in SMG and pMTG are able to ‘predict’ real tool actions, as shown here, provides an important extension of these previous findings, demonstrating that these areas also play an important and selective role in generating object-directed tool actions.</p><p>We also found several parietal and frontal brain regions (post. aIPS, aIPS, t-aIPS and motor cortex) that, although able to predict upcoming grasp vs reach movements with both the hand and the tool, did not generalize across the effector (i.e., no across-effector classification). When considering the particular tool used here—where the operating mechanics of the tool were opposite to those of the hand alone—this effector-specific level of action planning is imperative. It provides a coding for the kinematic properties and/or dynamics associated with each effector (<xref ref-type="bibr" rid="bib101">Umilta et al., 2008</xref>; <xref ref-type="bibr" rid="bib55">Jacobs et al., 2010</xref>) as well as the other low-level differences that exist between hand and tool trials (e.g., spatial location of target). These features match the known properties of motor cortex; it provides the largest source of descending motor commands to the spinal neurons that produce hand kinematics (<xref ref-type="bibr" rid="bib88">Porter and Lemon, 1993</xref>) and correspondingly, much of its activity can be accounted for in muscle control terms (see <xref ref-type="bibr" rid="bib59">Kalaska, 2009</xref>; <xref ref-type="bibr" rid="bib18">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib66">Lillicrap and Scott, 2013</xref>). In parietal cortex, aIPS has been strongly implicated in grasp planning and execution (e.g., <xref ref-type="bibr" rid="bib76">Murata et al., 2000</xref>; <xref ref-type="bibr" rid="bib22">Culham et al., 2003</xref>). Notably, it has also been implicated in tool use (<xref ref-type="bibr" rid="bib38">Gallivan et al., 2009</xref>; <xref ref-type="bibr" rid="bib55">Jacobs et al., 2010</xref>), but to date, its precise role in tool-related behaviour has remained unclear. The current findings provide two important clarifications with respect to this previous work. First, the anterior IPS is recruited in the planning of tool actions in addition to those of the hand, suggestive of an important role in preparing actions with both effectors. Second, this pattern of findings on its own does not demonstrate that hand and tool actions rely on the same underlying representations, as previously interpreted (e.g., <xref ref-type="bibr" rid="bib91">Rijntjes et al., 1999</xref>; <xref ref-type="bibr" rid="bib14">Castiello et al., 2000</xref>). Rather, as indicated by our cross-classification findings, the representations may differ, perhaps depending on the specifics of the kinematics or object-effector interactions.</p><p>At higher-levels within this hierarchy, we also found several areas (pIPS, midIPS, PMd and PMv) that not only discriminated movement plans for the hand and tool, but moreover, did so using a shared neural code. In the human and macaque monkey, the posterior IPS appears to serve a variety of high-level visual-motor- and cognitive-related functions, such as integrating target- and effector-related information for movement (<xref ref-type="bibr" rid="bib1">Andersen and Buneo, 2002</xref>) and encoding 3D features of objects for hand actions (<xref ref-type="bibr" rid="bib95">Sakata et al., 1998</xref>). One possibility, in line with this previous work, is that effector-independent responses in these areas emerge due to a common coding of object features that are more relevant for grasping than reaching. That is, the same set of object features pertinent for grasping with the hand (object contact points, orientation, distribution of mass, etc.) are pertinent for grasping with the tool and a coding of these features may explain why pattern classifiers trained on hand trials can decode actions performed on tool trials (and vice versa). We also found evidence for these same types of effector-independent representations in premotor areas, PMd and PMv. Each area is engaged in hand actions in both the monkey (<xref ref-type="bibr" rid="bib92">Rizzolatti and Luppino, 2001</xref>; <xref ref-type="bibr" rid="bib89">Raos et al., 2004</xref>, <xref ref-type="bibr" rid="bib90">2006</xref>) and human (<xref ref-type="bibr" rid="bib26">Davare et al., 2006</xref>; <xref ref-type="bibr" rid="bib39">Gallivan et al., 2011</xref>) and their implication in higher-level goal-related processing (<xref ref-type="bibr" rid="bib92">Rizzolatti and Luppino, 2001</xref>; <xref ref-type="bibr" rid="bib20">Cisek et al., 2003</xref>), particularly in the case of tool use with PMv (<xref ref-type="bibr" rid="bib101">Umilta et al., 2008</xref>), strongly resonates with the findings reported here.</p></sec><sec id="s3-2"><title>Linking perception and action through tool use</title><p>The focus of the present work was to reveal, at the level of the actor, how tool use is planned and implemented in the human brain. In addition to providing insights into how action-centred behavior is cortically represented (discussed above) these findings offer a new lens through which to view findings reported from previous observation-based fMRI studies. To date, nearly all fMRI studies examining action-centred coding have done so by adopting tasks that require the observation of others’ actions (<xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>; <xref ref-type="bibr" rid="bib44">Grafton and Hamilton, 2007</xref>; <xref ref-type="bibr" rid="bib83">Peeters et al., 2009</xref>; <xref ref-type="bibr" rid="bib102">Valyear and Culham, 2010</xref>), in which most commonly, 2D static images or movies of action-related behaviors or tool use are passively viewed by participants (<xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>; <xref ref-type="bibr" rid="bib44">Grafton and Hamilton, 2007</xref>; <xref ref-type="bibr" rid="bib83">Peeters et al., 2009</xref>; <xref ref-type="bibr" rid="bib102">Valyear and Culham, 2010</xref>). Notably, the aim of many of these previous investigations has not necessarily been to reveal how the brain plans and executes different actions per se, but instead, to reveal how the brain understands the goals and intentions of an observed actor. This particular line of research has been primarily motivated by the discovery of ‘mirror-neurons’ in the monkey (<xref ref-type="bibr" rid="bib93">Rizzolatti et al., 2001</xref>), located in inferior parietal and ventral premotor cortex (<xref ref-type="bibr" rid="bib35">Fogassi et al., 2005</xref>; <xref ref-type="bibr" rid="bib101">Umilta et al., 2008</xref>), which discharge both when the monkey performs a motor act and when the monkey views the same act performed by another individual. When embedded within this larger context, however, it becomes important to not just understand how the actions of other individuals are represented but also how these perceptual representations may relate to the coding of self-generated motor actions. With respect to the latter, past fMRI research has largely left open the question of how goal-directed movements, particularly in the case of tool use, are cortically represented.</p><p>Here we provide compelling evidence for a strong coupling between the categorical-selectivity of a brain region, as defined through visual-perceptual processing, and its specific role in behavior, as defined through movement planning. For instance, in occipitotemporal cortex we found that the preparatory activity in the independently localized body-selective EBA and the tool-selective pMTG decoded movement plans for hand and tool actions, respectively. This indicates that, similar to the highly modular nature of visual-perceptual processing in occipitotemporal cortex (<xref ref-type="bibr" rid="bib30">Downing et al., 2006</xref>; <xref ref-type="bibr" rid="bib61">Kanwisher, 2010</xref>), hand- and tool-related actions at certain cortical processing levels may also recruit distinct neural populations. As an interesting departure from these occipitotemporal cortex results, we found that we could decode upcoming movements for ‘both’ the hand and tool from the independently defined tool-selective t-aIPS. At the functional level, the decoding of tool actions in t-aIPS is entirely congruent with its activation in observation-based tool-related tasks (for reviews, see <xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>; <xref ref-type="bibr" rid="bib37">Frey, 2007</xref>) and, at the anatomical level, the decoding of hand actions in t-aIPS accords with its close proximity to parietal areas involved in hand preshaping and manipulation (<xref ref-type="bibr" rid="bib22">Culham et al., 2003</xref>; <xref ref-type="bibr" rid="bib103">Valyear et al., 2007</xref>; <xref ref-type="bibr" rid="bib39">Gallivan et al., 2011</xref>). When compared to the findings in occipitotemporal cortex, this result indicates that hand and tool movement planning may only begin recruiting similar neural structures at the level of parietal cortex.</p><p>The decoding of planned hand- and tool-related actions in EBA and pMTG, respectively, raises important questions as to what exactly is being represented in these two occipitotemporal cortex regions. Although others have shown that hand/arm movements can activate different regions in occipitotemporal cortex (<xref ref-type="bibr" rid="bib4">Astafiev et al., 2004</xref>; <xref ref-type="bibr" rid="bib33">Filimon et al., 2009</xref>; <xref ref-type="bibr" rid="bib15">Cavina-Pratesi et al., 2010</xref>; <xref ref-type="bibr" rid="bib79">Oosterhof et al., 2010</xref>; <xref ref-type="bibr" rid="bib80">Orlov et al., 2010</xref>), here we demonstrate that these signals reflect the ‘intention’ to perform a motor act rather than the sensory feedback responses (visual, proprioceptive, tactile) that accompany it. This distinction is important because it indicates that these occipitotemporal cortex areas may play a significant role in action planning and control, possibly by predicting the sensory consequences of actions/movement even before those consequences unfold. Given the delay of incoming sensory signals, this type of forward-state estimation is featured prominently in models of action control (<xref ref-type="bibr" rid="bib108">Wolpert and Ghahramani, 2000</xref>; <xref ref-type="bibr" rid="bib107">Wolpert and Flanagan, 2001</xref>) and, from the standpoint of perception, predicting the sensory consequences of movement can be used to disambiguate movements of the body (self) vs movements of the world (others) (<xref ref-type="bibr" rid="bib106">von Helmohltz, 1866</xref>). The current findings would suggest that such forward-state estimations, at least at the level of occipitotemporal cortex, remain linked to the type of effector (hand vs tool) to be used in an upcoming movement. Updating the considerably simpler notion that action planning, particularly in the case of tool use, merely involves ‘access’ to ventral stream resources (<xref ref-type="bibr" rid="bib74">Milner and Goodale, 1995</xref>; <xref ref-type="bibr" rid="bib102">Valyear and Culham, 2010</xref>), these findings show that hand- and tool-related action plans can actually be decoded from preparatory signals in body- and tool-selective occipitotemporal cortex areas.</p><p>In addition to suggesting a role for OTC in visual-motor planning, these findings might also shed light on the organizing principles of the ventral visual stream. Several theories have been proposed to account for the categorical-selectivity of responses throughout OTC (e.g., for faces, scenes, bodies, tools, etc), with the majority arguing that this modular arrangement arises due to similarities/differences in the visual structure of the world and/or how it is experienced (<xref ref-type="bibr" rid="bib61">Kanwisher, 2010</xref>). For example, according to one prominent view, faces and scenes activate different regions of OTC due to underlying visual field preferences (i.e., faces activate areas with stronger foveal representations, like FFA, whereas scenes activate areas with stronger peripheral representations, like PPA; <xref ref-type="bibr" rid="bib63">Levy et al., 2001</xref>). According to another well-known view, it is instead similarity in visual shape/form that is mapped onto ventral temporal cortex (<xref ref-type="bibr" rid="bib48">Haxby et al., 2001</xref>). One particularly compelling alternative view, however, argues that the organization of OTC may be largely invariant to bottom-up visual properties and that it instead emerges as a by-product of the distinct connectivity patterns of OTC areas with the rest of the brain, particularly the downstream motor structures that use the visual information processed in OTC to plan movements of the body (<xref ref-type="bibr" rid="bib69">Mahon et al., 2007</xref>; <xref ref-type="bibr" rid="bib67">Mahon and Caramazza, 2009</xref>, <xref ref-type="bibr" rid="bib68">2011</xref>). Under this view, the neural specificity frequently observed for the visual presentation of body parts and/or tools in particular regions of OTC may reflect, to a certain extent, their anatomical connectivity with frontoparietal areas involved in generating movements of the body and/or interacting with and manipulating tools, respectively—a notion that garners some empirical support from the ‘downstream’ functional connectivity patterns of areas involved in body part- and tool-related processing (<xref ref-type="bibr" rid="bib69">Mahon et al., 2007</xref>; <xref ref-type="bibr" rid="bib11">Bracci et al., 2012</xref>). Assuming the sharing of action-related information within functionally interconnected circuits, this conceptual framework might help explain the matching object-selective and planning-related responses observed here within both EBA and pMTG. This compatibility of visual- and motor-related responses within single brain areas resonates with neurophysiological findings in macaque parietal cortex showing that the visual-response selectivity of neurons in AIP (for size, shape, orientation, etc.) are often matched to their motor-response selectivity during action (e.g., <xref ref-type="bibr" rid="bib76">Murata et al., 2000</xref>). This coupling is thought to mediate the transformation of visual information regarding physical object properties into corresponding motor programs for grasping or use (<xref ref-type="bibr" rid="bib56">Jeannerod et al., 1995</xref>; <xref ref-type="bibr" rid="bib92">Rizzolatti and Luppino, 2001</xref>) and resonates with the broader concept of motor affordances, whereby the properties of objects linked to action are automatically represented in movement-related areas of the brain (<xref ref-type="bibr" rid="bib21">Cisek, 2007</xref>; <xref ref-type="bibr" rid="bib19">Cisek and Kalaska, 2010</xref>). Where exactly the current findings fit within the context of these broader frameworks remains unclear, nevertheless, our results provide novel evidence suggesting that the specificity of visual object categorical responses in OTC are in some way linked to a specific role in preparing related motor behaviors.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>Thirteen right-handed volunteers participated in the Motor experiment (seven females; mean age: 25.7 years, age range: 20–33 years) and were recruited from the University of Western Ontario (London, Ontario, Canada). Eight of these same participants (four females) participated in a second Localizer experiment. All subjects had normal or corrected-to-normal vision and were financially compensated for their participation. Informed consent and consent to publish was obtained in accordance with ethical standards set out by the Declaration of Helsinki (1964) and with procedures approved by the University of Western Ontario’s Health Sciences Research Ethics Board (ethics review number: 13507). Subjects were naive with respect to hypothesis testing.</p></sec><sec id="s4-2"><title>Motor experiment</title><sec id="s4-2-1"><title>Setup and apparatus</title><p>Each subject’s workspace consisted of a black platform placed over the waist and tilted away from the horizontal at an angle (∼15°) to maximize comfort and target visibility. To facilitate direct viewing of the workspace, we also tilted the head coil (∼20°) and used foam cushions to give an approximate overall head tilt of 30° (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Participants planned and performed individual movements with their hand or a tool (reverse tongs) towards a single centrally located object when required (use of the hand and tool were alternated across experimental runs). To minimize limb-related artifacts, participants had the right upper arm braced, limiting movement to the elbow, creating an arc of reachability (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The target object was made of white LEGO pieces (length: 7 cm × depth: 3 cm × height: 3 cm) and was secured to the workspace at one of two locations along the arc of reachability for the effector (hand or tool) to be used during each experimental run. The exact placement of the target object for hand and tool trials on the platform was adjusted to match each participant’s arm/tool length such that all required movements were comfortable. To mark the object location for hand runs, the target object was placed within reach by the participant’s right hand at a central position on the platform in line with the point of fixation and oriented to maximize the comfort for hand grasping. To mark the object location for tool runs, the target object was placed within reach of the tool by the participant at a further central position, in line with the point of fixation and with the same orientation as that used for the hand. Once marked and prior to initiation of each run type (Hand or Tool), the target object was secured to the platform at one of these two corresponding locations (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>During the experiment, the target object was illuminated from the front by a bright white Light Emitting Diode (LED) attached to flexible plastic stalks (Loc-Line; Lockwood Products, Lake Oswego, OR). During participant setup, the illuminator LED was positioned so as to equally illuminate both the hand and tool locations of the target object. Experimental timing and lighting were controlled with in-house software created with MATLAB (The Mathworks, Natick, MA). To control for eye movements, a small green fixation LED was placed above and at a further depth location than the pre-specified tool target position and subjects were required to always foveate the fixation LED during scanning. Eye fixation and arm movements were examined off-line from videos recorded using an MR-compatible infrared-sensitive camera (MRC Systems GmbH, Heidelberg, Germany) positioned underneath the fixation LED and directed toward the subject’s eyes and hand.</p><p>For each trial, subjects were required to perform one of two actions upon the target object, following a delay period: 1) reach towards and precision grasp (G) the object (‘Grasp’ auditory command) without lifting and 2) reach towards (R) and manually touch the top of the object (‘Touch’ auditory command). For experimental runs with the hand, grasping required the subjects to precision grasp the object with their thumb and index finger (Hand-G) without lifting whereas the reaching action required the subject to simply transport their hand to the object without hand pre-shaping (Hand-R). For experimental runs with the tool, grasping required the subjects to precision grasp the object with the set of reverse tongs without lifting, which involved squeezing the handle of the tongs (in order to initially open the distal ends of the tongs) and then subsequently releasing pressure on the handle (in order to close the distal ends of the tongs onto the object; Tool-G, ‘Grasp’ auditory command). Reaching actions with the reverse tongs simply required the subject to transport the tool to the object without any manipulation, and touch the top of the target (Tool-R; ‘Touch’ auditory command). Participants were instructed to keep the timing of movements for grasping and reaching trials as similar as possible. Other than the execution of these hand and tool actions, the hand throughout all other phases of the trial (Preview phase, Plan phase and ITI) was to remain still and in a relaxed ‘home’ position on the right surface of the platform. For each participant the home/starting position was marked with a small elevation of black tape and subjects were required to always return to this same position following execution of the instructed movement. For experimental runs with the hand, the required home position of the hand was a relaxed fist, and for experimental runs with the tool, the required home position was to have the thumb and index finger gently placed on the handle of the tool (without applying pressure). Importantly, within each experimental run, the target object never changed its centrally located position, thus eliminating retinal differences within the workspace of each effector across trials. Critically, however, this manipulation allowed us to maintain large retinal differences (i.e., position of the object with respect to fixation) and somatosensory differences (presence or absence of the tool in hand) between hand and tool runs. Although including hand and tool trials within the same run would have enabled direct statistical comparisons between them, this would have necessitated insertion and removal of the tool during experimental testing, possibly leading to additional movement artifacts. In between runs, the tool was given to and removed from the subject by the experimenter.</p><p>We chose a reverse set of tongs as the tool to be used in this experiment because it provided an opposite mapping between the proximal movements of the hand and the distal movements of the tool (i.e., when the hand closed on the reverse tongs, the end of the tongs opened, and vice versa). This incongruence was imperative to the aims of the study (i.e., decoding planned actions independent of the specific muscle activations required) because it allowed the object-directed motor plans for both effectors (hand and tool) to be held constant across the experiment (i.e., grasping or reaching), while at the same time, uncoupling the lower-level hand kinematics required to operate each effector. In contrast, when a normal set of tongs are used, the distal ends of the tool exactly mirror the movements made by the hand (i.e., when the hand closes on the tongs, the distal ends of the tongs would also close), and if we had used this type of tool instead, it would have made it difficult to rule out that any tool-related decoding was independent of the planned hand movements required to operate the tool (See also <xref ref-type="bibr" rid="bib101">Umilta et al., 2008</xref>).</p></sec></sec><sec id="s4-3"><title>Experiment design and timing</title><p>To extract the visual-motor planning response for the hand and tool from the simple visual and motor execution responses, we used a slow event-related planning paradigm with 34 s trials, each consisting of three distinct phases: ‘Preview’, ‘Plan’ and ‘Execute’ (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). We adapted this paradigm from previous fMRI work with eye- and arm-movements that have successfully isolated delay period activity from the transient neural responses following the onset of visual input and movement execution (<xref ref-type="bibr" rid="bib25">Curtis et al., 2004</xref>; <xref ref-type="bibr" rid="bib9">Beurze et al., 2007</xref>, <xref ref-type="bibr" rid="bib10">2009</xref>; <xref ref-type="bibr" rid="bib86">Pertzov et al., 2011</xref>) and from other previous studies from our lab in which we successfully used the spatial voxel patterns of delay period responses in order to show that different upcoming movements can be accurately predicted (<xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>; <xref ref-type="bibr" rid="bib40">2011b</xref>).</p><p>In our task, each trial began with the Preview phase, where the subject’s workspace was illuminated revealing the centrally located target object. After 6 s of the Preview phase, subjects were given an auditory cue (0.5 s), either ‘Grasp’ or ‘Touch’, informing them of the upcoming movement required; this cue marked the onset of the Plan phase. Although there were no visual differences between the Preview and Plan phase portions of the trial (i.e., the single object was always visually present), only in the Plan phase did participants have the necessary motor information in order to prepare the upcoming movement. After 12 s of the Plan phase, a 0.5-s auditory beep cued participants to immediately execute the planned action, initiating the Execute phase of the trial. 2 s following the beginning of this Go cue, the illuminator was turned off, providing the cue for subjects (during both hand and tool runs) to return the hand to its peripheral starting position. After the illuminator was extinguished, subjects then waited in the dark while maintaining fixation for 14 s, allowing the BOLD response to return to baseline prior to the next trial (ITI phase). The two trial types (grasp or reach), with ten repetitions per condition (20 trials total) were randomized within a run and balanced across all runs (that required the same effector) so that each trial type was preceded and followed equally often by every other trial type across the entire experiment.</p><p>Separate practice sessions were carried out before the actual experiment to familiarize participants with both the mechanics of the reverse tool and the timing of the paradigm, where in particular, the delay timing required the cued action to be performed only at the beep (Go) cue. These sessions were carried out before the subjects entered the scanner as well as during the anatomical scan (collected at the beginning of every experiment). A testing session for each participant included set-up time (∼45 min), 8 functional runs (although two subjects participated in 6 and 10 functional runs, respectively) and 1 anatomical scan, and lasted approximately 3 hr. Throughout the experiment, the subject’s fixation and hand movements were monitored using an MR-compatible infrared-sensitive camera optimally positioned directly below the fixation point and facing towards the subject. The videos captured during the experiment were analyzed off-line to verify that the subjects were indeed performing the task as instructed. A more rigorous tracking of the eyes was not performed because our eye-tracking system does not work while the head is tilted due to a partial occlusion from the eyelids.</p></sec><sec id="s4-4"><title>Localizer experiment</title><sec id="s4-4-1"><title>Setup and apparatus</title><p>Each of the localizer runs included stimulus blocks of color photos projected onto a 2D screen consisting of familiar tools (87 different identities), headless bodies (87 different identities; 44 were females), non-tool objects (87 different identities) and scrambled versions of these same stimuli. Each run lasted 7 min 30 s and was composed of six stimulus epochs per condition, seven scrambled epochs, and two fixation/baseline epochs (20 s) placed at the beginning and end of each run (see <xref ref-type="fig" rid="fig5">Figure 5</xref> for a protocol of one of the localizer runs). Stimulus epochs were organized into sets of three, separated by scrambled epochs, balanced for epoch history within a single run (photos were repeated across runs). To provide a fixation point, a small black circle was superimposed at the center of each image. Each image subtended 15° of the subject’s visual field. Stimuli were organized into separate 16 s epochs, with 18 photos per epoch, presented at a rate of 400 ms per photo with a 490-ms inter-stimulus interval. To encourage that participants maintained attention throughout the localizer scans, subjects performed a one-back task throughout, whereby responses were made, via a right-handed button press, whenever two successive photos were identical. In addition to running this localizer, we also collected a high-resolution anatomical image from each of the participating subjects. All subjects participated in at least three of these localizer runs.</p></sec></sec><sec id="s4-5"><title>Motor and Localizer experiments</title><sec id="s4-5-1"><title>MRI acquisition and preprocessing</title><p>Subjects were scanned using a 3-Tesla Siemens TIM MAGNETOM Trio MRI scanner. The T1-weighted anatomical image was collected using an ADNI MPRAGE sequence (TR = 2300 ms, TE = 2.98 ms, field of view = 192 mm × 240 mm × 256 mm, matrix size = 192 × 240 × 256, flip angle = 9°, 1-mm isotropic voxels). Functional MRI volumes were collected using a T2*-weighted single-shot gradient-echo echo-planar imaging (EPI) acquisition sequence (time to repetition [TR] = 2000 ms, slice thickness = 3 mm, in-plane resolution = 3 mm × 3 mm, time to echo [TE] = 30 ms, field of view = 240 mm × 240 mm, matrix size = 80 × 80, flip angle = 90°, and acceleration factor [integrated parallel acquisition technologies, iPAT] = 2 with generalized auto-calibrating partially parallel acquisitions [GRAPPA] reconstruction). Each volume comprised 34 contiguous (no gap) oblique slices acquired at a ∼30° caudal tilt with respect to the plane of the anterior and posterior commissure (AC-PC), providing near whole brain coverage. In the Motor experiment, we used a combination of imaging coils to achieve a good signal:noise ratio and to enable direct viewing without mirrors or occlusion. Specifically, we placed the posterior half of the 12-channel receive-only head coil (6-channels) beneath the head and tilted it at an angle of ∼20° and suspended a 4-channel receive-only flex coil over the forehead. In the Localizer experiment, subjects were scanned using a conventional setup (i.e., stimuli projected onto a 2D screen and viewed with a mirror), with a 32-channel receive-only head coil. Functional data from both the Motor and Localizer experiments were aligned to the high-resolution anatomical collected during the Localizer testing session. We reconstructed the cortical surface from one subject from a high-resolution anatomical image, a procedure that included segmenting the gray and white matter and inflating the surface boundary between them. This inflated cortical surface was used to overlay group activation for figure presentation (for <xref ref-type="fig" rid="fig2">Figure 2</xref> note that voxel activity was spatially interpolated from 3-mm functional iso-voxels to 1-mm functional iso-voxels). All preprocessing and univariate analyses were performed using Brain Voyager QX version 2.12 (Brain Innovation, Maastricht, the Netherlands).</p><p>Following slice scan-time correction, 3D motion correction (such that each volume was aligned to the volume of the functional scan closest in time to the anatomical scan), high-pass temporal filtering (5 cycles per run) and functional-to-anatomical co-registration, functional and anatomical images were rotated such that the axial plane passed through the anterior and posterior commissures (AC-PC space) and then transformed into Talairach space. Other than the sinc interpolation inherent in all transformations, no additional spatial smoothing was performed. Talairach data was only used for group voxelwise Random-effects (RFX) analyses in order to display the pre-defined action-related regions of interest (ROIs). For MVPA, these areas were defined anatomically within each subject’s AC-PC data. Given that MVPA discriminates spatial patterns across voxels, we have found it beneficial to select ROIs at the single-subject level using the AC-PC data in lieu of the Talairach data (<xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>; <xref ref-type="bibr" rid="bib40">2011b</xref>).</p><p>For each participant, functional data from each session were screened for motion and/or magnet artifacts by examining the time-course movies and the motion plots created with the motion correction algorithms. None of the runs revealed head motion that exceeded 1 mm translation or 1° rotation. In the Motor experiment, error trials—trials where the participant fumbled with the object (two trials, two participants), performed the incorrect instruction (one trial, one participant), or contaminated the plan phase data by slightly moving their limb or eyes or by performing the action before the ‘Go’ cue (six trials, three participants)—were identified off-line from the videos recorded during the session and were excluded from analysis by assigning these trials predictors of no interest. This generally low error rate more than likely reflects the fact that subjects were well-trained on the motor task before entering the scanner.</p></sec></sec><sec id="s4-6"><title>Motor experiment</title><sec id="s4-6-1"><title>Regions of interest (ROIs)</title><p>To localize the specific action-related areas in individual subjects in which to implement MVPA, we used a general linear model (GLM) with predictors created from boxcar functions convolved with the Boynton haemodynamic response function (HRF). For each trial, a boxcar function was aligned to the onset of each phase, with a height dependent upon the duration of each phase: i) 3 volumes for the Preview phase, ii) 6 volumes for the Plan phase, and iii) 1 volume for the Execute phase. The ITI was excluded from the model, therefore all regression coeffcients (betas) were defined relative to the baseline activity during the ITI. In addition, the time-course for each voxel was converted to percent signal change before applying the RFX-GLM.</p><p>To specify our pre-defined ROIs and select voxels for MVPA from each subject we searched for brain areas involved in movement generation. To do this, we contrasted activity for movement planning and execution (collapsed over hand, tool, grasp and touch) vs the simple visual response to object presentation, prior to instruction: (Plan &amp; Execute &gt; Preview)—(Plan [Hand-G + Hand-R + Tool-G + Tool-R] + Execute [Hand-G + Hand-R + Tool-G+ Tool-R] &gt;2*Preview [Hand-G + Hand-R + Tool-G + Tool-R]). The resulting statistical map of all positively active voxels in each subject (t = 3, p&lt;0.005, each subject’s activation map was cluster threshold corrected [corrected, p&lt;0.05] so that only voxels passing a minimum cluster size were selected; average minimum cluster size across subjects was 113.5 mm<sup>3</sup>; for details see ‘ROI selection’, below) was then used to define 10 different ROIs within the left (contralateral) hemisphere: 1) Superior parieto-occipital cortex (SPOC), 2) posterior intraparietal sulcus (pIPS), 3) middle IPS (midIPS), 4) posterior anterior IPS (post. aIPS), 5) anterior IPS (aIPS), 6) Supramarginal gyrus (SMG), 7) Somatosensory (SS) cortex, 8) Motor cortex, 9) Dorsal premotor (PMd) cortex, and 10) Ventral premotor (PMv) cortex. Nine of these ROIs were selected based on their well-documented involvement in movement planning/generation. We selected SS-cortex to function as a sensory control region (i.e., known to respond to transient stimuli [i.e., sensory events], but not expected to participate in sustained movement planning/intention-related processes). The voxels included in each ROI were selected based on all significant activity within a cube of (15 mm)<sup>3</sup> = 3375 mm<sup>3</sup> centered on pre-defined anatomical landmarks (see ‘ROI selection’ below for criteria). These ROI sizes were chosen as it not only allowed the inclusion of a sufficient number of functional voxels for pattern classification (an important consideration), but also ensured that a similar number of voxels were included within each ROI and that the regions could be reliably segregated from adjacent activations (for the average number of functional voxels selected across the 13 subjects, see <xref ref-type="table" rid="tbl1">Table 1</xref>). Critically, given the orthogonal contrast employed to select these 10 areas (i.e., Plan &amp; Execute &gt; Preview), their activity is not directionally biased to show any preview-, plan- or execute-related pattern differences ‘between’ any of the experimental conditions.</p></sec><sec id="s4-6-2"><title>ROI selection</title><p>Left Superior parieto-occipital cortex (SPOC)<list list-type="bullet"><list-item><p>Defined by selecting voxels located medially and directly anterior to the Parieto-occipital sulcus (POS) (<xref ref-type="bibr" rid="bib38">Gallivan et al., 2009</xref>).</p></list-item></list></p><p>Left posterior IPS (pIPS)<list list-type="bullet"><list-item><p>Defined by selecting activity at the caudal end of the IPS (<xref ref-type="bibr" rid="bib10">Beurze et al., 2009</xref>).</p></list-item></list></p><p>Left middle IPS (midIPS)<list list-type="bullet"><list-item><p>Defined by selecting voxels half-way up the length of the IPS, centred on the medial bank (<xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>, <xref ref-type="bibr" rid="bib40">2011b</xref>), near a characteristic ‘knob’ landmark observed consistently within each subject.</p></list-item></list></p><p>Left region located posterior to L-aIPS (L-post. aIPS)<list list-type="bullet"><list-item><p>Defined by selecting activity just posterior to the junction of the IPS and Post central sulcus (PCS), on the medial bank of the IPS (<xref ref-type="bibr" rid="bib24">Culham, 2004</xref>).</p></list-item></list></p><p>Left aIPS (L-aIPS)<list list-type="bullet"><list-item><p>Defined by selecting voxels located directly at the junction of the IPS and PCS (<xref ref-type="bibr" rid="bib22">Culham et al., 2003</xref>).</p></list-item></list></p><p>Left Supramarginal gyrus (L-SMG)<list list-type="bullet"><list-item><p>Defined by selecting activity along the supramarginal gyrus, directly lateral to the anterior segment of the IPS (<xref ref-type="bibr" rid="bib65">Lewis, 2006</xref>).</p></list-item></list></p><p>Left somatosensory cortex (L-SS cortex)<list list-type="bullet"><list-item><p>Defined by selecting voxels medial and anterior to the aIPS, encompassing the Post central gyrus and PCS (<xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>, <xref ref-type="bibr" rid="bib40">2011b</xref>).</p></list-item></list></p><p>Left Motor cortex<list list-type="bullet"><list-item><p>Defined by selecting voxels around the left ‘hand knob’ landmark in the Central sulcus (CS) (<xref ref-type="bibr" rid="bib109">Yousry et al., 1997</xref>).</p></list-item></list></p><p>Left Dorsal premotor cortex (PMd)<list list-type="bullet"><list-item><p>Defined by selecting voxels at the junction of the Precentral sulcus (PreCS) and Superior frontal sulcus (SFS) (<xref ref-type="bibr" rid="bib87">Picard and Strick, 2001</xref>).</p></list-item></list></p><p>Left Ventral premotor cortex (PMv)<list list-type="bullet"><list-item><p>Defined by selecting activity inferior and posterior to the junction of the Inferior frontal sulcus (IFS) and PreCS (<xref ref-type="bibr" rid="bib100">Tomassini et al., 2007</xref>).</p></list-item></list></p><p>See <xref ref-type="table" rid="tbl1">Table 1</xref> for details about ROI coordinates and sizes, and <xref ref-type="fig" rid="fig2">Figure 2</xref> for representative anatomical locations on one subject’s inflated brain.</p><p>To provide a control for our cross-decoding analyses in frontoparietal cortex (for motivation, see ‘Results’), we examined time-resolved and plan-epoch decoding in the left primary auditory cortex (Heschl’s gyrus). Using the same contrast and selection criteria as above, this ROI was neuroanatomically defined in each subject by selecting voxels halfway up along the superior temporal sulcus (STS), on the superior temporal gyrus (between the insular cortex and outer-lateral edge of the superior temporal gyrus; see <xref ref-type="bibr" rid="bib73">Meyer et al., 2010</xref>; <xref ref-type="bibr" rid="bib40">Gallivan et al., 2011b</xref>).</p><p>To ensure our decoding accuracies could not result from spurious factors (e.g., task-correlated head or arm movements), we also tested the performance of our classifiers in ROIs outside of our action-related network where no statistically significant classification should be possible. To select these ROIs we further reduced our statistical threshold (after specifying the [Plan &amp; Execute &gt; Preview] network within each subject) down to t = 0, p=1 and selected all positive activation within 3375 mm<sup>3</sup> centered on a consistent point 1) within each subject’s right ventricle and 2) at a location situated just outside the skull of the right hemisphere, in the AC-PC plane and directly in line with the posterior commissure.</p></sec></sec><sec id="s4-7"><title>Localizer experiment</title><sec id="s4-7-1"><title>Regions of interest (ROI)</title><p>To independently localize a specific set of object category-selective ROIs in individual subjects in which to implement MVPA (using the Motor experiment data), we used a GLM with predictor boxcar functions aligned to the onset of each stimulus block for each of the conditions (except for the fixation/baseline epochs) and then convolved the predictors with the Boyton HRF. The time-course for each voxel was converted to percent signal change before applying the GLM.</p><p>For each individual, data from the localizer scans was used to identify three distinct ROIs based on previous neuroimaging studies: t-aIPS, pMTG, and EBA. The following same procedure was used to define the three ROIs in each individual: The most significantly active voxel(s), or peak, was first identified based on a particular contrast, statistical thresholds were then set to a determined minimum (t = 3, p&lt;0.005), and the activity up to 3375 mm<sup>3</sup> around the peak was selected. Tool-selective (t-aIPS and pMTG) areas were localized based on a conjunction contrast of ([Tools &gt; Scrambled] AND [Tools &gt; Bodies] AND [Tools &gt; Objects]) and the Body-selective (EBA) area was localized based on a conjunction contrast of ([Bodies &gt; Scrambled] AND [Bodies &gt; Tools] AND [Bodies &gt; Objects]). Note that we define a conjunction contrast as a Boolean AND, such that for any one voxel to be flagged as significant, it must show a significant difference for each of the constituent contrasts. See <xref ref-type="table" rid="tbl1">Table 1</xref> for details about ROI coordinates and sizes, and <xref ref-type="fig" rid="fig5 fig6">Figures 5 and 6</xref> for representative locations on individual subject’s brains.</p></sec><sec id="s4-7-2"><title>Multi-voxel pattern analysis (MVPA)</title><p>We used the fine-grained sensitivity afforded by MVPA to not only examine if grasp vs reach movement plans with the hand or tool could be decoded from preparatory brain activity (where little or no signal amplitude differences may exist), but more importantly, because it allowed us to question in what areas the higher-level movement goals of an upcoming action were encoded independent of the lower-level kinematics required to implement them. More specifically, by training a pattern classifier to discriminate grasp vs reach movements with one effector (e.g., hand) and then testing whether that same classifier can be used to predict the same trial types with the other effector (e.g., tool), we could assess whether the object-directed action being planned (grasping vs reaching) was being represented with some level of invariance to the effector being used to perform the movement (see ‘Across-effector classification’ below for further details).</p></sec><sec id="s4-7-3"><title>Support vector machine classifiers</title><p>MVPA was performed with a combination of in-house software (using Matlab) and the Princeton MVPA Toolbox for Matlab (<ext-link ext-link-type="uri" xlink:href="http://code.google.com/p/princeton-mvpa-toolbox/">http://code.google.com/p/princeton-mvpa-toolbox/</ext-link>) using a Support Vector Machines (SVM) binary classifier (libSVM, <ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">http://www.csie.ntu.edu.tw/~cjlin/libsvm/</ext-link>). The SVM model used a linear kernel function and default parameters (a fixed regularization parameter C = 1) to compute a hyperplane that best separated the trial responses.</p></sec><sec id="s4-7-4"><title>Inputs to classifier</title><p>To prepare inputs for the pattern classifier, the BOLD percent signal change was computed from the time-course at a time point(s) of interest with respect to the time-course at a common baseline, for all voxels in the ROI. This was done in two fashions. The first, extracted percent signal change values for each time point in the trial (time-resolved decoding). The second, extracted the percent signal change values for a windowed-average of the activity for the 4 s (2 imaging volumes; TR = 2) prior to movement (plan-epoch decoding). For both approaches, the baseline window was defined as volume − 1, a time point prior to initiation of each trial and avoiding contamination from responses associated with the previous trial. For the plan-epoch approach—the time points of critical interest in order to examine whether we could predict upcoming movements (<xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>, <xref ref-type="bibr" rid="bib40">2011b</xref>)—we extracted the average pattern across imaging volumes 8–9 (the final 2 volumes of the Plan phase), corresponding to the sustained activity of the planning response prior to movement (<xref ref-type="fig" rid="fig1 fig2">Figures 1D and 2</xref>). Following the extraction of each trial’s percent signal change, these values were rescaled between −1 and +1 across all trials for each individual voxel within an ROI. Importantly, through the application of both time-dependent approaches, in addition to revealing which types of movements could be decoded, we could also examine specifically when in time predictive information pertaining to specific actions arose.</p></sec><sec id="s4-7-5"><title>Pair-wise discriminations</title><p>SVMs are designed for classifying differences between two stimuli and LibSVM (the SVM package implemented here) uses the so-called ‘one-against-one method’ for each pair-wise discrimination (<xref ref-type="bibr" rid="bib52">Hsu and Lin, 2002</xref>). We capitalized on this fact in order to characterize different brain regions according to the types of upcoming movements they could predict: Hand-G vs Hand-R and/or Tool-G vs Tool-R. We did not examine individual pair-wise discriminations for movements between hand and tool trials (e.g., Hand-G vs Tool-G, Hand-R vs Tool-R) given the fact that in addition to the differences in action planning, substantial visual and somatosensory differences already exist between the two types of trials, like the retinal position of the target object and presence/absence of the tool which both varied between experimental runs. Notably, while these low-level visual and somatosensory differences between experimental runs provided an inherent impediment for interpreting any direct comparisons between hand and tool trials, the presence of these differences significantly aided the interpretation of accurate across-effector classification results. That is, cross-decoding between effectors for the planned action (grasp vs reach) would be unequivocally independent of any visual or somatosensory similarities between the hand and tool runs.</p></sec><sec id="s4-7-6"><title>Single-trial classification</title><p>For each subject and for each of the ten action-related ROIs in the Motor experiment and three perception-related ROIs in the Localizer experiment, separate binary SVM classifiers were estimated for MVPA (i.e., for each pair-wise comparison, Hand-G vs Hand-R and Tool-G vs Tool-R, and for each time point). We used a ‘leave-one-trial-pair-out’ N-fold cross-validation to test the accuracy of the SVM classifiers (i.e., one trial from each of the conditions being compared [two trials total] were reserved for testing the classifier and the remaining [N − 1] trial pairs were used for classifier training). We performed this N − 1 cross-validation procedure until all trial pairs were tested, and then averaged across N-iterations in order to produce a classification accuracy measure for each pair-wise discrimination and subject (<xref ref-type="bibr" rid="bib31">Duda et al., 2001</xref>). We statistically assessed decoding significance with a two-tailed <italic>t</italic>-test vs 50% chance decoding. To control for the problem of multiple comparisons, a false discovery rate (FDR) correction of q ≤ 0.05 was applied based on all <italic>t</italic>-tests performed at each time point within an ROI (<xref ref-type="bibr" rid="bib7">Benjamini and Yekutieli, 2001</xref>).</p><p>Note that the data being used at any single time point (e.g., each TR in the time-resolved decoding approach) are independent as they are full trial-lengths removed from directly adjacent trials (recall that each trial = 34 s), providing more than adequate time for the hemodynamic responses associated with individual TRs used for classifier testing to sufficiently uncouple (this would not necessarily be the case in a rapid event-related design). Furthermore, the trial orders were fully randomized and so any possible correlations between train and test data is not obvious and should not bias the data towards correct vs incorrect classification (<xref ref-type="bibr" rid="bib75">Misaki et al., 2010</xref>).</p></sec><sec id="s4-7-7"><title>Permutation tests</title><p>In addition to the <italic>t</italic>-test, we separately assessed statistical significance with non-parametric randomization tests for the plan-epoch decoding accuracies (<xref ref-type="bibr" rid="bib43">Golland and Fischl, 2003</xref>; <xref ref-type="bibr" rid="bib32">Etzel et al., 2008</xref>; <xref ref-type="bibr" rid="bib97">Smith and Muckli, 2010</xref>; <xref ref-type="bibr" rid="bib17">Chen et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>, <xref ref-type="bibr" rid="bib40">2011b</xref>). For specific details pertaining to this test see our recent work (<xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>, <xref ref-type="bibr" rid="bib40">2011b</xref>, <xref ref-type="bibr" rid="bib41">2013</xref>). In sum, the important finding highlighted from these permutation tests is that the brain areas showing significant decoding with the one sample parametric <italic>t</italic>-tests (vs. 50% at p&lt;0.05) ‘also’ show significant decoding (albeit higher, p&lt;0.001) with the empirical non-parametric permutation tests.</p></sec><sec id="s4-7-8"><title>Across-effector classification</title><p>In order to test whether a SVM pattern classifier trained to discriminate between grasp vs reach trial types with one effector could then be used to accurately decode pattern differences when tested with trials belonging to the other effector (e.g., train set: Hand-G vs Hand-R, test set: Tool-G vs Tool-R), instead of using the N − 1 cross-validation procedure (implemented above) we used all the available single trial data for both classifier training and testing (i.e., one train-and-test iteration; <xref ref-type="bibr" rid="bib97">Smith and Muckli, 2010</xref>; <xref ref-type="bibr" rid="bib39">Gallivan et al., 2011a</xref>). Cross-decoding accuracies for each subject were computed by averaging together the two accuracies generated by using each pair of effector-specific trial types for classifier training and testing (e.g., Hand trials were used to train the classifier in one analysis when Tool trials were used for testing, and then they were used to test the classifier in the other analysis when the Tool trials were used for classifier training). The means across subjects of this cross-decoding procedure are reported in <xref ref-type="fig" rid="fig3 fig4 fig5 fig6">Figures 3–6</xref>. We statistically assessed decoding significance with a two-tailed <italic>t</italic>-test vs 50% chance decoding. A FDR correction of q ≤ 0.05 was applied based on all <italic>t</italic>-tests performed at each time point.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors are grateful to Craig Chapman, Fraser Smith, and Melvyn Goodale for helpful discussions and/or comments on previous versions of the manuscript.</p></ack><sec sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>JCC: Reviewing editor, <italic>eLife</italic>.</p></fn><fn fn-type="conflict" id="conf2"><p>The other authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>JPG, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article, Contributed unpublished essential data or reagents</p></fn><fn fn-type="con" id="con2"><p>DAM, Conception and design, Acquisition of data, Contributed unpublished essential data or reagents</p></fn><fn fn-type="con" id="con3"><p>KFV, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con4"><p>JCC, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All subjects had normal or corrected-to-normal vision and were financially compensated for their participation. Informed consent and consent to publish was obtained in accordance with ethical standards set out by the Declaration of Helsinki (1964) and with procedures approved by the University of Western Ontario’s Health Sciences Research Ethics Board (ethics review number: 13507). Subjects were naive with respect to hypothesis testing.</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Buneo</surname><given-names>CA</given-names></name></person-group><year>2002</year><article-title>Intentional maps in posterior parietal cortex</article-title><source>Ann Rev Neurosci</source><volume>25</volume><fpage>189</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.25.112701.142922</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Cui</surname><given-names>H</given-names></name></person-group><year>2009</year><article-title>Intention, action planning, and decision making in parietal-frontal circuits</article-title><source>Neuron</source><volume>63</volume><fpage>568</fpage><lpage>583</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.08.028</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arbib</surname><given-names>MA</given-names></name><name><surname>Bonaiuto</surname><given-names>JB</given-names></name><name><surname>Jacobs</surname><given-names>S</given-names></name><name><surname>Frey</surname><given-names>SH</given-names></name></person-group><year>2009</year><article-title>Tool use and the distalization of the end-effector</article-title><source>Psychol Res</source><volume>73</volume><fpage>441</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1007/s00426-009-0242-2</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astafiev</surname><given-names>SV</given-names></name><name><surname>Stanley</surname><given-names>CM</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name></person-group><year>2004</year><article-title>Extrastriate body area in human occipital cortex responds to the performance of motor actions</article-title><source>Nat Neurosci</source><volume>7</volume><fpage>542</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nn1241</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year>2007</year><article-title>Grounding object concepts in perception and action: evidence from fMRI studies of tools</article-title><source>Cortex</source><volume>43</volume><fpage>461</fpage><lpage>468</lpage><pub-id pub-id-type="doi">10.1016/S0010-9452(08)70470-2</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Lee</surname><given-names>KE</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year>2002</year><article-title>Parallel visual motion processing streams for manipulable objects and human movements</article-title><source>Neuron</source><volume>34</volume><fpage>149</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00642-6</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Yekutieli</surname><given-names>D</given-names></name></person-group><year>2001</year><article-title>The control of the false discovery rate in multiple testing under dependency</article-title><source>Ann Stat</source><volume>29</volume><fpage>1165</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1214/aos/1013699998</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berti</surname><given-names>A</given-names></name><name><surname>Frassinetti</surname><given-names>F</given-names></name></person-group><year>2000</year><article-title>When far becomes near: remapping of space by tool use</article-title><source>J Cogn Neurosci</source><volume>12</volume><fpage>415</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1162/089892900562237</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beurze</surname><given-names>SM</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name><name><surname>Medendorp</surname><given-names>WP</given-names></name></person-group><year>2007</year><article-title>Integration of target and effector information in the human brain during reach planning</article-title><source>J Neurophysiol</source><volume>97</volume><fpage>188</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1152/jn.00456.2006</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beurze</surname><given-names>SM</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name><name><surname>Medendorp</surname><given-names>WP</given-names></name></person-group><year>2009</year><article-title>Spatial and effector processing in the human parietofrontal network for reaches and saccades</article-title><source>J Neurophysiol</source><volume>101</volume><fpage>3053</fpage><lpage>3062</lpage><pub-id pub-id-type="doi">10.1152/jn.91194.2008</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Cavina-Pratesi</surname><given-names>C</given-names></name><name><surname>Ietswaart</surname><given-names>M</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year>2012</year><article-title>Closely overlapping responses to tools and hands in left lateral occipitotemporal cortex</article-title><source>J Neurophysiol</source><volume>107</volume><fpage>1443</fpage><lpage>1456</lpage><pub-id pub-id-type="doi">10.1152/jn.00619.2011</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardinali</surname><given-names>L</given-names></name><name><surname>Frassinetti</surname><given-names>F</given-names></name><name><surname>Brozzoli</surname><given-names>C</given-names></name><name><surname>Urquizar</surname><given-names>C</given-names></name><name><surname>Roy</surname><given-names>AC</given-names></name><name><surname>Farne</surname><given-names>A</given-names></name></person-group><year>2009</year><article-title>Tool-use induces morphological updating of the body schema</article-title><source>Curr Biol</source><volume>19</volume><fpage>R478</fpage><lpage>R479</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.05.009</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardinali</surname><given-names>L</given-names></name><name><surname>Jacobs</surname><given-names>S</given-names></name><name><surname>Brozzoli</surname><given-names>C</given-names></name><name><surname>Frassinetti</surname><given-names>F</given-names></name><name><surname>Roy</surname><given-names>AC</given-names></name><name><surname>Farne</surname><given-names>A</given-names></name></person-group><year>2012</year><article-title>Grab an object with a tool and change your body: tool-use-dependent changes of body representation for action</article-title><source>Exp Brain Res</source><volume>218</volume><fpage>259</fpage><lpage>271</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castiello</surname><given-names>U</given-names></name><name><surname>Bennett</surname><given-names>KM</given-names></name><name><surname>Egan</surname><given-names>GF</given-names></name><name><surname>Tochon-Danguy</surname><given-names>HJ</given-names></name><name><surname>Kritikos</surname><given-names>A</given-names></name><name><surname>Dunai</surname><given-names>J</given-names></name></person-group><year>2000</year><article-title>Human inferior parietal cortex “programs” the action class of grasping</article-title><source>Cogn Syst Res</source><volume>1</volume><fpage>140</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1016/S1389-0417(99)00011-X</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavina-Pratesi</surname><given-names>C</given-names></name><name><surname>Monaco</surname><given-names>S</given-names></name><name><surname>Fattori</surname><given-names>P</given-names></name><name><surname>Galletti</surname><given-names>C</given-names></name><name><surname>McAdam</surname><given-names>TD</given-names></name><name><surname>Quinlan</surname><given-names>DJ</given-names></name><etal/></person-group><year>2010</year><article-title>Functional magnetic resonance imaging reveals the neural substrates of arm transport and grip formation in reach-to-grasp actions in humans</article-title><source>J Neurosci</source><volume>30</volume><fpage>10306</fpage><lpage>10323</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2023-10.2010</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>LL</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year>2000</year><article-title>Representation of manipulable man-made objects in the dorsal stream</article-title><source>Neuroimage</source><volume>12</volume><fpage>478</fpage><lpage>484</lpage><pub-id pub-id-type="doi">10.1006/nimg.2000.0635</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Namburi</surname><given-names>P</given-names></name><name><surname>Elliott</surname><given-names>LT</given-names></name><name><surname>Heinzle</surname><given-names>J</given-names></name><name><surname>Soon</surname><given-names>CS</given-names></name><name><surname>Chee</surname><given-names>MW</given-names></name><etal/></person-group><year>2011</year><article-title>Cortical surface-based searchlight decoding</article-title><source>Neuroimage</source><volume>56</volume><fpage>582</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.035</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Foster</surname><given-names>JD</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><etal/></person-group><year>2012</year><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><volume>487</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature11129</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name><name><surname>Kalaska</surname><given-names>JF</given-names></name></person-group><year>2010</year><article-title>Neural mechanisms for interacting with a world full of action choices</article-title><source>Annu Rev Neurosci</source><volume>33</volume><fpage>269</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135409</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name><name><surname>Crammond</surname><given-names>DJ</given-names></name><name><surname>Kalaska</surname><given-names>JF</given-names></name></person-group><year>2003</year><article-title>Neural activity in primary motor and dorsal premotor cortex in reaching tasks with the contralateral versus ipsilateral arm</article-title><source>J Neurophysiol</source><volume>89</volume><fpage>922</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1152/jn.00607.2002</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year>2007</year><article-title>Cortical mechanisms of action selection: the affordance competition hypothesis</article-title><source>Phil Transact R Soc Lond B</source><volume>362</volume><fpage>1585</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2054</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Culham</surname><given-names>JC</given-names></name><name><surname>Danckert</surname><given-names>SL</given-names></name><name><surname>DeSouza</surname><given-names>JF</given-names></name><name><surname>Gati</surname><given-names>JS</given-names></name><name><surname>Menon</surname><given-names>RS</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><year>2003</year><article-title>Visually guided grasping produces fMRI activation in dorsal but not ventral stream brain areas</article-title><source>Exp Brain Res</source><volume>153</volume><fpage>180</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1007/s00221-003-1591-5</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Culham</surname><given-names>JC</given-names></name><name><surname>Cavina-Pratesi</surname><given-names>C</given-names></name><name><surname>Singhal</surname><given-names>A</given-names></name></person-group><year>2006</year><article-title>The role of parietal cortex in visuomotor control: what have we learned from neuroimaging?</article-title><source>Neuropsychologia</source><volume>44</volume><fpage>2668</fpage><lpage>2684</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.11.003</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><year>2004</year><article-title>Human brain imaging reveals a parietal area specialized for grasping</article-title><person-group person-group-type="editor"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><source>Attention and Performance XX: Functional brain imaging of human cognition</source><publisher-loc>Oxford, UK</publisher-loc><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>Rao</surname><given-names>VY</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year>2004</year><article-title>Maintenance of spatial and motor codes during oculomotor delayed response tasks</article-title><source>J Neurosci</source><volume>24</volume><fpage>3944</fpage><lpage>3952</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5640-03.2004</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davare</surname><given-names>M</given-names></name><name><surname>Andres</surname><given-names>M</given-names></name><name><surname>Cosnard</surname><given-names>G</given-names></name><name><surname>Thonnard</surname><given-names>JL</given-names></name><name><surname>Olivier</surname><given-names>E</given-names></name></person-group><year>2006</year><article-title>Dissociating the role of ventral and dorsal premotor cortex in precision grasping</article-title><source>J Neurosci</source><volume>26</volume><fpage>2260</fpage><lpage>2268</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3386-05.2006</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>N</given-names></name><name><surname>Cohen</surname><given-names>MX</given-names></name><name><surname>Newen</surname><given-names>A</given-names></name><name><surname>Bewernick</surname><given-names>BH</given-names></name><name><surname>Shah</surname><given-names>NJ</given-names></name><name><surname>Fink</surname><given-names>GR</given-names></name><etal/></person-group><year>2007</year><article-title>The extrastriate cortex distinguishes between the consequences of one’s own and others’ behavior</article-title><source>Neuroimage</source><volume>36</volume><fpage>1004</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.03.030</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year>2011</year><article-title>The role of occipitotemporal body-selective regions in person perception</article-title><source>Cogn Neurosci</source><volume>2</volume><fpage>186</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1080/17588928.2011.582945</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Shuman</surname><given-names>M</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year>2001</year><article-title>A cortical area selective for visual processing of the human body</article-title><source>Science</source><volume>293</volume><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="doi">10.1126/science.1063414</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Chan</surname><given-names>AW</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Dodds</surname><given-names>CM</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year>2006</year><article-title>Domain specificity in visual cortex</article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>1453</fpage><lpage>1461</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj086</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Duda</surname><given-names>RO</given-names></name><name><surname>Hart</surname><given-names>PE</given-names></name><name><surname>Stork</surname><given-names>DG</given-names></name></person-group><year>2001</year><source>Pattern Classification</source><edition>2nd ed</edition><publisher-loc>New York</publisher-loc><publisher-name>John Wiley &amp; Sons</publisher-name></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etzel</surname><given-names>JA</given-names></name><name><surname>Gazzola</surname><given-names>V</given-names></name><name><surname>Keysers</surname><given-names>C</given-names></name></person-group><year>2008</year><article-title>Testing simulation theory with cross-modal multivariate classification of fMRI data</article-title><source>PLOS ONE</source><volume>3</volume><fpage>e3690</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0003690</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Filimon</surname><given-names>F</given-names></name><name><surname>Nelson</surname><given-names>JD</given-names></name><name><surname>Huang</surname><given-names>RS</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year>2009</year><article-title>Multiple parietal reach regions in humans: cortical representations for visual and proprioceptive feedback during on-line reaching</article-title><source>J Neurosci</source><volume>29</volume><fpage>2961</fpage><lpage>2971</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3211-08.2009</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Filimon</surname><given-names>F</given-names></name></person-group><year>2010</year><article-title>Human cortical control of hand movements: parietofrontal networks for reaching, grasping, and pointing</article-title><source>Neuroscientist</source><volume>16</volume><fpage>388</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1177/1073858410375468</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fogassi</surname><given-names>L</given-names></name><name><surname>Ferrari</surname><given-names>PF</given-names></name><name><surname>Gesierich</surname><given-names>B</given-names></name><name><surname>Rozzi</surname><given-names>S</given-names></name><name><surname>Chersi</surname><given-names>F</given-names></name><name><surname>Rizzolatti</surname><given-names>G</given-names></name></person-group><year>2005</year><article-title>Parietal lobe: from action organization to intention understanding</article-title><source>Science</source><volume>308</volume><fpage>662</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1126/science.1106138</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Formisano</surname><given-names>E</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Bonte</surname><given-names>M</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name></person-group><year>2008</year><article-title>“Who” is saying “what”? Brain-based decoding of human voice and speech</article-title><source>Science</source><volume>322</volume><fpage>970</fpage><lpage>973</lpage><pub-id pub-id-type="doi">10.1126/science.1164318</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frey</surname><given-names>SH</given-names></name></person-group><year>2007</year><article-title>What puts the how in where? Tool use and the divided visual streams hypothesis</article-title><source>Cortex</source><volume>43</volume><fpage>368</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/S0010-9452(08)70462-3</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>Cavina-Pratesi</surname><given-names>C</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><year>2009</year><article-title>Is that within reach? fMRI reveals that the human superior parieto-occipital cortex encodes objects reachable by the hand</article-title><source>J Neurosci</source><volume>29</volume><fpage>4381</fpage><lpage>4391</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0377-09.2009</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>McLean</surname><given-names>DA</given-names></name><name><surname>Smith</surname><given-names>FW</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><year>2011a</year><article-title>Decoding effector-dependent and effector-independent movement intentions from human parieto-frontal brain activity</article-title><source>J Neurosci</source><volume>31</volume><fpage>17149</fpage><lpage>17168</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1058-11.2011</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>McLean</surname><given-names>DA</given-names></name><name><surname>Valyear</surname><given-names>KF</given-names></name><name><surname>Pettypiece</surname><given-names>CE</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><year>2011b</year><article-title>Decoding action intentions from preparatory brain activity in human parieto-frontal networks</article-title><source>J Neurosci</source><volume>31</volume><fpage>9599</fpage><lpage>9610</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0080-11.2011</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>McLean</surname><given-names>DA</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><year>2013</year><article-title>Where one hand meets the other: limb-specific and action-dependent movement plans decoded from preparatory signals in single human frontoparietal brain areas</article-title><source>J Neurosci</source><volume>33</volume><fpage>1991</fpage><lpage>2008</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0541-12.2013</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentilucci</surname><given-names>M</given-names></name><name><surname>Roy</surname><given-names>AC</given-names></name><name><surname>Stefanini</surname><given-names>S</given-names></name></person-group><year>2004</year><article-title>Grasping an object naturally or with a tool: are these tasks guided by a common motor representation?</article-title><source>Exp Brain Res</source><volume>157</volume><fpage>496</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1007/s00221-004-1863-8</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golland</surname><given-names>P</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year>2003</year><article-title>Permutation tests for classification: towards statistical significance in image-based studies</article-title><source>Inf Process Med Imaging</source><volume>18</volume><fpage>330</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1007/978-3-540-45087-0_28</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grafton</surname><given-names>ST</given-names></name><name><surname>Hamilton</surname><given-names>AF</given-names></name></person-group><year>2007</year><article-title>Evidence for a distributed hierarchy of action representation in the brain</article-title><source>Hum Mov Sci</source><volume>26</volume><fpage>590</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1016/j.humov.2007.05.009</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grezes</surname><given-names>J</given-names></name><name><surname>Decety</surname><given-names>J</given-names></name></person-group><year>2002</year><article-title>Does visual perception of object afford action? Evidence from a neuroimaging study</article-title><source>Neuropsychologia</source><volume>40</volume><fpage>212</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/S0028-3932(01)00089-6</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haaland</surname><given-names>KY</given-names></name><name><surname>Harrington</surname><given-names>DL</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name></person-group><year>2000</year><article-title>Neural representations of skilled movement</article-title><source>Brain</source><volume>123</volume><fpage>2306</fpage><lpage>2313</lpage><pub-id pub-id-type="doi">10.1093/brain/123.11.2306</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>SA</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year>2009</year><article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title><source>Nature</source><volume>458</volume><fpage>632</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nature07832</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Furey</surname><given-names>ML</given-names></name><name><surname>Ishai</surname><given-names>A</given-names></name><name><surname>Schouten</surname><given-names>JL</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><year>2001</year><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>JD</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><year>2006</year><article-title>Decoding mental states from brain activity in humans</article-title><source>Nat Rev Neurosci</source><volume>7</volume><fpage>523</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1038/nrn1931</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hebb</surname><given-names>DO</given-names></name></person-group><year>1949</year><source>The organization of behavior: a neuropsychological theory</source><publisher-loc>New York</publisher-loc><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoshi</surname><given-names>E</given-names></name><name><surname>Tanji</surname><given-names>J</given-names></name></person-group><year>2006</year><article-title>Differential involvement of neurons in the dorsal and ventral premotor cortex during processing of visual signals for action planning</article-title><source>J Neurophysiol</source><volume>95</volume><fpage>3596</fpage><lpage>3616</lpage><pub-id pub-id-type="doi">10.1152/jn.01126.2005</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>CW</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><year>2002</year><article-title>A comparison of methods for multi-class support vector machiens</article-title><source>IEEE Trans Neural Netw</source><volume>13</volume><fpage>415</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1109/72.991427</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iriki</surname><given-names>A</given-names></name><name><surname>Tanaka</surname><given-names>M</given-names></name><name><surname>Iwamura</surname><given-names>Y</given-names></name></person-group><year>1996</year><article-title>Coding of modified body schema during tool use by macaque postcentral neurones</article-title><source>Neuroreport</source><volume>7</volume><fpage>2325</fpage><lpage>2330</lpage><pub-id pub-id-type="doi">10.1097/00001756-199610020-00010</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>JH</given-names></name></person-group><year>1889</year><article-title>On the comparative study of diseases of the nervous system</article-title><source>BMJ</source><volume>2</volume><fpage>355</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1136/bmj.2.1494.355</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>S</given-names></name><name><surname>Danielmeier</surname><given-names>C</given-names></name><name><surname>Frey</surname><given-names>SH</given-names></name></person-group><year>2010</year><article-title>Human anterior intraparietal and ventral premotor cortices support representations of grasping with the hand or a novel tool</article-title><source>J Cogn Neurosci</source><volume>22</volume><fpage>2594</fpage><lpage>2608</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21372</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeannerod</surname><given-names>M</given-names></name><name><surname>Arbib</surname><given-names>MA</given-names></name><name><surname>Rizzolatti</surname><given-names>G</given-names></name><name><surname>Sakata</surname><given-names>H</given-names></name></person-group><year>1995</year><article-title>Grasping objects: the cortical mechanisms of visuomotor transformation</article-title><source>Trends Neurosci</source><volume>18</volume><fpage>314</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(95)93921-J</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>SH</given-names></name><name><surname>Grafton</surname><given-names>ST</given-names></name></person-group><year>2003</year><article-title>From ‘acting on’ to ‘acting with’: the functional anatomy of object-oriented action schemata</article-title><source>Prog Brain Res</source><volume>142</volume><fpage>127</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(03)42010-4</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson-Frey</surname><given-names>SH</given-names></name><name><surname>Newman-Norlund</surname><given-names>R</given-names></name><name><surname>Grafton</surname><given-names>ST</given-names></name></person-group><year>2005</year><article-title>A distributed left hemisphere network active during planning of everyday tool use skills</article-title><source>Cerebral cortex</source><volume>15</volume><fpage>681</fpage><lpage>695</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh169</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalaska</surname><given-names>JF</given-names></name></person-group><year>2009</year><article-title>From intention to action: motor cortex and the control of reaching movements</article-title><source>Adv Exp Med Biol</source><volume>629</volume><fpage>139</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1007/978-0-387-77064-2_8</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamitani</surname><given-names>Y</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year>2005</year><article-title>Decoding the visual and subjective contents of the human brain</article-title><source>Nat Neurosci</source><volume>8</volume><fpage>679</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1038/nn1444</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year>2010</year><article-title>Functional specificity in the human brain: a window into the functional architecture of the mind</article-title><source>Proc Natl Acad Sci USA</source><volume>107</volume><fpage>11163</fpage><lpage>11170</lpage><pub-id pub-id-type="doi">10.1073/pnas.1005062107</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year>2011</year><article-title>Pattern-information analysis: from stimulus decoding to computational-model testing</article-title><source>Neuroimage</source><volume>56</volume><fpage>411</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.01.061</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Avidan</surname><given-names>G</given-names></name><name><surname>Hendler</surname><given-names>T</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year>2001</year><article-title>Center-periphery organization of human object areas</article-title><source>Nat Neurosci</source><volume>4</volume><fpage>533</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1038/87490</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>JW</given-names></name><name><surname>Brefczynski</surname><given-names>JA</given-names></name><name><surname>Phinney</surname><given-names>RE</given-names></name><name><surname>Janik</surname><given-names>JJ</given-names></name><name><surname>DeYoe</surname><given-names>EA</given-names></name></person-group><year>2005</year><article-title>Distinct cortical pathways for processing tool versus animal sounds</article-title><source>J Neurosci</source><volume>25</volume><fpage>5148</fpage><lpage>5158</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0419-05.2005</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>JW</given-names></name></person-group><year>2006</year><article-title>Cortical networks related to human use of tools</article-title><source>Neuroscientist</source><volume>12</volume><fpage>211</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1177/1073858406288327</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Scott</surname><given-names>SH</given-names></name></person-group><year>2013</year><article-title>Preference distributions of primary motor cortex neurons reflect control solutions optimized for limb biomechanics</article-title><source>Neuron</source><volume>77</volume><fpage>168</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.041</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahon</surname><given-names>BZ</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year>2009</year><article-title>Concepts and categories: a cognitive neuropsychological perspective</article-title><source>Annu Rev Psychol</source><volume>60</volume><fpage>27</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.60.110707.163532</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahon</surname><given-names>BZ</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year>2011</year><article-title>What drives the organization of object knowledge in the brain?</article-title><source>Trends Cogn Sci</source><volume>15</volume><fpage>97</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.01.004</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahon</surname><given-names>BZ</given-names></name><name><surname>Milleville</surname><given-names>SC</given-names></name><name><surname>Negri</surname><given-names>GA</given-names></name><name><surname>Rumiati</surname><given-names>RI</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year>2007</year><article-title>Action-related properties shape object representations in the ventral stream</article-title><source>Neuron</source><volume>55</volume><fpage>507</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.07.011</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maravita</surname><given-names>A</given-names></name><name><surname>Iriki</surname><given-names>A</given-names></name></person-group><year>2004</year><article-title>Tools for the body (schema)</article-title><source>Trends Cogn Sci</source><volume>8</volume><fpage>79</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2003.12.008</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>A</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Lalonde</surname><given-names>FM</given-names></name><name><surname>Wiggs</surname><given-names>CL</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year>1995</year><article-title>Discrete cortical regions associated with knowledge of color and knowledge of action</article-title><source>Science</source><volume>270</volume><fpage>102</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1126/science.270.5233.102</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>A</given-names></name><name><surname>Wiggs</surname><given-names>CL</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year>1996</year><article-title>Neural correlates of category-specific knowledge</article-title><source>Nature</source><volume>379</volume><fpage>649</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1038/379649a0</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>K</given-names></name><name><surname>Kaplan</surname><given-names>JT</given-names></name><name><surname>Essex</surname><given-names>R</given-names></name><name><surname>Webber</surname><given-names>C</given-names></name><name><surname>Damasio</surname><given-names>H</given-names></name><name><surname>Damasio</surname><given-names>A</given-names></name></person-group><year>2010</year><article-title>Predicting visual stimuli on the basis of activity in auditory cortices</article-title><source>Nat Neurosci</source><volume>13</volume><fpage>667</fpage><lpage>668</lpage><pub-id pub-id-type="doi">10.1038/nn.2533</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Milner</surname><given-names>AD</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><year>1995</year><source>The Visual Brain in Action</source><publisher-loc>Oxford, England</publisher-loc><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Misaki</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year>2010</year><article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title><source>Neuroimage</source><volume>53</volume><fpage>103</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.051</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murata</surname><given-names>A</given-names></name><name><surname>Gallese</surname><given-names>V</given-names></name><name><surname>Luppino</surname><given-names>G</given-names></name><name><surname>Kaseda</surname><given-names>M</given-names></name><name><surname>Sakata</surname><given-names>H</given-names></name></person-group><year>2000</year><article-title>Selectivity for the shape, size, and orientation of objects for grasping in neurons of monkey parietal area AIP</article-title><source>J Neurophysiol</source><volume>83</volume><fpage>2580</fpage><lpage>2601</lpage></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year>2011</year><article-title>Encoding and decoding in fMRI</article-title><source>Neuroimage</source><volume>56</volume><fpage>400</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.073</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Polyn</surname><given-names>SM</given-names></name><name><surname>Detre</surname><given-names>GJ</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year>2006</year><article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title><source>Trends Cogn Sci</source><volume>10</volume><fpage>424</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.07.005</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Wiggett</surname><given-names>AJ</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Tipper</surname><given-names>SP</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><year>2010</year><article-title>Surface-based information mapping reveals crossmodal vision-action representations in human parietal and occipitotemporal cortex</article-title><source>J Neurophysiol</source><volume>104</volume><fpage>1077</fpage><lpage>1089</lpage><pub-id pub-id-type="doi">10.1152/jn.00326.2010</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orlov</surname><given-names>T</given-names></name><name><surname>Makin</surname><given-names>TR</given-names></name><name><surname>Zohary</surname><given-names>E</given-names></name></person-group><year>2010</year><article-title>Topographic representation of the human body in the occipitotemporal cortex</article-title><source>Neuron</source><volume>68</volume><fpage>586</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.032</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><year>2005</year><article-title>Is the extrastriate body area involved in motor actions?</article-title><source>Nat Neurosci</source><volume>8</volume><fpage>125</fpage><pub-id pub-id-type="doi">10.1038/nn0205-125a</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><year>2007</year><article-title>The neural basis of visual body perception</article-title><source>Nat Revi Neurosci</source><volume>8</volume><fpage>636</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1038/nrn2195</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peeters</surname><given-names>R</given-names></name><name><surname>Simone</surname><given-names>L</given-names></name><name><surname>Nelissen</surname><given-names>K</given-names></name><name><surname>Fabbri-Destro</surname><given-names>M</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Rizzolatti</surname><given-names>G</given-names></name><etal/></person-group><year>2009</year><article-title>The representation of tool use in humans and monkeys: common and uniquely human features</article-title><source>J Neurosci</source><volume>29</volume><fpage>11523</fpage><lpage>11539</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2040-09.2009</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><year>2011</year><article-title>Information mapping with pattern classifiers: a comparative study</article-title><source>Neuroimage</source><volume>56</volume><fpage>476</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.026</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Mitchell</surname><given-names>T</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><year>2009</year><article-title>Machine learning classifiers and fMRI: a tutorial overview</article-title><source>Neuroimage</source><volume>45</volume><supplement>1 Suppl</supplement><fpage>S199</fpage><lpage>S209</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.11.007</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pertzov</surname><given-names>Y</given-names></name><name><surname>Avidan</surname><given-names>G</given-names></name><name><surname>Zohary</surname><given-names>E</given-names></name></person-group><year>2011</year><article-title>Multiple reference frames for saccadic planning in the human parietal cortex</article-title><source>J Neurosci</source><volume>31</volume><fpage>1059</fpage><lpage>1068</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3721-10.2011</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picard</surname><given-names>N</given-names></name><name><surname>Strick</surname><given-names>PL</given-names></name></person-group><year>2001</year><article-title>Imaging the premotor areas</article-title><source>Curr Opin Neurobiol</source><volume>11</volume><fpage>663</fpage><lpage>672</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(01)00266-5</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Porter</surname><given-names>R</given-names></name><name><surname>Lemon</surname><given-names>RN</given-names></name></person-group><year>1993</year><source>Corticospinal function and voluntary movement</source><publisher-loc>Oxford, UK</publisher-loc><publisher-name>Clarendon Press</publisher-name></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raos</surname><given-names>V</given-names></name><name><surname>Umilta</surname><given-names>MA</given-names></name><name><surname>Gallese</surname><given-names>V</given-names></name><name><surname>Fogassi</surname><given-names>L</given-names></name></person-group><year>2004</year><article-title>Functional properties of grasping-related neurons in the dorsal premotor area F2 of the macaque monkey</article-title><source>J Neurophysiol</source><volume>92</volume><fpage>1990</fpage><lpage>2002</lpage><pub-id pub-id-type="doi">10.1152/jn.00154.2004</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raos</surname><given-names>V</given-names></name><name><surname>Umilta</surname><given-names>MA</given-names></name><name><surname>Murata</surname><given-names>A</given-names></name><name><surname>Fogassi</surname><given-names>L</given-names></name><name><surname>Gallese</surname><given-names>V</given-names></name></person-group><year>2006</year><article-title>Functional properties of grasping-related neurons in the ventral premotor area F5 of the macaque monkey</article-title><source>J Neurophysiol</source><volume>95</volume><fpage>709</fpage><lpage>729</lpage><pub-id pub-id-type="doi">10.1152/jn.00463.2005</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rijntjes</surname><given-names>M</given-names></name><name><surname>Dettmers</surname><given-names>C</given-names></name><name><surname>Buchel</surname><given-names>C</given-names></name><name><surname>Kiebel</surname><given-names>S</given-names></name><name><surname>Frackowiak</surname><given-names>RS</given-names></name><name><surname>Weiller</surname><given-names>C</given-names></name></person-group><year>1999</year><article-title>A blueprint for movement: functional and anatomical representations in the human motor system</article-title><source>J Neurosci</source><volume>19</volume><fpage>8043</fpage><lpage>8048</lpage></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname><given-names>G</given-names></name><name><surname>Luppino</surname><given-names>G</given-names></name></person-group><year>2001</year><article-title>The cortical motor system</article-title><source>Neuron</source><volume>31</volume><fpage>889</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(01)00423-8</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname><given-names>G</given-names></name><name><surname>Fogassi</surname><given-names>L</given-names></name><name><surname>Gallese</surname><given-names>V</given-names></name></person-group><year>2001</year><article-title>Neurophysiological mechanisms underlying the understanding and imitation of action</article-title><source>Nat Rev Neurosci</source><volume>2</volume><fpage>661</fpage><lpage>670</lpage><pub-id pub-id-type="doi">10.1038/35090060</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumiati</surname><given-names>RI</given-names></name><name><surname>Weiss</surname><given-names>PH</given-names></name><name><surname>Shallice</surname><given-names>T</given-names></name><name><surname>Ottoboni</surname><given-names>G</given-names></name><name><surname>Noth</surname><given-names>J</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><etal/></person-group><year>2004</year><article-title>Neural basis of pantomiming the use of visually presented objects</article-title><source>Neuroimage</source><volume>21</volume><fpage>1224</fpage><lpage>1231</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.11.017</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakata</surname><given-names>H</given-names></name><name><surname>Taira</surname><given-names>M</given-names></name><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Murata</surname><given-names>A</given-names></name><name><surname>Tanaka</surname><given-names>Y</given-names></name><name><surname>Tsutsui</surname><given-names>K</given-names></name></person-group><year>1998</year><article-title>Neural coding of 3D features of objects for hand action in the parietal cortex of the monkey</article-title><source>Philos Trans R Soc Lond B Biol Sci</source><volume>353</volume><fpage>1363</fpage><lpage>1373</lpage><pub-id pub-id-type="doi">10.1098/rstb.1998.0290</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sherrington</surname><given-names>CS</given-names></name></person-group><year>1906</year><source>The integrative action of the nervous system</source><publisher-loc>New Haven, CT</publisher-loc><publisher-name>Yale University Press</publisher-name></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>FW</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name></person-group><year>2010</year><article-title>Non-stimulated early visual areas carry information about surrounding context</article-title><source>Proc Natl Acad Sci USA</source><volume>107</volume><fpage>20099</fpage><lpage>20103</lpage><pub-id pub-id-type="doi">10.1073/pnas.1000233107</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname><given-names>LH</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year>1997</year><article-title>Coding of intention in the posterior parietal cortex</article-title><source>Nature</source><volume>386</volume><fpage>167</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1038/386167a0</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soon</surname><given-names>CS</given-names></name><name><surname>Brass</surname><given-names>M</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year>2008</year><article-title>Unconscious determinants of free decisions in the human brain</article-title><source>Nat Neurosci</source><volume>11</volume><fpage>543</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1038/nn.2112</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomassini</surname><given-names>V</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Klein</surname><given-names>JC</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Pozzilli</surname><given-names>C</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name><etal/></person-group><year>2007</year><article-title>Diffusion-weighted imaging tractography-based parcellation of the human lateral premotor cortex identifies dorsal and ventral subregions with anatomical and functional specializations</article-title><source>J Neurosci</source><volume>27</volume><fpage>10259</fpage><lpage>10269</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2144-07.2007</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Umilta</surname><given-names>MA</given-names></name><name><surname>Escola</surname><given-names>L</given-names></name><name><surname>Instkirvell</surname><given-names>I</given-names></name><name><surname>Grammont</surname><given-names>F</given-names></name><name><surname>Rochat</surname><given-names>M</given-names></name><name><surname>Caruana</surname><given-names>F</given-names></name><etal/></person-group><year>2008</year><article-title>When pliers become fingers in the monkey motor system</article-title><source>Proc Natl Acad Sci USA</source><volume>105</volume><fpage>2209</fpage><lpage>2213</lpage><pub-id pub-id-type="doi">10.1073/pnas.0705985105</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valyear</surname><given-names>KF</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><year>2010</year><article-title>Observing learned object-specific functional grasps preferentially activates the ventral stream</article-title><source>J Cogn Neurosci</source><volume>22</volume><fpage>970</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21256</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valyear</surname><given-names>KF</given-names></name><name><surname>Cavina-Pratesi</surname><given-names>C</given-names></name><name><surname>Stiglick</surname><given-names>AJ</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><year>2007</year><article-title>Does tool-related fMRI activity within the intraparietal sulcus reflect the plan to grasp?</article-title><source>Neuroimage</source><volume>36</volume><supplement>Suppl 2</supplement><fpage>T94</fpage><lpage>T108</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.03.031</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Dierker</surname><given-names>DL</given-names></name></person-group><year>2007</year><article-title>Surface-based and probabilistic atlases of primate cerebral cortex</article-title><source>Neuron</source><volume>56</volume><fpage>209</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.015</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van Lawick-Goodall</surname><given-names>J</given-names></name></person-group><year>1970</year><source>Advances in the Study of Behavior</source><publisher-loc>New York</publisher-loc><publisher-name>Academic</publisher-name></element-citation></ref><ref id="bib106"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Von Helmholtz</surname><given-names>H</given-names></name></person-group><year>1866</year><source>Handbook of Physiological Optics</source><edition>3rd ed</edition><publisher-loc>New York</publisher-loc><publisher-name>Dover Publications</publisher-name></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year>2001</year><article-title>Motor prediction</article-title><source>Curr Biol</source><volume>11</volume><fpage>R729</fpage><lpage>R732</lpage><pub-id pub-id-type="doi">10.1016/S0960-9822(01)00432-8</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><year>2000</year><article-title>Computational principles of movement neuroscience</article-title><source>Nat Neurosci</source><volume>3</volume><fpage>1212</fpage><lpage>1217</lpage><pub-id pub-id-type="doi">10.1038/81497</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yousry</surname><given-names>TA</given-names></name><name><surname>Schmid</surname><given-names>UD</given-names></name><name><surname>Alkadhi</surname><given-names>H</given-names></name><name><surname>Schmidt</surname><given-names>D</given-names></name><name><surname>Peraud</surname><given-names>A</given-names></name><name><surname>Buettner</surname><given-names>A</given-names></name><etal/></person-group><year>1997</year><article-title>Localization of the motor hand area to a knob on the precentral gyrus. A new landmark</article-title><source>Brain</source><volume>120</volume><issue>Pt 1</issue><fpage>141</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1093/brain/120.1.141</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.00425.015</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Angelaki</surname><given-names>Dora</given-names></name><role>Reviewing editor</role><aff><institution>Baylor College of Medicine</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://www.elifesciences.org/the-journal/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for choosing to send your work entitled “Decoding the neural mechanisms of human tool use” for consideration at <italic>eLife</italic>. Your article has been evaluated by a Senior editor and 3 reviewers, one of whom is a member of our Board of Reviewing Editors.</p><p>The following individuals responsible for the peer review of your submission want to reveal their identity: Dora Angelaki (Reviewing editor) and Brad Mahon (peer reviewer).</p><p>The Reviewing editor and the other reviewers discussed their comments before we reached this decision, and the Reviewing editor has assembled the following comments to help you prepare a revised submission.</p><p>The paper is clearly written and organized, and tackles an important question with a well-thought-out and sophisticated approach. Overall this is excellent work, although there are some concerns/questions that the authors should address:</p><p>1) An aspect of the Results that is not sufficiently brought out up front in the article (Abstract and Introduction) is that in different regions it is possible to distinguish different factors within the experiment. For instance, the fact that in pMTG upcoming actions can be decoded only when they are performed with the tools, whereas in Superior parietal-occipital cortex decoding was higher for the hand than for the tool, is a really important aspect of the findings that is somewhat buried. The authors should bring this out more fully.</p><p>2) It seems somewhat surprising that the regions able to decode across effector (tool→hand, hand→tool) are in IPS and premotor cortex, while regions within IPS/premotor cortex were also shown to have better decoding for hand than for tools. How can the authors reconcile these different effects (are they different populations of voxels driving the decoding accuracy?). Perhaps plotting the weights, voxel-wise, could help to address this.</p><p>3) It would be good to see the results (e.g., in figure form, and reduced statistics) for the control analysis of decoding of the ventricle and outside of the brain. This analysis addresses a really insidious issue that will be at the forefront of skeptical readers’ minds; it should be dealt with in full (rather than just saying the analysis was performed and there were no effects).</p><p>4) In discussing the occipital-temporal decoding of upcoming actions, the authors come close to, but shy away from, considering whether the organization of the ventral stream may be constrained by the way that the ventral stream is connected up with the rest of the brain (e.g., specificity for the visual representations of body parts could be driven by connectivity to regions of somatosensory or motor areas, and the same for regions that represent tools). The authors’ findings, showing that occipital temporal regions can decode upcoming actions would seem to resonate with that type of an explanation of the causes of neural specificity for different classes of visual stimuli in the ventral stream, at least as it applies to the representation of tools.</p><p>5) The auditory cue to determine the subjects’ movement seems to have been the same for both the hand and tool conditions. This consistency across effectors could be responsible for some or even all of the apparently cross-effector classification. That is, the activity in the ROIs that show above-chance cross-effector decoding of actions could be due to the content of the cue rather than preparation for the action per se. This issue must be addressed.</p><p>6) It was not clear that the authors balanced the number of voxels across ROIs. This is potentially important as other studies have shown that MVPA performance is strongly influenced by the number of voxels tested.</p><p>7) The authors averaged across both directions of the cross-effector test (hand→tool and tool→hand). It is possible however that there are asymmetries in action coding that would be revealed by disentangling these tests.</p><p>8) While the authors find that the univariate response properties of regions generally correspond to decoding performance with MVPA, this could partly reflect the ROI approach that was used. A whole-brain MVPA mapping approach might well reveal other regions that code action preparation that are not activated (or not selectively activated) in the univariate sense.</p><p>9) The authors note the increased sensitivity of their method relative to a univariate approach, but MVPA (like any other method) carries assumptions and limitations that should shape how the data are interpreted. This could be discussed.</p><p>10) The authors report MVPA analyses at each time point, yet we did not find much in the discussion of the results about how these decoding effects develop over time.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.00425.016</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>1) An aspect of the Results that is not sufficiently brought out up front in the article (Abstract and Introduction) is that in different regions it is possible to distinguish different factors within the experiment. For instance, the fact that in pMTG upcoming actions can be decoded only when they are performed with the tools, whereas in Superior parietal-occipital cortex decoding was higher for the hand than for the tool, is a really important aspect of the findings that is somewhat buried. The authors should bring this out more fully</italic>.</p><p>We thank the reviewers for this comment. Like the reviewers, we also believe that these hand-specific and tool-specific decoding results are a very important aspect of the findings, and, given the wide-range of findings observed and discussed, it is possible that these important results may have appeared somewhat buried in the Discussion section of our initial manuscript. In line with the reviewers’ suggestions we have modified our Abstract and Introduction to help bring these findings saliently to the forefront of our article.</p><p>Also, to more fully highlight our effector-specific decoding effects, we now end our Introduction section with the following new paragraph:</p><p>“Consistent with an effector-specific coding of hand- and tool-related movements we found that preparatory signals in SPOC and EBA differentiated upcoming movements of the hand only (i.e., hand-specific) whereas in SMG and pMTG they discriminated upcoming movements of the tool only (i.e., tool-specific). In addition, in anterior parietal regions (e.g., aIPS) and motor cortex we found that pre-movement activity patterns discriminated planned actions of <italic>both</italic> the hand and tool but, importantly, could not be used to predict upcoming actions of the other effector. Instead, we found that this effector-independent type of coding was constrained to the preparatory signals of a subset of frontoparietal areas (posterior IPS and premotor cortex), suggesting that in these regions neural representations are more tightly linked to the goal of the action (grasping versus reaching) rather than the specific hand movements required to implement those goals.”</p><p><italic>2) It seems somewhat surprising that the regions able to decode across effector (tool→hand, hand→tool) are in IPS and premotor cortex, while regions within IPS/premotor cortex were also shown to have better decoding for hand than for tools. How can the authors reconcile these different effects (are they different populations of voxels driving the decoding accuracy?)? Perhaps plotting the weights, voxel-wise, could help to address this</italic>.</p><p>We expect that this comment from the reviewers directly refers to the time-resolved decoding accuracies within the across-effector decoding regions (pIPS, midIPS, PMd, and PMv, <xref ref-type="fig" rid="fig4">Figure 4</xref>) rather than the Plan-epoch decoding accuracies, as the latter reveals very similar levels of decoding for the hand and tool conditions. More specifically, this reviewer comment likely pertains to the time-resolved decoding in L-midIPS (see <xref ref-type="fig" rid="fig4">Figure 4</xref>, second plot from top, gray shaded bar) – the <italic>only</italic> region in which decoding for the hand and across-effector conditions are <italic>both</italic> significantly above chance during the final two volumes of the plan phase (at FDR correction levels) and the <italic>tool condition is not</italic>. We fully expect that this observation, more than anything else, speaks to the noisier (and thus more variable) nature of the single time-point decoding approach (in which pattern decoding is performed at each individual imaging volume in the trial), compared to that when time-points are averaged together, producing a mean activity pattern. Indeed, this difference in statistical significance between both approaches (i.e., time-resolved vs. mean-based decoding analyses) has been well-documented and previously reported (e.g., Harrison and Tong, 2009). For this very reason, we also provided (for the benefit of readers as well as reviewers) the ‘less-noisy’ Plan-Epoch-based decoding analysis, in which decoding is performed based on the ‘average’ pattern of voxels over the two imaging volumes preceding movement onset (denoted by the gray shaded bars in <xref ref-type="fig" rid="fig4">Figure 4</xref>, for example).</p><p>Consistent with previously published findings (Harrison and Tong, 2009), this latter mean-pattern-based approach typically provides a more robust and statistically significant measure of decoding accuracy. However, we should note that despite tool-specific decoding in L-midIPS not passing the FDR correction level in the time-resolved decoding analysis, it is still significant at p&lt;0.05 using the two-sample parametric t-test (vs. 50%). With respect to this last point, it is worth recognizing that we did in fact perform a statistical analysis (included in the main manuscript with our previous submission) comparing decoding accuracies in the mean-based plan-epoch across pairwise comparisons within each region. While the results of these tests revealed several significant effects (e.g., the responses in L-SPOC were found to be hand-specific whereas the responses in L-SMG were found to be tool-specific), we did not find significant differences between decoding accuracies for the hand versus tool conditions in L-midIPS. This finding reinforces the notion that <italic>differences in significance</italic> for hand- and tool-related decoding (vs. 50% chance) do not necessarily equate to <italic>significant differences</italic> between those conditions.</p><p>With regards to the reviewer comment: “How can the authors reconcile these different effects (are they different populations of voxels driving the decoding accuracy?)? Perhaps plotting the weights, voxel-wise, could help to address this.” We thank them for providing this suggestion. It is indeed possible that by plotting the voxel weights we might potentially reveal whether it is the same (or different) population of voxels across hand- and tool-selective comparisons that are responsible for driving the observed across-effector classification effects. We have included such an analysis in the revised submission for two representative subjects and also include a detailed description of these results and the analyses employed (see <xref ref-type="fig" rid="fig4s2">Figure 4–figure supplement 2</xref> and the subheading entitled ‘Voxel weights analyses’). To briefly summarize the findings here, we found, like several previously published interpretations of the voxel weights (e.g., Kamitani and Tong, 2005; Harrison and Tong, 2009; Gallivan et al., 2011a; Gallivan et al., 2011b), that there can be quite a variable topography in the voxel weightings assigned by the classifier within subjects and across pair-wise comparisons, even in the regions that show successful cross-decoding. This spatial variability makes pinpointing the specific voxels responsible for driving the observed cross-classification effects largely speculative. [Note that we have opted to place these additional analyses as a figure supplement as we strongly believe they do not significantly enhance the narrative of the main manuscript.]</p><p><italic>3) It would be good to see the results (e.g., in figure form, and reduced statistics) for the control analysis of decoding of the ventricle and outside of the brain. This analysis addresses a really insidious issue that will be at the forefront of skeptical readers’ minds; it should be dealt with in full (rather than just saying the analysis was performed and there were no effects)</italic>.</p><p>We completely agree with this comment from the reviewers that inclusion of control analyses showing decoding from the ventricle and outside of the brain would be of particular benefit to ease sceptical readers’ minds concerning the validity of the effects being reported. We have now included these control analyses (see <xref ref-type="fig" rid="fig3s1">Figure 3–figure supplement 1</xref>) for interested readers. Again, we have opted to include these analyses in a figure supplement rather than within the main manuscript as we believe they do not enhance the narrative.</p><p><italic>4) In discussing the occipital-temporal decoding of upcoming actions, the authors come close to, but shy away from, considering whether the organization of the ventral stream may be constrained by the way that the ventral stream is connected up with the rest of the brain (e.g., specificity for the visual representations of body parts could be driven by connectivity to regions of somatosensory or motor areas, and the same for regions that represent tools). The authors’ findings, showing that occipital temporal regions can decode upcoming actions would seem to resonate with that type of an explanation of the causes of neural specificity for different classes of visual stimuli in the ventral stream, at least as it applies to the representation of tools</italic>.</p><p>This is an excellent comment from the reviewers and indeed, we admit that we did somewhat “shy away” from fully discussing this issue in our initiation submission. In drafting our initial manuscript we were uncertain as to how potential reviewers might respond to this type of “speculative” discussion (thus our reason for toning down our arguments in that regard) but we are quite happy, given this encouragement from the reviewers, to now fully engage this possible explanation of our results in the revised manuscript. Moreover, we believe that the capacity to discuss these intriguing possibilities more fully in the revised paper also allows us to highlight the reviewers’ suggestions raised in comment #1.</p><p>At the end of our Discussion section, we now close with the following paragraph:</p><p>“In addition to suggesting a role for OTC in visual-motor planning, these findings might also shed light on the organizing principles of the ventral visual stream. Several theories have been proposed to account for the categorical-selectivity of responses throughout OTC (e.g., for faces, scenes, bodies, tools, etc.), with the majority arguing that this modular arrangement arises due to similarities/differences in the visual structure of the world and/or how it is experienced (Kanwisher, 2010). For example, according to one prominent view, faces and scenes activate different regions of OTC due to underlying visual field preferences (i.e., faces activate areas with stronger foveal representations, like FFA, whereas scenes activate areas with stronger peripheral representations, like PPA, (Levy et al., 2001)). According to another well-known view, it is instead similarity in visual shape/form that is mapped onto ventral temporal cortex (Haxby et al., 2001). One particularly compelling alternative view, however, argues that the organization of OTC may be largely invariant to bottom-up visual properties and that it instead emerges as a by-product of the distinct connectivity patterns of OTC areas with the rest of the brain, particularly the downstream motor structures that use the visual information processed in OTC to plan movements of the body (Mahon et al., 2007; Mahon and Caramazza, 2009, 2011). Under this view, the neural specificity frequently observed for the visual presentation of body parts and/or tools in particular regions of OTC may reflect, to a certain extent, their anatomical connectivity with frontoparietal areas involved in generating movements of the body and/or interacting with and manipulating tools, respectively – a notion that garners some empirical support from the ‘downstream’ functional connectivity patterns of areas involved in body part- and tool-related processing (Mahon et al., 2007; Bracci et al., 2012). Assuming the sharing of action-related information within functionally interconnected circuits, this conceptual framework might help explain the matching object-selective and planning-related responses observed here within both EBA and pMTG. This compatibility of visual- and motor-related responses within single brain areas resonates with neurophysiological findings in macaque parietal cortex showing that the visual-response selectivity of neurons in AIP (for size, shape, orientation, etc) are often matched to their motor-response selectivity during action (e.g., Murata et al., 2000). This coupling is thought to mediate the transformation of visual information regarding physical object properties into corresponding motor programs for grasping or use (Jeannerod et al., 1995; Rizzolatti and Luppino, 2001) and resonates with the broader concept of motor affordances, whereby the properties of objects linked to action are automatically represented in movement-related areas of the brain (Cisek, 2007; Cisek and Kalaska, 2010). Where exactly the current findings fit within the context of these broader frameworks remains unclear, nevertheless, our results provide novel evidence suggesting that the specificity of visual object categorical responses in OTC are in some way linked to a specific role in preparing related motor behaviors.”</p><p><italic>5) The auditory cue to determine the subjects’ movement seems to have been the same for both the hand and tool conditions. This consistency across effectors could be responsible for some or even all of the apparently cross-effector classification. That is, the activity in the ROIs that show above-chance cross-effector decoding of actions could be due to the content of the cue rather than preparation for the action per se. This issue must be addressed</italic>.</p><p>This is a clever comment by the reviewers. It is indeed correct that the auditory cue for both hand and tool conditions was the same (i.e., “Grasp” or “Touch”) and so it is indeed a possibility that this consistency across the effectors could, to some extent, account for the across-effector classification effects reported. We have directly addressed this issue in our revised paper by separately localizing primary auditory cortex activity (i.e., Heschl’s gyrus) in all participants, extracting the activity related to the motor task, and then determining whether we could decode, using the time-resolved and plan-epoch based analyses, the content of the cue (i.e., “Grasp” or “Reach”) from the area.</p><p>We have included the results of this analysis (see <xref ref-type="fig" rid="fig4s3">Figure 4–figure supplement 3</xref>). Also, we now write:</p><p>“One alternative explanation to account for the accurate across-effector classification findings reported may be that our frontoparietal cortex results arise not because of the coding of effector-invariant movement goals (grasp vs. reach actions) but instead simply because grasp vs. reach movements for both hand and tool trials are cued according to the same “Grasp” and “Reach” auditory instructions. In other words, the cross-decoding observed in PPC and premotor cortex regions might only reflect the selective processing of the auditory commands common to Hand-G and Tool-G (“Grasp”) and Hand-R and Tool-R (“Reach”) trials and actually have nothing to do with the mutual upcoming goals of the object-directed movement. If this were the case, then we would expect to observe significant across-effector classification in primary auditory cortex (Heschl’s gyrus) for the same time-points as that found for PPC (pIPS and midIPS) and premotor (PMd and PMv) cortex. We directly tested for this possibility in our data by separately localizing left-Heschl’s gyrus in each subject for the same contrast used to define the sensorimotor frontoparietal network, [Plan &amp; Execute &gt; 2*Preview](recall that auditory cues initiate the onset of the Plan and Execute phases of the trial and so this was a robust contrast for localizing primary auditory cortex). We found that although accurate across-effector classification does indeed arise in Heschl’s gyrus during the trial, it does so distinctly earlier in the Plan-phase compared to that of the frontoparietal areas (see <xref ref-type="fig" rid="fig4s3">Figure 4–figure supplement 3</xref>). This observation is consistent with the noticeably transient percentage signal change response that accompanies the auditory instructions delivered to participants at the beginning of the Plan-phase (see time-course in <xref ref-type="fig" rid="fig4s3">Figure 4–figure supplement 3</xref>), as compared to the more sustained planning-related responses that emerge throughout the entire frontoparietal network (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). The temporal disconnect between the cross-decoding found in Heschl’s gyrus (which emerges in the fourth volume of the Plan-phase) and frontoparietal cortex (which generally emerges in the fifth-sixth volumes of the Plan-phase) makes it unlikely that the effector-invariant nature of the responses revealed in the wide-range of frontoparietal areas can be fully attributable to simple auditory commonalities in the planning cues.”</p><p>In addition, in our Materials and methods, we now also include details concerning the localization of auditory cortex. We write:</p><p>“To provide a control for our cross-decoding analyses in frontoparietal cortex (for motivation, see <italic>Results</italic>), we examined time-resolved and plan-epoch decoding in the left primary auditory cortex (Heschl’s gyrus). Using the same contrast and selection criteria as above, this ROI was neuroanatomically defined in each subject by selecting voxels halfway up along the superior temporal sulcus (STS), on the superior temporal gyrus (between the insular cortex and outer-lateral edge of the superior temporal gyrus).”</p><p>We are very thankful to the reviewers for bringing this important consideration with regards to interpreting the results to our attention.</p><p><italic>6) It was not clear that the authors balanced the number of voxels across ROIs. This is potentially important as other studies have shown that MVPA performance is strongly influenced by the number of voxels tested</italic>.</p><p>Our voxel selection criterion for ROIs was constrained by brain anatomy and the statistical significance of the voxels for the contrast of interest (for the frontoparietal ROIs, the contrast selected for voxels involved in movement generation; Plan &amp; Execute &gt; Preview; for the OTC ROIs, the contrast selected for voxels with a maximal response to preferred object category). Specifically, all frontoparietal ROI clusters were selected according to anatomical landmarks (see the section in the Materials and methods entitled “ROI selection”) at the single-subject level at a statistical threshold of t=3, p&lt;0.005 (cluster threshold corrected at p&lt;0.05) using a maximum of (15 mm)<sup>3</sup> cluster sizes. The localizer-defined ROI clusters were selected using well-documented ROI selection methods for the ventral visual stream (i.e., we selected (15 mm)<sup>3</sup> of activity around the peak voxel of activity for each contrast of interest). Together, these voxel selection criterions not only ensured that the voxels being selected for each ROI correspond with the selection criteria applied in our previously published work in frontoparietal cortex (Gallivan et al., 2011a; Gallivan et al., 2011b; Gallivan et al., 2013) and others’ work on the ventral stream in EBA and pMTG (e.g., Downing et al., 2006), but it also ensured that: 1) the selected voxels were defined objectively, 2) the ratio of voxels to trial repetitions in each ROI was reduced, decreasing the chance of classifier over-fitting (Pereira et al., 2009), 3) the voxels selected were statistically responsive to the task, reducing the chance of uninformative or noisy voxels being included in the ROI analysis, 4) ROI clusters were segregated from nearby activations (which is particularly important for parietal (i.e., SPOC, pIPS, midIPS, post. aIPS, aIPS) and lateral occipital areas (i.e., EBA and pMTG) which reside in close anatomical proximity and, at more liberal statistical thresholds, can have highly overlapping activations, 5) a similar number of voxels were included across frontoparietal cortex areas (from a minimum mean across subjects of 54 voxels in SPOC and PMv to a maximum mean across subjects of 89 voxels in motor cortex) and OTC areas (from a minimum mean across subjects of 23 voxels in pMTG to a maximum mean across subjects of 32 voxels in EBA), 6) the voxels included in each ROI were spatially contiguous, which may not be the case if voxel number was manipulated after cluster selection, and lastly, 7) a large enough number of voxels were included for spatial pattern classification.</p><p>We should also note that although the anatomical organization of frontoparietal cortex and lateral occipital cortex may be conducive to selecting and using <italic>fewer</italic> voxels from each region, it is not particularly conducive to including <italic>more</italic> voxels from each region. As well demonstrated by others, the lateral occipital cortex (where object-selective, face-selective, body-selective and tool-selective ROIs reside, the latter two areas localized and examined in the current study) contains several tightly spatially clustered category-selective ROIs, and their activations, at more liberal statistical thresholds, can be highly overlapping (e.g., Levy et al., 2001; Downing et al., 2007; Orlov et al., 2010). Thus, including additional voxels in each ROI (beyond what we have already selected, (15 mm)<sup>3</sup>) easily runs into the inherent problem that voxels being selected as belonging to the activity of one category-selective ROI, like EBA, become overlapping with the activity selected as part of a separate category-selective ROI, like pMTG (or some other ROI like the lateral occipital area, LO, or the occipital face area, OFA, both not localized in this study). Understandably, this would result in several interpretational issues for the data and this was a problem we wished to avoid in the current study by constraining the number of voxels included for analysis in each ROI to (15 mm)<sup>3</sup> spatially contiguous clusters. Also, it is worth noting that when considering reducing the number of voxels in each ROI (to balance voxel number across ROIs), that the number of voxels presented in our analysis already represents, to a large extent, the lower end to the number that is typically reported in other studies.</p><p>Nevertheless, given that some other studies have shown that MVPA performance may be influenced by the number of voxels tested we have specifically addressed this issue by examining the impact of voxel number across our ROIs on the resulting decoding accuracies. In the figure below we plot decoding accuracy for each pair-wise comparison (Hand: G vs. R; Tool: G vs. R; Across-Effector: G vs. R) as a function of the number of voxels included in each ROI (13 ROIs total), where each data point in the plot denotes a single ROI (indicating the number of voxels for that ROI and its resulting pair-wise comparison decoding accuracy, as indicated by its colour and symbol). Lines of best fit, color-coded to correspond to a specific pair-wise comparison, are overlaid atop the data points, and demonstrate near zero correlations between both factors (voxel number and decoding accuracy). Importantly, none of the correlations for the three comparisons showed even a trend toward statistical significance. Thus, although other studies have shown that MVPA performance can be influenced by the number of voxels tested, that does not appear to be of concern for the range of cluster sizes included here. This finding validates our approach of selecting ROIs according to statistical criteria (t-test significance) and brain anatomy and, for many of the reasons already mentioned above, we believe it is problematic to directly balance the number of voxels (by arbitrarily selected fewer or more) across ROIs.<fig id="fig8" position="float"><label>Author response image 1</label><caption><title>Correlation between mean Voxel Number and resulting mean Decoding Accuracy (shown for each ROI (N=13) and each of the three pair-wise comparisons for the Plan-Epoch).</title><p>Pearson correlation between Voxel Number and Hand Plan Epoch decoding accuracies (red diamond symbols): r<sup>2</sup>=0.108, p=0.272; Pearson Correlation between Voxel Number and Tool Plan Epoch decoding accuracies (blue square symbols): r<sup>2</sup>=0.025, p=0.602; Pearson Correlation between Voxel Number and Across-Effector Plan Epoch decoding accuracies (purple triangle symbols): r<sup>2</sup>=0.026, p=0.598.</p></caption><graphic xlink:href="elife-00425-resp-fig1-v1.tif"/></fig></p><p><italic>7) The authors averaged across both directions of the cross-effector test (hand→tool and tool→hand). It is possible however that there are asymmetries in action coding that would be revealed by disentangling these tests</italic>.</p><p>This is an intriguing suggestion from the reviewers. We have directly addressed this comment by performing and plotting analyses that disentangle both directions of the cross-effector tests (Train set: Hand →Test set: Tool and Train set: Tool → Test set: Hand). These findings are now reported in <xref ref-type="fig" rid="fig4s1">Figure 4–figure supplement 1</xref>. As can be clearly seen in the figure, we found a largely symmetrical relationship between both directions of cross-effector classifier training and testing. We do realize that some researchers have previously reported asymmetrical differences in the direction of cross-decoding – and have made claims concerning such effects (e.g., Eger et al., 2009) – but we would be cautious to conclude that such effects are particularly meaningful and, moreover, whether they can actually be fully explained by underlying neural mechanisms. In binary pattern classification, all that is being learned by the classification algorithm is an optimal decision boundary that discriminates the spatial patterns of activity associated with two experimental conditions and it should be expected that if this decision boundary accurately generalizes to a completely different set of conditions (i.e., cross-decodes) and truly reflects a meaningful relationship in the data, that the particular direction of classifier training and testing should not reveal large differences in the resulting cross-classification accuracies.</p><p>Note that in the main-manuscript we have made reference to these supplemental analyses and we now write:</p><p>“[Note that separating these tests, Train set: Hand →Test set: Tool and Train set: Tool → Test set: Hand, revealed no major asymmetries in classification, see <xref ref-type="fig" rid="fig4s1">Figure 4–figure supplement 1</xref>].”</p><p><italic>8) While the authors find that the univariate response properties of regions generally correspond to decoding performance with MVPA, this could partly reflect the ROI approach that was used. A whole-brain MVPA mapping approach might well reveal other regions that code action preparation that are not activated (or not selectively activated) in the univariate sense</italic>.</p><p>Although it is a fair point to note that our selection criteria of including frontoparietal ROIs involved in movement generation in our task (i.e., having higher activity for the Execute and Plan phases of the task than the Preview phase) perhaps makes it more likely that these areas will decode meaningful aspects of our motor task, we do find it worth noting that <italic>the univariate response properties of these regions alone cannot account for the multitude of decoding profiles observed</italic>. For instance, there is nothing in the signal amplitude time-course profiles of activity in SPOC or SMG that suggest that these areas will selectively code planned movements of the hand and tool, respectively (indeed, the same can be said for the OTC areas, EBA and pMTG). Likewise, there is nothing in the univariate response properties of areas aIPS and PMd to suggest that only the latter area will show accurate across-effector classification. These observations, of course, should not be particularly surprising given that our statistical selection criterion for voxel selection was fully orthogonal to the effects we report with MVPA. Indeed, as we wrote in the main manuscript: “… given the orthogonal contrast employed to select these 10 areas (i.e., Plan &amp; Execute &gt; Preview), their activity is not directionally biased to show any preview-, plan- or execute-related pattern differences between any of the experimental conditions.”</p><p>Indeed, it seems clear that if any strong interpretations are to be drawn from the voxel weight analyses performed in response to the reviewers’ suggestion #2, then it would be that successful decoding of planned movements cannot be explained by global differences in the response amplitudes across conditions but rather results from anisotropies in the distribution of voxel biases (see <xref ref-type="fig" rid="fig4s2">Figure 4– figure supplement 2</xref> for confirmation of that fact).</p><p>Stated simply, the goal of the current study was to localize several key brain areas in frontoparietal and occipitotemporal cortex and determine what types of information they might represent related to the planning of hand- and tool-related actions. In the case of frontoparietal cortex, areas within this network have a long and well-documented history of being involved in the planning and control of hand actions and thus, from our perspective, it made perfect sense to localize and examine the planning-related activity in these areas in the context of our task. [Also, it should be noted that the frontoparietal areas included in the current manuscript are many of the exact same areas localized and analyzed in our previously published work (Gallivan et al., 2011a; Gallivan et al., 2011b; Gallivan et al., 2013) and it was of particular benefit for us as investigators to determine whether (and how) these same regions are involved in the planning of object-directed motor tasks involving an implement that is not actually part of the body, such as a tool]. In the case of OTC, several of these areas have a long and well-documented history of being involved in object-related perception and by independently localizing areas involved in body- and tool-related visual processing (i.e., EBA and pMTG, respectively) and extracting their activity from the motor-related task, we thought that it might be possible to reveal substantial insights into the roles that these areas play in generating hand- and tool-related actions (indeed, we believe that the matching visual-perceptual and motor-related functions of these areas observed in the current study does reveal such insight).</p><p>With regards to the reviewer comment concerning a whole-brain MVPA mapping approach, we are strongly in favour of providing readers with a well-reasoned hypothesis-driven ROI-based MVPA approach rather than providing largely post-hoc interpretations concerning the results of whole-brain MVPA maps (particularly in cases when strong predictions and hypotheses can be formed with regards to the involvement of particular areas in the task, as here). Moreover, we believe it is important to not gloss-over the fact that we already perform analyses and discuss, in quite significant detail, the activity of 13 different brain areas in our manuscript (and that number rises to 14, if the newly added auditory cortex findings are included in that tally). We are concerned that by extending our analysis (and related discussions) to areas not involved in the motor task (as indicated by their standard univariate responses for the contrast of Plan &amp; Execute &gt; Preview) nor body- or tool-selective visual processing (such as the independently localized areas, EBA and pMTG) that we will significantly weaken the focus and narrative of our paper. We should also note that some of the motivation for performing ROI-based MVPA here, as well as in our previous work in frontoparietal cortex (Gallivan et al., 2011a; Gallivan et al., 2011b; Gallivan et al., 2013), stems from the Whole-brain Searchlight MVPA approach requiring inter-subject anatomical averaging. In our experience, we have found that group stereotaxic averaging (e.g., Talairaching) performs quite poorly in many areas of parietal and frontal cortex. For example, group-activation in SPOC doesn’t survive intersubject averaging well despite there being highly robust effects at the single subject level; this is perhaps because SPOC is not only located far from the anatomical landmarks used for Talairach stereotaxic alignment but also due to the significant heterogeneity in the sulci of parietal cortex across participants (e.g., Quinlan and Culham, 2007).</p><p>In contrast, the ROI-based approach allows localization of brain activation with respect to anatomical landmarks in individuals, thus side-stepping these inherent problems of between-subject anatomical variability. Lastly, with respect to whole-brain MVPA, it should also be noted that performing decoding analysis for <italic>each subject</italic> (N=13) and for <italic>each time-point</italic> in the trial (i.e., 17 imaging volumes) across <italic>each voxel of the brain</italic> (∼50,000 separate searchlight clusters) presents inherent computational challenges that we are simply not equipped with the necessary computer hardware to deal with (at least within any reasonable time frame). In sum, we are happy to keep the paper more focused and manageable by presenting the findings from the 14 frontoparietal, auditory, and occipitotemporal cortex ROIs.</p><p><italic>9) The authors note the increased sensitivity of their method relative to a univariate approach, but MVPA (like any other method) carries assumptions and limitations that should shape how the data are interpreted. This could be discussed</italic>.</p><p>The reviewers raise a very important point and indeed, we strongly believe that these assumptions and limitations should be adequately discussed in the context of interpreting our results. With this in mind, we have included such a discussion in the main manuscript.</p><p>As a final paragraph for our Results section, we now write:</p><p>“It is worth emphasizing that while accurate decoding in a region points to underlying differences in the neural representations associated with different experimental conditions (e.g., for reviews see Haynes and Rees, 2006; Norman et al., 2006; Kriegeskorte, 2011; Naselaris et al., 2011), a lack of decoding or ‘null effect’ (i.e., 50% chance classification) can either reflect that the region 1) is not recruited for the conditions being compared, 2) is similarly (but non-discriminately) engaged in those conditions, or 3) does in fact contain neural/pattern differences between the conditions but which cannot be discriminated by the pattern classification algorithm employed (i.e., a limit of methodology, see Pereira et al., 2009; Pereira and Botvinick, 2011). With respect to the first possibility, given that we selected frontoparietal cortex activity based on its involvement in the motor task at the single subject level (using the contrast of [Plan &amp; Execute &gt; Preview] across all conditions), it is reasonable to assume that all the localized areas are in some way engaged in movement generation. [Note that this general assumption is confirmed by the higher-than-baseline levels of activity observed in the signal amplitude responses during the Plan- and Execute-phases of the trial in areas of frontoparietal cortex (see <xref ref-type="fig" rid="fig2 fig5">Figures 2 and 5</xref>) and that this even appears to be the case in the independently localizer-defined lateral occipital cortex areas, EBA and pMTG (see <xref ref-type="fig" rid="fig6">Figure 6</xref>)]. Although it is understandably difficult to rule out the second possibility (i.e., that voxel pattern differences exist but are not detected with the SVM classifiers), it is worth noting that we do in fact observe null-effects with the classifiers in several regions where they are to be expected. For instance, SS-cortex is widely considered to be a lower-level sensory structure and anticipated to only show discrimination related to the motor task once the hand’s mechanoreceptors have been stimulated at object contact (either through the hand directly or through the tool, indirectly). Accordingly, here we find that SS-cortex activity only discriminates between grasp vs. reach movements following movement onset (i.e., during the Execute phase of the trial). Likewise, in motor cortex we show decoding for upcoming hand- and tool-related actions but, importantly, find no resulting across-effector classification. This latter result is highly consistent with the coding of differences in the hand kinematics required to operate the tool versus hand alone and accords with the presumed role of motor cortex in generating muscle-related activity (Kalaska, 2009; Churchland et al., 2012; Lillicrap and Scott, 2013). These findings in SS-cortex and motor cortex, when combined with the wide-range of decoding profiles found in other areas (i.e., from the hand-selective activity patterns in SPOC and EBA at one extreme, to the tool-selective activity patterns in SMG and pMTG at the other, for summary see <xref ref-type="fig" rid="fig7">Figure 7</xref>), suggest that the failure of some areas to decode information related to either hand- or tool-related trials (and not those of the other effector) is closely linked to invariance in the representations of those particular conditions. [To the extent that in cases where the activity of an area fails to discriminate between experimental conditions it can be said that the area is therefore not involved in coding (or invariant to) those particular conditions, we further expand upon interpretations related to these types of ‘null effects’ in the Discussion section.]”</p><p><italic>10) The authors report MVPA analyses at each time point, yet we did not find much in the discussion of the results about how these decoding effects develop over time</italic>.</p><p>It is true that in the discussion of the results we do not examine in great detail how these decoding effects develop over time. In part, this was because we found very high correspondence between the Plan-Epoch decoding results and those of the time-resolved decoding analysis (i.e., single time-point results). Nevertheless, we do feel it important to note that these “time-point” decoding results provide three rather general but significant observations. We have now noted these important observations in the main manuscript and we write:</p><p>“Three general observations can be made based on the results of these decoding analyses. First, predictive movement information, if it is to emerge, generally arises in the two time points prior to initiation of the movement (although note that in a few brain areas, such as L-pIPS and L-PMd, this information is also available prior to these two time points). Second, in support of the notion that this predictive motor information is directly related to the <italic>intention</italic> to make a movement, accurate classification never arises prior to the subject being aware of which action to execute (i.e., prior to the auditory instruction delivered at the initiation of the Plan phase). Finally, decoding related to the planning of a movement can be fully disentangled from decoding related to movement execution, which generally arises several imaging volumes later.”</p><p>As a brief aside, we find it important to note that these time-resolved decoding results also provide an validation of our past published work (Gallivan et al., 2011a; Gallivan et al., 2011b; Gallivan et al., 2013), which divided the trial into separate epochs for decoding analysis (i.e., Preview, Plan and Execute).</p><p><bold>References</bold></p><p>Bracci S, Cavina-Pratesi C, Ietswaart M, Caramazza A, Peelen MV (2012) Closely overlapping responses to tools and hands in left lateral occipitotemporal cortex. Journal of neurophysiology 107:1443-1456.</p><p>Churchland MM, Cunningham JP, Kaufman MT, Foster JD, Nuyujukian P, Ryu SI, Shenoy KV (2012) Neural population dynamics during reaching. Nature 487:51-56.</p><p>Cisek P (2007) Cortical mechanisms of action selection: the affordance competition hypothesis. Phil Transact R Soc Lond B 362:1585-1599.</p><p>Cisek P, Kalaska JF (2010) Neural Mechanisms for Interacting with a World Full of Action Choices. Annu Rev Neurosci.</p><p>Downing PE, Wiggett AJ, Peelen MV (2007) Functional magnetic resonance imaging investigation of overlapping lateral occipitotemporal activations using multi-voxel pattern analysis. The Journal of neuroscience : the official journal of the Society for Neuroscience 27:226-233.</p><p>Downing PE, Chan AW, Peelen MV, Dodds CM, Kanwisher N (2006) Domain specificity in visual cortex. Cerebral cortex 16:1453-1461.</p><p>Eger E, Michel V, Thirion B, Amadon A, Dehaene S, Kleinschmidt A (2009) Deciphering cortical number coding from human brain activity patterns. Current biology: CB 19:1608-1615.</p><p>Gallivan JP, McLean DA, Smith FW, Culham JC (2011a) Decoding effector-dependent and effector-independent movement intentions from human parieto-frontal brain activity. Journal of Neuroscience 31:17149-17168.</p><p>Gallivan JP, McLean DA, Flanagan JR, Culham JC (2013) Where one hand meets the other: limb-specific and action-dependent movement plans decoded from preparatory signals in single human frontoparietal brain areas. The Journal of neuroscience : the official journal of the Society for Neuroscience 33:1991-2008.</p><p>Gallivan JP, McLean DA, Valyear KF, Pettypiece CE, Culham JC (2011b) Decoding action intentions from preparatory brain activity in human parieto-frontal networks. Journal of Neuroscience 31:9599-9610.</p><p>Harrison SA, Tong F (2009) Decoding reveals the contents of visual working memory in early visual areas. Nature 458:632-635.</p><p>Haxby JV, Gobbini MI, Furey ML, Ishai A, Schouten JL, Pietrini P (2001) Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science 293:2425-2430.</p><p>Haynes JD, Rees G (2006) Decoding mental states from brain activity in humans. Nature reviews Neuroscience 7:523-534.</p><p>Jeannerod M, Arbib MA, Rizzolatti G, Sakata H (1995) Grasping objects: the cortical mechanisms of visuomotor transformation. Trends in Neurosciences 18:314-320.</p><p>Kalaska JF (2009) From intention to action: motor cortex and the control of reaching movements. Adv Exp Med Biol 629:139-178.</p><p>Kamitani Y, Tong F (2005) Decoding the visual and subjective contents of the human brain. Nature neuroscience 8:679-685.</p><p>Kanwisher N (2010) Functional specificity in the human brain: a window into the functional architecture of the mind. Proceedings of the National Academy of Sciences of the United States of America 107:11163-11170.</p><p>Kriegeskorte N (2011) Pattern-information analysis: From stimulus decoding to computational-model testing. NeuroImage 56:411-421.</p><p>Levy I, Hasson U, Avidan G, Hendler T, Malach R (2001) Center-periphery organization of human object areas. Nature neuroscience 4:533-539.</p><p>Lillicrap TP, Scott SH (2013) Preference distributions of primary motor cortex neurons reflect control solutions optimized for limb biomechanics. Neuron 77:168-179.</p><p>Mahon BZ, Caramazza A (2009) Concepts and categories: a cognitive neuropsychological perspective. Annu Rev Psychol 60:27-51.</p><p>Mahon BZ, Caramazza A (2011) What drives the organization of object knowledge in the brain? Trends in cognitive sciences 15:97-103.</p><p>Mahon BZ, Milleville SC, Negri GA, Rumiati RI, Caramazza A, Martin A (2007) Action-related properties shape object representations in the ventral stream. Neuron 55:507-520.</p><p>Murata A, Gallese V, Luppino G, Kaseda M, Sakata H (2000) Selectivity for the shape, size, and orientation of objects for grasping in neurons of monkey parietal area AIP. Journal of neurophysiology 83:2580-2601.</p><p>Naselaris T, Kay KN, Nishimoto S, Gallant JL (2011) Encoding and decoding in fMRI. NeuroImage 56:400-410.</p><p>Norman KA, Polyn SM, Detre GJ, Haxby JV (2006) Beyond mind-reading: multi-voxel pattern analysis of fMRI data. Trends in cognitive sciences 10:424-430.</p><p>Orlov T, Makin TR, Zohary E (2010) Topographic representation of the human body in the occipitotemporal cortex. Neuron 68:586-600.</p><p>Pereira F, Botvinick M (2011) Information mapping with pattern classifiers: A comparative study. NeuroImage.</p><p>Pereira F, Mitchell T, Botvinick M (2009) Machine learning classifiers and fMRI: a tutorial overview. NeuroImage 45:S199-209.</p><p>Quinlan DJ, Culham JC (2007) fMRI reveals a preference for near viewing in the human parieto-occipital cortex. NeuroImage 36:167-187.</p><p>Rizzolatti G, Luppino G (2001) The cortical motor system. Neuron 31:889-901.</p></body></sub-article></article>