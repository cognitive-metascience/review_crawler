<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">44111</article-id><article-id pub-id-type="doi">10.7554/eLife.44111</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Fixation-pattern similarity analysis reveals adaptive changes in face-viewing strategies following aversive learning</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-127598"><name><surname>Kampermann</surname><given-names>Lea</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9016-6212</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-54765"><name><surname>Wilming</surname><given-names>Niklas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0663-9828</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-93963"><name><surname>Alink</surname><given-names>Arjen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6468-5449</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-16197"><name><surname>Büchel</surname><given-names>Christian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1965-906X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-99818"><name><surname>Onat</surname><given-names>Selim</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4782-5603</contrib-id><email>selim.onat@free-now.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Systems Neuroscience</institution><institution>University Medical Center Hamburg-Eppendorf</institution><addr-line><named-content content-type="city">Hamburg</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Neurophysiology and Pathophysiology</institution><institution>University Medical Center Hamburg-Eppendorf</institution><addr-line><named-content content-type="city">Hamburg</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University Feinberg School of Medicine</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Senior Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>22</day><month>10</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e44111</elocation-id><history><date date-type="received" iso-8601-date="2018-12-04"><day>04</day><month>12</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-09-17"><day>17</day><month>09</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Kampermann et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Kampermann et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-44111-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.44111.001</object-id><p>Animals can effortlessly adapt their behavior by generalizing from past aversive experiences, allowing to avoid harm in novel situations. We studied how visual information was sampled by eye-movements during this process called fear generalization, using faces organized along a circular two-dimensional perceptual continuum. During learning, one face was conditioned to predict a harmful event, whereas the most dissimilar face stayed neutral. This introduced an adversity gradient along one specific dimension, while the other, unspecific dimension was defined solely by perceptual similarity. Aversive learning changed scanning patterns selectively along the adversity-related dimension, but not the orthogonal dimension. This effect was mainly located within the eye region of faces. Our results provide evidence for adaptive changes in viewing strategies of faces following aversive learning. This is compatible with the view that these changes serve to sample information in a way that allows discriminating between safe and adverse for a better threat prediction.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>eye movements</kwd><kwd>aversive learning</kwd><kwd>fear generalization</kwd><kwd>faces</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SFB TRR 58</award-id><principal-award-recipient><name><surname>Kampermann</surname><given-names>Lea</given-names></name><name><surname>Büchel</surname><given-names>Christian</given-names></name><name><surname>Onat</surname><given-names>Selim</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SFB 936/A7</award-id><principal-award-recipient><name><surname>Wilming</surname><given-names>Niklas</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010665</institution-id><institution>H2020 Marie Skłodowska-Curie Actions</institution></institution-wrap></funding-source><award-id>753441</award-id><principal-award-recipient><name><surname>Alink</surname><given-names>Arjen</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel approach based on eye-movement patterns reveals adaptive changes during viewing of faces and fear generalization.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>To avoid costly situations, individuals must be able to rapidly predict future adversity based on previously learnt aversive associations, as well as actively sampled information from the environment. However, new situations are seldom the same as previously experienced ones. To judge whether a situation holds potential harm, a careful balance between stimulus generalization and selectivity is needed. While fear generalization makes it possible to promptly deploy defensive behavior when similar situations are encountered anew (<xref ref-type="bibr" rid="bib20">Guttman and Kalish, 1956</xref>; <xref ref-type="bibr" rid="bib26">Hovland, 1937</xref>; <xref ref-type="bibr" rid="bib45">Pavlov, 1927</xref>; <xref ref-type="bibr" rid="bib51">Shepard, 1987</xref>; <xref ref-type="bibr" rid="bib53">Spence, 1937</xref>; <xref ref-type="bibr" rid="bib54">Struyf et al., 2015</xref>; <xref ref-type="bibr" rid="bib57">Tenenbaum and Griffiths, 2001</xref>), selectivity ensures that only truly aversive stimuli are recognized as aversive (<xref ref-type="bibr" rid="bib36">Li et al., 2008</xref>; <xref ref-type="bibr" rid="bib42">Onat and Büchel, 2015</xref>), thus avoiding costly false alarms. In real-world situations adversity predictions are based on sensory samples collected through active exploration (<xref ref-type="bibr" rid="bib22">Henderson, 2003</xref>; <xref ref-type="bibr" rid="bib27">Itti and Koch, 2001</xref>). A central part of active exploration are eye-movements which can rapidly determine what information is available in a scene for recognizing adversity (<xref ref-type="bibr" rid="bib16">Dowd et al., 2016</xref>). Yet, it is not known in how far representations of adversity interact with active exploration during viewing of complex visual information. Here we investigated this question by comparing exploration strategies during viewing of faces before and after aversive learning.</p><p>The approach to study exploration of faces is motivated by the fact that faces are characterized by subtle idiosyncratic differences on the stimulus continuum. Humans explore such complex stimuli serially (<xref ref-type="bibr" rid="bib27">Itti and Koch, 2001</xref>; <xref ref-type="bibr" rid="bib41">Onat et al., 2014</xref>; <xref ref-type="bibr" rid="bib44">Parkhurst et al., 2002</xref>; <xref ref-type="bibr" rid="bib55">Tatler et al., 2005</xref>) and rely heavily on their overt attentional resources due to well documented perceptual bottleneck for forming detailed representations of complex scenes (<xref ref-type="bibr" rid="bib2">Ahissar and Hochstein, 2004</xref>; <xref ref-type="bibr" rid="bib25">Hochstein and Ahissar, 2002</xref>; <xref ref-type="bibr" rid="bib58">Treisman and Gelade, 1980</xref>). Therefore, eye-movement recordings can mirror strategies used for foraging information from different faces, which in turn informs us on the way how aversive learning changes the exploration of relevant stimuli (<xref ref-type="bibr" rid="bib16">Dowd et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Henderson and Hayes, 2018</xref>; <xref ref-type="bibr" rid="bib32">König et al., 2016</xref>).</p><p>This way, face viewing behavior offers an ideal test bed for investigating changes in active exploration strategies through learning. First, active viewing of faces is a key ability during daily social interactions (<xref ref-type="bibr" rid="bib11">Cerf et al., 2009</xref>; <xref ref-type="bibr" rid="bib19">End and Gamer, 2017</xref>) where detecting minute differences in the configuration of facial elements is crucial for inferring the identity or emotional content of a face (<xref ref-type="bibr" rid="bib1">Adolphs, 2008</xref>; <xref ref-type="bibr" rid="bib28">Jack et al., 2014</xref>; <xref ref-type="bibr" rid="bib48">Peterson and Eckstein, 2012</xref>). For humans, it is therefore a natural choice of stimuli to investigate how exploration strategies change with learning. Second, the universal spatial configuration of facial elements makes it easily possible to generate faces with subtle differences that globally form a perceptual similarity continuum (<xref ref-type="bibr" rid="bib17">Dunsmoor et al., 2011</xref>; <xref ref-type="bibr" rid="bib42">Onat and Büchel, 2015</xref>). These key features make it possible to use a task that mimics a real-world exploration context, and therefore offers the possibility to probe changes in exploration strategies with aversive learning along a parametrically controlled stimulus continuum.</p><p>We investigated fear generalization using a two-dimensional perceptual space (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) with faces arranged along a circle within this space (<xref ref-type="bibr" rid="bib10">Butter, 1963</xref>; <xref ref-type="bibr" rid="bib42">Onat and Büchel, 2015</xref>). By pairing one item with an aversive outcome and keeping the most dissimilar one neutral (opposite face separated by 180 degrees), we introduced an adversity gradient defined exclusively along one perceptual dimension. The perceptual space can therefore be decomposed into threat-<italic>specific</italic> and <italic>unspecific</italic> components (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, middle panel), where the latter models perceptual similarity independent of adversity. This set of stimuli made it possible to dissociate independent contributions of perceptual factors related to similarity as such, from those relevant for the prediction of threat.</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.44111.002</object-id><label>Figure 1.</label><caption><title>Fixation-pattern similarity analysis.</title><p>(<bold>A</bold>) eight exploration patterns (colored frames) from a representative individual overlaid on eight face stimuli (1 to 8) calibrated to span a circular similarity continuum across gender and identity dimensions. A pair of maximally dissimilar faces was randomly selected as CS+ (red) and CS– (cyan; see color wheel). Similarity between the eight faces was calibrated to have a perfect circular organization with lowest dissimilarity (blue) between neighbors, and highest dissimilarity (yellow) for opposing pairs. FPSA summarizes the similarity relationship between the eight exploration patterns as a symmetric 8 × 8 matrix (bottom right panel). 4th and 8th columns (and rows) are aligned with the CS+ and CS–, respectively. (<bold>B–E</bold>) Multidimensional scaling representation of four theoretical similarity relationships investigated with FPSA (top row). Each colored node represents one exploration pattern (red: CS+; cyan: CS–), where internode distances are proportional their dissimilarity (bottom row). Shaded nodes in (<bold>C–E</bold>) depict the pre-learning state in (<bold>B</bold>). Dissimilarity matrices are further decomposed onto basic similarity components (middle row) centered either on the CS+/CS– (specific component) or +90°/–90° faces (unspecific component). A third component shown in (<bold>E</bold>) is uniquely centered on the CS+ face (<italic>Gaussian</italic> component). In (<bold>B</bold>), equal contribution of basic components results in circularly similar exploration patterns. In (<bold>C</bold>), a stronger equal contribution results in a better global separation of all exploration patterns, that is expansion (denoted by radial arrows). In (<bold>D</bold>), a stronger contribution of the specific component results in a biased separation of exploration patterns specifically along the adversity gradient defined between the CS+ and CS– nodes. In (<bold>E</bold>), the Gaussian component centered on the CS+ face can specifically decrease the dissimilarity of exploration patterns for faces similar to the CS+, resulting in circularly shifted nodes (circular arrows) while preserving the global circularity of the similarity relationships.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44111.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Face stimuli.</title><p>Set of 8 faces that were calibrated to form a circular similarity continuum. Faces vary along the two dimensions of gender (vertical axis) and identity (horizontal axis). See <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> for the calibration process.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44111.004</object-id><label>Figure 1—figure supplement 2.</label><caption><title>Calibration of faces using a simple V1 model tuned to human psychophysics.</title><p>(<bold>A</bold>) Using the FaceGen software, 160 faces forming five concentric circles were generated with coordinates varying in gender and identity dimensions (connected black dots in the left panel). Maximally male faces are located at 12 o’clock direction and indicated with the male symbol. (<bold>B</bold>) V1 representations of faces were modeled according to <xref ref-type="bibr" rid="bib62">Yue et al. (2012)</xref>. This is illustrated for faces 69 and 93. The difference between these two faces resulted in a Euclidean distance of 110. The pair-wise Euclidean distance for all the 160 faces are shown in (<bold>C</bold>) as a dissimilarity matrix. The resulting dissimilarity matrix exhibits five major bands corresponding to five concentric circles. By applying MDS, we obtained the representational space of V1 shown in (A, right panel). Note that the most male face is 45° counter-clockwise rotated with respect to the main axes of in V1 representation. The mapping between FaceGen coordinates and V1 representational space thus involved a rotation and scaling which was captured by the matrix <italic>M</italic>. We therefore used the inverse of <italic>M</italic>, to achieve coordinates of perfect circularity based on this V1 model. This ensured that faces along the similarity continuum were characterized by controlled changes for every angular step based on the model used.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig1-figsupp2-v1.tif"/></fig></fig-group><p>We based our main analysis on multivariate eye-movement patterns and introduced a similarity-based multivariate method that we termed fixation-pattern similarity analysis (FPSA). Instead of focusing on how often arbitrary parts of a face are fixated (<xref ref-type="bibr" rid="bib24">Hessels et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Malcolm et al., 2008</xref>; <xref ref-type="bibr" rid="bib50">Schurgin et al., 2014</xref>) as in traditional fixation count-based approaches, FPSA uses simultaneously all available fixations to derive a similarity metric, similar to representation similarity analysis in brain imaging (<xref ref-type="bibr" rid="bib34">Kriegeskorte, 2008</xref>). This way, it respects the strong individuality of eye-movements, that is idiosyncrasy (<xref ref-type="bibr" rid="bib14">Coutrot et al., 2016</xref>; <xref ref-type="bibr" rid="bib29">Kanan et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Mehoudar et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Walker-Smith et al., 1977</xref>). Another benefit of this approach is that the circular organization of our stimuli allowed us to formulate hypotheses on how the similarity relationships between exploration patterns could change: More explicitly, we could model changes along the adversity-specific direction, while controlling for changes along the adversity independent direction defined purely by perceptual differences. This way, the latter, unspecific dimension served as within-subject control condition for changes independent from aversive learning. We hypothesized three possible scenarios on how aversive learning might change exploration along the two-dimensional stimulus space (<xref ref-type="fig" rid="fig1">Figure 1B–E</xref>, middle panels).</p><p>First, the <italic>Perceptual Expansion</italic> hypothesis predicts that exploration patterns should enable evaluation of perceptual similarity with the CS+ face. This would result in exploration strategies that strongly mirror the physical similarity relationships between faces (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) and lead potentially to globally increased dissimilarity following learning. As similarity information varies both on the specific as well as the unspecific dimensions, the <italic>Perceptual Expansion</italic> hypothesis predicts a stronger, but more importantly equal contribution of the underlying specific and unspecific components (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, middle panel).</p><p>Second, it is possible that aversive learning induces changes in eye-movements specifically along the adversity specific dimension, in a way that exploration strategies would be tailored to target locations that are discriminative of the CS+ and CS– faces. This would lead to exploration patterns becoming more similar for faces sharing similar features with the CS+ and CS– faces, while simultaneously predicting an increased dissimilarity between these two sets of exploration patterns (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Increased similarity only along the adversity-relevant dimension would then result in an ellipsoid representation of similarity relationships. Therefore, the <italic>Adversity Gradient hypothesis</italic> would lead to an increase of the adversity specific component without influencing the unspecific component (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, middle panel).</p><p>Third, exploration strategies could change so that viewing patterns are tailored to quickly identify the CS+. A new sensorimotor strategy exclusively for the adversity predicting face would lead to a localized change in the similarity relationships around the adversity-predicting CS+ face (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Thus, the hypothesis of <italic>CS+ Attraction</italic> predicts an increased similarity of exploration patterns for faces closely neighboring the CS+ face, decaying proportionally with increasing dissimilarity to the CS+ face. This strategy is not exclusive and does not directly map to the specific or unspecific components.</p><p>In sum, using FPSA we analyzed the similarity relationships between exploration patterns during viewing of faces. We provide first evidence that exploration patterns during viewing of faces can be adaptively tailored during generalization following aversive learning. First, aversive learning changed exploration patterns in subtle ways that were not captured by univariate fixation counts, for example in predefined regions of interest. Second, before learning, exploration patterns showed an approximately circular similarity structure that followed the physical stimulus similarity structure. Third, after learning the similarity structure changed specifically along the adversity gradient, indicating that CS+ and CS– exploration patterns were modified, while the similarity between other faces remained largely unchanged.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We created eight face stimuli that were organized along a circular similarity continuum characterized by subtle physical differences in facial elements across two dimensions (gender and identity; see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for stimuli). We calibrated the degree of similarity between faces using a simple model of the primary visual cortex known to mirror human similarity judgments (<xref ref-type="bibr" rid="bib62">Yue et al., 2012</xref>) (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> for calibration). The physical similarity relationship between all pair-wise faces conformed with a circular organization (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, top right panel), such that dissimilarity varied with angular difference between faces (lowest for left and right neighbors and highest for opposing faces) with equidistant angular steps. Participants (n = 74) freely viewed these faces before and after an aversive learning procedure (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) while we measured their eye-movements. During the conditioning phase, one of the eight faces was introduced as the CS+, being partially reinforced with an aversive outcome (UCS, mild electric shock in ~30% trials). The CS– was the face most dissimilar to the CS+ (separated by 180°) and was not reinforced. During the subsequent generalization phase, all faces were presented again and the CS+ continued to be partially reinforced to prevent extinction of the previously learnt association. These reinforced trials were excluded from the analysis. To ensure comparable arousal states between the baseline and generalization phases, we administered UCSs also during the baseline period, however they were fully predictable as their occurrence was indicated by a shock symbol (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Furthermore, we inserted null trials during all phases (i.e. trials without face presentation but otherwise exactly the same) in order to obtain reliable baseline levels for skin-conductance responses.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.44111.005</object-id><label>Figure 2.</label><caption><title>Aversive learning procedure and univariate generalization profiles.</title><p>(<bold>A</bold>) On every trial, one randomly selected face was presented for 1.5 s preceded by a fixation cross placed outside of the face on either the left or right side. In null trials, no face was shown (<italic>gray frame</italic>) resulting in a SOA of ~6 or~12 s. For each volunteer, a pair of most dissimilar faces was randomly selected as the CS+ (<italic>red</italic>) and CS– (<italic>cyan</italic>, see color wheel). During baseline, UCSs (shock sign) were completely predictable by the presentation of a triangular signboard. During conditioning and generalization, the CS+ face was paired with an aversive outcome in ~30% of trials allowing recording of responses from non-reinforced CS+ trials. (<bold>B</bold>) Group-level fear-tuning based on subjective ratings of UCS expectancy (n = 74) and SCR (n = 63) for different phases. Responses are aligned to the CS+ of each volunteer separately (errorbars: SEM across subjects). Black horizontal lines or curves indicate the winning model (p&lt;0.001, log-likelihood ratio test), that is horizontal null model or the Gaussian model. Gray shaded areas in SCR depict response amplitudes evoked by null trials (mean and 95% CI). Scatter plots show amplitude parameter of Gaussian fits (denoted by alpha symbol) for each volunteer. Horizontal lines within the scatterplots depict group-level means, asterisks indicate significant differences in α (compared to baseline phase, paired t-test, ***: p&lt;0.001).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig2-v1.tif"/></fig><sec id="s2-1"><title>Fear tuning profiles in subjective ratings and autonomic activity</title><p>As expected, the effect of learning was mirrored both in autonomous nervous system activity as well as subjective ratings of UCS expectancy (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). During the conditioning phase, skin-conductance responses (SCR) were on average 3.8 times higher for CS+ than CS– (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, middle panel bottom row, paired t-test, p&lt;0.001) indicating that the CS+ face gained a stronger aversive quality already during the conditioning phase. In line with this view, expectancy ratings gathered right at the end of the conditioning phase were also highest for the CS+ face (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, middle panel top row).</p><p>As in previous studies, we characterized fear generalization by computing fear tuning profiles based on subjective ratings and SCR. In both recording modalities, responses decayed with increasing dissimilarity to the CS+ face and reached minimal values for CS–. We modeled these with a Gaussian function centered on the CS+ face. At the group-level, model comparison favored the flat null model over the Gaussian in both recording modalities before learning (p=0.44 for SCR, p=0.17 for ratings, log-likelihood ratio test; black horizontal lines in <xref ref-type="fig" rid="fig2">Figure 2B</xref>). However, following the conditioning phase the Gaussian model fitted the data significantly better (comparison to flat null model, p&lt;0.001 for SCR and subjective ratings, log-likelihood ratio test). Fear-tuning profiles at the single-subject level were in agreement with the overall group-level picture. As Gaussian generalization curves on the group level can equally emerge from binary generalization profiles, for example a boxcar shape spanning the CS+ and neighbors of different degrees, we tested whether a Gaussian would explain single-subject tunings better than a binary profile (optimized with regards to amplitude, width and offset). The Gaussian explained both ratings and SCR profiles of single subjects better than the binary function (AIC<sub>Gauss</sub> &lt;AIC<sub>Bin</sub>, Ratings: 72/74 and 68/74 subjects in conditioning (median ΔAIC = −3.6) and generalization phase (median ΔAIC = −3.6), SCR: 58/63 subjects in the generalization phase (median ΔAIC = −3.9). Therefore, we summarized fear-tuning profiles of individual participants with the amplitude parameter of the fitted Gaussian function, which characterizes the modulation depth of fear tuning (i.e. the strength of fear tuning) after accounting for baseline shifts. The average amplitude parameter following the conditioning phase was significantly bigger than the baseline phase in both recording modalities (paired t-test, p&lt;0.001, see <xref ref-type="fig" rid="fig2">Figure 2B</xref>). In summary, univariate fear-tuning in SCR and subjective ratings confirmed that aversive learning was successfully established and transferred towards other perceptually similar stimuli.</p></sec><sec id="s2-2"><title>Multivariate fear tuning profiles in eye movements</title><p>We analyzed exploration behavior using fixation density maps (FDMs). These are two-dimensional histograms of fixation counts across space, smoothed and normalized to have unit sum (see Materials and methods section for details). When computed separately for different faces, FDMs indicate how much different parts of a given face receive attentional resources. For each volunteer, we computed a dissimilarity matrix that summarized all pairwise comparisons of FDMs (using 1 - Pearson correlation as a pattern distance measure) and averaged these after separately aligning them to each volunteer’s custom CS+ face (shown always at the 4<sup>th</sup> column and row in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Furthermore, in order to gather an intuitive understanding of learning-induced changes in the similarity geometry, we used multidimensional scaling (jointly computed on the 16 × 16 matrices, that is all pairwise combinations of 8 conditions from baseline and generalization phase). Multidimensional scaling (MDS) summarizes similarity matrices by transforming observed dissimilarities as closely as possible onto distances between different nodes (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) representing different viewing patterns in 2D, therefore making it easily understandable at a descriptive level.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.44111.006</object-id><label>Figure 3.</label><caption><title>Fixation-pattern similarity analysis.</title><p>(<bold>A</bold>) Dissimilarity matrices of exploration patterns for baseline (<italic>left panel</italic>) and generalization phases (<italic>right panel</italic>). Fourth and eight columns (and rows) are aligned with each volunteer’s CS+ and CS– faces, respectively. Asterisks on the upper diagonal denote significant differences from the corresponding element in baseline. (<bold>B</bold>) Two-dimensional multidimensional scaling method used for visualization of the 16 × 16 dissimilarity matrix (not shown) comprising baseline and generalization phases. Distances between nodes are proportional to the dissimilarity between corresponding FDMs (<italic>open circles</italic>: baseline; <italic>filled circles</italic>: generalization phase; same color scheme as in <xref ref-type="fig" rid="fig1">Figure 1</xref>). (<bold>C</bold>) Bar plots (M ± SEM) depict predictor weights estimated for single-participants before (white bars) and after (gray bars) aversive learning (<italic>Left:</italic> Perceptual Expansion Model; <italic>Middle:</italic> Adversity Gradient Model; <italic>Right:</italic> CS+ Attraction Model). <italic>w</italic><sub>circle</sub>: weight for the circular component, which is the sum of equally weighted specific and unspecific components; <italic>w</italic><sub>specific</sub>/<italic>w</italic><sub>unspecific</sub>: weights for specific and unspecific components; <italic>w</italic><sub>Gauss</sub>: weight for Gaussian component centered uniquely on the CS+. (**: p&lt;0.01; ***: p&lt;0.001, paired <italic>t-test</italic>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44111.007</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Correlation between FPSA anisotropy and tuning strength.</title><p>Correlation between single subject’s model parameters from the winning Adversity Gradient model and (<bold>a</bold>) behavioral (rating) and (<bold>b</bold>) autonomous (SCR) outcomes. FPSA anisotropy in the individual fixation pattern is the difference between the specific and the unspecific component, that is higher values correspond to stronger ellipsoidness along the specific dimension within one individual. The tuning strength is given by the amplitude parameter α of the Gaussian fit on the respective outcome (cf. amplitude parameters in <xref ref-type="fig" rid="fig2">Figure 2B</xref>). The gray line depicts the least-squares fit line.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44111.008</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Model parameters for Adversity Gradient Model across subsequent test runs.</title><p>Beta estimates from the Adversity Gradient model of the specific and unspecific component when modeling data from the three generalization phase (‘Gen’) runs independently. The model estimates were computed on single subject dissimilarity matrices (cf. <xref ref-type="fig" rid="fig1">Figure 1</xref>). Shown is the average weight for the whole group, error bars indicate the SEM across subjects. Asterisks indicate significant differences of beta estimates of single generalization run compared to baseline phase (paired t-tests, ***: p<sub>corr</sub> &lt;0.001), showing that the unspecific component did not increase significantly from baseline to any of the generalization runs. Results of a repeated measures ANOVA (experimental run ×predictor (spec/unspec)): main effect of predictor (F(1, 73)=3.8, p=0.056, interaction of predictor (spec/unspec) with experimental run F(1,73) = 2.6, p=0.054).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44111.009</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Gaussian Mixture Models on individual anisotropy effects.</title><p>(<bold>A</bold>) Bayesian Information Criterion (BIC) for Gaussian Mixture Model (GMM) of n components based on single subject anisotropy values (baseline corrected, that is (w<sub>specific_gen</sub>-w<sub>unspecific_gen</sub>) – (w<sub>specific_base</sub>-w<sub>unspecific_base</sub>)). (<bold>B</bold>) Histogram of single subject anisotropy values (same as in A). Lines represent probability density functions of the resulting Gaussian distributions of the GMM on n = 1 (red, ‘GMM1’) and n = 2 (blue, ‘GMM2’) components, which are shown in detail as these models resulted in the lowest BIC (BIC<sub>GMM1</sub> = −23.7, BIC<sub>GMM2</sub> = −22.9). As visible, GMM1 resulted in one component centered on μ = 0.07, while GMM2 resulted in two Gaussians centered on μ<sub>1</sub> = −0.48 = and μ<sub>2</sub> = 0.09 with mixing proportions <italic>mp</italic><sub>1</sub> = 0.04 and <italic>mp</italic><sub>2</sub> = 0.96.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig3-figsupp3-v1.tif"/></fig></fig-group><p>Already during the baseline period the dissimilarity matrix was highly structured (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). In agreement with a circular similarity geometry and the MDS depiction, lowest dissimilarity values (1.05 ± 0.01; M ± SEM) were found between FDMs of neighboring faces (i.e. first off-diagonal), whereas FDMs for opposing faces separated by 180° exhibited significantly higher dissimilarity values (1.21 ± 0.01; paired t-test, <italic>t</italic>(73) = 7.41, p&lt;0.001). Using the Perceptual Baseline Model (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, 1<sup>st</sup> column), we investigated the contribution of physical characteristics of the stimulus set to the observed pre-learning dissimilarity structure. This model uses a theoretically circular similarity matrix (consisting of equally weighted sums of specific and unspecific components) as a linear predictor (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, theoretical similarity matrix). This model performed significantly better compared to a null model consisting of a constant similarity for all pairwise FDMs comparisons (for Perceptual Model adjusted <italic>r</italic><sup>2</sup> = 0.09; log-likelihood-ratio test for the alternative null model: p&lt;10<sup>−5</sup>; BIC<sub>NullModel</sub> = −96.1, BIC<sub>Perceptual</sub> = −244.7; see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1A</xref> for the results of model fitting). We additionally fitted the Perceptual Model for every volunteer separately (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, first white bar). Model parameters at the aggregate level were significantly different from zero (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; <italic>w</italic> <sub>Circle</sub> = 0.09 ± 0.01, M ± SEM; <italic>t</italic>(73) = 7.99, p&lt;0.001, BIC<sub>Perceptual</sub> = −3.5 ± 1.8, M ± SEM) indicating that exploration strategies prior to learning mirrored the physical similarity structure of the stimulus set. This provides evidence that fixation selection strategies are, at least to some extent, guided by physical stimulus properties during viewing of neutral faces.</p><p>We observed significant changes between baseline and generalization dissimilarity values in an element-wise comparison (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, indicated by asterisks). This provides evidence for learning-induced changes in the similarity relationships. Following learning, the circular Perceptual Model was again significant (adjusted <italic>r</italic><sup>2</sup> = 0.35; p&lt;0.001, log-likelihood ratio test), but now performed better compared to the baseline phase (BIC<sub>Perceptual</sub> = −244.7 for the baseline vs. BIC<sub>Perceptual</sub> = −1697.4 for the generalization phase; see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1B</xref> for model fitting results). Critically, we found a significant increase in the model parameter from baseline to generalization phase (<italic>w</italic> <sub>Circle</sub> = 0.13 ± 0.01; paired <italic>t</italic>-test, <italic>t</italic>(73) = 4.03, p&lt;0.001; <xref ref-type="fig" rid="fig3">Figure 3C</xref> compare two leftmost bars) suggesting a global increase in dissimilarity between FDMs. Overall, these results are compatible with the view that aversive learning led to a better separation of exploration patterns globally, in agreement with the Perceptual Expansion Model (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>However, as already visible in the multi-dimensional scaling method (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), the separation between exploration patterns occurred mainly along the adversity gradient defined by the CS+ and CS– faces, whereas the separation along the orthogonal unspecific direction did not exhibit any noticeable changes. Regarding individual dissimilarity values, this effect was subtle, with an increase of average dissimilarity on the specific dimension from 1.21 ± 0.02 in baseline to 1.27 ± 0.02 in the generalization phase (t(73) = 2.54, p&lt;0.05, <italic>paired t-test</italic>),as compared to the orthogonal dimension (1.21 ± 0.03 vs. 1.20 ± 0.02, t(73) = −﻿0.34, p = 0.73, <italic>paired t-test</italic>,). Accordingly, in the generalization phase, dissimilarity along the specific dimension was bigger than along the unspecific dimension (1.27 ± 0.03 vs. 1.20 ± 0.02, t(73) = 2.76, p&lt;0.01, <italic>paired t-test</italic>). We thus extended the circular Perceptual Model to capture independent variance along the two orthogonal directions using the Adversity Gradient Model (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Model comparison indicated that the more flexible model performed better than the simpler Perceptual Expansion Model (BIC<sub>Perceptual</sub> = −1697.4 vs. BIC<sub>Adversity</sub>. = −1957.5; adjusted <italic>r</italic><sup>2</sup> = 0.48 vs. 0.35; see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1C–D</xref> for fitting results with the Adversity Gradient model on baseline and generalization phases, respectively). A two-factor (experimental phase ×predictor type) repeated measures ANOVA showed a significant main effect of experimental phase (i.e. before vs. after conditioning, F(1, 73) = 6.29, p&lt;0.001), as well as a significant interaction of predictor (specific vs. unspecific) with experimental phase F(1,73) = 8.53, p&lt;0.005). Accordingly, post-hoc analyses showed that while the unspecific component did not change from baseline to generalization phase (t(73) = 0.75, p = 0.45), the specific component gained stronger contribution (t(73) = 4.64, p&lt;0.001), and these learning-induced changes were significantly larger in the specific as compared to unspecific component (<italic>t</italic>(73) = 2.92, p&lt;0.005, paired t-test). This observation provides evidence that increased overall dissimilarity was driven by changes in the scanning behavior specifically along the task-relevant adversity direction. We will refer to this difference in dissimilarity (w<sub>specific</sub> - w<sub>unspecific</sub>) as <italic>anisotropy</italic>. To explore whether the effect was stable across individual subjects, we analyzed the prevalence of the effect, that is in how many subjects the anisotropy increased from baseline (base) to generalization (gen) phase (w<sub>specific_gen</sub>-w<sub>unspecific_gen</sub> &gt; w<sub>specific_base</sub>-w<sub>unspecific_base</sub>). 48 subjects out of 74 (65%) showed stronger anisotropy of scanning patterns after learning. Next, we studied whether explicit learning of shock contingencies predicted this increase in anisotropy and found that within the subjects whose shock expectancy ratings indicated successful conditioning (CS+&gt;CS-, n<sub>learner</sub> = 61), the prevalence was slightly higher (n<sub>anisotropy</sub> = 42, i.e. 69%).</p><p>Given that only a portion of subjects showed the anisotropy effect, we tested whether this could be interpreted as evidence for model heterogeneity in the population. We thus employed a Gaussian mixture model analysis to fit the distribution of observed anisotropy values. We found that no Gaussian mixture model of more than one component outperformed the single component model (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). While the one and two component model were comparable in their BIC, the additional Gaussian component of the more complex model covered only a very small mixing proportion at the lower end of the distribution (4%, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3b</xref>). Thus, we conclude that the present data does not provide enough evidence for the presence of multiple subpopulations.</p><p>The remodeling of the similarity geometry along the adversity gradient can also be accompanied by exploration strategies that are specifically tailored for the adversity predicting face but not for CS– resulting in localized changes only around the CS+ face. We subjected this view to model comparison by augmenting the previous model with a similarity component that consisted of a two-dimensional Gaussian centered on the CS+ face. Positive contribution of this predictor would lead to more similar exploration patterns specifically around the CS+ (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). It can thus capture changes in similarity relationships that are specific to the CS+ face. The model comparison procedure favored the simpler Adversity Gradient model over the augmented CS+ Attraction Model (BIC<sub>Adversity</sub>. = −1957.5 vs. BIC<sub>CS+Attraction</sub>. = −1923.6 during the generalization phase; adjusted <italic>r</italic><sup>2</sup> = 0.48; see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1E–F</xref> for fitting results with CS+ Attraction Model in baseline and generalization phases, respectively). Hence the increase in the number of predictors did not result in a significant reduction in explained variance. In line with this result, the parameter estimates for the CS+ centered, Gaussian component were not significantly different from zero neither in baseline or generalization phases (w<sub>Gaussian</sub> = −0.009 ± 0.034 in baseline, <italic>t</italic>(73) = −0.27, p=0.78; w<sub>Gaussian</sub> = 0.02 ± 0.02 in generalization, <italic>t</italic>(73) = 0.74, p=0.46; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). Also, pair-wise differences between parameter estimates did not reach significance (t(73) = 0.72, p=0.47). We therefore conclude that further extensions of the Adversity Gradient model to include components for adversity-specific changes around the CS+, did not result in a better understanding of the adversity-induced changes in the similarity geometry of exploration strategies.</p><p>Due to our conditioning procedure, subjects explored CS+ and CS- faces more often than to the other faces. Therefore, the increase found in the specific component could be explained by repeated exposure of these faces. We tested this view with two different approaches. First, we evaluated whether the observed anisotropy (w<sub>specific</sub> - w<sub>unspecific</sub>) in the similarity geometry was relevant for the aversive quality associated with the CS+ face. We hypothesized that stronger anisotropy could predict stronger aversive learning, as measured with the modulation depth of fear-tuning profiles coming from subjective ratings and skin-conductance responses. We found weak, but significant evidence for an association with an increased tuning strength in the ratings (r = 0.25, p=0.03), which was only marginally present for SCR (r = 0.23, p=0.07) (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Second, if the increase in the specific component from baseline to test was merely driven by the exposure during the conditioning phase, the unspecific component would also increase when subjects were repeatedly exposed to the stimuli of the unspecific component over the three runs of the generalization phase. However, the weights of the unspecific component did not increase when analyzed for separate runs, but if anything showed a trend to decrease with further exposure (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Both findings suggest that separation of exploration patterns along the adversity gradient was related to aversive learning and not to exposure differences.</p></sec><sec id="s2-3"><title>Spatial changes in exploration strategy</title><p>While the FPSA detected increased similarity in eye-movement patterns dominantly along the specific component, it aggregates information across all spatial units and does therefore not give information where and how subjects sample information differently after aversive learning. We thus sought to complement our FPSA analysis in a model-free way and to further elucidate how spatial exploration strategies changed. To this end, we aimed to test whether viewing behavior was predictive of whether subjects observed CS+ or CS- stimuli (or a pair of faces along the orthogonal dimension). We trained support vector machines (SVM, <xref ref-type="fig" rid="fig4">Figure 4A</xref>) to decode CS+ from CS- trials and −90° and +90° trials in each subject and tested whether classification accuracy differed between faces along the adversity gradient and orthogonal to the adversity gradient. This analysis can be linked to the found anisotropy of eye-movements, as the anisotropy suggests the classification accuracy along the specific dimension to exceed accuracy along the unspecific dimension due to stronger dissimilarities.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.44111.010</object-id><label>Figure 4.</label><caption><title>Classification of FDMs using support vector machines.</title><p>(<bold>A</bold>) Classification of single trial FDMs using support vector machines. Before training, single FDMs of CS+ and CS- trials were reduced in their dimensionality (pixels) by a principal component analysis (PCA), resulting in a new representation in this feature space (red and cyan dots). A SVM was then trained to decode CS+ from CS- trials by finding a multidimensional hyperplane (dotted lines) separating CS+ from CS- patterns. The same was repeated for stimuli from the unspecific dimension (not shown). (<bold>B</bold>) Accuracy of SVM classification along the specific and unspecific dimension, trained within- subject for baseline (white) and generalization phase (gray) (M ± SEM, ***: t(73) = 4.8, p&lt;0.001, *: t(73) = 2.1, p&lt;0.05 paired <italic>t-test)</italic> (see <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>) (<bold>C</bold>) Activation patterns derived from hyperplanes of classification of CS+ vs CS- trials as shown in (<bold>A</bold>). Activation patterns were z-scored and averaged across subjects with the same underlying physical CS+ face<sub>i</sub> (n<sub>i</sub> = [10, 12, 8, 10, 8, 10, 8, 8]), then superimposed on the respective face stimulus (see <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44111.011</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Physical differences in opposing faces.</title><p>Difference maps for each pair of opposing faces taken from the circular similarity continuum shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. For visualization purposes difference maps were inverted so that darker shades indicate stronger differences.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig4-figsupp1-v1.tif"/></fig></fig-group><p>SVMs were able to decode CS+ vs CS- trials with an accuracy of 56% in the generalization phase compared to 50% in the baseline phase (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The unspecific direction (−90° vs 90°) could be decoded with an accuracy of 54% during the generalization phase and 52% during the baseline phase. A repeated measures ANOVA showed a significant interaction of generalization phase and component (F(72) = 5.4, p=0.02) and a main effect of phase (F(73) = 16.8, p=0.0001). These effects are compatible with FPSA results and reinforce the notion that changes were subtle. We also tested whether training SVMs across subjects instead of within subjects would reproduce this effect. Here the interaction between generalization phase and component was not significant (F(73)=0.43, p=0.51), suggesting that changes in viewing behavior are idiosyncratic and not necessarily comparable across subjects. Compatible with this, a traditional ROI based count analysis also showed no results (see below).</p><p>Next, we were interested to see which locations in a face were informative for the classification process. To this aim, we extracted hyperplanes from each subjects’ classification, converted them to activation patterns (<xref ref-type="bibr" rid="bib21">Haufe et al., 2014</xref>) and averaged activation patterns of subjects with the same CS+ face. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows these activation patterns superimposed on different physical faces. Qualitatively speaking activation patterns were remarkably different across faces. Furthermore, comparing activation patterns on a CS+/CS- combination with those activation patterns that have the opposite CS-/CS+ combination (compare top and bottom row in <xref ref-type="fig" rid="fig4">Figure 4C</xref>), suggested that these pairs are to some extent symmetric. In any case, all activation maps showed subtle changes along the eye region that qualitatively varied with the combination of CS+ and CS- face. Yet, physical differences between stimuli were not only present in the eye region, but spanned the whole face (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Moreover, these physical differences were also present for decoding along the orthogonal dimension, for which classification results were lower. Therefore, sampling more differently within the eye region could not be explained simply by the physical information they held.</p></sec><sec id="s2-4"><title>Temporal and spatial unfolding of adversity-specific exploration</title><p>While SCRs and subjective ratings provide insights about the aggregate cognitive evaluations of a given stimulus, eye-movements have the potential to provide information on how these cognitive evaluations unfold over both spatial and temporal domains. Aiming to explore the changes induced from aversive learning further with regards to these dimensions, we repeated the FPSA and fitted the Adversity Gradient Model on data slices from different temporal or spatial windows. For each temporal or spatial slice, we computed an anisotropy index, corresponding to the difference between specific and unspecific model parameters (w<sub>specific</sub> – w<sub>unspecific</sub>), and statistically evaluated the difference between before and after learning. For this analysis, we focused on participants that were able to correctly identify the CS+ based on their shock expectancy ratings (CS+&gt;CS–, n = 61).</p><p>For the temporal analysis, we used a moving window of 500 ms with steps of 50 ms and (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). While the time-course of adversity-specific and unspecific components was not distinguishable before learning, they diverged rather early in the subsequent generalization phase. The difference in anisotropy reached significance first at the time window corresponding to the interval 400–900 ms after stimulus onset (<xref ref-type="fig" rid="fig5">Figure 5A</xref> <italic>top row</italic>, paired t-test, p=0.03). As humans explore visual scenes serially with fixational eye-movements, the order of fixations (1<sup>st</sup> fixation, 2<sup>nd</sup> fixation, and so on) is another natural metric to evaluate temporal progress (<xref ref-type="bibr" rid="bib55">Tatler et al., 2005</xref>). The same analysis indicated that adversity-specific exploration started following the first fixation (note that the first fixation is the landing fixation on the face following stimulus onset; <xref ref-type="fig" rid="fig5">Figure 5A</xref>) and stayed constant during the stimulus presentation. Overall, the temporal FPSA indicated that humans started to forage for adversity-relevant information early, as soon as after the first landing fixation.</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.44111.012</object-id><label>Figure 5.</label><caption><title>Spatio-temporal fixation-pattern similarity analysis.</title><p>(<bold>A</bold>) Temporal development of adversity-specific and unspecific exploration strategies (n = 61). Parameters of the adversity categorization model (<italic>red</italic>: specific; <italic>blue</italic>: unspecific) are computed using a moving time window of 500 ms at intervals of 50 ms for baseline (<italic>left panels</italic>) and generalization phase (<italic>right panel</italic>). Numbers on the x-axis denote the center of the moving window. Second row depicts the same analysis with fixation points sorted by rank. Asterisks indicate time points with statistically significant interaction testing for difference in anisotropy (w<sub>specific</sub> – w<sub>unspecific</sub>) between test vs. baseline (*: p&lt;0.05; Shaded area: SEM) (<bold>B</bold>) Four maps resulting from the searchlight-FPSA on FDMs from before and after conditioning (<italic>left vs. right columns)</italic> and for unspecific and specific (<italic>top vs. bottom rows</italic>) model parameters overlaid on an average face. The map is masked to contain 90% of all fixation density. (<bold>C</bold>) Difference of anisoptropy between before and after aversive learning (<italic>generalization – baseline</italic>) for three different ROIs (*: p&lt;0.01, paired t-test).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44111.013</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Generalization profiles on ROI-based fixation counts.</title><p>Group-level fear-tuning based on percentage of fixations in a given ROI (n = 74) for different phases. Fixation count data expressed as percentages are based on three major facial regions, that is eyes, nose and mouth (see <xref ref-type="fig" rid="fig5">Figure 5</xref>). Responses are aligned to the CS+ of each volunteer separately (errorbars: SEM across subjects) and normalized to a mask comprising the three ROIs. Black horizontal lines or curves indicate the winning model (p&lt;0.001, log-likelihood ratio test), that is horizontal null model or the Gaussian model. In all situations, the null horizontal model was favored by the model comparison procedure. Scatter plots show amplitude parameter α of single-subject Gaussian fits. Horizontal lines within the scatterplots depict group-level means, asterisks indicate significant differences in α (compared to baseline phase, paired t-test, ***: p&lt;0.001, **: p&lt;0.01).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44111.014</object-id><label>Figure 5—figure supplement 2.</label><caption><title>Generalization profiles of common fixation features.</title><p>Common fixation features, such as fixation duration, number of fixations (fixation N), saccade distance and average entropy in FDMs for baseline and generalization phase. Features were extracted from single trial FDMs, z-scored within subject and experimental phase, then averaged across participants. Error bars depict the SEM across subjects. The black horizontal curve indicates where features a significantly tuned to the CS+ (p&lt;0.001, log-likelihood ratio test, Gaussian model vs. flat null-model).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig5-figsupp2-v1.tif"/></fig></fig-group><p>We ran spatial FPSA at localized portions of the FDMs in a similar manner to a searchlight analysis in brain imaging (<xref ref-type="bibr" rid="bib33">Kriegeskorte et al., 2007</xref>). For a given spatial portion (defined by a square window of 30 pixels,~1 visual degree), we fitted the Adversity Gradient Model, and assigned specific and unspecific weights to the center position of the searchlight (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). The map that is obtained by repeating this analysis at all spatial locations provides an indication of the facial locations that are explored either with a specific or unspecific exploration strategy. We found that both before and after learning, specific and unspecific components were strongly localized around the eye region (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). We tested the difference in anisotropy between the baseline and generalization phase within the three commonly used regions of interest at different facial elements (eyes, nose and mouth; ROIs shown in <xref ref-type="fig" rid="fig5">Figure 5C</xref>) (<xref ref-type="bibr" rid="bib24">Hessels et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Malcolm et al., 2008</xref>; <xref ref-type="bibr" rid="bib50">Schurgin et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Walker-Smith et al., 1977</xref>). We found this interaction to be significant only in the eye region (<xref ref-type="fig" rid="fig5">Figure 5C</xref>; paired t-test uncorrected, p&lt;0.024). This result could be explained by an increased number of fixations around the eye region following learning and potentially result in a stronger signal-to-noise ratio for parameter estimation (<xref ref-type="bibr" rid="bib15">Diedrichsen et al., 2011</xref>). Indeed, the eye region in this study accounted for ~72% of all fixations on the face . However, the increased anisotropy during generalization occurred despite a decrease of ~5% in fixation density around the same region (<italic>top row</italic> in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). This precludes the trivial explanation in terms of increased saliency of the eye region as a potential explanation for differences in similarity components. Overall, searchlight FPSA indicated that exploration strategies were specifically tailored to forage for adversity-specific information around the eye region, which corroborated the findings obtained by the SVM analysis described above.</p></sec><sec id="s2-5"><title>Comparison of FPSA to common eye-movement features and ROI-based analyses</title><p>As our model-based FPSA as well as machine-learning approaches are rather complex analyses, we aimed to compare these methods to classical approaches used for the analysis of eye movements. In an explorative approach, we tested whether aversive learning induced changes in classical measures of viewing behavior, such as changes in the number of fixations, their duration, saccade length and the entropy of individual FDMs for all eight faces. To be able to compare these features across subjects, they were z-scored within phase and subject, then averaged across subjects to check for commonalities in these individual profiles. Descriptive plots are shown in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>. We found that the number of fixations, the average saccade distance and entropy showed significant differences between CS+ and CS-, all of which were higher for the CS+. The only measure exhibiting a significant generalization profile, that is bell-shaped tuning curve, after aversive learning was the saccade length (p&lt;0.001, log-Likelihood Ratio Test of Gaussian against flat null-model). While simple outcome measures such as fixation and saccade characteristics changed over the course of the experiment, their univariate nature prevents us from correcting for changes not induced by aversive learning - that is developed along the unspecific component. To control for this and to connect the findings to the anisotropy found by FPSA, we set up a linear model testing whether the anisotropy in these measures (i.e. specific – unspecific) could predict the anisotropy of eye movements in the subjects. However, none of the features was able to robustly predict anisotropy (F(4,69) = 0.075, p=0.55, R<sup>2</sup><sub>adjusted</sub> = -0.01).</p><p>We also compared FPSA on eye movement patterns to common ROI-based analyses on fixation counts. We computed changes in fixation counts in the three common regions of face stimuli, that is eyes, nose and mouth (same as depicted <xref ref-type="fig" rid="fig5">Figure 5C</xref>). If conditioning before the generalization phase lead to an increased saliency of facial features that are diagnostic of the CS+ face, one would expect a non-flat fear tuning in the number of fixations towards these facial features, which would receive more fixations with increasing similarity to the CS+ face. In line with previous reports (<xref ref-type="bibr" rid="bib59">Walker-Smith et al., 1977</xref>), eyes together with the nose region were the most salient locations across the baseline and generalization phases, and attracted ~91% of all fixation density, whereas the mouth region had only a marginal contribution with ~3.5%. Investigating changes from baseline to generalization phase, we found that aversive learning increased the number of fixations directed at the nose (+3.6%) and mouth (+0.6%) regions at the expense of the eye region (–5.1%). Most importantly, model comparison on fixation density favored the flat null model for all regions even at low statistical thresholds in all facial elements across both baseline and generalization phases (p&gt;0.05, log-likelihood test; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). While a weak tuning was apparent in the mouth region during generalization, this did not reach significance (p&gt;0.1). Therefore, our observations at the group-level were limited to unspecific changes in fixation density across phases, that were independent of the adversity gradient introduced through conditioning.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The present work tested how aversive learning changed the exploration of faces following aversive learning. We used a stimulus continuum that was defined by perceptual similarity, as well as adversity-related information as two independent perceptual factors. As an extension of similarity-based multivariate pattern analyses used to investigate representational content of neuronal activity in fMRI (<xref ref-type="bibr" rid="bib34">Kriegeskorte, 2008</xref>) and MEG/EEG (<xref ref-type="bibr" rid="bib13">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Kietzmann et al., 2017</xref>), we introduced FPSA to characterize learning-induced changes in eye-movement behavior during free viewing of faces. As expected, before aversive learning exploration patterns mirrored the inherent circularity of stimulus similarity. This is compatible with the view that the exploration of neutral faces in our experimental settings is, at least to some extent, guided by physical characteristics, in contrast to a completely holistic viewing strategy (<xref ref-type="bibr" rid="bib48">Peterson and Eckstein, 2012</xref>). Aversive learning led to a specific increase in dissimilarity along the direction of the induced adversity gradient. This adversity-specific exploration strategy appeared early, as soon as the landing fixation, and lasted continuously for the duration of stimulus presentation.</p><p>It is an ongoing debate, what kind of information drives generalization after aversive learning. According to one prominent view, the perceptual model, the degree to which a novel stimulus is considered as harmful is directly related to the degree of overlap between shared sensory features with a previously learnt harmful stimulus. An alternative view proposes that fear generalization is an active cognitive act (<xref ref-type="bibr" rid="bib51">Shepard, 1987</xref>), related specifically to the prediction of potential threat in uncertain situations (<xref ref-type="bibr" rid="bib42">Onat and Büchel, 2015</xref>). According to the threat-prediction model, perceptual factors can contribute to fear generalization, but only to the extent they are predictive of harmful events (<xref ref-type="bibr" rid="bib35">Lashley and Wade, 1946</xref>; <xref ref-type="bibr" rid="bib51">Shepard, 1987</xref>; <xref ref-type="bibr" rid="bib57">Tenenbaum and Griffiths, 2001</xref>; <xref ref-type="bibr" rid="bib4">Baddeley et al., 2007</xref>; <xref ref-type="bibr" rid="bib52">Soto et al., 2014</xref>). Yet, in the majority of previous studies, perceptual similarity between the generalization and learning samples has been explicitly used as a cue for signaling the threat. As a result, threat-prediction has commonly been confounded by perceptual similarity, making it impossible to dissociate their independent contributions. The finding that fixation strategies change incrementally along the dimension that is predictive of threat, in our case captured by the anisotropy of specific vs unspecific components, offers support for the threat-prediction model.</p><p>Still, there are at least two different scenarios that are compatible with a selective separation of exploration patterns for the CS+ and CS– faces (<xref ref-type="fig" rid="fig6">Figure 6</xref>). This regards the way how learning potentiates sensory features as being diagnostic for the prediction of harmful outcomes, thereby making them target locations for attentional allocation. In the first scenario, aversive learning potentiates combinations of visual features that are specific to the harm-predicting item, namely the CS+ face identity (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, <italic>square box</italic>). In an alternative view, aversive learning consists of recovering the vector of features that defines the adversity gradient (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, <italic>black arrow</italic>), leading to an increased saliency for discriminative features that separate best the harm- and safety-predicting prototypes. This feature vector could overlap with categorical knowledge that is either naturally present (such as gender, ethnicity or emotional expression), or learned with experience (<xref ref-type="bibr" rid="bib18">Dunsmoor and Murphy, 2015</xref>; <xref ref-type="bibr" rid="bib31">Kietzmann and König, 2010</xref>; <xref ref-type="bibr" rid="bib49">Qu et al., 2016</xref>). While both scenarios lead to an increased separation between the CS+ and CS– poles, as observed in the present study, they have divergent predictions when tested with stimuli organized in three concentric circles (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Potentiation of identity-specific representations would lead faces that are similar to the CS+ on outer and inner circles to be explored similarly, resulting in a shrinkage of the similarity geometry around the CS+ face and thereby leading to a shift in the center of ellipses (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, <italic>right panel</italic>). On the other hand, if learning consists of forming a vector representation of the adversity gradient, the separation would add to differences that are already present, resulting in three concentric ellipses centered on the same point. Future investigations can distinguish these two hypotheses by testing the presence of a shift in the center of similarity geometry towards the CS+ face or not.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.44111.015</object-id><label>Figure 6.</label><caption><title>Predictions on the influence of aversive learning on sensory-motor foraging strategies.</title><p>Before learning, exploration strategies follow the circularity of the stimulus space (<italic>orange circle</italic>) in line with their presumed physical characteristics. (<bold>A</bold>) In one scenario, conditioning leads humans to learn the specific feature values that predict a harmful outcome (<italic>black square</italic>). When tested subsequently with stimuli organized as three concentric stimulus gradients, this scenario predicts faces that are similar to the CS+ face to be explored similarly. This results in a global shift in the center of gravity towards the CS+ face as indicated by <italic>yellow, orange</italic> and <italic>red horizontal dashes</italic> indicating the center of corresponding ellipses. (<bold>B</bold>) Humans learn the feature vector (<italic>black arrow</italic>) that best separates harmful and safety predicting stimuli. When tested with the concentric circular stimulus set, this scenario predicts three concentric ellipses sharing the same center of gravity (<italic>gray horizontal line</italic>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44111-fig6-v1.tif"/></fig><p>In either case, selective changes in the similarity of exploration patterns are compatible with theoretical work, that has shown that generalization can be viewed as the formation of an internal model of the environment in order to predict the occurrence of behaviorally relevant events. Shepard’s generalization theory conceives generalization as the formation of a binary categorical zone along a smooth perceptual continuum for the prediction of harmful or safe events (<xref ref-type="bibr" rid="bib51">Shepard, 1987</xref>). In this view, empirical generalization profiles can be understood as resulting from different stimuli to be registered into either the one or the other category in a probabilistic manner. In the same line, Bayesian frameworks refer explicitly to probability distributions to represent internal beliefs for the formation of generalization strategies (<xref ref-type="bibr" rid="bib57">Tenenbaum and Griffiths, 2001</xref>; <xref ref-type="bibr" rid="bib56">Tenenbaum et al., 2006</xref>). These theoretical works have already pointed to the possibility that humans can adaptively tailor generalization strategies by appropriately structuring their internal beliefs even in situations which are not defined by perceptual factors. Our results provide supporting evidence that these mechanisms have their validity even in settings that are defined solely by perceptual factors.</p><p>Introduction of the FPSA methodology was not a choice, but rather a necessity to test the outlined competing hypotheses. First, eye-movement patterns during free-viewing of faces are characterized by strong inter-individual variability (<xref ref-type="bibr" rid="bib14">Coutrot et al., 2016</xref>; <xref ref-type="bibr" rid="bib29">Kanan et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Mehoudar et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Walker-Smith et al., 1977</xref>). This was also true in the present study, where we could train linear classifiers to predict a volunteer’s identity with an average accuracy of ~80% based on fixation maps (results not shown). This is possibly one major reason why we did not observe adversity-tuned generalization profiles at different facial ROIs (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). While this inter-individual variability in exploration patterns can dilute the sensitivity of ROI-based approaches, FPSA exploits consistent changes in the similarity relationships of exploration patterns that were defined within-subject. Second, and equally importantly, using multivariate responses allowed to assess changes in similarity between all pairs of stimuli. It is not clear how the same could be achieved with subjective ratings or SCR measurements that are collapsed over time.</p><p>For humans, faces are a rich source of information for judging potential outcomes in social interactions. Face stimuli were thus a natural choice to investigate how active exploration strategies change with learning. Together with the FPSA method, a two dimensional circular continuum of faces allowed us to test different hypotheses that would have not been easily possible using a one-dimensional perceptual gradient. However, the circular continuum was based on two arbitrary dimensions (identity and gender). Any prior experiences that participants had in real-life situations with these dimensions might potentially bias our results. For example, males could generally be perceived more dangerous than females (<xref ref-type="bibr" rid="bib40">Navarrete et al., 2009</xref>). Therefore, any real-life categories present in the stimulus set might induce prior expectations about the potential harmfulness that have to be relearned during the aversive conditioning process. Yet, as the face chosen as CS+ was carefully counterbalanced across participants, we consider the individual impact of arbitrarily chosen dimensions to cancel out across subjects.</p><p>On the group-level, we found the adversity gradient model to be most compatible with observed changes in exploration patterns. While the majority of participants was in line with this view, on the individual level, many participants actually did not exhibit the effect of interest. We tested a possible heterogeneity of our participant sample, which was not supported by a Gaussian mixture model analysis. Future investigations should aim to understand whether this type of subtle effects results from motivational aspects during this type of learning tasks, or a genuine source of noise in the adversity learning and its interaction with exploration behavior. In any case, the rather small effects call for replication out of sample, as well as further investigations testing the predictions derived from our data. The above mentioned design of multiple circles as stimulus material (<xref ref-type="fig" rid="fig6">Figure 6</xref>) would allow to replicate, verify or falsify the predictions made in this study, as three circles would allow more specific and stronger predictions that could then be tested on an independent sample.</p><p>Could our results be explained by an unbalanced exposure to the CS+ and CS– faces that were presented during the conditioning phase? As participants have seen these faces more often, one can argue that this could potentially bias eye movement patterns. While we cannot completely exclude a contribution of exposure, we have shown that anisotropy correlates with indicators of aversive learning as measured by subjective ratings and, to lesser extent with autonomous activity. Furthermore, the unspecific component did not increase with exposure over multiple runs, which would be expected if it was solely driven by exposure. Therefore, we believe that the major drive that leads to the separation of patterns along the specific axis are due to the affective nature of learning, rather than occurring merely as a result of exposure.</p><p>Eye-movements patterns can provide important insights about what the nervous system tries to achieve as they summarize the final outcome of complex interactions at the neuronal level (<xref ref-type="bibr" rid="bib32">König et al., 2016</xref>). Our results demonstrate that changes induced by aversive generalization extend beyond autonomous systems or explicit subjective evaluations, but can also affect an entire sensory-motor loop at the systems level (<xref ref-type="bibr" rid="bib16">Dowd et al., 2016</xref>). Furthermore, the methodology applied here can easily be extended to neuronal recordings, where gradients of activity during generalization have been successfully used to characterize selectivity of aversive representations. Therefore, it will be highly informative to test different hypotheses we outlined here using neuronal recordings with representational similarity analysis during the emergence of aversive representations.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Participants were 74 naïve healthy males and females (n = 37 each) with normal (or corrected-to-normal) vision (age = 27 ± 4, M ± SD) and without history of psychiatric or neurological diseases, any medical condition or use of medication that would alter pain perception. Participants had not participated in any other study using facial stimuli in combination with aversive learning before. They were paid 12 Euros per hour for their participation in the experiment and provided written informed consent. All experimental procedures were approved by the Ethics committee of the General Medical Council Hamburg.</p></sec><sec id="s4-2"><title>Data sharing</title><p>The dataset used in this manuscript has been published as a dataset publication (<xref ref-type="bibr" rid="bib61">Wilming et al., 2017</xref>). We publicly provide the stimuli as well as the Matlab (MathWorks, Natick MA) code necessary for the reproduction of all the results including figures presented in this manuscript (<xref ref-type="bibr" rid="bib43">Onat and Kampermann, 2017</xref>). The code can be used to download the data as well. Code used for linear support vector machines (see below) is accompanying this publication as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p></sec><sec id="s4-3"><title>Stimulus preparation and calibration of generalization gradient</title><p>Using a two-step procedure, we created a final set of 8 calibrated faces (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, see also <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) that were perceptually organized along a circular similarity continuum based on a model of the primary visual (V1) cortex. Using the FaceGen software (FaceGen Modeller 2.0, Singular Inversion, Ontario Canada) we created two gender-neutral facial identities and mixed these identities (0%/100% to 100%/0%) while simultaneously changing the gender parameters in two directions (more male or female). In the first step, we created a total of 160 faces by appropriately mixing the gender and identity parameters to form five concentric circles (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) based on FaceGen defined parameter values for gender and identity. Using a simple model of the primary visual cortex known to closely mirror human perceptual similarity judgments (<xref ref-type="bibr" rid="bib62">Yue et al., 2012</xref>), we computed V1 representations for each face after converting them to grayscale. The spatial frequency sensitivity of the V1 model was adjusted to match human contrast sensitivity function with bandpass characteristics between 1 and 12 cycles/degree, peaking at six cycles/degrees (<xref ref-type="bibr" rid="bib7">Blakemore and Campbell, 1969</xref>). The V1 model consists of pair of Gabor filters in quadrature at five different spatial scales and eight orientations. The activities of these 40 channels were averaged in order to obtain one single V1 representation per face. We characterized the similarity relationship between the V1 representations of 160 faces using multidimensional scaling analysis with two dimensions (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). As expected, while two dimensions explained a large variance, the improvement with the addition of a third dimension was only minor, providing thus evidence that the physical properties of the faces were indeed organized along two-dimensions (stress values for 1D, 2D and 3D resulting from the MDS analysis were 0.42, 0.04, 0.03, respectively). The transformation between the coordinates of the FaceGen software values (gender and identity mixing values) and coordinates returned by the MDS analysis allowed us to gather FaceGen coordinates that would correspond to a perfect circle in the V1 model. In the second step, we thus generated eight faces that corresponded to a perfect circle. This procedure ensured that faces used in this study were organized perfectly along a circular similarity continuum according to a simple model of primary visual cortex with well-defined bandpass characteristics known to mirror human similarity judgments. Furthermore, it ensured that dimensions of gender and identity introduced independent variance on the faces.</p><p>To present these stimuli we resized them to 1000 × 1000 pixels (originals: 400 × 400) using bilinear interpolation, and slightly smoothed with a Gaussian kernel of 5 pixels with full-width at half maximum of 1.4 pixels to remove any possible pixel artifacts that could potentially lead participants to identify faces. Faces were then normalized to have equal luminance and root-mean-square contrast. The gray background was set to the same luminance level ensuring equal brightness throughout of the experiment. Faces were presented on a 20’ monitor (1600 × 1200 pixels, 60 Hz) using Matlab R2013a (Mathworks, Natick MA) with psychophysics toolbox (<xref ref-type="bibr" rid="bib8">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib47">Pelli, 1997</xref>). The distance of the participants’ eyes to the stimulus presentation screen was 50 cm. The center of the screen was at the same level as the participants’ eyes. Faces spanned horizontally ~17° and vertically ~30°, aiming to mimic a typical face-to-face social situation. Stimuli are available in <xref ref-type="bibr" rid="bib43">Onat and Kampermann (2017)</xref>.</p></sec><sec id="s4-4"><title>Experimental paradigm</title><p>The fear conditioning paradigm (similar to <xref ref-type="bibr" rid="bib42">Onat and Büchel, 2015</xref>) consisted of baseline, conditioning and test (or generalization) phases (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Participants were instructed that the delivery of UCSs during baseline would not be associated with faces, however in the following conditioning and generalization phases they were instructed that shocks would be delivered after particular faces have been presented. In all three phases, subjects were instructed to press a button when an oddball stimulus appeared on the screen.</p><p>Four equivalent runs with exactly same number of trials were used during baseline (one run) and generalization phases (three runs) consisting of 120 trials per run (~10 min). Every run started with an eye-tracker calibration. Between runs participants took a break and continued with the next run in a self-paced manner. We avoided having more than one run in the baseline period in order not to induce fatigue in participants. At each run during the baseline and generalization phases, eight faces were repeated 11 times, UCS trials occurred five times and one oddball was presented. This consisted of a blurred unrecognizable face, which volunteers were instructed to press a key. We presented 26 null trials with no face presentation but otherwise the same trial structure (see below sequence optimization). In order to keep arousal levels comparable to the generalization phase, UCSs were also delivered during baseline, however they were fully predictable by a shock symbol therefore avoiding any face to UCS associations.</p><p>During the conditioning phase, participants saw only the CS+ and the CS– faces (and null trials). These consisted of 2 maximally dissimilar faces separated by 180° on the circular similarity continuum and randomly assigned for every participant in a balanced manner. The conditioning was 124 trials long (~10 min) and CS+ and CS– faces were repeated 25 times. CS+ faces were additionally presented 11 times with the UCSs, resulting in a reinforcement rate of ~30%. The same reinforcement ratio was used during the subsequent generalization phase in order to avoid extinction of the learnt associations.</p><p>Stimulus presentation sequence was optimized for the deconvolution of the skin-conductance responses. This regards the choice of conditions and the duration of interstimulus intervals. Faces were presented using a rapid-event design with a stimulus onset asynchrony of 6 s and stimulus duration of 1.5 s. The presentation sequence was optimized using a modified m-sequence with 11 different conditions (<xref ref-type="bibr" rid="bib9">Buracas and Boynton, 2002</xref>; <xref ref-type="bibr" rid="bib37">Liu and Frank, 2004</xref>) (eight faces, UCS, oddball, null). An m-sequence is preferred as it balances all transitions from condition <italic>n</italic> to <italic>m</italic> (thus making the sequence as unpredictable as possible for the participant) while providing an optimal design efficiency (thus making deconvolution of autonomic skin conductance responses more reliable). However, all conditions in an m-sequences appear equally number of times. Therefore, in order to achieve the required reinforcement ratio (~30%), we randomly pruned UCS trials and transformed them to null trials. Similarly, oddball trials were pruned to have an overall rate of ~1%. This resulted in a total of 26 null trials. While this deteriorated the efficiency of the m-sequence, it was still a good compromise as the resulting sequence was much more efficient than a random sequence. Resulting from the intermittent null trials, SAOs were 6 or 12 s approximately exponentially distributed.</p><p>Face onsets were preceded by a fixation-cross, which appeared randomly outside of the face either on the left or right side along an virtual circle (r = 19.6°, + /- 15° above and below the horizontal center of the image). The side of fixation-cross was balanced across conditions to avoid confounds that might occur (<xref ref-type="bibr" rid="bib3">Arizpe et al., 2015</xref>). Therefore, the first fixation consisted of a landing fixation on the face.</p></sec><sec id="s4-5"><title>Calibration and delivery of electric stimulation</title><p>Mild electric shocks were delivered by a direct current stimulator (Digitimer Constant Current Stimulator, Hertfordshire UK), applied by a concentric electrode (WASP type, Speciality Developments, Kent UK) that was firmly connected to the back of the right hand and fixated by a rubber glove to ensure constant contact with the skin. Shocks were trains of 5 ms pulses at 66 Hz, with a total duration of 100 ms. During the experiment, they were delivered right before the offset of the face stimulus. The intensity of the electric shock applied during the experiment was calibrated for each participant before the start of the experiment. Participants underwent a QUEST procedure (<xref ref-type="bibr" rid="bib60">Watson and Pelli, 1983</xref>) presenting UCSs with varying amplitudes selected by an adaptive algorithm and were required to report whether a given trial was ‘painful’ or ‘not painful’ in a binary fashion using a sliding bar. The QUEST procedure was repeated twice to account for sensitization/habituation effects, thus obtaining a reliable estimate. Each session consisted of 12 stimuli, starting at an amplitude of 1mA. The subjective pain threshold was the intensity that participants would rate as ‘painful’ with a probability of 50%. The amplitude used during the experiment was two times this threshold value. Before starting the actual experiment, participants were asked to confirm whether the resulting intensity was bearable. If not then the amplitude was incrementally reduced and the final amplitude was used for the rest of the experiment.</p></sec><sec id="s4-6"><title>Eye tracking and fixation density maps</title><p>Eye tracking was done using an Eyelink 1000 Desktop Mount system (SR Research, Ontario Canada) recording the right eye at 1000 Hz. Participants placed their head on a headrest supported under the chin and forehead to keep a stable position. Participants underwent a 13 point calibration/validation procedure at the beginning of each run (1 Baseline run, 1 Conditioning run and 3 runs of Generalization). The average mean-calibration error across all runs was Mean = 0.36°, Median = 0.34°, SD = 0.11. 91% of all runs had a calibration better than or equal to. 5°.</p><p>Fixation events were identified using commonly used parameter definitions (<xref ref-type="bibr" rid="bib61">Wilming et al., 2017</xref>) (Eyelink cognitive configuration: saccade velocity threshold = 30°/second, saccade acceleration threshold = 8000° per second<sup>2</sup>, motion threshold = 0.1°). Fixation density maps (FDMs) were computed by spatially smoothing (Gaussian kernel of 1° of full width at half maximum) a 2D histogram of fixation locations, and were transformed to probability densities by normalizing to unit sum. FDMs included the center 500 × 500 pixels, including all facial elements where fixations were mostly concentrated (~95% of all fixations).</p></sec><sec id="s4-7"><title>Shock expectancy ratings and autonomic recordings</title><p>After baseline, conditioning and generalization phases, participants rated different faces for subjective shock expectancy by answering the following question, ‘<italic>How likely is it to receive a shock for this face?</italic>”. Faces were presented in a random order and rated twice. Subjects answered using a 10 steps scale ranging from ‘<italic>very unlikely</italic>’ to ‘<italic>very likely</italic>’ and confirmed by a button press in a self-paced manner.</p><p>Electrodermal activity evoked by individual faces was recorded throughout the three phases. Reusable Ag/AgCl electrodes filled with isotonic gel were connected to the palm of the subject’s left hand using adhesive collars, placed in thenar/hypothenar configuration. Skin-conductance responses were continuously recorded using a Biopac MP100 AD converter and amplifier system at a sampling rate of 500 Hz. Using the Ledalab toolbox (<xref ref-type="bibr" rid="bib6">Benedek and Kaernbach, 2010b</xref>; <xref ref-type="bibr" rid="bib5">Benedek and Kaernbach, 2010a</xref>), we decomposed the raw data to phasic and tonic response components after downsampling it to 100 Hz. Ledalab applies a positively constrained deconvolution technique in order to obtain phasic responses for each single trial. We averaged single-trial phasic responses separately for each condition and experimental phase to obtained 21 average values (9 (8 faces + one null condition) from baseline and generalization and 3 (2 faces + one null condition) from the conditioning phase). CS+ trials with UCS were excluded from this analysis. These values were first log-transformed (log<sub>10</sub>(1+SCR)) and subsequently z-scored for every subject separately (across all conditions and phases), then averaged across subjects. Therefore, negative values indicate phasic responses that are smaller than the average responses recorded throughout the experiment. Due to technical problems, SCR data could only be analyzed for n = 63 out of the 74 participants.</p></sec><sec id="s4-8"><title>Nonlinear modeling and model comparison</title><p>We fitted a Gaussian function to generalization profiles obtained from subjective ratings, skin-conductance responses and fixation counts at different ROIs by minimizing the following likelihood term in (1) following an initial grid-search for parameters<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mrow><mml:mo>)</mml:mo><mml:mrow/><mml:mo>|</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>x</italic> represents signed angular distances from a given volunteer’s CS+ face; G(<italic>x</italic>|<italic>θ)</italic> is a Gaussian function that was used to model the adversity tuning. It is defined by the parameter vector <italic>θ,</italic> which codes for the amplitude (difference between peak and base) and width of the resulting generalization profile; D(<italic>x</italic>) represents the observed generalization profile for different angular distances; and N(x| 0, <italic>σ</italic>) is the normal probability density function with mean zero and standard deviation of <italic>σ</italic>. The fitting procedure consisted of finding parameters' values that minimized the sum of negative log-transformed probability values. Using log-likelihood ratio test we tested whether this model performed better than a null model consisting of a horizontal line, effectively testing the significance of the additional variance explained by the model. G(x) was in the form<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>α</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">G</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p><italic>α</italic> represents the depth of adversity tuning which corresponds to the difference between peak and baseline responses, <italic>σ<sub>G</sub></italic> controls the width of the tuning.</p><p>As Gaussian shaped generalization profiles on the group level can equally result from binary profiles on subject level, we compared model fits of individual Gaussian tunings to binary profiles. The binary profile was defined by an amplitude parameter <italic>α,</italic> width and offset (base value) parameter and can be found at Tuning.binary_freeY at <xref ref-type="bibr" rid="bib43">Onat and Kampermann (2017)</xref>.</p><p>When comparing model-fits of these two profiles in individual subjects, we compare the likelihoods L(D(<italic>x</italic>) | <italic>θ, σ</italic>) of Gaussians to binary fits in each subject using the Akaike Information Criterion (AIC) to penalize for the more complex binary function (df<sub>bin</sub> = 3 vs. df<sub>Gauss</sub> = 2).</p></sec><sec id="s4-9"><title>Fixation-pattern similarity analysis</title><p>FPSA was conducted on single participants. Condition specific FDMs (eight faces per baseline and generalization phases) were computed by collecting all fixations across trials on a single map which was then normalized to unit sum. We corrected FDMs by removing the common mean pattern (done separately for baseline and generalization phases). We used 1 - Pearson correlation as the similarity metric. This resulted in a 16 × 16 similarity matrix per subject. Statistical tests for element-wise comparison of the similarity values were conducted after Fisher transformation of correlation values. The multidimensional scaling was conducted on the baseline and generalization phases jointly using the 16 × 16 similarity matrix as input (<italic>mdscale</italic> in MATLAB). Importantly, as the similarity metric is extremely sensitive to the signal to noise ratio (<xref ref-type="bibr" rid="bib15">Diedrichsen et al., 2011</xref>) present in the FDMs, we took precautions that the number of trials between generalization and baseline phases were exactly the same in order to avoid differences that would have been caused by different signal to noise ratios. To account for unequal number of trials during the baseline (11 repetitions) and generalization (3 runs x 11 = 33 repetitions) phases, we computed a similarity matrix for each run separately in the generalization phase. These were later averaged across runs for a given participant. This ensured that FDMs of the baseline and generalization phases had comparable signal-to-noise ratios, therefore not favoring the generalization phase for having more trials.</p><p>We generated three different models based on a quadrature decomposition of a circular similarity matrix. A circular similarity matrix of 8 × 8 can be obtained using the term <bold>M</bold>⊗<bold>M</bold>, where <bold>M</bold> is a 8 × 2 matrix in form of [cos(<italic>x</italic>) sin(<italic>x</italic>)], and the operator ⊗ denotes the outer product. <italic>x</italic> represents angular distances from the CS+ face, is equal to 0 for CS+ and π for CS–. Therefore, while cos(<italic>x</italic>) is symmetric around the CS+ face, sin(<italic>x</italic>) is shifted by 90°. For the Perceptual Baseline and Perceptual Expansion models (<xref ref-type="fig" rid="fig1">Figure 1B and C</xref>) we used M⊗M as a predictor together with a constant intercept. For the Adversity Gradient model depicted in <xref ref-type="fig" rid="fig1">Figure 1D</xref>, we used cos(<italic>x</italic>) ⊗cos(<italic>x</italic>) and sin(<italic>x</italic>)⊗sin(<italic>x</italic>) to independently model ellipsoid expansion along the specific and unspecific directions, respectively. Together with the intercept this model comprised three predictors. Finally, the CS+ Attraction model (<xref ref-type="fig" rid="fig1">Figure 1E</xref>) was created using the predictors of the tuned exploration model in conjunction with a two-dimensional Gaussian centered on the CS+ face (in total four predictors). We tested different widths for the Gaussian and took the one that resulted in the best fit. This was equal to 65° of FWHM and similar to the values we observed for univariate explicit ratings and SCR responses.</p><p>All linear modeling was conducted using non-redundant, vectorized forms of the symmetric dissimilarity matrices. For an 8 × 8 dissimilarity matrix this resulted in a vector of 28 entries. Different models were fitted as mixed-effects, where intercept and slope contributed both as fixed- and random-effects (<italic>fitlme</italic> in Matlab). We selected mixed-effect models as these performed better than models defined uniquely with fixed-effects on intercept and slope. To do model selection, we used Bayesian information criterion (BIC) as it compensates for an increase in the number of predictors between different models. Additionally, different models were also fitted to single participants (<italic>fitlm</italic> in Matlab) and the parameter estimates were separately tested for significance using <italic>t-test</italic>.</p><p>For the analysis of temporal and spatial unfolding of adversity specific exploration patterns, the same analysis was run, but restricted to include only the given time windows/fixations. In this analysis, we only included participants who had a significant fear-tuning of explicit shock ratings from the generalization phase (n = 61). For the time-windowed approach, periods of 500 ms were used, repeated shifted by 50 ms, thereby obtaining a rolling-window analysis, while fixation-wise analyses were based on FDMs that included only on the given (1<sup>st</sup>, 2<sup>nd</sup>, and so on) fixation.</p></sec><sec id="s4-10"><title>Classification with linear support vector machine</title><p>We used single trial FDMs for validation of linear support vector machines (<xref ref-type="bibr" rid="bib12">Chang and Lin, 2011</xref>), that were trained to classify exploration patterns obtained during viewing of CS+ and CS– faces or orthogonal faces. Analyses using SVM were executed using scikit learn and python (<xref ref-type="bibr" rid="bib46">Pedregosa et al., 2011</xref>). The code has been included as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref> accompanying this publication. To reduce the dimensionality, FDMs were first downscaled 10 times resulting in a vector of 2500 pixels and by discarding pixels whose standard deviation across all conditions and subjects was much larger than its mean (mean/std &gt; 0.075). We further reduced dimensionality by projecting FDMs onto their principal components using N principal components that explained 50% of the total variance in each run. SVMs were trained with l2 regularization and for each individual subject to account for idiosyncrasy in scanning patterns. We used 3-fold cross validation and ensured that all parameters were exclusively estimated from training data. In total we evaluated four SVMs per subject, two with data from the baseline phase and two with data from the generalization phase. For visualization purposes, SVMs were also trained on 100% of the trials. We converted resulting hyperplanes to activation patterns (<xref ref-type="bibr" rid="bib21">Haufe et al., 2014</xref>) and averaged across subjects that shared the same combination of CS+ and CS- faces.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors wish to thank Tim Kietzmann for his input on an early version of this manuscript, Cliodhna Quigley for proof-reading, Helen Blank for comments, Lukas Neugebauer for helpful input on the comparison of different tuning profiles, Patricia Billaudelle and Katrin Harland for their assistance with data collection. Selim Onat is supported by the DFG SFB TRR 58.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Validation, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Validation, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Funding acquisition, Validation, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All experimental procedures were approved by the Ethics committee of the General Medical Council Hamburg (PV 4164). Participants had not participated in any other study using facial stimuli in combination with aversive learning before. They were paid 12 Euros per hour for their participation in the experiment and provided written informed consent.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><object-id pub-id-type="doi">10.7554/eLife.44111.016</object-id><label>Source code 1.</label><caption><title>Code implementing classification of single trials with linear support vector machines (see <xref ref-type="fig" rid="fig4">Figure 4</xref>).</title></caption><media mime-subtype="x-script.phyton" mimetype="text" xlink:href="elife-44111-code1-v1.py"/></supplementary-material><supplementary-material id="supp1"><object-id pub-id-type="doi">10.7554/eLife.44111.017</object-id><label>Supplementary file 1.</label><caption><title>Supplementary tables reporting mixed-effects modeling of the three models shown in <xref ref-type="fig" rid="fig1">Figure 1B–E</xref>.</title><p>(<bold>A</bold>) Mixed-effects modeling of the similarity matrices during the baseline phase with the Perceptual model shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. (<bold>B</bold>) Mixed-effects modeling of the similarity matrices during the generalization phase with the Perceptual model shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. (<bold>C</bold>) Mixed-effects modeling of the similarity matrices during the baseline phase with the Adversity Gradient model shown in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. (<bold>D</bold>) Mixed-effects modeling of the similarity matrices during the generalization phase with the Adversity Gradient model shown in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. (<bold>E</bold>) Mixed-effects modeling of the similarity matrices during the baseline phase with the CS+ Attraction model shown in <xref ref-type="fig" rid="fig1">Figure 1E</xref>. (<bold>F</bold>) Mixed-effects modeling of the similarity matrices during the generalization phase with the CS+ Attraction model shown in <xref ref-type="fig" rid="fig1">Figure 1E</xref>.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-44111-supp1-v1.docx"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.44111.018</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-44111-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All source data and analysis- and figure-generating scripts are available for download from the project home page at the Open Science Framework (at <ext-link ext-link-type="uri" xlink:href="https://osf.io/zud6h/">https://osf.io/zud6h/</ext-link>). Source code for the analyses shown in Figure 4 has been added to the manuscript as Source code 1.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Wilming</surname><given-names>N</given-names></name><name><surname>Onat</surname><given-names>S</given-names></name><name><surname>Ossandón</surname><given-names>J</given-names></name><name><surname>Acik</surname><given-names>A</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Kaspar</surname><given-names>K</given-names></name><name><surname>Gameiro</surname><given-names>RR</given-names></name><name><surname>Vormberg</surname><given-names>A</given-names></name><name><surname>König</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Data from: An extensive dataset of eye movements during viewing of complex images</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="accession" xlink:href="https://doi.org/10.5061/dryad.9pf75">10.5061/dryad.9pf75</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Onat</surname><given-names>S</given-names></name><name><surname>Kampermann</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Similarity Analysis of Fixation Patterns during Aversive Generalization</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/OSF.IO/ZUD6H</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adolphs</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Fear, faces, and the human amygdala</article-title><source>Current Opinion in Neurobiology</source><volume>18</volume><fpage>166</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2008.06.006</pub-id><pub-id pub-id-type="pmid">18655833</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahissar</surname> <given-names>M</given-names></name><name><surname>Hochstein</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The reverse hierarchy theory of visual perceptual learning</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>457</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.08.011</pub-id><pub-id pub-id-type="pmid">15450510</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arizpe</surname> <given-names>JM</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Characteristic visuomotor influences on eye-movement patterns to faces and other high level stimuli</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>1027</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.01027</pub-id><pub-id pub-id-type="pmid">26283982</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname> <given-names>RJ</given-names></name><name><surname>Osorio</surname> <given-names>D</given-names></name><name><surname>Jones</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Generalization of color by chickens: experimental observations and a bayesian model</article-title><source>The American Naturalist</source><volume>169 Suppl 1</volume><fpage>S27</fpage><lpage>S41</lpage><pub-id pub-id-type="doi">10.1086/510142</pub-id><pub-id pub-id-type="pmid">19426091</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benedek</surname> <given-names>M</given-names></name><name><surname>Kaernbach</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010a</year><article-title>A continuous measure of phasic electrodermal activity</article-title><source>Journal of Neuroscience Methods</source><volume>190</volume><fpage>80</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2010.04.028</pub-id><pub-id pub-id-type="pmid">20451556</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benedek</surname> <given-names>M</given-names></name><name><surname>Kaernbach</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010b</year><article-title>Decomposition of skin conductance data by means of nonnegative deconvolution</article-title><source>Psychophysiology</source><volume>146</volume><fpage>647</fpage><lpage>658</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2009.00972.x</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blakemore</surname> <given-names>C</given-names></name><name><surname>Campbell</surname> <given-names>FW</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>On the existence of neurones in the human visual system selectively sensitive to the orientation and size of retinal images</article-title><source>The Journal of Physiology</source><volume>203</volume><fpage>237</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1969.sp008862</pub-id><pub-id pub-id-type="pmid">5821879</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buracas</surname> <given-names>GT</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Efficient design of event-related fMRI experiments using M-sequences</article-title><source>NeuroImage</source><volume>16</volume><fpage>801</fpage><lpage>813</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1116</pub-id><pub-id pub-id-type="pmid">12169264</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butter</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Stimulus generalization along one and two dimensions in pigeons</article-title><source>Journal of Experimental Psychology</source><volume>65</volume><fpage>339</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1037/h0040258</pub-id><pub-id pub-id-type="pmid">14017413</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cerf</surname> <given-names>M</given-names></name><name><surname>Frady</surname> <given-names>EP</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Faces and text attract gaze independent of the task: experimental data and computer model</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/9.12.10</pub-id><pub-id pub-id-type="pmid">20053101</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>C-C</given-names></name><name><surname>Lin</surname> <given-names>C-J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIBSVM: a library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><fpage>27</fpage><pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coutrot</surname> <given-names>A</given-names></name><name><surname>Binetti</surname> <given-names>N</given-names></name><name><surname>Harrison</surname> <given-names>C</given-names></name><name><surname>Mareschal</surname> <given-names>I</given-names></name><name><surname>Johnston</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Face exploration dynamics differentiate men and women</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>16</elocation-id><pub-id pub-id-type="doi">10.1167/16.14.16</pub-id><pub-id pub-id-type="pmid">27893894</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diedrichsen</surname> <given-names>J</given-names></name><name><surname>Ridgway</surname> <given-names>GR</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Wiestler</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Comparing the similarity and spatial structure of neural representations: a pattern-component model</article-title><source>NeuroImage</source><volume>55</volume><fpage>1665</fpage><lpage>1678</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.01.044</pub-id><pub-id pub-id-type="pmid">21256225</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dowd</surname> <given-names>EW</given-names></name><name><surname>Mitroff</surname> <given-names>SR</given-names></name><name><surname>LaBar</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fear generalization gradients in visuospatial attention</article-title><source>Emotion</source><volume>16</volume><fpage>1011</fpage><lpage>1018</lpage><pub-id pub-id-type="doi">10.1037/emo0000197</pub-id><pub-id pub-id-type="pmid">27213724</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunsmoor</surname> <given-names>JE</given-names></name><name><surname>Prince</surname> <given-names>SE</given-names></name><name><surname>Murty</surname> <given-names>VP</given-names></name><name><surname>Kragel</surname> <given-names>PA</given-names></name><name><surname>LaBar</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neurobehavioral mechanisms of human fear generalization</article-title><source>NeuroImage</source><volume>55</volume><fpage>1878</fpage><lpage>1888</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.01.041</pub-id><pub-id pub-id-type="pmid">21256233</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunsmoor</surname> <given-names>JE</given-names></name><name><surname>Murphy</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Categories, concepts, and conditioning: how humans generalize fear</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>73</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.12.003</pub-id><pub-id pub-id-type="pmid">25577706</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>End</surname> <given-names>A</given-names></name><name><surname>Gamer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Preferential processing of social features and their interplay with physical saliency in complex naturalistic scenes</article-title><source>Frontiers in Psychology</source><volume>8</volume><elocation-id>418</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2017.00418</pub-id><pub-id pub-id-type="pmid">28424635</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guttman</surname> <given-names>N</given-names></name><name><surname>Kalish</surname> <given-names>HI</given-names></name></person-group><year iso-8601-date="1956">1956</year><article-title>Discriminability and stimulus generalization</article-title><source>Journal of Experimental Psychology</source><volume>51</volume><fpage>79</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1037/h0046219</pub-id><pub-id pub-id-type="pmid">13286444</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haufe</surname> <given-names>S</given-names></name><name><surname>Meinecke</surname> <given-names>F</given-names></name><name><surname>Görgen</surname> <given-names>K</given-names></name><name><surname>Dähne</surname> <given-names>S</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name><name><surname>Blankertz</surname> <given-names>B</given-names></name><name><surname>Bießmann</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title><source>NeuroImage</source><volume>87</volume><fpage>96</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.067</pub-id><pub-id pub-id-type="pmid">24239590</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Human gaze control during real-world scene perception</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>498</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2003.09.006</pub-id><pub-id pub-id-type="pmid">14585447</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname> <given-names>JM</given-names></name><name><surname>Hayes</surname> <given-names>TR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Meaning guides attention in real-world scene images: evidence from eye movements and meaning maps</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/18.6.10</pub-id><pub-id pub-id-type="pmid">30029216</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hessels</surname> <given-names>RS</given-names></name><name><surname>Kemner</surname> <given-names>C</given-names></name><name><surname>van den Boomen</surname> <given-names>C</given-names></name><name><surname>Hooge</surname> <given-names>IT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The area-of-interest problem in eyetracking research: a noise-robust solution for face and sparse stimuli</article-title><source>Behavior Research Methods</source><volume>48</volume><fpage>1694</fpage><lpage>1712</lpage><pub-id pub-id-type="doi">10.3758/s13428-015-0676-y</pub-id><pub-id pub-id-type="pmid">26563395</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochstein</surname> <given-names>S</given-names></name><name><surname>Ahissar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>View from the top: hierarchies and reverse hierarchies in the visual system</article-title><source>Neuron</source><volume>36</volume><fpage>791</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01091-7</pub-id><pub-id pub-id-type="pmid">12467584</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hovland</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="1937">1937</year><article-title>The generalization of conditioned responses: I. the sensory generalization of conditioned responses with varying frequencies of tone</article-title><source>The Journal of General Psychology</source><volume>17</volume><fpage>125</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1080/00221309.1937.9917977</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itti</surname> <given-names>L</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Computational modelling of visual attention</article-title><source>Nature Reviews Neuroscience</source><volume>2</volume><fpage>194</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1038/35058500</pub-id><pub-id pub-id-type="pmid">11256080</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jack</surname> <given-names>RE</given-names></name><name><surname>Garrod</surname> <given-names>OG</given-names></name><name><surname>Schyns</surname> <given-names>PG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic facial expressions of emotion transmit an evolving hierarchy of signals over time</article-title><source>Current Biology</source><volume>24</volume><fpage>187</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.11.064</pub-id><pub-id pub-id-type="pmid">24388852</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanan</surname> <given-names>C</given-names></name><name><surname>Bseiso</surname> <given-names>DNF</given-names></name><name><surname>Ray</surname> <given-names>NA</given-names></name><name><surname>Hsiao</surname> <given-names>JH</given-names></name><name><surname>Cottrell</surname> <given-names>GW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Humans have idiosyncratic and task-specific scanpaths for judging faces</article-title><source>Vision Research</source><volume>108</volume><fpage>67</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2015.01.013</pub-id><pub-id pub-id-type="pmid">25641371</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname> <given-names>TC</given-names></name><name><surname>Gert</surname> <given-names>AL</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name><name><surname>König</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Representational dynamics of facial viewpoint encoding</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>637</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01070</pub-id><pub-id pub-id-type="pmid">27791433</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname> <given-names>TC</given-names></name><name><surname>König</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perceptual learning of parametric face categories leads to the integration of high-level class-based information but not to high-level pop-out</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.1167/10.13.20</pub-id><pub-id pub-id-type="pmid">21106685</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>König</surname> <given-names>P</given-names></name><name><surname>Wilming</surname> <given-names>N</given-names></name><name><surname>Kietzmann</surname> <given-names>TC</given-names></name><name><surname>Ossandón</surname> <given-names>JP</given-names></name><name><surname>Onat</surname> <given-names>S</given-names></name><name><surname>Ehinger</surname> <given-names>BV</given-names></name><name><surname>Kaspar</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Eye movements as a window to cognitive processes</article-title><source>Journal of Eye Movement Research</source><volume>5</volume><elocation-id>3383</elocation-id><pub-id pub-id-type="doi">10.16910/jemr.9.5.3</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name><name><surname>Sorger</surname> <given-names>B</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Individual faces elicit distinct response patterns in human anterior temporal cortex</article-title><source>PNAS</source><volume>104</volume><fpage>20600</fpage><lpage>20605</lpage><pub-id pub-id-type="doi">10.1073/pnas.0705654104</pub-id><pub-id pub-id-type="pmid">18077383</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis – connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>2008</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lashley</surname> <given-names>KS</given-names></name><name><surname>Wade</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1946">1946</year><article-title>The pavlovian theory of generalization</article-title><source>Psychological Review</source><volume>53</volume><fpage>72</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1037/h0059999</pub-id><pub-id pub-id-type="pmid">21023320</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>W</given-names></name><name><surname>Howard</surname> <given-names>JD</given-names></name><name><surname>Parrish</surname> <given-names>TB</given-names></name><name><surname>Gottfried</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Aversive learning enhances perceptual and cortical discrimination of indiscriminable odor cues</article-title><source>Science</source><volume>319</volume><fpage>1842</fpage><lpage>1845</lpage><pub-id pub-id-type="doi">10.1126/science.1152837</pub-id><pub-id pub-id-type="pmid">18369149</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>TT</given-names></name><name><surname>Frank</surname> <given-names>LR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Efficiency, power, and entropy in event-related FMRI with multiple trial types. Part I: theory</article-title><source>NeuroImage</source><volume>21</volume><fpage>387</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.09.030</pub-id><pub-id pub-id-type="pmid">14741676</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malcolm</surname> <given-names>GL</given-names></name><name><surname>Lanyon</surname> <given-names>LJ</given-names></name><name><surname>Fugard</surname> <given-names>AJ</given-names></name><name><surname>Barton</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Scan patterns during the processing of facial expression versus identity: an exploration of task-driven and stimulus-driven effects</article-title><source>Journal of Vision</source><volume>8</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1167/8.8.2</pub-id><pub-id pub-id-type="pmid">18831625</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehoudar</surname> <given-names>E</given-names></name><name><surname>Arizpe</surname> <given-names>J</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name><name><surname>Yovel</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Faces in the eye of the beholder: unique and stable eye scanning patterns of individual observers</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1167/14.7.6</pub-id><pub-id pub-id-type="pmid">25057839</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarrete</surname> <given-names>CD</given-names></name><name><surname>Olsson</surname> <given-names>A</given-names></name><name><surname>Ho</surname> <given-names>AK</given-names></name><name><surname>Mendes</surname> <given-names>WB</given-names></name><name><surname>Thomsen</surname> <given-names>L</given-names></name><name><surname>Sidanius</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Fear extinction to an out-group face: the role of target gender</article-title><source>Psychological Science</source><volume>20</volume><fpage>155</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02273.x</pub-id><pub-id pub-id-type="pmid">19175529</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Onat</surname> <given-names>S</given-names></name><name><surname>Açık</surname> <given-names>A</given-names></name><name><surname>Schumann</surname> <given-names>F</given-names></name><name><surname>König</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The contributions of image content and behavioral relevancy to overt attention</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e93254</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0093254</pub-id><pub-id pub-id-type="pmid">24736751</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Onat</surname> <given-names>S</given-names></name><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The neuronal basis of fear generalization in humans</article-title><source>Nature Neuroscience</source><volume>18</volume><elocation-id>1811</elocation-id><pub-id pub-id-type="doi">10.1038/nn.4166</pub-id><pub-id pub-id-type="pmid">26571459</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Onat</surname> <given-names>S</given-names></name><name><surname>Kampermann</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Similarity analysis of fixation patterns during aversive generalization</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/ZUD6H</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkhurst</surname> <given-names>D</given-names></name><name><surname>Law</surname> <given-names>K</given-names></name><name><surname>Niebur</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Modeling the role of salience in the allocation of overt visual attention</article-title><source>Vision Research</source><volume>42</volume><fpage>107</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(01)00250-4</pub-id><pub-id pub-id-type="pmid">11804636</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pavlov</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1927">1927</year><source>Conditioned Reflexes an Investigation of the Physiological Activity of the Cerebral Cortex.</source><publisher-loc>London</publisher-loc><publisher-name>Oxford University Press Humphrey Milford</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>Varoquaux</surname> <given-names>G</given-names></name><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Michel</surname> <given-names>V</given-names></name><name><surname>Thirion</surname> <given-names>B</given-names></name><name><surname>Grisel</surname> <given-names>O</given-names></name><name><surname>Duchesnay</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in Python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>1825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname> <given-names>MF</given-names></name><name><surname>Eckstein</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Looking just below the eyes is optimal across face recognition tasks</article-title><source>PNAS</source><volume>109</volume><fpage>E3314</fpage><lpage>E3323</lpage><pub-id pub-id-type="doi">10.1073/pnas.1214269109</pub-id><pub-id pub-id-type="pmid">23150543</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qu</surname> <given-names>LP</given-names></name><name><surname>Kahnt</surname> <given-names>T</given-names></name><name><surname>Cole</surname> <given-names>SM</given-names></name><name><surname>Gottfried</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>De novo emergence of odor category representations in the human brain</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>468</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3248-15.2016</pub-id><pub-id pub-id-type="pmid">26758838</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schurgin</surname> <given-names>MW</given-names></name><name><surname>Nelson</surname> <given-names>J</given-names></name><name><surname>Iida</surname> <given-names>S</given-names></name><name><surname>Ohira</surname> <given-names>H</given-names></name><name><surname>Chiao</surname> <given-names>JY</given-names></name><name><surname>Franconeri</surname> <given-names>SL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Eye movements during emotion recognition in faces</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/14.13.14</pub-id><pub-id pub-id-type="pmid">25406159</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Toward a universal law of generalization for psychological science</article-title><source>Science</source><volume>237</volume><fpage>1317</fpage><lpage>1323</lpage><pub-id pub-id-type="doi">10.1126/science.3629243</pub-id><pub-id pub-id-type="pmid">3629243</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soto</surname> <given-names>FA</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Explaining compound generalization in associative and causal learning through rational principles of dimensional generalization</article-title><source>Psychological Review</source><volume>121</volume><fpage>526</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1037/a0037018</pub-id><pub-id pub-id-type="pmid">25090430</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname> <given-names>KW</given-names></name></person-group><year iso-8601-date="1937">1937</year><article-title>The differential response in animals to stimuli varying within a single dimension</article-title><source>Psychological Review</source><volume>44</volume><fpage>430</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1037/h0062885</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Struyf</surname> <given-names>D</given-names></name><name><surname>Zaman</surname> <given-names>J</given-names></name><name><surname>Vervliet</surname> <given-names>B</given-names></name><name><surname>Van Diest</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Perceptual discrimination in fear generalization: mechanistic and clinical implications</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>59</volume><fpage>201</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.11.004</pub-id><pub-id pub-id-type="pmid">26571437</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tatler</surname> <given-names>BW</given-names></name><name><surname>Baddeley</surname> <given-names>RJ</given-names></name><name><surname>Gilchrist</surname> <given-names>ID</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Visual correlates of fixation selection: effects of scale and time</article-title><source>Vision Research</source><volume>45</volume><fpage>643</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.09.017</pub-id><pub-id pub-id-type="pmid">15621181</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>Kemp</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Theory-based bayesian models of inductive learning and reasoning</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>309</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.05.009</pub-id><pub-id pub-id-type="pmid">16797219</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Generalization, similarity, and bayesian inference</article-title><source>Behavioral and Brain Sciences</source><volume>24</volume><fpage>629</fpage><lpage>640</lpage><pub-id pub-id-type="doi">10.1017/S0140525X01000061</pub-id><pub-id pub-id-type="pmid">12048947</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treisman</surname> <given-names>AM</given-names></name><name><surname>Gelade</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>A feature-integration theory of attention</article-title><source>Cognitive Psychology</source><volume>12</volume><fpage>97</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(80)90005-5</pub-id><pub-id pub-id-type="pmid">7351125</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker-Smith</surname> <given-names>GJ</given-names></name><name><surname>Gale</surname> <given-names>AG</given-names></name><name><surname>Findlay</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Eye movement strategies involved in face perception</article-title><source>Perception</source><volume>6</volume><fpage>313</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1068/p060313</pub-id><pub-id pub-id-type="pmid">866088</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname> <given-names>AB</given-names></name><name><surname>Pelli</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>QUEST: a bayesian adaptive psychometric method</article-title><source>Perception &amp; Psychophysics</source><volume>33</volume><fpage>113</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.3758/BF03202828</pub-id><pub-id pub-id-type="pmid">6844102</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilming</surname> <given-names>N</given-names></name><name><surname>Onat</surname> <given-names>S</given-names></name><name><surname>Ossandón</surname> <given-names>JP</given-names></name><name><surname>Açık</surname> <given-names>A</given-names></name><name><surname>Kietzmann</surname> <given-names>TC</given-names></name><name><surname>Kaspar</surname> <given-names>K</given-names></name><name><surname>Gameiro</surname> <given-names>RR</given-names></name><name><surname>Vormberg</surname> <given-names>A</given-names></name><name><surname>König</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An extensive dataset of eye movements during viewing of complex images</article-title><source>Scientific Data</source><volume>4</volume><elocation-id>160126</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2016.126</pub-id><pub-id pub-id-type="pmid">28140391</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname> <given-names>X</given-names></name><name><surname>Biederman</surname> <given-names>I</given-names></name><name><surname>Mangini</surname> <given-names>MC</given-names></name><name><surname>Malsburg</surname> <given-names>C</given-names></name><name><surname>Amir</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Predicting the psychophysical similarity of faces and non-face complex shapes by image-based measures</article-title><source>Vision Research</source><volume>55</volume><fpage>41</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.12.012</pub-id><pub-id pub-id-type="pmid">22248730</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.44111.024</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University Feinberg School of Medicine</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Threat prediction as the relevant factor for understanding fear generalization&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Richard Ivry as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This study uses eye movements collected from a large sample of human subjects (N=74) to test whether generalization of learned fear responses is driven by unspecific perceptual similarity or by perceptual similarity exclusively along dimensions that are relevant for distinguishing the CS+ from the CS-. The experimental design used 8 face stimuli which varied along two dimensions (gender and identity), spanning a circle. Subjects first viewed all 8 faces in a baseline session and then two faces (180 degree apart, thus varying only on one dimension) were associated with shock (CS+) and no-shock (CS-) in a conditioning session. Finally, subjects viewed all stimuli again in a generalization session. Fixation patterns in the baseline session were well predicted by a model of the circular organization of the stimulus set. However, pre-post conditioning changes were best explained by a model that separated the relevant and irrelevant dimension, and showed that changes occurred exclusively along the relevant stimulus dimension. This shows that aversive learning leads to specific changes only along the relevant stimulus dimension.</p><p>Overall the reviewers felt that this is an interesting study that uses a sophisticated analysis to address an important question. However, reviewers also had major concerns about the framing of the paper, the data and conclusions. Perhaps the most critical is the focus on generalization with limited ability to really test the two models of generalization using the current design, and the lack of evidence for the reproducibility of the findings.</p><p>Essential revisions:</p><p>1) The reviewers agreed that there is an element of this study which is potentially informative for models of generalization, but that it doesn't reduce to the simple competing/alternative theories set out by the authors because the study design is not optimal to test the hypothesis as stated. However, there was broad consensus among reviewers that the strongest and most interesting part of this paper is that fear conditioning directs eye movements to features of faces that are most relevant for predicting whether a shock will occur. This is related to the concept of active sensing (i.e., information-seeking) and conditioning driving attentional resources in the domain of face processing. This was the focus of the 2017 biorxiv paper (doi: doi.org/10.1101/125682), which concluded: &quot;These findings show that aversive learning can introduce substantial remodeling of exploration patterns in an adaptive manner during viewing of faces.&quot; This conclusion is much closer to and fully supported by the data. The reviewers would therefore encourage the authors to rewrite their manuscript (and change the title) such that it focuses on the core findings concerning the effect of conditioning on exploratory eye movements, and fully move potential implications for models of generalization to the Discussion.</p><p>2) With the new focus on the basic patterns of eye movements, it would be important to also include more descriptive data to show what features in the fixation patterns drive the dissimilarity between faces along the relevant and irrelevant dimension. For instance, fixation heatmaps, scan paths, saccade statistics, blink statistics, etc. might be useful.</p><p>3) Reviewers were concerned that the current design does not dissociate effects of fear conditioning from the effects of repeated exposure to the CS faces during conditioning. This could be ruled out by separately analyzing the first and second half of the generalization session, with the prediction that if the effects are driven by differential exposure, the effect of the unspecific component should increase over time (i.e., after subjects are also exposed to all other stimuli). In contrast, no changes in the unspecific component would suggest that the results are driven by aversive learning rather than exposure per se.</p><p>4) Although the study sample is large (N=74), there were concerns that because the results come from a complex analysis, they may not replicate out-of-sample. Ideally, the authors would collect new data and show that the effects replicate in an independent sample. Alternatively, they could use analytical approaches (k-fold cross-validation, etc.) to provide more evidence that the results are reliable and reproducible. In particular, it would be important to show that the pre-post conditioning difference in CS+ and in CS- gaze patterns, and their interaction, is robust and replicable.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Fixation-Pattern Similarity Analysis Reveals Adaptive Changes in Face-Viewing Strategies Following Aversive Learning&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Richard Ivry as the Senior Editor, and three reviewers, one of whom is a member of our Board of Reviewing Editors.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>Whereas reviewer 1 and 2 felt that the revised manuscript adequately addressed the initial concerns, reviewer 3 had some remaining comments. Based on our discussion, we ask:</p><p>1) The reviewers agreed that the manuscript would benefit from discussing two key limitations. These include an acknowledgement of the exploratory and relatively weak nature of the findings, and a discussion of possible disadvantages that come with the circular stimulus organization.</p><p>2) An additional concern regards the lack of an out-of-sample replication, which is compounded by the fact that only 65% of subjects behave according to the adversity gradient model. The reviewers are aware that collection of new data is not possible at this point. Instead, reviewer 3 recommends an additional analysis treating &quot;model identity&quot; as a random effect (e.g. using spm_BMS). In the event that the model comparison does not yield conclusive results, it should be discussed why this is the case. There are two possibilities why the group-level winning model only wins in 65% of the subjects. First, there is heterogeneity between subjects in the true mechanism. In this case, RFX is appropriate and should be used and reported. Or, the theoretical assumption is that they all use the same mechanism but data are too noisy for single-subject inference. It would be important if the authors could take a strong stance here (either model heterogeneity or noisy data), based on some theoretical considerations, and suggest appropriate future research steps to verify or falsify these conclusions.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.44111.025</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The reviewers agreed that there is an element of this study which is potentially informative for models of generalization, but that it doesn't reduce to the simple competing/alternative theories set out by the authors because the study design is not optimal to test the hypothesis as stated. However, there was broad consensus among reviewers that the strongest and most interesting part of this paper is that fear conditioning directs eye movements to features of faces that are most relevant for predicting whether a shock will occur. This is related to the concept of active sensing (i.e., information-seeking) and conditioning driving attentional resources in the domain of face processing. This was the focus of the 2017 biorxiv paper (doi: doi.org/10.1101/125682), which concluded: &quot;These findings show that aversive learning can introduce substantial remodeling of exploration patterns in an adaptive manner during viewing of faces.&quot; This conclusion is much closer to and fully supported by the data. The reviewers would therefore encourage the authors to rewrite their manuscript (and change the title) such that it focuses on the core findings concerning the effect of conditioning on exploratory eye movements, and fully move potential implications for models of generalization to the discussion.</p></disp-quote><p>We thank the reviewers for underlining the strong points of the paper and guiding its revision into a more descriptive and data-based direction. We revised the Introduction so that models are presented more descriptively with regards to how aversive learning can change the geometrical structure of fixation pattern similarities. We now formulated the models and hypotheses in a way that touches generalization, but does not overstate potential implications. We interpret our models as possible shapes of similarity relationships, and not as models of fear generalization. Accordingly we changed names from Baseline and Perceptual Similarity to Perceptual Baseline and <italic>Perceptual Expansion</italic> for Model 01, from Adversity Categorization to <italic>Adversity Gradient</italic> for Model 02, and Adversity Tuning to <italic>CS+ Attraction</italic> for Model 03 (see Figure 1). References to the debate of perceptual versus threat-prediction accounts of fear generalization were moved to the Discussion (second paragraph).</p><p>Moreover, we changed the title back to “Fixation-Pattern Similarity Analysis Reveals Adaptive Changes in Face-Viewing Strategies Following Aversive Learning”.</p><p>We hope to have revised the manuscript such that changes in exploration patterns are presented more descriptively, and discuss implications for fear generalization in the discussion.</p><disp-quote content-type="editor-comment"><p>2) With the new focus on the basic patterns of eye movements, it would be important to also include more descriptive data to show what features in the fixation patterns drive the dissimilarity between faces along the relevant and irrelevant dimension. For instance, fixation heatmaps, scan paths, saccade statistics, blink statistics, etc. might be useful.</p></disp-quote><p>We support the notion to present a variety of descriptive data on patterns of eye movements, so that it becomes clear what changes in how humans explore faces when they become associated with potential harm. While our modelling approach based on individual fixation patterns was motivated by strong idiosyncrasies in scanning paths (e.g. Mehoudar, Arizpe, Baker, and Yovel, 2014), we agree with the reviewers that the found anisotropy does not answer explicitly, how exploration strategies change with learning.</p><p>We added new analyses and text on what drives the increased dissimilarity, i.e. discrimination of CS+ vs. CS- trials (see subsection “Spatial changes in exploration strategy”). To this end, we complemented the linear model with a machine learning (support vector machine, SVM) approach, trained SVMs on individual subjects to explore if one can decode CS+ from CS- trials, and which information is used for this classification process (see new Figure 4). We show that decoding accuracy is better along the specific dimension than for the orthogonal dimension, and improved from baseline to test phase only in the specific dimension. This mirrors the results from our linear models, thus corroborates our findings. To shed light on what differs between CS+ and CS- trials, we visualize activation maps of this classification (see Figure 4C). This way, we show which regions are most informative to decode along the specific dimension.</p><p>As our method of FPSA is based on spatial features of eye-movements (fixation density maps), other features as proposed by the reviewers (such as saccade statistics), were not considered in our model so far. Therefore, while cautious not to overly extend explorative analyses, we agree that it might be of interest to consider learning induced changes in these features as well, as they are common outcome measures in the literature on scanning behavior. We chose features that go beyond the spatial domain, and focused on traditional outcome measures, such as number and duration of fixations, saccade length and the entropy of FDMs as a compact summary statistic. The results are described in the subsection “Comparison of FPSA to common eye-movement features and ROI-based analyses”, and a plot on changes in these univariate outcome measures was added as Figure 5—figure supplement 2.</p><p>Moreover, we tried to link these features to individuals’ anisotropy obtained from FPSA, however none of these features could predict the anisotropy significantly (see the aforementioned subsection).</p><disp-quote content-type="editor-comment"><p>3) Reviewers were concerned that the current design does not dissociate effects of fear conditioning from the effects of repeated exposure to the CS faces during conditioning. This could be ruled out by separately analyzing the first and second half of the generalization session, with the prediction that if the effects are driven by differential exposure, the effect of the unspecific component should increase over time (i.e., after subjects are also exposed to all other stimuli). In contrast, no changes in the unspecific component would suggest that the results are driven by aversive learning rather than exposure per se.</p></disp-quote><p>We agree with the reviewers that our conditioning plan introduces exposure differences by showing only the CS+ and CS- face during the conditioning phase. This motivated the correlation of the individual anisotropy parameter with behavioral outcomes (Rating, SCR). The scatterplot of this relationship was added as Figure 3—figure supplement 1.</p><p>Going beyond this, we took up the suggestion to check for an increase in the unspecific component over time to rule out this potential bias. We analyzed the three runs separately and found that while the specific component increases from baseline to the first generalization phase run, subsequently staying on a stable level throughout the 2nd and 3rd run, the unspecific component does not increase with further exposure to these faces. We added a figure showing specific and unspecific components for each generalization run separately (Figure 3—figure supplement 2), as well as address this in the Results section (subsection “Multivariate fear tuning profiles in eye movements”, last paragraph).</p><disp-quote content-type="editor-comment"><p>4) Although the study sample is large (N=74), there were concerns that because the results come from a complex analysis, they may not replicate out-of-sample. Ideally, the authors would collect new data and show that the effects replicate in an independent sample. Alternatively, they could use analytical approaches (k-fold cross-validation, etc.) to provide more evidence that the results are reliable and reproducible. In particular, it would be important to show that the pre-post conditioning difference in CS+ and in CS- gaze patterns, and their interaction, is robust and replicable.</p></disp-quote><p>We agree with the reviewers about the complexity of the models. As argued in the manuscript, we chose this approach based on the strong idiosyncrasy in individual eye movements. However, this rightfully poses the question of how replicable the findings of the current 74 individuals would be in a new sample. While we strongly agree with the reviewers’ suggestion to see replication of the results in new samples, at present, we do unfortunately not have the resources to collect new data.</p><p>We therefore aimed to frame our findings more cautiously (e.g. effect size on individual level, subsection “Multivariate fear tuning profiles in eye movements”, fourth paragraph). We now additionally report the prevalence in the sample (65%) to indicate that not all subjects might show the effect. Yet, we think that the finding that the amount of anisotropy predicts behavioral outcome measures (ratings, SCR) does speak for the effect being induced by aversive learning and against overfitting within a large sample. Moreover, we complemented our FPSA approach with a second method, the SVM showing the same anisotropy, i.e. better classification accuracy along the specific dimension as compared to the orthogonal dimension.</p><p>Of course, a formal replication with new subjects would be preferable and we are hoping that our sharing the experimental script as well as code used for analysis openly encourages fruitful replication from other sites in the future.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>Whereas reviewer 1 and 2 felt that the revised manuscript adequately addressed the initial concerns, reviewer 3 had some remaining comments. Based on our discussion, we ask:</p><p>1) The reviewers agreed that the manuscript would benefit from discussing two key limitations. These include an acknowledgement of the exploratory and relatively weak nature of the findings, and a discussion of possible disadvantages that come with the circular stimulus organization.</p></disp-quote><p>We thank the reviewers for this suggestion aimed at guiding the reader to a better understanding of possible limitations of the study. Concerning the choice of a circular stimulus continuum, we added a paragraph discussing the choice of our stimulus set to the Discussion section of the manuscript (sixth paragraph). Moreover, we discuss the need for replication and clarification more elaborately (Discussion, seventh paragraph), in line with the considerations about model heterogeneity or noise as underlying factors for the prevalence of 65%, as discussed below. We hope this contributes to describing the results of the present study in a transparent way including implications for and the necessity of further research.</p><disp-quote content-type="editor-comment"><p>2) An additional concern regards the lack of an out-of-sample replication, which is compounded by the fact that only 65% of subjects behave according to the adversity gradient model. The reviewers are aware that collection of new data is not possible at this point. Instead, reviewer 3 recommends an additional analysis treating &quot;model identity&quot; as a random effect (e.g. using spm_BMS). In the event that the model comparison does not yield conclusive results, it should be discussed why this is the case. There are two possibilities why the group-level winning model only wins in 65% of the subjects. First, there is heterogeneity between subjects in the true mechanism. In this case, RFX is appropriate and should be used and reported. Or, the theoretical assumption is that they all use the same mechanism but data are too noisy for single-subject inference. It would be important if the authors could take a strong stance here (either model heterogeneity or noisy data), based on some theoretical considerations, and suggest appropriate future research steps to verify or falsify these conclusions.</p></disp-quote><p>We have addressed this issue by performing an additional Gaussian Mixture Models (GMM) analysis, with which we determine whether the distribution of effect sizes of our effect of interest ((w<sub>specific_gen</sub>-w<sub>unspecific_gen</sub>) – (w<sub>specific_base</sub>-w<sub>unspecific_base</sub>)) is best explained by a model with a single Gaussian component or by models containing multiple Gaussian components (2-5 components). If the 65% fit on the single-subject level results from population heterogeneity, then one would expect one of the multiple Gaussian component models to explain the observed effect size distribution better than the single component model, and vice versa for the alternative explanation in line with a single mechanism with high noise. The results of this analysis reveal that none of the multiple component models outperformed the single component model (Figure 3—figure supplement 3). While the two component model’s BIC differed only slightly from the one component model (BIC<sub>GMM1</sub> = -23.7 BIC<sub>GMM2</sub> = -22.9, ΔBIC = -.8), its bigger complexity resulted in a second component only capturing outliers at the lower end of the distribution with a mixing proportion of only 4% (Figure 3—figure supplement 3B). We conclude that we do not find enough evidence for more than one subpopulation underlying the distribution of anisotropy values. The results of the analysis are mentioned in the Results section: (subsection “Multivariate fear tuning profiles in eye movements”, fifth paragraph) and in more detail in Figure 3—figure supplement 3 and its legend.</p><p>We consider the present study as a first step in exploring how fixation pattern similarities change after aversive learning. We tested our models on a continuum of differently strong changes, thus it is intuitive that these models capture the data differently well on the subject level. Therefore, in a next step, one would need to test predictions arising from our work in a more complex scenario allowing stronger predictions and therefore stronger evidence for or against specific models. One possibly already introduced in the Discussion paragraph of the manuscript would be to set up a more complex experiment with circles of different diameters. This enriched stimulus space would offer testing more complex predictions about dissimilarity relationships and this way would allow to replicate, verify or falsify our recent findings. As mentioned in the response to the first comment, we added these elaborations to the Discussion (Discussion, seventh paragraph), in line with the reviewers’ suggestion to discuss limitations and the small effect sizes further to guide the reader to a better understanding of the results.</p></body></sub-article></article>