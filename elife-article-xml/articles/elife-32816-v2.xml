<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">32816</article-id><article-id pub-id-type="doi">10.7554/eLife.32816</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The representational dynamics of task and object processing in humans</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-99408"><name><surname>Hebart</surname><given-names>Martin N</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7257-428X</contrib-id><email>martin.hebart@nih.gov</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-100488"><name><surname>Bankson</surname><given-names>Brett B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7663-3918</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-100489"><name><surname>Harel</surname><given-names>Assaf</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-28129"><name><surname>Baker</surname><given-names>Chris I</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6861-8964</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-100490"><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Section on Learning and Plasticity, Laboratory of Brain and Cognition</institution><institution>National Institute of Mental Health, National Institutes of Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychology</institution><institution>Wright State University</institution><addr-line><named-content content-type="city">Dayton</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Education and Psychology</institution><institution>Free University of Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Berlin School of Mind and Brain</institution><institution>Humboldt Universität zu Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Bernstein Center for Computational Neuroscience</institution><institution>Charité Universitätsmedizin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-1066"><name><surname>Culham</surname><given-names>Jody C</given-names></name><role>Reviewing Editor</role><aff id="aff6"><institution>University of Western Ontario</institution><country>Canada</country></aff></contrib></contrib-group><author-notes><fn id="fn1"><p>Conflict of Interest: The authors declare no competing financial interests.</p></fn><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>31</day><month>01</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e32816</elocation-id><history><date date-type="received" iso-8601-date="2017-10-14"><day>14</day><month>10</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-01-30"><day>30</day><month>01</month><year>2018</year></date></history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-32816-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.32816.001</object-id><p>Despite the importance of an observer’s goals in determining how a visual object is categorized, surprisingly little is known about how humans process the task context in which objects occur and how it may interact with the processing of objects. Using magnetoencephalography (MEG), functional magnetic resonance imaging (fMRI) and multivariate techniques, we studied the spatial and temporal dynamics of task and object processing. Our results reveal a sequence of separate but overlapping task-related processes spread across frontoparietal and occipitotemporal cortex. Task exhibited late effects on object processing by selectively enhancing task-relevant object features, with limited impact on the overall pattern of object representations. Combining MEG and fMRI data, we reveal a parallel rise in task-related signals throughout the cerebral cortex, with an increasing dominance of task over object representations from early to higher visual areas. Collectively, our results reveal the complex dynamics underlying task and object representations throughout human cortex.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>task context</kwd><kwd>object processing</kwd><kwd>MEG</kwd><kwd>fMRI</kwd><kwd>multivariate analysis</kwd><kwd>MEG-fMRI fusion</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>ZIA-MH-002909</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name><name><surname>Bankson</surname><given-names>Brett B</given-names></name><name><surname>Baker</surname><given-names>Chris I</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>Emmy Noether Grant CI241-1/1</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100005156</institution-id><institution>Alexander von Humboldt-Stiftung</institution></institution-wrap></funding-source><award-id>Feodor-Lynen fellowship</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Task representations emerge rapidly throughout human cortex, with parallel object representations in occipitotemporal cortex that are increasingly dominated by task in higher visual areas.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Our tasks and behavioral goals strongly influence how we interpret and categorize the objects around us. Despite the importance of task context in our everyday perception, object recognition is commonly treated as a hierarchical feedforward process localized to occipitotemporal cortex (<xref ref-type="bibr" rid="bib48">Riesenhuber and Poggio, 2002</xref>; <xref ref-type="bibr" rid="bib52">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib13">DiCarlo et al., 2012</xref>) with little to no modulation by task, while categorization and task rule-related processing are mainly localized to prefrontal and parietal cortex (<xref ref-type="bibr" rid="bib14">Duncan, 2010</xref>; <xref ref-type="bibr" rid="bib17">Freedman and Assad, 2016</xref>). Recent fMRI work has extended this view, revealing task representations in occipitotemporal cortex, as well as documenting the impact of task on object representations in frontoparietal and occipitotemporal cortex (<xref ref-type="bibr" rid="bib64">Çukur et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Erez and Duncan, 2015</xref>; <xref ref-type="bibr" rid="bib4">Bracci et al., 2017</xref>; <xref ref-type="bibr" rid="bib40">Nastase et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Vaziri-Pashkam and Xu, 2017</xref>).</p><p>While these studies demonstrate <italic>where</italic> in the brain task may be represented and where it may affect object representations, due to the low temporal resolution of fMRI they provide only an incomplete picture of <italic>when</italic> these signals emerge across different brain regions, <italic>what</italic> processes they reflect across the time course of a trial, and <italic>how</italic> task affects object representations in time. For example, are task representations first found in frontoparietal regions, first in occipitotemporal regions, or do they emerge in parallel (<xref ref-type="bibr" rid="bib53">Siegel et al., 2015</xref>)? Does task affect the strength of object representations (<xref ref-type="bibr" rid="bib43">Peelen et al., 2009</xref>), does it impose qualitative changes to object representations (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>), or both? Likewise, do task-dependent changes of object representations in occipitotemporal cortex reflect an expectation-relation top-down modulation of feedforward processing (<xref ref-type="bibr" rid="bib33">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib32">Kok et al., 2013</xref>) or a late modulatory influence of task (<xref ref-type="bibr" rid="bib38">McKee et al., 2014</xref>); see also <xref ref-type="bibr" rid="bib15">Emadi and Esteky, 2014</xref>)?</p><p>We addressed these questions using multivariate analysis techniques on magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) data in humans. Using multivariate decoding, we studied the temporal evolution of task and object-related brain signals and their interaction (<xref ref-type="bibr" rid="bib7">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">van de Nieuwenhuijzen et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib27">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Clarke et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Kaiser et al., 2016</xref>). Using temporal generalization analysis (<xref ref-type="bibr" rid="bib31">King and Dehaene, 2014</xref>), we probed the dynamics of the cognitive processes underlying different phases of the task. Finally, we combined MEG data with fMRI data of the same paradigm (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>) using MEG-fMRI fusion based on representational similarity analysis (<xref ref-type="bibr" rid="bib10">Cichy et al., 2014</xref>). By developing a novel model-based MEG-fMRI fusion approach, we targeted the unique contribution of task and objects to the spatiotemporal activity patterns found in human cortex.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To characterize the spatial and temporal evolution of task and object representations in the human brain, we designed a paradigm that allowed us to separately assess the effects of task and objects, as well as their interaction (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Human participants (<italic>n</italic> = 17) categorized objects according to one of four different tasks while we monitored their brain activity using MEG. On each trial, participants were first presented with a task cue indicating the task to be carried out on an ensuing object stimulus. Two of those tasks targeted low-level perceptual dimensions (Color: red/blue, and Tilt: clockwise/counterclockwise), while the other two targeted high-level conceptual dimensions (Content: manmade/natural, Size: large/small, relative to an oven). Following the task cue, after a short delay participants were presented with an object stimulus from a set of eight different objects (five exemplars each). After another delay, a response mapping screen appeared that provided both possible answers left and right of fixation (random order). After onset of the response mapping screen, participants responded with a button press and an instructed eye blink. Participants responded fast and highly accurately (accuracy <italic>M</italic>: 97.19%, <italic>SD</italic>: 2.40; response time <italic>M:</italic> 712.2 ms, <italic>SD</italic>: 121.8), demonstrating their adaptability to the varying task demands. There were no significant behavioral differences between tasks or between objects (all <italic>F</italic> &lt; 1). On average, participants missed responses or responded too slowly (RT &gt;1,600 ms) in 1.80% of all trials (<italic>SD</italic>: 2.26).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.32816.002</object-id><label>Figure 1.</label><caption><title>Experimental paradigm.</title><p>On each trial (Procedure depicted in Panel <bold>C</bold>), participants were presented with a stimulus from one of eight different object classes (Panel <bold>B</bold>) embedded in one of four task contexts (Panel <bold>A</bold>, top) indicated at the beginning of each trial. Participants carried out a task that either targeted low-level features (perceptual tasks) of the object or its high-level, semantic content (conceptual tasks). After a short delay, a response-mapping screen was shown that presented the possible response alternatives (Panel <bold>A</bold>, bottom) in random order either left or right of fixation to decouple motor responses from the correct response.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32816-fig1-v2"/></fig><sec id="s2-1"><title>Time-resolved representation of task context and objects</title><p>All MEG analyses were carried out in a time-resolved manner. Prior to multivariate analyses, to speed up computations and increase sensitivity (<xref ref-type="bibr" rid="bib22">Grootswagers et al., 2017</xref>), MEG sensor patterns (272 channels) were spatially transformed using principal component analysis, followed by removal of the components with the lowest 1% of variance, temporal smoothing (15 ms half duration at half maximum) and downsampling (120 samples/s).</p><p>To separately characterize the temporal evolution of task and object-related signals, we conducted time-resolved multivariate decoding across the trial (see <xref ref-type="fig" rid="fig1">Figure 1C</xref> and <xref ref-type="fig" rid="fig2">Figure 2A</xref>) using support vector machine classification (<xref ref-type="bibr" rid="bib8">Chang and Lin, 2011</xref>) of all pairwise comparisons of conditions. For a given decoding analysis (e.g. task decoding), all pairwise classification accuracy time courses were averaged, leading to an overall chance-level of 50%. This provided temporal profiles of two resulting classification time courses, one for objects averaged across task, and one for task averaged across objects (<xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.32816.003</object-id><label>Figure 2.</label><caption><title>Schematic for multivariate analyses of MEG data.</title><p>All multivariate analyses were carried out in a time-resolved manner on principal components (PCs) based on MEG sensor patterns (see <italic>Materials and methods</italic> for transformation of sensor patterns to PCs). (<bold>A</bold>) Time-resolved multivariate decoding was conducted using pairwise SVM classification at each time point, classifying all pairs of tasks or categories, and averaging classification accuracies within a given decoding analysis (e.g. decoding of task or category). (<bold>B</bold>) For model-based MEG-fMRI fusion, 32 × 32 representational dissimilarity matrices were constructed using Pearson's <italic>r</italic> for all combinations of task and category.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32816-fig2-v2"/></fig><p>In the following, we describe and report results from the ‘Task Cue Period’ (0 to 2,000 ms) from onset of the task cue to onset of the object stimulus, and the ‘Object Stimulus Period’ (2000 to 3500 ms) from onset of the object stimulus to onset of the response screen. We did not statistically analyze the ensuing ‘Response-Mapping Period’ (3500 ms to 5000 ms), because it was contaminated by instructed blinks and response screen-related processes (see <italic>Materials and methods, Statistical Testing)</italic>. However, for completeness, we show results from this Response-Mapping Period in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.32816.004</object-id><label>Figure 3.</label><caption><title>Time-resolved MEG decoding of task and objects across the trial.</title><p>After onset of the task cue (Task Cue Period), task-related accuracy increased rapidly, followed by a decay toward chance and significant above-chance decoding ~200 ms prior to object onset. After onset of the object stimulus (Object Stimulus Period), object-related accuracy increased rapidly, decaying back to chance with the onset of the response-mapping screen. This was paralleled by a gradual increase in task-related accuracy, starting 242 ms and peaking 638 ms after object onset and remaining high until onset of the response-mapping screen. Error bars reflect SEM across participants for each time-point separately. Significance is indicated by colored lines above accuracy plots (non-parametric cluster-correction at p&lt;0.05). Time periods after the onset of the response-mapping screen were excluded from statistical analyses (see <italic>Materials and methods</italic> and <italic>Results</italic>), but are shown for completeness.</p><p><supplementary-material id="fig3sdata1"><object-id pub-id-type="doi">10.7554/eLife.32816.005</object-id><label>Figure 3—source data 1.</label><caption><title>Per subject time courses of mean classification accuracy for task and object.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-32816-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32816-fig3-v2"/></fig><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.32816.006</object-id><label>Figure 4.</label><caption><title>Results of temporal generalization analysis of task.</title><p>(<bold>A</bold>) Temporal cross-classification matrix. The y-axis reflects the classifier training time relative to task cue onset, the x-axis the classifier generalization time, and the color codes the cross-classification accuracy for each combination of training and generalization time. The outline reflects significant clusters (p&lt;0.05, cluster-corrected sign permutation test). Results after the onset of the response-mapping screen are not included in the statistical evaluation but are shown for completeness. (see <italic>Results</italic>) (<bold>B</bold>). Panels that schematically indicate three patters in the temporal generalization results. First, there was a block structure (<italic>Within-Period Cross-Decoding)</italic> separately spanning the Task Cue Period and the Object Stimulus Period, indicating largely different representations during the different periods of the task (left panel). At the same time, there were two separate patterns of temporal generalization in the off-diagonals (<italic>Between-Period I and Between-Period II Cross-Decoding</italic> illustrated in middle and right panel, respectively), indicating a shared representational format between these time periods.</p><p><supplementary-material id="fig4sdata1"><object-id pub-id-type="doi">10.7554/eLife.32816.009</object-id><label>Figure 4—source data 1.</label><caption><title>Per subject matrices of temporal cross-classification analysis of task.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-32816-fig4-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32816-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.32816.007</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Results of temporal generalization analysis of task separated by task type.</title><p>Each map reflects the average of all pairwise classifications of a given task with all other tasks (e.g. color vs. tilt, color vs. content, color vs. size). The y-axis reflects the classifier training time relative to task cue onset, the x-axis the classifier generalization time, and the color codes the cross-classification accuracy for each combination of training and generalization time. The outline reflects significant clusters (p&lt;0.05, cluster-corrected sign permutation test). Time periods after the onset of the response-mapping screen were excluded from statistical analyses (see Materials and methods and Results), but are shown for completeness.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32816-fig4-figsupp1-v2"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.32816.008</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Results of temporal generalization analysis of objects.</title><p>The y-axis reflects the classifier training time relative to task cue onset, the x-axis the classifier generalization time, and the color codes the cross-classification accuracy for each combination of training and generalization time. The outline reflects significant clusters (p&lt;0.05, cluster-corrected sign permutation test). Time periods after the onset of the response-mapping screen were excluded from statistical analyses (see Materials and methods and Results), but are shown for completeness.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32816-fig4-figsupp2-v2"/></fig></fig-group><p><italic>Task Cue Period.</italic> As expected, classification of objects remained at chance prior to the presentation of the object stimulus (<xref ref-type="fig" rid="fig3">Figure 3</xref>, red curve). In contrast, task-related information (<xref ref-type="fig" rid="fig3">Figure 3</xref>, blue curve) rose rapidly in response to the task cue, peaking at 100 ms (bootstrap 95 % CI: 96–121). This was followed by a slow decay of information that approached chance-level and remained significant until ~1200 ms after cue presentation. Notably, around 1800 ms, that is, 200 ms prior to onset of the object stimulus, task information was again significantly above chance. This result demonstrates the presence of a task representation that is available prior to the onset of the object stimulus.</p><p><italic>Object Stimulus Period.</italic> After onset of the object stimulus at 2000 ms, object information increased sharply, peaking after 104 ms (bootstrap 95 % CI: 100–108). This was followed by a gradual decline that remained significantly above chance until the onset of the response-mapping screen at 3500 ms. This rapid increase in object-related information was accompanied by a slow rise of task-related information starting 242 ms (bootstrap 95 % CI: 167–308) after object onset and peaking at 638 ms after object onset (95 % CI: 517–825). Information about task then remained well above-chance until the presentation of the response-mapping screen.</p><p>Together, these results demonstrate the emergence of different components of the task, including the processing of the task cue and the presence of task-related signals before and during object processing. Further, they highlight an actively maintained or reactivated task representation prior to object onset that becomes increasingly relevant during object processing. Importantly, the rise of task-related information 242 ms after object onset – more than 130 ms after peak object decoding – suggests that task context has limited impact on initial feedforward object processing, but points towards later modulation of object representations.</p></sec><sec id="s2-2"><title>Multiple stages of task processing revealed by temporal generalization analysis</title><p>The decoding of task at different time points as described above characterizes the temporal progression of task-related information across the trial. However, these results alone do not distinguish whether the decoding of task reflects a single or a sequence of multiple cognitive processes across time and more generally what cognitive processes may underlie task decoding at different time points. There are three pertinent candidates: For one, early decoding of task after task cue onset may reflect an early visual representation of the task cue that is maintained in short-term memory and accessed when the object stimulus appears in order to carry out the task. Alternatively, the task representation during object processing may reflect an abstract representation of the participant’s emerging choice. Finally, the visual information about the task cue may reflect a more abstract representation of task rule that has been formed after initial visual and semantic processing of the task cue and that is maintained and applied to the object stimulus representation.</p><p>To reveal and characterize the processing stages of task, we conducted temporal generalization analysis (<xref ref-type="bibr" rid="bib39">Meyers et al., 2008</xref>; <xref ref-type="bibr" rid="bib31">King and Dehaene, 2014</xref>), a method to systematically analyze the similarities and differences of neural activation patterns across time. The degree to which representations are similar in different stages of the trial allows us to draw inferences about the cognitive processes involved. If a classifier can generalize from one timepoint to another, this implies similar cognitive processes at those time points. If, however, there is no temporal generalization, then this may indicate different cognitive processes. We conducted temporal generalization analysis by training a classifier at each time point during the trial to distinguish the four different tasks and then tested it at all other time points, providing us with a time ×time temporal generalization matrix.</p><p>The temporal generalization analysis revealed multiple separate, but partially overlapping stages of processing after the onset of the task cue (<xref ref-type="fig" rid="fig4">Figure 4A</xref> and <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>, for results separated by task type see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, for results of a temporal generalization analysis of objects, see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). At a coarse level, the temporal generalization matrix exhibited a block structure within the Task Cue Period and Object Stimulus Period (<italic>Within-Period Cross-Decoding</italic>, <xref ref-type="fig" rid="fig4">Figure 4B</xref>, left panel). This indicates a shared representational format <italic>within</italic> each time period of the trial, but a largely different representational format <italic>between</italic> those time periods, and an abrupt change in the representational format of task after onset of the object stimulus. Importantly, this result speaks against a visual or semantic representation of task during the Object Stimulus Period, since those representations would likely have emerged already early in the Task Cue Period and would have led to between-period cross-decoding.</p><p>At a more fine-grained level, During the Task Cue Period (0 to 2000 ms) the results revealed cross-decoding lasting from ~100 ms until 2000 ms. This reinforces the idea of an active maintenance of task throughout this time period, as suggested by the time-resolved decoding analysis presented above. During the Object Stimulus Period, there was a gradual build-up of task-related information until ~200 ms after object onset. At that point, the results exhibited high levels of cross-decoding, indicating a maintained representation of task context that does not change until the onset of the response mapping screen.</p><p>Importantly, there was also evidence for a shared representational format between time periods (<italic>Between-Period Cross-Decoding</italic>, <xref ref-type="fig" rid="fig4">Figure 4B</xref>, middle and right panels), as demonstrated by the off-diagonals of the generalization matrix (i.e. training time 0 to 2000 ms, testing time 2000 to 3500 ms, and vice versa). First, there was generalization from the Task Cue Period to the first ~200 ms of the Object Stimulus Period (training time ~300 to 2000 ms, testing time 2000 to ~2200 ms, <xref ref-type="fig" rid="fig4">Figure 4B</xref>, middle panel), possibly reflecting a maintained short-term representation that continued until the task rule could be applied to the object. Second, there was generalization from the end of the Task Cue Period to the Object Stimulus Period (training time ~1500 to 2000 ms, testing time 2000 ms to ~3300 ms, <xref ref-type="fig" rid="fig4">Figure 4B</xref>, right panel), indicating that the short-term memory representation of task was similar to the representation during application of the task rule to the object. Interestingly, this cross-classification was specific to the late short-term memory representation and did not generalize to other time points of the Task Cue Period. Note that this result cannot be explained by a representation of the correct response, because participants could not know the correct response during this short-term memory representation prior to the presentation of the object.</p><p>Together, this pattern of results suggests that the representation of task during the Object Stimulus Period likely does not reflect visual or semantic processing of the task cue (which would predict cross-classification from the early Task Cue Period); nor does it reflect only a representation of participants’ choices. Rather, the results indicate that participants form an abstract representation of task rule during the short-term retention period prior to object onset, which they apply to the object stimulus when it is presented.</p></sec><sec id="s2-3"><title>Effects of task context on object representations</title><p>The robust decoding of task that increases during object processing raises the question whether the task representation is independent of object processing, to what extent task influences object representations, and when those effects emerge. Task may influence object representations in two non-exclusive ways: First, task may affect the <italic>strength</italic> of object representations, which would be indicated by differences in the decoding accuracy between task types. Second, task may <italic>qualitatively</italic> influence the representation of objects, which would be reflected in different activation patterns in response to object stimuli. These effects may emerge early (before 150 ms), indicating that task affects feedforward processing of objects. Alternatively, the effects may emerge late (after 150 ms), indicating modulatory effects of existing object representations.</p><p>To investigate whether and when task affects the strength of object representations, we conducted time-resolved multivariate decoding of objects separately for perceptual and conceptual task types and compared the time courses (<xref ref-type="fig" rid="fig5">Figure 5A</xref> and <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>). The overall time course of object decoding was very similar for conceptual and perceptual tasks, as expected (see <italic>Time-resolved Representation of Task Context and Objects</italic> and <xref ref-type="fig" rid="fig3">Figure 3</xref>): decoding accuracies increased sharply after stimulus onset, followed by a gradual decline, dropping back to chance level toward the end of the Object Period. Comparing the decoding curves for conceptual and perceptual tasks directly revealed higher accuracies for conceptual tasks emerging after 542 ms (95 % CI: 283–658). In agreement with the results of the time-resolved analysis of task, this suggests that task exerts late modulatory effects on object representations, again arguing against a strong influence of task on feedforward processing. Responses to high-level conceptual tasks were more pronounced than those to low-level perceptual tasks, likely reflecting the fact that conceptual tasks entail more in-depth processing of the object than perceptual tasks.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.32816.010</object-id><label>Figure 5.</label><caption><title>Comparison of object decoding for different task types (p&lt;0.05, cluster-corrected sign permutation test).</title><p>Error bars reflect standard error of the difference of the means. (<bold>A</bold>) Object decoding separated by perceptual and conceptual task types. Initially, object decoding for conceptual and perceptual tasks is the same, followed by decoding temporarily remaining at a higher level for conceptual tasks than perceptual tasks between 542 and 833 ms post-stimulus onset. (<bold>B</bold>) Object decoding within and across task types. A classifier was trained on data of different objects from one task type and tested either on object-related data from the same task type (within tasks) or on object-related data from the other task type (between tasks). There was no difference in within and between-task decoding.</p><p><supplementary-material id="fig5sdata1"><object-id pub-id-type="doi">10.7554/eLife.32816.011</object-id><label>Figure 5—source data 1.</label><caption><title>Per subject time courses of mean classification accuracy for task separated by task type and cross-classification accuracy between task types.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-32816-fig5-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32816-fig5-v2"/></fig><p>In addition to these quantitative differences in object representations across task types, we investigated whether the object representations were qualitatively similar but differently strong (more separable patterns), or whether they were qualitatively different across task types (different patterns). To this end, we compared object classification <italic>within</italic> task to object classification <italic>between</italic> tasks. The rationale of this approach is that if the between-task cross-classification accuracy is lower than the within-task accuracy, this demonstrates that the classifier cannot rely on the same source of information in these two conditions, that is the patterns must be qualitatively different between tasks. The results of this analysis are shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. We found no differences in object decoding accuracies within vs. between task types, indicating that task affected only the strength, but not the quality of object representations.</p></sec><sec id="s2-4"><title>Model-based MEG-fMRI fusion for spatiotemporally-resolved neural dynamics of task and objects</title><p>Previous studies investigating task representations in humans focused primarily on the spatial localization of task effects to areas of the human brain. However, the representation of task does not emerge instantateously in all brain regions involved in processing task and is not static, but changes dynamically over time. To provide a more nuanced view of the cortical origin and the neural dynamics underlying task and object representations, we carried out MEG-fMRI fusion based on representational similarity analysis (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, <xref ref-type="bibr" rid="bib10">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib11">Cichy et al., 2016</xref>). We calculated time-resolved MEG representational dissimilarity matrices (RDMs) for all combinations of task and objects (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) and compared them to fMRI RDMs derived from brain activity patterns from five ROIs of a previously published study employing the same paradigm (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>). Similarity between an fMRI RDM and MEG RDMs indicates a representational format common to that location (i.e. ROI) and those time points (for fMRI and MEG RDMs, see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> and <xref ref-type="video" rid="fig6video1">Figure 6—video 1</xref>). While previous versions of MEG-fMRI fusion reveal the shared variance between RDMs of both modalities, they leave open what portion of this variance can be attributed uniquely to specific conditions (e.g. task or objects). To overcome this limitation, we developed an approach for <italic>model-based</italic> MEG-fMRI fusion which not only provides a spatiotemporally resolved signal, but which also allows us to ascribe portions of this signal to the cognitive process of study. Our model-based MEG-fMRI fusion approach is based on commonality analysis (<xref ref-type="bibr" rid="bib51">Seibold and McPHEE, 1979</xref>), a variance partitioning approach that identifies the variance uniquely shared between multiple variables, in our case MEG, fMRI and a given model RDM (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Model RDMs were constructed based on the expected dissimilarity for task and objects (0 within the same condition, one between different conditions). The procedure results in localized time courses of task-specific and object-specific information.</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.32816.012</object-id><label>Figure 6.</label><caption><title>Model-based MEG-fMRI fusion procedure and results.</title><p>(<bold>A</bold>) Model-based MEG-fMRI fusion in the current formulation reflects the shared variance (commonality) between three dissimilarity matrices: (1) an fMRI RDM generated from voxel patterns of a given ROI, (2) a model RDM reflecting the expected dissimilarity structure for a variable of interest (e.g. task) excluding the influence of another variable of interest (e.g. object) and (3) an MEG RDM from MEG data at a given time point. This analysis was conducted for each MEG time point independently, yielding a time course of commonality coefficients for each ROI. (<bold>B-F</bold>). Time courses of shared variance and commonality coefficients for five regions of interest (ROIs) derived from model-based MEG-fMRI fusion (p&lt;0.05, cluster-corrected randomization test, corrected for multiple comparisons across ROIs): PPC (Panel <bold>B</bold>), lPFC (Panel <bold>C</bold>), EVC (Panel <bold>D</bold>), LO (Panel <bold>E</bold>) and pFS (Panel <bold>F</bold>). Blue plots reflect the variance attributed uniquely to task, while red plots reflect the variance attributed uniquely to object. Grey-shaded areas reflect the total amount of variance shared between MEG and fMRI RDMs, which additionally represents the upper boundary of the variance that can be explained by task or object models. Y-axes are on a quadratic scale for better comparability to previous MEG-RSA and MEG-fMRI fusion results reporting correlations (<xref ref-type="bibr" rid="bib10">Cichy et al., 2014</xref>) and to highlight small but significant commonality coefficients.</p><p><supplementary-material id="fig6sdata1"><object-id pub-id-type="doi">10.7554/eLife.32816.014</object-id><label>Figure 6—source data 1.</label><caption><title>Mean representational dissimilarity matrices for all combinations of task and object, both for all five fMRI ROIs and each MEG time point, including pre-calculated permutations used for permutation testing.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-32816-fig6-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32816-fig6-v2"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.32816.013</object-id><label>Figure 6—figure supplement 1.</label><caption><title>FMRI representational dissimilarity matrices (RDMs) for the five regions of interest: Posterior parietal cortex (PPC), lateral prefrontal cortex (lPFC), early visual cortex (EVC), object-selective lateral occipital cortex (LO), and posterior fusiform sulcus (pFS).</title><p>Since RDMs are compared to MEG data using Spearman <italic>r</italic>, rank-transformed dissimilarities are plotted.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32816-fig6-figsupp1-v2"/></fig><media id="fig6video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-32816-fig6-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.32816.015</object-id><label>Figure 6–video 1.</label><caption><title>Movie of time-resolved MEG representational dissimilarity matrices, scaled using the rank transform across dissimilarities.</title></caption></media></fig-group><p>The results of this model-based MEG-fMRI fusion are shown in <xref ref-type="fig" rid="fig6">Figure 6B–F</xref> separately for five regions of interest (ROIs): early visual cortex (EVC), object-selective lateral occipital cortex (LO), posterior fusiform sulcus (pFS), lateral prefrontal cortex (lPFC), and posterior parietal cortex (PPC). The grey shaded area indicates the total amount of variance captured by MEG-fMRI fusion. Blue and red lines indicate the amount of variance in the MEG-fMRI fusion uniquely explained by the task and object model, respectively (see <xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1</xref>).</p><p>In all ROIs and at most time points, the task or object models collectively explained the majority of the shared variance between MEG and fMRI, as indicated by the close proximity of the colored lines to the upper boundary of the grey-shaded area. This result demonstrates that task and object model RDMs are well suited for describing the observed spatio-temporal neural dynamics.</p><p>All regions carried information about task and objects at some point throughout the trial, indicating that task and object representations coexist in the same brain regions, albeit not necessarily at the same point in time. Importantly, regions differed in the predominance and mixture of the represented content. Both PPC and lPFC were clearly dominated by effects of task, with much weaker object-related commonality coefficients present in these areas. These regions exhibited high-task-related commonality coefficients both during the Task Cue Period and the Object Stimulus Period. Interestingly, PPC exhibited significant task-related commonality coefficients throughout the short-term retention period that were not found in lPFC (p&lt;0.05, cluster-corrected randomization test on differences of commonality coefficients), which may speak toward a different functional role of these regions in the retention of task rules. We found no difference between any regions in the onset of task effects after task cue onset (all p&gt;0.05), indicating the parallel rise of task-related information in these brain regions at the temporal precision afforded by our analysis approach.</p><p>In contrast to frontoparietal regions, occipitotemporal regions EVC, LO and pFS generally exhibited weaker but significant task-related commonality coefficients than PPC and lPFC. All three regions displayed significant task-related commonality coefficients in the Task Cue Period. Interestingly, in the Object Stimulus Period all three regions exhibited a mixture of task and object-related commonality coefficients, indicating the concurrent encoding of task and objects in these brain areas. Moreover, the relative size of task-related commonalities increased gradually from EVC through LO to pFS (randomization test comparing difference of task and object representations between regions: p=0.0002), indicating an increasing importance of task encoding when progressing up the visual hierarchy. Visual inspection of the results suggests a temporal shift in the dominance of task over object representations along occipitotemporal cortex, with an earlier dominance of task in pFS than EVC. In all five regions, after onset of the object stimulus object-related commonality coefficients peaked earlier than task-related commonality coefficients (all p&lt;0.05, based on bootstrap CI for differences in peaks), in line with the results of the time-resolved multivariate decoding analysis.</p><p>Together, we found that the spatiotemporal neural dynamics as revealed by model-based MEG-fMRI fusion predominantly reflected task or object processing, with systematic differences across cortical regions: While PPC and lPFC were dominated by task and PPC carried task information throughout the Task Cue Period, EVC, LO and pFS exhibited a mixture of task and object-related information during the Object Stimulus Period, with relative increases in the size of task-related effects when moving up the visual cortical hierarchy. This indicates the parallel representation of object and task-related signals in those brain regions, with an increasing relevance of task in category-selective brain regions.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We used MEG and time-resolved multivariate decoding to unravel the representational dynamics of task context, objects, and their interaction. Information about task was found rapidly after onset of the task cue and throughout the experimental trial, which was paralleled by information about objects after onset of the object stimulus. Temporal cross-decoding revealed separate and overlapping task-related processes, suggesting a cascade of representations including visual encoding of the task cue, the retention of the task rule, and its application to the object stimulus. Investigating the interaction of task context and object, we found evidence for late effects of task context on object representations, with task impacting the strength rather than the quality of object-related MEG patterns. Finally, model-based MEG-fMRI fusion revealed a parallel rise of task-related information across all regions of interest and differences in the timecourses of task and object information. Parietal and frontal regions were strongly dominated by effects of task, whereas occipitotemporal regions reflected a mixture of task and object representations following object presentation, with relative increases in task-related effects over time and along the visual cortical hierarchy.</p><sec id="s3-1"><title>Representational dynamics of task context</title><p>Previous fMRI studies investigating the effects of task context during visual object processing focused on the cortical location of task effects (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Erez and Duncan, 2015</xref>; <xref ref-type="bibr" rid="bib4">Bracci et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Bugatus et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Vaziri-Pashkam and Xu, 2017</xref>), leaving open the question of the temporal dynamics of task representations in those regions. Our work addressed this gap in knowledge by examining the emergence of task representations and probing what cognitive processes underlie task representations at different points in time. By manipulating task context on a trial-by-trial basis we (1) mapped out the temporal evolution of task context effects across different stages of the trial, (2) uncovered different stages of processing using temporal generalization analysis, and (3) localized task-related information to different regions of the brain using model-based MEG-fMRI fusion.</p><p>The results from multivariate decoding and temporal generalization analyses indicate that following initial encoding of visual information about task cue, there was a weak but consistent short-term memory representation of this information, paralleled by an abstract representation of the task rule. Temporal generalization analysis additionally revealed multiple distinct but overlapping stages of task processing, extending previous findings of prefrontal recordings in non-human primates (<xref ref-type="bibr" rid="bib54">Sigala et al., 2008</xref>; <xref ref-type="bibr" rid="bib56">Stokes et al., 2013</xref>). The pattern of generalization results suggests that during object processing task is not represented in a purely visual (or semantic) format; rather, they suggest a more abstract representation of task rule that is applied to the visually-presented object stimulus (<xref ref-type="bibr" rid="bib61">Wallis et al., 2001</xref>; <xref ref-type="bibr" rid="bib55">Stoet and Snyder, 2004</xref>; <xref ref-type="bibr" rid="bib3">Bode and Haynes, 2009</xref>; <xref ref-type="bibr" rid="bib63">Woolgar et al., 2011</xref>; see also <xref ref-type="bibr" rid="bib45">Peters et al., 2016</xref>).</p><p>Of note, the representation of task in monkey prefrontal cortex has been shown to be even more dynamic than described above and not to generalize at all between different periods of the task (<xref ref-type="bibr" rid="bib56">Stokes et al., 2013</xref>). Since our results demonstrate phases of cross-classification between these time periods, this suggests that the source of the cross-classification between these task periods may originate from other brain regions such as posterior parietal cortex. Indeed, this interpretation is supported by our MEG-fMRI fusion results that show no significant prefrontal representations of task context during the delay period prior to the onset of the object stimulus, but a representation of task in posterior parietal cortex.</p></sec><sec id="s3-2"><title>Frontoparietal and Occipitotemporal brain areas are differentially involved in task and object representations</title><p>Previous research has suggested a dominance of parietal and prefrontal cortex in representing task context (<xref ref-type="bibr" rid="bib14">Duncan, 2010</xref>; <xref ref-type="bibr" rid="bib63">Woolgar et al., 2011</xref>), while the processing of objects has been attributed to occipitotemporal cortex (<xref ref-type="bibr" rid="bib20">Grill-Spector et al., 1999</xref>; <xref ref-type="bibr" rid="bib35">Kravitz et al., 2010</xref>; <xref ref-type="bibr" rid="bib9">Cichy et al., 2011</xref>). More recently, this view has been challenged: First, object representations have been found – with some dependence on task context – in both parietal (<xref ref-type="bibr" rid="bib34">Konen and Kastner, 2008</xref>; <xref ref-type="bibr" rid="bib29">Jeong and Xu, 2016</xref>; <xref ref-type="bibr" rid="bib4">Bracci et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Vaziri-Pashkam and Xu, 2017</xref>) and prefrontal cortex (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Erez and Duncan, 2015</xref>; <xref ref-type="bibr" rid="bib4">Bracci et al., 2017</xref>). Second, there is some evidence for task effects in occipitotemporal cortex, although the extent of such effects remains debated (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Erez and Duncan, 2015</xref>; <xref ref-type="bibr" rid="bib37">Lowe et al., 2016</xref>; <xref ref-type="bibr" rid="bib4">Bracci et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Bugatus et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Vaziri-Pashkam and Xu, 2017</xref>), and the time course of any such effects has remained elusive.</p><p>Our model-based MEG-fMRI fusion results provide a nuanced spatiotemporal characterization of task and object representations in frontoparietal and occipitotemporal cortex. Task representations emerged in parallel across all brain regions, emphasizing the importance of task representations throughout human cortex and suggesting a rapid communication of task information between brain regions (<xref ref-type="bibr" rid="bib53">Siegel et al., 2015</xref>). Frontoparietal cortex was strongly dominated by task context, with much weaker object representations. This finding reinforces the notion that the dominant role of frontoparietal cortex is the representation of task, with a secondary role in representing objects (but see <xref ref-type="bibr" rid="bib4">Bracci et al., 2017</xref>). In contrast, in occipitotemporal cortex, responses reflected a mixture of object and task-related effects after object onset, with an increasing dominance of task over time and along the visual cortical hierarchy from low- to high-level visual cortex (EVC, LO, pFS). These results reveal that both task and objects are encoded in parallel in the same regions of occipitotemporal cortex and suggest an increasing role of task context in high-level visual cortex.</p><p>The finding of parallel effects of task and object suggests an important role of task during object processing already in occipitotemporal cortex. This contrasts with the view of a ‘passive’ role of occipitotemporal cortex in the processing of objects, according to which object representations are read out by prefrontal cortex (<xref ref-type="bibr" rid="bib18">Freedman et al., 2003</xref>). Instead, our results suggest that task may bias late components of object processing along occipitotemporal cortex (albeit at relatively late stages), an influence that may originate in brain regions strongly dominated by task in frontoparietal cortex (<xref ref-type="bibr" rid="bib62">Waskom et al., 2014</xref>). In addition, our results suggest that this influence may increase along the visual cortical hierarchy. Indeed, pFS but not EVC or LO was found to represent task immediately prior to object onset, suggesting that task has the potential to affect the early stages of visual processing through a top-down bias. This bias may reflect a task-specific modulation of the representational strength of task-relevant object features after object onset. While this interpretation is in line with studies of attentional enhancement of objects and their features in occipitotemporal cortex (<xref ref-type="bibr" rid="bib28">Jehee et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Peelen and Kastner, 2011</xref>), our results go further by demonstrating the concurrent representation of both task and objects in the same brain region, which may be beneficial for optimizing the tuning of categorical brain responses to the demands of the task.</p><p>While it is possible that the MEG-fMRI fusion results found in this study are driven by a small subset of conditions or a simple one-dimensional representation, we believe this to be unlikely based on the complexity of the empirically observed MEG and fMRI RDMs (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> and <xref ref-type="video" rid="fig6video1">Figure 6—video 1</xref>). However, future studies are required to assess the degree to which complex patterns found in multivariate analyses are driven by low-dimensional representations (<xref ref-type="bibr" rid="bib24">Hebart and Baker, 2017</xref>).</p></sec><sec id="s3-3"><title>Task affects the strength of object representations late in time</title><p>The direct investigation of the temporal dynamics of task and object interactions revealed three key findings. First, we found that differences in object processing between low-level perceptual and high-level conceptual tasks emerged late in time, suggesting a late top-down modulation of object processing after initial object processing has been completed, arguing against an early expectation-related modulation of feedforward processing. This finding is consistent with a previous EEG study using natural images in an animal and vehicle detection task, reporting a fast initial object-related signal followed by later task-related responses after ~160–170 ms signaling the presence of a target stimulus (<xref ref-type="bibr" rid="bib59">VanRullen and Thorpe, 2001</xref>). Similarly, a more recent MEG study (<xref ref-type="bibr" rid="bib50">Ritchie et al., 2015</xref>) reported results for visual category processing in two different tasks (object categorization vs. letter discrimination) that are indicative of late differences in task-dependent stimulus processing. Finally, another recent EEG study reported late effect of task on scene processing (<xref ref-type="bibr" rid="bib21">Groen et al., 2016</xref>). Overall, these combined results suggest that task representations affect late, rather than early processing of visual information.</p><p>Second, object-related information leveled off more slowly for conceptual than perceptual tasks, indicating different neural dynamics for different task types. This suggests that for conceptual tasks encoding and maintenance of object category may be beneficial for carrying out the task, in contrast to perceptual tasks for which the extraction of task-relevant features may be sufficient. Differences in the difficulty between the tasks may account for this pattern of results; however, we found no differences in response times or accuracy for the different tasks, arguing against the relevance of task difficulty. In support of this view, a previous study emploing a speeded version of the same tasks and objects found no differences in response times between tasks (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>).</p><p>Finally, while task context affected the separability of object-related MEG patterns between task types, we found no evidence that the overall structure of those patterns changed. This result contrasts with a prior study demonstrating qualitatively different object-related patterns in lateral prefrontal and high-level object-selective cortex (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>; <xref ref-type="bibr" rid="bib40">Nastase et al., 2017</xref>). However, the contribution of multiple brain regions to the MEG response may be masking an interaction between object and task representations. Indeed, our MEG-fMRI fusion data suggest that both task and objects are being processed in parallel in pFS, although future work with independent data will be needed to resolve this issue.</p><p>While our experimental design precluded interpretation of results in the response period, future studies could explicitly target all stages of the task, from the instructional cue to the response. In addition, our study did not distinguish between different stages of object processing (e.g. low-level features or high-level categories), and our temporal generalization analysis of objects did not reveal multiple apparent object processing stages (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Task may interact with objects at any stage of processing, and while in the present study interactions arose late in time, it is still a matter of debate to what degree late responses reflect high-level categorical processing of objects (<xref ref-type="bibr" rid="bib30">Kaiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib1">Bankson et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Proklova et al., 2017</xref>). Future studies on task effects during object processing could address this issue by using a larger, controlled set of objects (<xref ref-type="bibr" rid="bib5">Bracci and Op de Beeck, 2016</xref>) or by explicitly including models of shape (<xref ref-type="bibr" rid="bib2">Belongie et al., 2002</xref>) or texture (<xref ref-type="bibr" rid="bib47">Proklova et al., 2016</xref>). By revealing the spatiotemporal dynamics of task and object processing, our results serve as a stepping stone for future investigations addressing these questions.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Twenty-two healthy volunteers with normal or corrected-to-normal visual acuity took part in the study. Five participants were excluded due to at least one of the following exclusion criteria: behavioral performance below 90% correct, excessive artifacts, or incomplete or corrupted recordings. Data from the remaining 17 participants (eight female, mean age 25.12, SD = 5.16) were used in all analyses throughout the study. The sample size was chosen based on previous studies employing multivariate decoding of MEG signals (<xref ref-type="bibr" rid="bib7">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib11">Cichy et al., 2016</xref>). All participants gave written informed consent as part of the study protocol (93 M-0170, NCT00001360) prior to participation in the study. The study was approved by the Institutional Review Board of the National Institutes of Health and was conducted according to the Declaration of Helsinki.</p></sec><sec id="s4-2"><title>Experimental design and stimuli</title><p>We chose four tasks that could be carried out on a set of object images, two targeting low-level perceptual dimensions of the images, and two high-level conceptual dimensions (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The perceptual dimensions were Color (red/blue) and Tilt (clockwise/counterclockwise), and the conceptual dimensions were Content (manmade/natural) and Size (real world, large/small relative to an oven). Object images were chosen from eight different categories (<xref ref-type="fig" rid="fig1">Figure 1B</xref>): Butterfly, cow, dresser, flower, motorbike, skate, tree, and vase. For each of the eight object categories, we chose five different image exemplars. For the Color and Tilt tasks, each object was presented with a thin red or blue outline, and objects were either tilted 30 degrees clockwise or counterclockwise relative to the principal axis of the object. The combination of stimulus types led to 160 unique stimulus combinations (8 categories × 5 exemplars×2 colors×2 tilts). Each stimulus was presented once in each task context, making a total of 640 stimulus presentations per participant. The presentation order of these stimulus-task combinations was randomized. In addition, we interspersed 80 catch trials that were chosen to be random combinations of task and stimulus (see below).</p><p>All stimuli were presented on black background with a white central fixation cross present throughout the experiment. Object images were greyscale cropped images of objects and were a subset selected from a previous fMRI study (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>). Both task cues (e.g. ‘Content’) and possible responses (e.g. ‘manmade’ or ‘natural’) were shown as words in white font. Task cues were always presented centrally and possible responses were shown left and right of fixation.</p></sec><sec id="s4-3"><title>Procedure</title><p>Prior to the experiment, participants were familiarized with the task by carrying out 36 randomly chosen trials outside of the MEG. For the actual experiment, participants were seated in an electromagnetically shielded MEG chamber with their head placed in the mold of the dewar while stimuli were backprojected on a translucent screen in front of them (viewing distance: 70 cm, image size: 6 degrees of visual angle). Each trial was preceded by a white fixation cross (0.5 s) that turned green (0.5 s) to prepare participants for the upcoming trial. A trial consisted of three major components: (1) A task cue which indicated the relevant task for the trial, (2) an object stimulus which was categorized according to the task, and (3) a response-mapping screen which indicated the task-relevant response options left and right of fixation (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Based on these components, in the following we separate each trial into three different time periods: a ‘Task Cue Period’, an ‘Object Stimulus Period’, and a ‘Response Mapping Period’. Each trial lasted 5 s. A trial began with the Task Cue Period consisting of the presentation of a task cue (0.5 s) followed by a fixation cross (1.5 s). This was followed by the Object Stimulus Period consisting of the presentation of an object stimulus (0.5 s) followed by another fixation cross (1.0 s). Finally, the trial ended with the Response Mapping Period during which a response-mapping screen was displayed (1.5 s). Participants responded with the left or right index finger using an MEG-compatible response box. In addition to the button press, participants were instructed to make an eye blink during the response period to minimize the contribution of eye blink artifacts to other time periods. The order of the options on the response-mapping screen was intermixed randomly to prevent the planning of motor responses before the onset of the response screen (<xref ref-type="bibr" rid="bib25">Hebart et al., 2012</xref>).</p><p>Participants were instructed to encode the task rule as soon as being presented with the task cue and to apply it immediately to the stimulus. To encourage this strategy, they were asked to respond as fast and accurately as possible. To enforce a faster application of task to object category, we introduced catch trials for which the fixation period between stimulus offset and response-mapping screen onset was shortened from 1.0 s to 0.2 s. The experiment consisted of 20 runs of 36 trials each (32 experimental trials, 4 catch trials).</p></sec><sec id="s4-4"><title>MEG recordings and preprocessing</title><p>MEG data were collected on a 275 channel CTF system (MEG International Services, Ltd., Coquitlam, BC, Canada) with a sampling rate of 1200 Hz. Recordings were available from 272 channels (dead channels: MLF25, MRF43, MRO13). Preprocessing and data analysis were carried out using Brainstorm (version 02/2016, <xref ref-type="bibr" rid="bib57">Tadel et al., 2011</xref>) and MATLAB (version 2015b, The Mathworks, Natick, MA). The specifics of preprocessing and multivariate decoding (see below) were based on previously published MEG decoding work (<xref ref-type="bibr" rid="bib10">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Grootswagers et al., 2017</xref>) and fine-tuned on a pilot subject that did not enter the final data set. MEG triggers were aligned to the exact presentation time on the screen that had been recorded using an optical sensor attached to the projection mirror. Data were epoched in 5.1 s trials, starting 100 ms prior to the onset of the task cue and ending with the offset of the response-mapping screen. Then, data were band-pass filtered between 0.1 and 300 Hz and bandstop filtered at 60 Hz including harmonics to remove line noise.</p><p>To further increase SNR and to reduce computational costs, we carried out (1) PCA dimensionality reduction, (2) temporal smoothing on PCA components, and (3) downsampling of the data. For PCA, data were concatenated within each channel across all trials. Note that PCA leads to orthogonal temporal components without mixing the MEG signal in time. After PCA, the components with the lowest 1% of the variance were removed, unless this would remove more than 50% of components. Since all subjects exceeded this 50% criterion, this led to 136 components for all subjects. All further analyses were conducted on the reduced set of principal components. Then, data were normalized relative to the baseline period (for task decoding: −0.1 to 0 s, for object category decoding: 1.9 to 2.0 s). To this end, for each channel we calculated the mean and standard deviation of the baseline period and subtracted this mean from the rest of the data before dividing it by the standard deviation (univariate noise normalization). Finally, the components were temporally smoothed with a Gaussian kernel of ±15 ms half duration at half maximum, and downsampled to 120 Hz (621 samples/trial).</p></sec><sec id="s4-5"><title>Time-resolved multivariate decoding</title><p>Multivariate decoding was carried out using custom-written code in MATLAB (Mathworks, Natick, MA), as well as functions from The Decoding Toolbox (<xref ref-type="bibr" rid="bib26">Hebart et al., 2014</xref>), and LIBSVM (<xref ref-type="bibr" rid="bib8">Chang and Lin, 2011</xref>) using linear support vector machine classification (<italic>C</italic> = 1). Classification was conducted for each participant separately in a time-resolved manner, that is independently for each time point. Each pattern that entered the classification procedure consisted of the principal component scores at a given time point. In the following, we describe one iteration of the multivariate classification procedure that was carried out for the example of object category classification. In the first step, we created supertrials by averaging 10 trials of the same object category without replacement (<xref ref-type="bibr" rid="bib27">Isik et al., 2014</xref>). In the next step, we separated these supertrials into training and testing data, with one supertrial pattern per object category serving as test data and all other supertrial patterns as training data. This was followed by one-vs-one classification of all 28 pairwise comparisons of the eight object categories (chance-level 50%). To test the trained classifier on the left-out data, we compared the two predicted decision values and assigned an accuracy of 100% if the order of the two test samples was predicted correctly and an accuracy of 0% if the order was the opposite (for two samples and two classes this is mathematically equivalent to the common area-under-the-curve measure of classification performance and represents a classification metric that is independent of the bias term of the classifier). In a last step, the resulting pairwise comparisons were averaged, leading to an estimate of the mean accuracy across all comparisons. This training and testing process was then repeated for each time point. This completes the description of one multivariate classification iteration for the decoding of object category. The procedure for task classification was analogous, with four tasks and six pairwise combinations. To achieve a more fine-grained and robust estimate of decoding accuracy, we ran a total of 500 such iterations of trial averaging and classification, and the final accuracy time series reflects the average across these iterations. This provided us with time-resolved estimates of MEG decoding accuracy for object category and task classification, respectively.</p></sec><sec id="s4-6"><title>Temporal generalization of task</title><p>To investigate whether the task representation remained stable across time or whether it changed, we carried out cross-classification across time, also known as temporal generalization analysis (<xref ref-type="bibr" rid="bib31">King and Dehaene, 2014</xref>). The rationale of this method is that if a classifier can generalize from one time point to another, this demonstrates that the representational format is similar for these two time points. If, however, a classifier does not generalize, then under the assumption of stable noise (<xref ref-type="bibr" rid="bib24">Hebart and Baker, 2017</xref>) this indicates that the representational format is different. To carry out this analysis, we repeated the same approach as described in the previous section, but instead of testing a classifier only at a given time point, we tested the same classifier for all other time points separately. This cross-classification analysis was repeated with each time point once serving as training data, yielding a time–time decoding matrix that captures classifier generalization performance across time.</p></sec><sec id="s4-7"><title>Model-based MEG-fMRI fusion for spatiotemporally-resolved information</title><p>To resolve task and category-related information both in time and space simultaneously, we carried out RSA-based MEG-fMRI fusion (<xref ref-type="bibr" rid="bib10">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib11">Cichy et al., 2016</xref>). RSA makes it possible to compare brain patterns across modalities in terms of pattern dissimilarity, abstracting from the activity patterns of measurement channels (e.g. MEG sensors) to all pairwise distances of those patterns in form of a representational dissimilarity matrices (RDMs). RSA-based MEG-fMRI fusion allows a researcher to ask the following question: At what point in time does the representational structure in a given brain area (as determined from fMRI) match the representational structure determined from the time-resolved MEG signal? The reasoning for this approach is that if the fMRI RDM of a brain region and the MEG RDM of a time point show a correspondence, this suggests that there is a shared representational format in a given brain location and at a given point in time. Here, we apply this approach to investigate the spatiotemporal evolution of object category and task representations.</p><p>FMRI RDMs for each combination of task and category (4 × 8 = 32×32 matrices) were available from five regions of interest (ROIs) in 25 participants who took part in a separate study employing the same task (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>). None of these participants overlapped with the sample from the MEG study. The major difference between the MEG and the fMRI experiments were (1) that the fMRI study used an extended set of 6 tasks and (2) the exact timing of each trial was slower and jittered in the fMRI study. Details about data preprocessing have been described previously (<xref ref-type="bibr" rid="bib23">Harel et al., 2014</xref>). RDMs were based on parameter estimates in a GLM for each condition which were converted to <italic>t</italic>-values (univariate noise normalization). Each entry in the matrix reflects one minus the correlation coefficient of the <italic>t</italic>-values across conditions, calculated separately for each ROI. RDMs were reduced to the relevant four task types. The five ROIs were early visual cortex (EVC), object-selective LO and pFS, lateral prefrontal cortex (lPFC) and posterior parietal cortex (PPC). EVC, LO and pFS were defined based on contrasts in an independent visual and object localizer session, and lPFC and PPC were defined by a combination of anatomical criteria and responses in the functional localizer session to the presence of objects.</p><p>For better comparability to this previous study, we created correlation-based MEG pattern dissimilarity matrices for all combinations of task and object category. In particular, for each combination of task and category, we created a mean pattern, yielding a total 32 brain patterns per participant (8 categories × 4 tasks). We then ran a Pearson correlation between all patterns and converted these similarity estimates to dissimilarity estimates (using one minus correlation), providing us with a 32 × 32 RDM for each time point and participant.</p><p>Since different groups of participants were tested in the fMRI and MEG studies, we used the group average pattern dissimilarity matrices of each modality as the best estimate of the true pattern dissimilarity. These RDMs were symmetrical around the diagonal, so we extracted the lower triangular component of each pattern dissimilarity matrix – importantly, excluding the diagonal (<xref ref-type="bibr" rid="bib49">Ritchie et al., 2017</xref>) – and converted them to vector format for further analyses, in the following referred to as representational dissimilarity vector (RDV).</p><p>For a given brain region, we conducted MEG-fMRI fusion by calculating the squared Spearman correlation between the fMRI RDV and the MEG RDV for each time point separately. The squared correlation coefficient is mathematically equivalent to the coefficient of determination (<italic>R<sup>2</sup></italic>) of the fMRI RDV explaining the MEG RDV. This approach was repeated for each fMRI RDV of the five ROIs, providing us with five time courses of representational similarity between MEG and fMRI.</p><p>While MEG-fMRI fusion provides a temporal profile of representational similarities for a given brain region, these MEG-fMRI fusion time courses do not distinguish whether MEG-fMRI representational similarities reflect task, object category, or a mixture of the two. To disentangle task and object category-related information with MEG-fMRI fusion, we extended this approach by introducing model RDMs of the same size (32 × 32). These RDMs reflected the expected dissimilarity for the representation of task and category, respectively, with entries of 1 for high expected dissimilarity (different task/category) and 0 for low expected dissimilarity (same task/category). This model-based MEG-fMRI fusion approach was carried out using commonality analysis (<xref ref-type="bibr" rid="bib51">Seibold and McPHEE, 1979</xref>), a variance decomposition approach that makes it possible to estimate the shared variance between more than two variables (see <xref ref-type="bibr" rid="bib19">Greene et al., 2016</xref>), for a similar approach using multiple model RDMs). For a given brain region and time point, these variables reflect (1) an MEG RDV, (2) an fMRI RDV and (3) the two model RDVs for task and object category representations.</p><p>A schematic of this model-based MEG-fMRI fusion is shown in <xref ref-type="fig" rid="fig6">Figure 6A</xref>. We conducted commonality analysis by comparing two squared semi-partial correlation coefficients (Spearman correlation), one reflecting the proportion of variance shared between MEG and fMRI partialling out all model variables excluding the variable of interest (e.g. task) from fMRI, and the other reflecting the proportion of shared variance when partialling out all model variables from fMRI including this variable of interest. The difference between both coefficients of determination (<italic>R<sup>2</sup></italic>) then provides the commonality, which is the variance shared between MEG and fMRI that is uniquely explained by the variable of interest. Formally, the commonality at time <italic>t</italic> and location <italic>j</italic> can be described as:<disp-formula id="equ1"><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>-</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></disp-formula>where <italic>X</italic> reflects MEG, <italic>Y</italic> reflect fMRI, <italic>A</italic> reflects task, and <italic>B</italic> reflects object category. Note that this variable can become slightly larger than the total <italic>R<sup>2</sup></italic> or slightly negative, due to numerical inaccuracies or the presence of small suppression effects (<xref ref-type="bibr" rid="bib42">Pedhazur, 1997</xref>). In addition, commonality coefficients always reflect the shared variance relative to a target variable (in our case MEG), but depending on the relationship between the variables the estimate of shared variance can change when a different target variable is used (in our case fMRI). In the present study, the pattern of results was comparable irrespective of which variable served as a target variable.</p></sec><sec id="s4-8"><title>Statistical testing</title><p>Throughout this article, we used a non-parametric, cluster-based statistical approach to test for time periods during which the group of participants showed a significant effect (<xref ref-type="bibr" rid="bib41">Nichols and Holmes, 2002</xref>), and bootstrap sampling to determine confidence intervals for peak latencies and peak latency differences. We did not compute statistics in time periods after the onset of the response-mapping screen, because (1) these time periods were corrupted by the instructed eye blinks and (2) information about task is contained in the response-mapping screen, making it difficult to uniquely assign these responses to task or response-mapping screen. For object category-related responses we did not compute statistics for time periods prior to the onset of the object stimulus, because it was not reasonable to assume that these periods would contain information about the category before its identity is revealed. For completeness, however, we plot these results in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>. Please note that the pattern of results reported is very similar when including these time periods into our statistical analyses.</p><sec id="s4-8-1"><title>Non-parametric cluster-based statistical approach</title><p>We carried out a non-parametric, cluster-based statistical analysis using the maximum cluster size method (<xref ref-type="bibr" rid="bib41">Nichols and Holmes, 2002</xref>). Clusters were defined as neighboring time points that all exceed a statistical cutoff (cluster-inducing threshold). This cutoff was determined through a sign-permutation test based on the distribution of <italic>t</italic>-values from all possible permutations of the measured accuracy values (2<sup>17</sup> = 131,072). The cluster-inducing threshold was defined as the 95<sup>th</sup> percentile of the distribution at each time point (equivalent to p&lt;0.05, one-sided). To identify significant clusters, we determined the 95<sup>th</sup> percentile of maximum cluster sizes across all permutations (equivalent to p&lt;0.05, one-sided). This provided us with significant clusters at the pre-specified statistical cutoffs.</p><p>For temporal generalization matrices, we extended the cluster-based approach described above to two dimensions, revealing significant 2D clusters. Because of computational limitations, we ran only a subset of 10,000 permutations drawn randomly without replacement from all available permutations.</p><p>For model-based MEG-fMRI fusion, we used an approach similar to that described above. However, instead of running a sign-permutation test across participants, we conducted a randomization test for which we created 5000 MEG similarity matrices for each of the five ROIs. These matrices were based on random permutations of the rows and columns of the group average MEG similarity matrix (<xref ref-type="bibr" rid="bib36">Kriegeskorte et al., 2008</xref>). We then carried out model-based MEG-fMRI fusion using these matrices to create an estimated null distribution of information time courses for each ROI. For each time point in each ROI, a cluster-inducing threshold was determined by choosing the 95<sup>th</sup> percentile of this estimated null distribution (equivalent to p&lt;0.05, one-sided). This was followed by determining the maximum cluster sizes across all permutations as described above, but across all ROIs to correct for multiple comparisons (equivalent to p&lt;0.05, one-sided, corrected for multiple comparisons across ROIs).</p></sec><sec id="s4-8-2"><title>Determining confidence intervals for peak latencies</title><p>We used bootstrap sampling to estimate the 95% confidence intervals (CI) of peak latencies and peak latency differences, respectively. For each iteration of the bootstrap sampling approach, we calculated a time course based on the bootstrap sample. For multivariate decoding analyses, this was a time course of accuracy from an average of <italic>n</italic> = 17 time courses of decoding accuracy sampled with replacement from the pool of subjects. For MEG-fMRI fusion, this was a time course of commonality coefficients, generated by sampling <italic>n</italic> = 17 time courses of MEG similarity matrices from the pool of subjects with replacement, averaging them, and repeating the model-based MEG-fMRI fusion approach as described above. For each bootstrap sample time course, we then calculated timing estimates in the relevant time periods (for peak latency: timing of maximum, for peak latency difference: time difference between maxima). This process was repeated (100,000 times for multivariate decoding and 5000 times for MEG-fMRI fusion), which generated a distribution of timing estimates. The 2.5 and 97.5 percentiles of this distribution reflect the 95% confidence interval of the true timing estimate. Since we downsampled our data (bin width: 8.33 ms), the confidence intervals were conservative and overestimated by up to 16.67 ms.</p></sec></sec><sec id="s4-9"><title>Source data and code</title><p>Data and Matlab code used for statistical analyses and producing results in the main figures is available as Source Data and Source Code (source code one file) with this article.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Maryam Vaziri-Pashkam for helpful discussions and Matthias Guggenmos and Edward Silson for helpful comments on previous versions of our manuscript. This work was supported by the Intramural Research Program of the National Institutes of Health (ZIA-MH-002909) - National Institute of Mental Health Clinical Study Protocol 93 M-0170, NCT00001360, the German Research Foundation (Emmy Noether Grant CI241-1/1), and a Feodor-Lynen fellowship of the Humboldt Foundation to MNH.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing—original draft, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants gave written informed consent as part of the study protocol (93-M-0170, NCT00001360) prior to participation in the study. The study was approved by the Institutional Review Board of the National Institutes of Health and was conducted according to the Declaration of Helsinki.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><object-id pub-id-type="doi">10.7554/eLife.32816.016</object-id><label>Source code 1.</label><caption><title>Matlab scripts including helper functions to produce <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6</xref> based on available source data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-32816-code1-v2.zip"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.32816.017</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-32816-transrepform-v2.pdf"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bankson</surname> <given-names>BB</given-names></name><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Groen II</surname> <given-names>BCI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/223990</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belongie</surname> <given-names>S</given-names></name><name><surname>Malik</surname> <given-names>J</given-names></name><name><surname>Puzicha</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Shape matching and object recognition using shape contexts</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>24</volume><fpage>509</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1109/34.993558</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bode</surname> <given-names>S</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding sequential stages of task preparation in the human brain</article-title><source>NeuroImage</source><volume>45</volume><fpage>606</fpage><lpage>613</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.11.031</pub-id><pub-id pub-id-type="pmid">19111624</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Daniels</surname> <given-names>N</given-names></name><name><surname>Op de Beeck</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task context overrules object- and category-related representational content in the human parietal cortex</article-title><source>Cerebral Cortex</source><volume>84</volume><fpage>1-12</fpage><pub-id pub-id-type="doi">10.1093/cercor/bhw419</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Op de Beeck</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>432</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2314-15.2016</pub-id><pub-id pub-id-type="pmid">26758835</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bugatus</surname> <given-names>L</given-names></name><name><surname>Weiner</surname> <given-names>KS</given-names></name><name><surname>Grill-Spector</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task alters category representations in prefrontal but not high-level visual cortex</article-title><source>NeuroImage</source><volume>155</volume><fpage>437</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.062</pub-id><pub-id pub-id-type="pmid">28389381</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname> <given-names>T</given-names></name><name><surname>Tovar</surname> <given-names>DA</given-names></name><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational dynamics of object vision: the first 1000 ms</article-title><source>Journal of Vision</source><volume>13</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1167/13.10.1</pub-id><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>C-C</given-names></name><name><surname>Lin</surname> <given-names>C-J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIBSVM: a library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><fpage>27</fpage></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Encoding the identity and location of objects in human LOC</article-title><source>NeuroImage</source><volume>54</volume><fpage>2297</fpage><lpage>2307</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.044</pub-id><pub-id pub-id-type="pmid">20869451</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Similarity-based fusion of meg and fmri reveals spatio-temporal dynamics in human cortex during visual object recognition</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3563</fpage><lpage>3579</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw135</pub-id><pub-id pub-id-type="pmid">27235099</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname> <given-names>A</given-names></name><name><surname>Devereux</surname> <given-names>BJ</given-names></name><name><surname>Randall</surname> <given-names>B</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predicting the time course of individual objects with MEG</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3602</fpage><lpage>3612</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu203</pub-id><pub-id pub-id-type="pmid">25209607</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name><name><surname>Zoccolan</surname> <given-names>D</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>172</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.004</pub-id><pub-id pub-id-type="pmid">20171926</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emadi</surname> <given-names>N</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Behavioral demand modulates object category representation in the inferior temporal cortex</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>2628</fpage><lpage>2637</lpage><pub-id pub-id-type="doi">10.1152/jn.00761.2013</pub-id><pub-id pub-id-type="pmid">25080572</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erez</surname> <given-names>Y</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Discrimination of visual categories based on behavioral relevance in widespread regions of frontoparietal cortex</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>12383</fpage><lpage>12393</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1134-15.2015</pub-id><pub-id pub-id-type="pmid">26354907</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname> <given-names>DJ</given-names></name><name><surname>Assad</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neuronal mechanisms of visual categorization: An abstract view on decision making</article-title><source>Annual Review of Neuroscience</source><volume>39</volume><fpage>129</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-071714-033919</pub-id><pub-id pub-id-type="pmid">27070552</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname> <given-names>DJ</given-names></name><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A comparison of primate prefrontal and inferior temporal cortices during visual categorization</article-title><source>Journal of Neuroscience</source><volume>23</volume><fpage>5235</fpage><lpage>5246</lpage><pub-id pub-id-type="pmid">12832548</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greene</surname> <given-names>MR</given-names></name><name><surname>Baldassano</surname> <given-names>C</given-names></name><name><surname>Esteva</surname> <given-names>A</given-names></name><name><surname>Beck</surname> <given-names>DM</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual scenes are categorized by function</article-title><source>Journal of Experimental Psychology: General</source><volume>145</volume><fpage>82</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1037/xge0000129</pub-id><pub-id pub-id-type="pmid">26709590</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname> <given-names>K</given-names></name><name><surname>Kushnir</surname> <given-names>T</given-names></name><name><surname>Edelman</surname> <given-names>S</given-names></name><name><surname>Avidan</surname> <given-names>G</given-names></name><name><surname>Itzchak</surname> <given-names>Y</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Differential processing of objects under various viewing conditions in the human lateral occipital complex</article-title><source>Neuron</source><volume>24</volume><fpage>187</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80832-6</pub-id><pub-id pub-id-type="pmid">10677037</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname> <given-names>II</given-names></name><name><surname>Ghebreab</surname> <given-names>S</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The time course of natural scene perception with reduced attention</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>931</fpage><lpage>946</lpage><pub-id pub-id-type="doi">10.1152/jn.00896.2015</pub-id><pub-id pub-id-type="pmid">26609116</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname> <given-names>T</given-names></name><name><surname>Wardle</surname> <given-names>SG</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decoding dynamic brain patterns from evoked responses: A tutorial on multivariate pattern analysis applied to time series neuroimaging data</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>677</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01068</pub-id><pub-id pub-id-type="pmid">27779910</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harel</surname> <given-names>A</given-names></name><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Task context impacts visual object processing differentially across the cortex</article-title><source>PNAS</source><volume>111</volume><fpage>E962</fpage><lpage>E971</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312567111</pub-id><pub-id pub-id-type="pmid">24567402</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deconstructing multivariate decoding for the study of brain function</article-title><source>NeuroImage</source><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.005</pub-id><pub-id pub-id-type="pmid">28782682</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Donner</surname> <given-names>TH</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Human visual and parietal cortex encode visual choices independent of motor plans</article-title><source>NeuroImage</source><volume>63</volume><fpage>1393</fpage><lpage>1403</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.08.027</pub-id><pub-id pub-id-type="pmid">22922368</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Görgen</surname> <given-names>K</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00088</pub-id><pub-id pub-id-type="pmid">25610393</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname> <given-names>L</given-names></name><name><surname>Meyers</surname> <given-names>EM</given-names></name><name><surname>Leibo</surname> <given-names>JZ</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The dynamics of invariant object recognition in the human visual system</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1152/jn.00394.2013</pub-id><pub-id pub-id-type="pmid">24089402</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>Brady</surname> <given-names>DK</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Attention improves encoding of task-relevant features in the human visual cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>8210</fpage><lpage>8219</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6153-09.2011</pub-id><pub-id pub-id-type="pmid">21632942</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname> <given-names>SK</given-names></name><name><surname>Xu</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Behaviorally relevant abstract object identity representation in the human parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1607</fpage><lpage>1619</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1016-15.2016</pub-id><pub-id pub-id-type="pmid">26843642</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Azzalini</surname> <given-names>DC</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Shape-independent object category responses revealed by MEG and fMRI decoding</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>2246</fpage><lpage>2250</lpage><pub-id pub-id-type="doi">10.1152/jn.01074.2015</pub-id><pub-id pub-id-type="pmid">26740535</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id><pub-id pub-id-type="pmid">24593982</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Brouwer</surname> <given-names>GJ</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Prior expectations bias sensory representations in visual cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>16275</fpage><lpage>16284</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0742-13.2013</pub-id><pub-id pub-id-type="pmid">24107959</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konen</surname> <given-names>CS</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Two hierarchically organized neural systems for object information in human visual cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>224</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1038/nn2036</pub-id><pub-id pub-id-type="pmid">18193041</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>High-level visual object representations are constrained by position</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>2916</fpage><lpage>2925</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq042</pub-id><pub-id pub-id-type="pmid">20351021</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowe</surname> <given-names>MX</given-names></name><name><surname>Gallivan</surname> <given-names>JP</given-names></name><name><surname>Ferber</surname> <given-names>S</given-names></name><name><surname>Cant</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Feature diagnosticity and task context shape activity in human scene-selective cortex</article-title><source>NeuroImage</source><volume>125</volume><fpage>681</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.10.089</pub-id><pub-id pub-id-type="pmid">26541082</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKee</surname> <given-names>JL</given-names></name><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Freedman</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Task dependence of visual and category representations in prefrontal and inferior temporal cortices</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>16065</fpage><lpage>16075</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1660-14.2014</pub-id><pub-id pub-id-type="pmid">25429147</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname> <given-names>EM</given-names></name><name><surname>Freedman</surname> <given-names>DJ</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamic population coding of category information in inferior temporal and prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>1407</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1152/jn.90248.2008</pub-id><pub-id pub-id-type="pmid">18562555</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname> <given-names>SA</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Guntupalli</surname> <given-names>JS</given-names></name><name><surname>Visconti di Oleggio Castello</surname> <given-names>M</given-names></name><name><surname>Gors</surname> <given-names>J</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention selectively reshapes the geometry of distributed semantic representation</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>4277</fpage><lpage>4291</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx138</pub-id><pub-id pub-id-type="pmid">28591837</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname> <given-names>TE</given-names></name><name><surname>Holmes</surname> <given-names>AP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Nonparametric permutation tests for functional neuroimaging: a primer with examples</article-title><source>Human Brain Mapping</source><volume>15</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id><pub-id pub-id-type="pmid">11747097</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pedhazur</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>Multiple Regression in Behavioral Research: Explanation and Prediction</source><publisher-loc>Orlando, FL</publisher-loc><publisher-name>Harcourt Brace</publisher-name></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural mechanisms of rapid natural scene categorization in human visual cortex</article-title><source>Nature</source><volume>460</volume><fpage>94</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1038/nature08103</pub-id><pub-id pub-id-type="pmid">19506558</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A neural basis for real-world visual search in human occipitotemporal cortex</article-title><source>PNAS</source><volume>108</volume><fpage>12125</fpage><lpage>12130</lpage><pub-id pub-id-type="doi">10.1073/pnas.1101042108</pub-id><pub-id pub-id-type="pmid">21730192</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname> <given-names>B</given-names></name><name><surname>Bledowski</surname> <given-names>C</given-names></name><name><surname>Rieder</surname> <given-names>M</given-names></name><name><surname>Kaiser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Recurrence of task set-related MEG signal patterns during auditory working memory</article-title><source>Brain Research</source><volume>1640</volume><fpage>232</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2015.12.006</pub-id><pub-id pub-id-type="pmid">26683086</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Proklova</surname> <given-names>D</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Peelen</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>MEG sensor patterns reflect perceptual but not categorical similarity of animate and inanimate objects</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/238584</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname> <given-names>D</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Disentangling representations of object shape and object category in human visual cortex: The animate-inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00924</pub-id><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural mechanisms of object recognition</article-title><source>Current Opinion in Neurobiology</source><volume>12</volume><fpage>162</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(02)00304-5</pub-id><pub-id pub-id-type="pmid">12015232</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname> <given-names>JB</given-names></name><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Op de Beeck</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Avoiding illusory effects in representational similarity analysis: What (not) to do with the diagonal</article-title><source>NeuroImage</source><volume>148</volume><fpage>197</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.12.079</pub-id><pub-id pub-id-type="pmid">28069538</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname> <given-names>JB</given-names></name><name><surname>Tovar</surname> <given-names>DA</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Emerging object representations in the visual system predict reaction times for categorization</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004316</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004316</pub-id><pub-id pub-id-type="pmid">26107634</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seibold</surname> <given-names>DR</given-names></name><name><surname>McPHEE</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Commonality analysis: A method for decomposing explained variance in multiple regression analyses</article-title><source>Human Communication Research</source><volume>5</volume><fpage>355</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1111/j.1468-2958.1979.tb00649.x</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A feedforward architecture accounts for rapid categorization</article-title><source>PNAS</source><volume>104</volume><fpage>6424</fpage><lpage>6429</lpage><pub-id pub-id-type="doi">10.1073/pnas.0700622104</pub-id><pub-id pub-id-type="pmid">17404214</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname> <given-names>M</given-names></name><name><surname>Buschman</surname> <given-names>TJ</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical information flow during flexible sensorimotor decisions</article-title><source>Science</source><volume>348</volume><fpage>1352</fpage><lpage>1355</lpage><pub-id pub-id-type="doi">10.1126/science.aab0551</pub-id><pub-id pub-id-type="pmid">26089513</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sigala</surname> <given-names>N</given-names></name><name><surname>Kusunoki</surname> <given-names>M</given-names></name><name><surname>Nimmo-Smith</surname> <given-names>I</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Hierarchical coding for sequential task events in the monkey prefrontal cortex</article-title><source>PNAS</source><volume>105</volume><fpage>11969</fpage><lpage>11974</lpage><pub-id pub-id-type="doi">10.1073/pnas.0802569105</pub-id><pub-id pub-id-type="pmid">18689686</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoet</surname> <given-names>G</given-names></name><name><surname>Snyder</surname> <given-names>LH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Single neurons in posterior parietal cortex of monkeys encode cognitive set</article-title><source>Neuron</source><volume>42</volume><fpage>1003</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.06.003</pub-id><pub-id pub-id-type="pmid">15207244</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname> <given-names>MG</given-names></name><name><surname>Kusunoki</surname> <given-names>M</given-names></name><name><surname>Sigala</surname> <given-names>N</given-names></name><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id><pub-id pub-id-type="pmid">23562541</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadel</surname> <given-names>F</given-names></name><name><surname>Baillet</surname> <given-names>S</given-names></name><name><surname>Mosher</surname> <given-names>JC</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Leahy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Brainstorm: a user-friendly application for MEG/EEG analysis</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1155/2011/879716</pub-id><pub-id pub-id-type="pmid">21584256</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Nieuwenhuijzen</surname> <given-names>ME</given-names></name><name><surname>Backus</surname> <given-names>AR</given-names></name><name><surname>Bahramisharif</surname> <given-names>A</given-names></name><name><surname>Doeller</surname> <given-names>CF</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>MEG-based decoding of the spatiotemporal dynamics of visual category perception</article-title><source>NeuroImage</source><volume>83</volume><fpage>1063</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.07.075</pub-id><pub-id pub-id-type="pmid">23927900</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname> <given-names>R</given-names></name><name><surname>Thorpe</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The time course of visual processing: from early perception to decision-making</article-title><source>Journal of Cognitive Neuroscience</source><volume>13</volume><fpage>454</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1162/08989290152001880</pub-id><pub-id pub-id-type="pmid">11388919</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaziri-Pashkam</surname> <given-names>M</given-names></name><name><surname>Xu</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Goal-directed visual processing differentially impacts human ventral and dorsal visual representations</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8767</fpage><lpage>8782</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3392-16.2017</pub-id><pub-id pub-id-type="pmid">28821655</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname> <given-names>JD</given-names></name><name><surname>Anderson</surname> <given-names>KC</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Single neurons in prefrontal cortex encode abstract rules</article-title><source>Nature</source><volume>411</volume><fpage>953</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1038/35082081</pub-id><pub-id pub-id-type="pmid">11418860</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waskom</surname> <given-names>ML</given-names></name><name><surname>Kumaran</surname> <given-names>D</given-names></name><name><surname>Gordon</surname> <given-names>AM</given-names></name><name><surname>Rissman</surname> <given-names>J</given-names></name><name><surname>Wagner</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Frontoparietal representations of task context support the flexible control of goal-directed cognition</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>10743</fpage><lpage>10755</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5282-13.2014</pub-id><pub-id pub-id-type="pmid">25100605</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolgar</surname> <given-names>A</given-names></name><name><surname>Thompson</surname> <given-names>R</given-names></name><name><surname>Bor</surname> <given-names>D</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multi-voxel coding of stimuli, rules, and responses in human frontoparietal cortex</article-title><source>NeuroImage</source><volume>56</volume><fpage>744</fpage><lpage>752</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.04.035</pub-id><pub-id pub-id-type="pmid">20406690</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Çukur</surname> <given-names>T</given-names></name><name><surname>Nishimoto</surname> <given-names>S</given-names></name><name><surname>Huth</surname> <given-names>AG</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention during natural vision warps semantic representation across the human brain</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>763</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1038/nn.3381</pub-id><pub-id pub-id-type="pmid">23603707</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.32816.021</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Culham</surname><given-names>Jody C</given-names></name><role>Reviewing Editor</role><aff id="aff7"><institution>University of Western Ontario</institution><country>Canada</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;The representational dynamics of task and object category processing in humans&quot; for consideration by <italic>eLife</italic>. Your article has been favorably evaluated by David Van Essen (Senior Editor) and two reviewers, one of whom, Jody C Culham (Reviewer #1), is a member of our Board of Reviewing Editors and the other is Stefania Bracci (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>Both reviewers were positive about the manuscript, particularly the novel combination of MEG and fMRI to address the influence of task goals upon object recognition unfolds over time. Results complement previous work and provide novel results by characterising how task and object representations evolve and interact throughout the visual hierarchy.</p><p>Essential revisions:</p><p>1) Key methods in the main text should be clarified.</p><p>Specifically, the exact inputs into the classifiers should be explained briefly in the main text (and perhaps also more explicitly in the detailed methods). In the Cichy et al. (2014, Nat Neurosci) paper, it looks like the data put into RSMs was signal strength at each spatial channel. In the present paper, some sort of PCA was done to extract seemingly spatiotemporal components. Are the features being decoded purely spatial or is there any component of rates of temporal change? Some explanation in the main text, not just the Materials and methods, perhaps along with a schematic in the figures (as in Figures 1B and 4A of Cichy) would help readers understand key elements needed to understand this as-yet-not-that-common combination of fMRI and MEG RSA.</p><p>The commonality analysis (subsection “Model-based MEG-fMRI Fusion for Spatiotemporally-Resolved Neural Dynamics of Task and Object Category”, first paragraph) could have also been summarized more clearly.</p><p>2) Some additional discussion of the interpretation of MEG-fMRI correlations is warranted.</p><p>If the MEG features are purely spatial, then what does it mean to say that the voxelwise pattern of activation in a region measured with fMRI is correlated with a sensorwise pattern of activation in MEG. If there are univariate as well as multivariate activation changes in the region, then is this relationship somewhat trivial? As the spatial features (voxels) in fMRI that contribute to successful classification can be plotted (voxel weight maps), can the sensors in MEG that contribute to classification be plotted and compared with the fMRI maps.</p><p>3) The reviewers have some concerns about the degree to which object decoding representations reflect specific object categories (e.g., cow), or object-specific visual properties. This should be addressed in the Discussion and/or the strength of the claim for category specificity should be toned down. The authors may wish to consider an additional analysis suggested by one reviewer.</p><p><italic>Reviewer 1 notes:</italic> </p><p>In Figure 1B, labels under the categories or more information in the figure legend would help (I had to look at the detailed methods to learn that the categories shown were specific categories not the general categories typically studied in ventral-stream processing (e.g., cows not animals). After seeing reviewer 2's comments, I too wonder how strong the category-specific arguments can be when the stimuli would have been even more visually similar than in most studies of category-selective processing.</p><p><italic>Reviewer 2 notes:</italic> </p><p>In Figure 2, the authors show significant object decoding (regardless of task) during the object stimulus period. I wonder to what extent these object – presumably, objects within the same category share very similar visual properties (e.g., shape, texture, luminance etc.). Some of the results shown make me think that the latter interpretation might be possible. In particularly, the highest decoding accuracy is found immediately after image onset. Subsequently, decoding accuracy smoothly and constantly decreases (Figure 2). A similar pattern is also visible in Figure 4A. After object onset, during the period of highest decoding accuracy, there was no decoding difference between conceptual and perceptual tasks (this difference emerges later on), thus suggesting that this effect might be related to objects' visual attributes. To address this point the authors could run an additional temporal generalisation analysis (Figure 3A) to test pattern similarities across time for object categories. If during object onsets the results show two different clusters, results would point to different cognitive processes across time. In my view, it would be a nice addition the current analyses.</p><p>I also wonder whether the authors found similar patterns of object decoding for all 8 conditions? Based on evidence from the literature, I would expect some conditions (e.g., animals) to show better decoding than others (e.g., plants).</p><p>Related to the first comment, regarding the model-based MEG-fMRI fusion analysis shown in Figure 5, the authors state: &quot;…EVC, LO and pFS exhibited a mixture of task and category-related information during the Object Stimulus Period, with relative increases in the size of task-related effects when moving up the visual cortical hierarchy&quot;. Wouldn't you expect the same hierarchical increasing pattern for object representations? Instead, looking at the figure, it seems that the object category model explains more variance in EVC than pFS. I wonder whether a visual model (e.g., shape – such as the geometric blur descriptor (Belongie et al., 2002), which captures object shape similarities irrespective of orientation) would do a better work – especially in EVC and LO. Maybe the authors could discuss this possibility in the Discussion section.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.32816.022</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Key methods in the main text should be clarified.</p><p>Specifically, the exact inputs into the classifiers should be explained briefly in the main text (and perhaps also more explicitly in the detailed methods). In the Cichy et al. (2014, Nat Neurosci) paper, it looks like the data put into RSMs was signal strength at each spatial channel. In the present paper, some sort of PCA was done to extract seemingly spatiotemporal components. Are the features being decoded purely spatial or is there any component of rates of temporal change? Some explanation in the main text, not just the Materials and methods, perhaps along with a schematic in the figures (as in Figures 1B and 4A of Cichy) would help readers understand key elements needed to understand this as-yet-not-that-common combination of fMRI and MEG RSA.</p></disp-quote><p>We thank the reviewers for highlighting the need for clarification. Indeed, the methods used in this article deviated slightly from those in Cichy et al. (2014). To answer the specific question, the features being decoded are purely spatial. Following the suggestion of Grootswagers et al. (2016), we used standard PCA as a preprocessing step to reduce the dimensionality of the MEG data by removing the components with the smallest 1% of variance explained. This served the purpose of speeding up analyses and increasing the sensitivity of decoding. While PCA was indeed carried out across time, importantly this preprocessing step does not mix signals in the temporal domain, i.e. decoding and similarity analyses can still be interpreted as being temporally-resolved.</p><p>Following the reviewers’ suggestion, we now better explain the input to the classifiers in the Results section (main text) and better highlight the consequences of the preprocessing steps in the Materials and methods. In addition, we introduce a new figure (<xref ref-type="fig" rid="fig2">Figure 2</xref>) illustrating the methods used for decoding and generating MEG dissimilarity matrices (see <xref ref-type="fig" rid="fig6">Figure 6A</xref> for a schematic of model-based MEG-fMRI fusion). Finally, we extended the text in the Results section to better describe the steps of MEG-fMRI fusion (see response to next comment).</p><p>Results:</p><p>“All MEG analyses were carried out in a time-resolved manner. Prior to multivariate analyses, to speed up computations and increase sensitivity (Grootswagers et al., 2016), MEG sensor patterns (272 channels) were spatially transformed using principal component analysis, followed by removal of the components with the lowest 1% of variance, temporal smoothing (15 ms half duration at half maximum) and downsampling (120 samples / s). […] This provided temporal profiles of two resulting classification time courses, one for objects averaged across task, and one for task averaged across objects (<xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3–source data 1</xref>).”</p><p>Materials and methods:</p><p>“For PCA, data were concatenated within each channel across all trials. Note that PCA leads to orthogonal temporal components without mixing the MEG signal in time. After PCA, the […]”</p><disp-quote content-type="editor-comment"><p>The commonality analysis (subsection “Model-based MEG-fMRI Fusion for Spatiotemporally-Resolved Neural Dynamics of Task and Object Category”, first paragraph) could have also been summarized more clearly.</p></disp-quote><p>We modified and expanded the text to summarize the commonality analysis more clearly in the Results section:</p><p>“Similarity between an fMRI RDM and MEG RDMs indicates a representational format common to that location (i.e. ROI) and those time points (for fMRI and MEG RDMs, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> and Figure 6—figure supplement 2). […] The procedure results in localized time courses of task-specific and object-specific information.”</p><disp-quote content-type="editor-comment"><p>2) Some additional discussion of the interpretation of MEG-fMRI correlations is warranted.</p><p>If the MEG features are purely spatial, then what does it mean to say that the voxelwise pattern of activation in a region measured with fMRI is correlated with a sensorwise pattern of activation in MEG. If there are univariate as well as multivariate activation changes in the region, then is this relationship somewhat trivial? As the spatial features (voxels) in fMRI that contribute to successful classification can be plotted (voxel weight maps), can the sensors in MEG that contribute to classification be plotted and compared with the fMRI maps.</p></disp-quote><p>As noted above, the MEG features are purely spatial, but it is important to realize that we are correlating RDMs derived from MEG and fMRI features and not directly correlating the two signals (see <xref ref-type="fig" rid="fig6">Figure 6A</xref>). The reviewers also raise an interesting question concerning the relationship between univariate and multivariate effects. We recently addressed the question of univariate vs. multivariate regional responses in a review/opinion article (Hebart and Baker, 2017). One conclusion in this article is that even a distributed (multivariate) representation may originate from a simple one-dimensional representation, which would be expressed as a univariate signal after appropriate (linear) spatial transformations. For that reason, we believe the question of simple or complex relationships to be a general element of representational similarity analysis, irrespective of whether responses are driven by univariate or multivariate responses. However, it is generally possible that RSA results are driven by a one-dimensional representation, which would lead to low dissimilarity for only one condition, e.g. one object alone and which could drive RSA results. To address this issue, we have added all fMRI ROI RDMs (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>) and a movie of MEG RDMs (Figure 6—figure supplement 2) demonstrating the complexity of the RDMs at most times, making it unlikely that the results can be accounted for by a simple one-dimensional representation. We discuss this result and the caveats in the interpretability of results derived from multivariate analysis.</p><p>Discussion:</p><p>“While it is possible that the MEG-fMRI fusion results found in this study are driven by a small subset of conditions or a simpler one-dimensional representation, we believe this to be unlikely based on the complexity of the empirically observed MEG and fMRI RDMs (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> and Figure 6—figure supplement 2). However, future studies are required to assess the degree to which complex patterns found in multivariate analyses are driven by low-dimensional representations (Hebart and Baker, 2017).”</p><p>In addition, to address the reviewers’ suggestion, we have made a figure of sensor pattern maps for relevant time points during task and object decoding (see <xref ref-type="fig" rid="respfig1">Author response image 1</xref>). We used the method of Haufe et al. (2014, Neuroimage) to convert the weight maps – which are difficult to interpret (see Hebart &amp; Baker, 2017) – to more interpretable pattern maps. While such pattern maps can give some intuition about the importance of particular sensors, their interpretability is nevertheless limited, for the following reasons. While such maps may highlight sensors implicated in successful decoding, low decoding accuracies lead to unreliable pattern map estimates. In addition, for the same reason even during time periods of high decoding accuracies sensors that pick up weak representations may not be highlighted in these maps. Further, for objects the pattern maps are the average of all 28 pairwise comparisons (6 pairwise comparisons for task) and are then averaged across participants. Any difference in the fine-scale pattern maps specific to one pair of conditions or specific to any participant would therefore wash out, even though one such map may highlight highly relevant sensors in different regions. For all those reasons, we would prefer not to directly compare those maps or draw any conclusions based on the sensor pattern maps.</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.32816.019</object-id><label>Author response image 1.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-32816-resp-fig1-v2"/></fig><disp-quote content-type="editor-comment"><p>3) The reviewers have some concerns about the degree to which object decoding representations reflect specific object categories (e.g., cow), or object-specific visual properties. This should be addressed in the Discussion and/or the strength of the claim for category specificity should be toned down. The authors may wish to consider an additional analysis suggested by one reviewer.</p></disp-quote><p>We would like to thank the reviewers for this important remark. In our revised manuscript, we address this issue in multiple ways: First, we now discuss the results more generally as object processing. Second, we address the role of low-level vs. high-level processing of objects in the Discussion section. Third, we ran the analysis suggested by reviewer 2. Please see below for a point-by-point response.</p><disp-quote content-type="editor-comment"><p>Reviewer 1 notes:</p><p>In Figure 1B, labels under the categories or more information in the figure legend would help (I had to look at the detailed methods to learn that the categories shown were specific categories not the general categories typically studied in ventral-stream processing (e.g., cows not animals). After seeing reviewer 2's comments, I too wonder how strong the category-specific arguments can be when the stimuli would have been even more visually similar than in most studies of category-selective processing.</p></disp-quote><p>We appreciate the opportunity to improve the clarity of our manuscript and have added object labels under the objects in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. We agree with both reviewers and have now modified the text to make clear that we were not intending to make a category-specific argument. See our response to reviewer 2 below for a detailed response.</p><disp-quote content-type="editor-comment"><p>Reviewer 2 notes:</p><p>In Figure 2, the authors show significant object decoding (regardless of task) during the object stimulus period. I wonder to what extent these object – presumably, objects within the same category share very similar visual properties (e.g., shape, texture, luminance etc.). Some of the results shown make me think that the latter interpretation might be possible. In particularly, the highest decoding accuracy is found immediately after image onset. Subsequently, decoding accuracy smoothly and constantly decreases (Figure 2). A similar pattern is also visible in Figure 4A. After object onset, during the period of highest decoding accuracy, there was no decoding difference between conceptual and perceptual tasks (this difference emerges later on), thus suggesting that this effect might be related to objects' visual attributes. To address this point the authors could run an additional temporal generalisation analysis (Figure 3A) to test pattern similarities across time for object categories. If during object onsets the results show two different clusters, results would point to different cognitive processes across time. In my view, it would be a nice addition the current analyses.</p></disp-quote><p>There are two parts to the reviewer’s comment. First, the reviewer wonders about the contribution of visual properties to the object decoding. We would like to thank the reviewer for this insightful comment. We realize that our use of the term “object category” may imply an intention to study task effects exclusively during high-level semantic / conceptual processing of objects. However, we were interested more generally in the processing of task and the processing of objects while varying task, which likely includes both low-level and high-level object processing. Based on the reviewer’s input, throughout the manuscript we have changed any mention of “<italic>object category</italic>” or “<italic>category</italic>” to “<italic>object</italic>”. In addition, we now discuss the question of categorical brain responses as a prospect for future studies in the Discussion section.</p><p>Second, the reviewer proposed an interesting additional analysis to investigate whether there are different phases of object processing. This analysis revealed only one large cluster of significant cross-classification (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Note that while this result demonstrates extended time periods of temporal generalization, this does not rule out the presence of high-level categorical effects (for similar results demonstrating conceptual responses, see our recent work: Bankson, Hebart et al., 2017).</p><p>Discussion:</p><p>“In addition, our study did not distinguish between different stages of object processing (e.g. low-level features or high-level categories), and our temporal generalization analysis of objects did not reveal multiple apparent object processing stages (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). […] By revealing the spatiotemporal dynamics of task and object processing, our results serve as a stepping stone for future investigations addressing these questions.”</p><disp-quote content-type="editor-comment"><p>I also wonder whether the authors found similar patterns of object decoding for all 8 conditions? Based on evidence from the literature, I would expect some conditions (e.g., animals) to show better decoding than others (e.g., plants).</p></disp-quote><p>Following the reviewer’s suggestion, we have now compared object decoding for all 8 object classes, the results of which are shown in <xref ref-type="fig" rid="respfig2">Author response image 2</xref>. Note that object decoding always depends on a comparison to other objects (e.g. accuracies for “cow” reflect the comparison to “butterfly”, “dresser” etc.), which affects the interpretability of such results. While there are slight differences between the different objects, the overall pattern of accuracy time courses is the same across objects, suggesting that task effects were comparable. While the results are interesting, we believe a more detailed analysis to be beyond the scope of the present study. We now mention this as a prospect for future studies in the Discussion section.</p><fig id="respfig2"><object-id pub-id-type="doi">10.7554/eLife.32816.020</object-id><label>Author response image 2.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-32816-resp-fig2-v2"/></fig><disp-quote content-type="editor-comment"><p>Related to the first comment, regarding the model-based MEG-fMRI fusion analysis shown in Figure 5, the authors state: &quot;…EVC, LO and pFS exhibited a mixture of task and category-related information during the Object Stimulus Period, with relative increases in the size of task-related effects when moving up the visual cortical hierarchy&quot;. Wouldn't you expect the same hierarchical increasing pattern for object representations? Instead, looking at the figure, it seems that the object category model explains more variance in EVC than pFS. I wonder whether a visual model (e.g., shape – such as the geometric blur descriptor (Belongie et al., 2002), which captures object shape similarities irrespective of orientation) would do a better work – especially in EVC and LO. Maybe the authors could discuss this possibility in the Discussion section.</p></disp-quote><p>Indeed, the object-specific responses could have increased along the cortical hierarchy. Note, though, that the overall explained variance was relatively low in LO and pFS based on the fMRI ROI RDMs in that region, which translates to smaller <italic>absolute</italic> commonality coefficients in the MEG-fMRI fusion results. In other words, this result need not reflect weaker encoding (see Hebart and Baker, 2017, for interpretability of comparisons between regions), making a comparison of absolute commonality coefficients between regions non-trivial. For that reason, we prefer focusing our interpretation on <italic>relative</italic> comparisons within region. However, we agree with the reviewer that it is possible that object similarities were more strongly driven by low-level than high-level categorical responses. Since we removed any reference to categorical processing, we now discuss this as a prospect for future studies in the Discussion section.</p><p>Discussion:</p><p>“Task may interact with objects at any stage of processing, and while in the present study interactions arose late in time, it is still a matter of debate to what degree late responses reflect high-level categorical processing of objects (Kaiser et al., 2016; Bankson et al., 2017; Proklova et al., 2017). Future studies on task effects during object processing could address this issue by using a larger, controlled set of objects (Bracci and Op de Beeck, 2016) or by explicitly including models of shape (Belongie et al., 2002) or texture (Proklova et al., 2016).”</p></body></sub-article></article>