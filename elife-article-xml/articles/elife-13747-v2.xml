<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">13747</article-id><article-id pub-id-type="doi">10.7554/eLife.13747</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The modulation of savouring by prediction error and its effects on choice</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-42284"><name><surname>Iigaya</surname><given-names>Kiyohito</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4748-8432</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-47967"><name><surname>Story</surname><given-names>Giles W</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-19376"><name><surname>Kurth-Nelson</surname><given-names>Zeb</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-19714"><name><surname>Dolan</surname><given-names>Raymond J</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-11068"><name><surname>Dayan</surname><given-names>Peter</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3476-1839</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Gatsby Computational Neuroscience Unit</institution>, <institution>University College London</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">The Wellcome Trust Centre for Neuroimaging</institution>, <institution>University College London</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Max Planck UCL Centre for Computational Psychiatry and Ageing Research</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Uchida</surname><given-names>Naoshige</given-names></name><role>Reviewing editor</role><aff id="aff4"><institution>Harvard University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>kiigaya@gatsby.ucl.ac.uk</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>21</day><month>04</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e13747</elocation-id><history><date date-type="received"><day>13</day><month>12</month><year>2015</year></date><date date-type="accepted"><day>14</day><month>04</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Iigaya et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Iigaya et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-13747-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.13747.001</object-id><p>When people anticipate uncertain future outcomes, they often prefer to know their fate in advance. Inspired by an idea in behavioral economics that the anticipation of rewards is itself attractive, we hypothesized that this preference of advance information arises because reward prediction errors carried by such information can boost the level of anticipation. We designed new empirical behavioral studies to test this proposal, and confirmed that subjects preferred advance reward information more strongly when they had to wait for rewards for a longer time. We formulated our proposal in a reinforcement-learning model, and we showed that our model could account for a wide range of existing neuronal and behavioral data, without appealing to ambiguous notions such as an explicit value for information. We suggest that such boosted anticipation significantly drives risk-seeking behaviors, most pertinently in gambling.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.001">http://dx.doi.org/10.7554/eLife.13747.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.13747.002</object-id><title>eLife digest</title><p>People, pigeons and monkeys often want to know in advance whether they will receive a reward in the future. This behaviour is irrational when individuals pay for costly information that makes no difference to an eventual outcome. One explanation is that individuals seek information because anticipating reward has hedonic value (it produces a feeling of pleasure). Consistent with this, pigeons are more likely to seek information when they have to wait longer for the potential reward. However, existing models cannot account for why this anticipation of rewards leads to irrational information-seeking.</p><p>In many situations, animals are uncertain about what is going to happen. Providing new information can produce a “prediction error” that indexes a discrepancy between what is expected and what actually happens. Iigaya et al. have now developed a mathematical model of information-seeking in which anticipation is boosted by this prediction error.</p><p>The model accounts for a wide range of previously unexplained data from monkeys and pigeons. It also successfully explains the behaviour of a group of human volunteers from whom Iigaya et al. elicited informational and actual decisions concerning uncertain and delayed rewards. The longer that the participants had to wait for possible rewards, the more avidly they wanted to find out about them. Further research is now needed to investigate the neural underpinnings of anticipation and its boosting by prediction errors.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.002">http://dx.doi.org/10.7554/eLife.13747.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>reward</kwd><kwd>anticipation</kwd><kwd>prediction error</kwd><kwd>decision-making</kwd><kwd>reinforcement-learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd><kwd>Rhesus macaque</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Iigaya</surname><given-names>Kiyohito</given-names></name><name><surname>Dayan</surname><given-names>Peter</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>091593/Z/10/Z</award-id><principal-award-recipient><name><surname>Story</surname><given-names>Giles W</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Zeb</given-names></name><name><surname>Dolan</surname><given-names>Raymond J</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>098362/Z/12/Z</award-id><principal-award-recipient><name><surname>Story</surname><given-names>Giles W</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Zeb</given-names></name><name><surname>Dolan</surname><given-names>Raymond J</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>The Joint Initiative on Computational Psychiatry and Ageing Research between the Max Planck Society and University College London</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Story</surname><given-names>Giles W</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Zeb</given-names></name><name><surname>Dolan</surname><given-names>Raymond J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The anticipation of rewards turns out to have its own hedonic value, on top of that of the reward itself; a wide range of behavioral and neurophysiological data suggest that this anticipation is boosted by prediction errors.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>When people anticipate possible future outcomes, they often prefer their fate to be revealed in advance by a predictive cue, even when this cue does not influence outcome contingency. This is usually called ‘observing’, or ‘information-seeking’, behavior.</p><p>Recently, Bromberg-Martin and Hikosaka (<xref ref-type="bibr" rid="bib9">Bromberg-Martin and Hikosaka, 2009</xref>; <xref ref-type="bibr" rid="bib10">2011</xref>) reported an influential series of studies into the neural basis of the observing behavior of macaque monkeys. They tested subjects’ preferences between three targets that were followed by cues that resolved uncertainty about the volume of an upcoming reward (small or large) to different degrees. Subjects strongly preferred a ‘100% info’ target, which was followed by uncertainty-resolving, definitive, cues, over a ‘50% info’ target, which was followed either by definitive cues or by a totally ambiguous cue; and preferred the latter target over a ‘0% info target’, which was always followed by an entirely ambiguous cue. Neurons in lateral habenula responded differently to the same definitive, reward-predicting, cue depending on the target that had previously been chosen (100% or 50% info). The authors concluded that these neurons index what they called ‘information prediction errors’ along with conventional reward prediction errors (<xref ref-type="bibr" rid="bib58">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="bib42">Matsumoto and Hikosaka, 2007</xref>), and that biological agents ascribe intrinsic value to information.</p><p>A yet more striking finding is that animals appear willing to sacrifice reward for taking advance information. This is known for birds (<xref ref-type="bibr" rid="bib67">Zentall, 2016</xref>; <xref ref-type="bibr" rid="bib47">McDevitt et al., 2016</xref>), monkeys (<xref ref-type="bibr" rid="bib5">Blanchard et al., 2015a</xref>), and humans (<xref ref-type="bibr" rid="bib14">Eliaz and Schotter, 2010</xref>; <xref ref-type="bibr" rid="bib48">Molet et al., 2012</xref>). Extensive studies on birds (mostly pigeons) showed that animals prefer a less rewarding target that is immediately followed by uncertainty-resolving cues, over a more rewarding target without such cues (e.g. 20% chance over 50% chance of reward (<xref ref-type="bibr" rid="bib62">Stagner and Zentall, 2010</xref>; <xref ref-type="bibr" rid="bib66">Vasconcelos et al., 2015</xref>), 50% chance over 75% chance of reward (<xref ref-type="bibr" rid="bib22">Gipson et al., 2009</xref>), 50% chance over certain reward (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>; <xref ref-type="bibr" rid="bib46">McDevitt et al., 1997</xref>; <xref ref-type="bibr" rid="bib52">Pisklak et al., 2015</xref>). Crucially however, the pigeons only show this preference when the delay, <inline-formula><mml:math id="inf1"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula>, between the choice and the reward is sufficiently long. Another salient experimental observation in <xref ref-type="bibr" rid="bib61">Spetch et al. (1990)</xref> is that after choosing a less rewarding, 50% chance, target, some of the pigeons were also seen to peck enthusiastically during the delay following the cue informing them that reward would arrive. By contrast, they were comparatively quiescent during the delay after choosing the certain reward target followed by a similar cue (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>).</p><p>It remains a challenge to account for these data on the preference of advance information. The delay-dependent preference reward predictive cues shown by <xref ref-type="bibr" rid="bib61">Spetch et al. (1990)</xref> cannot depend on conventional Shannon information, since this is normally independent of delay. Furthermore, targets associated with less Shannon information can be more attractive (<xref ref-type="bibr" rid="bib54">Roper and Zentall, 1999</xref>; <xref ref-type="bibr" rid="bib67">Zentall, 2016</xref>). Equally, evidence from the activity of lateral habenula neurons (<xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka, 2011</xref>) provides no support for the recent suggestion that disengagement caused by uninformative cues could cause the seeking of informative cues (<xref ref-type="bibr" rid="bib1">Beierholm and Dayan, 2010</xref>).</p><p>Here, we offer a new explanation of observing and information-seeking behavior that accounts for the effects of delays reported in the pigeon experiments and the effects of changing the probability of reward. We follow an established notion called the <italic>utility of anticipation</italic> (<xref ref-type="bibr" rid="bib39">Loewenstein, 1987</xref>; <xref ref-type="bibr" rid="bib2">Berns et al., 2006</xref>; <xref ref-type="bibr" rid="bib63">Story et al., 2013</xref>). These investigators have shown that subjects consider the delay to a future reward as itself being appetitive (think, for instance, of yourself waiting for an upcoming vacation trip), associated with a positive utility of anticipation. This is often referred to as savouring (the anticipation of negative outcomes is called dread), and coexists with a more conventional effect of delay, namely temporal discounting (<xref ref-type="bibr" rid="bib39">Loewenstein, 1987</xref>; <xref ref-type="bibr" rid="bib40">Loewenstein and Prelec, 1993</xref>; <xref ref-type="bibr" rid="bib59">Schweighofer et al., 2006</xref>; <xref ref-type="bibr" rid="bib32">Kable et al., 2010</xref>).</p><p>In this framework, we hypothesize that the level of anticipation can be boosted by the (temporal difference) prediction errors caused by predictive cues that resolve reward uncertainty. Pigeons’ vigorous pecking following those cues is a sign of the boost. That is, the definitive reward cue following a partial target evokes a positive (temporal difference) prediction error – the difference between the expected (partial chance) and actual outcome (reward for sure) is positive (<xref ref-type="bibr" rid="bib58">Schultz et al., 1997</xref>). We suggest that the impact of this is to increase savouring. By contrast, the certain target elicits no such prediction error and so, within this account, will not increase savouring. Put simply, our model posits that unexpected news of upcoming pleasant (or unpleasant) outcomes boosts the savouring (or dread) associated with such outcomes.</p><p>The hypothesis of boosting yields a parsimonious explanation for observing and information-seeking, consistent with all existing neural and behavioral data, including a range of seemingly paradoxical findings (<xref ref-type="bibr" rid="bib22">Gipson et al., 2009</xref>; <xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>; <xref ref-type="bibr" rid="bib62">Stagner and Zentall, 2010</xref>; <xref ref-type="bibr" rid="bib9">Bromberg-Martin and Hikosaka, 2009</xref>; <xref ref-type="bibr" rid="bib10">2011</xref>; <xref ref-type="bibr" rid="bib67">Zentall, 2016</xref>). Here we also conducted human behavioral studies to test the delay <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula> dependence of observing and information seeking, which has so far only been subject to limited tests in animal studies (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The RPE-anticipation model</title><p>Our model relies on the established economic theory called the utility of anticipation (<xref ref-type="bibr" rid="bib39">Loewenstein, 1987</xref>), which proposes that the anticipation of future reward has a positive subjective value which is added to the actual value of the reward. Our formulation follows prior characterizations (<xref ref-type="bibr" rid="bib39">Loewenstein, 1987</xref>; <xref ref-type="bibr" rid="bib2">Berns et al., 2006</xref>; <xref ref-type="bibr" rid="bib63">Story et al., 2013</xref>). Consider the case in which a subject takes an action and receives a pre-reward cue at <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and then a reward <inline-formula><mml:math id="inf4"><mml:mi>R</mml:mi></mml:math></inline-formula> at <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The anticipation of the reward is worth <inline-formula><mml:math id="inf6"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM2">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM1"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> at time <inline-formula><mml:math id="inf7"><mml:mi>t</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1A-(ii)</xref>), where <inline-formula><mml:math id="inf8"><mml:mi>ν</mml:mi></mml:math></inline-formula> governs the rate of growth of this factor. A small <inline-formula><mml:math id="inf9"><mml:mi>ν</mml:mi></mml:math></inline-formula> means that the anticipation grows gradually in time, while a large <inline-formula><mml:math id="inf10"><mml:mi>ν</mml:mi></mml:math></inline-formula> means that the anticipation increases steeply near the delivery of rewards.<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.13747.003</object-id><label>Figure 1.</label><caption><title>The model.</title><p>(<bold>A</bold>) The value of the cue is determined by (ii) the anticipation of upcoming reward in addition to (i) the reward itself. The two are (iii) linearly combined and discounted; with the weight of anticipation being (iv) boosted by the RPE associated with the predicting cue. (<bold>B</bold>) The contribution of different time points to the value of predicting cue. The horizontal axis shows the time of reward delivery. The vertical axis shows the contribution of different time points to the value of the predicting cue. (<bold>C</bold>) The total value of the predicting cue, which integrates the contribution along the vertical axis of panel (<bold>B</bold>), shows an inverted U-shape.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.003">http://dx.doi.org/10.7554/eLife.13747.003</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig1-v2"/></fig></p><p>The value of the pre-reward cue is determined by what follows the cue, which, under conventional temporal difference (TD) learning, would have been the reward itself (<xref ref-type="fig" rid="fig1">Figure 1A-(i)</xref>), discounted in time with a rate <inline-formula><mml:math id="inf11"><mml:mi>γ</mml:mi></mml:math></inline-formula>. Here, however, in addition to the reward itself, we have the anticipation of the reward that takes place continuously over time. Thus the total value of the predictive cue, <inline-formula><mml:math id="inf12"><mml:mi>Q</mml:mi></mml:math></inline-formula> is the sum of the discounted reward (blue bar in <xref ref-type="fig" rid="fig1">Figure 1A-(iii)</xref>), and the temporally discounted anticipation, where the latter is integrated over time from the presentation of the cue up to reward delivery (red area in <xref ref-type="fig" rid="fig1">Figure 1A-(iii)</xref>).</p><p>Note that the integration of anticipation suggests that the total amount of anticipation contributing to the value of the cue can increase as <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula> is increased. This can be seen in <xref ref-type="fig" rid="fig1">Figure 1B</xref>, in which the color code indicates the contribution of the temporally discounted anticipation at time <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>Delay</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to a predictive cue. The horizontal axis indicates different delay conditions <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula>, and the vertical axis shows the different time points <inline-formula><mml:math id="inf16"><mml:mi>t</mml:mi></mml:math></inline-formula> between the cue (<inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) and the time of reward delivery (<inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>). <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows the total values for different delay length <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula>, which are the integrals of contributions over the vertical axis in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. As seen, the total value usually takes the maximum value at a finite <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula>, which is larger than the value <inline-formula><mml:math id="inf21"><mml:mi>R</mml:mi></mml:math></inline-formula> of the reward itself (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). While the actual peak is determined by the competition between the anticipation <inline-formula><mml:math id="inf22"><mml:mi>ν</mml:mi></mml:math></inline-formula> and discounting <inline-formula><mml:math id="inf23"><mml:mi>γ</mml:mi></mml:math></inline-formula>, this inverted U-shape was confirmed previously for the case of savoring, using hypothetical questionnaire studies (<xref ref-type="bibr" rid="bib39">Loewenstein, 1987</xref>). (This inverted U-shape holds in general in the model, unless the relative weight of anticipation <inline-formula><mml:math id="inf24"><mml:mi>η</mml:mi></mml:math></inline-formula> is zero, or growth and discounting are too steep relative to each other <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>≪</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>≫</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula>. )</p><p>Previously, the total value of cue <inline-formula><mml:math id="inf27"><mml:mi>Q</mml:mi></mml:math></inline-formula> has been expressed as a sum of the value of anticipation and the reward itself:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mtext> </mml:mtext><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Anticipation]</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Reward]</mml:mtext></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>with the relative weight of anticipation <inline-formula><mml:math id="inf28"><mml:mi>η</mml:mi></mml:math></inline-formula> being treated as a constant. Here we hypothesized that reward prediction errors (RPE) <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in response to the predictive cue (<xref ref-type="bibr" rid="bib58">Schultz et al., 1997</xref>) can boost anticipation (<xref ref-type="fig" rid="fig1">Figure 1A-(iv)</xref>). Our proposal was inspired by findings of a dramatically enhanced excitement that follows predictive cues that resolve reward uncertainty appetitively (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>), which will generate positive RPEs. A simple form of boosting arises from the relationship<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> specifies base anticipation, and <inline-formula><mml:math id="inf31"><mml:mi>c</mml:mi></mml:math></inline-formula> determines the gain. That anticipation is boosted by the <italic>absolute value</italic> of RPE turns out to be important in applying our model to comparatively unpleasant outcomes, as confirmed in our own experiment. Note that anticipation can only be boosted by the RPE that precedes it – in this case arising from the predictive cue. Any RPE associated with the delivery of reward would have no anticipatory signal within the trial that it could boost. We ignore any subsidiary anticipation that could cross trial boundaries.</p></sec><sec id="s2-2"><title>Physiological and behavioral aspects of observing and information-seeking</title><p><xref ref-type="fig" rid="fig2">Figure 2A,B,C</xref> illustrates the design and results of the experiment mentioned above in which macaque monkeys exhibit observing, or information-seeking, behavior (<xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka, 2011</xref>). Briefly, there were three targets: 1) a 100% info target that was always followed by a cue whose shapes indicated the upcoming reward size (big or small); 2) a 0% info target that was always followed by a random cue whose shapes conveyed no information about reward size; and 3) a 50% info target that was followed half the time by informative cues and half the time by random cues (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). <xref ref-type="fig" rid="fig2">Figure 2B</xref> shows the strong preference the subjects exhibited for the 100% over the 50%, and the 50% over the 0% info targets. <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows the difference in activity of lateral habenula neurons at the time of the predictive cues depending on the preceding choice of target.<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.13747.004</object-id><label>Figure 2.</label><caption><title>Our model accounts for the behavioral and neural findings in <xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka (2011)</xref>.</title><p>(<bold>A</bold>) The task in (<xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka, 2011</xref>). On each trial, monkeys viewed a fixation point, used a saccadic eye movement to choose a colored visual target, viewed a visual cue, and received a big or small water reward. The three potential targets led to informative cues with 100%, 50% or 0% probability. (<xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka, 2011</xref>; reproduced with permission) (<bold>B</bold>) Monkeys strongly preferred to choose the target that led to a higher probability of viewing informative cues (<xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka, 2011</xref>; reproduced with permission). (<bold>C</bold>) The activity of lateral habenula neurons at the predicting cues following the 100% target (predictable) were different from the case where the cues followed the 50% target (unpredicted) (<xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka, 2011</xref>; reproduced with permission). The mean difference in firing rate between unpredicted and predictable cues are shown in case of small-reward and big-reward (the error bars indicate SEM.). (<bold>D</bold>) Our model predicts the preference for more informative targets. (<bold>E</bold>,<bold>F</bold>) Our model’s RPE, which includes the anticipation of rewards, can account for the neural activity. Note the activity of the lateral habenula neurons is negatively correlated with RPE.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.004">http://dx.doi.org/10.7554/eLife.13747.004</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.13747.005</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Our model can capture the preference of info targets with a wide range of parameters.</title><p>The color map (bottom) shows the squared errors of our model’s prediction with respect to the choice preference of one of the monkeys (Monkey Z) reported in <xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka (2011)</xref>, while the top two panels show model’s predictions the corresponding parameters. The parameters are fixed, not optimized, as <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn><mml:mo>,</mml:mo><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:msup><mml:mrow><mml:mtext>sec</mml:mtext></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>Delay</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.25</mml:mn><mml:mrow><mml:mtext>sec</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:msup><mml:mrow><mml:mtext>sec</mml:mtext></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.08</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.005">http://dx.doi.org/10.7554/eLife.13747.005</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig2-figsupp1-v2"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.13747.006</object-id><label>Figure 2—figure supplement 2.</label><caption><title>RPE-boosting of anticipation is necessary to capture the choice preference of monkeys reported in <xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka (2011)</xref>.</title><p>The baseline anticipation is the same for three targets with different levels of advance reward information. Hence the model exhibits no preference.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.006">http://dx.doi.org/10.7554/eLife.13747.006</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig2-figsupp2-v2"/></fig></fig-group></p><p><xref ref-type="fig" rid="fig2">Figure 2D</xref> shows that our model captured subjects’ preferences for each of the targets; and <xref ref-type="fig" rid="fig2">Figure 2E,F</xref> shows that it also accounted for the different sizes of RPEs to the same reward predictive cues when they followed different info targets (noting that the responses of lateral habenula neurons are negatively correlated with the size of reward prediction errors; <xref ref-type="bibr" rid="bib42">Matsumoto and Hikosaka, (2007)</xref>; <xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka, (2011)</xref>). The difference in RPE sizes arose from the different values of the targets, which itself arose from the different magnitudes of anticipation associated with the cues. We found that these results held across a wide range of parameter settings (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Note that RPE-boosting is necessary to capture the data, as the simple baseline anticipation model predicted the same level of anticipation following each target, leading to no preference between the targets (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). We also note that our model further predicts that the magnitude of RPE to a reward predictive cue can be larger than the magnitude of RPE to the reward itself following random cues. This is because the predictive cues include the value of anticipation.</p><p>Our model also accounted for puzzling irrational gambling behaviors that have been reported in many experiments (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>; <xref ref-type="bibr" rid="bib22">Gipson et al., 2009</xref>; <xref ref-type="bibr" rid="bib14">Eliaz and Schotter, 2010</xref>; <xref ref-type="bibr" rid="bib66">Vasconcelos et al., 2015</xref>). These include a perplexingly greater preference for a target offering 50%chance of reward over either a target offering 75% chance of reward, or a certain target offering 100% chance of reward, at least when the delays between the predicting cues and rewards are long (<xref ref-type="fig" rid="fig3">Figure 3A–C</xref>) (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>; <xref ref-type="bibr" rid="bib22">Gipson et al., 2009</xref>).<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.13747.007</object-id><label>Figure 3.</label><caption><title>Our model accounts for a wide range of seemingly paradoxical findings of observing and information-seeking.</title><p>(<bold>A</bold>) Abstraction of the pigeon tasks reported in <xref ref-type="bibr" rid="bib61">Spetch et al. (1990)</xref>; <xref ref-type="bibr" rid="bib22">Gipson et al. 2009)</xref>. On each trial, subjects chose either of two colored targets (Red or Blue in this example). Given Red, cue <inline-formula><mml:math id="inf33"><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> or <inline-formula><mml:math id="inf34"><mml:msup><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:math></inline-formula> was presented, each with probability 0.5; was followed by a reward after time <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula>, while <inline-formula><mml:math id="inf36"><mml:msup><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:math></inline-formula> was not followed by reward. Given Blue, a cue <inline-formula><mml:math id="inf37"><mml:msup><mml:mi>S</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> was presented, and reward possibly followed after the fixed time delay <inline-formula><mml:math id="inf38"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula> with probability p<sub>B</sub>, or otherwise nothing. In <xref ref-type="bibr" rid="bib61">Spetch et al. (1990)</xref>, p<sub>B</sub>=1, and in <xref ref-type="bibr" rid="bib22">Gipson et al. (2009)</xref> p<sub>B</sub>=0.75. (<bold>B</bold>) Results with p<sub>B</sub>=1 in <xref ref-type="bibr" rid="bib61">Spetch et al. (1990)</xref>. Animals showed an increased preference for the less rewarding target (Red) as delay time <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula> was increased. The results of four animals are shown. (Adapted from <xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>) (<bold>C</bold>) Results with p<sub>B</sub>=0.75 in <xref ref-type="bibr" rid="bib22">Gipson et al. (2009)</xref>. Most animals preferred the informative but less rewarding target (Red). (Adapted from <xref ref-type="bibr" rid="bib22">Gipson et al., 2009</xref>) (<bold>D</bold>) Our model predicted changes in the values of cues when p<sub>B</sub>=1, accounting for (<bold>B</bold>). Thanks to the contribution of anticipation of rewards, both values first increase as the delay increased. Even though choosing Red provides fewer rewards, the prediction error boosts anticipation and hence the value of Red (solid red line), which eventually exceeds the value of Blue (solid blue line), given a suitably long delay. Without boosting, this does not happen (dotted red line). At the delay gets longer still, the values decay and the preference is reversed due to discounting. This second preference reversal is our model’s novel prediction. Note that x-axis is unit-less and scaled by <inline-formula><mml:math id="inf40"><mml:mi>γ</mml:mi></mml:math></inline-formula>. (<bold>E</bold>) The changes in the values of Red and Blue targets across different probability conditions p<sub>B</sub>. Our model predicted the reversal of preference across different probability conditions of p<sub>B</sub>. The dotted red line represents when the target values were equal. We set parameters as <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.007">http://dx.doi.org/10.7554/eLife.13747.007</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.13747.008</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>The changes in the values of Red and Blue targets across different probability conditions <inline-formula><mml:math id="inf42"><mml:msub><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:math></inline-formula> when we assume a different function form for <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo>:</mml:mo><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (the task described in <xref ref-type="fig" rid="fig3">Figure 3</xref>). The model’s behavior does not change qualitatively compared to <xref ref-type="fig" rid="fig3">Figure 3D;E</xref>. We set parameters as <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.008">http://dx.doi.org/10.7554/eLife.13747.008</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig3-figsupp1-v2"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.13747.009</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Related to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>(<bold>A</bold>) The task reported in <xref ref-type="bibr" rid="bib62">Stagner and Zentall (2010)</xref>; <xref ref-type="bibr" rid="bib66">Vasconcelos et al. (2015)</xref>; <xref ref-type="bibr" rid="bib67">Zentall (2016)</xref>. Subjects (birds) had to choose either the 100% info target (shown as Red here) associated with 20% chance of reward, or the 0% info target (shown as Blue here) associated with 50% chance of reward. Subjects preferred less rewarding Red target. (<bold>B</bold>) Our model accounts for the data. Because of the boosted anticipation of rewards, the model predicted a preference of less rewarding, but informative, Red target at finite delay periods <inline-formula><mml:math id="inf45"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula>. Without boosting, the model predicts a preference of Blue over Red at any delay conditions. Model parameters were taken as the same as <xref ref-type="fig" rid="fig3">Figure 3</xref>: <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.009">http://dx.doi.org/10.7554/eLife.13747.009</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig3-figsupp2-v2"/></fig></fig-group></p><p><xref ref-type="fig" rid="fig3">Figure 3D</xref> shows that our model correctly predicted that the value of the 50% reward target (red line) would be smaller than that of the 100% reward target (blue line) when the delay <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>T</mml:mi><mml:mtext>delay</mml:mtext></mml:msub></mml:math></inline-formula> between the cues until rewards, and hence the contribution of anticipation, was small. Thus, the modelled subjects would prefer the reliable target (indicated by the blue background on left). However, when the delay <inline-formula><mml:math id="inf48"><mml:msub><mml:mi>T</mml:mi><mml:mtext>delay</mml:mtext></mml:msub></mml:math></inline-formula> was increased, the contribution to the value coming from anticipation was boosted for the 50% reward target, but not for the certain target, because of the RPE associated with the former. This resulted in a preference for the lower probability target (indicated by the red background in the middle), which is consistent with the experimental finding shown in <xref ref-type="fig" rid="fig3">Figure 3A,B</xref> (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>). Note that RPE-boosting is again necessary to capture the reported suboptimal behaviors, as the expected non-boosted value of the 50% reward target would be always smaller than the value of sure reward target (see the dotted red line in <xref ref-type="fig" rid="fig3">Figure 3D</xref>). This is because in both cases, the rewards generate conventional anticipation.</p><p>Finally, as the time delay increased further, both values decayed, making the value of the certain target again greater than that of the 50% reward target (indicated by the blue background on right). This is because the discounting dominated the valuation, leaving the impact of anticipation relatively small. This second reversal is predicted by our model. However, it has not been observed experimentally, other than in findings based on the use of hypothetical questionnaires in <xref ref-type="bibr" rid="bib39">Loewenstein (1987)</xref>. (We note that one of the four animals in <xref ref-type="bibr" rid="bib61">Spetch et al. (1990)</xref> did appear to show such a non monotonic preference; however, individual differences also appeared to be very large.)</p><p>The model can be used to interpolate between the experiments in <xref ref-type="bibr" rid="bib61">Spetch et al. (1990)</xref> (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) and <xref ref-type="bibr" rid="bib22">Gipson et al. (2009)</xref> (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), showing a full range of possible tradeoffs between the probability of reward and informative cueing. The phase diagram <xref ref-type="fig" rid="fig3">Figure 3E</xref> shows this trade-off for the task described in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, as either the probability (p<sub>B</sub>) of reward associated with the 0% info (blue) target, or the delay <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>T</mml:mi><mml:mtext>delay</mml:mtext></mml:msub></mml:math></inline-formula> change, while the reward probability of 100% info target (red) is fixed at 50%. The experiment in <xref ref-type="bibr" rid="bib61">Spetch et al. (1990)</xref> (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) corresponds to p<sub>B</sub>=1, while the experiment in <xref ref-type="bibr" rid="bib22">Gipson et al. (2009)</xref> (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), which used a 75% chance target, corresponds to p<sub>B</sub>=0.75. As we show in the case of p<sub>B</sub>≥0.5, the chance of reward is higher for the 0% info (blue) target than the 100% info (red), 50% reward target in this diagram (except for the case p<sub>B</sub>=0.5, where both targets offer 50% rewards). Hence the conventional reinforcement learning model would predict a preference for the 0% info (blue) target everywhere.</p><p>As seen in <xref ref-type="fig" rid="fig3">Figure 3E</xref>, the model predicted a similar reversal of preference as in <xref ref-type="fig" rid="fig3">Figure 3D</xref> across different probability conditions of p<sub>B</sub>&gt;0.5 We also confirmed that this prediction depends on neither the details of functional form by which RPE influenced anticipation, nor specific parameter values in the model (See Materials and methods and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> ).</p><p>In these calculations, we set the value of no outcome to zero, implying a lack of dread in the no outcome condition. The reason for this was that there was no effect on the preference of pigeons when the delay between the cues signalling no-reward and the no-reward was changed (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>), while the impact of dread should change over the delay. Moreover changing the delay between choice and cues that signalled no-reward had no impact on preference (<xref ref-type="bibr" rid="bib46">McDevitt et al., 1997</xref>). Note, however, that our results would still hold in case of adding a value to the no outcome. In fact, as detailed in the next section, we found in our human behavioral task that participants assigned the same magnitudes of values to reward and no-reward outcomes, and yet our model still accounted for the preference of advance information.</p><p>We note the generality of our model. It can account for other various experimental results in different conditions. This includes experiments showing the relative preference for a 100% info target with 25% chance of reward over a 50% info target with a 50% chance of reward (<xref ref-type="bibr" rid="bib62">Stagner and Zentall, 2010</xref>; <xref ref-type="bibr" rid="bib66">Vasconcelos et al., 2015</xref>), illustrated in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>.</p></sec><sec id="s2-3"><title>Testing the preference for advance information about upcoming rewards across delays in human subjects</title><p>A consequence of our model is that the values of predictive cues will be affected by how long subjects subsequently have to wait for the reward – the dynamic changes in values across delay conditions shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, <xref ref-type="fig" rid="fig3">Figure 3D,E</xref>. This has so far only been subject to rather limited tests in animal studies (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>). We therefore conducted a new human behavioral experiment to test these predictions.</p><p>In Experiment-1, 14 heterosexual male human volunteers chose between a 0% info target, which was followed by no cue (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), and a 100% info target, which was immediately followed by cues that predicted the presence or absence of reward. The rewards were previously validated lascivious images of female models (<xref ref-type="bibr" rid="bib12">Crockett et al., 2013</xref>). Using this type of primary rewards was crucial for our task design, as was also the case in <xref ref-type="bibr" rid="bib12">Crockett et al. (2013)</xref>. This is because other types of rewards, such as monetary rewards, cannot be utilized by participants immediately on each trial as they become available.<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.13747.010</object-id><label>Figure 4.</label><caption><title>Human decision-making Experiment-1.</title><p>(<bold>A</bold>) On each trial, subjects chose either of two colored targets (Red or Blue in this example). Given Red, cue <inline-formula><mml:math id="inf50"><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> (oval) or <inline-formula><mml:math id="inf51"><mml:msup><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:math></inline-formula> (triangle) was presented, each with probability <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>; was followed by a reward (an erotic picture) after time <inline-formula><mml:math id="inf53"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula>, while <inline-formula><mml:math id="inf54"><mml:msup><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:math></inline-formula> was not followed by reward. Given Blue, either a reward or nothing followed after the fixed time delay <inline-formula><mml:math id="inf55"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf56"><mml:mn>0.5</mml:mn></mml:math></inline-formula> each. (<bold>B</bold>) Results. Human participants (n=14) showed a significant modulation of choice over delay conditions [one-way ANOVA, F(3,52)=3.09, p=0.035]. They showed a significant preference for the 100% info target (Red) for the case of long delays [20 s: <inline-formula><mml:math id="inf57"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn id="XM267">13</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>3.14</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0078</mml:mn></mml:mrow></mml:math></inline-formula>, 40 s: <inline-formula><mml:math id="inf59"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn id="XM268">13</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>2.60</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.022</mml:mn></mml:mrow></mml:math></inline-formula>]. The mean +/- SEM indicated by the solid line. The dotted line shows simulated data using the fitted parameters. (<bold>C</bold>) Mean Q-values of targets and predicting cues estimated by the model. The value of informative cue is the mean of the reward predictive cue (oval), which has an inverted U-shape due to positive anticipation, and the no-reward predictive cue (triangle), which has the opposite U-shape due to negative anticipation. The positive anticipation peaks at around 25 s, which is consistent with animal studies shown in <xref ref-type="fig" rid="fig3">Figure 3(B,C</xref>). See <xref ref-type="table" rid="tbl2">Table 2</xref> for the estimated model parameters. (<bold>D</bold>) Model comparison based on integrated Bayesian Information Criterion (iBIC) scores. The lower the score, the more favorable the model. Our model of RPE-boosted anticipation with a negative value for no-outcome enjoys significantly better score than the one without a negative value, the one without RPE-boosting, the one without temporal discounting, or other conventional Q-learning models with or without discounting.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.010">http://dx.doi.org/10.7554/eLife.13747.010</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.13747.011</object-id><label>Figure 4—figure supplement 1.</label><caption><title>(<bold>A</bold>) Control experiment, where the first block and the last (5th) block of the experiment had the same delay duration of <inline-formula><mml:math id="inf61"><mml:mn>2.5</mml:mn></mml:math></inline-formula> s.</title><p>Subjects showed no difference [<inline-formula><mml:math id="inf62"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn id="XM336">10</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.04</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.32</mml:mn></mml:mrow></mml:math></inline-formula>] in the preference before and after experiencing the other delay conditions. (<bold>B</bold>) The large change in the delay duration affects on choice behavior. Y-axis shows the difference in choice percentage between the shortest (2.5 s) and the longest (40s) delay conditions. In our main experiment, the delay duration was gradually increased (Left), while in the control experiment, the delay was abruptly increased. The difference between the two procesures was significant [2 sample <inline-formula><mml:math id="inf64"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn id="XM337">23</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>2.15</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.042</mml:mn></mml:mrow></mml:math></inline-formula>]. Subjects reported particularly unpleasant feeling for the long delay condition in the control experiment.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.011">http://dx.doi.org/10.7554/eLife.13747.011</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig4-figsupp1-v2"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.13747.012</object-id><label>Figure 4—figure supplement 2.</label><caption><title>The generated choice by the model without the negative value assigned to the no-reward outcome.</title><p>The model fails to capture the short delay period (7.5 s). This corresponds to the time point at which the the effect of negative anticipation was the largest, according to the model with <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.012">http://dx.doi.org/10.7554/eLife.13747.012</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig4-figsupp2-v2"/></fig></fig-group></p><p>Subjects experienced blocks of trials with fixed delays (2.5 s, 7.5 s, 20 s and 40 s), where the blocks were indicated by target colors. We set the chance of reward to p<sub>B</sub>=0.5, consistent with the macaque experiments (<xref ref-type="bibr" rid="bib9">Bromberg-Martin and Hikosaka, 2009</xref>; <xref ref-type="bibr" rid="bib10">2011</xref>). Subjects were not told the exact reward probabilities, merely that rewards would be ‘random’.</p><p>We confirmed our model’s central prediction. Subjects showed increased preference for informative cues as the delay increased (the solid line in <xref ref-type="fig" rid="fig4">Figure 4B</xref> indicates group mean and SEM). Subjects were on average indifferent in the case of short delays (2.5 s and 7.5 s). However, they strongly preferred to choose the informative target in the case of longer delays (20 s, 40 s).</p><p>We fitted the choices to our reinforcement-learning (RL) model’s trial-by-trial predictions, including the effects of learning from RPE-boosted anticipation. We used a form of hierarchical Bayesian analysis to estimate group level parameters (<xref ref-type="bibr" rid="bib30">Huys et al., 2011</xref>) (see Materials and methods for more details). We found that preferences generated from the estimated parameters were consistent with subjects’ choices, as indicated by the black dotted line with the predicted standard error in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. Our model predicted striking U-shape changes in the values of the 100% info target and of the 0% info target with respect to the changes in delay length (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). The model enjoyed a substantial iBIC score advantage over other possible well-studied reinforcement learning models, or our anticipation RL model without RPE-boosting (<xref ref-type="fig" rid="fig4">Figure 4D</xref> and <xref ref-type="table" rid="tbl1">Table 1</xref>).<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.13747.013</object-id><label>Table 1.</label><caption><p>iBIC scores. Related to <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.013">http://dx.doi.org/10.7554/eLife.13747.013</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>N of parameters</th><th>Parameters</th><th>iBIC</th></tr></thead><tbody><tr><td>Q-learning (with no discounting)</td><td>3</td><td><inline-formula><mml:math id="inf67"><mml:mi>α</mml:mi></mml:math></inline-formula>,<inline-formula><mml:math id="inf68"><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>,<inline-formula><mml:math id="inf69"><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula></td><td>2598</td></tr><tr><td>Q-learning (with discounting)</td><td>5</td><td><inline-formula><mml:math id="inf70"><mml:mi>α</mml:mi></mml:math></inline-formula>,<inline-formula><mml:math id="inf71"><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>,<inline-formula><mml:math id="inf72"><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula>,<inline-formula><mml:math id="inf73"><mml:msub><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula>,<inline-formula><mml:math id="inf74"><mml:msub><mml:mi>γ</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula></td><td>2643</td></tr><tr><td>Anticipation RL without RPE-boosting</td><td>7</td><td><inline-formula><mml:math id="inf75"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf76"><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf77"><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf78"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mspace width="veryverythickmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM279"><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>ν</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf80"><mml:msub><mml:mi>ν</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf81"><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula></td><td>2659</td></tr><tr><td>Boosted anticipation RL without <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td>4</td><td><inline-formula><mml:math id="inf83"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf84"><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>,<inline-formula><mml:math id="inf85"><mml:msub><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula>,<inline-formula><mml:math id="inf86"><mml:msub><mml:mi>ν</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula></td><td>2616</td></tr><tr><td>Boosted anticipation RL with no discounting</td><td>5</td><td><inline-formula><mml:math id="inf87"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf88"><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf89"><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>ν</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf91"><mml:msub><mml:mi>ν</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula></td><td>2595</td></tr><tr><td>Boosted anticipation RL</td><td>6</td><td><inline-formula><mml:math id="inf92"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf93"><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf94"><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf95"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mspace width="veryverythickmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM280"><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>ν</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf97"><mml:msub><mml:mi>ν</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula></td><td>2583</td></tr></tbody></table></table-wrap></p><p>To investigate possible adaptation to delay, we also ran a control experiment on an additional 11 subjects for whom we changed the order of the delays, and also repeated the same 2.5 s delay in the first and last (fifth) blocks. Preferences did not differ (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>) from beginning to end, suggesting stability. However, there was a moderately significant evidence of adaptation, with the effect of the 40 s delay being much greater following the extensive experience of 2.5 s delay than the 20 s delay (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>).</p><p>To investigate the robustness of the delay dependent preference of advance information further, we conducted an additional experiment (Experiment-2) on a newly recruited population of 31 participants (<xref ref-type="fig" rid="fig5">Figure 5</xref>). At the beginning of each trial, explicit cues provided participants with full information about the current delay condition (either 1 s, 5 s, 10 s, 20 s or 40 s) and the (constant, 50%) reward probability (<xref ref-type="fig" rid="fig5">Figure 5A,B</xref>). The basic structure of the task was similar to Experiment-1 (<xref ref-type="fig" rid="fig5">Figure 5C</xref>); participants had to choose either 100% info target or 0% info target. Crucially, though, the delay condition was randomized across trials. As seen in <xref ref-type="fig" rid="fig5">Figure 5D</xref> participants showed significant preference of the 100% info target when delay was long, replicating the results of Experiment-1. We also found no effect of the delay condition of a previous trial, providing further support for the absence of a block order confound in Experiment 1. (We computed each participant’s preference of 100% info target at a particular delay condition <inline-formula><mml:math id="inf98"><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, conditioned on a previous delay condition <inline-formula><mml:math id="inf99"><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. We tested the effect of previous delay conditions <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> over participants via one-way ANOVA for (1) the preference at <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>=1 s [<inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>111</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2.36</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.06</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>]; (2) the preference at <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>=5 s [<inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>101</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.85</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.50</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>]; (3) the preference at <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>=10 s [<inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>99</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1.45</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.22</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>]; (4) the preference at <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>=20 s [<inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>87</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1.48</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.21</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>]; (5) the preference at <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>=40 s [<inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>98</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.56</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>]). Also, Experiment-2 was designed to have an equal number of trials per delay condition, while Experiment-1 was designed to equalize the amount of time that participants spent in each condition (see Task Procedures in Materials and methods). The fact that we obtained the same results in both experiments illustrates the robustness of our findings.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.13747.014</object-id><label>Figure 5.</label><caption><title>Human decision-making Experiment-2.</title><p>(<bold>A</bold>) A screen-shot from the beginning of each trial. The meaning of targets ('Find out now' or 'Keep it secret'), the duration of <inline-formula><mml:math id="inf111"><mml:msub><mml:mi>T</mml:mi><mml:mtext>delay</mml:mtext></mml:msub></mml:math></inline-formula> (the number of hourglass), and the chance of rewards (the hemisphere <inline-formula><mml:math id="inf112"><mml:mrow><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>) were indicated explicitly. (<bold>B</bold>) The number of hourglasses indicated the duration of <inline-formula><mml:math id="inf113"><mml:msub><mml:mi>T</mml:mi><mml:mtext>delay</mml:mtext></mml:msub></mml:math></inline-formula> until reward. One hourglass indicated 5 s of <inline-formula><mml:math id="inf114"><mml:msub><mml:mi>T</mml:mi><mml:mtext>delay</mml:mtext></mml:msub></mml:math></inline-formula>. When <inline-formula><mml:math id="inf115"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mtext>delay</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> s, a fraction of an hourglass was shown. This was instructed before the experiment began. The delay condition <inline-formula><mml:math id="inf116"><mml:msub><mml:mi>T</mml:mi><mml:mtext>delay</mml:mtext></mml:msub></mml:math></inline-formula> was changed randomly across trials. (<bold>C</bold>) The task structure. The task structure was similar to Experiment-1, except that the 0% info target (Blue) was followed by a no-info cue, and an image symbolizing the lack of reward was presented when no reward outcome was delivered. (<bold>D</bold>) Results. Human participants (n=31) showed a significant modulation of choice over delay conditions [one-way ANOVA, F(4,150)=3.72, p=0.0065]. The choice fraction was not different from 0.5 at short delays [1 s: <inline-formula><mml:math id="inf117"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn id="XM274">30</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.42</mml:mn></mml:mrow></mml:math></inline-formula> 5 s: <inline-formula><mml:math id="inf119"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn id="XM275">30</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.70</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.49</mml:mn></mml:mrow></mml:math></inline-formula>, 10 s: <inline-formula><mml:math id="inf121"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn id="XM276">30</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.26</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.80</mml:mn></mml:mrow></mml:math></inline-formula>] but it was significantly different from 0.5 at long delays [20 s: <inline-formula><mml:math id="inf123"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn id="XM277">30</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>2.86</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf124"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0077</mml:mn></mml:mrow></mml:math></inline-formula>, 40 s: <inline-formula><mml:math id="inf125"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn id="XM278">30</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>3.17</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0035</mml:mn></mml:mrow></mml:math></inline-formula>], confirming our model’s key prediction. The mean and +/- SEM are indicated by the point and error bar.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.014">http://dx.doi.org/10.7554/eLife.13747.014</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13747-fig5-v2"/></fig></p><p>In fitting the model, we were surprised to find that subjects assigned negative values to no-reward outcomes and its predicting cue (the bottom dotted line in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, see also <xref ref-type="table" rid="tbl2">Table 2</xref>) for the estimated model parameters). This negativity <italic>emerged</italic> through our fitting, as we did not assume the sign of the value. Forcing this outcome to be worth 0 led to a significantly worse iBIC score (<xref ref-type="fig" rid="fig4">Figure 4D</xref> and <xref ref-type="table" rid="tbl1">Table 1</xref>). We found a particular effect of dread at the delay of 7.5 s, which the model interpreted as implying that the time scale of savoring was longer than that of dread (see <inline-formula><mml:math id="inf127"><mml:msup><mml:mi>ν</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf128"><mml:msup><mml:mi>ν</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> in <xref ref-type="table" rid="tbl2">Table 2</xref>). Omitting the negative value of the no-reward cue led to a failure to fit this effect (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).<table-wrap id="tbl2" position="float"><object-id pub-id-type="doi">10.7554/eLife.13747.015</object-id><label>Table 2.</label><caption><p>Related to <xref ref-type="fig" rid="fig4">Figure 4</xref>. The group means <inline-formula><mml:math id="inf129"><mml:mi>μ</mml:mi></mml:math></inline-formula> that estimated by hierarchical Bayesian analysis for our human experiment.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13747.015">http://dx.doi.org/10.7554/eLife.13747.015</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th><inline-formula><mml:math id="inf130"><mml:mi>α</mml:mi></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf133"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mspace width="veryverythickmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM281"><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf134"><mml:msub><mml:mi>ν</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf135"><mml:msub><mml:mi>ν</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula></th></tr></thead><tbody><tr><td>0.17</td><td>0.85</td><td>-0.84</td><td>0.041 (<inline-formula><mml:math id="inf136"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>)</td><td>0.082 (<inline-formula><mml:math id="inf137"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>)</td><td>0.41 (<inline-formula><mml:math id="inf138"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>)</td></tr></tbody></table></table-wrap></p><p>Thus analysis using our model showed that the values of targets were computed via a competition between savoring and dread (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). That is, when participants chose 0% info target, they experienced a mixture of the baseline savouring of possible reward and the baseline dread of possible no-reward during the wait period. On the other hand, when they chose the 100% info target, they experienced either savoring that was boosted by the RPE from the reward predictive cue, or dread that was boosted by the RPE from the no-reward predictive cue. It is this that required us to use the absolute value of the RPE to boost the effects of both savouring and dread. Specifically, RPE from the no-reward predictive cue also boosted the impact of dread, rather than damped it.</p><p>Note that the difference between the data and the RPE-boosted anticipation model without dread, seen in (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>) at short delay periods, implies that the increase of preference of 100% info target was lower than what the model with only savouring can predict. This is because the preference of targets reflected the competition between positive savouring and negative dread, both of which changed non-monotonically with delay (<xref ref-type="fig" rid="fig4">Figure 4C</xref>).</p><p>More precisely, for delays wherein the impact of savoring and dread were similarly strong, choice preference remained at around the chance level. This phenomenon was confirmed in our Experiment-1 and Experiment-2 at short delay conditions (<inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> s), where the choice probability remained around 0.5 (<xref ref-type="fig" rid="fig4">Figure 4C</xref>,<xref ref-type="fig" rid="fig5">5D</xref> and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Since the timescale of dread was smaller than that for savoring, the effect of competition was present only at the short delay conditions. By contrast, at longer delay conditions (<inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> s), dread was discounted and savoring became dominant. This resulted in a large increase of preference for the 100% info target.</p><p>This sudden increase in choice preference was caused by the non-monotonic Q-value functions of targets (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Our model comparison analysis also supported this conclusion (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), where the model with non-monotonic value functions (our original model with temporal discounting with the estimated group mean of the discounting rate: <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) outperformed the model with monotonic value functions (our model without temporal discounting <inline-formula><mml:math id="inf142"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). The difference in the iBIC score was 12.</p><p>Note that temporal discounting of savoring at extremely long delays should also make subjects indifferent between 100% and 0% info targets. Unfortunately, we failed to confirm this prediction in the current study. In fact, analysis of our model suggests that in order to confirm this effect the delays concerned would need to be more than 135 s to enable detection of a difference between the preference at the extremely long delay and the preference at <inline-formula><mml:math id="inf143"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> s (<inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf145"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula>). Thus, we were not able to confirm this indifference in our task, and leave this for future studies. Nonetheless, we note that our model with temporal discounting outperformed a model without temporal discounting in terms of the iBIC scores (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), where the latter model predicts monotonic value functions.</p><p>The magnitude of our discount rate (0.04 <inline-formula><mml:math id="inf146"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>) is comparable with other intertemporal choice studies (e.g. 0.095 <inline-formula><mml:math id="inf147"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> for monetary rewards (<xref ref-type="bibr" rid="bib60">Schweighofer et al., 2008</xref>), and 0.014 <inline-formula><mml:math id="inf148"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> for primary (juice) rewards (<xref ref-type="bibr" rid="bib44">McClure et al., 2007</xref>). Note the latter was inferred using a double exponential model, with the faster decay being 0.014 <inline-formula><mml:math id="inf149"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and the slower decay not being significantly different from 0 <inline-formula><mml:math id="inf150"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. Nonetheless, we should also point out that comparing discounting rates across different experimental designs is extremely difficult. For instance, the nature of discounting of primary rewards (e.g. juice or pleasant images) is very likely to be different from discounting of monetary rewards, as money cannot be spent immediately. In fact, <xref ref-type="bibr" rid="bib53">Reuben et al. (2010)</xref> reported that primary rewards (chocolates) were more rapidly discounted than monetary rewards. It is also known that addicts discount the addictive substance at a higher rate than money (e.g. see (<xref ref-type="bibr" rid="bib3">Bickel et al., 1999</xref>) for cigarettes, (<xref ref-type="bibr" rid="bib4">Bickel et al., 2011</xref>) for cocaine). The characteristic timescales of these experiments were much longer (weeks or months) and discounting rates in these literatures are, however, very small compared to ours. This also suggests that comparisons across experiments could be very misleading, since discounting can be adaptive to experimental timescales (<xref ref-type="bibr" rid="bib32">Kable and Glimcher, 2010</xref>).</p><p>We acknowledge that in the current study we did not test our model’s prediction of preference reversal, as we designed the task so that the average amount of reward obtained from each target per trial was the same for both targets. This is a limitation of our current study, and we leave this issue to future investigations.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Although reward prediction errors (RPEs) have historically been treated as straightforward learning signals, there is increasing evidence that they also play more direct roles, including in classical conditioning (or Pavlovian) approach (<xref ref-type="bibr" rid="bib43">McClure et al., 2003</xref>) and subjective well-being (<xref ref-type="bibr" rid="bib55">Rutledge et al., 2014</xref>, <xref ref-type="bibr" rid="bib56">2015</xref>). The latter study found that subjective well-being, or happiness, is influenced more prominently by RPE than reward itself, which has echoes with an older idea often referred to as the ‘hedonic treadmill’ (<xref ref-type="bibr" rid="bib7">Brickman and Campbell, 1971</xref>; <xref ref-type="bibr" rid="bib18">Frederick and Loewenstein, 1999</xref>). Here we considered a further contribution of RPEs, stemming from their ability to boost the value of the anticipation of reward.</p><p>RPE-boosted anticipation provides a natural account for gambling behaviors. Indeed, our model further predicts that the tendency to be risk-seeking or risk-averse is subject to change as a function of the delay between the cues and rewards. This has important consequences for gambling, as well as the nature and measurement of risk attitudes in general. Specifically, our findings suggest that an unexpected prize will have greater motivational impact when there is a moderate delay between its revelation and its realization. Further experiments will be required to confirm this novel prediction in the context of more conventional economic gambling tasks. We also note that it has been shown in macaque monkeys that changing inter-trial-intervals can impact risk sensitivity (<xref ref-type="bibr" rid="bib45">McCoy and Platt, 2005</xref>; <xref ref-type="bibr" rid="bib28">Hayden and Platt, 2007</xref>). This indicates that the anticipation and discounting of future outcomes over multiple trials may also play important roles in determining risk attitudes.</p><p>RPE-boosted anticipation, like many apparently Pavlovian behaviors that involve innate responses, appears evidently suboptimal. As we have seen, choice can be strikingly non-normative. Our results are consistent with notions such as curiosity/exploration bonuses, and uncertainty aversion (<xref ref-type="bibr" rid="bib38">Loewenstein, 1994</xref>; <xref ref-type="bibr" rid="bib11">Caplin and Leahy, 2001</xref>; <xref ref-type="bibr" rid="bib36">Litman, 2005</xref>; <xref ref-type="bibr" rid="bib13">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib16">Fiorillo, 2011</xref>; <xref ref-type="bibr" rid="bib19">Friston et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">2015</xref>; <xref ref-type="bibr" rid="bib23">Gottlieb et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Blanchard et al., 2015a</xref>; <xref ref-type="bibr" rid="bib34">Kidd and Hayden, 2015</xref>). However, whether the behaviors reflect mechanistic constraints on neural computation (<xref ref-type="bibr" rid="bib33">Kakade and Dayan, 2002</xref>), or a suitable adaptation to typical evolutionary environments, remains a question for further research.</p><p>In our human experiments, we found that participants assigned a negative value to a no-outcome. This appears not to be the case in reported pigeon experiments. One idea is that this negative value emerges from a form of normalization of subjective values (<xref ref-type="bibr" rid="bib64">Tobler et al., 2005</xref>; <xref ref-type="bibr" rid="bib65">2007</xref>; <xref ref-type="bibr" rid="bib41">Louie et al., 2013</xref>) consistent with the finding that human subjects can assign ‘unpleasantness’ to no-reward stimuli (<xref ref-type="bibr" rid="bib65">Tobler et al., 2007</xref>). The effect in our task would be that subjects would apparently experience the anticipation of both positive and negative outcomes in this task as being pleasant for the reward predictive cue (savouring) but unpleasant for the no-outcome predictive cue (dread). This was confirmed in informal debriefing after the experiment. Note that in the monkey experiments (<xref ref-type="bibr" rid="bib9">Bromberg-Martin and Hikosaka, 2009</xref>; <xref ref-type="bibr" rid="bib10">2011</xref>), the lower-value outcome still involved an actual reward, albeit of a smaller size.</p><p>We showed that responses of habenula neurons to reward predictive cues, which have been proposed as an ‘information prediction error’ (<xref ref-type="bibr" rid="bib9">Bromberg-Martin and Hikosaka, 2009</xref>; <xref ref-type="bibr" rid="bib10">2011</xref>), could be accounted for by our model in terms of conventional reward prediction errors. This is because our model included the value of anticipation of rewards that can be boosted by RPE.</p><p>Further studies are necessary to explore the calculation and representation of the anticipation itself, for both savouring (of positive outcomes) and dread (of negative ones). We note recent experimental findings in basal forebrain suggest seductive similarities (<xref ref-type="bibr" rid="bib49">Monosov and Hikosaka, 2013</xref>; <xref ref-type="bibr" rid="bib50">Monosov et al., 2015</xref>), while other brain areas, such as ventral striatum (<xref ref-type="bibr" rid="bib31">Jensen et al., 2003</xref>; <xref ref-type="bibr" rid="bib25">Hariri et al., 2006</xref>; <xref ref-type="bibr" rid="bib57">Salimpoor et al., 2011</xref>), posterior insula and anterior cingulate cortex (<xref ref-type="bibr" rid="bib2">Berns et al., 2006</xref>; <xref ref-type="bibr" rid="bib6">Blanchard et al., 2015b</xref>), may also contribute. Furthermore, ramping dopamine signals toward the delivery of rewards (if they generally exist, see [<xref ref-type="bibr" rid="bib51">Morris et al., 2004</xref>]) could also be related to the anticipation of rewards (<xref ref-type="bibr" rid="bib15">Fiorillo et al., 2003</xref>; <xref ref-type="bibr" rid="bib29">Howe et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">Lloyd and Dayan, 2015</xref>; <xref ref-type="bibr" rid="bib24">Hamid et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Hart et al., 2015</xref>), while dopamine neurons have also been shown to manifest a stronger phasic response to one predicting an uncertain reward than to a cue predicting a certain reward (<xref ref-type="bibr" rid="bib16">Fiorillo, 2011</xref>). It would also be interesting to study the difference between savouring and dread, particularly given debates concerning the encoding of information about punishment <xref ref-type="bibr" rid="bib15">Fiorillo et al. (2003)</xref>, <xref ref-type="bibr" rid="bib8">Brischoux et al. (2009)</xref>, <xref ref-type="bibr" rid="bib35">Lammel et al. (2014)</xref>, and about the symmetry or otherwise between the encoding of positive and negative prediction errors for reward (<xref ref-type="bibr" rid="bib17">Fiorillo, 2013</xref>; <xref ref-type="bibr" rid="bib26">Hart et al., 2014</xref>)</p><p>One issue that merits further future study is adaptation to different delays. It has been shown that human subjects are capable of optimizing their discounting rate according to task demands (<xref ref-type="bibr" rid="bib59">Schweighofer et al., 2006</xref>), and also that the discounting may be computed relative to the timing of other available options, rather than absolute time (<xref ref-type="bibr" rid="bib32">Kable and Glimcher, 2010</xref>). Our control experiment for Experiment-1 showed a signature of adaptation (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>), with subjects reacting differently to a sudden large increase in delays after many trials with small delays, compared to a gradual increase in delays as in Experiment-1. However we found no evidence for this effect in our Experiment-2 in which delay conditions were randomized on a trial-by-trial basis. It would be interesting to study this further, in relation to the uncertainty in timing of rewards. In our task there was no effect of timing uncertainty on choice, as both targets are associated with the same delay. However it would become important if a task involves a choice between targets with different delay conditions. Furthermore, if the prediction error could influence subjective time (for instance via a known effect of dopamine on aspects of timing [<xref ref-type="bibr" rid="bib21">Gibbon et al., 1997</xref>]), then this could have complex additional effects on anticipation.</p><p>In sum, we account for a well-described preference for observing behavior through a suggestion that reward prediction errors modulate the contribution to subjective value that arises from the anticipation of upcoming rewards. Our study provides a new perspective on reward-based learning and decision-making under uncertainty, and may be of special relevance to gambling and addiction.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>56 heterosexual male participants (age 18–40) were recruited from the UCL community. Participants were paid 10 British pounds at the end of the experiment. Participants provided informed consent for their participation in the study, which was approved by the UCL ethics committee (UCL Research Ethics Reference: 3450/002).</p></sec><sec id="s4-2"><title>Task procedures</title><sec id="s4-2-1"><title>Experiment-1</title><p>14 participants performed the main experimental task, which we call Experiment-1. First, the procedure was explained to them; they then underwent a practice session consisting of 30 trials with the same format as the main session except with different target colors and reward images. The practice session was followed by the experimental trials, involving four blocks with different (not-randomized) delay conditions: 2.5 s (90 trials), 7.5 s (36 trials), 20 s (18 trials), 40 s (18 trials).</p><p>At the beginning of each trial, two rectangular targets with different colors appeared side-by-side on the screen. The targets were either informative or uninformative, indicated by colors. Participants were instructed that one of the colors led to ‘signs’ that indicate the future presence of a reward or no-reward (see below). The positions of the targets were determined randomly on each trial. Participants had to choose the left or right target by pressing ‘F’ or ‘J’ respectively within 3 s; responses on other keys or that were longer than 3 s resulted in a penalty of 3 s, followed by another trial. Once a target had been chosen, it remained stationary on the screen for 1.5 s; the other target having been extinguished. If participants chose the informative target, then one of the two shape cues (triangle or oval) with the same color as the target randomly appeared in the center of the screen. Each shape consistently indicated that the outcome would be reward or no-reward. The cue remained visible for the length of the delay determined by the block. If participants chose the no-information target, the target disappeared after 1.5 s, and no cue appeared on the screen for the delay period, followed randomly by a reward or no-reward. In case of a reward, an image of an attractive female model was presented for 1.3 s. In case of no-reward, no image was shown for this period. After the reward or no-reward period, a blank screen was shown for 1.2 s before the next trial began, when two targets appeared on the screen.</p></sec><sec id="s4-2-2"><title>Experiment-1 - control</title><p>A control experiment was conducted on additional 11 participants. Everything was the same as for the main task, except that these participants faced the delay conditions in a different order from the main task: 2.5 s (90 trials), 40 s (18 trials), 20 s (18 trials), 7.5 s (36 trials), and then an additional block with the same delay as the first block 2.5 s (36 trials).</p><p>As seen in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>, subjects’ choices in the first and fifth blocks (under the same delay condition; 2.5 s) did not differ. We found, however, the change between the first and the second blocks was so dramatic in terms of the change in <inline-formula><mml:math id="inf151"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> that subjects behaved differently between the first (2.5 s) and second (40 s) block compared to the main task, in which the delay length increased gradually across the blocks <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>. It is possible that the perception of delay was influenced by history; something that we did not take into the account in our model. Hence we focused on our analysis on the main task subjects.</p></sec><sec id="s4-2-3"><title>Experiment-2</title><p>An additional 31 participants performed Experiment-2. The basic structure was similar to Experiment-1, however, in this task, 1) the delay conditions <inline-formula><mml:math id="inf152"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula> were randomized trial-by-trial; 2) the delay length (1 s, 5 s, 10 s, 20 s or 40 s) was explicitly indicated by a cue at the beginning of each trial; 3) the targets, either informative or uninformative, were explicitly indicated by messages presented at targets; 4) the uninformative target was also followed by a cue, rather than a blank screen; 5) a no-reward outcome was indicated by a no-entry sign.</p><p>As in Experiment-1, the procedure was explained to participants before they underwent a practice session consisting of 10 trials (2 trials per each delay condition).</p><p>At the beginning of each trial, pictures of hourglasses and a silhouette of woman covered by a semicircle appeared on the screen. Participants were instructed that the number of hourglasses indicated the delay between their choice and the reward delivery (<inline-formula><mml:math id="inf153"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula>), and that one hourglass indicated 5 s of delay. When the delay was 1 s, a fraction of an hourglass was displayed on the screen; this was also instructed to participants. Further, they were instructed that the semicircle appeared on the screen indicating a 50% chance of getting a reward. The delay conditions were randomized across trials.</p><p>After 500 ms, two rectangular targets with different colors appeared side by side on the screen. One target always presented the message 'Find out now', while the other target had a message 'Keep it Secret'. The same message appeared on the same colored target across trials, but the sides on which target and message appeared were randomized across trials. Participants had to choose the left or right target by pressing ‘F’ or ‘J’. Once a target had been chosen, the chosen target was highlighted by yellow exterior for 1.5 s. If participants chose the informative target, then one of the two cues (a symbolic picture of a butterfly or an ant) randomly appeared at the center of the screen. Each cue consistently indicated that the outcome would be reward or no-reward. The cue remained visible for the length of the delay that was determined by the number of hourglasses presented at the beginning of each trial. If participants chose the no-information target, a cue (a symbolic picture of a turtle) appeared on the screen for the delay period, followed randomly by a reward or no-reward. In case of a reward, an image of an attractive female model was presented for 1.3 s. In case of no-reward, the image of a no-entry sign was presented for 1.3 s. After the reward or no-reward period, a blank screen was shown for 1.2 s before the next trial began. In the main task, each participant performed 25 trials (5 trials per delay condition), and received 10 British pounds at the end of experiment.</p></sec></sec><sec id="s4-3"><title>Reward images</title><p>We sought to use basic rewards that could be consumed by subjects on each trial at the time of provision. We therefore employed images of female models that had previously been rated by heterosexual male subjects (<xref ref-type="bibr" rid="bib12">Crockett et al., 2013</xref>). In case of reward, a random one of the top 100 highest rated images was presented to subjects without replacement.</p></sec><sec id="s4-4"><title>Analysis</title><p>We sought to determine the distribution of model parameters <inline-formula><mml:math id="inf154"><mml:mi mathvariant="bold">𝐡</mml:mi></mml:math></inline-formula>. Thus following (<xref ref-type="bibr" rid="bib30">Huys et al., 2011</xref>), we conducted a hierarchical Bayesian, random effects analysis, where the (suitably transformed) parameters <inline-formula><mml:math id="inf155"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of individual <inline-formula><mml:math id="inf156"><mml:mi>i</mml:mi></mml:math></inline-formula> are treated as a random sample from a population distribution, which we assume to be Gaussian, with means and variance <inline-formula><mml:math id="inf157"><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub id="XM27"><mml:mi mathvariant="bold-italic">𝝁</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM28"><mml:mi mathvariant="bold">𝚺</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The prior group distribution <inline-formula><mml:math id="inf158"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula> can be set as the maximum likelihood estimate:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left right right left right left right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>∫</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We optimized <inline-formula><mml:math id="inf159"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula> using an approximate Expectation-Maximization procedure. For the E-step of the k-th iteration, we employed a Laplace approximation, obtaining,<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-4_4"><mml:mtext>(4)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-5_4"><mml:mtext>(5)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf160"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup id="XM31"><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup id="XM32"><mml:mi mathvariant="bold">𝚺</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the Normal distribution with the mean <inline-formula><mml:math id="inf161"><mml:msubsup><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> and the covariance <inline-formula><mml:math id="inf162"><mml:msubsup><mml:mi mathvariant="bold">𝚺</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> that is obtained from the inverse Hessian around <inline-formula><mml:math id="inf163"><mml:msubsup><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula>. For the M step:<disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-6"><mml:mtext>(6)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-7"><mml:mtext>(7)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>For simplicity, we assumed that the covariance <inline-formula><mml:math id="inf164"><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> had zero off-diagonal terms, assuming that the effects were independent. Also, in order to treat different delay conditions equally, we randomly sub-sampled the trials to equalize the number used per condition in order to calculate the statistics. Additionally we obtained the same results by normalizing the posterior for each delay condition when estimating the expectation.</p><p>For the model-free data analysis, we used the t-test, as the data passed the Shapiro-Wilk normality test and the paired F-test for equal variances for each and between conditions.</p></sec><sec id="s4-5"><title>Model comparison</title><p>We compared models according to their integrated Bayes Information Criterion (iBIC) scores, based on a flat prior over models. We analysed model log likelihood <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ6"><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right center left" columnspacing="0 thickmathspace" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-8_8"><mml:mtext>(8)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-9_8"><mml:mtext>(9)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>iBIC</mml:mtext></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where iBIC is the <italic>integrated</italic> Baysian Information Criterion, <inline-formula><mml:math id="inf166"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi id="XM35">M</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> is the number of fitted parameters of the prior and <inline-formula><mml:math id="inf167"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi id="XM36">D</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> is the number of data points (total number of choices made by all subjects). Here, <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> can be computed by integrating out individual parameters:<disp-formula id="equ7"><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-10_5"><mml:mtext>(10)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo>∫</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-11_5"><mml:mtext>(11)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where we approximated the integral as the average over <inline-formula><mml:math id="inf169"><mml:mi>K</mml:mi></mml:math></inline-formula> samples <inline-formula><mml:math id="inf170"><mml:msup><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:math></inline-formula>’s generated from the prior <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>As seen in <xref ref-type="table" rid="tbl1">Table 1</xref>, our model of anticipation fit better than conventional Q-learning models with or without discounting (the latter two models being equivalent to our model with parameters set such that there is no anticipation.)</p></sec><sec id="s4-6"><title>Model</title><p>We describe our model for the case of a simple conditioning task. Suppose that a subject takes an action and receives a reward predictive cue <inline-formula><mml:math id="inf172"><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> at <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> with a probability of <inline-formula><mml:math id="inf174"><mml:mi>q</mml:mi></mml:math></inline-formula> followed by a reward <inline-formula><mml:math id="inf175"><mml:mi>R</mml:mi></mml:math></inline-formula> at <inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mspace width="veryverythickmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM37"><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, or no-reward predictive cue <inline-formula><mml:math id="inf177"><mml:msup><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:math></inline-formula> at <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> with a probability of <inline-formula><mml:math id="inf179"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula> followed by no reward. Following (<xref ref-type="bibr" rid="bib39">Loewenstein, 1987</xref>; <xref ref-type="bibr" rid="bib2">Berns et al., 2006</xref>; <xref ref-type="bibr" rid="bib63">Story et al., 2013</xref>), the anticipation of the reward at time <inline-formula><mml:math id="inf180"><mml:mi>t</mml:mi></mml:math></inline-formula> is worth <inline-formula><mml:math id="inf181"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM39">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM38"><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf182"><mml:mi>ν</mml:mi></mml:math></inline-formula> governs its rate. Including <inline-formula><mml:math id="inf183"><mml:mi>R</mml:mi></mml:math></inline-formula> itself, and taking temporal discounting into account, the total value of the reward predictive cue, <inline-formula><mml:math id="inf184"><mml:msub><mml:mi>Q</mml:mi><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:msub></mml:math></inline-formula>, is<disp-formula id="equ8"><label>(12)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left right left right left right left right left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Anticipation]</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Reward]</mml:mtext></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mi>ν</mml:mi><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf185"><mml:mi>η</mml:mi></mml:math></inline-formula> is the relative weight of anticipation and <inline-formula><mml:math id="inf186"><mml:mi>γ</mml:mi></mml:math></inline-formula> is the discounting rate. In previous work, <inline-formula><mml:math id="inf187"><mml:mi>η</mml:mi></mml:math></inline-formula> has been treated as a constant; however, here we propose that it can vary with the prediction error <inline-formula><mml:math id="inf188"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> at the predicting cue. While the simplest form is the linear relationship given by <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>, our model’s behavior does not depend on the details of the RPE dependence of anticipation. In fact, one can instead assume<disp-formula id="equ9"><label>(13)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr/><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>η</mml:mi></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>tanh</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf189"><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf190"><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are constants, or<disp-formula id="equ10"><label>(14)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr/><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>η</mml:mi></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>θ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf191"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM60">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the step function that we define: <inline-formula><mml:math id="inf192"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM61">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf194"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM62">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. All the findings in this paper hold for <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>, <xref ref-type="disp-formula" rid="equ9">Equation (13)</xref>, and <xref ref-type="disp-formula" rid="equ10">Equation (14)</xref> (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> ). In fact, <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> and <xref ref-type="disp-formula" rid="equ10">Equation (14)</xref> can be thought as different approximations to <xref ref-type="disp-formula" rid="equ9">Equation (13)</xref> (see below).</p><p>Note that in our model, RPE affects the total value <inline-formula><mml:math id="inf196"><mml:msub><mml:mi>Q</mml:mi><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:msub></mml:math></inline-formula>, which also affects subsequent RPEs. One might therefore wonder whether there is a stable value for the cue. In the Results, we introduced a linear ansatz for the boosting of anticipation on RPE (<xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>. In a wide range of parameter regime, this ansatz has a stable, self-consistent, solution; however, in a small parameter regime, the linear ansatz fails to provide such solutions. This is because the linear assumption allows <italic>unbounded</italic> boosting. Crudely, the RPE can usually be expressed as a linear combination of Q-values. Thus, the following equation has to have a real solution:<disp-formula id="equ11"><label>(15)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are determined by the task. The solution is the intercept of two lines: <inline-formula><mml:math id="inf198"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub id="XM67"><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which does not exist when <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>In this example, the prediction error at the reward predictive cue is positive, and<disp-formula id="equ12"><label>(16)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right left right left left right left right left left right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext> [Anticipation]</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Reward]</mml:mtext></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This has a solution at <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> only if<disp-formula id="equ13"><label>(17)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Anticipation]</mml:mtext></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1.</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This condition can be violated, for instance, if <inline-formula><mml:math id="inf203"><mml:mi>c</mml:mi></mml:math></inline-formula> is very large (more precisely, if <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Anticipation]</mml:mtext></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>). Roughly speaking, the stability condition is violated when the boosted anticipation is very large.</p><p>If it indeed exists, the solution is<disp-formula id="equ14"><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Anticipation]</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Reward]</mml:mtext></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Anticipation]</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>which gives<disp-formula id="equ15"><label>(19)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Anticipation]</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Reward]</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>[Anticipation]</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To avoid the stability problem problem, one can instead assume <xref ref-type="disp-formula" rid="equ9">Equation (13)</xref>. This is a more general form which leads to the self-consistency equation:<disp-formula id="equ16"><label>(20)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi id="XM92">tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM93"><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub id="XM91"><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which requires <inline-formula><mml:math id="inf205"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf206"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi id="XM95">tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM96"><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub id="XM94"><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to intersect. This can happen for any real <inline-formula><mml:math id="inf207"><mml:mrow><mml:mi id="XM97">α</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM98">β</mml:mi></mml:mrow></mml:math></inline-formula>. However, importantly, our model’s behavior does not depend on the details of the RPE dependence of anticipation (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> ). Hence we did not attempt to determine the exact functional form.</p><p>Note that <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> can be thought of as an approximation to <xref ref-type="disp-formula" rid="equ9">Equation (13)</xref> when <inline-formula><mml:math id="inf208"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub id="XM99"><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is small. In the limit of <inline-formula><mml:math id="inf209"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>, on the other hand, <xref ref-type="disp-formula" rid="equ9">Equation (13)</xref> becomes <xref ref-type="disp-formula" rid="equ10">Equation (14)</xref>. <xref ref-type="disp-formula" rid="equ10">Equation (14)</xref> can be used instead of <xref ref-type="disp-formula" rid="equ9">Equation (13)</xref> when <inline-formula><mml:math id="inf210"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub id="XM100"><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is large; or also can be used as an approximation of <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> when the size of RPE is roughly the same from trial to trial.</p></sec><sec id="s4-7"><title>Model application 1: <xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka task (2011)</xref></title><p>To see how the model works, take the task introduced in <xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka (2011)</xref> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). We assume that the 100% info target is followed randomly by a cue <inline-formula><mml:math id="inf211"><mml:msub><mml:mi>S</mml:mi><mml:mtext>Big</mml:mtext></mml:msub></mml:math></inline-formula> that is always followed by a big reward <inline-formula><mml:math id="inf212"><mml:msub><mml:mi>R</mml:mi><mml:mtext>Big</mml:mtext></mml:msub></mml:math></inline-formula> after a delay <inline-formula><mml:math id="inf213"><mml:mi>T</mml:mi></mml:math></inline-formula>, or by a cue <inline-formula><mml:math id="inf214"><mml:msub><mml:mi>S</mml:mi><mml:mtext>Small</mml:mtext></mml:msub></mml:math></inline-formula> that is always followed by a small reward <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The 0% info target is followed by a cue <inline-formula><mml:math id="inf216"><mml:msub><mml:mi>S</mml:mi><mml:mtext>Random</mml:mtext></mml:msub></mml:math></inline-formula>, which is followed by the reward <inline-formula><mml:math id="inf217"><mml:msub><mml:mi>R</mml:mi><mml:mtext>Big</mml:mtext></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf218"><mml:msub><mml:mi>R</mml:mi><mml:mtext>Small</mml:mtext></mml:msub></mml:math></inline-formula> with equal probabilities. The 50% info target is followed by either of the three cues <inline-formula><mml:math id="inf219"><mml:msub><mml:mi>S</mml:mi><mml:mtext>Big</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf220"><mml:msub><mml:mi>S</mml:mi><mml:mtext>Small</mml:mtext></mml:msub></mml:math></inline-formula>, or <inline-formula><mml:math id="inf221"><mml:msub><mml:mi>S</mml:mi><mml:mtext>Random</mml:mtext></mml:msub></mml:math></inline-formula> with a probability of <inline-formula><mml:math id="inf222"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf223"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf224"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, respectively.</p><p>The expected values of targets (<inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>100</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) are<disp-formula id="equ17"><label>(21)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>100</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ18"><label>(22)</label><mml:math id="m18"><mml:mtable columnalign=" left right left left right right"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mtext>Random</mml:mtext></mml:msub></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ19"><label>(23)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Random</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Random</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the expected values of cues <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Random</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. The RPE at the cues depends on the chosen target, implying that the average values of the cues can be expressed (assuming that the transition probabilities are properly learned) as<disp-formula id="equ20"><label>(24)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ21"><label>(25)</label><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ22"><label>(26)</label><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Random</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Random</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Random</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf228"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub id="XM135"><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow id="XM136"><mml:mi>X</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is the mean values of the cues <inline-formula><mml:math id="inf229"><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf230"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula> Big, Small, or Random) in case following the <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>X</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:math></inline-formula> info target (<inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> or 0):<disp-formula id="equ23"><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-27_16"><mml:mtext>(27)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi mathvariant="normal">%</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>η</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-28_16"><mml:mtext>(28)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>η</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-29_16"><mml:mtext>(29)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Random</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi mathvariant="normal">%</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Random</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>Random</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where, under the assumption <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>,<disp-formula id="equ24"><label>(30)</label><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>η</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf233"><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub id="XM175"><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow id="XM176"><mml:mi>X</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> is RPE at the cue <inline-formula><mml:math id="inf234"><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> after choosing <inline-formula><mml:math id="inf235"><mml:mrow><mml:mi>X</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:math></inline-formula> target<disp-formula id="equ25"><label>(31)</label><mml:math id="m25"><mml:mtable columnalign=" left right left left right right"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub id="XM179"><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow id="XM180"><mml:mi>X</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>and <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>A</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf237"><mml:msub><mml:mi>B</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf238"><mml:mi>l</mml:mi></mml:math></inline-formula> is the reward size; in our case Big or Small) are the anticipation and the reward itself:<disp-formula id="equ26"><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-32_7"><mml:mtext>(32)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>ν</mml:mi><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-33_7"><mml:mtext>(33)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that we ignored any anticipation of the cues themselves after the choice. This would not alter the qualitative predictions of the model. Recent experiments (<xref ref-type="bibr" rid="bib46">McDevitt et al., 1997</xref>; <xref ref-type="bibr" rid="bib47">2016</xref>) showed that delaying the timing of reward predictive cues decreased the preference for the informative target. This is consistent with our model because delaying the cue presentation means decreasing the wait time; hence leads to a smaller impact of boosted anticipation. In our experiment, the time between choice and the cue presentation was too short to be significant.</p><p>The probability of choosing <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>X</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:math></inline-formula> target over <inline-formula><mml:math id="inf240"><mml:mrow><mml:mi>Y</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:math></inline-formula> target, <inline-formula><mml:math id="inf241"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, is assumed to be a sigmoid function of the difference between the target values:<disp-formula id="equ27"><label>(34)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>These equations account well for the behavioral and neuronal findings in <xref ref-type="bibr" rid="bib10">Bromberg-Martin and Hikosaka (2011)</xref>. The results in <xref ref-type="fig" rid="fig2">Figure 2D,E</xref> are obtained from these equations with <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mtext>Big</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mtext>Small</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mtext>sec</mml:mtext></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>Delay</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.25</mml:mn><mml:mtext> </mml:mtext><mml:mrow><mml:mtext>sec</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mtext>sec</mml:mtext></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.08</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-8"><title>Model application 2: Spetch et al. task (1990) (and also Gipson et al. task (2009), Stagner and Zentall (2010) task.)</title><p><xref ref-type="bibr" rid="bib61">Spetch et al. (1990)</xref> reported the striking finding that pigeons can prefer a target that is followed by reward with a probability of <inline-formula><mml:math id="inf243"><mml:mn>0.5</mml:mn></mml:math></inline-formula> over a target that is always followed by a reward under certain conditions. Here we show that our model can also account for this surprisingly ‘irrational’ behavior. A generalized version of the task is schematically shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. On each trial, a subject chooses either of two colored targets (Red or Blue in this example). If Red is chosen, one of the cues <inline-formula><mml:math id="inf244"><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> or <inline-formula><mml:math id="inf245"><mml:msup><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:math></inline-formula> is randomly presented, where <inline-formula><mml:math id="inf246"><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> is always followed by a reward after time <inline-formula><mml:math id="inf247"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula>, while <inline-formula><mml:math id="inf248"><mml:msup><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:math></inline-formula> is never followed by reward. If Blue is chosen, a cue <inline-formula><mml:math id="inf249"><mml:msup><mml:mi>S</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> is presented and a reward is followed after the fixed time delay <inline-formula><mml:math id="inf250"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula> with a probability of <inline-formula><mml:math id="inf251"><mml:msub><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:math></inline-formula>. The task in <xref ref-type="bibr" rid="bib61">Spetch et al. (1990)</xref> corresponds to the case with <inline-formula><mml:math id="inf252"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and the task in <xref ref-type="bibr" rid="bib22">Gipson et al. (2009)</xref> corresponds to the case with <inline-formula><mml:math id="inf253"><mml:msub><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:math></inline-formula>=0.75. In both cases, by always choosing Blue, animals can get the maximum amount of rewards; however, it has been shown that animals can prefer to choose Red over Blue (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>; <xref ref-type="bibr" rid="bib22">Gipson et al., 2009</xref>), where the preference of Red with <inline-formula><mml:math id="inf254"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> appeared to be heavily dependent on the length of the delay between the predicting cues and the delivery of rewards.</p><p>Our model can account for the irrational behaviors. We first determine the value of choice, <inline-formula><mml:math id="inf255"><mml:msub><mml:mi>Q</mml:mi><mml:mtext>Red</mml:mtext></mml:msub></mml:math></inline-formula>. Under the linear ansatz (2), the prediction error at the cue <inline-formula><mml:math id="inf256"><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> is<disp-formula id="equ28"><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-35_4"><mml:mtext>(35)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>Red</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-36_4"><mml:mtext>(36)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-37_4"><mml:mtext>(37)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>or<disp-formula id="equ29"><label>(38)</label><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>c</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>and<disp-formula id="equ30"><label>(39)</label><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>c</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ31"><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-40"><mml:mtext>(40)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>red</mml:mtext></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-41"><mml:mtext>(41)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>c</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where<disp-formula id="equ32"><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-42_6"><mml:mtext>(42)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mi>ν</mml:mi><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-43_6"><mml:mtext>(43)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf257"><mml:mi>R</mml:mi></mml:math></inline-formula> being the size of reward. <xref ref-type="disp-formula" rid="equ30">Equation (39)</xref> shows how the Q-value of reward predictive cue is boosted. When there is no boosting, <inline-formula><mml:math id="inf258"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the denominator is 1. Increasing the boosting <inline-formula><mml:math id="inf259"><mml:mi>c</mml:mi></mml:math></inline-formula> will decrease the denominator; hence it will increase the Q-value. Note that the denominator is assumed to be positive within the linear ansatz. The value of choice Blue is simply<disp-formula id="equ33"><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>Blue</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Hence the difference in the values in two choices is<disp-formula id="equ34"><label>(45)</label><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>Red</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>Blue</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>c</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Even in the case of <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, this expression can become positive by changing the time delay <inline-formula><mml:math id="inf261"><mml:mi>T</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3B,C</xref>), which can account for the irrational observing behaviors (<xref ref-type="bibr" rid="bib61">Spetch et al., 1990</xref>; <xref ref-type="bibr" rid="bib22">Gipson et al., 2009</xref>). The diagram in <xref ref-type="fig" rid="fig3">Figure 3E</xref> shows the results with the ansatz of <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>, while <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> shows the results with the ansatz of <xref ref-type="disp-formula" rid="equ9">Equation (13)</xref>. Note that the two ansatz provide qualitatively very similar results.</p><p>Since there are numerous variations of this experiment, here we provide with a formula for a more general case. Suppose a subject chooses either of two colored targets 100% info (I) or 0% info (N). If target I is chosen, one of the cues <inline-formula><mml:math id="inf262"><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> or <inline-formula><mml:math id="inf263"><mml:msup><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:math></inline-formula> is randomly presented with a probability of <inline-formula><mml:math id="inf264"><mml:msub><mml:mi>p</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf265"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf266"><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> is always followed by a reward with a size <inline-formula><mml:math id="inf267"><mml:msub><mml:mi>R</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> after time <inline-formula><mml:math id="inf268"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula>, while <inline-formula><mml:math id="inf269"><mml:msup><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:math></inline-formula> is never followed by reward. If target N is chosen, a cue <inline-formula><mml:math id="inf270"><mml:msup><mml:mi>S</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> is presented and a reward with a size <inline-formula><mml:math id="inf271"><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is followed after the fixed time delay <inline-formula><mml:math id="inf272"><mml:msub><mml:mi>T</mml:mi><mml:mtext>Delay</mml:mtext></mml:msub></mml:math></inline-formula> with a probability of <inline-formula><mml:math id="inf273"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>The difference in the values in two choices with the linear boosting ansatz is expressed as<disp-formula id="equ35"><mml:math id="m35"><mml:mtable columnalign=" left right left left right right"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mtext>Info</mml:mtext></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mtext>No-Info</mml:mtext></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM219"><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mi>ν</mml:mi><mml:mo>-</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM218"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM220"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM217"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where,<disp-formula id="equ36"><label>(47)</label><mml:math id="m36"><mml:mtable columnalign=" left right left left right right"><mml:mtr><mml:mtd><mml:msub><mml:mi>A</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:msub><mml:mi>R</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mrow><mml:mi>ν</mml:mi><mml:mo>-</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM222"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>It is straightforward to apply this formula to specific experiments with a specific set of condition. For example, (<xref ref-type="bibr" rid="bib62">Stagner and Zentall, 2010</xref>)’s experimental results can be accounted for by setting <inline-formula><mml:math id="inf274"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf275"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf276"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>. As shown in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, our model reproduced the reported sub-optimal observing behavior.</p></sec><sec id="s4-9"><title>Model application 3: Our task on human participants</title><p>Our experiment, shown schematically in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, was designed to test key aspects of the model. Subjects choose between info and non-info targets of values <inline-formula><mml:math id="inf277"><mml:msub><mml:mi>Q</mml:mi><mml:mtext>info</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf278"><mml:msub><mml:mi>Q</mml:mi><mml:mtext>no-info</mml:mtext></mml:msub></mml:math></inline-formula> respectively. The info target is followed by the reward predicting cue <inline-formula><mml:math id="inf279"><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> with the value of <inline-formula><mml:math id="inf280"><mml:msup><mml:mi>Q</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>, followed by a reward <inline-formula><mml:math id="inf281"><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> after a delay of <inline-formula><mml:math id="inf282"><mml:mi>T</mml:mi></mml:math></inline-formula>, or the no-reward prediction cue <inline-formula><mml:math id="inf283"><mml:msup><mml:mi>S</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> (here we write <inline-formula><mml:math id="inf284"><mml:msup><mml:mi>S</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf285"><mml:msup><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:math></inline-formula> for our convention) with the value <inline-formula><mml:math id="inf286"><mml:msup><mml:mi>Q</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula>, followed by a no-reward with a value of <inline-formula><mml:math id="inf287"><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula>. The no-info target is followed by no cue (which we write <inline-formula><mml:math id="inf288"><mml:msup><mml:mi>S</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> for our convention) and randomly followed by a reward <inline-formula><mml:math id="inf289"><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> or no-reward <inline-formula><mml:math id="inf290"><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> after the delay of <inline-formula><mml:math id="inf291"><mml:mi>T</mml:mi></mml:math></inline-formula>. Note that we needed to introduce the value for the no-reward outcome based on the behaviors and self-reports.</p><p>Here, we fit the simplest form of the model to the subjects’ behavior. Since we fit the model trial by trial, we need to introduce <italic>learning</italic>. After each trial, the value of chosen target was updated as<disp-formula id="equ37"><mml:math id="m37"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM224"><mml:mi>V</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf292"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula>info or no-info, <inline-formula><mml:math id="inf293"><mml:mi>α</mml:mi></mml:math></inline-formula> is the learning rate and <inline-formula><mml:math id="inf294"><mml:mi>V</mml:mi></mml:math></inline-formula> is the reward function:<disp-formula id="equ38"><label>(49)</label><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right left left right right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>with<disp-formula id="equ39"><mml:math id="m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-50_6"><mml:mtext>(50)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-51_6"><mml:mtext>(51)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf295"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf296"><mml:mo>-</mml:mo></mml:math></inline-formula>, in case of a reward or a no-reward, respectively. We assumed that the anticipation rate <inline-formula><mml:math id="inf297"><mml:msup><mml:mi>ν</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf298"><mml:msup><mml:mi>ν</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> can be different but the discounting rate is the same <inline-formula><mml:math id="inf299"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. The coefficient <inline-formula><mml:math id="inf300"><mml:msub><mml:mi>η</mml:mi><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:msub></mml:math></inline-formula> was assumed to be<disp-formula id="equ40"><label>(52)</label><mml:math id="m40"><mml:mtable columnalign=" left right left left right right"><mml:mtr><mml:mtd><mml:msub><mml:mi>η</mml:mi><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false"/><mml:msubsup id="XM235"><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msubsup><mml:mo stretchy="false"/></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>This assumption allows us to reduce the number of free parameters. To see it, we write the expected values of targets:<disp-formula id="equ41"><mml:math id="m41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-53_8"><mml:mtext>(53)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>info</mml:mtext></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-54_8"><mml:mtext>(54)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>no-info</mml:mtext></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Hence the expected difference is<disp-formula id="equ42"><label>(55)</label><mml:math id="m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>info</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>no-info</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>and the probability of choosing the info target <inline-formula><mml:math id="inf301"><mml:msub><mml:mi>P</mml:mi><mml:mtext>info</mml:mtext></mml:msub></mml:math></inline-formula> is<disp-formula id="equ43"><label>(56)</label><mml:math id="m43"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mtext>info</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As is common, <inline-formula><mml:math id="inf302"><mml:mi>c</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf303"><mml:mi>σ</mml:mi></mml:math></inline-formula> appear together with <inline-formula><mml:math id="inf304"><mml:msup><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:math></inline-formula>’s (<inline-formula><mml:math id="inf305"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf306"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula>). Thus, we set <inline-formula><mml:math id="inf307"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf308"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for the fitting.Thus the model has six free parameters that are fit: <inline-formula><mml:math id="inf309"><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf310"><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf311"><mml:msub><mml:mi>ν</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf312"><mml:msub><mml:mi>ν</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf313"><mml:mi>γ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf314"><mml:mi>α</mml:mi></mml:math></inline-formula>.</p><p>Note that a model that can allow asymmetric dependence of boosting on prediction errors (if it is positive or negative) leads to a related expression:<disp-formula id="equ44"><label>(57)</label><mml:math id="m44"><mml:mtable columnalign=" left right left left right right"><mml:mtr><mml:mtd><mml:msub><mml:mi>η</mml:mi><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup id="XM247"><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM248"><mml:mo>-</mml:mo><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>which leads to<disp-formula id="equ45"><mml:math id="m45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-58_6"><mml:mtext>(58)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>info</mml:mtext></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-59_6"><mml:mtext>(59)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>no-info</mml:mtext></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Hence the expected difference is<disp-formula id="equ46"><label>(60)</label><mml:math id="m46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>info</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>no-info</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus <inline-formula><mml:math id="inf315"><mml:mrow><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf316"><mml:mrow><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> will be fitted as independent variables, which means that the asymmetric boosting will appear in the ratio between <inline-formula><mml:math id="inf317"><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf318"><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula>.</p><p>For the purpose of model comparison, we also fitted the simple Q-learning model (<inline-formula><mml:math id="inf319"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), the Q-learning model with discounting (<inline-formula><mml:math id="inf320"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), the RPE-boosting RL model with no value for the no-outcome (<inline-formula><mml:math id="inf321"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf322"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> ), the RPE-boosting RL model with no discounting (<inline-formula><mml:math id="inf323"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), and the anticipation RL model with no-boosting. To fit the model with no-boosting (<inline-formula><mml:math id="inf324"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), we fitted a full model with <inline-formula><mml:math id="inf325"><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>.</p><p>Note that from <xref ref-type="disp-formula" rid="equ46">Equation (60)</xref>, the expected difference between the values of targets is zero <inline-formula><mml:math id="inf326"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> when there is no boosting <inline-formula><mml:math id="inf327"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. This means that RPE-boosting is necessary to account for observing behaviors that prefer advance reward information.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Ethan Bromberg-Martin, George Loewenstein, Kevin Lloyd, Mehdi Keramati for fruitful discussions, Molly Crockett for reward images with scoring data, and Elliot Ludvig for sharing his parallel studies. This work was supported by the Gatsby Charitable Foundation, the Wellcome Trust (091593/Z/10/Z, and Senior Investigator Award to RJD, 098362/Z/12/Z), the Joint Initiative on Computational Psychiatry and Ageing Research between the Max Planck Society and University College London (RJD)</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>KI, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>GWS, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>ZK-N, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con4"><p>RJD, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con5"><p>PD, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: All participants provided written informed consent and consent to publish prior to start of the experiment, which was approved by the Research Ethics Committee at University College London (UCL Research Ethics Reference: 3450/002)</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beierholm</surname><given-names>UR</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Pavlovian-instrumental interaction in 'observing behavior'</article-title><source>PLoS Computational Biology</source><volume>6</volume><elocation-id>e1000903</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000903</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berns</surname><given-names>GS</given-names></name><name><surname>Chappelow</surname><given-names>J</given-names></name><name><surname>Cekic</surname><given-names>M</given-names></name><name><surname>Zink</surname><given-names>CF</given-names></name><name><surname>Pagnoni</surname><given-names>G</given-names></name><name><surname>Martin-Skurski</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neurobiological substrates of dread</article-title><source>Science (New York, N.Y.)</source><volume>312</volume><fpage>754</fpage><lpage>758</lpage><pub-id pub-id-type="doi">10.1126/science.1123721</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bickel</surname><given-names>WK</given-names></name><name><surname>Odum</surname><given-names>AL</given-names></name><name><surname>Madden</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Impulsivity and cigarette smoking: Delay discounting in current, never, and ex-smokers</article-title><source>Psychopharmacology</source><volume>146</volume><fpage>447</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1007/PL00005490</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bickel</surname><given-names>WK</given-names></name><name><surname>Landes</surname><given-names>RD</given-names></name><name><surname>Christensen</surname><given-names>DR</given-names></name><name><surname>Jackson</surname><given-names>L</given-names></name><name><surname>Jones</surname><given-names>BA</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Single- and cross-commodity discounting among cocaine addicts: The commodity and its temporal location determine discounting rate</article-title><source>Psychopharmacology</source><volume>217</volume><fpage>177</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1007/s00213-011-2272-x</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchard</surname><given-names>TC</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Bromberg-Martin</surname><given-names>ES</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Orbitofrontal cortex uses distinct codes for different choice attributes in decisions motivated by curiosity</article-title><source>Neuron</source><volume>85</volume><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.050</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchard</surname><given-names>TC</given-names></name><name><surname>Strait</surname><given-names>CE</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Ramping ensemble activity in dorsal anterior cingulate neurons during persistent commitment to a decision</article-title><source>Journal of Neurophysiology</source><volume>114</volume><fpage>2439</fpage><lpage>2449</lpage><pub-id pub-id-type="doi">10.1152/jn.00711.2015</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brickman</surname><given-names>P</given-names></name><name><surname>Campbell</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Hedonic relativism and planning the good society</article-title><source>Adaptation-Level Theory</source><fpage>287</fpage><lpage>305</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brischoux</surname><given-names>F</given-names></name><name><surname>Chakraborty</surname><given-names>S</given-names></name><name><surname>Brierley</surname><given-names>DI</given-names></name><name><surname>Ungless</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Phasic excitation of dopamine neurons in ventral VTA by noxious stimuli</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>106</volume><fpage>4894</fpage><lpage>4899</lpage><pub-id pub-id-type="doi">10.1073/pnas.0811507106</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bromberg-Martin</surname><given-names>ES</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Midbrain dopamine neurons signal preference for advance information about upcoming rewards</article-title><source>Neuron</source><volume>63</volume><fpage>119</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.06.009</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bromberg-Martin</surname><given-names>ES</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Lateral habenula neurons signal errors in the prediction of reward information</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1209</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1038/nn.2902</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caplin</surname><given-names>A</given-names></name><name><surname>Leahy</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Psychological expected utility theory and anticipatory feelings</article-title><source>The Quarterly Journal of Economics</source><volume>116</volume><fpage>55</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1162/003355301556347</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crockett</surname><given-names>MJ</given-names></name><name><surname>Braams</surname><given-names>BR</given-names></name><name><surname>Clark</surname><given-names>L</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name><name><surname>Kalenscher</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Restricting temptations: Neural mechanisms of precommitment</article-title><source>Neuron</source><volume>79</volume><fpage>391</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.05.028</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>O'Doherty</surname><given-names>JP</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical substrates for exploratory decisions in humans</article-title><source>Nature</source><volume>441</volume><fpage>876</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1038/nature04766</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eliaz</surname><given-names>K</given-names></name><name><surname>Schotter</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Paying for confidence: An experimental study of the demand for non-instrumental information</article-title><source>Games and Economic Behavior</source><volume>70</volume><fpage>304</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/j.geb.2010.01.006</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiorillo</surname><given-names>CD</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Discrete coding of reward probability and uncertainty by dopamine neurons</article-title><source>Science</source><volume>299</volume><fpage>1898</fpage><lpage>1902</lpage><pub-id pub-id-type="doi">10.1126/science.1077349</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiorillo</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Transient activation of midbrain dopamine neurons by reward risk</article-title><source>Neuroscience</source><volume>197</volume><fpage>162</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2011.09.037</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiorillo</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Two dimensions of value: Dopamine neurons represent reward but not aversiveness</article-title><source>Science</source><volume>341</volume><fpage>546</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1126/science.1238699</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Frederick</surname><given-names>S</given-names></name><name><surname>Loewenstein</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1999">1999</year><chapter-title>Hedonic adaptation</chapter-title><person-group person-group-type="editor"><name><surname>Kahneman</surname> <given-names>D</given-names></name><name><surname>Diener</surname> <given-names>E</given-names></name><name><surname>Schwarz</surname> <given-names>N</given-names></name></person-group><source>Well-Being: The Foundations of Hedonic Psychology</source></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Schwartenbeck</surname><given-names>P</given-names></name><name><surname>Fitzgerald</surname><given-names>T</given-names></name><name><surname>Moutoussis</surname><given-names>M</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The anatomy of choice: Active inference and agency</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><fpage>598</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2013.00598</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Ognibene</surname><given-names>D</given-names></name><name><surname>Mathys</surname><given-names>C</given-names></name><name><surname>Fitzgerald</surname><given-names>T</given-names></name><name><surname>Pezzulo</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Active inference and epistemic value</article-title><source>Cognitive Neuroscience</source><volume>6</volume><fpage>187</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1080/17588928.2015.1020053</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibbon</surname><given-names>J</given-names></name><name><surname>Malapani</surname><given-names>C</given-names></name><name><surname>Dale</surname><given-names>CL</given-names></name><name><surname>Gallistel</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Toward a neurobiology of temporal cognition: Advances and challenges</article-title><source>Current Opinion in Neurobiology</source><volume>7</volume><fpage>170</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(97)80005-0</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gipson</surname><given-names>CD</given-names></name><name><surname>Alessandri</surname><given-names>JJ</given-names></name><name><surname>Miller</surname><given-names>HC</given-names></name><name><surname>Zentall</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Preference for 50% reinforcement over 75% reinforcement by pigeons</article-title><source>Learning &amp; Behavior</source><volume>37</volume><fpage>289</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.3758/LB.37.4.289</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottlieb</surname><given-names>J</given-names></name><name><surname>Oudeyer</surname><given-names>PY</given-names></name><name><surname>Lopes</surname><given-names>M</given-names></name><name><surname>Baranes</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Information-seeking, curiosity, and attention: Computational and neural mechanisms</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>585</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.09.001</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamid</surname><given-names>AA</given-names></name><name><surname>Pettibone</surname><given-names>JR</given-names></name><name><surname>Mabrouk</surname><given-names>OS</given-names></name><name><surname>Hetrick</surname><given-names>VL</given-names></name><name><surname>Schmidt</surname><given-names>R</given-names></name><name><surname>Vander Weele</surname><given-names>CM</given-names></name><name><surname>Kennedy</surname><given-names>RT</given-names></name><name><surname>Aragona</surname><given-names>BJ</given-names></name><name><surname>Berke</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mesolimbic dopamine signals the value of work</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>117</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/nn.4173</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hariri</surname><given-names>AR</given-names></name><name><surname>Brown</surname><given-names>SM</given-names></name><name><surname>Williamson</surname><given-names>DE</given-names></name><name><surname>Flory</surname><given-names>JD</given-names></name><name><surname>de Wit</surname><given-names>H</given-names></name><name><surname>Manuck</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Preference for immediate over delayed rewards is associated with magnitude of ventral striatal activity</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>13213</fpage><lpage>13217</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3446-06.2006</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>AS</given-names></name><name><surname>Rutledge</surname><given-names>RB</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Phillips</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phasic dopamine release in the rat nucleus accumbens symmetrically encodes a reward prediction error term</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>698</fpage><lpage>704</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2489-13.2014</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>AS</given-names></name><name><surname>Clark</surname><given-names>JJ</given-names></name><name><surname>Phillips</surname><given-names>PEM</given-names></name><name><surname>Hart</surname><given-names>AS</given-names></name><name><surname>Clark</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamic shaping of dopamine signals during probabilistic pavlovian conditioning</article-title><source>Neurobiology of Learning and Memory</source><volume>117</volume><fpage>84</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2014.07.010</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Platt</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Temporal discounting predicts risk sensitivity in rhesus macaques</article-title><source>Current Biology</source><volume>17</volume><fpage>49</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.10.055</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howe</surname><given-names>MW</given-names></name><name><surname>Tierney</surname><given-names>PL</given-names></name><name><surname>Sandberg</surname><given-names>SG</given-names></name><name><surname>Phillips</surname><given-names>PE</given-names></name><name><surname>Graybiel</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Prolonged dopamine signalling in striatum signals proximity and value of distant rewards</article-title><source>Nature</source><volume>500</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/nature12475</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname><given-names>QJ</given-names></name><name><surname>Cools</surname><given-names>R</given-names></name><name><surname>Gölzer</surname><given-names>M</given-names></name><name><surname>Friedel</surname><given-names>E</given-names></name><name><surname>Heinz</surname><given-names>A</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Disentangling the roles of approach, activation and valence in instrumental and pavlovian responding</article-title><source>PLoS Computational Biology</source><volume>7</volume><elocation-id>e1002028</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002028</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>J</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Crawley</surname><given-names>AP</given-names></name><name><surname>Mikulis</surname><given-names>DJ</given-names></name><name><surname>Remington</surname><given-names>G</given-names></name><name><surname>Kapur</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Direct activation of the ventral striatum in anticipation of aversive stimuli</article-title><source>Neuron</source><volume>40</volume><fpage>1251</fpage><lpage>1257</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00724-4</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An &quot;as soon as possible&quot; effect in human intertemporal decision making: Behavioral evidence and neural mechanisms</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>2513</fpage><lpage>2531</lpage><pub-id pub-id-type="doi">10.1152/jn.00177.2009</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kakade</surname><given-names>S</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Dopamine: Generalization and bonuses</article-title><source>Neural Networks : The Official Journal of the International Neural Network Society</source><volume>15</volume><fpage>549</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1016/s0893-6080(02)00048-5</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kidd</surname><given-names>C</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The psychology and neuroscience of curiosity</article-title><source>Neuron</source><volume>88</volume><fpage>449</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.010</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lammel</surname><given-names>S</given-names></name><name><surname>Lim</surname><given-names>BK</given-names></name><name><surname>Malenka</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reward and aversion in a heterogeneous midbrain dopamine system</article-title><source>Neuropharmacology</source><volume>76</volume><fpage>351</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1016/j.neuropharm.2013.03.019</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Curiosity and the pleasures of learning: Wanting and liking new information</article-title><source>Cognition &amp; Emotion</source><volume>19</volume><fpage>793</fpage><lpage>814</lpage><pub-id pub-id-type="doi">10.1080/02699930541000101</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lloyd</surname><given-names>K</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Tamping ramping: Algorithmic, implementational, and computational explanations of phasic dopamine signals in the accumbens</article-title><source>PLoS Computational Biology</source><volume>11</volume><elocation-id>e1004622</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004622</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewenstein</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>The psychology of curiosity: A review and reinterpretation</article-title><source>Psychological Bulletin</source><volume>116</volume><fpage>75</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.116.1.75</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewenstein</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Anticipation and the valuation of delayed consumption</article-title><source>The Economic Journal</source><volume>97</volume><fpage>666</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.2307/2232929</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewenstein</surname><given-names>GF</given-names></name><name><surname>Prelec</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Preferences for sequences of outcomes</article-title><source>Psychological Review</source><volume>100</volume><fpage>91</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.100.1.91</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Khaw</surname><given-names>MW</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Khaw</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Normalization is a general neural mechanism for context-dependent decision making</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>110</volume><fpage>6139</fpage><lpage>6144</lpage><pub-id pub-id-type="doi">10.1073/pnas.1217854110</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsumoto</surname><given-names>M</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Lateral habenula as a source of negative reward signals in dopamine neurons</article-title><source>Nature</source><volume>447</volume><fpage>1111</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1038/nature05860</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClure</surname><given-names>SM</given-names></name><name><surname>Berns</surname><given-names>GS</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Temporal prediction errors in a passive learning task activate human striatum</article-title><source>Neuron</source><volume>38</volume><fpage>339</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00154-5</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClure</surname><given-names>SM</given-names></name><name><surname>Ericson</surname><given-names>KM</given-names></name><name><surname>Laibson</surname><given-names>DI</given-names></name><name><surname>Loewenstein</surname><given-names>G</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Time discounting for primary rewards</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>5796</fpage><lpage>5804</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4246-06.2007</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCoy</surname><given-names>AN</given-names></name><name><surname>Platt</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Risk-sensitive neurons in macaque posterior cingulate cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1220</fpage><lpage>1227</lpage><pub-id pub-id-type="doi">10.1038/nn1523</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDevitt</surname><given-names>M</given-names></name><name><surname>Spetch</surname><given-names>M</given-names></name><name><surname>Dunn</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Contiguity and conditioned reinforcement in probabilistic choice</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>68</volume><fpage>317</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1901/jeab.1997.68-317</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDevitt</surname><given-names>MA</given-names></name><name><surname>Dunn</surname><given-names>RM</given-names></name><name><surname>Spetch</surname><given-names>ML</given-names></name><name><surname>Ludvig</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>When good news leads to bad choices</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>105</volume><fpage>23</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1002/jeab.192</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molet</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>HC</given-names></name><name><surname>Laude</surname><given-names>JR</given-names></name><name><surname>Kirk</surname><given-names>C</given-names></name><name><surname>Manning</surname><given-names>B</given-names></name><name><surname>Zentall</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decision making by humans in a behavioral task: Do humans, like pigeons, show suboptimal choice?</article-title><source>Learning &amp; Behavior</source><volume>40</volume><fpage>439</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.3758/s13420-012-0065-7</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monosov</surname><given-names>IE</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Selective and graded coding of reward uncertainty by neurons in the primate anterodorsal septal region</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>756</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1038/nn.3398</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monosov</surname><given-names>IE</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neurons in the primate medial basal forebrain signal combined information about reward uncertainty, value, and punishment anticipation</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>7443</fpage><lpage>7459</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0051-15.2015</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>G</given-names></name><name><surname>Arkadir</surname><given-names>D</given-names></name><name><surname>Nevet</surname><given-names>A</given-names></name><name><surname>Vaadia</surname><given-names>E</given-names></name><name><surname>Bergman</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Coincident but distinct messages of midbrain dopamine and striatal tonically active neurons</article-title><source>Neuron</source><volume>43</volume><fpage>133</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.06.012</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pisklak</surname><given-names>JM</given-names></name><name><surname>McDevitt</surname><given-names>MA</given-names></name><name><surname>Dunn</surname><given-names>RM</given-names></name><name><surname>Spetch</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>When good pigeons make bad decisions: Choice with probabilistic delays and outcomes</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>104</volume><fpage>241</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1002/jeab.177</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reuben</surname><given-names>E</given-names></name><name><surname>Sapienza</surname><given-names>P</given-names></name><name><surname>Zingales</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Time discounting for primary and monetary rewards</article-title><source>Economics Letters</source><volume>106</volume><fpage>125</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1016/j.econlet.2009.10.020</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roper</surname><given-names>KL</given-names></name><name><surname>Zentall</surname><given-names>TR</given-names></name><name><surname>T. R</surname></name><name><surname>Roper</surname><given-names>KL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Observing behavior in pigeons: The effect of reinforcement probability and response cost using a symmetrical choice procedure</article-title><source>Learning and Motivation</source><volume>30</volume><fpage>201</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1006/lmot.1999.1030</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutledge</surname><given-names>RB</given-names></name><name><surname>Skandali</surname><given-names>N</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A computational and neural model of momentary subjective well-being</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>111</volume><fpage>12252</fpage><lpage>12257</lpage><pub-id pub-id-type="doi">10.1073/pnas.1407535111</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutledge</surname><given-names>RB</given-names></name><name><surname>Skandali</surname><given-names>N</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dopaminergic modulation of decision making and subjective well-being</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>9811</fpage><lpage>9822</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0702-15.2015</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salimpoor</surname><given-names>VN</given-names></name><name><surname>Benovoy</surname><given-names>M</given-names></name><name><surname>Larcher</surname><given-names>K</given-names></name><name><surname>Dagher</surname><given-names>A</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Anatomically distinct dopamine release during anticipation and experience of peak emotion to music</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>257</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1038/nn.2726</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schweighofer</surname><given-names>N</given-names></name><name><surname>Shishida</surname><given-names>K</given-names></name><name><surname>Han</surname><given-names>CE</given-names></name><name><surname>Okamoto</surname><given-names>Y</given-names></name><name><surname>Tanaka</surname><given-names>SC</given-names></name><name><surname>Yamawaki</surname><given-names>S</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Humans can adopt optimal discounting strategy under real-time constraints</article-title><source>PLoS Computational Biology</source><volume>2</volume><elocation-id>e152</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0020152</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schweighofer</surname><given-names>N</given-names></name><name><surname>Bertin</surname><given-names>M</given-names></name><name><surname>Shishida</surname><given-names>K</given-names></name><name><surname>Okamoto</surname><given-names>Y</given-names></name><name><surname>Tanaka</surname><given-names>SC</given-names></name><name><surname>Yamawaki</surname><given-names>S</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Low-serotonin levels increase delayed reward discounting in humans</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>4528</fpage><lpage>4532</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4982-07.2008</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spetch</surname><given-names>ML</given-names></name><name><surname>Belke</surname><given-names>TW</given-names></name><name><surname>Barnet</surname><given-names>RC</given-names></name><name><surname>Dunn</surname><given-names>R</given-names></name><name><surname>Pierce</surname><given-names>WD</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Suboptimal choice in a percentage-reinforcement procedure: Effects of signal condition and terminal-link length</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>53</volume><fpage>219</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1901/jeab.1990.53-219</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stagner</surname><given-names>JP</given-names></name><name><surname>Zentall</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Suboptimal choice behavior by pigeons</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>17</volume><fpage>412</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.3758/PBR.17.3.412</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Story</surname><given-names>GW</given-names></name><name><surname>Vlaev</surname><given-names>I</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Winston</surname><given-names>JS</given-names></name><name><surname>Darzi</surname><given-names>A</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dread and the disvalue of future pain</article-title><source>PLoS Computational Biology</source><volume>9</volume><elocation-id>e1003335</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003335</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Fiorillo</surname><given-names>CD</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Adaptive coding of reward value by dopamine neurons</article-title><source>Science</source><volume>307</volume><fpage>1642</fpage><lpage>1645</lpage><pub-id pub-id-type="doi">10.1126/science.1105370</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>O'Doherty</surname><given-names>JP</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reward value coding distinct from risk attitude-related uncertainty coding in human reward systems</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>1621</fpage><lpage>1632</lpage><pub-id pub-id-type="doi">10.1152/jn.00745.2006</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasconcelos</surname><given-names>M</given-names></name><name><surname>Monteiro</surname><given-names>T</given-names></name><name><surname>Kacelnik</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Irrational choice and the value of information</article-title><source>Scientific Reports</source><volume>5</volume><fpage>13874</fpage><pub-id pub-id-type="doi">10.1038/srep13874</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zentall</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Resolving the paradox of suboptimal choice</article-title><source>Journal of Experimental Psychology. Animal Learning and Cognition</source><volume>42</volume><fpage>1</fpage><pub-id pub-id-type="doi">10.1037/xan0000085</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.13747.016</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Uchida</surname><given-names>Naoshige</given-names></name><role>Reviewing editor</role><aff id="aff5"><institution>Harvard University</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your work entitled &quot;The Modulation of Savouring by Prediction Error and its Effects on Choice&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by Naoshige Uchida as the Reviewing Editor and Jody Culham as the Senior Editor. One of the three reviewers has agreed to reveal his identity: Sam Gershman.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The authors present a model that is aimed to explain the following three behaviors:</p><p>1) &quot;Information-seeking&quot; behaviors and the observed neuronal activity in the lateral habenula in Bromberg-Martin and Hikosaka (2011).</p><p>2) The above &quot;information-seeking&quot; behavior becomes more prominent with longer delays to the extent that the animal chooses even the option that was associated with lower expected value (Spetch et al., 1990; Gipson et al., 2009).</p><p>3) Human data that the authors obtained (<xref ref-type="fig" rid="fig4">Figure 4A,B</xref>).</p><p>The model has two key components:</p><p>1) Anticipation: The integral of the instantaneous anticipation function which grows over time (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, ii).</p><p>2) Boosting of anticipation by reward prediction error (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, iv).</p><p>Expected value of a cue at the time of cue presentation is determined by the sum of the (conventional) discounted reward value and the anticipation which is also temporally discounted (i.e. the instantaneous anticipation at future moments is more strongly temporally-discounted, <xref ref-type="fig" rid="fig1">Figure 1A</xref>, iii). The model also makes other assumptions such as a linear relationship between the objective and subjective values of reward (which could affect risk preference of the model).</p><p>The authors conclude that their model can explain all the three data sets described above although the previous models were not able to explain these data. Although the question that the authors address is important, and the model is of potential interest, the reviewers have raised a number of concerns. Although the concerns regarding the authors' experiment are important points, the reviewers also noted that the main contribution of the present work could be the theory, and the modeling work can be of significance by itself if the authors perform more simulations and address the concerns described below. Therefore, the authors may choose to reduce the tone of the experimental part if it is difficult to address the concerns regarding the experiment.</p><p>Essential revisions:</p><p>1) It is unclear whether the data support the key prediction of a non-monotonic function of delay. In the legend of <xref ref-type="fig" rid="fig4">Figure 4B</xref>, the authors indicate that &quot;human subjects (n=14) showed a significant preference for the informative target for the case of long delays [20 sec: t(13) = 3.14, p=0.0078, 20 sec and 40 sec: t(27) = 4.00, p=0.00044]&quot;, and the three-star symbol (***) was placed between 20sec and 40sec delays. It is unclear whether these meant that the data supported one of the important predictions of the model (i.e. reversed preference with very long delays) or the comparison was between the short delays (2.5/7.5) versus pooled data for the two long delays (20/40). Please be more explicit. If the latter, the key prediction of a non-monotonic function of delay is not borne out by the data. This indicates that the model prediction is not supported by the data, at least for the range of delays tested.</p><p>2) The control experiment that had a different order of delays resulted in different results. This indicates that the result is sensitive to the ordering of trial types and might not be robust to experimental conditions.</p><p>3) Some of the comparisons involve the differing number of trials across conditions. The authors should correct this either by performing more experiments or by analyzing the data with a fixed number of trials.</p><p>4) The reviewers raised a number of concerns that the novelty over (or relationship with) previous studies is unclear. They pointed out specific literatures that the authors should refer to either basing on further simulations or with explicit discussions.</p><p>5) The model has several important assumptions, in particular, the two features described above (anticipation and boosting of anticipation by reward prediction errors). However, it is unclear what model features are essential in explaining the specific aspects of the data. What model features are important in explaining each of the three experiments? The authors should perform more explicit analyses addressing this question, for instance, by testing models with different components beyond that reported in <xref ref-type="fig" rid="fig4">Figure 4D</xref>. Also, the robustness of the chosen model parameters should be reported further.</p><p>6) Reviewer #3 raised the issue that the predicted behavior of the model is suboptimal. Other referees appreciate, however, that even without a clear normative basis, the proposed model can be informative as far as the model explains a range of behaviors. However, it is sometimes the case that the behavior is suboptimal in specific conditions, yet the model performs more advantageously in different (perhaps more natural) conditions. If so, such performance could provide a reason for a given model. Please discuss this issue further.</p><p>7) Reviewer #3 was concerned that the model assumes perfect timing, and that the model might not be able to explain the behavior if it had timing that was realistic with respect to temporal uncertainty.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;The Modulation of Savouring by Prediction Error and its Effects on Choice&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Jody Culham (Senior editor), a Reviewing editor, and three reviewers. The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>Two reviewers (1 and 2) supported publication of your work but the other reviewer is still concerned that the present manuscript emphasizes the non-monotonicity of choice preference over delay too strongly. For instance, in the fourth paragraph of the subsection “Testing the preference for advance information about upcoming rewards across delays in human subjects” it is emphasized that the model predicted the inverted U-shape despite the experimental result not showing a significant drop with a longer delay. Overall, it would be more appropriate to reduce the tone, and discuss this issue as a limitation of the present model. One reviewer (3) remains unsatisfied about two issues. During discussion, the other reviewers noted, however, that incorporating all the details sometimes loses the power of modeling, and in this case, these issues can be dealt with in future investigations.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.13747.017</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>Essential revisions:</p><p>1) It is unclear whether the data support the key prediction of a non-monotonic function of delay. In the legend of <xref ref-type="fig" rid="fig4">Figure 4B</xref>, the authors indicate that &quot;human subjects (n=14) showed a significant preference for the informative target for the case of long delays [20 sec: t(13) = 3.14, p=0.0078, 20 sec and 40 sec: t(27) = 4.00, p=0.00044]&quot;, and the three-star symbol (***) was placed between 20sec and 40sec delays. It is unclear whether these meant that the data supported one of the important predictions of the model (i.e. reversed preference with very long delays) or the comparison was between the short delays (2.5/7.5) versus pooled data for the two long delays (20/40). Please be more explicit. If the latter, the key prediction of a non-monotonic function of delay is not borne out by the data. This indicates that the model prediction is not supported by the data, at least for the range of delays tested.</p><p>One of the predictions of our model is indeed a non-monotonic value dependence on delay. This has been confirmed by hypothetical questionnaire studies (Lowenstein, 1983). However, because we followed a design used in recent primate studies (Bromberg-Martin and Hikosaka, 2009, 2011), the reward probabilities of both targets were designed to be the same. Since a standard reinforcement-learning model predicts no preference in this design, we would not necessarily predict a preference reversal.</p><p>Nevertheless, the non-monotonic value functions that we hypothesize would be expected to have two consequences in our task. One is a finite interval of delays for which participants prefer approximately equally between info and no-info targets. This arises from the relative balance of savouring and dread, each of which has its own temporal characteristic, and both of which are boosted by prediction errors. <xref ref-type="fig" rid="fig4">Figure 4C</xref> shows how the various non-monotonic contributions can cancel each other, in our case for short delay conditions (T&lt;10), the difference between the Q values of two targets were more similar to each other. This is due to the cancellation between the boosted savoring (the dotted-red line in the positive domain) and the boosted dread (the dotted-red line in the negative domain). Hence the choice probability should remain around 0.5. This is exactly what we found in our previous and new experiments (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig5">5D</xref>), with the significant preference for 100% info target emerging only at longer delays of 20 sec and 40 sec. The sudden increase of preference was indeed due to the non-monotonic (discounting) dread function. This is now discussed explicitly in the Results:</p><p>“More precisely, for delays wherein the impact of savoring and dread were similarly strong, choice preference remained at around the chance level. […] By contrast, at longer delay conditions (&gt; 20 sec), dread was discounted and savoring became dominant. This resulted in a large increase of preference for the 100% info target.”</p><p>Also, our additional model comparison analysis confirmed that the model with non-monotonic value functions (our original model with temporal discounting) outperformed the model with monotonic value functions (our model without temporal discounting). This is now stated:</p><p>“We stress that this sudden increase in choice preference was caused by the non-monotonic Q-value functions of targets (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Our model comparison analysis also supported this conclusion (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), where the model with non-monotonic value functions (our original model with temporal discounting) outperformed the model with monotonic value functions (our model without temporal discounting).”</p><p>The second consequence of non-monotonicity is that, as pointed out by the reviewers, temporal discounting of savoring at extremely long delays should also make subjects become indifferent between info and non-info targets. Unfortunately, analysis of our model suggests that the delays concerned would be more than a few minutes – which was too long to be practicable in our design. Thus, we were not able to confirm this indifference in our task, and leave this for future studies. We now discuss this explicitly in the Results:</p><p>“Note that temporal discounting of savoring at extremely long delays should also make subjects indifferent between 100% and 0% info targets. […] Nonetheless, we stress that our model with temporal discounting outperformed a model without temporal discounting in terms of the iBIC scores (<xref ref-type="fig" rid="fig4">Figure 4D</xref>).”</p><p>We apologize for the confusing comparisons in <xref ref-type="fig" rid="fig4">Figure 4</xref>, and have removed them. We now stress the significant preference of 100% info target at delay = 20 sec and 40 sec.</p><p> <italic>2) The control experiment that had a different order of delays resulted in different results. This indicates that the result is sensitive to the ordering of trial types and might not be robust to experimental conditions.</italic> We ran a new experiment in which the orders of the delays were randomized, and show this factor had no effect. The difference may arise out of the fact that for the control experiment in our original study participants were over-exposed to the same delay condition (&gt;100 trials with 2.5 sec) before an abrupt change in delay (from 2.5 sec to 40 sec), while in our new experiment delay was randomized across trials and explicitly instructed by cues on each trial. We have added a comment about this to the Discussion:</p><p>“One issue that merits further future study is adaptation to different delays. […] Furthermore, if the prediction error could influence subjective time (for instance via a known effect of dopamine on aspects of timing (Gibbon et al. 1997)), then this could have complex additional effects on anticipation.”</p><p><italic>3) Some of the comparisons involve the differing number of trials across conditions. The authors should correct this either by performing more experiments or by analyzing the data with a fixed number of trials.</italic> First, the new task involved the same number of trials in each condition, and again replicated the preference of 100% info target. Note that the original task was designed to equalize the amount of time in each condition – again with the same result.</p><p>Second, as suggested, we randomly sub-sampled the trials in our original experiment to equalize the number used per conditioning in calculating the statistics. This again confirmed our original result.</p><p>We now highlight both these facts in the Results and Methods:</p><p>“Also, Experiment-2 was designed to have an equal number of trials per delay condition, while Experiment-1 was designed to equalize the amount of time that participants spent in each condition (see Task Procedures in Materials and methods). The fact that we obtained the same results in both experiments illustrates the robustness of our findings.”</p><p>“Also, in order to treat different delay conditions equally, we randomly sub-sampled the trials to equalize the number used per condition in order to calculate the statistics. Additionally we obtained the same results by normalizing the posterior for each delay condition when estimating the expectation.”</p><p><italic>4) The reviewers raised a number of concerns that the novelty over (or relationship with) previous studies is unclear. They pointed out specific literatures that the authors should refer to either basing on further simulations or with explicit discussions.</italic> First, we have attempted to make clearer the novelty of our suggestion – boosted anticipation and dread had never previously been considered – and also the relationship between our work and others. This led to changes in the Introduction and Discussion.</p><p>As suggested, we also applied our model to the task introduced by Zentall and colleagues involving comparison between 20% and 50% chance of rewards. Again, we could account for the reported suboptimal behavior (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). We have also provided with a general formula in the Methods section that can be applied to a wide range of experiments.</p><p>5) The model has several important assumptions, in particular, the two features described above (anticipation and boosting of anticipation by reward prediction errors). However, it is unclear what model features are essential in explaining the specific aspects of the data. What model features are important in explaining each of the three experiments? The authors should perform more explicit analyses addressing this question, for instance, by testing models with different components beyond that reported in <xref ref-type="fig" rid="fig4">Figure 4D</xref>. Also, the robustness of the chosen model parameters should be reported further. We apologize that this was not clear in our previous version and we agree it is an absolutely critical point.</p><p>The assumption that is essential is that of boosting. Anticipation itself does not account for an advance information preference, as the advance information is irrelevant to the original formulation of anticipation of rewards (Loewenstain, 1987).</p><p>We conducted additional analysis of our model without boosting and showed how it behaves. In our behavioral task, and in the Bromberg-Martin task, the model predicts no preference without boosting (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>); and a model restricted in this way fits our experimental data extremely poorly (76 iBIC units less than the model with boosting; <xref ref-type="fig" rid="fig4">Figure 4D</xref>). In an experimental design from other labs in which behavior is known to be <italic>suboptimal</italic>, again, assuming no boosting leads, incorrectly, to <italic>optimal</italic> choice (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p><p><italic>6) Reviewer #3 raised the issue that the predicted behavior of the model is suboptimal. Other referees appreciate, however, that even without a clear normative basis, the proposed model can be informative as far as the model explains a range of behaviors. However, it is sometimes the case that the behavior is suboptimal in specific conditions, yet the model performs more advantageously in different (perhaps more natural) conditions. If so, such performance could provide a reason for a given model. Please discuss this issue further.</italic> The observed preference for information is so strikingly suboptimal in a wide range of experiments, that it is hard to generate a plausibly normative explanation. Prediction errors have various untoward effects (for instance on happiness; Rutledge et al. 2014, 2015), leading to other non-normative consequences such as the hedonic treadmill. We now discuss non-normativity in more detail in Discussion:</p><p>“RPE-boosted anticipation, and like many apparently Pavlovian behaviors that involve innate responses, this appears evidently suboptimal – indeed as seen in strikingly non-normative choices. […] However, whether the behaviors reflect mechanistic constraints on neural computation (Kakade and Dayan, 2002), or a suitable adaptation to typical evolutionary environments, remains a question for further research.”</p><p><italic>7) Reviewer #3 was concerned that the model assumes perfect timing, and that the model might not be able to explain the behavior if it had timing that was realistic with respect to temporal uncertainty.</italic>In all experiments that we analyzed in our paper, the delay was balanced between two targets. Thus the effect of timing uncertainty should be the same for both targets. Hence, we expect no effect on choice. We note this in the Discussion:</p><p>“In our task there was no effect of timing uncertainty on choice, as both targets are associated with the same delay. However it would become important if a task involves a choice between targets with different delay conditions. Furthermore, if the prediction error could influence subjective time (for instance via the known effect of dopamine on aspects of timing (Gibbon et al., 1997)), then this could have complex additional effects on anticipation.”</p><p>The reason that we assumed no dread in animal experiments is (as stated in Results) that:</p><p>“In these calculations, we set the value of no outcome to zero, implying a lack of dread in the no outcome condition. […] Moreover changing the delay between choice and cues that signaled no-reward had no impact on preference (McDevitt et al., 2016) Note, however, that our results still held in case of adding a finite value to the no outcome and this is something that we indeed found in our experiment, as detailed in the sections that follow.”</p><p>By contrast, we found that human participants assigned a negative value to no-outcome (Tobler et al., 2007), and therefore experienced dread. In fact, dread played a very important role in our task. We discuss this extensively for example in Results:</p><p>“In fitting the model, we were surprised to find that subjects assigned negative values to no-reward outcomes and its predicting cue (the bottom dotted line in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, see also <xref ref-type="table" rid="tbl2">Table 2</xref>) for the estimated model parameters). […] Omitting the negative value of the no-reward cue led to a failure to fit this effect (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).”</p><p>Also in Discussion:</p><p>“In our human experiments, we found that participants assigned a negative value to a no-outcome. […] Note that in the monkey experiments (Bromberg-Martin and Hikosaka, 2009, 2011), the lower-value outcome still involved an actual reward, albeit of a smaller size.”</p><p>The amount of dread was also boosted by reward prediction errors, hence the weighting function of anticipation (Eq.1) had to depend on an absolute value of prediction error.</p><p>With reference to reviewer #3’s concern about ‘absolute value’: we were using this in its purely mathematical form; that is the Euclidian distance from <inline-formula><mml:math id="inf328"><mml:mn>0</mml:mn></mml:math></inline-formula>. For example, “the absolute value of <inline-formula><mml:math id="inf329"><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="inf330"><mml:mn>5</mml:mn></mml:math></inline-formula>”.</p><p>It is indeed a very good point that savoring and dread may not operate in the same dimension – we now refer to the important recent paper Fiorillo (2013) that considers this issue:</p><p>“It would be also interesting to study the difference between savouring and dread, as evidence shows that reward and punishment are not encoded in the same dimension (Fiorillo, 2013)”.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p><italic>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below: Two reviewers (1 and 2) supported publication of your work but the other reviewer is still concerned that the present manuscript emphasizes the non-monotonicity of choice preference over delay too strongly. For instance, in the fourth paragraph of the subsection “Testing the preference for advance information about upcoming rewards across delays in human subjects” it is emphasized that the model predicted the inverted U-shape despite the experimental result not showing a significant drop with a longer delay. Overall, it would be more appropriate to reduce the tone, and discuss this issue as a limitation of the present model. One reviewer (3) remains unsatisfied about two issues. During discussion, the other reviewers noted, however, that incorporating all the details sometimes loses the power of modeling, and in this case, these issues can be dealt with in future investigations.</italic> Following your instructions, we made the suggested modest revisions.</p><p>In summary, we agree with your two major points. First, we appreciate Reviewer #1’s observation that we emphasized the non-monotonicity of choice preference over delay too strongly. We have implemented the suggestion to tone down this part of our discussion and claims, and indeed now cite it as a limitation of our current study.</p><p>Second, we agree with Reviewers #1 and #2’s suggestion that the issues raised by Reviewer #3’s can be most effectively addressed in future investigations. In fact, we think that some of Reviewer #3’s concerns are driven by a slight misunderstanding of our paper. This is because the majority of the issues that reviewer #3 raised in the review had in fact been addressed in our previous revision. We have therefore edited our manuscript to avoid any misreading.</p></body></sub-article></article>