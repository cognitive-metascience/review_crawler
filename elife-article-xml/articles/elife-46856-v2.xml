<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">46856</article-id><article-id pub-id-type="doi">10.7554/eLife.46856</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Resolving multisensory and attentional influences across cortical depth in sensory cortices</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-134585"><name><surname>Gau</surname><given-names>Remi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1535-9767</contrib-id><email>remi_gau@hotmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-134586"><name><surname>Bazin</surname><given-names>Pierre-Louis</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0141-5510</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131273"><name><surname>Trampel</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-91970"><name><surname>Turner</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-168415"><name><surname>Noppeney</surname><given-names>Uta</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Computational Neuroscience and Cognitive Robotics Centre</institution><institution>University of Birmingham</institution><addr-line><named-content content-type="city">Birmingham</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Institute of Psychology</institution><institution>Université Catholique de Louvain</institution><addr-line><named-content content-type="city">Louvain-la-Neuve</named-content></addr-line><country>Belgium</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Institute of Neuroscience</institution><institution>Université Catholique de Louvain</institution><addr-line><named-content content-type="city">Louvain-la-Neuve</named-content></addr-line><country>Belgium</country></aff><aff id="aff4"><label>4</label><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Integrative Model-based Cognitive Neuroscience research unit</institution><institution>University of Amsterdam</institution><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Sir Peter Mansfield Imaging Centre</institution><institution>University of Nottingham</institution><addr-line><named-content content-type="city">Nottingham</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff7"><label>7</label><institution content-type="dept">Donders Institute for Brain, Cognition and Behaviour</institution><institution>Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Reviewing Editor</role><aff><institution>Radboud University</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Büchel</surname><given-names>Christian</given-names></name><role>Senior Editor</role><aff><institution>University Medical Center Hamburg-Eppendorf</institution><country>Germany</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>08</day><month>01</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e46856</elocation-id><history><date date-type="received" iso-8601-date="2019-03-14"><day>14</day><month>03</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-01-07"><day>07</day><month>01</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Gau et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Gau et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-46856-v2.pdf"/><abstract><p>In our environment, our senses are bombarded with a myriad of signals, only a subset of which is relevant for our goals. Using sub-millimeter-resolution fMRI at 7T, we resolved BOLD-response and activation patterns across cortical depth in early sensory cortices to auditory, visual and audiovisual stimuli under auditory or visual attention. In visual cortices, auditory stimulation induced widespread inhibition irrespective of attention, whereas auditory relative to visual attention suppressed mainly central visual field representations. In auditory cortices, visual stimulation suppressed activations, but amplified responses to concurrent auditory stimuli, in a patchy topography. Critically, multisensory interactions in auditory cortices were stronger in deeper laminae, while attentional influences were greatest at the surface. These distinct depth-dependent profiles suggest that multisensory and attentional mechanisms regulate sensory processing via partly distinct circuitries. Our findings are crucial for understanding how the brain regulates information flow across senses to interact with our complex multisensory world.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>multisensory integration</kwd><kwd>audiovisual</kwd><kwd>primary sensory cortices</kwd><kwd>deactivations,</kwd><kwd>crossmodal attention</kwd><kwd>cortical layers</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>Mult-sens</award-id><principal-award-recipient><name><surname>Noppeney</surname><given-names>Uta</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max Planck Society</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Turner</surname><given-names>Robert</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The different laminar profiles observed across the cortical depth for multisensory and attentional influences indicate partly distinct neural circuitries of information-flow control.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In our natural environment, our senses are exposed to a constant influx of sensory signals that arise from many different sources. How the brain flexibly regulates information flow across the senses to enable effective interactions with the world remains unclear.</p><p>Mounting evidence from neuroimaging (<xref ref-type="bibr" rid="bib3">Beauchamp et al., 2004</xref>; <xref ref-type="bibr" rid="bib63">Noesselt et al., 2007</xref>; <xref ref-type="bibr" rid="bib67">Rohe and Noppeney, 2016</xref>), neurophysiology (<xref ref-type="bibr" rid="bib1">Atilgan et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Kayser et al., 2010</xref>; <xref ref-type="bibr" rid="bib35">Kayser et al., 2008</xref>; <xref ref-type="bibr" rid="bib39">Lakatos et al., 2007</xref>) and neuroanatomy (<xref ref-type="bibr" rid="bib20">Falchier et al., 2002</xref>; <xref ref-type="bibr" rid="bib65">Rockland and Ojima, 2003</xref>) suggests that interactions across the senses are pervasive in neocortex, arising even in primary cortices (<xref ref-type="bibr" rid="bib16">Driver and Noesselt, 2008</xref>; <xref ref-type="bibr" rid="bib26">Ghazanfar and Schroeder, 2006</xref>; <xref ref-type="bibr" rid="bib46">Liang et al., 2013</xref>; <xref ref-type="bibr" rid="bib70">Schroeder and Foxe, 2002</xref>). Visual stimuli can directly drive as well as modulate responses in cortices that are dedicated to other sensory modalities. Most prominently, functional magnetic resonance imaging (fMRI) in humans has shown that visual stimuli can induce crossmodal deactivations in primary and secondary auditory cortices (<xref ref-type="bibr" rid="bib42">Laurienti et al., 2002</xref>; <xref ref-type="bibr" rid="bib43">Leitão et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Mozolic et al., 2008</xref>), yet enhance the response to a concurrent auditory stimulus (<xref ref-type="bibr" rid="bib90">Werner and Noppeney, 2011</xref>; <xref ref-type="bibr" rid="bib89">Werner and Noppeney, 2010</xref>). Further, neurophysiological research has suggested that multisensory influences emerge early in sensory cortices and are to some extent preserved in anaesthetized animals (<xref ref-type="bibr" rid="bib7">Butler et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Iurilli et al., 2012</xref>; <xref ref-type="bibr" rid="bib34">Kayser et al., 2007</xref>; <xref ref-type="bibr" rid="bib56">Mercier et al., 2013</xref>). Yet, the ability to extrapolate from neurophysiological findings in animals to human fMRI studies is limited by the nature of the BOLD response, which pools neural activity over time and across a vast number of neurons (<xref ref-type="bibr" rid="bib47">Logothetis, 2008</xref>).</p><p>Information flow is regulated not only by multisensory but also by attentional mechanisms that are guided by our current goals (<xref ref-type="bibr" rid="bib19">Fairhall and Macaluso, 2009</xref>; <xref ref-type="bibr" rid="bib78">Talsma et al., 2010</xref>). Critically, multisensory and attentional mechanisms are closely intertwined. Both enhance perceptual sensitivity (<xref ref-type="bibr" rid="bib44">Leo et al., 2011</xref>) and precision of sensory representations (<xref ref-type="bibr" rid="bib18">Ernst and Bülthoff, 2004</xref>; <xref ref-type="bibr" rid="bib22">Fetsch et al., 2012</xref>; <xref ref-type="bibr" rid="bib55">Meijer et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Rohe and Noppeney, 2015</xref>). Most importantly, the co-occurrence of two congruent sensory stimuli boosts the salience of an event (<xref ref-type="bibr" rid="bib45">Lewis and Noppeney, 2010</xref>; <xref ref-type="bibr" rid="bib84">Van der Burg et al., 2008</xref>), which may thereby attract greater attention. Conversely, a stimulus presented in one sensory modality alone may withdraw attentional resources from other sensory modalities. Behavioural and functional imaging studies have shown that shifting attention endogenously to one sensory modality reduces processing and activations in the unattended sensory systems (<xref ref-type="bibr" rid="bib11">Ciaramitaro et al., 2007</xref>; <xref ref-type="bibr" rid="bib33">Johnson and Zatorre, 2005</xref>; <xref ref-type="bibr" rid="bib58">Mozolic et al., 2008</xref>; <xref ref-type="bibr" rid="bib68">Rohe and Noppeney, 2018</xref>; <xref ref-type="bibr" rid="bib66">Rohe and Noppeney, 2015</xref>; <xref ref-type="bibr" rid="bib72">Shomstein and Yantis, 2004</xref>). As a consequence, attentional mechanisms may contribute to competitive and cooperative interactions across the senses, for instance by amplifying responses for congruent audiovisual stimuli, and generating crossmodal deactivations for unisensory stimuli. Inter-sensory attention can also profoundly modulate multisensory interactions (<xref ref-type="bibr" rid="bib77">Talsma et al., 2007</xref>). Most prominently, the influence of visual stimuli on auditory cortices was shown to be enhanced when attention was focused on the visual sense (<xref ref-type="bibr" rid="bib40">Lakatos et al., 2009</xref>).</p><p>While previous neurophysiological studies have revealed influences of modality-specific attention predominantly in superficial laminae in non-human primates (<xref ref-type="bibr" rid="bib40">Lakatos et al., 2009</xref>), visual influences on auditory cortices have recently been shown to be most prominent in deep layer 6 of auditory cortex in rodents (<xref ref-type="bibr" rid="bib57">Morrill and Hasenstaub, 2018</xref>). In other words, combined neurophysiological evidence from primates and rodents suggests a double disassociation of attentional and multisensory influences.</p><p>To investigate whether this double dissociation can be found in human neocortex, we exploited recent advances in submillimeter-resolution fMRI at 7T that allow the characterization of depth-dependent activation profiles (<xref ref-type="bibr" rid="bib12">De Martino et al., 2015a</xref>; <xref ref-type="bibr" rid="bib17">Duong et al., 2003</xref>; <xref ref-type="bibr" rid="bib29">Harel et al., 2006</xref>; <xref ref-type="bibr" rid="bib37">Kok et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Koopmans et al., 2010</xref>; <xref ref-type="bibr" rid="bib59">Muckli et al., 2015</xref>; <xref ref-type="bibr" rid="bib64">Polimeni et al., 2010</xref>; <xref ref-type="bibr" rid="bib81">Trampel et al., 2012</xref>). While gradient-echo echo-planar-imaging (GE-EPI) BOLD fMRI is not yet able to attribute activations unequivocally to specific cortical layers (<xref ref-type="bibr" rid="bib17">Duong et al., 2003</xref>; <xref ref-type="bibr" rid="bib27">Goense et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Harel et al., 2006</xref>; <xref ref-type="bibr" rid="bib30">Huber et al., 2017</xref>; <xref ref-type="bibr" rid="bib51">Markuerkiaga et al., 2016</xref>; <xref ref-type="bibr" rid="bib82">Trampel et al., 2019</xref>), the observation in the same cortical territories of distinct laminar activation profiles induced by multisensory and attentional influences would strongly imply distinct neural mechanisms.</p><p>The current study investigated the processing of auditory, visual or audiovisual looming stimuli under auditory and visual attention in the human brain. Combining submillimeter-resolution fMRI at 7T with laminar and multivariate pattern analyses, we show distinct depth-dependent activation profiles and/or patterns for multisensory and attentional influences in early auditory and visual cortices. These results suggest that multisensory and attentional mechanisms regulate sensory processing in early sensory cortices via partly distinct neural circuitries.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In this fMRI study, participants were presented with blocks of auditory (A), visual (V) and audiovisual (AV) looming stimuli interleaved with fixation (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We used looming motion as a biologically relevant and highly salient stimulus that reliably evokes crossmodal influences in sensory cortices in human neuroimaging and animal neurophysiology (<xref ref-type="bibr" rid="bib9">Cappe et al., 2012</xref>; <xref ref-type="bibr" rid="bib50">Maier et al., 2008</xref>; <xref ref-type="bibr" rid="bib83">Tyll et al., 2013</xref>). Modality-specific attention was manipulated by requiring participants to detect and respond selectively to weak auditory or visual targets, which were adjusted prior to the main study to threshold performance in sound amplitude or visual size for each participant. The targets were interspersed throughout all auditory, visual and audiovisual looming blocks (e.g. visual targets were presented during both visual and auditory looming blocks; see <xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design, timeline and cortical layering.</title><p>(<bold>A</bold>) Experimental design: Participants were presented with auditory, visual and audiovisual looming stimuli under auditory and visual attention. (<bold>B</bold>) Example trial and timeline: Participants were presented with brief auditory, visual or audiovisual looming stimuli in 33 s blocks interleaved with 16 s fixation. At the beginning of each block, a cue indicated whether the auditory or visual modality needed to be attended. Brief visual and auditory targets (grey) were interspersed in the looming activation blocks. Participants were instructed to respond to the targets in the attended and ignore the targets in the unattended sensory modality. (<bold>C</bold>) Cortical layering: Left: A parasagittal section of a high resolution T1 map is shown with a colour coded laminar label for each voxel (voxel size: (0.4 mm)<sup>3</sup>). The primary auditory cortex is circled in yellow. Right: The cortical sheet is defined by the pial and white matter surfaces (thick black solid lines). Six additional surfaces (thin black solid lines) were determined at different cortical depths. Data were mapped onto those surfaces by sampling (blue dots) radially along the normal (white dashed line) to the mid-cortical depth surface (not shown here). WM: white matter, GM: grey matter, CSF: cerebrospinal fluid.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Behavioural results.</title><p>Percentage of responses to auditory (red) and visual (blue) targets in auditory, visual and audiovisual blocks (along the x-axis) and under auditory (top panel) and visual (bottom panel) attention. Thick lines represent group means + /- SEM and each thin line represents a participant. Responses to visual targets under auditory attention and responses to auditory targets under visual attention are false alarms (and vice versa for visual attention).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Segmentation and coregistration.</title><p>Sagittal section of T1 structural images (left), the corresponding coregistered mean EPI images of four representative participants. The definition of the six laminae is overlaid in A1.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig1-figsupp2-v2.tif"/></fig></fig-group><p>For the behavioural analysis, the percentages of visual and auditory targets that gained a response (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for ‘% target responses’) were entered into a 2 (target modality: auditory vs. visual) X 2 (attended modality: auditory vs. visual) X 3 (stimulus block modality: auditory, visual, audio-visual) repeated measures ANOVA. We observed significant main effects of attended modality (F(1, 10)=291.263, p&lt;0.001, ηp<sup>2</sup> = 0.967), target modality (F(1, 10)=13.289, p=0.004, ηp<sup>2</sup> = 0.571), and stimulus modality (F(1.986, 19.864)=5.304, p=0.014, ηp<sup>2</sup> = 0.347), together with a significant interaction between target modality and attended modality (F(1, 10)=14.786, p=0.003, ηp<sup>2</sup> = 0.597). This interaction confirmed that participants had successfully maintained auditory or visual attention as instructed.</p><p>Further, we observed significant interactions between target modality and stimulus block modality (F(1.813, 18.126)=8.149, p=0.004, ηp<sup>2</sup> = 0.449) and between attended modality and stimulus block modality (F(1.436, 14.360)=24.034, p&lt;0.001, ηp<sup>2</sup> = 0.706). These interactions resulted from greater hit rates for auditory targets given auditory attention than for visual targets given visual attention during both auditory (t(10)=2.845, p=0.017, Hedges g<italic><sub>av</sub></italic> = 0.804) and visual blocks (t(10)=4.432, p=0.001, Hedges g<italic><sub>av</sub></italic> = 2.037), but not for audio-visual blocks (t(10)=0.276, p=0.788, Hedges g<italic><sub>av</sub></italic> = 0.081) suggesting that observers’ performance was not completely matched across all conditions. Specifically, the presentation of auditory and visual stimuli in the audiovisual blocks interfered with the detection of auditory targets. For completeness, there was no significant three-way interaction (F(1.783, 17.826)=2.467, p=0.118, ηp<sup>2</sup> = 0.198).</p><p>Using sub-millimeter resolution fMRI, we characterized the laminar profiles in auditory (primary auditory cortex, A1; planum temporale, PT) and visual (primary, V1; higher order, V2/3) regions for the following effects: 1. sensory deactivations in unisensory contexts for non-preferred stimuli (i.e. crossmodal, e.g. [V-Fix] in auditory cortices), 2. crossmodal modulation (e.g. [AV-A] in auditory cortices, [AV-V] in visual cortices) in audiovisual context and 3. direct and modulatory effects of modality-specific attention.</p><p>Briefly, we used the following methodological approach (see Material and methods): in each ROI and participant, we estimated the regional BOLD response (i.e. ‘B parameter estimate’) (e.g. <xref ref-type="fig" rid="fig2">Figure 2A</xref> row 1, left) and the multivariate pattern decoding accuracy for each of the six laminae (e.g. <xref ref-type="fig" rid="fig3">Figure 3A</xref> row 1, right). We then characterized the laminar profiles of BOLD response and decoding accuracy in terms of a constant and a linear shape parameter (i.e. ‘S parameter estimates’) and show their across-subjects’ mean and distribution in violin plots (e.g. <xref ref-type="fig" rid="fig2">Figure 2A</xref> row 2). Finally, we characterized the spatial topography of those S parameter estimates by projecting their group mean onto a normalized group cortical surface (e.g. <xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Auditory and visual deactivations.</title><p>(<bold>A</bold>) BOLD response profiles: Rows 1 and 3: The BOLD response (i.e. B parameters, across subjects’ mean ± SEM) for visual and auditory looming stimuli averaged over auditory and visual attention in A1, PT, V1, V2/3 is shown as a function of percentage cortical depth. WM: white matter; GM: grey matter; CSF: cerebrospinal fluid. Percentage cortical depth is indicated by the small numbers and colour coded in red. Rows 2 and 4: Across subjects’ mean (± SEM) and violin plot of the participants’ shape parameter estimates that characterize the mean (C: constant) and linear increase (L: linear) of the laminar BOLD response profile. n = 11. (<bold>B</bold>) Surface projections: Across subject’ mean of the ‘constant’ (row i) and ‘linear’ increase (row ii) shape parameter estimates of the laminar BOLD response profile for auditory and visual looming stimuli (averaged over auditory and visual attention) are projected on an inflated group mean surface to show auditory and visual regions of the left hemisphere. A1 and V1 are delineated by black solid lines, PT and V2/3 by dashed lines. For visualization purposes only: i. surface projections were smoothed (FWHM = 1.5 mm); ii. values are also presented for vertices for which data were not available from all subjects and which were therefore not included in our formal statistical analysis. Grey areas denote vertices with no available data for any subject.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Auditory and visual activations.</title><p>(<bold>A</bold>) BOLD response profiles: Row 1 and 3: The BOLD response (i.e. B parameters, across subjects’ mean ± SEM) for auditory and visual activations induced by looming stimuli averaged over auditory and visual attention in A1, PT, V1, V2/3 is shown as a function of percentage cortical depth. WM: white matter; GM: grey matter; CSF: cerebrospinal fluid. Percentage cortical depth is indicated by the small numbers and colour coded in red. Rows 2 and 4: Across subjects’ mean (± SEM) and violin plot of the participants’ shape parameter estimates that characterize the mean (C: constant) and linear increase (L: linear) of the laminar BOLD response profile. n = 11. (<bold>B</bold>) Surface projections: Across subject’ mean of the ‘constant’ (row i) and ‘linear’ increase (row ii) shape parameter estimates of the laminar BOLD response profile for auditory and visual looming stimuli (averaged over auditory and visual attention) are projected on an inflated group mean surface to show auditory and visual regions of the left hemisphere. A1 and V1 are delineated by black solid lines, PT and V2/3 by dashed lines. For visualization purposes only: i. borders between visual-induced activations and deactivations (white dashed lines) were defined based on visual inspection; ii. surface projections were smoothed (FWHM = 1.5 mm); iii. values are also presented for vertices for which data were not available from all subjects and which were therefore not included in our formal statistical analysis. Grey areas denote vertices with no available data for any subject.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Auditory and visual responses in visual areas.</title><p>Surface projection for individual subjects: Within subject ‘constant’ shape parameter estimates of the laminar BOLD response profile for visual (column i and ii) and auditory (column iii and iv) stimuli are projected on each subject’s inflated surface to show visual regions of the left (column i and iii) and right (column ii and iv) hemisphere. V1 and V2/3 are delineated by black solid and dashed lines respectively. For visualization purposes only, surface projections were smoothed (FWHM = 1.5 mm). Grey areas denote vertices with no available data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Auditory and visual responses in auditory areas.</title><p>Surface projection for individual subjects: Within subject ‘constant’ shape parameter estimates of the laminar BOLD response profile for visual (column i and ii) and auditory (column iii and iv) are projected on each subject’s inflated surface to show visual regions of the left (column i and iii) and right (column ii and iv) hemisphere. A1 and PT are delineated by black solid and dashed lines, respectively. For visualization purposes only, surface projections were smoothed (FWHM = 1.5 mm). Grey areas denote vertices with no available data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig2-figsupp3-v2.tif"/></fig></fig-group><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Cross-modal modulation in auditory areas.</title><p>(<bold>A</bold>) Laminar profiles: Rows 1 and 3: The BOLD response (solid line; column 1 and 3) and decoding accuracy (dashed line; columns 2 and 4) (across subjects’ mean ± SEM) for [AV-A] in A1 and PT is shown as a function of percentage cortical depth pooled (i.e. averaged) over auditory and visual attention. WM: white matter, GM: grey matter, CSF: cerebrospinal fluid. Percentage cortical depth is indicated by the small numbers and colour coded in red. Rows 2 and 4: Across subjects’ mean (± SEM) and violin plot of participants’ shape parameter estimates that characterize the mean (C: constant) and linear increase (L: linear) of the laminar BOLD response and decoding accuracy profiles. n = 11. (<bold>B</bold>) Surface projections and raster plots: <bold>Left</bold>: Across subject’ mean of the ‘constant’ (row i) and ‘linear increase’ (row ii) shape parameter estimates of the laminar BOLD response profile for [AV-A] (averaged over auditory and visual attention) are projected on an inflated group mean surface to show auditory regions of the left hemisphere. A1 and PT are delineated by black solid and dashed lines. For visualization purposes only: i. surface projections were smoothed (FWHM = 1.5 mm); ii. values are also presented for vertices for which data were not available from all subjects and which were therefore not included in our formal statistical analysis. Grey areas denote vertices with no available data for any subject. Right: Row i. PT: The raster plot illustrates the statistical relationship between the ‘constant’ shape parameters for the visual evoked response [V-Fix]<sub>AttA, AttV</sub> and the crossmodal modulation [AV-A]<sub>AttA, AttV</sub> in PT. Each raster plot shows the laminar profiles (colour coded along abscissa) of the vertices for the ‘predicting contrast’ [V-Fix] and of the ‘predicted contrast’ [AV-A]. The laminar profiles of the vertices were sorted along the ordinate according to the value of the ‘constant’ shape parameter for [V-Fix]. The raster plot shows that the laminar profile of a vertex for [V-Fix] can predict its laminar profile for [AV-A]: PT vertices with less deactivations across laminae for [V-Fix] are associated with greater crossmodal enhancement [AV-A]. Row ii. The raster plot illustrates the statistical relationship between the ‘linear slope’ shape parameters for the visual evoked response [V-Fix]<sub>AttA, AttV</sub> and the crossmodal modulation [AV-A] in A1. Each raster plot shows the laminar profiles (colour coded along abscissa) of the vertices for the ‘predicting contrast’ [V-Fix] and of the ‘predicted contrast’ [AV-A]. The laminar profiles of the vertices are sorted along the ordinate according to the value of the ‘linear’ shape parameter for [V-Fix]. A1 vertices with less deactivations in deeper laminae for [V-Fix] are associated with greater crossmodal enhancement [AV-A] in deeper laminae. To enable averaging the raster plots across participants, the vertices were binned after sorting (number of bins for A1: 10100, number of bins for PT: 6800). For visualization purposes all raster plots were smoothed along the vertical axis (FWHM = 1% of the number of data bins). The subplot (i.e. black line) to the left of the raster plots shows the across subjects’ mean value (+ /- STD) of the shape parameters (i.e. i. constant, ii, linear) of the sorting contrast. n = 11.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Cross-modal modulation in visual areas.</title><p>Laminar profiles: Rows 1 and 3: The BOLD response (solid line; column 1 and 3) and decoding accuracy (dashed line; columns 2 and 4) (across subjects’ mean ± SEM) for [AV-A] in V1 and V2/3 is shown as a function of percentage cortical depth pooled (i.e. averaged) over auditory and visual attention. WM: white matter, GM: grey matter, CSF: cerebrospinal fluid. Percentage cortical depth is indicated by the small numbers and colour coded in red. Rows 2 and 4: Across subjects’ mean (± SEM) and violin plot of participants’ shape parameter estimates that characterize the mean (C: constant) and linear increase (L: linear) of the laminar BOLD response and decoding accuracy profiles. n = 11.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Raster plots for auditory induced activations and visual induced deactivations in auditory areas.</title><p>The raster plots illustrate the statistical relationship between the ‘constant’ shape parameters for the visual evoked response [V-Fix]<sub>AttA, AttV</sub> and auditory evoked response [A-Fix]<sub>AttA, AttV</sub> in A1 (left) and PT (right). Each raster plot shows the laminar profiles (colour coded along abscissa) of the vertices for the ‘predicting contrast’ [V-Fix] and of the ‘predicted contrast’ [A-Fix]. The vertex profiles were sorted along the ordinate according to the value of the ‘constant’ shape parameter for [V-Fix]. The raster plot shows that the laminar profile of a vertex for [V-Fix] cannot predict its laminar profile for [A-Fix]. To enable averaging the raster plots across participants, the vertices were binned after sorting (number of bins for A1: 10100, number of bins for PT: 6800). For visualization purposes, all raster plots were smoothed along the vertical axis (FWHM = 1% of the number of data bins). The subplot (i.e. black line) to the left of the raster plots shows the across subjects’ mean value (+ /- STD) of the shape parameters (i.e. i. constant, ii, linear) of the sorting contrast. The violin plot show the distribution across subject of beta values for ‘a’ in the regression model <italic>[A-Fix]=a [V-FIX] + b</italic>. n = 11.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig3-figsupp2-v2.tif"/></fig></fig-group><p>All statistical results are presented in <xref ref-type="table" rid="table1">Table 1</xref>, <xref ref-type="table" rid="table2">Table 2</xref>, <xref ref-type="table" rid="table3">Table 3</xref> and <xref ref-type="supplementary-material" rid="supp3">Supplementary files 3</xref> and <xref ref-type="supplementary-material" rid="supp4">4</xref>. Additional descriptive statistics as well as effect sizes can be found here: <ext-link ext-link-type="uri" xlink:href="https://osf.io/tbh37/">https://osf.io/tbh37/</ext-link>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Auditory and visual deactivations.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom"/><th valign="bottom"/><th colspan="2">Linear or constan<bold>t</bold></th><th valign="bottom"/><th colspan="2">Constant</th><th colspan="2">Linear</th></tr></thead><tbody><tr><td rowspan="3">[V-fix]Att_A, Att_V</td><td valign="bottom">Mean(A1, PT)</td><td valign="bottom">F(2,40) = 9.280</td><td valign="bottom">p&lt;0.001</td><td valign="bottom"/><td valign="bottom">t(10)=−2.460</td><td valign="bottom">p=0.017*</td><td valign="bottom">F(1,20) = 2.083</td><td valign="bottom">p=0.164</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">A1</td><td valign="bottom">t(10)=−2.077</td><td valign="bottom">p=0.032*</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">PT</td><td valign="bottom">t(10)=−2.042</td><td valign="bottom">p=0.034*</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td rowspan="3">[A-fix]Att_A, Att_V</td><td valign="bottom">mean(V1, V23)</td><td valign="bottom">F(2,40) = 58.615</td><td valign="bottom">p&lt;0.001</td><td valign="bottom"/><td valign="bottom">t(10)=−5.547</td><td valign="bottom">p&lt;0.001*</td><td valign="bottom">F(1,20) = 22.433</td><td valign="bottom">p&lt;0.001</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">V1</td><td valign="bottom">t(10)=−6.538</td><td valign="bottom">p&lt;0.001*</td><td valign="bottom">t(10)=−5.080</td><td valign="bottom">p&lt;0.001</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">V2-3</td><td valign="bottom">t(10)=−4.305</td><td valign="bottom">p&lt;0.001*</td><td valign="bottom">t(10)=−4.142</td><td valign="bottom">p=0.002</td></tr></tbody></table><table-wrap-foot><fn><p>*indicates p-values based on a one-sided t-test based on a priori hypotheses. p-values&lt;0.05 are indicated in bold. n = 11</p><p>Using 2 (shape parameter: constant, linear) x 2 (ROI: primary, non-primary) linear mixed effects models, we performed the following statistical comparisons in a 'step down procedure':</p></fn><fn><p>1. Two-dimensional F-test assessing whether the constant or linear parameter (e.g. each averaged across ROIs in auditory resp. visual cortices), was significantly different from zero (dark grey),</p><p>2. If this two-dimensional F-test was significant, we computed one dimensional F-tests separately for the constant and the linear parameters (again averaged across auditory resp. visual ROIs) (light grey),</p></fn><fn><p>3. If the one dimensional F-test was significant, we computed follow-up t-tests separately for each of the two ROIs (white).</p></fn></table-wrap-foot></table-wrap><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Effects of the cross-modal modulation on the laminar BOLD response and decoding accuracy profiles in auditory areas.</title></caption><table frame="hsides" rules="groups"><tbody><tr><th>A) BOLD profile</th><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td valign="bottom"/><td valign="bottom"/><th colspan="2">linear or constant</th><td valign="bottom"/><th colspan="2">constant</th><th colspan="2">linear</th></tr><tr><td valign="bottom">[AV - A]Att_A, Att_V</td><td valign="bottom">mean(A1, PT)</td><td valign="bottom">F(2,40) = 0.196</td><td valign="bottom">p=0.823</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><th>B) Decoding profile</th><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td valign="bottom"/><td valign="bottom"/><th colspan="2">linear or constant</th><td valign="bottom"/><th colspan="2">constant</th><th colspan="2">linear</th></tr><tr><td rowspan="3">[AV VS A]att A, att V</td><td valign="bottom">mean(A1, PT)</td><td valign="bottom">F(2,40) = 34.946</td><td valign="bottom">p&lt;0.001</td><td valign="bottom"/><td valign="bottom">F(1,20) = 21.966</td><td valign="bottom">p&lt;0.001</td><td valign="bottom">F(1,20) = 1.850</td><td valign="bottom">p=0.189</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">A1</td><td valign="bottom">t(10)=3.867</td><td valign="bottom">p=0.003</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">PT</td><td valign="bottom">t(10)=4.992</td><td valign="bottom">p&lt;0.001</td><td valign="bottom"/><td valign="bottom"/></tr></tbody></table><table-wrap-foot><fn><p>Using 2 (shape parameter: constant, linear) x 2 (ROI: primary, non-primary) linear mixed effects models, we performed the following statistical comparisons in a 'step down procedure':</p><p>1. Two-dimensional F-test assessing whether the constant or linear parameter (e.g. each averaged across ROIs in auditory resp. visual cortices), was significantly different from zero (dark grey),</p></fn><fn><p>2. If this two-dimensional F-test was significant, we computed one dimensional F-tests separately for the constant and the linear parameters (again averaged across auditory resp. visual ROIs) (light grey),</p><p>3. If the one dimensional F-test was significant, we computed follow-up t-tests separately for each of the two ROIs (white).</p></fn></table-wrap-foot></table-wrap><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Effects of the attentional modulation (irrespective of stimulus type) on the laminar BOLD response and decoding accuracy profiles.</title></caption><table frame="hsides" rules="groups"><tbody><tr><th>A) BOLD profile</th><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td valign="bottom"/><td valign="bottom"/><th colspan="2">linear or constant</th><td valign="bottom"/><th colspan="2">constant</th><th colspan="2">linear</th></tr><tr><td rowspan="3">[Att_V - Att_A]A, V, AV</td><td valign="bottom">mean(A1, PT)</td><td valign="bottom">F(2,40) = 12.602</td><td valign="bottom">p&lt;0.001</td><td valign="bottom"/><td valign="bottom">F(1,20) = 9.249</td><td valign="bottom">p=0.006</td><td valign="bottom">F(1,20) = 12.163</td><td valign="bottom">p=0.002</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">A1</td><td valign="bottom">t(10)=1.882</td><td valign="bottom">p=0.089</td><td valign="bottom">t(10)=3.123</td><td valign="bottom">p=0.011</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">PT</td><td valign="bottom">t(10)=4.523</td><td valign="bottom">p=0.001</td><td valign="bottom">t(10)=3.361</td><td valign="bottom">p=0.007</td></tr><tr><td valign="bottom">[Att_V - Att_A]A, V, AV</td><td valign="bottom">mean(V1, V23)</td><td valign="bottom">F(2,40) = 0.669</td><td valign="bottom">p=0.518</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><th>B) Decoding profile</th><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td valign="bottom"/><th colspan="2">linear or constant</th><td valign="bottom"/><th colspan="2">constant</th><th colspan="2">linear</th></tr><tr><td rowspan="3">[Att_A VS Att_V]A, V, AV</td><td valign="bottom">mean(A1, PT)</td><td valign="bottom">F(2,40) = 4.687</td><td valign="bottom">p=0.015</td><td valign="bottom"/><td valign="bottom">F(1,20) = 4.882</td><td valign="bottom">p=0.039</td><td valign="bottom">F(1,20) = 4.028</td><td valign="bottom">p=0.058</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">A1</td><td valign="bottom">t(10)=1.260</td><td valign="bottom">p=0.236</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">PT</td><td valign="bottom">t(10)=2.031</td><td valign="bottom">p=0.070</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td rowspan="3">[Att_A VS Att_V]A, V, AV</td><td valign="bottom">mean(V1, V23)</td><td valign="bottom">F(2,40) = 20.026</td><td valign="bottom">p&lt;0.001</td><td valign="bottom"/><td valign="bottom">F(1,20) = 13.564</td><td valign="bottom">p=0.001</td><td valign="bottom">F(1,20) = 9.951</td><td valign="bottom">p=0.005</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">V1</td><td valign="bottom">t(10)=2.472</td><td valign="bottom">p=0.033</td><td valign="bottom">t(10)=1.359</td><td valign="bottom">p=0.204</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom">V2-3</td><td valign="bottom">t(10)=4.298</td><td valign="bottom">p=0.002</td><td valign="bottom">t(10)=3.089</td><td valign="bottom">p=0.011</td></tr></tbody></table><table-wrap-foot><fn><p>Using 2 (shape parameter: constant, linear) x 2 (ROI: primary, non-primary) linear mixed effects models, we performed the following statistical comparisons in a 'step down procedure':</p><p>1. Two-dimensional F-test assessing whether the constant or linear parameter (e.g. each averaged across ROIs in auditory resp. visual cortices), was significantly different from zero (dark grey),</p></fn><fn><p>2. If this two-dimensional F-test was significant, we computed one dimensional F-tests separately for the constant and the linear parameters (again averaged across auditory resp. visual ROIs) (light grey),</p><p>3. If the one dimensional F-test was significant, we computed follow-up t-tests separately for each of the two ROIs (white).p-values&lt;0.05 are indicated in bold. n = 11.</p></fn></table-wrap-foot></table-wrap><sec id="s2-1"><title>Auditory cortices</title><p>Auditory stimuli evoked a positive BOLD response in primary auditory cortex and especially in anterior portions of the planum temporale (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1B</xref> and <xref ref-type="fig" rid="fig2s3">3</xref> right). As expected from the typical physiological point spread function of the GE-EPI BOLD signal, the positive BOLD signal increased roughly linearly towards the cortical surface (<xref ref-type="bibr" rid="bib51">Markuerkiaga et al., 2016</xref>) (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref> and <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>).</p><p>Deactivations induced by crossmodal visual stimuli (i.e. a negative BOLD response for [V-Fix]) were observed in both A1 and PT with a constant response profile and based on visual inspection even a trend towards stronger deactivations in deeper laminae (<xref ref-type="fig" rid="fig2">Figure 2A</xref> left, <xref ref-type="table" rid="table1">Table 1</xref>). These visually induced deactivations were generally observed for both auditory and visual attention conditions, with no significant difference between them ([Att<sub>A</sub>-Att<sub>V</sub>] for visual stimuli in A1 and PT: F(2,40) = 0.644, p=0.530).</p><p>We did not observe a significant crossmodulatory effect of visual stimuli on the BOLD-response in A1 and PT in the context of concurrent auditory stimuli (<xref ref-type="fig" rid="fig3">Figure 3A</xref> left, <xref ref-type="table" rid="table2">Table 2A</xref>). However, pattern classifiers succeeded in discriminating between patterns elicited by [AV vs. A] conditions across all laminae in both PT and A1 (<xref ref-type="fig" rid="fig3">Figure 3A</xref> right, <xref ref-type="table" rid="table2">Table 2B</xref>). Thus, even when the mean BOLD response across vertices averaged across A1 and PT did not differ significantly for AV and A stimuli at a particular depth, the visual stimulus changed the activation pattern elicited by a concurrent auditory stimulus. This provides evidence that sub-regions with crossmodal enhancement and suppression co-exist in A1 and PT. The visual induced changes in activation patterns were again not significantly affected by modality-specific attention (for the classification [AV-V]<sub>att A</sub> VS [AV-V]<sub>att V</sub>, F(2,40) = 0.185, p=0.832).</p><p>Taken together these results suggest that salient visual looming stimuli affected auditory cortices irrespective of the direction of modality-specific attention in both unisensory and audiovisual contexts. Further, the surface projections of the shape parameters of the BOLD response profile at the group level revealed a patchy topography both for visually-induced deactivations in a unisensory context (<xref ref-type="fig" rid="fig2">Figure 2B</xref> left) and for visual-induced modulations of the auditory response (<xref ref-type="fig" rid="fig3">Figure 3B</xref> left i and ii).</p><p>We next explored whether visual stimuli influenced activation patterns in auditory cortices in a similar patchy topography during unisensory visual and audiovisual contexts. For this, we defined a general linear model for each subject that used the ‘constant’ or ‘linear’ shape parameters from the visual deactivations (i.e. [V-Fix]) for each vertex to predict the ‘constant’, and respectively ‘linear’, shape parameters for the visually induced crossmodal modulation (i.e. [AV-A]) over vertices (again in a cross-validated fashion). We visualized the results of this regression in the form of raster plots (<xref ref-type="fig" rid="fig3">Figure 3B</xref> right i and ii). These raster plots show the laminar BOLD response profile for the difference [AV-A] across vertices, sorted according to their BOLD response profile for [V-fix].</p><p>In A1 and PT, the shape profile of a vertex for visual deactivations significantly predicted its profile for cross-modal modulation suggesting similar patchy topographies for visual deactivations and crossmodal modulation. In PT, the constant parameter for [V-Fix] predicted the constant for [AV-A]: the less a vertex is deactivated in PT by visual stimuli in a unisensory context, the more it shows a visually induced enhancement of the response to a concurrent auditory stimulus (constant: t(10)=3.460, p=0.006, linear: t(10)=1.853, p=0.094, see <xref ref-type="fig" rid="fig3">Figure 3Bi</xref> right). More interestingly, in A1 the linear shape parameter for [V-Fix] predicted the linear shape parameter of [AV-A]: vertices with less deactivations in deeper relative to superficial laminae in unisensory visual context showed a robust crossmodal enhancement that was most pronounced in deeper laminae (linear: t(10)=3.121, p=0.011, constant: t(10)=0.021, p=0.983, <xref ref-type="fig" rid="fig3">Figure 3Bii</xref> right). These results strongly suggest that visual stimuli generate a BOLD response in primary auditory cortex with a patchy topography similar for unisensory and audiovisual contexts.</p><p>Because these commonalities in topography across two contrasts could in principle arise from spurious factors (such as registration errors, curvature-dependent segmentation errors, heterogeneous occurrence of principal veins, curvature dependent occurrence of veins, orientation dependence to B0, signal leakage of kissing gyri) that could affect BOLD-response magnitude similarly in different contrasts, we repeated the statistical analysis and raster plots for the constant shape parameters of the [V-Fix] and [A-Fix] contrasts in auditory areas. However, as shown in the raster plots in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> this control analysis did not reveal any significant relationship between the two contrasts. The absence of a significant effect in this control analysis thus suggests that the similarity in topography between visually induced deactivations and cross-modal modulations reflects similarities in neural organization rather than non-specific effects.</p><p>In contrast to these multisensory influences, attentional modulation was greatest at the cortical surface in both A1 and PT for the regional BOLD response (i.e. significant positive linear effect in A1 and PT, <xref ref-type="fig" rid="fig4">Figure 4Ai</xref> left, <xref ref-type="table" rid="table3">Table 3A</xref>). Averaged across A1 and PT, pattern classifiers also discriminated between the auditory and visual attention conditions (<xref ref-type="fig" rid="fig4">Figure 4Ai</xref> right, <xref ref-type="table" rid="table3">Table 3B</xref>). The decoding accuracy profiles were characterized by a significant constant term and a non-significant trend for the linear term. Surprisingly, even though the mean BOLD-response profiles revealed a profound effect of attention, the discrimination between auditory and visual attention conditions was rather limited. Discriminating between activation patterns may be limited, if modality-specific attention predominantly amplifies and scales the BOLD response for A, V or AV stimuli whilst preserving the activation patterns, whereas the crossmodal influences impacts the activation patterns.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Attentional modulation.</title><p>(<bold>A</bold>) Laminar profiles: Rows 1, 3, 5, 7: The BOLD response (solid line; columns 1 and 3) and decoding accuracy (dashed line; columns 2 and 4) (across subjects’ mean ± SEM) for attentional modulation (i: top - [Att<sub>A</sub>-Att<sub>V</sub>] in A1 and PT; ii: bottom - [Att<sub>V</sub>-Att<sub>A</sub>] in V1 and V2/3) is shown as a function of percentage cortical depth pooled (i.e. averaged) over auditory, visual and audiovisual looming stimuli. WM: white matter, GM: grey matter, CSF: cerebrospinal fluid. Percentage cortical depth is indicated by the small numbers and colour coded in red. Rows 2, 4, 6, 8: Across subjects’ mean (± SEM) and violin plot of the participants’ shape parameter estimates that characterize the mean (C: constant) and linear increase (L: linear) of the laminar BOLD response and decoding accuracy profiles. n = 11. B. Surface projections: Across subject’ mean of the ‘constant’ shape parameter estimates for attentional modulation [Att<sub>A</sub>-Att<sub>V</sub>]<sub>A,V, AV</sub> in A1 and PT (i: top) and in V1 and V2/3 (ii: bottom) are projected on an inflated group mean surface of the left hemisphere. A1 and V1 are delineated by black solid lines. PT and V2/3 are delineated by dashed lines. For visualization purposes only: i. surface projections were smoothed (FWHM = 1.5 mm); ii. values are also presented for vertices for which data were not available from all subjects and which were therefore not included in our formal statistical analysis; iii borders between visual-induced activations and deactivations (white dashed lines on the right) are reported here from <xref ref-type="fig" rid="fig2">Figure 2B</xref>. Grey areas denote vertices with no available data for any subject.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46856-fig4-v2.tif"/></fig><p>To investigate this explanation further, we quantified and compared the similarity in A1 between activation patterns of auditory vs. visual attention conditions (averaging over i. A_Att<sub>V</sub> vs. A_Att<sub>A</sub>; and ii. AV_Att<sub>V</sub> vs. AV_Att<sub>A</sub>) and of auditory vs. audiovisual conditions (averaging over iii. A_Att<sub>A</sub> vs. AV_Att<sub>A</sub>; and iv. A_Att<sub>V</sub> vs. AV_Att<sub>V</sub>) using the Spearman correlation coefficient computed over vertices. The average similarity between auditory and visual attention conditions was indeed greater than the average similarity between A and AV conditions (two-sided exact sign permutation test on Fisher transformed correlation coefficients, p=0.008). These results suggest that auditory attention mainly scales the BOLD response in A1 whilst preserving the activation pattern, whereas visual stimuli alter the activation pattern in A1.</p><p>In summary, we observed distinct laminar BOLD response profiles and activation patterns for multisensory and attentional influences in auditory cortices. Visual stimuli induced deactivations with a constant profile both in A1 and PT. Likewise, they influenced the activation pattern in auditory cortices similarly across all laminae (i.e. constant effect for decoding accuracy) in audiovisual context leading to successful pattern decoding accuracy for [AV vs. A] even in deeper laminae. In fact, in A1 and PT visual inputs altered the activation pattern with a similar patchy topography when presented alone (i.e. visual-induced deactivation) or together with an auditory stimulus (i.e. crossmodal modulation).</p><p>Modality-specific attention amplified the mean BOLD-signal within A1 and PT mainly at the cortical surface (i.e. significant linear parameter). Likewise the decoding accuracy was better than chance with a non-significant increase toward the surface.</p></sec><sec id="s2-2"><title>Visual cortices</title><p>In V1 and V2/3 visual stimuli induced activations that increased toward the cortical surface as expected for GE-EPI BOLD (<xref ref-type="bibr" rid="bib51">Markuerkiaga et al., 2016</xref>) (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref> and <xref ref-type="fig" rid="fig2s2">2</xref> left and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Contrary to previous research (<xref ref-type="bibr" rid="bib10">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib38">Koopmans et al., 2010</xref>), we did not observe a more selective increase in activations for layer 4. Potentially, this selective increase in BOLD-response may have been smoothed across multiple layers, because the relative cortical depth of layer four varies between foveal and lateral projections in V1 and our region of interest is larger than those used in previous work (for similar findings see figure 6B in <xref ref-type="bibr" rid="bib64">Polimeni et al., 2010</xref> for fMRI and figure 6E in <xref ref-type="bibr" rid="bib86">Waehnert et al. (2016)</xref> for structural anatomy). Consistent with previous research (<xref ref-type="bibr" rid="bib71">Shmuel et al., 2002</xref>; <xref ref-type="bibr" rid="bib73">Silver et al., 2008</xref>; <xref ref-type="bibr" rid="bib80">Tootell et al., 1998</xref>), the centrally presented visual looming stimuli activated predominantly areas representing the centre of the visual field with response suppressions (i.e. negative BOLD) in adjacent areas (see dashed white lines in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref> right). By contrast, auditory deactivations were not confined to the central field that was activated by visual stimuli. Instead, they were particularly pronounced in the rostral part of the calcarine sulcus representing more eccentric positions of the visual field (<xref ref-type="fig" rid="fig2">Figure 2B</xref> right and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> right). Furthermore, auditory looming stimuli induced deactivations that increased in absolute magnitude toward the cortical surface (significant constant and linear effect, <xref ref-type="table" rid="table1">Table 1</xref>). Again as in auditory areas, these auditory deactivations were not significantly modulated by modality-specific attention ([Att<sub>V</sub>-Att<sub>A</sub>] for auditory stimuli in V1 and V2/3: F(2,40) = 1.651, p=0.205).</p><p>Despite the pronounced deactivations elicited by unisensory auditory stimuli, we did not observe any significant cross-modal modulation of the regional BOLD response in V1 and V2/3 (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> left and <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4A</xref>). Likewise, the support vector classifiers were not able to discriminate between activation patterns for [AV vs. V] stimuli better than chance. In summary, the effect of auditory stimuli on visual cortices was abolished under concurrent visual stimulation both in terms of regional BOLD response and activation patterns (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> right and <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4B</xref>).</p><p>With respect to attentional modulation, we observed no significant differences in mean BOLD responses in V1 or V2/3 for visual vs. auditory attention conditions (see <xref ref-type="table" rid="table3">Table 3A</xref>). As shown in the violin plots of <xref ref-type="fig" rid="fig4">Figure 4Aii</xref> left, only few participants showed a substantial attentional modulation effect when averaged across the entire regions of interest. Yet, the classifier was able to discriminate between patterns for visual and auditory attention conditions successfully across all cortical depth surfaces in V1 and V2/3 (i.e. significant constant term in V1 and V2/3 and linear term in V2/3, see <xref ref-type="table" rid="table3">Table 3B</xref>, <xref ref-type="fig" rid="fig4">Figure 4Aii</xref> right). The surface projection of shape parameters of the BOLD response profile at the group level explains this discrepancy between the attentional effects on regional BOLD response and activation patterns (<xref ref-type="fig" rid="fig4">Figure 4Bii</xref>). Consistent with mechanisms of contrast enhancement (<xref ref-type="bibr" rid="bib5">Bressler et al., 2013</xref>; <xref ref-type="bibr" rid="bib60">Müller and Kleinschmidt, 2004</xref>; <xref ref-type="bibr" rid="bib75">Smith et al., 2000</xref>; <xref ref-type="bibr" rid="bib80">Tootell et al., 1998</xref>), attending to vision relative to audition increased responses in regions that were activated by visual looming stimuli, yet suppressed the deactivations in the surrounding regions that were deactivated by visual stimuli thereby cancelling out global attentional effects for the regional BOLD response (<xref ref-type="fig" rid="fig4">Figure 4Bii</xref>, note the white lines indicate the border between visual activation and deactivation from <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref> right).</p><p>In summary, auditory looming induced widespread deactivations in visual cortices that were maximal at the cortical surface and extended into regions that represent more peripheral visual fields. By contrast, attention to vision predominantly increased BOLD response in central parts that were activated by visual stimuli and suppressed BOLD responses in the periphery that were deactivated by visual stimuli.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The current high-resolution 7T fMRI study revealed distinct depth-dependent BOLD response profiles and patterns for multisensory and attentional influences in early sensory cortices.</p><sec id="s3-1"><title>Distinct laminar profiles and activation patterns for multisensory and attentional influences in auditory cortices</title><p>Visual looming suppressed activations in auditory cortices (see also [<xref ref-type="bibr" rid="bib42">Laurienti et al., 2002</xref>; <xref ref-type="bibr" rid="bib43">Leitão et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Mozolic et al., 2008</xref>], but enhanced the response to concurrent auditory looming in posterior auditory cortices (<xref ref-type="bibr" rid="bib90">Werner and Noppeney, 2011</xref>; <xref ref-type="bibr" rid="bib89">Werner and Noppeney, 2010</xref>). In other words, intersensory competition for purely visual stimuli turned into cooperative interactions for audiovisual stimuli. While the visual-induced response suppression was constant across cortical depth in A1 and PT, the crossmodal BOLD-response enhancement was observed mainly in the caudal parts of PT, that is in parts of PT where visual influences have previously been shown (<xref ref-type="bibr" rid="bib34">Kayser et al., 2007</xref>). At a finer spatial resolution, multivariate analyses revealed significant differences between audiovisual and auditory activation patterns across all laminae in both A1 and PT. As shown in the surface projections (see <xref ref-type="fig" rid="fig3">Figure 3B</xref>), visual looming enhanced and suppressed auditory responses in adjacent patches consistent with neurophysiological results in non-human primates (<xref ref-type="bibr" rid="bib35">Kayser et al., 2008</xref>). Critically, as illustrated in the raster plots, the response profile of a vertex to visual stimuli significantly predicted its laminar profile for crossmodal modulation (see <xref ref-type="fig" rid="fig3">Figure 3B</xref> right). In A1, vertices whose activations were only weakly suppressed by visual looming in deeper laminae showed a greater visual-induced response enhancement for audiovisual looming again in deeper laminae. These results suggest that visual looming influences activations in A1 mainly in deeper layers with similar patchy topographies in unisensory and audiovisual contexts.</p><p>By contrast, modality-specific attention affected auditory and visual evoked responses predominantly at the cortical surface (see significant linear term in <xref ref-type="table" rid="table3">Table 3</xref>). The large attentional BOLD-response effects in superficial laminae dovetail nicely with previous neurophysiological research, which located effects of modality-specific attention in supragranular layers of auditory cortices (<xref ref-type="bibr" rid="bib12">De Martino et al., 2015a</xref>; <xref ref-type="bibr" rid="bib40">Lakatos et al., 2009</xref>). Yet, because the GE-EPI BOLD response measured at superficial laminae includes contributions from deeper laminae (<xref ref-type="bibr" rid="bib29">Harel et al., 2006</xref>; <xref ref-type="bibr" rid="bib51">Markuerkiaga et al., 2016</xref>), we cannot unequivocally attribute the attentional influences at the cortical surface solely to superficial layers as neural origin. It is also possible that the increase in attentional BOLD-response effects towards the cortical surface may arise from neural effects across all cortical layers. Consistent with this conjecture, the BOLD-response profiles to auditory induced activations (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref> left) and attentional modulation (<xref ref-type="fig" rid="fig4">Figure 4Ai</xref> left) in A1 and PT are quite similar.</p><p>Critically, however, the laminar profiles for attentional effects were distinct from those for multisensory (i.e. crossmodal) effects in auditory cortices. This dissociation in laminar profiles in the same territories cannot be explained by vascular effects that limit the laminar specificity of the BOLD response, but strongly implies that multisensory and attentional mechanisms regulate information flow in auditory cortices via partly distinct neural circuitries (<xref ref-type="bibr" rid="bib12">De Martino et al., 2015a</xref>; <xref ref-type="bibr" rid="bib40">Lakatos et al., 2009</xref>). Further, the constant laminar profiles for crossmodal influences in auditory cortices suggest that visual signals influence auditory cortices mainly via infragranular layers – a finding that converges with a recent neurophysiological study in mouse that likewise highlighted deep layer six as the key locus for visual influences on auditory cortex (<xref ref-type="bibr" rid="bib57">Morrill and Hasenstaub, 2018</xref>). Anatomical studies in monkeys have previously suggested that visual inputs can influence auditory cortices via three routes (<xref ref-type="bibr" rid="bib26">Ghazanfar and Schroeder, 2006</xref>; <xref ref-type="bibr" rid="bib61">Musacchia and Schroeder, 2009</xref>; <xref ref-type="bibr" rid="bib69">Schroeder et al., 2003</xref>; <xref ref-type="bibr" rid="bib74">Smiley and Falchier, 2009</xref>): i. thalamic afferents, ii. direct connectivity between sensory areas (<xref ref-type="bibr" rid="bib20">Falchier et al., 2002</xref>; <xref ref-type="bibr" rid="bib65">Rockland and Ojima, 2003</xref>) and iii. connections from higher order association cortices.</p></sec><sec id="s3-2"><title>Common laminar profiles, but distinct activation patterns for multisensory and attentional influences in visual cortices</title><p>In visual cortices, visual and auditory stimuli evoked responses that were maximal in absolute amplitude at the cortical surface, yet differed in their activation pattern. Visual looming stimuli induced activations in areas representing the centre of the visual field and deactivations in surrounding areas representing the periphery (<xref ref-type="bibr" rid="bib71">Shmuel et al., 2002</xref>). In contrast to this visual ‘centre-surround’ pattern, auditory stimuli induced widespread deactivations (<xref ref-type="bibr" rid="bib42">Laurienti et al., 2002</xref>; <xref ref-type="bibr" rid="bib43">Leitão et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Mozolic et al., 2008</xref>) particularly in peripheral visual field representations known to have denser direct fibre connections to auditory cortices (<xref ref-type="bibr" rid="bib20">Falchier et al., 2002</xref>; <xref ref-type="bibr" rid="bib65">Rockland and Ojima, 2003</xref>).</p><p>Thus, our study revealed three types of deactivations. In visual cortices, both visual and auditory stimuli induced deactivations that were most evident at the cortical surface, as previously reported for deactivations to ipsilateral visual stimuli (<xref ref-type="bibr" rid="bib24">Fracasso et al., 2018</xref>), though see [<xref ref-type="bibr" rid="bib27">Goense et al., 2012</xref>]. In auditory cortices, the visual-induced deactivations were constant across cortical depth. These differences in laminar profiles or BOLD patterns between visual and auditory cortices may reflect differences in neural mechanisms. For instance, neurophysiological studies in rodents have shown asymmetries in multisensory influences between auditory and visual cortices, reporting robust auditory induced inhibition in layers 2/3 in visual cortices but not visual-induced inhibition in auditory cortices (<xref ref-type="bibr" rid="bib32">Iurilli et al., 2012</xref>).</p><p>While we observed no statistically significant crossmodal nor attentional effects on the regional BOLD response profiles in visual cortices, multivariate pattern analyses indicated that modality-specific attention altered the activation patterns at a sub-regional spatial resolution (<xref ref-type="fig" rid="fig4">Figure 4Aii</xref>): attention to vision amplified activations in areas representing the central visual field and suppressed activations in areas representing peripheral visual fields (see <xref ref-type="fig" rid="fig4">Figure 4Bii</xref>).</p><p>The differences in activation patterns for multisensory and attentional influences indicate that the impact of purely auditory looming (<xref ref-type="bibr" rid="bib50">Maier et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Maier et al., 2004</xref>) on visual cortices cannot solely be attributed to a withdrawal of attentional resources from vision. Instead, purely auditory looming may inhibit activations in visual cortices via direct connectivity between auditory and visual areas (<xref ref-type="bibr" rid="bib4">Bizley et al., 2007</xref>; <xref ref-type="bibr" rid="bib6">Budinger et al., 2006</xref>; <xref ref-type="bibr" rid="bib8">Campi et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Falchier et al., 2002</xref>; <xref ref-type="bibr" rid="bib31">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="bib65">Rockland and Ojima, 2003</xref>). In line with this conjecture, research in rodents has shown that auditory stimuli modulates activity in supragranular layers in primary visual areas via direct connectivity from auditory cortices and translaminar inhibitory circuits – though the studies disagreed in whether layer 5 (<xref ref-type="bibr" rid="bib32">Iurilli et al., 2012</xref>) or layer 1 (<xref ref-type="bibr" rid="bib14">Deneux et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Ibrahim et al., 2016</xref>) were the primary targets of auditory inputs.</p></sec><sec id="s3-3"><title>Multisensory effects in visual and auditory cortices – a comparison</title><p>Our results reveal distinct laminar profiles for multisensory deactivations in auditory and visual cortices. In auditory cortices the visual induced deactivations were constant across laminae, in visual cortices the deactivations linear increased (in absolute magnitude) toward cortical surface. These distinct laminar profiles converge with previous research in rodents that also reveal visual influences in auditory cortices predominantly in infragranular layer (<xref ref-type="bibr" rid="bib57">Morrill and Hasenstaub, 2018</xref>), but auditory influences in visual cortices mainly in supragranular layers (<xref ref-type="bibr" rid="bib14">Deneux et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Iurilli et al., 2012</xref>). Potentially, these distinct laminar profiles in auditory and visual cortices may suggest that multisensory influences are mediated by different neural circuitries in auditory and visual cortices. However, the distinct laminar profiles across cortical areas may not only reflect differences in neural circuitries but also in vascular organization. Moreover, a wealth of research has previously shown that multisensory responses profoundly depend on stimulus salience and other input characteristics (<xref ref-type="bibr" rid="bib14">Deneux et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Meijer et al., 2017</xref>; <xref ref-type="bibr" rid="bib76">Stein et al., 2020</xref>; <xref ref-type="bibr" rid="bib89">Werner and Noppeney, 2010</xref>). It is currently unclear how these factors influence laminar BOLD-response profiles. Finally, interpreting laminar BOLD-response profiles in relation to previous findings in rodents remains tentative because of cross-species and methodological differences.</p></sec><sec id="s3-4"><title>Conclusions</title><p>Using submillimetre resolution fMRI at 7T, we resolved BOLD response and activation patterns of multisensory and attentional influences across cortical depth in early sensory cortices. In visual cortices auditory stimulation induced widespread inhibition, while attention to vision enhanced central but suppressed peripheral field representations. In auditory cortices, competitive and cooperative multisensory interactions were stronger deep within the cortex, while attentional influences were greatest at the cortical surface. The distinctiveness of these depth-dependent activation profiles and patterns suggests that multisensory and attentional mechanisms control information flow via partly distinct neural circuitries.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All procedures were approved by the Ethics Committee of the University of Leipzig.</p><sec id="s4-1"><title>Participants</title><p>Thirteen healthy German native speakers (6 females; 7 males; mean age: 24.8 years, standard deviation: 1.5 years, range: 22–27 years) gave written informed consent to participate in this fMRI study. Participants reported no history of psychiatric or neurological disorders, and no current use of any psychoactive medications. All had normal or corrected to normal vision and reported normal hearing. Two of those subjects did not complete all fMRI sessions and were therefore not included in the analysis. All procedures were approved by the Ethics Committee of the University of Leipzig.</p></sec><sec id="s4-2"><title>Looming stimuli and targets</title><p>The visual looming stimulus (500 ms duration) was a sequence of 1 to 4 radially expanding white annuli (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Each of the four annuli started in the centre of the screen with a diameter of 0.25° visual angle and expanded at a radial speed of 15.52° visual angle per second to a maximum radius of 3.88° visual angle which covered the entire screen visible in the scanner bore along the vertical dimension. During the 500 ms sequence, an additional annulus started in the centre every 125 ms. Given the retinotopic organization of early visual cortices (<xref ref-type="bibr" rid="bib87">Wandell et al., 2007</xref>) these parameters ensured that visual looming elicited relatively widespread activations in visual cortices.</p><p>The auditory looming stimulus (500 ms duration) was composed of several pure tones (55, 110, 150, 220, 330, 380, 440, 660, 880, 990, 1110, 1500, 2500, 3000 Hz) that increased in frequency and amplitude over the entire 500 ms duration. The amplitude of the sound was exponentially modulated with the amplitude at time t given by A<sub>t</sub> = A<sub>0</sub> e<sup>0.68t</sup>-1 and A<sub>0</sub> being the initial baseline amplitude for each pure tone. The frequency of each pure tone component was increased by two thirds of its original base frequency over the 500 ms. Given the tonotopic organization of primary auditory cortices (<xref ref-type="bibr" rid="bib23">Formisano et al., 2003</xref>) this mixture of sound frequencies ensured widespread activations in auditory cortices.</p><p>Audiovisual stimuli were generated by presenting the auditory and visual stimuli in synchrony together.</p><p>The visual target was a grey dot presented for 50 ms within a circular area that increased over the 500 ms duration consistent with the auditory, visual or audiovisual looming stimulus (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Auditory targets were a single 440 Hz pure tone that lasted for 100 ms. The size of the visual target and the sound amplitude of the auditory target were adjusted in a subject-specific fashion to match the approximate target detection performance across sensory modalities and then held constant across the entire experiment. This was done using the method of constant stimuli in a brief ‘psychometric function experiment’, performed prior to the fMRI study inside the scanner and with scanner noise present.</p></sec><sec id="s4-3"><title>Experimental design</title><p>The experiment conformed to a 3 (stimulus modality: auditory, visual, audiovisual) X 2 (attended modality: auditory, visual) factorial design (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In an intersensory selective attention paradigm, participants were presented with blocks of audio (A), visual (V) or audiovisual (AV) looming stimuli as highly salient stimuli that drive bottom-up attention and elicit crossmodal deactivations (<xref ref-type="bibr" rid="bib43">Leitão et al., 2013</xref>). Brief auditory (i.e. weak and brief beep) and visual (i.e. small and brief grey dot) targets were presented interspersed in all stimulation blocks irrespective of the sensory modality of the looming stimuli or the focus of modality-specific attention (e.g. A and V targets were presented during a unisensory auditory looming block and auditory attention). The visual grey dot could be presented at any time at any location within the area defined by the radially expanding outermost annulus until they reach a radius of maximal 3.88° visual angle. Likewise, the auditory target is presented at random time interspersed in the looming sound. Consistent with previous research on modality-specific attention (<xref ref-type="bibr" rid="bib40">Lakatos et al., 2009</xref>; <xref ref-type="bibr" rid="bib90">Werner and Noppeney, 2011</xref>; <xref ref-type="bibr" rid="bib89">Werner and Noppeney, 2010</xref>) these additional targets were included to ensure that the effects of attention could not be attributed to top-down effects associated with explicit responses.</p><p>During auditory attention (Att<sub>A</sub>), participants were instructed to respond selectively to all auditory targets and ignore visual ones. During visual attention (Att<sub>V</sub>) participants were instructed to respond selectively to all visual targets, that is brief grey dots and ignore auditory targets. In this way, we matched the auditory (i.e. attention to frequency) and visual attention (i.e. attention to targets in spatial field) tasks to the tonotopic and retinotopic organizational principles of primary auditory and visual areas as well as the specifics of auditory and visual looming bottom-up salient stimuli.</p></sec><sec id="s4-4"><title>Experimental procedures</title><p>Auditory, visual or audiovisual looming stimuli were presented in 33 s blocks of 50 stimuli (stimulus duration: 500 ms, inter-stimuli interval: 160 ms giving a fixed stimulus onset asynchrony of 660 ms with no jitter) (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Activation blocks were preceded by 16 s fixation baseline periods. We opted for a block design as they have the highest design efficiency despite their potential drawbacks from a cognitive perspective (<xref ref-type="bibr" rid="bib25">Friston et al., 1999</xref>). Throughout the entire experiment, participants fixated a cross (0.16° visual angle) in the centre of the black screen. Therefore, processes related to fixation are present in all conditions (i.e. all stimulation and fixation conditions). The fixation cross was white during the activation conditions and blue during the fixation baseline periods. A blue letter (‘V’ or ‘A’) presented 3 s prior to the activation blocks instructed participants to direct their attention to the visual or auditory modality. During visual attention blocks, participants were instructed to attend generally to the visual modality and respond selectively to visual targets. Conversely, during auditory attention blocks, participants had to attend generally to the auditory modality and respond selectively to auditory targets. Sixteen targets of each modality were presented for each condition. To maintain observer’s attention and minimize the effect of target stimuli on measured stimulus-induced activations, targets were more frequently presented at the end of the block (i.e. two thirds of the blocks included one auditory target and one visual target, the last third of blocks included 2 targets of each type). The final block of each run was followed by a fixation baseline period of 20 s. The order of blocks was pseudo randomized across participants. Each fMRI run included 18 blocks (three blocks for each condition in our 2 × 3 design) and lasted 15 min. There were four runs per participant yielding 12 blocks per condition per participant.</p></sec><sec id="s4-5"><title>Experimental set up</title><p>Stimuli were presented using Presentation (v17.6, Neurobehavioral system, <ext-link ext-link-type="uri" xlink:href="http://www.neurobs.com/">www.neurobs.com</ext-link>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002521">SCR_002521</ext-link>) on a PC desktop (Windows XP). Visual stimuli were back-projected onto a plexiglas screen using an LCD projector (60 Hz) visible to the participant through a mirror mounted on the MR head coil. The MR head coil enabled a visual field with a width of 10.35° and height of 7.76° visual angle. Auditory stimuli were presented using MR-compatible headphones (MR-Confon; HP-SI01 adapted for the head-coil). Behavioral responses were collected using a MR-compatible button device connected to the stimulus computer. The code to run the fMRI experiment is available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3581316">https://doi.org/10.5281/zenodo.3581316</ext-link>.</p></sec><sec id="s4-6"><title>Behavioral data analysis</title><p>The response window extended from the target onset and for either 2.5 s or until the beginning of the next target. Any recorded response outside of that window was coded as an extra-response (i.e. to be modeled in the fMRI design matrix). The percentage of auditory (or visual) targets that participants responded to in a particular condition were entered as dependent variables into a 2 (target modality: auditory vs. visual) X 2 (attended modality: auditory vs. visual) X 3 (stimulus block modality: auditory, visual, audio-visual) repeated measures ANOVA (results were corrected for non-sphericity using Greenhouse-Geiser correction). Alpha was set to 0.05. This analysis was done with IBM SPSS Statistics, version 17.0 (IBM, Armonk, NY, USA).</p></sec><sec id="s4-7"><title>MRI data acquisition</title><p>All experiments were performed on a Siemens 7T MAGNETOM MRI scanner (Siemens Healthineers, Erlangen, Germany) equipped with an SC72 gradient coil (Siemens Healthineers). For radio-frequency transmission and reception, a single-channel-transmit/32-channel-receive phased array head coil (Nova Medical, Wilmington, MA) was used. The experiment was run over 2 days with 50 min MRI scanning in total per day per participant.</p><p>On the first day a whole-brain MP2RAGE sequence (<xref ref-type="bibr" rid="bib52">Marques et al., 2010</xref>) was used to acquire a quantitative T1 map and a weighted T1 image and a second inversion image to enable high-resolution segmentation and cortical laminae definition [repetition time (TR)/echo time (TE) = 5000/2.45 ms, inversion time 1 = 900 ms, flip angle 1 = 5°, inversion time 2 = 2750 ms, flip angle 2 = 3°, 240 slices, matrix = 320 × 320, acquisition time = 600 s, spatial resolution = 0.7 × 0.7 × 0.7 mm<sup>3</sup> voxels, partial Fourier factor = 6/8, parallel imaging using GRAPPA with an acceleration factor of 4, field of view = 168×224 mm]. On the second day, we used a shorter version of that sequence with a coarser resolution only in order to position our EPI acquisition slab.</p><p>Two functional runs were acquired every day using a 2D gradient echo single-shot echo planar imaging (EPI) readout [TR/TE = 3000/25 ms, flip angle = 90°, 48 axial slices acquired in descending direction, matrix size = 256 × 240, phase encoding along the second dimension (posterior to anterior), slice thickness = 0.75 mm, pixel bandwidth: 1085 Hz, brain coverage = 36 mm, spatial resolution = 0.75 × 0.75 × 0.75 mm<sup>3</sup> voxels, partial Fourier factor = 6/8, parallel imaging using GRAPPA with an acceleration factor of 4, number of volumes = 302, acquisition time = 906 s, field of view = 192×180 mm. In order to avoid spin history effects due to imperfect pulse profiles a slice gap of 0.05 mm was introduced. For every session, a new and optimum set of shim values was used.]. A corresponding vendor-provided homodyne online image reconstruction algorithm was used. The first six volumes in each run were automatically discarded to avoid for T1 saturation effects.</p><p>The acquisition slab was inclined and positioned to include Heschl’s gyri, the posterior portions of superior temporal gyri and the calcarine sulci. Both hemispheres were acquired. On the second day the slab was auto-aligned on the acquisition slab of the first day. This provided us with nearly full coverage over all our four regions of interest (see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><p>One field-map was acquired on each day with a coverage and orientation matching that of the EPI slab [TR/TE1/TE2 = 1500/6/7.02 ms, flip angle = 72°; 18 axial slices, matrix = 96×90, slice thickness = 2 mm, spatial resolution = 2.00×2.00 X 2.00 mm<sup>3</sup> voxels, field of view: FOV = 192×180 mm].</p></sec><sec id="s4-8"><title>MRI structural analysis</title><p>The MRI structural data were processed using the CBS toolbox (v3.0.8, Max Planck Institute for human cognitive and brain science, Leipzig, Germany, <ext-link ext-link-type="uri" xlink:href="http://www.nitrc.org/projects/cbs-tools/">www.nitrc.org/projects/cbs-tools/</ext-link>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_009452">SCR_009452</ext-link>) based on the MIPAV software package (v7.0.1, NIH, Bethesda, USA, <ext-link ext-link-type="uri" xlink:href="http://www.mipav.cit.nih.gov/">www.mipav.cit.nih.gov</ext-link>, (<xref ref-type="bibr" rid="bib53">McAuliffe et al., 2001</xref>); RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_007371">SCR_007371</ext-link>) and the JIST pipeline environment(v2.0, (<xref ref-type="bibr" rid="bib41">Landman et al., 2013</xref>; <xref ref-type="bibr" rid="bib48">Lucas et al., 2010</xref>); RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008887">SCR_008887</ext-link>). The T1 map was coregistered to the Montreal's Neurological Institute brain space (rigid body registration and normalized mutual information as a cost function) and resampled to an isometric resolution of 0.4 mm. The image was segmented fully automatically, and the cortical boundary surfaces were reconstructed (<xref ref-type="bibr" rid="bib2">Bazin et al., 2014</xref>). Between these surfaces, we computed six intracortical surfaces that defined cortical laminae with the equivolume approach (<xref ref-type="bibr" rid="bib85">Waehnert et al., 2014</xref>) (<xref ref-type="fig" rid="fig1">Figure 1C</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). It is important to note that the term ‘lamina’ used in this communication does not directly refer to cytoarchitectonically defined cortical layers: for example, the variation across cortical regions of the relative thickness between layers was not modelled here (<xref ref-type="bibr" rid="bib62">Nieuwenhuys, 2013</xref>). However, it should be mentioned that this method provides biologically plausible cortical contours (<xref ref-type="bibr" rid="bib85">Waehnert et al., 2014</xref>).</p></sec><sec id="s4-9"><title>Definition of region of interest</title><p>We defined four regions of interest (ROI), each combining left and right hemispheres, as follows (see Table S2 for ROI effective sizes):</p><list list-type="alpha-upper"><list-item><p>Several studies have shown that the core of primary auditory cortex (A1) because of its characteristic myelination profile can be localized using its low T1 intensity (<xref ref-type="bibr" rid="bib13">De Martino et al., 2015b</xref>; <xref ref-type="bibr" rid="bib15">Dick et al., 2012</xref>; <xref ref-type="bibr" rid="bib28">Hackett et al., 2001</xref>). We therefore defined it by manual delineation (RG, UN) that was guided by anatomical structure (Heschl’s gyrus) and informed by the quantitative T1 map that was sampled at mid-depth of the cortex and projected on the corresponding inflated surface. The final bilateral A1 region was defined as the union of the two delineations (RG, UN). As a result the sub-region of Heschl’s gyrus that we refer to as primary auditory cortex may differ from primary auditory cortex defined based on cytoarchitecture.</p></list-item> <list-item><p>The planum temporale (PT) was defined bilaterally using the area 41–42 of the brainnetome atlas (<xref ref-type="bibr" rid="bib21">Fan et al., 2016</xref>) (<ext-link ext-link-type="uri" xlink:href="http://atlas.brainnetome.org/">http://atlas.brainnetome.org/</ext-link>). The full probability maps of this ROI and its anatomical neighbours were inverse-normalized into native space using the deformation field given by the SPM segmentation of the T1 weighted structural scan. The probability map was then re-gridded to an isometric resolution of 0.4 mm, sampled at mid-depth of the cortex and projected on the corresponding surface. Vertices were included as part of PT if their probability exceeded 40% unless i. they were already defined as being part of A1 or ii. the sum of the probabilities of the neighbouring regions exceeded that of the area 41–42.</p></list-item> <list-item><p>Primary Visual area (V1) and high order visual areas (V2 and V3) were defined bilaterally in a similar fashion as PT. We used the probabilistic retinotopic maps of the ROIs for V1, V2/3 (<xref ref-type="bibr" rid="bib88">Wang et al., 2015</xref>) (<ext-link ext-link-type="uri" xlink:href="http://scholar.princeton.edu/napl/resources">http://scholar.princeton.edu/napl/resources</ext-link>). In this case vertices were included if their probability exceeded 10% unless the sum of the probabilities of the neighbouring regions exceeded that of the area of interest.</p></list-item></list><p>We obtained &gt;95% coverage for A1, PT and V1 and &gt;82% coverage for V2/3. (For the number of vertices and percentage coverage across subjects, <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p></sec><sec id="s4-10"><title>fMRI preprocessing</title><p>The fMRI data were pre-processed and analyzed using statistical parametric mapping (SPM12 – v6685; Wellcome Center for Neuroimaging, London, UK; <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">www.fil.ion.ucl.ac.uk/spm</ext-link>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_007037">SCR_007037</ext-link>) running on matlab (Mathworks). The fieldmaps were co-registered to the first functional scan of the first run (i.e. rigid-body transformations optimized using normalized mutual information as cost function) and a voxel displacement map was then created. Functional scans from each participant were realigned and unwarped using the first scan as a reference (interpolation: 4th degree b-spline, unwarping done using the voxel displacement map of the corresponding day for each run).</p></sec><sec id="s4-11"><title>Functional to anatomical coregistration</title><p>We co-registered the mean EPI image to the pre-processed T1 map (i.e. rigid-body transformations optimized using normalised mutual information as cost function). We assessed the accuracy of co-registration using FSLview (FSL 5.0 - Analysis Group, FMRIB, Oxford, UK, <ext-link ext-link-type="uri" xlink:href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL</ext-link>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002823">SCR_002823</ext-link>) by flipping back and forth between the mean EPI and the T1 map images in all the ROIs (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). If the co-registration was not sufficiently precise, the mean EPI was initially co-registered manually and the co-registration repeated until anatomical structures of the mean EPI and the T1 map were precisely realigned. The transformation matrix of the co-registration was later applied to the beta images from the fMRI general linear model (see fMRI statistical analysis).</p></sec><sec id="s4-12"><title>fMRI statistical analysis</title><p>At the first (i.e subject) level, we performed a mass univariate analysis with a linear regression at each voxel, using generalized least squares with a global approximate AR(1) autocorrelation model and a drift fit with discrete cosine transform basis (128 s cut-off). We modelled the fMRI experiment with a mixed block-event related model with regressors entered into the run-specific design matrix after convolving the onsets of each block or event with a canonical hemodynamic response function (HRF) and its temporal derivative. More specifically, we modelled each of the three looming blocks in each run for each of the six conditions in our 3 (visual, auditory, audiovisual) X 2 (auditory vs. visual attention) factorial design separately. Hence for each run the statistical model included the following regressors: 3 block regressors for each of the 6 conditions (A_Att<sub>A</sub>, A_Att<sub>V</sub>, V_Att<sub>A</sub>, V_Att<sub>V</sub>, AV_Att<sub>A</sub>, AV_Att<sub>V</sub>), 4 event-related regressors for each target type under each attention condition (i.e. TargetA_Att<sub>A</sub>, TargetA_Att<sub>V</sub>, TargetV_Att<sub>A</sub>, TargetV_Att<sub>v</sub>) and one event-related regressor modelling any additional responses that participants produced in the absence of targets. We modelled the targets and additional responses as independent regressors in our first (i.e. subject) level general linear model (GLM) to minimize confounding effects of perceptual decision making, response selection and motor preparation on the activations reported for the A, V, AV looming activation blocks that are the focus of this communication. Nuisance covariates included the realignment parameters to account for residual motion artefacts.</p><p>All subsequent analyses included only the beta images pertaining to the HRF of the activation blocks in our 2 (modality specific attention) X 3 (stimulation modality) design: 3 blocks per condition per run X 6 conditions X 4 runs = 72 beta images per subject.</p><p>To minimize the possibility that attentional lapses reduced the sensitivity of our analysis we also repeated our neuroimaging analysis selectively for blocks that included neither misses (i.e. missed responses to targets of the attended modality) nor false alarms (i.e. responses to targets from the unattended modality). This control analysis basically showed similar results similar to those from the main analysis that is reported in this manuscript.</p></sec><sec id="s4-13"><title>Sampling of the BOLD-response along cortical depth</title><p>The 72 beta images (6 conditions X 3 blocks per condition per run X 4 runs) were co-registered to the MRI structural by applying the transformation matrix obtained from the mean EPI image co-registration and up-sampled to a 0.4 mm isometric resolution (4th degree b-spline interpolation). Finally, we sampled each beta image along vertices defined by the normal to the mid-cortical surface at the depths defined by the 6 intra-cortical surfaces from the MRI structural analysis (tri-linear interpolation) (see ‘MRI structural analysis’ above, <xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>In total, the fMRI data were therefore resampled 3 times for: 1. realignment+unwarping, 2. upsampling the beta images to 0.4 mm and 3. sampling the beta images along the surfaces. The smoothness estimation using AFNI 3dFWHMx gave the following results (FWHM mean and standard deviation across subjects in X/Y/Z): smoothness of the raw data: 1.70 (0.08), 2.05 (0.09), 1.65 (0.08) mm; smoothness after upsampling at 0.4 mm: 2.22 (0.14), 2.88 (0.31), 2.11 (0.21) mm. We were not able to estimate the smoothness of the data after sampling along the surface<italic>s.</italic> Note that no additional smoothing was applied beyond the one due to interpolation during pre-processing.</p><p>The activity values at the vertices of these 6 intra-cortical surfaces in our four ROIs (i.e. primary auditory, planum temporale, V1, V2/3) pooled over both hemispheres form the basis for our univariate ROI analysis of the laminar BOLD-response profiles and laminar decoding accuracy profiles (based on multivariate pattern classification).</p></sec><sec id="s4-14"><title>Laminar BOLD-response profiles for contrasts of interest</title><p>The laminar profiles of each ROI were obtained for each block per run by collapsing data over vertices. For each of our 72 beta images (6 conditions X 3 blocks per condition per run X 4 runs) we summarized the BOLD response at each of the six cortical depth levels in terms of the median of the parameter estimates across all vertices at this cortical depth within a particular ROI (n.b. each ROI pools over both hemispheres). The median was used as a summary index because the parameter estimates distribution was skewed especially for the most superficial laminae.</p><p>For each ROI and lamina, we computed the following contrasts of interest independently for the i<sup>th</sup> block of a given condition of the j<sup>th</sup> run:</p><list list-type="order"><list-item><p>The deactivation induced</p><list list-type="alpha-lower"><list-item><p>by auditory stimuli relative to fixation baseline irrespective of modality-specific attention: <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></p></list-item> <list-item><p>by visual stimuli relative to fixation baseline irrespective of modality-specific attention: <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></p></list-item></list> </list-item> <list-item><p>The crossmodal enhancement or suppression irrespective of attention, i.e. specifically for auditory and visual regions:</p><list list-type="alpha-lower"><list-item><p>A1 and PT: the visual-induced modulation of auditory activations irrespective of modality-specific attention:<inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></p></list-item> <list-item><p>In V1 and V2/3: the auditory-induced modulation of visual activations irrespective of modality-specific attention:<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></p></list-item></list> </list-item> <list-item><p>Attentional modulation irrespective of stimulus modality, that is specifically for auditory and visual regions</p><list list-type="alpha-lower"><list-item><p>A1 and PT: Modulation of stimulus responses by auditory relative to visual attention <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3.</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></p></list-item> <list-item><p>V1 and V2/3: Modulation of stimulus responses by visual relative to auditory attention <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></p></list-item></list> </list-item></list><p>For completeness, we also assessed whether the differences in BOLD-response for the contrasts listed 1 and 2 – in cases when they were significantly different from zero - depended on modality-specific attention (i.e. by testing for the interaction of sensory evoked responses or crossmodal enhancement with modality-specific attention). Moreover, we also show the activations &gt; fixation for auditory or visual stimuli in the <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> to confirm that our experimental manipulations were effective.</p></sec><sec id="s4-15"><title>Shape parameters for characterization of laminar BOLD response profiles</title><p>The laminar profile of each ROI was then summarized by parameters by collapsing across blocks per runs. For each statistical comparison, lamina and ROI, we thus obtained 12 (i.e. 3 blocks X 4 runs) contrast estimates (as the median over vertices within a bilateral ROI) per subject that is 12 laminar profiles. We refer to those parameter estimates that quantify the BOLD response for a particular lamina as BOLD response parameter estimates. For instance, <xref ref-type="fig" rid="fig2">Figure 2A</xref> (first row) shows the. B parameter estimates (across participants mean ± SEM) as a line plot for each of the six laminae as a function of cortical depth along the x-axis.</p><p>To characterize the overall shape of the laminar profile, we estimated for each subject a second-level general linear model (i.e. a laminar GLM) that modeled the activation in each lamina in a given ROI as dependent variable by two predictors: 1. a constant term (i.e. activation mean across laminae) and 2. a linear term characterizing a linear increase across laminae (mean-centered and orthogonalized with respect to the constant term). The parameter estimates of this 2nd or laminar GLM are referred to as shape parameters. For instance, <xref ref-type="fig" rid="fig2">Figure 2A</xref> (2nd row) shows the S parameter estimates as violin plots for the constant and the linear term. The S-parameters enable us to make inferences about the shape of a laminar BOLD-response profile rather than using an omnibus F-test that assesses whether the BOLD-response differs across any of the six laminae followed by numerous post hoc pairwise comparisons between layers. However, we acknowledge that this characterization makes our inference less specific about locating even the BOLD-response to a particular lamina. Moreover, because the laminar GLM in the current report did not include any higher order (e.g quadratic) terms they would not be to model U-shaped laminar profiles.</p></sec><sec id="s4-16"><title>Statistical analysis for the shape parameters at the between subject that is group level</title><p>To enable generalization to the population, we entered these shape parameters into linear mixed effects models at the group level separately for each contrast. To limit the number of statistical tests we used the following step-down approach:</p><list list-type="order"><list-item><p>We formed 2 (shape parameter: constant, linear) x 2 (ROI: primary, non-primary) linear mixed effects models separately for the six contrasts, that is 2 (sensory cortices: visual, auditory) x 3 (contrasts: crossmodal deactivation, crossmodal modulation, attentional modulation). In each linear mixed effects model we tested whether the constant or the linear parameter, each averaged across primary and higher order ROIs, was significantly different from zero using a two dimensional F-contrast. Hence, we computed three F-tests for visual cortices and three F-tests for auditory cortices.</p></list-item> <list-item><p>If a two dimensional F-test was significant, we computed follow-up one dimensional F-tests separately for the constant and the linear parameters (again averaged across primary and higher order sensory cortices).</p></list-item> <list-item><p>If a one dimensional F-test was significant, we computed follow-up t-tests separately for the primary and the higher order sensory cortices.</p></list-item></list><p>Based on our a-priori predictions (<xref ref-type="bibr" rid="bib43">Leitão et al., 2013</xref>) that auditory stimuli induce deactivations in visual areas and that visual stimuli induce deactivations in auditory areas, we employed one-sided t-tests in step 2 and 3 (i.e. negative constant term for contrasts). In all other cases unidirectional F-tests were used. Unless otherwise stated, we report statistical results as significant at p&lt;0.05.</p></sec><sec id="s4-17"><title>Multivariate analysis of pattern across vertices and decoding accuracy profiles</title><p>Multivariate pattern analyses were performed using a linear support vector classifier (SVC) that was trained in a leave-one-run-out cross-validation scheme (LIBSVM 3.21, <ext-link ext-link-type="uri" xlink:href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">https://www.csie.ntu.edu.tw/~cjlin/libsvm/</ext-link>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_010243">SCR_010243</ext-link>, with C = 1 and mean-centred activations for each feature separately for training and test sets).</p><p>We performed multivariate pattern analysis directly on the BOLD response patterns pooled over the two hemispheres, that is the patterns of B-parameters, independently at each of the six cortical depths to generate laminar profiles of decoding accuracy for the following comparisons:</p><list list-type="order"><list-item><p>[AV vs A]<sub>AttA, AttV</sub> (pooled over attended modality) for A1 and PT,</p></list-item> <list-item><p>[AV vs V]<sub>AttA, AttV</sub> (pooled over attended modality) for V1 and V2-3,</p></list-item> <list-item><p>[Att<sub>A</sub> vs Att<sub>V</sub>]<sub>A,V,AV</sub> (pooled over stimulation modality) for A1, PT, V1 and V2/3.</p></list-item></list><p>For completeness, we also assessed whether the classification performance for the comparisons listed above – in cases when they were significantly different from zero - depended on modality-specific attention by running the comparison for the equivalent contrast between the the two modality specific attention conditions (e.g the comparison [AV-V]<sub>att A</sub> vs [AV-V]<sub>att V</sub> for the one in the list above).</p><p>In line with our analysis of laminar BOLD response profiles, we modeled the laminar profiles of decoding accuracy using a laminar GLM with a constant term and a linear term as linear increase across laminae as predictors.</p><p>Again as in our analysis of laminar BOLD-response profiles, the ‘constant’ and ‘linear’ shape parameters characterizing the decoding accuracy profiles were entered into 2 (shape parameter: constant, linear) x 2 (ROI: primary, non-primary) linear mixed effects models separately for each of the four decoding comparisons, that is 2 (sensory cortices: visual, auditory) x 2 (contrasts: crossmodal modulation, attentional modulation). We then applied the step down procedure exactly as described in detail for the BOLD-response profiles. Please note that the laminar profiles of decoding accuracy need to be interpreted cautiously. First, decoding accuracy depends on BOLD-signal and noise characteristics that can both vary across laminae. Second, the laminar profile of decoding accuracy is more difficult to interpret, because accuracy is bounded between 0 and 1.</p><p>Raster plots: statistical relationship of BOLD response profiles between different conditions.</p><p>Next, we explored whether the response profile in a vertex as characterized by their ‘constant’ and ‘linear’ shape parameters in one condition is statistically predictive of this vertex laminar profile in another condition or contrast (see figure 5 from [<xref ref-type="bibr" rid="bib24">Fracasso et al., 2018</xref>]. For instance, we asked whether the magnitude of visual induced deactivations (e.g. [V-Fix] averaged over attention conditions) in a vertex predicts its crossmodal enhancement (e.g. [AV-A] averaged over attention conditions). For this, we sorted and averaged the vertices in percentile-like bins based on the value of a shape parameter (e.g. ‘constant’ or ‘linear’) for a particular sorting contrast (e.g. [V–Fix]).</p><p>This bin order was then used to sort and average the values of the 2nd contrast (e.g. [AV-A]). We entered the shape parameter value (e.g. ‘constant’ or ‘linear’) for the sorting contrast (e.g. [V–Fix]) for each bin as a linear regressor (+ a constant) to predict the corresponding shape parameter in each bin in the sorted contrast (e.g. [AV-A]) in a subject-specific general linear model.</p><p>To allow for generalization to the population, we entered the parameter for the linear term (i.e. slope of regression line) of this regression model into one sample t-tests at the group level.</p><p>We illustrated the statistical relationships of the laminar profiles between different contrasts at the group level in raster plots by averaging the laminar profiles for each bin across subjects of the sorting (i.e. predicting) and the sorted (i.e. predicted) values. For instance, in <xref ref-type="fig" rid="fig3">Figure 3B</xref> i (right) we sorted the vertices according to [V-fix] in PT such that the constant parameter (i.e. average BOLD response across laminae) increases from bottom to top (i.e. predicting contrast). Unsurprisingly, the raster plot for the [V-Fix] panel as the predicting contrast thus goes from a negative BOLD response profile (i.e. coded in blue) in the bottom rows of the panel to a positive BOLD response profile (i.e. coded in red) in the top rows of the panel. Under the null-hypothesis the laminar BOLD response profile for the predicting contrast (e.g. [V-fix]) in a vertex is unrelated to its laminar BOLD response profile for the predicted contrast (e.g. [AV-A]) and we would not expect any structure in the raster plots for the predicted contract (e.g. [AV-A], n.b. we accounted for spurious correlation by performing all these analyses in a crossvalidated fashion). Conversely, if the shape parameter for [V-fix] in a vertex significantly predicts its shape parameters for [AV-A], we would expect a structured raster plot also for [AV-A].</p><p>Hence, these raster plots can reveal additional BOLD signal structure that is averaged out by the group surface projections: while group surface projections (see next section, or <xref ref-type="fig" rid="fig3">Figure 3B</xref> left) reveal only spatial topographies which are consistent across participants, the raster plots reveal whether the similarity between patterns of laminar profiles from different conditions is consistent across subjects, even when the activation patterns themselves are not similar across subjects. In other words, raster plots illustrate the similarity or covariance between patterns that is consistent across subjects even when the patterns themselves vary across subjects.</p><p>Please note that both the regression model and the raster plots were computed in a leave one day out cross-validation to avoid biases and spurious correlations between sorting and sorted contrast. The number of bins was set to the smallest number of vertices found within an ROI (pooled over hemispheres) across subjects. For visualization purposes all depicted raster plots were smoothed along the vertical axis (FWHM = 1% of the number of data bins).</p><p>We also note that the statistical results were basically equivalent without any binning (i.e. directly entering the shape parameter values of vertices into the GLM), the binning was applied only to enable illustration of the results in raster plots.</p><p>These regression models over vertices and raster plots were used to evaluate whether crossmodal modulation for auditory or visual stimuli in A1, PT, V1 or V2/3 depended on the unisensory visual or auditory response.</p></sec><sec id="s4-18"><title>Intersubject registration for surface projection of group results</title><p>To visualize patterns of the laminar profile shape parameters (i.e. ‘constant’ or ‘linear’) that are consistent across subjects on the cortical surface, we transformed the individual surface projections into a standardized study group space using a multimodal, multi-contrast surface registration (MMSR) approach (<xref ref-type="bibr" rid="bib79">Tardif et al., 2015</xref>). The transformation matrix for group normalization was computed for the level-set corresponding to the mid-cortical surface and the high-resolution T1 maps. The T1 map was sampled radially along the cortical profile, averaged between 20% and 80% cortical depth and smoothed tangentially (FWHM = 1.5 mm). To reduce computation time, the level-set and the T1 map data were down-sampled to a resolution of 0.8 mm isotropic. We used a two-stage registration. In the first step, we computed an intermediate mean surface by using the median subject in the study group as the initial target. In the second step, MMSR was repeated using this intermediate mean surface as the new target.</p><p>To apply this registration to our fMRI data, we computed the mean beta images for each of our six conditions, sampled them at the depths defined by the six intra-cortical surfaces (trilinear interpolation) and then down-sampled to a resolution of 0.8 mm isotropic. We then used the deformation field resulting from the second stage MMSR to transform those 36 images per subject (six conditions X six depths) into the group surface template space (trilinear interpolation). Mass univariate laminar GLM were then performed in normalized group space and the parameters corresponding to the ‘constant’ and ‘linear’ terms were averaged across subjects for each vertex.</p></sec><sec id="s4-19"><title>Materials and data availability</title><p>The raw data of the results presented here are available in a BIDS format upon request (remi.gau@gmail.com). Beta values extracted from our layers/ROIs for each participant as well as the summary data necessary to reproduce our figures have been uploaded as CSV or mat files on the open-science framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/63dba/">https://osf.io/63dba/</ext-link>). Group average statistical maps are available in an NIDM format from neurovault (<ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/5209/">https://neurovault.org/collections/5209/</ext-link>).</p><p>The results of the quality control MRIQC pipeline (<ext-link ext-link-type="uri" xlink:href="https://mriqc.readthedocs.io/en/stable/">https://mriqc.readthedocs.io/en/stable/</ext-link>) on the BOLD data as well as information about motion and framewise displacement during scanning is also available from the repository.</p><p>The code for the analysis is available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3581319">https://doi.org/10.5281/zenodo.3581319</ext-link>. The code to run the fMRI experiment is available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3581316">https://doi.org/10.5281/zenodo.3581316</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This project was funded by the ERC starter grant (mult-sens) and the Max Planck Society.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Resources, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Software, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Methodology</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Validation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All procedures were approved by the Ethics Committee of the University of Leipzig under the protocol number 273-14: &quot;Magnetresonanz-Untersuchungen am Menschen bei 7 Tesla&quot;. Participants gave written informed consent to participate in this fMRI study.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Behavioural results.</title><p>Notes: Percentage of target responses (mean and STD across subjects) in the six conditions of our 2 × 3 experimental design. n = 11 Please note that responses to visual targets under auditory attention and auditory targets under visual attention are false alarms.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-46856-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>ROI size and coverage.</title><p>Notes: ‘Number of vertices’ refers to the vertices with valid data at all the sampled cortical depths. This vertex count was divided by the total number of vertices included in the initial ROI definition to compute the ‘Fraction of the ROI covered’. Note that those numbers are pooled over both hemispheres. n = 11</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-46856-supp2-v2.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Auditory and visual activations.</title><p>Using 2 (shape parameter: constant, linear) x 2 (ROI: primary, non-primary) linear mixed effects models, we performed the following statistical comparisons in a 'step down procedure':</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-46856-supp3-v2.docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Cross-modal modulation in visual areas.</title><p>Using 2 (shape parameter: constant, linear) x 2 (ROI: primary, non-primary) linear mixed effects models, we performed the following statistical comparisons in a 'step down procedure':</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-46856-supp4-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-46856-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data (sufficient to recreate figures) are publicly available on the OSF project of this study: <ext-link ext-link-type="uri" xlink:href="https://osf.io/63dba/">https://osf.io/63dba/</ext-link>. The raw data of the results presented here are available in a BIDS format upon request: the consent form originally signed by the participants did not allow for making raw data publicly available.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Gau</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>AV - attention - 7T</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="archive" xlink:href="https://osf.io/63dba/">63dba</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atilgan</surname> <given-names>H</given-names></name><name><surname>Town</surname> <given-names>SM</given-names></name><name><surname>Wood</surname> <given-names>KC</given-names></name><name><surname>Jones</surname> <given-names>GP</given-names></name><name><surname>Maddox</surname> <given-names>RK</given-names></name><name><surname>Lee</surname> <given-names>AKC</given-names></name><name><surname>Bizley</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Integration of visual information in auditory cortex promotes auditory scene analysis through multisensory binding</article-title><source>Neuron</source><volume>97</volume><fpage>640</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.12.034</pub-id><pub-id pub-id-type="pmid">29395914</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bazin</surname> <given-names>PL</given-names></name><name><surname>Weiss</surname> <given-names>M</given-names></name><name><surname>Dinse</surname> <given-names>J</given-names></name><name><surname>Schäfer</surname> <given-names>A</given-names></name><name><surname>Trampel</surname> <given-names>R</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A computational framework for ultra-high resolution cortical segmentation at 7tesla</article-title><source>NeuroImage</source><volume>93 Pt 2</volume><fpage>201</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.077</pub-id><pub-id pub-id-type="pmid">23623972</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname> <given-names>MS</given-names></name><name><surname>Lee</surname> <given-names>KE</given-names></name><name><surname>Argall</surname> <given-names>BD</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Integration of auditory and visual information about objects in superior temporal sulcus</article-title><source>Neuron</source><volume>41</volume><fpage>809</fpage><lpage>823</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(04)00070-4</pub-id><pub-id pub-id-type="pmid">15003179</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname> <given-names>JK</given-names></name><name><surname>Nodal</surname> <given-names>FR</given-names></name><name><surname>Bajo</surname> <given-names>VM</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Physiological and anatomical evidence for multisensory interactions in auditory cortex</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2172</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl128</pub-id><pub-id pub-id-type="pmid">17135481</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bressler</surname> <given-names>DW</given-names></name><name><surname>Fortenbaugh</surname> <given-names>FC</given-names></name><name><surname>Robertson</surname> <given-names>LC</given-names></name><name><surname>Silver</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visual spatial attention enhances the amplitude of positive and negative fMRI responses to visual stimulation in an eccentricity-dependent manner</article-title><source>Vision Research</source><volume>85</volume><fpage>104</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2013.03.009</pub-id><pub-id pub-id-type="pmid">23562388</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Budinger</surname> <given-names>E</given-names></name><name><surname>Heil</surname> <given-names>P</given-names></name><name><surname>Hess</surname> <given-names>A</given-names></name><name><surname>Scheich</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Multisensory processing via early cortical stages: connections of the primary auditory cortical field with other sensory systems</article-title><source>Neuroscience</source><volume>143</volume><fpage>1065</fpage><lpage>1083</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2006.08.035</pub-id><pub-id pub-id-type="pmid">17027173</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname> <given-names>JS</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name><name><surname>Fiebelkorn</surname> <given-names>IC</given-names></name><name><surname>Mercier</surname> <given-names>MR</given-names></name><name><surname>Molholm</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Multisensory representation of frequency across audition and touch: high density electrical mapping reveals early sensory-perceptual coupling</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>15338</fpage><lpage>15344</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1796-12.2012</pub-id><pub-id pub-id-type="pmid">23115172</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campi</surname> <given-names>KL</given-names></name><name><surname>Bales</surname> <given-names>KL</given-names></name><name><surname>Grunewald</surname> <given-names>R</given-names></name><name><surname>Krubitzer</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Connections of auditory and visual cortex in the prairie vole (Microtus ochrogaster): evidence for multisensory processing in primary sensory Areas</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>89</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp082</pub-id><pub-id pub-id-type="pmid">19395525</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cappe</surname> <given-names>C</given-names></name><name><surname>Thelen</surname> <given-names>A</given-names></name><name><surname>Romei</surname> <given-names>V</given-names></name><name><surname>Thut</surname> <given-names>G</given-names></name><name><surname>Murray</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Looming signals reveal synergistic principles of multisensory integration</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>1171</fpage><lpage>1182</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5517-11.2012</pub-id><pub-id pub-id-type="pmid">22279203</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>G</given-names></name><name><surname>Wang</surname> <given-names>F</given-names></name><name><surname>Gore</surname> <given-names>JC</given-names></name><name><surname>Roe</surname> <given-names>AW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Layer-specific BOLD activation in awake monkey V1 revealed by ultra-high spatial resolution functional magnetic resonance imaging</article-title><source>NeuroImage</source><volume>64</volume><fpage>147</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.08.060</pub-id><pub-id pub-id-type="pmid">22960152</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ciaramitaro</surname> <given-names>VM</given-names></name><name><surname>Buracas</surname> <given-names>GT</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatial and cross-modal attention alter responses to unattended sensory information in early visual and auditory human cortex</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>2399</fpage><lpage>2413</lpage><pub-id pub-id-type="doi">10.1152/jn.00580.2007</pub-id><pub-id pub-id-type="pmid">17715196</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Frequency preference and attention effects across cortical depths in the human primary auditory cortex</article-title><source>PNAS</source><volume>112</volume><fpage>16036</fpage><lpage>16041</lpage><pub-id pub-id-type="doi">10.1073/pnas.1507552112</pub-id><pub-id pub-id-type="pmid">26668397</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>Xu</surname> <given-names>J</given-names></name><name><surname>van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>High-Resolution mapping of myeloarchitecture in vivo: localization of auditory Areas in the human brain</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3394</fpage><lpage>3405</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu150</pub-id><pub-id pub-id-type="pmid">24994817</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneux</surname> <given-names>T</given-names></name><name><surname>Harrell</surname> <given-names>ER</given-names></name><name><surname>Kempf</surname> <given-names>A</given-names></name><name><surname>Ceballo</surname> <given-names>S</given-names></name><name><surname>Filipchuk</surname> <given-names>A</given-names></name><name><surname>Bathellier</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Context-dependent signaling of coincident auditory and visual events in primary visual cortex</article-title><source>eLife</source><volume>8</volume><elocation-id>e44006</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.44006</pub-id><pub-id pub-id-type="pmid">31115334</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dick</surname> <given-names>F</given-names></name><name><surname>Tierney</surname> <given-names>AT</given-names></name><name><surname>Lutti</surname> <given-names>A</given-names></name><name><surname>Josephs</surname> <given-names>O</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Weiskopf</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>In vivo functional and myeloarchitectonic mapping of human primary auditory Areas</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>16095</fpage><lpage>16105</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1712-12.2012</pub-id><pub-id pub-id-type="pmid">23152594</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driver</surname> <given-names>J</given-names></name><name><surname>Noesselt</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multisensory interplay reveals crossmodal influences on 'sensory-specific' brain regions, neural responses, and judgments</article-title><source>Neuron</source><volume>57</volume><fpage>11</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.12.013</pub-id><pub-id pub-id-type="pmid">18184561</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duong</surname> <given-names>TQ</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Adriany</surname> <given-names>G</given-names></name><name><surname>Hu</surname> <given-names>X</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Kim</surname> <given-names>SG</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Microvascular BOLD contribution at 4 and 7 T in the human brain: gradient-echo and spin-echo fMRI with suppression of blood effects</article-title><source>Magnetic Resonance in Medicine</source><volume>49</volume><fpage>1019</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.1002/mrm.10472</pub-id><pub-id pub-id-type="pmid">12768579</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname> <given-names>MO</given-names></name><name><surname>Bülthoff</surname> <given-names>HH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Merging the senses into a robust percept</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>162</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.02.002</pub-id><pub-id pub-id-type="pmid">15050512</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname> <given-names>SL</given-names></name><name><surname>Macaluso</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spatial attention can modulate audiovisual integration at multiple cortical and subcortical sites</article-title><source>European Journal of Neuroscience</source><volume>29</volume><fpage>1247</fpage><lpage>1257</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2009.06688.x</pub-id><pub-id pub-id-type="pmid">19302160</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falchier</surname> <given-names>A</given-names></name><name><surname>Clavagnier</surname> <given-names>S</given-names></name><name><surname>Barone</surname> <given-names>P</given-names></name><name><surname>Kennedy</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Anatomical evidence of multimodal integration in primate striate cortex</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>5749</fpage><lpage>5759</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-13-05749.2002</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname> <given-names>L</given-names></name><name><surname>Li</surname> <given-names>H</given-names></name><name><surname>Zhuo</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Wang</surname> <given-names>J</given-names></name><name><surname>Chen</surname> <given-names>L</given-names></name><name><surname>Yang</surname> <given-names>Z</given-names></name><name><surname>Chu</surname> <given-names>C</given-names></name><name><surname>Xie</surname> <given-names>S</given-names></name><name><surname>Laird</surname> <given-names>AR</given-names></name><name><surname>Fox</surname> <given-names>PT</given-names></name><name><surname>Eickhoff</surname> <given-names>SB</given-names></name><name><surname>Yu</surname> <given-names>C</given-names></name><name><surname>Jiang</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The human brainnetome atlas: a new brain atlas based on connectional architecture</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3508</fpage><lpage>3526</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw157</pub-id><pub-id pub-id-type="pmid">27230218</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname> <given-names>CR</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural correlates of reliability-based cue weighting during multisensory integration</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>146</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1038/nn.2983</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Formisano</surname> <given-names>E</given-names></name><name><surname>Kim</surname> <given-names>DS</given-names></name><name><surname>Di Salle</surname> <given-names>F</given-names></name><name><surname>van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Mirror-symmetric tonotopic maps in human primary auditory cortex</article-title><source>Neuron</source><volume>40</volume><fpage>859</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00669-X</pub-id><pub-id pub-id-type="pmid">14622588</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fracasso</surname> <given-names>A</given-names></name><name><surname>Luijten</surname> <given-names>PR</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Petridou</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Laminar imaging of positive and negative BOLD in human visual cortex at 7T</article-title><source>NeuroImage</source><volume>164</volume><fpage>100</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.02.038</pub-id><pub-id pub-id-type="pmid">28213112</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Zarahn</surname> <given-names>E</given-names></name><name><surname>Josephs</surname> <given-names>O</given-names></name><name><surname>Henson</surname> <given-names>RN</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Stochastic designs in event-related fMRI</article-title><source>NeuroImage</source><volume>10</volume><fpage>607</fpage><lpage>619</lpage><pub-id pub-id-type="doi">10.1006/nimg.1999.0498</pub-id><pub-id pub-id-type="pmid">10547338</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Is neocortex essentially multisensory?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>278</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.04.008</pub-id><pub-id pub-id-type="pmid">16713325</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goense</surname> <given-names>J</given-names></name><name><surname>Merkle</surname> <given-names>H</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>High-resolution fMRI reveals laminar differences in neurovascular coupling between positive and negative BOLD responses</article-title><source>Neuron</source><volume>76</volume><fpage>629</fpage><lpage>639</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.019</pub-id><pub-id pub-id-type="pmid">23141073</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hackett</surname> <given-names>TA</given-names></name><name><surname>Preuss</surname> <given-names>TM</given-names></name><name><surname>Kaas</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Architectonic identification of the core region in auditory cortex of macaques, chimpanzees, and humans</article-title><source>The Journal of Comparative Neurology</source><volume>441</volume><fpage>197</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1002/cne.1407</pub-id><pub-id pub-id-type="pmid">11745645</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harel</surname> <given-names>N</given-names></name><name><surname>Lin</surname> <given-names>J</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Combined imaging-histological study of cortical laminar specificity of fMRI signals</article-title><source>NeuroImage</source><volume>29</volume><fpage>879</fpage><lpage>887</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.08.016</pub-id><pub-id pub-id-type="pmid">16194614</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname> <given-names>L</given-names></name><name><surname>Handwerker</surname> <given-names>DA</given-names></name><name><surname>Jangraw</surname> <given-names>DC</given-names></name><name><surname>Chen</surname> <given-names>G</given-names></name><name><surname>Hall</surname> <given-names>A</given-names></name><name><surname>Stüber</surname> <given-names>C</given-names></name><name><surname>Gonzalez-Castillo</surname> <given-names>J</given-names></name><name><surname>Ivanov</surname> <given-names>D</given-names></name><name><surname>Marrett</surname> <given-names>S</given-names></name><name><surname>Guidi</surname> <given-names>M</given-names></name><name><surname>Goense</surname> <given-names>J</given-names></name><name><surname>Poser</surname> <given-names>BA</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>High-Resolution CBV-fMRI allows mapping of laminar activity and connectivity of cortical input and output in human M1</article-title><source>Neuron</source><volume>96</volume><fpage>1253</fpage><lpage>1263</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.005</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ibrahim</surname> <given-names>LA</given-names></name><name><surname>Mesik</surname> <given-names>L</given-names></name><name><surname>Ji</surname> <given-names>XY</given-names></name><name><surname>Fang</surname> <given-names>Q</given-names></name><name><surname>Li</surname> <given-names>HF</given-names></name><name><surname>Li</surname> <given-names>YT</given-names></name><name><surname>Zingg</surname> <given-names>B</given-names></name><name><surname>Zhang</surname> <given-names>LI</given-names></name><name><surname>Tao</surname> <given-names>HW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cross-Modality sharpening of visual cortical processing through Layer-1-Mediated inhibition and disinhibition</article-title><source>Neuron</source><volume>89</volume><fpage>1031</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.01.027</pub-id><pub-id pub-id-type="pmid">26898778</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iurilli</surname> <given-names>G</given-names></name><name><surname>Ghezzi</surname> <given-names>D</given-names></name><name><surname>Olcese</surname> <given-names>U</given-names></name><name><surname>Lassi</surname> <given-names>G</given-names></name><name><surname>Nazzaro</surname> <given-names>C</given-names></name><name><surname>Tonini</surname> <given-names>R</given-names></name><name><surname>Tucci</surname> <given-names>V</given-names></name><name><surname>Benfenati</surname> <given-names>F</given-names></name><name><surname>Medini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sound-driven synaptic inhibition in primary visual cortex</article-title><source>Neuron</source><volume>73</volume><fpage>814</fpage><lpage>828</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.026</pub-id><pub-id pub-id-type="pmid">22365553</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>JA</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Attention to simultaneous unrelated auditory and visual events: behavioral and neural correlates</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>1609</fpage><lpage>1620</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi039</pub-id><pub-id pub-id-type="pmid">15716469</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Petkov</surname> <given-names>CI</given-names></name><name><surname>Augath</surname> <given-names>M</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Functional imaging reveals visual modulation of specific fields in auditory cortex</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>1824</fpage><lpage>1835</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4737-06.2007</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Petkov</surname> <given-names>CI</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual modulation of neurons in auditory cortex</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>1560</fpage><lpage>1574</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm187</pub-id><pub-id pub-id-type="pmid">18180245</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual enhancement of the information representation in auditory cortex</article-title><source>Current Biology</source><volume>20</volume><fpage>19</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.10.068</pub-id><pub-id pub-id-type="pmid">20036538</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Bains</surname> <given-names>LJ</given-names></name><name><surname>van Mourik</surname> <given-names>T</given-names></name><name><surname>Norris</surname> <given-names>DG</given-names></name><name><surname>De Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Selective activation of the deep layers of the human primary visual cortex by Top-Down feedback</article-title><source>Current Biology</source><volume>26</volume><fpage>371</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.12.038</pub-id><pub-id pub-id-type="pmid">26832438</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koopmans</surname> <given-names>PJ</given-names></name><name><surname>Barth</surname> <given-names>M</given-names></name><name><surname>Norris</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Layer-specific BOLD activation in human V1</article-title><source>Human Brain Mapping</source><volume>31</volume><fpage>1297</fpage><lpage>1304</lpage><pub-id pub-id-type="doi">10.1002/hbm.20936</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>Chen</surname> <given-names>CM</given-names></name><name><surname>O'Connell</surname> <given-names>MN</given-names></name><name><surname>Mills</surname> <given-names>A</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neuronal oscillations and multisensory interaction in primary auditory cortex</article-title><source>Neuron</source><volume>53</volume><fpage>279</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.12.011</pub-id><pub-id pub-id-type="pmid">17224408</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>O'Connell</surname> <given-names>MN</given-names></name><name><surname>Barczak</surname> <given-names>A</given-names></name><name><surname>Mills</surname> <given-names>A</given-names></name><name><surname>Javitt</surname> <given-names>DC</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The leading sense: supramodal control of neurophysiological context by attention</article-title><source>Neuron</source><volume>64</volume><fpage>419</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.10.014</pub-id><pub-id pub-id-type="pmid">19914189</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landman</surname> <given-names>BA</given-names></name><name><surname>Bogovic</surname> <given-names>JA</given-names></name><name><surname>Carass</surname> <given-names>A</given-names></name><name><surname>Chen</surname> <given-names>M</given-names></name><name><surname>Roy</surname> <given-names>S</given-names></name><name><surname>Shiee</surname> <given-names>N</given-names></name><name><surname>Yang</surname> <given-names>Z</given-names></name><name><surname>Kishore</surname> <given-names>B</given-names></name><name><surname>Pham</surname> <given-names>D</given-names></name><name><surname>Bazin</surname> <given-names>PL</given-names></name><name><surname>Resnick</surname> <given-names>SM</given-names></name><name><surname>Prince</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>System for integrated neuroimaging analysis and processing of structure</article-title><source>Neuroinformatics</source><volume>11</volume><fpage>91</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1007/s12021-012-9159-9</pub-id><pub-id pub-id-type="pmid">22932976</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laurienti</surname> <given-names>PJ</given-names></name><name><surname>Burdette</surname> <given-names>JH</given-names></name><name><surname>Wallace</surname> <given-names>MT</given-names></name><name><surname>Yen</surname> <given-names>YF</given-names></name><name><surname>Field</surname> <given-names>AS</given-names></name><name><surname>Stein</surname> <given-names>BE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Deactivation of sensory-specific cortex by cross-modal stimuli</article-title><source>Journal of Cognitive Neuroscience</source><volume>14</volume><fpage>420</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1162/089892902317361930</pub-id><pub-id pub-id-type="pmid">11970801</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leitão</surname> <given-names>J</given-names></name><name><surname>Thielscher</surname> <given-names>A</given-names></name><name><surname>Werner</surname> <given-names>S</given-names></name><name><surname>Pohmann</surname> <given-names>R</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Effects of parietal TMS on visual and auditory processing at the primary cortical level -- a concurrent TMS-fMRI study</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>873</fpage><lpage>884</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs078</pub-id><pub-id pub-id-type="pmid">22490546</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leo</surname> <given-names>F</given-names></name><name><surname>Romei</surname> <given-names>V</given-names></name><name><surname>Freeman</surname> <given-names>E</given-names></name><name><surname>Ladavas</surname> <given-names>E</given-names></name><name><surname>Driver</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Looming sounds enhance orientation sensitivity for visual stimuli on the same side as such sounds</article-title><source>Experimental Brain Research</source><volume>213</volume><fpage>193</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1007/s00221-011-2742-8</pub-id><pub-id pub-id-type="pmid">21643714</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname> <given-names>R</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Audiovisual synchrony improves motion discrimination via enhanced connectivity between early visual and auditory Areas</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>12329</fpage><lpage>12339</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5745-09.2010</pub-id><pub-id pub-id-type="pmid">20844129</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname> <given-names>M</given-names></name><name><surname>Mouraux</surname> <given-names>A</given-names></name><name><surname>Hu</surname> <given-names>L</given-names></name><name><surname>Iannetti</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Primary sensory cortices contain distinguishable spatial patterns of activity for each sense</article-title><source>Nature Communications</source><volume>4</volume><elocation-id>e1979</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms2979</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>What we can do and what we cannot do with fMRI</article-title><source>Nature</source><volume>453</volume><fpage>869</fpage><lpage>878</lpage><pub-id pub-id-type="doi">10.1038/nature06976</pub-id><pub-id pub-id-type="pmid">18548064</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucas</surname> <given-names>BC</given-names></name><name><surname>Bogovic</surname> <given-names>JA</given-names></name><name><surname>Carass</surname> <given-names>A</given-names></name><name><surname>Bazin</surname> <given-names>PL</given-names></name><name><surname>Prince</surname> <given-names>JL</given-names></name><name><surname>Pham</surname> <given-names>DL</given-names></name><name><surname>Landman</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The java image science toolkit (JIST) for rapid prototyping and publishing of neuroimaging software</article-title><source>Neuroinformatics</source><volume>8</volume><fpage>5</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1007/s12021-009-9061-2</pub-id><pub-id pub-id-type="pmid">20077162</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maier</surname> <given-names>JX</given-names></name><name><surname>Neuhoff</surname> <given-names>JG</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Multisensory integration of looming signals by rhesus monkeys</article-title><source>Neuron</source><volume>43</volume><fpage>177</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.06.027</pub-id><pub-id pub-id-type="pmid">15260954</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maier</surname> <given-names>JX</given-names></name><name><surname>Chandrasekaran</surname> <given-names>C</given-names></name><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Integration of bimodal looming signals through neuronal coherence in the temporal lobe</article-title><source>Current Biology</source><volume>18</volume><fpage>963</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.05.043</pub-id><pub-id pub-id-type="pmid">18585039</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markuerkiaga</surname> <given-names>I</given-names></name><name><surname>Barth</surname> <given-names>M</given-names></name><name><surname>Norris</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A cortical vascular model for examining the specificity of the laminar BOLD signal</article-title><source>NeuroImage</source><volume>132</volume><fpage>491</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.02.073</pub-id><pub-id pub-id-type="pmid">26952195</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marques</surname> <given-names>JP</given-names></name><name><surname>Kober</surname> <given-names>T</given-names></name><name><surname>Krueger</surname> <given-names>G</given-names></name><name><surname>van der Zwaag</surname> <given-names>W</given-names></name><name><surname>Van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Gruetter</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>MP2RAGE, a self bias-field corrected sequence for improved segmentation and T1-mapping at high field</article-title><source>NeuroImage</source><volume>49</volume><fpage>1271</fpage><lpage>1281</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.10.002</pub-id><pub-id pub-id-type="pmid">19819338</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McAuliffe</surname> <given-names>MJ</given-names></name><name><surname>Lalonde</surname> <given-names>FM</given-names></name><name><surname>McGarry</surname> <given-names>D</given-names></name><name><surname>Gandler</surname> <given-names>W</given-names></name><name><surname>Csaky</surname> <given-names>K</given-names></name><name><surname>Trus</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Medical image processing, analysis and visualization in clinical research</article-title><conf-name>IEEE Computer-Based Medical Systems (CBMS)</conf-name><fpage>381</fpage><lpage>386</lpage></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijer</surname> <given-names>GT</given-names></name><name><surname>Montijn</surname> <given-names>JS</given-names></name><name><surname>Pennartz</surname> <given-names>CMA</given-names></name><name><surname>Lansink</surname> <given-names>CS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Audiovisual modulation in mouse primary visual cortex depends on Cross-Modal stimulus configuration and congruency</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8783</fpage><lpage>8796</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0468-17.2017</pub-id><pub-id pub-id-type="pmid">28821672</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijer</surname> <given-names>D</given-names></name><name><surname>Veselič</surname> <given-names>S</given-names></name><name><surname>Calafiore</surname> <given-names>C</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Integration of audiovisual spatial signals is not consistent with maximum likelihood estimation</article-title><source>Cortex</source><volume>119</volume><fpage>74</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2019.03.026</pub-id><pub-id pub-id-type="pmid">31082680</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mercier</surname> <given-names>MR</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name><name><surname>Fiebelkorn</surname> <given-names>IC</given-names></name><name><surname>Butler</surname> <given-names>JS</given-names></name><name><surname>Schwartz</surname> <given-names>TH</given-names></name><name><surname>Molholm</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Auditory-driven phase reset in visual cortex: human electrocorticography reveals mechanisms of early multisensory integration</article-title><source>NeuroImage</source><volume>79</volume><fpage>19</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.060</pub-id><pub-id pub-id-type="pmid">23624493</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrill</surname> <given-names>RJ</given-names></name><name><surname>Hasenstaub</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual information present in infragranular layers of mouse auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>2854</fpage><lpage>2862</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3102-17.2018</pub-id><pub-id pub-id-type="pmid">29440554</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mozolic</surname> <given-names>JL</given-names></name><name><surname>Joyner</surname> <given-names>D</given-names></name><name><surname>Hugenschmidt</surname> <given-names>CE</given-names></name><name><surname>Peiffer</surname> <given-names>AM</given-names></name><name><surname>Kraft</surname> <given-names>RA</given-names></name><name><surname>Maldjian</surname> <given-names>JA</given-names></name><name><surname>Laurienti</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cross-modal deactivations during modality-specific selective attention</article-title><source>BMC Neurology</source><volume>8</volume><elocation-id>e35</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2377-8-35</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muckli</surname> <given-names>L</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Vizioli</surname> <given-names>L</given-names></name><name><surname>Petro</surname> <given-names>LS</given-names></name><name><surname>Smith</surname> <given-names>FW</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Contextual feedback to superficial layers of V1</article-title><source>Current Biology</source><volume>25</volume><fpage>2690</fpage><lpage>2695</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.057</pub-id><pub-id pub-id-type="pmid">26441356</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname> <given-names>NG</given-names></name><name><surname>Kleinschmidt</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The attentional 'spotlight's' penumbra: center-surround modulation in striate cortex</article-title><source>NeuroReport</source><volume>15</volume><fpage>977</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1097/00001756-200404290-00009</pub-id><pub-id pub-id-type="pmid">15076718</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musacchia</surname> <given-names>G</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neuronal mechanisms, response dynamics and perceptual functions of multisensory interactions in auditory cortex</article-title><source>Hearing Research</source><volume>258</volume><fpage>72</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2009.06.018</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieuwenhuys</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The myeloarchitectonic studies on the human cerebral cortex of the Vogt-Vogt school, and their significance for the interpretation of functional neuroimaging data</article-title><source>Brain Structure and Function</source><volume>218</volume><fpage>303</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1007/s00429-012-0460-z</pub-id><pub-id pub-id-type="pmid">23076375</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noesselt</surname> <given-names>T</given-names></name><name><surname>Rieger</surname> <given-names>JW</given-names></name><name><surname>Schoenfeld</surname> <given-names>MA</given-names></name><name><surname>Kanowski</surname> <given-names>M</given-names></name><name><surname>Hinrichs</surname> <given-names>H</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Driver</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Audiovisual temporal correspondence modulates human multisensory superior temporal sulcus plus primary sensory cortices</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>11431</fpage><lpage>11441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2252-07.2007</pub-id><pub-id pub-id-type="pmid">17942738</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polimeni</surname> <given-names>JR</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Greve</surname> <given-names>DN</given-names></name><name><surname>Wald</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Laminar analysis of 7T BOLD using an imposed spatial activation pattern in human V1</article-title><source>NeuroImage</source><volume>52</volume><fpage>1334</fpage><lpage>1346</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.005</pub-id><pub-id pub-id-type="pmid">20460157</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rockland</surname> <given-names>KS</given-names></name><name><surname>Ojima</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Multisensory convergence in calcarine visual areas in macaque monkey</article-title><source>International Journal of Psychophysiology</source><volume>50</volume><fpage>19</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/S0167-8760(03)00121-1</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname> <given-names>T</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical hierarchies perform bayesian causal inference in multisensory perception</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002073</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002073</pub-id><pub-id pub-id-type="pmid">25710328</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname> <given-names>T</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distinct computational principles govern multisensory integration in primary sensory and association cortices</article-title><source>Current Biology</source><volume>26</volume><fpage>509</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.12.056</pub-id><pub-id pub-id-type="pmid">26853368</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname> <given-names>T</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reliability-Weighted integration of audiovisual signals can be modulated by Top-down attention</article-title><source>Eneuro</source><volume>5</volume><elocation-id>ENEURO.0315-17.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0315-17.2018</pub-id><pub-id pub-id-type="pmid">29527567</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Smiley</surname> <given-names>J</given-names></name><name><surname>Fu</surname> <given-names>KG</given-names></name><name><surname>McGinnis</surname> <given-names>T</given-names></name><name><surname>O'Connell</surname> <given-names>MN</given-names></name><name><surname>Hackett</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Anatomical mechanisms and functional implications of multisensory convergence in early cortical processing</article-title><source>International Journal of Psychophysiology</source><volume>50</volume><fpage>5</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1016/S0167-8760(03)00120-X</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The timing and laminar profile of converging inputs to multisensory Areas of the macaque neocortex</article-title><source>Cognitive Brain Research</source><volume>14</volume><fpage>187</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(02)00073-3</pub-id><pub-id pub-id-type="pmid">12063142</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shmuel</surname> <given-names>A</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Pfeuffer</surname> <given-names>J</given-names></name><name><surname>Van de Moortele</surname> <given-names>P-F</given-names></name><name><surname>Adriany</surname> <given-names>G</given-names></name><name><surname>Hu</surname> <given-names>X</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Sustained Negative BOLD, Blood Flow and Oxygen Consumption Response and Its Coupling to the Positive Response in the Human Brain</article-title><source>Neuron</source><volume>36</volume><fpage>1195</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01061-9</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shomstein</surname> <given-names>S</given-names></name><name><surname>Yantis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Control of attention shifts between vision and audition in human cortex</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>10702</fpage><lpage>10706</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2939-04.2004</pub-id><pub-id pub-id-type="pmid">15564587</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname> <given-names>MA</given-names></name><name><surname>Shenhav</surname> <given-names>A</given-names></name><name><surname>D'Esposito</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cholinergic enhancement reduces spatial spread of visual responses in human early visual cortex</article-title><source>Neuron</source><volume>60</volume><fpage>904</fpage><lpage>914</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.038</pub-id><pub-id pub-id-type="pmid">19081383</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smiley</surname> <given-names>JF</given-names></name><name><surname>Falchier</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Multisensory connections of monkey auditory cerebral cortex</article-title><source>Hearing Research</source><volume>258</volume><fpage>37</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2009.06.019</pub-id><pub-id pub-id-type="pmid">19619628</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>AT</given-names></name><name><surname>Singh</surname> <given-names>KD</given-names></name><name><surname>Greenlee</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Attentional suppression of activity in the human visual cortex</article-title><source>NeuroReport</source><volume>11</volume><fpage>271</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1097/00001756-200002070-00010</pub-id><pub-id pub-id-type="pmid">10674469</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname> <given-names>BE</given-names></name><name><surname>Stanford</surname> <given-names>TR</given-names></name><name><surname>Rowland</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multisensory integration and the society for neuroscience: then and now</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0737-19.2019</pub-id><pub-id pub-id-type="pmid">31676599</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talsma</surname> <given-names>D</given-names></name><name><surname>Doty</surname> <given-names>TJ</given-names></name><name><surname>Woldorff</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Selective attention and audiovisual integration: is attending to both modalities a prerequisite for early integration?</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>679</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhk016</pub-id><pub-id pub-id-type="pmid">16707740</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talsma</surname> <given-names>D</given-names></name><name><surname>Senkowski</surname> <given-names>D</given-names></name><name><surname>Soto-Faraco</surname> <given-names>S</given-names></name><name><surname>Woldorff</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The multifaceted interplay between attention and multisensory integration</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>400</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.06.008</pub-id><pub-id pub-id-type="pmid">20675182</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tardif</surname> <given-names>CL</given-names></name><name><surname>Schäfer</surname> <given-names>A</given-names></name><name><surname>Waehnert</surname> <given-names>M</given-names></name><name><surname>Dinse</surname> <given-names>J</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name><name><surname>Bazin</surname> <given-names>PL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Multi-contrast multi-scale surface registration for improved alignment of cortical Areas</article-title><source>NeuroImage</source><volume>111</volume><fpage>107</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.02.005</pub-id><pub-id pub-id-type="pmid">25676917</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Hadjikhani</surname> <given-names>N</given-names></name><name><surname>Hall</surname> <given-names>EK</given-names></name><name><surname>Marrett</surname> <given-names>S</given-names></name><name><surname>Vanduffel</surname> <given-names>W</given-names></name><name><surname>Vaughan</surname> <given-names>JT</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The retinotopy of visual spatial attention</article-title><source>Neuron</source><volume>21</volume><fpage>1409</fpage><lpage>1422</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80659-5</pub-id><pub-id pub-id-type="pmid">9883733</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Trampel</surname> <given-names>R</given-names></name><name><surname>Bazin</surname> <given-names>P-L</given-names></name><name><surname>Heidemann</surname> <given-names>RM</given-names></name><name><surname>Schäfer</surname> <given-names>A</given-names></name><name><surname>Ivanov</surname> <given-names>D</given-names></name><name><surname>Lohmann</surname> <given-names>G</given-names></name><name><surname>Geyer</surname> <given-names>S</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Laminar-specific fingerprints of different sensorimotor Areas obtained during imagined and actual finger tapping</article-title><conf-name>Proceedings of the 20th Annual Meeting of ISMRM, Melbourne, Australia</conf-name></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trampel</surname> <given-names>R</given-names></name><name><surname>Bazin</surname> <given-names>PL</given-names></name><name><surname>Pine</surname> <given-names>K</given-names></name><name><surname>Weiskopf</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>In-vivo magnetic resonance imaging (MRI) of laminae in the human cortex</article-title><source>NeuroImage</source><volume>197</volume><fpage>707</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.09.037</pub-id><pub-id pub-id-type="pmid">28942063</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyll</surname> <given-names>S</given-names></name><name><surname>Bonath</surname> <given-names>B</given-names></name><name><surname>Schoenfeld</surname> <given-names>MA</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Ohl</surname> <given-names>FW</given-names></name><name><surname>Noesselt</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural basis of multisensory looming signals</article-title><source>NeuroImage</source><volume>65</volume><fpage>13</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.09.056</pub-id><pub-id pub-id-type="pmid">23032489</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Burg</surname> <given-names>E</given-names></name><name><surname>Olivers</surname> <given-names>CNL</given-names></name><name><surname>Bronkhorst</surname> <given-names>AW</given-names></name><name><surname>Theeuwes</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Pip and pop: nonspatial auditory signals improve spatial visual search</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>34</volume><fpage>1053</fpage><lpage>1065</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.34.5.1053</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waehnert</surname> <given-names>MD</given-names></name><name><surname>Dinse</surname> <given-names>J</given-names></name><name><surname>Weiss</surname> <given-names>M</given-names></name><name><surname>Streicher</surname> <given-names>MN</given-names></name><name><surname>Waehnert</surname> <given-names>P</given-names></name><name><surname>Geyer</surname> <given-names>S</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name><name><surname>Bazin</surname> <given-names>PL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Anatomically motivated modeling of cortical laminae</article-title><source>NeuroImage</source><volume>93 Pt 2</volume><fpage>210</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.078</pub-id><pub-id pub-id-type="pmid">23603284</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waehnert</surname> <given-names>MD</given-names></name><name><surname>Dinse</surname> <given-names>J</given-names></name><name><surname>Schäfer</surname> <given-names>A</given-names></name><name><surname>Geyer</surname> <given-names>S</given-names></name><name><surname>Bazin</surname> <given-names>PL</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name><name><surname>Tardif</surname> <given-names>CL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A subject-specific framework for in vivo myeloarchitectonic analysis using high resolution quantitative MRI</article-title><source>NeuroImage</source><volume>125</volume><fpage>94</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.10.001</pub-id><pub-id pub-id-type="pmid">26455795</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Brewer</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual field maps in human cortex</article-title><source>Neuron</source><volume>56</volume><fpage>366</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.012</pub-id><pub-id pub-id-type="pmid">17964252</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Mruczek</surname> <given-names>RE</given-names></name><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probabilistic maps of visual topography in human cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id><pub-id pub-id-type="pmid">25452571</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Werner</surname> <given-names>S</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Distinct functional contributions of primary sensory and association Areas to audiovisual integration in object categorization</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>2662</fpage><lpage>2675</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5091-09.2010</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Werner</surname> <given-names>S</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The contributions of transient and sustained response codes to audiovisual integration</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>920</fpage><lpage>931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq161</pub-id><pub-id pub-id-type="pmid">20810622</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.46856.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Reviewing Editor</role><aff><institution>Radboud University</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Reviewer</role><aff><institution>Radboud University</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Huber</surname><given-names>Laurentius</given-names> </name><role>Reviewer</role><aff><institution/><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Norris</surname><given-names>David G</given-names></name><role>Reviewer</role><aff><institution>Radboud University Nijmegen</institution><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This work uses high-field imaging and innovative analytical approaches to examine the laminar profile of two distinct modulatory influences on sensory processing: multisensory interactions and attentional modulation. Interestingly, multisensory and attentional mechanisms modulated the laminar activity profile in distinct ways. This strongly suggests that these two forms of modulatory responses modulate sensory processing via different neural pathways.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Resolving multisensory and attentional influences across cortical depth in sensory cortices&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Floris P Lange as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Christian Büchel as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Laurentius Huber (Reviewer #2); David G Norris (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The manuscript entitled &quot;Resolving multisensory and attentional influences across cortical depth in sensory cortices&quot; describes a layer-fMRI study with audio and visual stimuli and it investigates layer dependent signal changes across different attention and modality conditions. The main conclusion of the study is that cross-modal activity modulates the deeper layers, whereas attentional differences modulate the superficial layers. The fact that this can be measured noninvasively in humans, will be of great interest to a large research field.</p><p>Essential revisions:</p><p>Some of the most significant concerns raised are:</p><p>1) Risk of false positive significance scores. Possibly exacerbated by a low sample size (N=11)?</p><p>2) None of the effects in the main conclusion are reproduced in control task conditions: The attention effect in visual areas is not reproduced by attention effects in auditory areas. The cross-modal modulation in auditory areas is not reproduced in visual areas (neither in size, direction, nor shape of layer signals).</p><p>3) Similarly, none of the effects in the main conclusion are reproduced across the alternative control analysis approaches (shape parameters vs. decodability): Crossmodal layer-profiles in A1 and V1 (Figure 3A) look very different for B-parameters compared to the respective decodability values (zero, constant, decreasing, zero).</p><p>These and other points are elaborated in the individual reviews below.</p><p><italic>Reviewer #1:</italic></p><p>The authors present an elegant study of visual, auditory and multimodal processing of looming stimuli. In addition, they employ an attention modulation, effectively generating a 3x2 design. They record high resolution (0.75mm isotropic) gradient-echo BOLD of 11 subjects and analyze it using the GLM framework. They use equivolume layering and the median activation over four ROIs (two auditory and two visual) to extract 4x6 laminar profiles. Summarizing their findings, they generally find the expected increase in visual areas for visual and auditory areas for auditory stimuli. They in addition find deactivations across modalities. They find cross-modal modulations only in auditory cortex, interestingly in Heschel's gyrus only in multi-voxel decoding, not in amplitude. Attentional modulations were mostly constrained to auditory regions as well.</p><p>The study is complex, but with a solid design. The analyses are described in detail, the code is freely available (the data not yet, so I didn't test their pipeline), and the methods used are appropriate. The paper is also well written, clear and consistent. The figures are detailed, give single subject estimates and are mostly clear. I especially enjoyed that the authors provide all results, not only the significant tests. This is even more important, given that they test quite a lot (which should probably be discussed a bit more, especially in light of a small n=11), and thus there is no hard significance-filter for the results.</p><p><italic>Reviewer #2:</italic></p><p>The manuscript entitled &quot;Resolving multisensory and attentional influences across cortical depth in sensory cortices&quot; describes a layer-fMRI study with audio and visual stimuli and it investigates layer dependent signal changes across different attention and modality conditions. The main conclusion of the study is that cross-modal activity modulates the deeper layers, whereas attentional differences modulate the superficial layers. The fact that this can be measured noninvasively in humans, will be of great interest to a large research field. And upon some revisions, I ultimately recommend its publication with great enthusiasm.</p><p>The novelty and strengths:</p><p>– While some other groups are also currently working on it, I believe this is the first manuscript of layer-dependent analyses of multi-modal integration.</p><p>– I believe I have not seen any layer-fMRI manuscript that combines so many different brain areas (including PT, which is new for layer-fMRI) and so many different task conditions in one study.</p><p>– The authors developed a novel analysis methodology of interpreting the layer-profiles as a combination of linear 'slopes' and 'constant' offsets that allow straightforward summary statistical tests across task conditions.</p><p>– The data-acquisition methodology is technically sound and appropriate to address the research questions. Like in previous studies of that group, they use the most advanced imaging hardware, sequences and imaging protocols. Without being too advanced that it would require additional method-validation-studies.</p><p>– The statistical results are shown very honestly in violin diagrams for all conditions in and all participants. And data will be shared.</p><p>The weaknesses:</p><p>– I feel the task design might have been pushed it a bit too much. There are as many as 6 task conditions with subtle differences. Thus, either one of the condition differentiations does not have so many trials to average across compared to comparable layer-fMRI studies. As a result, multiple different tasks conditions needed to be averaged together and the main conclusions are based on effects that (almost) disappear in the noise level.</p><p>– I believe the clarity of the manuscript can be improved. Each figure has up to 20 sub panels, whereas most of them show insignificant effects that cannot be used to support the main conclusion. It took me quite a while to filter out the relevant information.</p><p>– I believe the way the analysis is conducted and the data are presented could benefit from more discussions on the limits of their interpretability. I am hesitant whether I can interpret the shape parameters and decodability profiles as measures of neural activity in a way that the main conclusion is the most plausible explanation for all the depicted results.</p><p><italic>Reviewer #3:</italic></p><p>In this paper, the authors examine the responses of early sensory cortices to auditory, visual, and combined auditory-visual stimuli, in conjunction with an attentional modulation, using laminar resolution fMRI. The results presented are of interest, and convincing. Nevertheless, I feel that the paper could be improved by consideration of the following:</p><p>1) In the Abstract the claim is made that these findings are crucial for understanding &quot;how the brain regulates information flow across senses&quot;. A sceptic could claim that the article only succeeds in reproducing animal literature, and I would suggest that the authors expand their Discussion so as to better substantiate this claim.</p><p>2) The figures are organised to present the results of auditory and visual stimulation separately, whereas the text is organised to first deal with auditory and then visual stimulation. It may be more logical to deal with both modalities in parallel. Beyond the criticism at the organisational level, the current structure rather masks obvious asymmetries between the responses in primary auditory and visual cortices. The narrative that emerges would seem to be of a dominance visual modality. The Discussion rather misses the opportunity to compare and contrast across the modalities. For example, there are obvious differences shown in Figures 2A, and 3A which could benefit from some in-depth discussion.</p><p>3) Cross modal modulation is assessed by examining the contrast AV-A in auditory cortex and correspondingly AV-V in visual cortex. To me it would seem more logical to construct a contrast AV-(A + V). For example, if AV-A would produce no significant activation, but a visual stimulus in isolation would deactivate auditory cortex, then there would be a clear cross-modal effect that would be missed by your chosen contrast parameter. I would suggest re-analysing the data.</p><p>4) Please justify the inclusion of, and discuss the interpretation of the results from, the planum temporale. In this context please also explain in the Results why the left hemisphere was imaged.</p><p>5) It could be easier to follow some of the text if the authors would deal with areas of early visual cortex that activate upon visual stimulus separately from those that deactivate upon visual stimulus.</p><p>6) I had some concerns about the stimulus design, which I could not find addressed in the text. First, the baseline condition of visual fixation is itself some form of visual task. Second, the auditory target apparently has a fixed amplitude but is presented at a variable time against a looming auditory stimulus: doesn't this affect the detectability?</p><p>7) The English is generally good, but the authors persistently introduce comparative statements with no clear object. This is particularly confusing when paragraphs start with: &quot;by contrast&quot;; &quot;in contrast&quot;; &quot;hence&quot;. Most of these can be deleted without any effect on the meaning. The authors also tend to introduce a chronology (&quot;next we…&quot;) which is not really necessary.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.46856.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Some of the most significant concerns raised are:</p><p>1) Risk of false positive significance scores. Possibly exacerbated by a low sample size (N=11)?</p></disp-quote><p>Thanks. We appreciate the reviewer’s concern.</p><p>However, while we have indeed reported a large number of tests for full characterization of the data, our key hypotheses and research interests focused only on a subset of these contrasts:</p><p>1) Crossmodal deactivations: (i) A in visual cortices, (ii) V in auditory cortices</p><p>2) Crossmodal modulation: (iii) AV &gt; V in visual cortices, (iv) AV &gt; A in auditory cortices</p><p>3) Attentional modulation: (v) Att V vs. Att A in auditory cortices, (vi) Att A vs. Att V in visual cortices</p><p>Further, we discussed only those effects that were consistently observed in both primary and higher order areas (e.g. A1 and PT). This results in 6 statistical comparisons for the mean BOLD-response. Further, we also assessed decoding accuracy for the crossmodal modulation and attentional modulation giving a total of 10 statistical comparisons.</p><p>We have now revised our statistical analysis such that these constraints are not only implicit in our thinking and discussion, but explicit in the analysis itself: we have now combined linear mixed effects models with a step down approach that protects us against false positives.</p><p>1) In step 1, we perform only exactly 10 statistical comparisons as listed above using 2 (shape parameter: constant, linear) x 2 (ROI: primary, non-primary) LME models and 2 dimensional F-tests that assessed whether (i) the constant parameter (averaged across the 2 ROIs) or (ii) the linear parameter (averaged across the two ROIs) is significantly different from zero.</p><p>2) Only if this F-test is significant, we test separately for the linear and the constant parameters (each averaged across the two ROIs) whether they are significantly different from zero.</p><p>3) Only if this F-test is significant, we test separately for each ROI, whether the linear (or constant) is significantly different from zero in separate t-tests.</p><p>For the crossmodal deactivations, we use directed contrasts in step 2 and 3 to accommodate our apriori hypothesis that non-preferred stimuli induce deactivations.</p><p>This is described in a new Materials and methods section: Statistical analysis for the shape parameters at the between subject i.e. group level.</p><p>Tables 1, 2, 3 and Supplementary files 3 and 4 have been updated to reflect this change. All p-values that were effectively computed are now only displayed in the tables.</p><disp-quote content-type="editor-comment"><p>2) None of the effects in the main conclusion are reproduced in control task conditions: The attention effect in visual areas is not reproduced by attention effects in auditory areas. The cross-modal modulation in auditory areas is not reproduced in visual areas (neither in size, direction, nor shape of layer signals).</p></disp-quote><p>Thanks for this comment. Neurophysiological studies have shown that auditory influences on primary visual cortices rely on different translaminar mechanisms than visual influences on auditory cortices. Likewise, modality-specific attention is known to affect auditory and visual processing differently. Therefore, we would expect different laminar profiles in visual and auditory cortices.</p><p>As a result, visual areas cannot be considered control regions for auditory areas and vice versa. Instead, we would like to highlight that A1 and PT show comparable profiles for the visual induced deactivations and crossmodal modulations as the key effects reported in our study. In fact, laminar profiles for all effects are comparable for A1 and PT as well as V1 and V2/3 and can be considered as within study replication.</p><disp-quote content-type="editor-comment"><p>3) Similarly, none of the effects in the main conclusion are reproduced across the alternative control analysis approaches (shape parameters vs. decodability): Crossmodal layer-profiles in A1 and V1 (Figure 3A) look very different for B-parameters compared to the respective decodability values (zero, constant, decreasing, zero).</p></disp-quote><p>Thanks. Laminar BOLD profile and laminar decoding profile should not be viewed as control for one another. They focus on BOLD-response profiles that are expressed at different spatial scales and can therefore provide different results. For instance, if half of the ROI is activated and the other half is deactivated, the average ROI response will be zero, but MVPA will be able to discriminate between these two activation patterns. Likewise, one can simulate activation patterns that differ in their mean BOLD-response, but cannot be discriminated with MVPA. For instance, in our analysis we mean centre each feature individually before MVPA, which can easily lead to this pattern. Hence, ROI mean and ROI activation pattern analyses are complementary approaches to characterize our data and not expected to show comparable results.</p><disp-quote content-type="editor-comment"><p>These and other points are elaborated in the individual reviews below.</p><p>Reviewer #1:</p><p>The authors present an elegant study of visual, auditory and multimodal processing of looming stimuli. In addition, they employ an attention modulation, effectively generating a 3x2 design. They record high resolution (0.75mm isotropic) gradient-echo BOLD of 11 subjects and analyze it using the GLM framework. They use equivolume layering and the median activation over four ROIs (two auditory and two visual) to extract 4x6 laminar profiles. Summarizing their findings, they generally find the expected increase in visual areas for visual and auditory areas for auditory stimuli. They in addition find deactivations across modalities. They find cross-modal modulations only in auditory cortex, interestingly in Heschel's gyrus only in multi-voxel decoding, not in amplitude. Attentional modulations were mostly constrained to auditory regions as well.</p><p>The study is complex, but with a solid design. The analyses are described in detail, the code is freely available (the data not yet, so I didn't test their pipeline), and the methods used are appropriate. The paper is also well written, clear and consistent. The figures are detailed, give single subject estimates and are mostly clear. I especially enjoyed that the authors provide all results, not only the significant tests. This is even more important, given that they test quite a lot (which should probably be discussed a bit more, especially in light of a small n=11), and thus there is no hard significance-filter for the results.</p></disp-quote><p>Thanks. This is an important point. Indeed, we included many tests. As explained in our response under ‘essential revision’ we implicitly imposed additional constraints to ensure that the key effects that we interpret are robust. Further, we would like to emphasize that we maximized within-subject reliability with a block design and acquiring 1200 functional volumes in total per subject. In our summary statistical approach, the variability (and hence statistics) at the 2<sup>nd</sup> level implicitly combines within and between subject variability.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…]</p><p>– I feel the task design might have been pushed it a bit too much. There are as many as 6 task conditions with subtle differences. Thus, either one of the condition differentiations does not have so many trials to average across compared to comparable layer-fMRI studies. As a result, multiple different tasks conditions needed to be averaged together and the main conclusions are based on effects that (almost) disappear in the noise level.</p></disp-quote><p>While we appreciate the reviewer’s concerns, we have relied on a standard 2 x 3 factorial design. The strength of a factorial design is that in the absence of significant interactions we can pool over one factor (e.g. attention) to assess effects of the other factor making factorial designs flexible and powerful.</p><disp-quote content-type="editor-comment"><p>– I believe the clarity of the manuscript can be improved. Each figure has up to 20 sub panels, whereas most of them show insignificant effects that cannot be used to support the main conclusion. It took me quite a while to filter out the relevant information.</p></disp-quote><p>We have now improved our figures and highlighted the key effects we want to focus on. We hope that this improves the overall readability of the results.</p><p>Figure 2 has been split into two figures. Because we are not interested in the ‘obvious’ activations for the preferred sensory modality we have moved those results to a separate supplementary figure. Similarly Figure 3 has been split into two figures and the results from the visual areas have been put into a supplementary figure. Figure 4 has been reorganized to hopefully be made more readable.</p><disp-quote content-type="editor-comment"><p>– I believe the way how the analysis is conducted and the data are presented could benefit from more discussions on the limits of their interpretability. I am hesitant whether I can interpret the shape parameters and decodability profiles as measures of neural activity in a way that the main conclusion is the most plausible explanation for all the depicted results.</p></disp-quote><p>Thanks. We have now included a critical discussion of our approach.</p><p>Subsection “Shape parameters for characterization of laminar BOLD response profiles”: “The S-parameters enable us to make inferences about the shape of a laminar BOLD-response profile rather than using an omnibus F-test that assesses whether the BOLD-response differs across any of the six laminae followed by numerous post hoc pairwise comparisons between layers. However, we acknowledge that this characterization makes our inference less specific about locating even the BOLD-response to a particular lamina. Moreover, because the laminar GLM in the current report did not include any higher order quadratic etc. terms they would not be to model U-shaped laminar profiles.”</p><p>Subsection “Multivariate analysis of pattern across vertices and decoding accuracy profiles”: “Please note that the laminar profiles of decoding accuracy need to be interpreted cautiously. First, decoding accuracy depends on BOLD-signal and noise characteristics that can both vary across laminae. Second, the laminar profile of decoding accuracy is more difficult to interpret, because accuracy is bounded between 0 and 1.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>In this paper, the authors examine the responses of early sensory cortices to auditory, visual, and combined auditory-visual stimuli, in conjunction with an attentional modulation, using laminar resolution fMRI. The results presented are of interest, and convincing. Nevertheless, I feel that the paper could be improved by consideration of the following:</p><p>1) In the Abstract the claim is made that these findings are crucial for understanding &quot;how the brain regulates information flow across senses&quot;. A sceptic could claim that the article only succeeds in reproducing animal literature, and I would suggest that the authors expand their discussion so as to better substantiate this claim.</p></disp-quote><p>Thanks. We have now expanded the Discussion to compare with previous rodent literature. However, given the enormous differences between rodents and humans, we feel even a simple replication would be sufficient to excite the reader.</p><p>Subsection “Multisensory effects in visual and auditory cortices – a comparison”: “Our results reveal distinct laminar profiles for multisensory deactivations in auditory and visual cortices. […]Finally, interpreting laminar BOLD-response profiles in relation to previous findings in rodents remains tentative because of cross-species and methodological differences.”</p><disp-quote content-type="editor-comment"><p>2) The figures are organised to present the results of auditory and visual stimulation separately, whereas the text is organised to first deal with auditory and then visual stimulation. It may be more logical to deal with both modalities in parallel. Beyond the criticism at the organisational level, the current structure rather masks obvious asymmetries between the responses in primary auditory and visual cortices. The narrative that emerges would seem to be of a dominance visual modality. The Discussion rather misses the opportunity to compare and contrast across the modalities. For example, there are obvious differences shown in Figures 2A, and 3A which could benefit from some in-depth discussion.</p></disp-quote><p>Thanks for this suggestion. In fact, we had initially explored an organization based on effect rather than anatomical region. However, this direct opposition seemed to make the Discussion a little cumbersome. Moreover, we are cautious about directly comparing the effects we found in auditory cortices with those not found in visual cortices, for two reasons: First, we cannot exclude the possibility that we observe effects in auditory cortices, but not in visual cortices because of differences in vascular organization. Second, we are cautious about inferring directly a dominance of one sensory modality, because we did not elaborately fine tune the stimuli. For instance, in the past it was thought that vision dominated audition in spatial perception. But research has now established that in fact this is just a question of signal reliability. If we degraded visual information, potentially we may observe that audition dominates vision. Hence, we feel it is premature to draw strong inferences about sensory dominance.</p><p>We have now included a brief paragraph comparing effects in visual and auditory cortices in the Discussion, please see above.</p><disp-quote content-type="editor-comment"><p>3) Cross modal modulation is assessed by examining the contrast AV-A in auditory cortex and correspondingly AV-V in visual cortex. To me it would seem more logical to construct a contrast AV-(A + V). For example, if AV-A would produce no significant activation, but a visual stimulus in isolation would deactivate auditory cortex, then there would be a clear cross-modal effect that would be missed by your chosen contrast parameter. I would suggest re-analysing the data.</p></disp-quote><p>The reviewer is absolutely correct that many studies investigating audiovisual integration report audiovisual interactions, i.e. AV-(A + V). However, as we have discussed in greater detail in Noppeney, 2012, audiovisual interactions in particular when observers are engaged in a task are difficult to interpret. For instance, attentional and response selection processes will be counted once for AV, but twice for the sum A+V. A related issue occurred to us when we tried to perform the decoding analysis of [AV vs. A+V] where we realized that the results of such classifications were mostly driven by differences in noise, because the variance of the sum A+V differ from AV even under the null-hypothesis of no difference. As a result, discriminating between AV vs. A+V would not be valid with MVPA. In this study, we have therefore taken a different approach. First, we focus on crossmodal (de)activations in a unisensory context. Second, we investigate how a concurrent visual signal modulates the response to an auditory signal in auditory cortices (and vice versa in visual cortices). The latter contrast is also a standard statistical comparison in the multisensory integration community. The basic rationale of both statistical comparisons is: If primary and secondary auditory (resp. visual) cortices are specific to processing auditory (resp. visual) stimuli we should not observe and difference.</p><disp-quote content-type="editor-comment"><p>4) Please justify the inclusion of, and discuss the interpretation of the results from, the planum temporale. In this context please also explain in the Results why the left hemisphere was imaged.</p></disp-quote><p>We are not sure whether we understand the reviewer correctly. Both hemispheres were imaged; we presented the laminar profiles results pooled over both hemispheres. A sample of individual surface projections from both hemispheres is now presented in the Figure 2—figure supplement 2 and 3 to clarify this point.</p><p>The passages below illustrate where some of the instances where this fact is now mentioned:</p><p>– The MRI data acquisition section:</p><p>“The acquisition slab was inclined and positioned to include Heschl’s gyri, the posterior portions of superior temporal gyri and the calcarine sulci. Both hemispheres were acquired. On the second day the slab was auto-aligned on the acquisition slab of the first day. This provided us with nearly full coverage over all our four regions of interest.”</p><p>– The sampling of the BOLD-response along cortical depth section:</p><p>“The activity values at the vertices of these 6 intra-cortical surfaces in our four ROIs (i.e. primary auditory, planum temporale, V1, V2/3) pooled over both hemispheres form the basis for our univariate ROI analysis of the laminar BOLD-response profiles and laminar decoding accuracy profiles (based on multivariate pattern classification).”</p><disp-quote content-type="editor-comment"><p>5) It could be easier to follow some of the text if the authors would deal with areas of early visual cortex that activate upon visual stimulus separately from those that deactivate upon visual stimulus.</p></disp-quote><p>One of our initial analysis did investigate the shape of laminar profile in response to visual stimuli in the sub-regions activated VS deactivated by visual stimuli but we found that this could lead to biased laminar profile shapes due to double dipping issues. We have therefore decided not to present such results.</p><disp-quote content-type="editor-comment"><p>6) I had some concerns about the stimulus design, which I could not find addressed in the text. First, the baseline condition of visual fixation is itself some form of visual task.</p></disp-quote><p>Participants fixated a central cross in all conditions, i.e. A, V, AV and Baseline. Because the baseline is not modelled, we subtract all baseline processes implicitly from all conditions. The following sentence was added to the Experimental procedures section:</p><p>“Throughout the entire experiment, participants fixated a cross (0.16º visual angle) in the centre of the black screen. Therefore, processes related to fixation are present in all conditions (i.e. all stimulation and fixation conditions). “</p><disp-quote content-type="editor-comment"><p>Second, the auditory target apparently has a fixed amplitude but is presented at a variable time against a looming auditory stimulus: doesn't this affect the detectability?</p></disp-quote><p>This is correct. We have made this choice to equate the visual and auditory tasks:</p><p>– Visual targets of a fixed radius could also be presented at increasing at levels of visual eccentricities during the duration of the visual stimulus (“The visual grey dot could be presented at any time at any location within the area defined by the radially expanding outermost annulus until they reach a radius of maximal 3.88º visual angle.”) hence visual targets presented later during a visual stimulus would also be harder to detect.</p><p>– Auditory target intensity and visual target radius were adapted for each subject to try to equate their detectability across conditions (“The size of the visual target and the sound amplitude of the auditory target were adjusted in a subject-specific fashion to match the approximate target detection performance across sensory modalities and then held constant across the entire experiment. This was done using the method of constant stimuli in a brief ‘psychometric function experiment’, performed prior to the fMRI study inside the scanner and with scanner noise present.”)</p><disp-quote content-type="editor-comment"><p>7) The English is generally good, but the authors persistently introduce comparative statements with no clear object. This is particularly confusing when paragraphs start with: &quot;by contrast&quot;; &quot;in contrast&quot;; &quot;hence&quot;. Most of these can be deleted without any effect on the meaning. The authors also tend to introduce a chronology (&quot;next we…&quot;) which is not really necessary.</p></disp-quote><p>Thanks. Following the reviewer’s advice, we have removed all these words.</p></body></sub-article></article>