<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">47994</article-id><article-id pub-id-type="doi">10.7554/eLife.47994</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-119284"><name><surname>Graving</surname><given-names>Jacob M</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5826-467X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140474"><name><surname>Chae</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-154182"><name><surname>Naik</surname><given-names>Hemal</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-154183"><name><surname>Li</surname><given-names>Liang</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140477"><name><surname>Koger</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-154184"><name><surname>Costelloe</surname><given-names>Blair R</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-154185"><name><surname>Couzin</surname><given-names>Iain D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="other" rid="par-6"/><xref ref-type="other" rid="par-7"/><xref ref-type="other" rid="par-8"/><xref ref-type="other" rid="par-10"/><xref ref-type="other" rid="par-11"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><institution content-type="dept">Department of Collective Behaviour</institution>, <institution>Max Planck Institute of Animal Behavior</institution>, <addr-line><named-content content-type="city">Konstanz</named-content></addr-line>, <country>Germany</country></aff><aff id="aff2"><institution content-type="dept">Department of Computer Science</institution>, <institution>Princeton University</institution>, <addr-line><named-content content-type="city">Princeton</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><institution content-type="dept">Department for Collective Behaviour</institution>, <institution>Max Planck Institute of Animal Behavior</institution>, <addr-line><named-content content-type="city">Konstanz</named-content></addr-line>, <country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-17888"><name><surname>Shaevitz</surname><given-names>Josh W</given-names></name><role>Reviewing editor</role><aff><institution>Princeton University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>jgraving@gmail.com</email> (JG);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>01</day><month>10</month><year>2019</year></pub-date><volume>8</volume><elocation-id>e47994</elocation-id><history><date date-type="received"><day>26</day><month>04</month><year>2019</year></date><date date-type="accepted"><day>18</day><month>09</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Graving et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Graving et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-47994-v1.pdf"/><abstract><p>Quantitative behavioral measurements are important for answering questions across scientific disciplines-from neuroscience to ecology. State-of-the-art deep-learning methods offer major advances in data quality and detail by allowing researchers to automatically estimate locations of an animal's body parts directly from images or videos. However, currently-available animal pose estimation methods have limitations in speed and robustness. Here we introduce a new easy-to-use software toolkit, <italic>DeepPoseKit</italic>, that addresses these problems using an efficient multi-scale deep-learning model, called <italic>Stacked DenseNet</italic>, and a fast GPU-based peak-detection algorithm for estimating keypoint locations with subpixel precision. These advances improve processing speed &gt;2× with no loss in accuracy compared to currently-available methods. We demonstrate the versatility of our methods with multiple challenging animal pose estimation tasks in laboratory and field settings-including groups of interacting individuals. Our work reduces barriers to using advanced tools for measuring behavior and has broad applicability across the behavioral sciences.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>IOS-1355061</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>Marie Sklodowska-Curie grant agreement No. 748549</award-id><principal-award-recipient><name><surname>Costelloe</surname><given-names>Blair R</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007065</institution-id><institution>Nvidia</institution></institution-wrap></funding-source><award-id>GPU Grant</award-id><principal-award-recipient><name><surname>Costelloe</surname><given-names>Blair R</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N00014-09-1-1074</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N00014-14-1-0635</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000183</institution-id><institution>Army Research Office</institution></institution-wrap></funding-source><award-id>W911NG-11-1-0385</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="par-7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000183</institution-id><institution>Army Research Office</institution></institution-wrap></funding-source><award-id>W911NF14-1-0431</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="par-8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>DFG Centre of Excellence 2117</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="par-9"><funding-source><institution-wrap><institution>University of Konstanz</institution></institution-wrap></funding-source><award-id>Zukunftskolleg Investment Grant</award-id><principal-award-recipient><name><surname>Costelloe</surname><given-names>Blair R</given-names></name></principal-award-recipient></award-group><award-group id="par-10"><funding-source><institution-wrap><institution>The Strukture-und Innovations fonds fur die Forschung of the State of Baden-Wurttemberg</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="par-11"><funding-source><institution-wrap><institution>Max Planck Society</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf2"><p>Iain D Couzin, Reviewing editor, <italic>eLife</italic>.</p></fn><fn fn-type="conflict" id="conf1"><p>The other authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All procedures for collecting the zebra (E. grevyi) dataset were reviewed and approved by Ethikrat, the independent Ethics Council of the Max Planck Society. The zebra dataset was collected with the permission of Kenya's National Commission for Science, Technology and Innovation (NACOSTI/P/17/59088/15489 and NACOSTI/P/18/59088/21567) using drones operated by B.R.C. with the permission of the Kenya Civil Aviation Authority (authorization numbers: KCAA/OPS/2117/4 Vol. 2 (80), KCAA/OPS/2117/4 Vol. 2 (81), KCAA/OPS/2117/5 (86) and KCAA/OPS/2117/5 (87); RPAS Operator Certificate numbers: RPA/TP/0005 AND RPA/TP/000-0009).</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Data used and generated for experiments and model comparisons are included in the supporting files. Posture datasets can be found at: https://github.com/jgraving/deepposekit-dataThe code for DeepPoseKit is publicly available at the URL we provided in the paper: https://github.com/jgraving/deepposekit/The reviewers should follow the provided instructions for installation in the README file https://github.com/jgraving/deepposekit/blob/master/README.md#installation. Example Jupyter notebooks for how to use the code are provided here: https://github.com/jgraving/deepposekit/tree/master/examples</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Graving JM</collab><collab>Chae D</collab><collab>Naik H</collab><collab>Li L</collab><collab>Koger B</collab><collab>Costelloe BR</collab><collab>Couzin IA</collab></person-group><year iso-8601-date="2019">2019</year><source>Example Datasets for DeepPoseKit (Version v0.1-doi) [Data set].</source><ext-link ext-link-type="uri" xlink:href="http://doi.org/10.5281/zenodo.3366908">http://doi.org/10.5281/zenodo.3366908</ext-link><comment>Zenodo, 3366908</comment></element-citation></p><p>The following previously published datasets were used:</p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Pereira TD</collab><collab>Aldarondo DE</collab><collab>Willmore L</collab><collab>Kislin M</collab><collab>Wang SS-H</collab><collab>Murthy M</collab><collab>Shaevitz JW</collab></person-group><year iso-8601-date="2018">2018</year><source>Fast animal pose estimation using deep neural networks</source><ext-link ext-link-type="uri" xlink:href="https://dataspace.princeton.edu/jspui/handle/88435/dsp01pz50gz79z">https://dataspace.princeton.edu/jspui/handle/88435/dsp01pz50gz79z</ext-link><comment>http://arks.princeton.edu/ark:/88435/dsp01pz50gz79z</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-47994-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>