<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">09000</article-id><article-id pub-id-type="doi">10.7554/eLife.09000</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Testing sensory evidence against mnemonic templates</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-34452"><name><surname>Myers</surname><given-names>Nicholas E</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-35337"><name><surname>Rohenkohl</surname><given-names>Gustavo</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-38787"><name><surname>Wyart</surname><given-names>Valentin</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-7718"><name><surname>Woolrich</surname><given-names>Mark W</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-35336"><name><surname>Nobre</surname><given-names>Anna C</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-5"/><xref ref-type="other" rid="par-6"/><xref ref-type="other" rid="par-7"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-34811"><name><surname>Stokes</surname><given-names>Mark G</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Experimental Psychology</institution>, <institution>University of Oxford</institution>, <addr-line><named-content content-type="city">Oxford</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Oxford Centre for Human Brain Activity</institution>, <institution>University of Oxford</institution>, <addr-line><named-content content-type="city">Oxford</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Ernst Strüngmann Institute for Neuroscience</institution>, <addr-line><named-content content-type="city">Frankfurt</named-content></addr-line>, <country>Germany</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Laboratoire de Neurosciences Cognitives, Département d'Etudes Cognitives</institution>, <institution>Ecole Normale Supérieure</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Oxford Centre for Functional MRI of the Brain</institution>, <institution>University of Oxford</institution>, <addr-line><named-content content-type="city">Oxford</named-content></addr-line>, <country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing editor</role><aff id="aff6"><institution>Brown University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>nicholas.myers@ohba.ox.ac.uk</email> (NEM);</corresp><corresp id="cor2"><email>mark.stokes@ohba.ox.ac.uk</email> (MGS)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>14</day><month>12</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>4</volume><elocation-id>e09000</elocation-id><history><date date-type="received"><day>28</day><month>05</month><year>2015</year></date><date date-type="accepted"><day>13</day><month>12</month><year>2015</year></date></history><permissions><copyright-statement>© 2015, Myers et al</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Myers et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-09000-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.09000.001</object-id><p>Most perceptual decisions require comparisons between current input and an internal template. Classic studies propose that templates are encoded in sustained activity of sensory neurons. However, stimulus encoding is itself dynamic, tracing a complex trajectory through activity space. Which part of this trajectory is pre-activated to reflect the template? Here we recorded magneto- and electroencephalography during a visual target-detection task, and used pattern analyses to decode template, stimulus, and decision-variable representation. Our findings ran counter to the dominant model of sustained pre-activation. Instead, template information emerged transiently around stimulus onset and quickly subsided. Cross-generalization between stimulus and template coding, indicating a shared neural representation, occurred only briefly. Our results are compatible with the proposal that template representation relies on a matched filter, transforming input into task-appropriate output. This proposal was consistent with a signed difference response at the perceptual decision stage, which can be explained by a simple neural model.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.001">http://dx.doi.org/10.7554/eLife.09000.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.09000.002</object-id><title>eLife digest</title><p>Imagine searching for your house keys on a cluttered desk. Your eyes scan different items until they eventually find the keys you are looking for. How the brain represents an internal template of the target of your search (the keys, in this example) has been a much-debated topic in neuroscience for the past 30 years. Previous research has indicated that neurons specialized for detecting the sought-after object when it is in view are also pre-activated when we are seeking it. This would mean that these ‘template’ neurons are active the entire time that we are searching.</p> <p><xref ref-type="bibr" rid="bib58">Myers et al.</xref> recorded brain activity from human volunteers using a non-invasive technique called magnetoencephalography (MEG) as they tried to detect when a particular shape appeared on a computer screen. The patterns of brain activity could be analyzed to identify the template that observers had in mind, and to trace when it became active. This revealed that the template was only activated around the time when a target was likely to appear, after which the activation pattern quickly subsided again.</p> <p><xref ref-type="bibr" rid="bib58">Myers et al.</xref> also found that holding a template in mind largely activated different groups of neurons to those activated when seeing the same shape appear on a computer screen. This is contrary to the idea that the same cells are responsible both for maintaining a template and for perceiving its presence in our surroundings.</p> <p>The brief activation of the template suggests that templates may come online mainly to filter new sensory evidence to detect targets. This mechanism could be advantageous because it lowers the amount of neural activity (and hence energy) needed for the task. Although this points to a more efficient way in which the brain searches for targets, these findings need to be replicated using other methods and task settings to confirm whether the brain generally uses templates in this way.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.002">http://dx.doi.org/10.7554/eLife.09000.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>visual attention</kwd><kwd>working memory</kwd><kwd>magnetoencephalography</kwd><kwd>perceptual decision-making</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Graduate Student Fellowship (CQRTDY0)</award-id><principal-award-recipient><name><surname>Myers</surname><given-names>Nicholas Edward</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>HQRWVLO</award-id><principal-award-recipient><name><surname>Stokes</surname><given-names>Mark G</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003135</institution-id><institution>Fondation Fyssen</institution></institution-wrap></funding-source><award-id>Post-doctoral research grant</award-id><principal-award-recipient><name><surname>Wyart</surname><given-names>Valentin</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000719</institution-id><institution>St. John's College, University of Oxford</institution></institution-wrap></funding-source><award-id>Research Centre grant</award-id><principal-award-recipient><name><surname>Myers</surname><given-names>Nicholas Edward</given-names></name><name><surname>Stokes</surname><given-names>Mark G</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000272</institution-id><institution>National Institute for Health Research</institution></institution-wrap></funding-source><award-id>Biomedical Research Centre Programme Award</award-id><principal-award-recipient><name><surname>Nobre</surname><given-names>Anna Christina</given-names></name><name><surname>Stokes</surname><given-names>Mark G</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Senior Investigator Award, 104571/Z/14/Z</award-id><principal-award-recipient><name><surname>Nobre</surname><given-names>Anna Christina</given-names></name></principal-award-recipient></award-group><award-group id="par-7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MEG Partnership Grant, MR/K005464/1</award-id><principal-award-recipient><name><surname>Nobre</surname><given-names>Anna Christina</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Visual search templates are reactivated only temporarily to act as input filters for target detection.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Human perception is flexible: the dimensions guiding perceptual decisions can be updated rapidly as a function of the current task. When decisions are based on perceptual analysis, task goals influence behaviour by creating an internal template: incoming sensory information is then matched against it. The representation of templates therefore plays a fundamental role in guiding perception and decision-making. Biased competition (<xref ref-type="bibr" rid="bib21">Desimone and Duncan, 1995</xref>) provides a broad framework for how the brain interprets new sensory information in light of the current search template. A central tenet is that attention tonically pre-activates visual cortical neurons with receptive fields for relevant, template-matching stimuli (<xref ref-type="bibr" rid="bib66">Reynolds and Chelazzi, 2004</xref>; <xref ref-type="bibr" rid="bib13">Chelazzi et al., 2011</xref>). Single-cell neurophysiology (<xref ref-type="bibr" rid="bib15">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="bib45">Luck et al., 1997</xref>; <xref ref-type="bibr" rid="bib14">Chelazzi et al., 1998</xref>) and human functional magnetic resonance imaging (fMRI; <xref ref-type="bibr" rid="bib12">Chawla et al., 1999</xref>; <xref ref-type="bibr" rid="bib36">Kastner and Ungerleider, 2000</xref>; <xref ref-type="bibr" rid="bib73">Silver et al., 2007</xref>; <xref ref-type="bibr" rid="bib35">Kastner et al., 2009</xref>; <xref ref-type="bibr" rid="bib65">Reddy et al., 2009</xref>) have demonstrated that template representation and stimulus processing can occur in overlapping neural populations in the visual cortex. Moreover, stimulus and template activity patterns cross-generalize (when measured with fMRI, <xref ref-type="bibr" rid="bib76">Stokes et al., 2009</xref>), implying that the two share a common neural code. In the simplest case, increasing baseline activity of a stimulus-specific representation could boost target processing (<xref ref-type="bibr" rid="bib82">Sylvester et al., 2009</xref>). This boost could facilitate target selection and reduce distractor competition for downstream processing resources (<xref ref-type="bibr" rid="bib8">Bundesen et al., 2005</xref>; <xref ref-type="bibr" rid="bib50">Maunsell and Treue, 2006</xref>).</p><p>However, recent findings complicate this simple model. Population-level analyses of time-resolved neural recordings show that stimulus decoding is highly time-specific (<xref ref-type="bibr" rid="bib37">King and Dehaene, 2014</xref>), with discriminative activity patterns changing at the millisecond scale. Such dynamic coding has been observed at the level of population spiking patterns within individual brain areas (<xref ref-type="bibr" rid="bib52">Meyers et al., 2008</xref>; <xref ref-type="bibr" rid="bib20">Crowe et al., 2010</xref>; <xref ref-type="bibr" rid="bib77">Stokes et al., 2013</xref>), and at the level of distributed activation patterns across the cortex (<xref ref-type="bibr" rid="bib38">King et al., 2013</xref>; <xref ref-type="bibr" rid="bib16">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib89">Wolff et al., 2015</xref>), suggesting that this temporal dimension is an inherent aspect of neural coding (<xref ref-type="bibr" rid="bib9">Buonomano and Maass, 2009</xref>). Importantly, neural populations in visual (<xref ref-type="bibr" rid="bib52">Meyers et al., 2008</xref>; <xref ref-type="bibr" rid="bib74">Sreenivasan et al., 2014</xref>) and prefrontal cortex (<xref ref-type="bibr" rid="bib32">Hussar and Pasternak, 2012</xref>; <xref ref-type="bibr" rid="bib33">2013</xref>; <xref ref-type="bibr" rid="bib77">Stokes et al., 2013</xref>; <xref ref-type="bibr" rid="bib2">Astrand et al., 2015</xref>) appear to represent a memorized stimulus with an independent pattern from that used during initial encoding. As a consequence, it is necessary to distinguish between a neural <italic>pattern</italic> (which may vary from moment to moment), and the <italic>representational content</italic> that is encoded in that pattern (which may be stable even when the pattern changes over time, see <xref ref-type="bibr" rid="bib31">Haxby et al., 2014</xref>).</p><p>The highly dynamic trajectory that stimulus processing traces through activation state-space challenges classic models of template representation. These propose tonic activation of a static neural pattern, begging the question: which of the many points along the processing trajectory should be pre-activated?</p><p>An alternative scheme enables templates to guide perceptual decision-making even when stimulus processing is dynamic. If stimulus and template representations rely on different patterns of neural activity in the circuit, then a matched-filter process (c.f. <xref ref-type="bibr" rid="bib80">Sugase-Miyamoto et al., 2008</xref>; <xref ref-type="bibr" rid="bib59">Nikolic et al., 2009</xref>; <xref ref-type="bibr" rid="bib79">Stokes, 2015</xref>) could be envisaged in which the dynamic pattern of stimulus encoding would be automatically transformed into a pattern reflecting the degree of overlap to the template. This could be achieved if the pattern of activity elicited by the incoming stimulus is weighted by the neural pattern associated with the stored template information.</p><p>While visual templates for target detection have been central to attention research, their role has been somewhat neglected in the study of perceptual decision-making. Perceptual decision-making tasks usually require the judgment of a visual stimulus feature against a fixed decision boundary or template (<xref ref-type="bibr" rid="bib28">Gold and Shadlen, 2007</xref>). These tasks typically require judgments to be made at varying levels of perceptual difficulty (<xref ref-type="bibr" rid="bib86">Vogels and Orban, 1990</xref>; <xref ref-type="bibr" rid="bib27">Ghose et al., 2002</xref>; <xref ref-type="bibr" rid="bib64">Purushothaman and Bradley, 2005</xref>; <xref ref-type="bibr" rid="bib81">Summerfield and Koechlin, 2008</xref>; <xref ref-type="bibr" rid="bib70">Scolari and Serences, 2010</xref>; <xref ref-type="bibr" rid="bib90">Wyart et al, 2012</xref>). The majority of perceptual decision-making studies have kept the decision boundary (or template) constant over the entire experiment, impeding a clear evaluation of the representation of templates as distinct from stimulus representation and from the sensory-to-template comparison. In the present study, we varied template and stimulus values independently, enabling us to examine the extent to which their coding and their temporal profiles overlap. We used pattern analysis of simultaneously recorded magneto- (MEG) and electroencephalography (EEG) to track visual template matching with high temporal resolution as human observers performed a parametric match-to-template orientation task (<xref ref-type="fig" rid="fig1">Figure 1A</xref>).<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.09000.003</object-id><label>Figure 1.</label><caption><title>Task design and behavior.</title><p>(<bold>A</bold>) Each block began with the presentation of a target orientation, which observers maintained for the duration of a task block. Template presentation was followed by a serial stream of randomly oriented stimuli. Observers were asked to respond with a button press whenever the stimulus matched the template orientation. (<bold>B</bold>) Response frequency was highest for target trials, and dropped off sharply for non-targets with increasing angular distance between template and stimulus orientation. Error bars indicate standard error of the mean across observers. The black line denotes a von Mises distribution fit to the responses.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.003">http://dx.doi.org/10.7554/eLife.09000.003</ext-link></p></caption><graphic xlink:href="elife-09000-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.09000.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Reaction Time Distribution and Effects of Target Proximity.</title><p>(<bold>A</bold>) Distribution of reaction time frequencies (as a proportion of all responses), from stimulus onset (collapsing over hits and false alarms). Beginning around 400 ms after stimulus onset, response frequency rises rapidly up to approximately 550 ms, after which responses slowly taper off. (<bold>B</bold>) Target proximity (absolute angular distance between the stimulus and template angles) does not affect reaction time (F<sub>3,27</sub> = 1.036, p = 0.393).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.004">http://dx.doi.org/10.7554/eLife.09000.004</ext-link></p></caption><graphic xlink:href="elife-09000-fig1-figsupp1-v2.tif"/></fig></fig-group></p><p>Neural responses rapidly traversed a cascade of discriminative patterns, transforming the initial task-invariant stimulus code, in conjunction with the template code, into a decision-relevant code. Template patterns and stimulus patterns cross-generalized only in a short time window during initial processing, suggesting some independence in the two neural codes. Despite these differences in the neural patterns, the content of the representation encoded in these patterns (as measured by their representational similarity) corresponded over a more sustained period. This might be expected if templates are encoded as a matched filter in the connections between stimulus-sensitive and decision-relevant populations.</p><p>Interestingly, after the stimulus information was already reliably present and the response-relevant information had begun to emerge, neural signals also encoded the (task-irrelevant) <italic>signed</italic> difference between the current stimulus and the search template. This processing stage additionally suggests the presence of a matched filter that permits the flexible calculation of deviations from a search template. We argue, on the basis of a simple neural model, that this effect is consistent with the use of a population code for perceptual decision-making (<xref ref-type="bibr" rid="bib46">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib91">Zemel et al., 1998</xref>; <xref ref-type="bibr" rid="bib3">Beck et al., 2008</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavior</title><p>We recorded simultaneous MEG and EEG signals from 10 human observers as they performed a serial visual match-to-template task (see <xref ref-type="fig" rid="fig1">Figure 1A</xref> and Materials and methods). At the beginning of each block, observers viewed a target orientation to be maintained in memory and used as a search template for the duration of the block. Each block consisted of a centrally presented stream of Gabor patches (randomly drawn from a distribution of 16 orientations, uniformly spaced along the circle). Observers were instructed to respond with a button press whenever the target appeared. Over two sessions, each observer viewed a total of 7680 stimuli to maximize the statistical power of within-participant pattern analyses. On average, observers correctly detected approximately 70% of targets (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). They also made a large proportion of false alarms to near targets (approximately 50% for offsets from the target angle of ± 11.25º), with false alarms rapidly dropping for more distant non-targets. Reaction times were distributed around 550 ms (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>), with no strong effect of target proximity on reaction time (p &gt; 0.35, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>).</p></sec><sec id="s2-2"><title>MEG/EEG signals reflect population tuning curves for stimulus and template orientations</title><p>The stimulus information encoded in MEG/EEG signals was captured by calculating time-resolved population tuning curves (<xref ref-type="fig" rid="fig2">Figure 2</xref>, see Materials and methods). This approach transforms sensor-level responses into responses of virtual stimulus orientation channels: if a stimulus orientation is reflected in the MEG/EEG signal, virtual channel responses should peak at the corresponding orientation. In order to calculate the transformation of sensor data to tuning curves, the data were split into training and test sets. The training data were used to calculate each sensor’s sensitivity to each stimulus orientation, yielding a weight matrix. This weight matrix was then multiplied with the data in the independent test set and averaged over sensors. Single-trial virtual channel responses were then centered on the orientation presented on that trial and averaged over trials, providing an average population tuning curve. Tuning curves were calculated separately for each time point in the trial. Stimulus tuning curves showed that MEG/EEG signals (EEG sensors were added to the analysis in combined MEG/EEG sessions) reflected stimulus orientation shortly after its onset (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig3">Figure 3B,C</xref>, <xref ref-type="fig" rid="fig3">Figure 3A</xref>, 52–500 ms relative to stimulus onset, cluster-corrected p = 0.0019).<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.09000.005</object-id><label>Figure 2.</label><caption><title>Stimulus-evoked population tuning curves.</title><p>(<bold>A</bold>) Average population tuning curve, 50–300 ms after stimulus onset. (<bold>B</bold>) Time-resolved population tuning curve, showing a sharp increase in the tuning curve slope shortly after stimulus onset, tapering off within 500 ms.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.005">http://dx.doi.org/10.7554/eLife.09000.005</ext-link></p></caption><graphic xlink:href="elife-09000-fig2-v2.tif"/></fig><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.09000.006</object-id><label>Figure 3.</label><caption><title>Task variable representation using population tuning curves (see <xref ref-type="fig" rid="fig2">Figure 2</xref>).</title><p>(<bold>A</bold>) Stimulus orientation was represented in the early visual response. We fit weights (using linear regression of stimulus orientation on the neural response) using all trials in all training blocks and estimated virtual channel responses in the test block. Orientation-specific coding was estimated by calculating the linear slope of the tuning curve (between 0° and 90°). Consistent positive slopes indicate orientation selectivity at a given time point. Shading indicates between-subject standard error of the mean. Black bars denote significant time points (cluster-corrected). (<bold>B</bold>) Univariate sensitivity for stimulus orientation, calculated at each sensor and time point. Topography shows the shuffle-corrected orientation sensitivity (z-scored against a distribution generated from permuting stimulus orientations 1000 times), averaged across sensor triplets (two orthogonal planar gradiometers and one magnetometer) and across the stimulus-decoding window. Color coding denotes the z-score, averaged across observers. (<bold>C</bold>) Tuning curve slope and topography (<bold>D</bold>) for template orientation sensitivity. <bold>E</bold> and <bold>F</bold> show the same analyses, sorting trials by the angular distance between template and stimulus (i.e., the decision value).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.006">http://dx.doi.org/10.7554/eLife.09000.006</ext-link></p></caption><graphic xlink:href="elife-09000-fig3-v2.tif"/></fig></p><p>Template orientation information was also present in the MEG/EEG signal during stimulus processing (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, –72 to 324 ms, cluster p = 0.0045). A jack-knife analysis comparing onset latencies between template and stimulus coding showed that template coding began significantly earlier (template: –72 ± 13 ms, stimulus: 52 ± 6 ms, t<sub>9 </sub>= –8.61, p = 3*10<sup>–6</sup>).</p><p>To track the temporal evolution of task-relevant coding (i.e., the decision-value), we also decoded the distance of the current stimulus to the current template (i.e. the signed angular distance between stimulus and template angles, from here on simply ‘angular distance’). A strong effect of angular distance emerged around 200 ms (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, 164–596 ms, cluster p = 0.0024, with an onset that was significantly later than for the stimulus orientation, t<sub>9 </sub>= 3.25, p = 0.002). This effect was also present when training only on trials without a response (172–596 ms, cluster p = 0.0014), which discounts the possibility that this analysis simply reflected the difference in neural signals between responded trials (which were most frequent for small angular distances) and non-responded trials (most frequent for large angular distances).</p><p>While the three main task variables (stimulus, template, and angular distance) were all present in the signal, it is unclear whether different brain regions are involved. We calculated the sensor-level univariate sensitivity to each variable (at MEG sensors only, averaging across the magnetometer and two gradiometers at each location) to determine the topographies associated with different task variables. To a first approximation, all three variables were encoded in signals in visuo-parietal sensors (<xref ref-type="fig" rid="fig3">Figure 3B,D,F</xref>). While sensitivity was again strongest to stimulus orientation, template and angular-distance responses nonetheless showed very similar topographies, indicating that all three variables might be computed in overlapping or nearby populations.</p></sec><sec id="s2-3"><title>Stimulus and task activity patterns vary dynamically throughout the epoch</title><p>To examine whether patterns of stimulus activity changed dynamically throughout the epoch, we tested for cross-temporal generalization of decoding (as elaborated in <xref ref-type="bibr" rid="bib37">King and Dehaene, 2014</xref>). The population tuning curve approach was extended across time by calculating weights on one time point in the trial (on a training data set) and applying those weights to all time points in the trial (on the left-out test data set).</p><p>Stimulus orientation decoding was significant in one main cluster along the diagonal (64–544 ms in training time, 52–436 ms in test time, cluster p = 0.0024, significant cluster extent indicated by color saturation in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). More importantly, stimulus decoding was time-specific, with decoding dropping at off-diagonal train-test time points. To quantify the degree of dynamic coding statistically, we evaluated the off-diagonal results using a conjunction t-test: each off-diagonal combination of timepoints (t<sub>1,2</sub>) was compared against both on-diagonal within-time pairs (t<sub>1,1</sub> and t<sub>2,2</sub>). Evidence for dynamic decoding was inferred if decoding for t<sub>1,2</sub> was significantly lower than both t<sub>1,1</sub> and t<sub>2,2</sub>.<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.09000.007</object-id><label>Figure 4.</label><caption><title>Cross-temporal generalization of orientation decoding.</title><p>(<bold>A</bold>) Tuning curve amplitude for stimulus orientation, estimated by calculating weights at one time point and applying them to test data at all time points in a trial. While decoding is consistently high along the diagonal (in the time window that contains significant stimulus information, between 52 and 544 ms, significant cluster indicated by color saturation/opacity), the slope drops sharply at off-diagonal train-test time coordinates. This indicates that the discriminative patterns are not consistent across time—rather they change rapidly, even while the stimulus can be readily decoded (i.e. off-diagonal decoding is significantly lower than on-diagonal decoding, black outline). <bold><bold>B</bold></bold> and <bold><bold>C</bold></bold> show the same analyses as in A, but sorting all trials by the template angle and the decision-relevant angular distance, respectively.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.007">http://dx.doi.org/10.7554/eLife.09000.007</ext-link></p></caption><graphic xlink:href="elife-09000-fig4-v2.tif"/></fig></p><p>This drop-off is characteristic of dynamic coding (<xref ref-type="bibr" rid="bib37">King and Dehaene, 2014</xref>; <xref ref-type="bibr" rid="bib79">Stokes, 2015</xref>): despite significant decoding at two respective time points, the discriminative patterns do not generalize from one time point to the other. Off-diagonal generalization was significantly lower in a cluster (black outline in <xref ref-type="fig" rid="fig4">Figure 4A</xref>) stretching from 52–304 ms (training time) and from 88–436 ms (generalization time, cluster p = 0.0031, based on cluster extent, see Materials and methods). Since cross-generalization across time was significantly worse than within-time decoding, multiple stimulus-specific activity patterns seem to have been triggered in sequence.</p><p>Importantly, <xref ref-type="fig" rid="fig4">Figure 4A</xref> shows that the cluster of significant decoding (indicated by color saturation) and the cluster of significant dynamic coding (indicated by black outline) partially overlap. In this overlapping region, training on timepoint t<sub>1</sub> and testing on t<sub>2</sub> still leads to significant decoding, but this generalization across time is nonetheless significantly lower than training and testing at either t<sub>1</sub> or t<sub>2</sub> alone. Such overlap can occur if decoding draws on a combination of dynamic and stationary patterns during the same epoch (see also below). It is perhaps also interesting to note that we do not observe any evidence for periodic reactivation of orientation-specific patterns, which would be expected if the discriminating signal was oscillatory and phase-locked to the stimulus presentation (<xref ref-type="bibr" rid="bib37">King and Dehaene, 2014</xref>).</p><p>Template information (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) was present in an early cluster (training time: −140–340 ms, test time: −140–316 ms, relative to stimulus onset, cluster p = 0.0191). In contrast to the stimulus decoding, template decoding showed no significant attenuation of decoding on the off-diagonal.</p><p>Decision-relevant angular-distance decoding showed a dynamic pattern, although off-diagonal decoding appeared to be more pronounced (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) compared with stimulus orientation decoding (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Nevertheless, the strongest decoding was along the diagonal (training time: 4–592 ms, test time: 64–592 ms, cluster p = 0.0009), with significantly reduced off-diagonal generalization in this time window (training time 316–424 ms, generalization time 472–592 ms, cluster p = 0.008). Since off-diagonal decoding was nonetheless significant in a large time window, it is possible that the angular distance exhibits both a dynamic and a static element. This could happen for two reasons. First, it could occur if one population follows a dynamic processing cascade, while a separate population is tonically active in response to a given angular distance. Additionally, significant off-diagonal generalization could occur if there is temporal variability in the underlying processes, smoothing out the dynamic time-specificity across trials.</p></sec><sec id="s2-4"><title>Cross-generalization between stimulus and template neural patterns</title><p>Training the population tuning-curve weights on template orientations around the time of stimulus onset (–150 to +300 ms) showed a strong trend toward generalizing to <italic>stimulus</italic> decoding shortly <italic>after</italic> onset (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, 52–124 ms, corr. p = 0.063). Using only the pre-stimulus time window (–150 to 0 ms) to train the template pattern, stimulus information could still be extracted in this immediate post-stimulus period (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, average over 50–150 ms after stimulus onset, t<sub>9</sub> = 2.45, p = 0.037), indicating that template-specific neural patterns may be pre-activated immediately before stimulus onset. This result indicates that template activity patterns and stimulus activity patterns do cross-generalize (e.g., <xref ref-type="bibr" rid="bib76">Stokes et al., 2009</xref>), but only transiently. The template- and the stimulus-discriminative patterns correspond only in the earliest encoding period, but not later (even though stimulus decoding itself persisted up to 500 ms).<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.09000.008</object-id><label>Figure 5.</label><caption><title>Cross-generalization from template-discriminative patterns to stimulus-discriminative patterns.</title><p>(<bold>A</bold>) Calculating tuning-curve weights relative to the template orientations in a training data set (in window from –150 to +300 ms around stimulus onset), applying these weights on test data, and sorting them relative to the stimulus orientation, showed decoding early after stimulus onset that quickly returned to baseline. (<bold>B</bold>) Calculating population weights only on the pre-stimulus period (with respect to the template orientations) yielded a population tuning curve with a significant peak around the presented stimulus orientation (e.g. a significant peak above the average response around 0º, and a significant positive tuning curve slope between ± 90º and 0º). Shading indicates the standard error of the mean. Black bars indicate significant time points or orientations (p &lt; 0.05).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.008">http://dx.doi.org/10.7554/eLife.09000.008</ext-link></p></caption><graphic xlink:href="elife-09000-fig5-v2.tif"/></fig></p></sec><sec id="s2-5"><title>Stable representational structure for stimulus and task variables</title><p>While the underlying patterns separating different stimulus orientations change dynamically after stimulus onset, the information content that is represented might be more stable. The basic decoding analysis already implies that dynamic neural patterns contain stable information: the same basis set is used for decoding throughout the epoch. Therefore, significant decoding along the time-specific diagonal axis in the cross-temporal analysis suggests that stimulus orientation (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), or angular difference (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), is represented throughout significant changes in the underlying neural patterns. However, a more formal test of the representational structure of multivariate activity is provided by representational similarity analysis (RSA; <xref ref-type="bibr" rid="bib40">Kriegeskorte et al., 2008</xref>). Rather than testing for discrimination per se, this approach focuses on the second-order pattern of condition-specific differences. This allows us to characterise the representational structure of the population code independently of the specific neural patterns associated with different stimulus orientations (<xref ref-type="bibr" rid="bib39">Kriegeskorte and Kievit, 2013</xref>). As an example, a 40º-tilted stimulus might elicit a topography shortly after onset that is more similar to the topography from a 30º-stimulus than that of a 90º-stimulus. While the MEG patterns separating these stimuli might change throughout the trial, the relative difference between them could be preserved. This would indicate that the same kind of information about the stimuli is represented. This approach has been developed specifically to characterize implementation-independent representational geometry (<xref ref-type="bibr" rid="bib39">Kriegeskorte and Kievit, 2013</xref>), and therefore is well-suited here to test whether dynamic neural patterns essentially code the same information.</p><p>We tested this by repeating the cross-temporal analyses on the dissimilarity relationships between MEG responses evoked by different orientations. Dissimilarity was quantified by the Mahalanobis distance matrices between all pairs of stimulus orientations (on one half of trials), separately for each time point in the trial. At each timepoint, this yielded a 16×16 distance matrix. We next calculated the same distance matrix for the remaining half of the data, and calculated the Pearson correlation coefficients between distance matrices from the two independent data sets, for each combination of time points.</p><p>In contrast to the dynamically varying stimulus-discriminative patterns, the representational similarity remained much more stationary (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), with a stable plateau of high correlations (Fisher-transformed Pearson’s rho) from the earliest time of stimulus decoding. We found a significant (on- and off-diagonal) cluster early in the epoch (28–596 ms, cluster p = 0.005). The temporal stability of the representation was, as above, tested by examining whether on-diagonal similarity was higher than off-diagonal similarity. While one short-lived dynamic cluster emerged (training time 82–146 ms, generalization time 130–226 ms, cluster p=0.0075, see black outline in <xref ref-type="fig" rid="fig6">Figure 6A</xref>), the majority of the epoch was dominated by time-stable correlations.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.09000.009</object-id><label>Figure 6.</label><caption><title>Cross-temporal generalization of representational similarity.</title><p>(<bold>A</bold>) Pearson correlations between stimulus-orientation-sorted distance matrices, calculated at different time points and on independent data sets. Color saturation shows significant cluster at the group level (permutation test). The cluster extends off the diagonal in a square, indicating substantial cross-temporal generalization. In addition, there is a small dynamic cluster (black outline), meaning that pairs of time points within the black outline showed significantly lower correlations than their corresponding time points on the diagonal (even though they were still significantly greater than 0). (<bold>B</bold>) shows the same analysis as in A, but sorting all trials by the decision-relevant angular distance. There were no significant dynamic clusters. RSA, representational similarity analysis.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.009">http://dx.doi.org/10.7554/eLife.09000.009</ext-link></p></caption><graphic xlink:href="elife-09000-fig6-v2.tif"/></fig></p><p>Similarly, the representational similarity of angular distance was stable throughout the trial (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Dissimilarity matrices correlated significantly in a later window in the trial (172–588 ms, cluster p = 0.0026), with no time points where within-time correlations were significantly higher than between-time correlations (all p &gt; 0.20). These complementary analyses highlight the cardinal feature of dynamic coding: discriminative dimensions vary with time even though the information content remains constant (<xref ref-type="bibr" rid="bib42">Laurent, 2002</xref>; <xref ref-type="bibr" rid="bib75">Stokes, 2011</xref>).</p></sec><sec id="s2-6"><title>Circular representational structure suggests a common coding scheme between stimuli and templates</title><p>As the previous section indicates, the representational similarity of different stimulus orientations is more temporally stable than the underlying discriminative pattern. If RSA can reveal stable representations over time, it could also uncover representational similarity between the template and the stimulus. In other words, even though the MEG patterns did not persistently cross-generalize between stimulus and template decoding, the dissimilarity matrices calculated for template orientations and for stimulus orientations might reveal a more stable match. This would indicate that similar content is being stored about stimuli and templates, even though the precise neural implementation might differ.</p><p>To quantify this relationship, we again tested for cross-temporal generalization of the dissimilarity matrix. However, here we correlated the dissimilarity matrix calculated between template orientations with the matrix calculated between stimulus orientations. Specifically, we correlated the Mahalanobis distance matrix between all eight template orientations, calculated at each time point, with the distance matrix between stimulus orientations (limiting our analyses to the same eight stimulus orientations that served as targets in the experimental session). The template-sorted dissimilarity matrix correlated significantly with the stimulus-sorted dissimilarity around the time of visual processing (cluster in <xref ref-type="fig" rid="fig7">Figure 7A</xref>, template structure from –48 to 196 ms, stimulus structure from 88–208 ms, cluster p = 0.038). The within-time comparison between templates and stimuli (i.e., the values along the diagonal) also showed a significant correlation (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, 104–176 ms, p = 0.036, with trends toward significance between 412–464 ms, p = 0.079, and 552–596 ms, p = 0.086).<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.09000.010</object-id><label>Figure 7.</label><caption><title>Geometry of stimulus and template coding.</title><p>(<bold>A</bold>) The representational similarity structures between template- and stimulus-ordered responses were significantly correlated in the early stimulus-processing window (saturated colors indicate significant cluster). (<bold>B</bold>) The within-time comparison also showed a significant correlation in the representational similarity structure from 104 to 176 ms. Values correspond to the mean regression coefficient across all observers. Shading is between-subjects standard error of the mean. (<bold>C</bold>) Multi-dimensional scaling of the distances between stimulus orientations was not visible before stimulus onset. (<bold><bold>D</bold></bold>) Shortly after stimulus onset, the circular structure indicated that responses used a circular geometry. (<bold><bold>E</bold></bold> To quantify the representational structure over time, we fit (using regression) to the neural distance matrix between all angles (16 different angles, split randomly into two sets of trials, resulting in a 32×32 distance matrix of Mahalanobis distances) the distance matrix of a 16-point circular simplex, shown in (<bold><bold>F</bold></bold>). (<bold><bold>G</bold></bold>) Similarly, relationships between the eight template orientations fit a circular structure, particularly around stimulus onset time. (<bold><bold>H</bold></bold>) An example of a simplex from one session, with the eight chosen template angles highlighted in color, and the eight stimulus orientations which were never targets shown in gray.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.010">http://dx.doi.org/10.7554/eLife.09000.010</ext-link></p></caption><graphic xlink:href="elife-09000-fig7-v2.tif"/></fig></p><p>What is the basis of this representational similarity between templates and stimuli? Given the simplicity of the stimulus set, a straightforward representational structure comes to mind: more similar stimulus (or template) orientations evoke more similar MEG topographies. However, this cannot be deduced from the population tuning-curve analysis alone. To evaluate the possibility, we calculated neural dissimilarity matrices between the mean responses evoked by each of the 16 stimulus orientations, and projected this 16×16 neural dissimilarity matrix into two dimensions for visualization (using multi-dimensional scaling, <xref ref-type="fig" rid="fig7">Figure 7D</xref>). During the stimulus-encoding period (50–250 ms after stimulus onset), conditions fell onto a well-ordered circle: topographies were more similar (had a smaller Mahalanobis distance) if they were evoked by more similar stimulus orientations. This geometry was not present in the data before the onset of stimulus processing (–50 to +50 ms relative to stimulus onset, <xref ref-type="fig" rid="fig7">Figure 7C</xref>). We tested for the temporal stability of this representational structure by correlating the data-derived (Mahalanobis) distance matrix at each time point with an idealized distance matrix, derived from the angular distances of the respective stimulus orientations (i.e. a 16-point simplex, corresponding to the pairwise angular distances between all 16 presented orientations, <xref ref-type="fig" rid="fig7">Figure 7F</xref>). We used linear regression to fit the idealized distance matrix of the stimuli to the neural distance matrix at each time point. The stimulus similarity matrix significantly fit the neural data (<xref ref-type="fig" rid="fig7">Figure 7E</xref>, 44–432 ms, cluster p = 0.002).</p><p>Likewise, the neural dissimilarity matrix between different <italic>template</italic> orientations was well described by the same simplex structure (<xref ref-type="fig" rid="fig7">Figure 7G,H</xref>, 48–300 ms, cluster p = 0.0012, with a second cluster around the time of the next stimulus). Therefore, while the discriminative patterns for stimuli and templates cross-generalized only briefly, the content of their representation appears to be both stable over time and similar between task variables.</p></sec><sec id="s2-7"><title>MEG responses represent the task-irrelevant sign of the stimulus-template angular difference</title><p>We tested whether the decision-relevant angular distances (between the current stimulus and the template), which were necessary for guiding behavior, showed a comparable similarity structure. The angular distance could be decomposed into two components: its magnitude and its sign. The magnitude of the angular distance (i.e. the absolute difference between stimulus and template orientation) solely determined the task-relevance of a stimulus: the closer the magnitude is to 0º, the more likely the stimulus led to a target response. By contrast, the sign of the angular distance (i.e., whether a stimulus was oriented clockwise or counter-clockwise with respect to the template) had no relevance to the task, because it did not influence how close that stimulus was to the current template. In the next analysis, we therefore attempted to isolate the effects of magnitude and sign on the neural response. We projected (using independently calculated weights, see Materials and methods) neural responses (from all 16 possible angular distances, and from all MEG/EEG sensors) onto two axes measuring separately the influence of magnitude and sign of the angular distance at each point in the trial (<xref ref-type="bibr" rid="bib47">Mante et al., 2013</xref>). This allowed an analysis of the MEG signal’s sensitivity to the decision-relevant magnitude (measured by the amplitude of the response along the magnitude axis), independently of its sensitivity to the decision-irrelevant sign of the angular distance (measured along the sign axis).</p><p>The mean responses to the 16 angular distances fell roughly onto a circle that stretched out along the magnitude axis, with targets and near-targets clearly separable from the definite non-targets (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). Interestingly, near non-targets that were either clockwise or counter-clockwise to the target also separated along the decision-irrelevant sign axis, indicating an unexpected result: angular distances with equal magnitude but different sign (i.e., stimuli at an identical distance to the template orientation, such as –23º and +23°) evoked distinct and separable neural responses. Mean projections along the task-irrelevant axis for conditions with equal magnitude but different sign diverged around the time of decision formation (348–588 ms, corrected p = 0.004, <xref ref-type="fig" rid="fig8">Figure 8B</xref>). We verified that this was the case even without relying on the task-projection approach by calculating Mahalanobis distances between pairs of angular distance trials that had equal magnitude (i.e., –11° vs. +11°, –22° vs. +22°, –34° vs. +34°), and found similar results (see <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). In addition, we confirmed in a control analysis that different angular distances were not separable merely because of possible differences in response bias (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>).<fig-group><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.09000.011</object-id><label>Figure 8.</label><caption><title>Geometry of response-related coding.</title><p>(<bold>A</bold>) Dissimilarity structure of angular distances. Data dimensionality was reduced using PCA, and weights calculated between sensor activity and different task variables using independent training data. Mean responses for each angular distance, calculated using the left-out data, were then projected via the calculated weights onto the task axes (the magnitude and sign of the angular distance). Since the task relevance of a particular angular distance was defined solely by its magnitude, projections onto the sign axis measured sensitivity to task-irrelevant signed differences between conditions. Prior to decision onset (250–500 ms after stimulus onset), the neural geometry is elliptical: in addition to conditions separating along the target-relevant magnitude axis (horizontal), near non-targets separate along the task-irrelevant sign axis (vertical). (<bold>B</bold>) Task-irrelevant coding emerges approximately 350 ms after stimulus onset. Time courses for the three nearest non-targets (11º, 22º, 34º offset from target angle) separate along the task-irrelevant axis, depending on whether they are clockwise or counterclockwise to the target.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.011">http://dx.doi.org/10.7554/eLife.09000.011</ext-link></p></caption><graphic xlink:href="elife-09000-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.09000.012</object-id><label>Figure 8—figure supplement 1.</label><caption><title>Multidimensional Scaling and Pairwise Mahalanobis distances between Angular Distances.</title><p>(<bold>A</bold>) Dissimilarity structure of angular distances. We used MDS, which maps the multidimensional (32× 32) Mahalanobis distance matrix between target-relative angles into two dimensions. During relatively early stimulus processing (250–400 ms after stimulus onset), geometry is elliptical—that is, in addition to conditions separating along the target-relative axis (horizontal), conditions separate along a task-irrelevant axis (vertical). During later processing stages (<bold><bold>B</bold></bold>: 450–900 ms), the task-related axis accounts for most of the condition differences. Since MDS is rotation-invariant, the solution in B happens to have flipped axis 2, without affecting the geometrical relationship between points. (<bold>C</bold>) Mahalanobis distances (shuffle-corrected) between trials with equal target proximity, but different direction (i.e., clockwise vs. counter-clockwise deviations of the stimulus angle, with respect to the template angle). The figure shows the mean z-score (with respect to 250 random permutations of the trial labels) of pairwise distances between equal target proximities, averaged over the pairs ± 11.25º, ± 22.5º, and ± 33.75º. Shading indicates standard error of the mean. The black bar denotes significant time points (p&lt;0.05, cluster-corrected). MDS, multi-dimensional scaling</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.012">http://dx.doi.org/10.7554/eLife.09000.012</ext-link></p></caption><graphic xlink:href="elife-09000-fig8-figsupp1-v2.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.09000.013</object-id><label>Figure 8—figure supplement 2.</label><caption><title>Figure is identical to Panel 8c, but includes in the graph the fit to the distance matrix provided by the linear decision value (i.e., the unsigned target proximity, stimulus—target ).</title><p>This variable was also included in the analysis described in the main text, but omitted from <xref ref-type="fig" rid="fig8">Figure 8C</xref> for clarity (since it a nuisance variable). Shading indicates standard error of the mean and colored bars at the bottom denote significant time points for each regressor (p&lt;0.05, cluster-corrected).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.013">http://dx.doi.org/10.7554/eLife.09000.013</ext-link></p></caption><graphic xlink:href="elife-09000-fig8-figsupp2-v2.tif"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.09000.014</object-id><label>Figure 8—figure supplement 3.</label><caption><title>Neural Population Model.</title><p>(<bold>A</bold>) Probabilistic population code model architecture. (<bold>B</bold>) Dissimilarity structure of responses in the stimulus layer (left panel) and the decision layer (right panel). (<bold>C</bold>) Accumulator model architecture. In contrast to the population code, decision value here is represented only in a single node (red unit in the decision layer). Otherwise, the architectures are identical. (<bold><bold>D</bold></bold>) Dissimilarity structure of responses in the accumulator model. While responses in the stimulus layer are identical in both cases, the decision layer differs from the population code model, in that the magnitude, but not the direction, of angular differences between stimulus and template, is represented. (<bold><bold>E</bold></bold>) Model response on an exact template match trial. (<bold><bold>F</bold></bold>–<bold><bold>H</bold></bold>) Model responses on mismatch trials.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09000.014">http://dx.doi.org/10.7554/eLife.09000.014</ext-link></p></caption><graphic xlink:href="elife-09000-fig8-figsupp3-v2.tif"/></fig></fig-group></p></sec><sec id="s2-8"><title>Template matching based on probabilistic population codes</title><p>The neural encoding and sustained representation of the <italic>signed</italic> difference between the current stimulus and the template was unexpected because representing the sign of the angular distance was not necessary for solving the task (since it would be sufficient to calculate only the magnitude). However, the result yields some insight into the particular neural implementation of the decision process in this task. Specifically, a representation of both magnitude and sign is consistent with the use of a probabilistic population code (<xref ref-type="bibr" rid="bib46">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Beck et al., 2008</xref>). Probabilistic population codes assume that the brain uses the activation pattern across a population of neurons, each tuned to a different stimulus value (angular distance, in our case), to encode a probability distribution across the entire stimulus space (<xref ref-type="bibr" rid="bib46">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib91">Zemel et al., 1998</xref>). The peak of activity of this distribution lies in or near neurons tuned to the presented angular distance. Therefore, angular distances with equal magnitude but different sign can be naturally separated in the population response, even if that dimension of the neural pattern is task-irrelevant. Importantly, this would not be the case if the entire population simply encoded the magnitude of the absolute distance (i.e., the overall match between stimulus and template, as is the case with some accumulator models of decision-making). Therefore, the presence of signed difference signals in the MEG response suggests that the brain uses a (probabilistic) population code to represent the decision variable in this task.</p><p>For illustration, we created a simple neural architecture to elaborate this argument for a population code and to link it to the neural data. The model consisted of three interconnected layers (see Materials and methods and <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref> for details of model behavior). Each layer encoded information about one of the three task variables (stimulus, template, and angular distance). Each unit in a layer was tuned to a different orientation. Tuning in the decision layer represented <italic>decision-relevant</italic> angular distances, meaning that angles closer to 0° represented stimuli closer to the current template. We created the same dissimilarity matrices used in the MEG/EEG analyses from synthetic responses generated by each layer in the model. Identical to <xref ref-type="fig" rid="fig8">Figure 8A</xref>, conditions with equal magnitude but different sign led to separable population responses in the template and angular distance layers. In contrast, a simpler model with only a single accumulator unit in the decision layer showed only a differentiation of conditions if they differed in magnitude, reflecting that here the signed angular difference was not encoded.</p><p>The population coding model used here is almost identical in architecture and behavior to a more elaborate biophysical model that was recently developed to predict the learning of new categories in a population of lateral intraparietal neurons in monkeys performing an orientation discrimination task (<xref ref-type="bibr" rid="bib24">Engel et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Freedman and Assad, 2006</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In a visual match-to-template orientation task, we found distinct, dynamically evolving neural responses that reflected the orientation of the stimulus and the template, as well as the angular distance between the two (i.e., the task-relevant variable). Contrary to standard models of top-down attention, we did not find a tonic activation of the template neural pattern. Instead, the template pattern emerged transiently around the time of the stimulus onset and then quickly returned to baseline.</p><p>While pattern analysis is a well-established methodology for intracranial multi-unit recordings and for fMRI, it is becoming clear that it can provide a useful approach to MEG and EEG as well. Although MEG/EEG measures neural activity at a larger scale relative to micro-electrode recordings, recent modelling demonstrates that the electromagnetic signal contains rich spatiotemporal information suitable for multivariate decoding (<xref ref-type="bibr" rid="bib17">Cichy et al., 2015</xref>). Even subtle differences in dipole position or angle elicit statistically separable patterns at the scalp surface. For orientation decoding, these differences presumably depend on idiosyncrasies in the distribution of orientation columns along the cortical surface (<xref ref-type="bibr" rid="bib17">Cichy et al., 2015</xref>). Such subtleties average out at the group level (or are lost during source localization due to inherent ambiguities with the inverse solution), but can be characterized within individual participants using pattern analysis (see <xref ref-type="bibr" rid="bib78">Stokes et al., 2015</xref>).</p><p>This logic extends in time: small differences in the spatial distribution of activity patterns at different time points would result in idiosyncratic changes in the dipole, resulting in a time-varying signal at the scalp surface. Indeed, the cross-temporal analyses suggest that orientation-specific patterns are also time-specific (see also <xref ref-type="bibr" rid="bib89">Wolff et al., 2015</xref>). In animal models, similar spatiotemporal patterns have been attributed to a cascade of neural engagement within the same brain area (<xref ref-type="bibr" rid="bib30">Harvey et al., 2012</xref>) or time-specific changes in cell preferences (<xref ref-type="bibr" rid="bib72">Sigala et al., 2008</xref>; <xref ref-type="bibr" rid="bib77">Stokes et al., 2013</xref>). It is important to appreciate that decodability does not necessarily imply that the brain is making use of the decodable information (<xref ref-type="bibr" rid="bib84">Tong and Pratte, 2012</xref>). Nonetheless, neural circuits with complex spatiotemporal dynamics could plausibly provide a rich source of information for guiding flexible (<xref ref-type="bibr" rid="bib56">Miller and Fusi, 2013</xref>) and context-dependent behavior (<xref ref-type="bibr" rid="bib9">Buonomano and Maass, 2009</xref>).</p><p>The rapidly changing patterns encoding the stimulus orientation raised the question of when the stimulus is compared to the template. Even though template decoding lasted up to 300 ms after stimulus onset, the template-specific neural patterns cross-generalized to stimulus-specific patterns only in the earliest encoding phase. The transient, rather than sustained, activation of template-specific patterns may reflect the reactivation of a latent code (<xref ref-type="bibr" rid="bib57">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib9">Buonomano and Maass, 2009</xref>) that was laid down in altered synaptic weights, but which is reactivated via top-down or stimulus-driven input. Template decoding began shortly before stimulus onset, suggesting that the semi-regular timing of events may have allowed for top-down re-activation of the template (as in ‘rhythmic sampling’, c.f. <xref ref-type="bibr" rid="bib69">Schroeder and Lakatos, 2009</xref>; <xref ref-type="bibr" rid="bib41">Lakatos et al., 2013</xref>). Since template decoding peaked during the stimulus presentation period, bottom-up stimulus drive may have additionally activated the template pattern.</p><p>The rapid dynamics of stimulus decoding further raise the question of how the brain compares dynamically evolving population codes (<xref ref-type="bibr" rid="bib42">Laurent, 2002</xref>; <xref ref-type="bibr" rid="bib52">Meyers et al., 2008</xref>; <xref ref-type="bibr" rid="bib77">Stokes et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">King and Dehaene, 2014</xref>). Representational similarity analysis (<xref ref-type="bibr" rid="bib39">Kriegeskorte and Kievit, 2013</xref>; <xref ref-type="bibr" rid="bib31">Haxby et al., 2014</xref>; <xref ref-type="bibr" rid="bib60">Nili et al., 2014</xref>) permits higher-order comparisons between different task variables, even if their underlying neural patterns are different. We speculate that matched filters provide a natural solution to the problem of comparing a dynamically evolving stimulus-encoding pattern to a template-encoding pattern: the stimulus pattern is filtered by a population that is matched to the visual characteristics of the template, leading to output that quantifies their overlap.</p><p>Unexpectedly, the task-irrelevant sign of the angular distance was encoded in the MEG/EEG response pattern. This finding could provide an interesting insight into the potential mechanism underlying perceptual decision-making in our task. Specifically, probabilistic population codes may underlie the representation of the angular distance, and encode the sign of the angular distance as a by-product of decision-making. This could be a simple result of the use of a matched filter at an earlier stage. There is evidence that stimulus orientation is represented via population codes in early visual cortex (<xref ref-type="bibr" rid="bib29">Graf et al., 2011</xref>; <xref ref-type="bibr" rid="bib4">Berens et al., 2012</xref>), with the activation profile across neurons tuned to many different orientations reflecting a probability distribution peaking at the orientation that is most likely present in the environment. If this population activity pattern is passed through a filter tuned to the orientation of the template, the resulting output population pattern could again reflect a probability distribution peaking at the most likely <italic>relative</italic> orientation of the stimulus with respect to the template. Because of the orientation symmetry of the filter mechanism, the output would also reflect the direction of the angular distance (clockwise or counterclockwise).</p><p>Recent computational and empirical work on the maintenance of items in working memory has argued that mnemonic information (such as a visual template) can be stored through a reconfiguration of synaptic weights (<xref ref-type="bibr" rid="bib57">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib43">Lewis-Peacock et al., 2012</xref>; <xref ref-type="bibr" rid="bib77">Stokes et al., 2013</xref>), without requiring strong persistent activity (<xref ref-type="bibr" rid="bib88">Watanabe and Funahashi, 2014</xref>). The encoding of decision-relevant mnemonic templates in the weights of a network has the crucial advantage that processing of any new information can immediately pass through the modified weights and produce a matched filter response (<xref ref-type="bibr" rid="bib57">Mongillo et al., 2008</xref>). Conceived in this way, contents in working memory are decision rules that enforce the current stimulus-response mapping (<xref ref-type="bibr" rid="bib78">Stokes, 2015</xref>). Read-out at the time of the probe then consists in a perceptual decision (<xref ref-type="bibr" rid="bib49">Martínez-García et al., 2011</xref>; <xref ref-type="bibr" rid="bib62">Pearson et al., 2014</xref>). These hidden states, reflecting the current task context, could be in operation in our task, and would map onto the representation of the target orientation in the template layer of our toy model.</p><p>The transient reactivation of the template shortly before stimulus onset could also reflect the nature of our task, where the majority of stimuli were non-targets that may have discouraged the use of tonic template activation. For instance, single-cell studies have shown that visual distractors presented in a memory delay can disrupt tonic activity of cells coding the remembered item (in IT, <xref ref-type="bibr" rid="bib54">Miller et al., 1993</xref>, and transiently in PFC, <xref ref-type="bibr" rid="bib53">Miller et al., 1996</xref>). By contrast, tasks without intervening distractors may be more conducive to the use of tonic activation (<xref ref-type="bibr" rid="bib15">Chelazzi et al., 1993</xref>, <xref ref-type="bibr" rid="bib14">1998</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Ten healthy, right-handed volunteers (age range: 21–27 years, 6 females) took part in the study, and were paid £10/hr for their time. Visual acuity was normal or corrected to normal. Ethical approval for methods and procedures was obtained from the Central University Research Ethics Committee of the University of Oxford. Each participant completed two experimental sessions, approximately 1 week apart, with each lasting approximately 2 hr (of which approximately 1 hr was spent on performing the task). Each participant completed a large number of trials (7680 across two sessions), providing robust within-participant statistical power for within-participant decoding.</p></sec><sec id="s4-2"><title>Experimental setup</title><p>Participants completed the MEG/EEG scan inside a sound-attenuated, dimly lit, and magnetically shielded room. Stimuli were displayed onto a rear-projection screen (placed at a viewing distance of 90 cm) via a projector (Panasonic DLP Projector, PT-D7700E) with a spatial resolution of 1024 × 768 pixels and a refresh rate of 60 Hz. Stimuli were presented using Psychophysics Toolbox (<xref ref-type="bibr" rid="bib5">Brainard, 1997</xref>), running on MATLAB (Mathworks, Natick, WA). Participants responded using an optic-fibre response box by lifting their right index finger to indicate whenever they had seen a target. Participants were instructed to respond as quickly and accurately as possible.</p></sec><sec id="s4-3"><title>Task</title><p>The task required the detection of visual targets within a stream of sequentially presented stimuli. The stream consisted of oriented Gabor patches (diameter: 4° visual angle, spatial frequency: 2 cycles/°), presented foveally for 100 ms, at an average rate of 650 ms (inter-stimulus interval, ranging from 516 to 783 ms). Orientations were drawn without replacement from a set of 16 possible angles. Stimuli were equally spaced from 5.625° to 174.375°, in steps of 11.25°. The task consisted of eight brief (approximately 6 min) blocks, in which 480 stimuli were presented (resulting in a total of 3840 stimulus presentations per session). Each block began with the presentation of a target orientation (drawn at random, without replacement, from the 16 stimulus orientations), displayed centrally as a green line (4° length). Thus, each session contained eight randomly drawn target orientations (they did not need to repeat across experimental sessions). The participants were instructed to respond whenever a Gabor patch with a matching orientation appeared. Since stimuli were drawn equiprobably from the 16 possible orientations, 1/16 of all stimuli were targets. Each block was cut into three shorter segments, giving participants brief rest periods. During the rest periods, the target orientation was presented again as a reminder.</p></sec><sec id="s4-4"><title>Data sharing</title><p>In accordance with the principles of open evaluation in science (<xref ref-type="bibr" rid="bib87">Walther and van den Bosch, 2012</xref>), all data and fully annotated analysis scripts from this study are publicly available at <ext-link ext-link-type="uri" xlink:href="http://datasharedrive.blogspot.co.uk/2015/08/testing-sensory-evidence-against.html">http://datasharedrive.blogspot.co.uk/2015/08/testing-sensory-evidence-against.html</ext-link> (see also <xref ref-type="bibr" rid="bib58">Myers et al., 2015</xref>). We also hope these will provide a valuable resource for future re-use by other researchers. In line with the Organisation for Economic Cooperation and Development (OECD) Principles and Guidelines for Access to Research Data from Public Funding (<xref ref-type="bibr" rid="bib63">Pilat and Fukasaku, 2007</xref>), we have made every effort to provide all necessary task/condition information within a self-contained format to maximise the re-use potential of our data. We also provide fully annotated analysis scripts that were used in this paper. Any further queries can be addressed to the corresponding author.</p></sec><sec id="s4-5"><title>Behavioral data analysis</title><p>Because of the rapid succession of stimuli, it is difficult to attribute unequivocally each response to a single stimulus. Therefore, a stimulus-response assignment procedure was designed in order to attribute, in a probabilistic fashion, each response to a single stimulus.</p><p>First, response-time (RT) distributions to stimuli were computed on the basis of their absolute angular distance (tilt) from the target orientation (from 0 to ± 90º). When RTs were averaged relative to the orientation of the stimuli, it was clear that the responses fell within a certain time window (from approximately 200 to 1000 ms), consistent with the approximately periodic presentation of stimuli. Tilt-dependent RT distributions were used to estimate the tuning of responses to the target. At each RT, the response tuning profile—the probability of a response given the tilt of the stimulus, from 0 to 90º—was fitted with a von Mises distribution having two free parameters: the peak of the distribution P<sub>MAX</sub>, and the concentration parameter kappa κ. The von Mises distribution was constrained to be centred at the target orientation (tilt = 0), and the definition of κ was modified such that κ = 0 indicates no tuning, κ &gt; 0 indicates a preferred tuning for the target orientation, and κ &lt; 0 indicates a preferred tuning for the orientation perpendicular/opposite to the target. For each subject, the tuning concentration showed a clear positive response following stimulus onset (approximately 200 to 1000 ms post-stimulus).</p><p>This tuning information was then used to assign probabilistically each response to an individual stimulus. First, for each response, all stimuli that fell into the time window during which the tuning concentration was positive were preselected. Next, among these candidate stimuli (which had different tilts with respect to the target), the stimulus that maximised the probability of a response at the observed RT was selected. The resultant RT distributions truncated the low and high RT values leaving the central part of the original RT distributions</p></sec><sec id="s4-6"><title>MEG and EEG data acquisition</title><p>Each participant completed two sessions: one MEG-only session, and one session in which EEG data were recorded concurrently. Participants were seated in the MEG scanner in a magnetically shielded room. Their legs were placed on leg rests and arms on their lap to avoid movements. Both experimental sessions lasted approximately one hour. Participants were instructed to maintain fixation on the centre of the screen during the stimulus blocks and minimize blinking.</p><p>Neuromagnetic data were acquired using a whole-head VectorView system (204 planar gradiometers, 102 magnetometers, Elekta Neuromag Oy, Helsinki, Finland). Magnetoencephalographic signals were sampled at a rate of 1,000 Hz and on-line band-pass filtered between 0.03 and 300 Hz. The participant’s head position inside the scanner was localised and tracked continuously using head-position index coils placed at four distributed points on the head. Electrodes were placed above and below the right eye for the vertical electro-oculogram (EOG) and to the side of each eye for the horizontal EOG. In addition, eye movements were monitored using a remote infrared eye-tracker (SR research, EyeLink 1000, sampling one eye at 1000 Hz, controlled via Psychophysics Toolbox, <xref ref-type="bibr" rid="bib19">Cornelissen et al., 2002</xref>).</p><p>EEG data were collected in half of the sessions (for each participant), using 60 channels distributed across the scalp via the international 10–10 positioning system (<xref ref-type="bibr" rid="bib1">AEEGS, 1991</xref>). Filtering, downsampling, epoching, and rejection of artefactual trials were performed on EEG data in the same way as on the MEG data. EEG data were added to all decoding analyses for the MEG+EEG sessions (except for the topographies in <xref ref-type="fig" rid="fig3">Figure 3</xref>). We found no substantial differences in decoding between MEG-only and MEG+EEG sessions, apart from a small increase in decoding sensitivity in the latter. Therefore, all within-session analyses were averaged to arrive at participant-level results.</p></sec><sec id="s4-7"><title>MEG data preprocessing</title><p>Data were preprocessed using the in-house OHBA software library (OSL), drawing on SPM8 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>), Fieldtrip (<xref ref-type="bibr" rid="bib61">Oostenveld et al., 2011</xref>), and Elekta software. The raw MEG data were visually inspected to remove and interpolate any channels with excessive noise, and were further de-noised and motion-corrected using Maxfilter Signal Space Separation (<xref ref-type="bibr" rid="bib83">Taulu et al., 2004</xref>). Next, data were downsampled to 500 Hz. Remaining epochs with unsystematic noise corruption were then excluded via visual inspection. Systematic artefacts, arising from eye blinks and heart beats, were identified via independent component analysis, and regressed out of the raw data. The cleaned data were then epoched with respect to each stimulus onset (from –1 to + 1 s). In a final step, data were imported into Fieldtrip and inspected using the semi-automatic rejection tool to eliminate any remaining trials with excessive variance. All data were then baseline-corrected by subtracting the mean signal between –150 and –50 ms relative to stimulus onset (for analyses relating to the template, we used an earlier baseline, from –200 to –150 ms relative to stimulus onset, to explore the possibility that template information might be ‘pre-activated’ around the expected onset time. Using the standard baseline from –150 to –50 ms, however, did not change the results presented here). In addition, the data were smoothed with a 32-ms Gaussian kernel for template-based analyses to reduce noise.</p></sec><sec id="s4-8"><title>Orientation decoding</title><p>We used a population tuning curve model to recover information about the stimulus orientation from the full M/EEG signal. Instead of looking to relate imaging data to different stimulus orientations directly, each stimulus orientation is represented using weights from a linear basis set of population tuning curves. Tuning curve models are well suited to recovering information about parametric features like orientations (<xref ref-type="bibr" rid="bib68">Saproo and Serences, 2010</xref>; <xref ref-type="bibr" rid="bib7">Brouwer and Heeger, 2011</xref>; <xref ref-type="bibr" rid="bib71">Serences and Saproo, 2012</xref>; <xref ref-type="bibr" rid="bib26">Garcia et al., 2013</xref>) or colors (<xref ref-type="bibr" rid="bib6">Brouwer and Heeger, 2009</xref>).</p><p>To recover stimulus orientations, data were separated into a training set (all trials from 7 of 8 blocks) and a test set (the left-out block). For all trials in the training set, we then created a matrix of 16 regressors, with the height of the each regressor on any trial reflecting that trial’s stimulus orientation (i.e. a regressor was set to 1 when the corresponding orientation was presented on that trial, and to 0 otherwise). The regressor matrix was then convolved with a half-cosine basis set (raised to the 15<sup>th</sup> power, see <xref ref-type="bibr" rid="bib6">Brouwer and Heeger, 2009</xref>), in order to pool information across similar orientations. Orientation sensitivity at each MEG/EEG sensor was then calculated by regressing the design matrix against the signal (across all 306 sensors or all 366 sensors in MEG+EEG sessions), separately for all time points in the epoch (in 4 ms steps, using a sliding window of 20 ms). We solved the linear regression equation:</p><p><disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:msub><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mi>W</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo></mml:math></disp-formula></p><p>where <italic>C</italic><sub>1</sub> is the design matrix (16 regressors × no. of training trials), <italic>B</italic><sub>1</sub> is the training data set (306/366 sensors × no. of training trials), and W is the weight matrix (306/366 sensors × 16 orientation values) that we want to estimate. This was done using ordinary least squares:</p><p><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mi>W</mml:mi><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>T</mml:mi></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>;</mml:mo></mml:math></disp-formula></p><p>Overall differences in signal magnitude between sensors were modeled out using a constant regressor in <italic>C</italic><sub>1</sub>. We used W to estimate the population orientation response (or tuning curve) in the test set, <italic>B</italic><sub>2</sub> (306/366 sensors × no. of test trials):</p><p><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>W</mml:mi><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>;</mml:mo></mml:math></disp-formula></p><p>where <italic>C</italic><sub>2</sub> is the tuning curve, <italic>W</italic> is the weight matrix, <italic>W</italic><sup>T</sup> is its transpose, and <italic>W</italic><sup>–1</sup> is its pseudo-inverse. Since both the design matrix and the estimated weight matrix were of full rank, this approach was equivalent to using the pseudoinverse for estimation. For each trial, this curve was then zero-centered relative to the presented orientation. This procedure was repeated for each time point in the epoch before moving to the next iteration in the leave-one-out procedure. Zero-centered orientation curves were then averaged across trials.</p><p>The time course of the tuning curve was then converted into a stimulus information time course by calculating the linear slope of the tuning curve from –90° to 0°. We first averaged stimulus channels that were equidistant from 0° (i.e. +11.25° and –11.25°, +33.75° and –33.75°, etc.) and smoothed each resulting (sign-invariant) orientation channel time course (with a 16-ms Gaussian kernel). We then fit a linear slope across the orientation channels (from –90° to 0°), separately for each time point, session, and participant. Decoding accuracy was then evaluated using one-sample t-tests (against 0), under the assumption that slopes are randomly distributed around 0 if there is no stimulus information in the signal. Multiple comparisons across time were corrected for using cluster-based permutation testing (10,000 permutations, <xref ref-type="bibr" rid="bib48">Maris and Oostenveld, 2007</xref>).</p><p>We used a similar approach to test for encoding of the current template orientation, with the exception that here we used a 32-ms sliding window to increase sensitivity to a more slowly evolving effect. Since there were only eight template orientations per session, and these were randomly selected from the 16 possible stimulus orientations, they were not always equally spaced across the circle. We estimated orientation tuning curves across the eight irregularly spaced angles (using eight equally spaced regressors), and then linearly interpolated the estimated tuning values at the eight intermediate values. After interpolation, the template orientation tuning curves were treated as above to derive decoding time courses.</p><p>Finally, we also applied this approach to calculating tuning profiles for information about the angular distance between the orientation of the current stimulus and the template (ranging from 0° for template matches, in steps of 11.25°, to 90°, for stimuli that were orthogonal to the template).</p><p>Onset latencies between stimulus, template, and angular distance were compared using a jack-knife approach (<xref ref-type="bibr" rid="bib55">Miller, Patterson and Ulrich, 1998</xref>). We compared the onset times of significant coding (p&lt;0.05, corrected) using t-tests. To estimate the variance of each onset time, we used an iterative procedure that left out one participant in turn and calculated the onset time of significant coding across all remaining participants. The standard error of the latency difference was calculated using a revised measure that takes into account the reduced variability caused by the jack-knife procedure (<xref ref-type="bibr" rid="bib55">Miller et al., 1998</xref>). The latency difference calculated across the entire set of participants was divided by this standard error estimate to provide t-statistics that were then evaluated using the conventional t-distribution.</p></sec><sec id="s4-9"><title>Univariate orientation sensitivity analysis</title><p>In addition to the pattern analyses that averaged signals over all sensors, we tested the orientation sensitivity of individual MEG/EEG sensors, to generate a topographical distribution of the sensitivity to the three task variables (stimulus orientation, template orientation, and decision-relevant angular distances). The baseline-corrected signal at each sensor and time point in the epoch was fit (across all trials) using a general linear model (GLM) consisting of pairs of regressors containing the sine and cosine of the three task orientations, along with a constant regressor. From the pair of regression coefficients for the sine (β<sub>SIN</sub>) and cosine (β<sub>COS</sub>) of an orientation, we calculated orientation sensitivity A:</p><p><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi>A</mml:mi><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mo>√</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:msup><mml:mo> </mml:mo><mml:mo>+</mml:mo><mml:mo> </mml:mo><mml:msup><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo><mml:mo>;</mml:mo></mml:math></disp-formula></p><p>We calculated the amplitude A expected by chance alone by permuting the design matrix and repeating the amplitude analysis 1000 times. The observed (unpermuted) amplitude was ranked within the permutation distribution of amplitudes to calculate a p-value, which was transformed into a z score using the inverse of the cumulative Gaussian distribution (with center 0 and standard deviation 1). Sensitivity at the group level was then estimated by averaging z-scored amplitudes across session, participant, and the magnetometer and two gradiometers at each sensor location. These values were then plotted as topographies to illustrate the distribution of orientation sensitivity for the three task variables.</p></sec><sec id="s4-10"><title>Cross-temporal orientation decoding</title><p>To assess the temporal stability of stimulus-specific topographies, we trained the population tuning-curve model on one time point in the epoch, and applied the estimated weights to all time points in the test data (using a sliding window of width 20 ms, applied every 12 ms). This was then repeated for all time points, creating a two-dimensional matrix of cross-temporal tuning-curve slopes (with no additional smoothing). Dynamic coding can be inferred by comparing the decoding slopes on within-time training (i.e., training and testing on time <italic>t</italic><sub>1</sub>, or time <italic>t</italic><sub>2</sub>) with the decoding slopes on between-time training (i.e., training on <italic>t</italic><sub>1</sub> and testing on <italic>t</italic><sub>2</sub>). Our criterion for a dynamic epoch was: for each pair of time points <italic>t<sub>i,j</sub></italic>, coding is dynamic if the tuning curve slope is significantly higher (as measured by a paired t-test across 10 participants) within time than across time (<italic>t<sub>i,i</sub></italic> &gt; <italic>t<sub>i,j</sub></italic> AND <italic>t<sub>j,j</sub></italic> &gt; <italic>t<sub>i,j</sub></italic>). Time windows of significant decoding (<italic>t<sub>i,j</sub></italic> &gt; 0) and windows of significant <italic>dynamic coding</italic> were identified using 2-dimensional cluster-based permutation testing (i.e., across both time axes).</p></sec><sec id="s4-11"><title>Cross-generalization between stimulus and template patterns</title><p>To test whether stimulus-specific patterns cross-generalize to template-specific patterns, we repeated the cross-temporal tuning-curve analysis, but calculated weights based on the presented template orientations in the training set, and then zero-centered the tuning curves of the test set with respect to the <italic>stimulus</italic> orientations. Here, a significantly positive tuning curve slope at time pair <italic>t<sub>i,j</sub></italic> indicates that stimulus coding around time point i shares orientation-specific topographic patterns with template coding around time point <italic>j</italic>. For consistency with the other analyses, we treated the training data as in the analyses evaluating template coding, and treated the training data as in the stimulus decoding analyses. Therefore we used a baseline of –200 to –150 ms for the training data, and smoothed with a Gaussian kernel of width 32 ms. For the test data, we used a baseline of –150 to –50 ms and did not smooth. For calculating weights, we used a sliding window of 32 ms, moving in 12-ms steps. The results were smoothed with a 20-ms Gaussian kernel. Again, we used permutation testing to correct for multiple comparisons.</p></sec><sec id="s4-12"><title>Representational similarity analysis</title><p>In light of the rapid dynamics of the population tuning curve data, we reasoned that, while the exact neural pattern might differ between time points and task variables, the information represented (as measured by their representational geometry) might be more constant over time. We tested for this possibility with RSA. Specifically, our approach involved calculating neural dissimilarity matrices between the MEG/EEG topographies evoked by different stimulus orientations. For each session, we sorted all trials by the presented stimulus orientation (into 16 bins), and then split each of these in half (separating odd and even trials). The odd–even split allowed us to compare dissimilarity structures in two independent data sets, and to verify the reliability of the RSA. For each of the 32 bins, we calculated the baseline-corrected average evoked response (across trials) at all sensors and time points. Next, for each time point (moving in 4-ms steps), we calculated the neural dissimilarity matrix by computing all pairwise Mahalanobis distances between orientations (using the within-condition covariance, pooled over all conditions). We interpret these dissimilarity matrices as reflections of the representational structure at each time point in the epoch.</p><p>In the first instance, we were simply interested in whether the neural dissimilarity structure was more stable over time than the underlying neural patterns (that were calculated in the tuning curve analyses). To test for this, we correlated (with Pearson correlations) the dissimilarity matrix from one-half of trials at one time point with the dissimilarity matrix from the other half of trials at all time points, generating a cross-temporal matrix of correlations between dissimilarity structures. If the dissimilarity structure is stable over time, this should result in significant correlations (measured via one-sample t-tests at the group level on the Fisher-transformed correlation coefficients) between time points (e.g., off-diagonal coding). We repeated this analysis for the decision-relevant angular distance.</p><p>For the analyses comparing the neural dissimilarity structures for template coding and stimulus coding, we used one-half of trials to calculate the 8×8 template-based dissimilarity matrix (on data baselined at –200 to –150 ms, as above), and the other half of trials (on data baselined at –150 to –50 ms, as above) to calculate the 8×8 stimulus-based dissimilarity matrix (using the eight stimulus orientations that also served as target orientations in that session). As above, the resulting within-time correlations were then smoothed with a 20-ms Gaussian kernel.</p><p>Next, we asked whether the neural dissimilarity structure, or geometry, was related to the parametric dissimilarity structure of the stimuli: since a 45° angle is more similar to a 60° angle than a 90° angle, the corresponding MEG/EEG topographies might be more similar as well. The stimulus dissimilarity matrix based on the pairwise angular distances between all presented orientations was regressed against the MEG/EEG dissimilarity matrix using a general linear model, fitting the model separately for each time point, session, and participant. Significant fits were assessed via one-sample t-tests. As an illustration of the presence of circular structure in the representational geometry, we projected the 32×32 dissimilarity matrix into two dimensions using multi-dimensional scaling (MDS).</p><p>We repeated geometric analyses on the dissimilarity structure with respect to the template orientation, and the decision-relevant angular distance. For the latter (angular distances), the MDS results indicated that the circular geometry of stimulus relationships was distorted by the decision likelihood. Therefore we used multiple regression to account for its possible influence (pattern component modeling, <xref ref-type="bibr" rid="bib22">Diedrichsen et al., 2011</xref>). A first nuisance regressor captured the differences in the absolute decision value, i.e. the distance between the <italic>unsigned</italic> angular distances (i.e. the distance between 0° and –22.5° was 22.5°, but the distance between –22.5° and +22.5° was 0°, rather than 45°). The second nuisance regressor was based on the similarity in response likelihood, which we estimated by calculating the participant-wise differences in response frequency between all decision-relevant angular distances. This regressor reflected differences in response likelihood between orientations, accounting for any effect of motor preparation. This regressor accounted for the effect of linear decision value on the MEG pattern. This pattern was regressed out because it could reflect two possible decision mechanisms: population-based coding, as proposed here, or linear evidence accumulation (<xref ref-type="bibr" rid="bib28">Gold and Shadlen, 2007</xref>). While the latter may still be at play in this task, we were specifically interested in dissociating the two coding mechanisms.</p></sec><sec id="s4-13"><title>Angular distance analysis</title><p>A final analysis examined how the entire population of MEG/EEG sensors dynamically encodes different task variables relating to angular distance representation. This was done by representing population responses as trajectories in neural state space (with each dimension representing a unique task variable). One approach, emulated here, has recently been described for populations of neural spike trains (<xref ref-type="bibr" rid="bib47">Mante et al., 2013</xref>). First, in order to de-noise the data, we smoothed data with a 20-ms Gaussian kernel and reduced the dimensionality of the MEG signal from 306 sensors (or 306+60 sensors for MEG+EEG sessions) to 30 principal components (PCs) by calculating coefficients over the average time series at each sensor. We then fit the task variables to the reduced-dimensionality data using a GLM. The regressors were derived from the three main task variables: stimulus orientation, template orientation, and angular distance. Since all three are circular variables, we used pairs of regressors, consisting of the sine and cosine of each task angle, yielding a design matrix consisting of six regressors in total.</p><p>The fitting was done in a leave-one-block-out procedure: in turn, we held out all trials from one task block as a test set, and fit the GLM on the trials in the remaining seven blocks (the training set). The GLM was solved on normalized data (by subtracting the mean and dividing by the variance across all trials in the training set). This yielded a set of six regression coefficients (‘betas’) for each time point in the trial and for each of the 30 PCs, which were then symmetrically orthogonalized (<xref ref-type="bibr" rid="bib18">Colclough et al., 2015</xref>; following <xref ref-type="bibr" rid="bib47">Mante et al., 2013</xref>). After normalizing the data from the test set (using the mean and variance from the training set), we calculated mean responses for all 16 angular distances in the test set (yielding a 16 angles × 30 PCs matrix). The means (16×30) were then projected onto the task axes by multiplying them, time point by time point, with the betas (30 PCs × 6 regressors) from the training set, creating a 16×6 matrix at each timepoint for each left-out block. We then averaged projections across the eight cross-validation folds. The resulting projections estimate the sensitivity of each condition (i.e., the 16 angular distances) to each task variable (i.e., the six regressors), separately for each time point in the trial.</p><p>In line with Mante and colleagues, we interpreted consistent deviations from 0 (as measured by one-sample t-tests across observers), in either direction along an axis, as task variable sensitivity. In particular, the two regressors for the angular distance partialled out task-relevant and task-irrelevant aspects of the angular distance between stimulus and template: the cosine regressor, with a maximum of 1 at 0º (targets), a minimum of –1 at the farthest non-targets ( ± 90º, in our 180º orientation space), and equal magnitudes for equivalent non-targets (e.g., 0.92 for both +11º and –11º), measured only the task-relevant aspect of the angle (i.e., the decision value, as shown in <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). By contrast, the sine regressor is insensitive to decision value (since, in 180º orientation space, sin(0º) = sin( ± 90º) = 0), but distinguished between signed differences between non-targets (e.g., sin(+11º) = 0.38 = −sin(–11º)).</p></sec><sec id="s4-14"><title>Neural population model</title><p>To summarize our MEG results, and to illustrate how they could arise from a very simple decision circuit, we created a population-based neural coding model capable of performing the template-matching task used in our experiment. The model consisted of a three-layer architecture, with each layer consisting of neurons coding for different task variables (a stimulus layer, a template layer, and a decision layer). The stimulus layer consisted of a set of 100 units, each tuned to a different veridical stimulus orientation, with tuning determined by a von Mises distribution:</p><p><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>*</mml:mo><mml:mi>cos</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>A</mml:mi><mml:mo>;</mml:mo></mml:math></disp-formula></p><p><italic>R<sub>i</sub></italic>(<italic>θ</italic>) indicates the response <italic>R</italic> of model unit <italic>i</italic> (tuned to <italic>θ<sub>i</sub></italic>) to orientation <italic>θ</italic>, with concentration parameter κ determining the tuning width of the response, and A reflecting a normalizing constant. Activation in the layer was then normalized to a range between 0 and 1. In the stimulus layer, the concentration parameter was set to 5 (see <xref ref-type="bibr" rid="bib3">Beck et al., 2008</xref>, for the same parameter choice).</p><p>The template layer was identical to the stimulus layer, with the exception that template tuning was broader (κ = 2), under the assumption that remembered stimuli would be encoded with lower precision than currently visible stimuli. Finally, the decision layer was identical to the template layer, with the conceptual difference that here, units were not tuned to veridical stimulus orientations, but to decision-relative orientations. In other words, activation of units tuned to near 0° in the decision layer reflected choice-relevant signals, irrespective of the current template orientation.</p><p>The stimulus and template layers were connected via a one-to-one mapping between identically tuned units (weight matrix <bold>W<sup>ST</sup></bold>, with all connections between non-identical orientations set to 0). On each trial, the stimulus layer was initialized by setting the population response vector <bold>R<sup>S</sup></bold> in accordance with the stimulus orientation, and the template layer response vector <bold>R<sup>T</sup></bold> in accordance with the template orientation. In a second step, corresponding to a later processing stage, activation in the template layer was updated as a function of activation in the stimulus layer, by computing the element-wise product between <bold>R<sup>S</sup></bold> and <bold>R<sup>T</sup></bold>. This step is similar to a Bayesian update, in which the prior distribution (the template layer response) is multiplied with the current evidence (the stimulus layer response) to produce a posterior distribution.</p><p>The crucial mapping for the task was between the template and decision layers, which consisted of an all-to-all reciprocal weight matrix <bold>W<sup>TD</sup></bold>. The template layer unit tuned to the current target orientation had the strongest connection with the 0° unit in the decision layer (and neighboring template layer units were connected to correspondingly shifted units in the decision layer). All other connection weights fell off according to a von Mises distribution with κ = 5 (although the exact tuning width did not substantially alter model behavior). This weight matrix shifted the response profile <bold>R<sup>T </sup></bold>in the template layer (which was still in veridical orientation space) to a response <bold>R<sup>D</sup></bold> in decision space. The decision layer response therefore permitted a direct mapping to decision- or motor-related output regions (which are omitted here). Importantly, only the weight matrix <bold>W<sup>TD</sup></bold> needs to change in response to a change in the current template orientation.</p><p>Since we were mainly interested in the effects of reading out population activity in the decision layer, this model contained the simplification that codes in the three layers did not change over time. However, the dynamics of coding were not of interest for the question of whether population activity, <italic>in principle</italic>, could account for our neural results.</p></sec><sec id="s4-15"><title>Behavior of neural population model</title><p>The model behavior (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3E–H</xref>) followed a simple trajectory over the course of a hypothetical trial. At the beginning of the trial, before current stimulus input has been processed, the template layer encodes the current template layer, via a bump in activation in template-tuned neurons. This input can be instantiated in the template layer via top-down input from the 0° unit in the decision layer (although other mechanisms for activating the template are also conceivable, such as periodic reactivation, e.g. <xref ref-type="bibr" rid="bib10">Buzsáki and Moser, 2013</xref>; <xref ref-type="bibr" rid="bib23">Eichenbaum, 2013</xref>; <xref ref-type="bibr" rid="bib34">Johnson and Redish, 2007</xref>; <xref ref-type="bibr" rid="bib44">Lisman and Jensen, 2013</xref>; <xref ref-type="bibr" rid="bib69">Schroeder and Lakatos, 2009</xref>). This activation of the template layer around the time of stimulus onset might correspond to the decoding profile for template information in the MEG data (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Next, stimulus input is represented in the stimulus layer, again via a population activity profile peaking at neurons tuned to the currently presented stimulus (again corresponding to the decoding profile, <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Stimulus layer activation is then fed forward into the template layer, where activation is multiplied point-by-point with the existing activation state. This could happen, for example, if neurons in the template layer change their gain to all input depending on their proximity to the current target orientation (<xref ref-type="bibr" rid="bib11">Carandini and Heeger, 2012</xref>; <xref ref-type="bibr" rid="bib51">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib67">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib73">Silver et al., 2007</xref>; <xref ref-type="bibr" rid="bib85">Treue and Trujillo, 1999</xref>). The resulting activation profile now represents the stimulus orientation, scaled by its similarity to the template—while neurons tuned to the current orientation again have high activation, the height of that activation depends on stimulus-template similarity (and the peak is shifted towards the template orientation, with the magnitude of the shift depending on the ratio of stimulus and template tuning widths). This representation of the template and the stimulus in the same population (at slightly different but overlapping timepoints) might reflect why stimulus and template geometries cross-generalize (<xref ref-type="fig" rid="fig7">Figure 7G,H</xref>).</p><p>By passing the template layer profile on to the decision layer, it is shifted into a decision-relative (i.e. stimulus-invariant) space. Here, the response exhibits two decision-relevant features. First, the closer the current stimulus is to the template, the closer the decision layer peak is to the 0° neuron. Second, as in the template layer, the height of the decision layer profile also depends on the proximity between stimulus and template, with highest activation for targets (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3B</xref>), and the peak dropping off for increasingly distant non-targets (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3C–E</xref>). This decreasing amplitude may explain why near-targets were more separable than definite non-targets later in the trial epoch (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Downstream read-out units could use either the population’s <italic>peak location</italic> (i.e. how close the maximum response is to the 0° unit, as in <xref ref-type="bibr" rid="bib3">Beck et al., 2008</xref>) or its <italic>peak activation</italic> (as in more classical decision models) to determine whether a target is present or absent.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Michael Frank, Floris de Lange, and another reviewer for helpful advice on the manuscript. We are grateful to Laura Turner and Katrina Quinn for help with data collection. This study was funded by the Medical Research Council (to M.G.S.), the Wellcome Trust (A.C.N.: Senior Investigator Award (ACN) 104571/Z/14/Z, G.R., M.W.W., and N.E.M.), St. John’s College, Oxford (N.E.M.), the Fyssen Foundation and the French National Resarch Agency (V.W., grants ANR-10-LABX-0087 and ANR—10-IDEX-0001-02), an MRC UK MEG Partnership Grant (MR/K005464/1), and the National Institute for Health Research Oxford Biomedical Research Centre Programme based at the Oxford University Hospitals Trust, Oxford University. The views expressed are those of the authors and not necessarily those of the NHS, the NIHR, or the Department of Health.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>NEM, Conception and design, Analysis and interpretation of data, Drafting or revising the article, Contributed unpublished essential data or reagents</p></fn><fn fn-type="con" id="con2"><p>GR, Conception and design, Acquisition of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>VW, Conception and design, Acquisition of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con4"><p>MWW, Drafting or revising the article, Contributed unpublished essential data or reagents</p></fn><fn fn-type="con" id="con5"><p>ACN, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con6"><p>MGS, Conception and design, Analysis and interpretation of data, Drafting or revising the article, Contributed unpublished essential data or reagents</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Ethical approval for methods and procedures was obtained from the Central University Research Ethics Committee of the University of Oxford. All participants provided written, informed consent.</p></fn> </fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><sec id="s7" sec-type="datasets"><title>Major datasets</title><p>The following datasets were generated:</p> <p><related-object content-type="generated-dataset" id="dataro1" source-id="http://datadryad.org/review?doi=doi:10.5061/dryad.m57sd" source-id-type="uri"><collab>Myers N</collab>, <collab>Rohenkohl G</collab>, <collab>Wyart V</collab>, <collab>Woolrich M</collab>, <collab>Nobre A</collab>, <collab>Stokes M</collab><x>,</x> <year>2015</year><x>,</x><source>Data from: Testing sensory evidence against mnemonic templates</source><x>,</x> <ext-link ext-link-type="uri" xlink:href="http://datadryad.org/review?doi=doi:10.5061/dryad.m57sd">http://datadryad.org/review?doi=doi:10.5061/dryad.m57sd</ext-link><x>,</x> <comment>Available at Dryad Digital Repository under a CC0 Public Domain Dedication</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>AEEGS</collab></person-group><year iso-8601-date="1991">1991</year><article-title>American electroencephalographic society guidelines for standard electrode position nomenclature</article-title><source>Journal of Clinical Neurophysiology</source><volume>8</volume><fpage>200</fpage><lpage>202</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astrand</surname><given-names>E</given-names></name><name><surname>Ibos</surname><given-names>G</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Ben Hamed</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Differential dynamics of spatial attention, position, and color coding within the parietofrontal network</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>3174</fpage><lpage>3189</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2370-14.2015</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Hanks</surname><given-names>T</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Roitman</surname><given-names>J</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Probabilistic population codes for bayesian decision making</article-title><source>Neuron</source><volume>60</volume><fpage>1142</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.021</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Cotton</surname><given-names>RJ</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A fast and simple population code for orientation in primate V1</article-title><source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source><volume>32</volume><fpage>10618</fpage><lpage>10626</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1335-12.2012</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname><given-names>GJ</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding and reconstructing color from responses in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>13992</fpage><lpage>14003</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3577-09.2009</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname><given-names>GJ</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cross-orientation suppression in human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>2108</fpage><lpage>2119</lpage><pub-id pub-id-type="doi">10.1152/jn.00540.2011</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bundesen</surname><given-names>C</given-names></name><name><surname>Habekost</surname><given-names>T</given-names></name><name><surname>Kyllingsbaek</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A neural theory of visual attention: bridging cognition and neurophysiology</article-title><source>Psychological Review</source><volume>112</volume><fpage>291</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.112.2.291</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname><given-names>DV</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>State-dependent computations: spatiotemporal processing in cortical networks</article-title><source>Nature Reviews. Neuroscience</source><volume>10</volume><fpage>113</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/nrn2558</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Memory, navigation and theta rhythm in the hippocampal-entorhinal system</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>130</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nn.3304</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chawla</surname><given-names>D</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The physiological basis of attentional modulation in extrastriate visual areas</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>671</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.1038/10230</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Della Libera</surname><given-names>C</given-names></name><name><surname>Sani</surname><given-names>I</given-names></name><name><surname>Santandrea</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural basis of visual selective attention</article-title><source>Wiley Interdisciplinary Reviews: Cognitive Science</source><volume>2</volume><fpage>392</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1002/wcs.117</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Responses of neurons in inferior temporal cortex during memory-guided visual search</article-title><source>Journal of Neurophysiology</source><volume>80</volume><fpage>2918</fpage><lpage>2940</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>A neural basis for visual search in inferior temporal cortex</article-title><source>Nature</source><volume>363</volume><fpage>345</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1038/363345a0</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Ramirez</surname><given-names>FM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Can visual information encoded in cortical columns be decoded from magnetoencephalography data in humans?</article-title><source>NeuroImage</source><volume>121</volume><fpage>193</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.07.011</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colclough</surname><given-names>GL</given-names></name><name><surname>Brookes</surname><given-names>MJ</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A symmetric multivariate leakage correction for MEG connectomes</article-title><source>NeuroImage</source><volume>117</volume><fpage>439</fpage><lpage>448</lpage><uri xlink:href="http://doi.org/">http://doi.org/</uri><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.03.071</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cornelissen</surname><given-names>FW</given-names></name><name><surname>Peters</surname><given-names>EM</given-names></name><name><surname>Palmer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The eyelink toolbox: eye tracking with MATLAB and the psychophysics toolbox</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><volume>34</volume><fpage>613</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.3758/BF03195489</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowe</surname><given-names>DA</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Chafee</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Rapid sequences of population activity patterns dynamically encode task-critical spatial information in parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>11640</fpage><lpage>11653</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0954-10.2010</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neural mechanisms of selective visual attention</article-title><source>Annual Review of Neuroscience</source><volume>18</volume><fpage>193</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Ridgway</surname><given-names>GR</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Wiestler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Comparing the similarity and spatial structure of neural representations: a pattern-component model</article-title><source>NeuroImage</source><volume>55</volume><fpage>1665</fpage><lpage>1678</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.01.044</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Memory on time</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>81</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.12.007</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>TA</given-names></name><name><surname>Chaisangmongkon</surname><given-names>W</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Choice-correlated activity fluctuations underlie learning of neuronal category representation</article-title><source>Nature Communications</source><volume>6</volume><fpage>6454</fpage><pub-id pub-id-type="doi">10.1038/ncomms7454</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Experience-dependent representation of visual categories in parietal cortex</article-title><source>Nature</source><volume>443</volume><fpage>85</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1038/nature05078</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia</surname><given-names>JO</given-names></name><name><surname>Srinivasan</surname><given-names>R</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Near-real-time feature-selective modulations in human cortex</article-title><source>Current Biology : CB</source><volume>23</volume><fpage>515</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.02.013</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghose</surname><given-names>GM</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Physiological correlates of perceptual learning in monkey V1 and V2</article-title><source>Journal of Neurophysiology</source><volume>87</volume><fpage>1867</fpage><lpage>1888</lpage><pub-id pub-id-type="doi">10.1152/jn.00690.2001</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graf</surname><given-names>AB</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Decoding the activity of neuronal populations in macaque primary visual cortex</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>239</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1038/nn.2733</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Coen</surname><given-names>P</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title><source>Nature</source><volume>484</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1038/nature10918</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decoding neural representational spaces using multivariate pattern analysis</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>435</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170325</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hussar</surname><given-names>CR</given-names></name><name><surname>Pasternak</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Memory-guided sensory comparisons in the prefrontal cortex: contribution of putative pyramidal cells and interneurons</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>2747</fpage><lpage>2761</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5135-11.2012</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hussar</surname><given-names>CR</given-names></name><name><surname>Pasternak</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Common rules guide comparisons of speed and direction of motion in the dorsolateral prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>972</fpage><lpage>986</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4075-12.2013</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural ensembles in CA3 transiently encode paths forward of the animal at a decision point</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>12176</fpage><lpage>12189</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3761-07.2007</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>McMains</surname><given-names>SA</given-names></name><name><surname>Beck</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Mechanisms of selective attention in the human visual system: evidence from neuroimaging</article-title><source> The Cognitive Neurosciences</source></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Mechanisms of visual attention in the human cortex</article-title><source>Annual Review of Neuroscience</source><volume>23</volume><fpage>315</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.23.1.315</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>JR</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>JR</given-names></name><name><surname>Faugeras</surname><given-names>F</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Schurger</surname><given-names>A</given-names></name><name><surname>El Karoui</surname><given-names>I</given-names></name><name><surname>Sitt</surname><given-names>JD</given-names></name><name><surname>Rohaut</surname><given-names>B</given-names></name><name><surname>Wacongne</surname><given-names>C</given-names></name><name><surname>Labyt</surname><given-names>E</given-names></name><name><surname>Bekinschtein</surname><given-names>T</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Single-trial decoding of auditory novelty responses facilitates the detection of residual consciousness</article-title><source>NeuroImage</source><volume>83</volume><fpage>726</fpage><lpage>738</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.07.013</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis – connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Musacchia</surname><given-names>G</given-names></name><name><surname>O'Connel</surname><given-names>MN</given-names></name><name><surname>Falchier</surname><given-names>AY</given-names></name><name><surname>Javitt</surname><given-names>DC</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The spectrotemporal filter mechanism of auditory selective attention</article-title><source>Neuron</source><volume>77</volume><fpage>750</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.11.034</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Olfactory network dynamics and the coding of multidimensional signals</article-title><source>Nature Reviews. Neuroscience</source><volume>3</volume><fpage>884</fpage><lpage>895</lpage><pub-id pub-id-type="doi">10.1038/nrn964</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis-Peacock</surname><given-names>JA</given-names></name><name><surname>Drysdale</surname><given-names>AT</given-names></name><name><surname>Oberauer</surname><given-names>K</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural evidence for a distinction between short-term memory and the focus of attention</article-title><source>Journal of Cognitive Neuroscience</source><volume>24</volume><fpage>61</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00140</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname><given-names>JE</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The θ-γ neural code</article-title><source>Neuron</source><volume>77</volume><fpage>1002</fpage><lpage>1016</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.03.007</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of macaque visual cortex</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>24</fpage><lpage>42</lpage></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian inference with probabilistic population codes</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1432</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1038/nn1790</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><uri xlink:href="http://doi.org/">http://doi.org/</uri><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez-Garcia</surname><given-names>M</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural and computational mechanisms of postponed decisions</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>108</volume><fpage>11626</fpage><lpage>11631</lpage><uri xlink:href="http://doi.org/">http://doi.org/</uri><pub-id pub-id-type="doi">10.1073/pnas.1108137108</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname><given-names>JH</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Feature-based attention in visual cortex</article-title><source>Trends in Neurosciences</source><volume>29</volume><fpage>317</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2006.04.001</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname><given-names>CJ</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>431</fpage><lpage>441</lpage></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname><given-names>EM</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamic population coding of category information in inferior temporal and prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>1407</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1152/jn.90248.2008</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Erickson</surname><given-names>CA</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Neural mechanisms of visual working memory in prefrontal cortex of the macaque</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>5154</fpage><lpage>5167</lpage></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Activity of neurons in anterior inferior temporal cortex during a short-term memory task</article-title><source>The Journal of Neuroscience</source><volume>13</volume><fpage>1460</fpage><lpage>1478</lpage></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>J</given-names></name><name><surname>Patterson</surname><given-names>T</given-names></name><name><surname>Ulrich</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Jackknife-based method for measuring LRP onset latency differences</article-title><source>Psychophysiology</source><volume>35</volume><fpage>99</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1111/1469-8986.3510099</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Limber neurons for a nimble mind</article-title><source>Neuron</source><volume>78</volume><fpage>211</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.007</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname><given-names>G</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Synaptic theory of working memory</article-title><source>Science</source><volume>319</volume><fpage>1543</fpage><lpage>1546</lpage><pub-id pub-id-type="doi">10.1126/science.1150769</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>N</given-names></name><name><surname>Rohenkohl</surname><given-names>G</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name><name><surname>Nobre</surname><given-names>A</given-names></name><name><surname>Stokes</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Data from: testing sensory evidence against mnemonic templates</article-title><source>Dryad Digital Repository</source><uri xlink:href="http://dx.doi.org/10.5061/dryad.m57sd">http://dx.doi.org/10.5061/dryad.m57sd</uri></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nikolić</surname><given-names>D</given-names></name><name><surname>Häusler</surname><given-names>S</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Distributed fading memory for stimulus properties in the primary visual cortex</article-title><source>PLoS Biology</source><volume>7</volume><elocation-id>e1000260</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000260</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Wingfield</surname><given-names>C</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Su</surname><given-names>L</given-names></name><name><surname>Marslen-Wilson</surname><given-names>W</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Prlic</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A toolbox for representational similarity analysis</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>B</given-names></name><name><surname>Raskevicius</surname><given-names>J</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name><name><surname>Pertzov</surname><given-names>Y</given-names></name><name><surname>Husain</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Working memory retrieval as a decision process</article-title><source>Journal of Vision</source><volume>14</volume><fpage>2</fpage><pub-id pub-id-type="doi">10.1167/14.2.2</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pilat</surname><given-names>D</given-names></name><name><surname>Fukasaku</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>OECD principles and guidelines for access to research data from public funding</article-title><source>Data Science Journal</source><volume>6</volume><fpage>OD4</fpage><lpage>OD11</lpage><pub-id pub-id-type="doi">10.2481/dsj.6.OD4</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purushothaman</surname><given-names>G</given-names></name><name><surname>Bradley</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural population code for fine perceptual decisions in area MT</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1038/nn1373</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname><given-names>L</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention and biased competition in multi-voxel object representations</article-title><source><italic>Proceedings of the National Academy of Sciences  of the United States of America</italic></source><volume>106</volume><fpage>21447</fpage><lpage>21452</lpage><pub-id pub-id-type="doi">10.1073/pnas.0907330106</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Attentional modulation of visual processing</article-title><source>Annu Rev Neurosci</source><volume>27</volume><fpage>611</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.26.041002.131039</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saproo</surname><given-names>S</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Spatial attention improves the quality of population codes in human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>885</fpage><lpage>895</lpage><uri xlink:href="http://doi.org/">http://doi.org/</uri><pub-id pub-id-type="doi">10.1152/jn.00369.2010</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Low-frequency neuronal oscillations as instruments of sensory selection</article-title><source>Trends in Neurosciences</source><volume>32</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.09.012</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scolari</surname><given-names>M</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Basing perceptual decisions on the most informative sensory neurons</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>2266</fpage><lpage>2273</lpage><pub-id pub-id-type="doi">10.1152/jn.00273.2010</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Saproo</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Computational advances towards linking BOLD and behavior</article-title><source>Neuropsychologia</source><volume>50</volume><fpage>435</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2011.07.013</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sigala</surname><given-names>N</given-names></name><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Nimmo-Smith</surname><given-names>I</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Hierarchical coding for sequential task events in the monkey prefrontal cortex</article-title><source>Proceedings of the National Academy of Sciences</source><volume>105</volume><fpage>11969</fpage><lpage>11974</lpage><pub-id pub-id-type="doi">10.1073/pnas.0802569105</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>MA</given-names></name><name><surname>Ress</surname><given-names>D</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural correlates of sustained spatial attention in human early visual cortex</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>229</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1152/jn.00677.2006</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sreenivasan</surname><given-names>KK</given-names></name><name><surname>Vytlacil</surname><given-names>J</given-names></name><name><surname>D'Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Distributed and dynamic storage of working memory stimulus information in extrastriate cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>1141</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00556</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The spatiotemporal structure of population coding in monkey parietal cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>1167</fpage><lpage>1169</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5144-10.2011</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>M</given-names></name><name><surname>Thompson</surname><given-names>R</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Shape-specific preparatory activity mediates attention to targets in human visual cortex</article-title><source><italic>Proceedings of the National Academy of Sciences of the United States of America</italic></source><volume>106</volume><fpage>19569</fpage><lpage>19574</lpage><pub-id pub-id-type="doi">10.1073/pnas.0905306106</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Sigala</surname><given-names>N</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Wolff</surname><given-names>MJ</given-names></name><name><surname>Spaak</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Decoding rich spatial information with high temporal resolution</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>636</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.08.016</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>‘activity-silent’ working memory in prefrontal cortex: a dynamic coding framework</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>394</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.05.004</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugase-Miyamoto</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Wiener</surname><given-names>MC</given-names></name><name><surname>Optican</surname><given-names>LM</given-names></name><name><surname>Richmond</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Short-term memory trace in rapidly adapting synapses of inferior temporal cortex</article-title><source>PLoS Computational Biology</source><volume>4</volume><elocation-id>e1000073</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000073</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A neural representation of prior information during perceptual inference</article-title><source>Neuron</source><volume>59</volume><fpage>336</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.05.021</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sylvester</surname><given-names>CM</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name><name><surname>Jack</surname><given-names>AI</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Anticipatory and stimulus-evoked blood oxygenation level-dependent modulations related to spatial attention reflect a common additive signal</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>10671</fpage><lpage>10682</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1141-09.2009</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taulu</surname><given-names>S</given-names></name><name><surname>Kajola</surname><given-names>M</given-names></name><name><surname>Simola</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Suppression of interference and artifacts by the signal space separation method</article-title><source>Brain Topography</source><volume>16</volume><fpage>269</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1023/B:BRAT.0000032864.93890.f9</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>F</given-names></name><name><surname>Pratte</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decoding patterns of human brain activity</article-title><source>Annual Review of Psychology</source><volume>63</volume><fpage>483</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-120710-100412</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Martínez Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title><source>Nature</source><volume>399</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/21176</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>R</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>How well do response changes of striate neurons signal differences in orientation: a study in the discriminating monkey</article-title><source>The Journal of Neuroscience</source><volume>10</volume><fpage>3543</fpage><lpage>3558</lpage></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>van den Bosch</surname><given-names>JJF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FOSE: a framework for open science evaluation</article-title><source>Frontiers in Computational Neuroscience</source><volume>6</volume><pub-id pub-id-type="doi">10.3389/fncom.2012.00032</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watanabe</surname><given-names>K</given-names></name><name><surname>Funahashi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural mechanisms of dual-task interference and cognitive capacity limitation in the prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>601</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1038/nn.3667</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolff</surname><given-names>MJ</given-names></name><name><surname>Ding</surname><given-names>J</given-names></name><name><surname>Myers</surname><given-names>NE</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Revealing hidden states in visual working memory using electroencephalography</article-title><source>Frontiers in Systems Neuroscience</source><volume>9</volume><fpage>123</fpage><pub-id pub-id-type="doi">10.3389/fnsys.2015.00123</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>de Gardelle</surname><given-names>V</given-names></name><name><surname>Scholl</surname><given-names>J</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rhythmic fluctuations in evidence accumulation during decision making in the human brain</article-title><source>Neuron</source><volume>76</volume><fpage>847</fpage><lpage>858</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.015</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zemel</surname><given-names>RS</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Probabilistic interpretation of population codes</article-title><source>Neural Computation</source><volume>10</volume><fpage>403</fpage><lpage>430</lpage><uri xlink:href="http://doi.org/">http://doi.org/</uri><pub-id pub-id-type="doi">10.1162/089976698300017818</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.09000.017</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing editor</role><aff id="aff7"><institution>Brown University</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub> <body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for submitting your work entitled &quot;Testing sensory evidence against mnemonic templates&quot; for peer review at <italic>eLife</italic>. Your submission has been favorably evaluated by Eve Marder (Senior Editor), Michael J Frank (Reviewing Editor), and two reviewers, one of whom, Floris de Lange, has agreed to reveal his identity.</p> <p>Overall, we are enthusiastic about this paper. The conceptual issue at hand is topical, the experiment is cleverly designed, and the analyses are novel and insightful. However there are a number of interpretative and analysis limitations that need to be addressed for further consideration.</p> <p>The reviewers have discussed the reviews with one another and the Reviewing editor has drafted this decision to help you prepare a revised submission.</p> <p>Summary:</p> <p>The authors measured EEG/MEG responses during a perceptual decision-making task, in which human subjects had to compare visual stimuli against a mnemonic template. They find evidence of stimulus coding, template coding, as well as coding of the signed difference between stimulus and template. Template coding appeared shortly before/during initial stages of stimulus processing, showing that the search template is transiently re-activated just prior to and during encoding of each stimulus. The authors mimic the geometry of neural responses with a toy computational model that consists of a perceptual, template and decision layer in which the decision layer uses population coding.</p> <p>Essential revisions (contributed by all reviewers):</p> <p>1) Reviewers agreed that while the findings are informative and novel, they are somewhat oversold, with language that is and should be interpreted more conservatively. The main advances of the paper are in showing the efficacy of MEEG methods for decoding and in demonstrating that the search template is transiently re-activated, which is worthwhile. Many of the other interpretations seem to be based on overly stringent (and probably incorrect) assumptions about how a neural code must be perfectly stationary over time, in conjunction with a possible misunderstanding about how different dipole orientations would produce different topographies without reflecting different neural computations. The paper should be reframed. The authors should consider which of the elements they have direct evidence for and which are speculations.</p> <p>2) The core of the paper is in the first 4 or 5 figures; the remaining ones and analysis were less well motivated. One reviewer notes that, after <xref ref-type="fig" rid="fig3">Figure 3</xref>, no data are shown and, by <xref ref-type="fig" rid="fig7">Figure 7</xref>, interpretations are based on parameters far abstracted that it is difficult to know how those relate back to the data. As such, the manuscript is quite dense, with many highly complex analyses and results. These analyses are highly sophisticated, but it is not always clear why a particular analysis is performed. In the same vein, some details of the response-related coding analysis were also unclear. For example, why does the cosine of 'stim-template' reflect the task-relevant information, and the sine the irrelevant information?</p> <p>3) Concerning the section &quot;Stimulus and task codes vary dynamically throughout the epoch&quot;, the authors have not really shown this to be the case. We would need to see how the codes differ, for example with qualitatively changing topographies. If the neural source is oscillatory, then rapid changes in the phases would limit temporal extendability to one cycle and could lead to the misinterpretation that &quot;multiple stimulus-specific codes must have been activated in sequence&quot;. Thus if the codes really vary over time, the topographies would have to be different in a way that is inconsistent with rotating or oscillating dipoles. This is a critical point, because the authors rely on the interpretation of the &quot;code&quot; being dynamic through the rest of the manuscript.</p> <p>But this is more than just methodological. There is an implicit assumption here, which is that a &quot;neural code&quot; is an instantaneous spatial pattern, and so if it changes slightly from millisecond to millisecond, the &quot;code&quot; must be different. This assumption is inconsistent with neurophysiology: It is well established that neurons are sensitive to their history of inputs, and that their precise spike timing carries information about input patterns. Thus a specific representation can include time (frequency multiplexing is a simple illustration of this). Therefore, what the authors call a dynamic code could simply be a static code that has time as a dimension. If the authors were to interpret the findings more conservatively this wouldn't be problematic.</p> <p>4) Rather strong statements are made about sensory representations on the basis of whole-head MEG recordings. For example, they state: &quot;Since coding did not cross-generalize over time, multiple stimulus-specific codes must have been activated in sequence&quot;.</p> <p>As noted above, we are a bit worried about the logic that the (in)ability to read out orientation signals in a generalizable fashion over time must mean that the neural codes are not stable. There are plenty of other reasons why a stable sensory code would nevertheless not generalize over time. For example, MEG measures a mixture of activity of many sources at any moment in time. If a stable source would be accompanied by a variety of other neural sources that change over time (which is almost certainly the case) this could also result in a (partial) break-down of generalization. More generally, the fact that a machine learning algorithm can &quot;decode&quot; a particular feature/stimulus, does not imply that the brain has &quot;encoded&quot; this feature. The 'decoder' could have picked up on any feature that has a non-zero correlation with the feature under investigation. Again it would be prudent to better qualify what can and cannot be concluded from the data.</p> <p>5) Differences between decoding and representational geometry:</p> <p>The differences in generalizability between decoding and representational similarity are intriguing. At first, it appeared that these may simply be an artifact of the method, as opposed to conceptually different characteristics of the neural activity patterns. This is better explained in the Discussion, but the reason for doing these two types of analyses, and how they provide an answer to distinct questions, could be better motivated upfront.</p> <p>6) Discussion on predictive coding:</p> <p>We found the link made to predictive coding somewhat of a stretch. The authors state that the signed difference between template and stimulus is passed down, in line with predictive coding. But also the stimulus itself is encoded, see <xref ref-type="fig" rid="fig3">Figure 3A</xref>. And the prediction error in predictive coding would serve to update perception, not a decision about whether a percept is different from an internal template. This signed difference is mandated by the task in this case. Finally, the statement &quot;Our results, using non-invasive imaging, suggest that template and stimulus coding may be similarly segregated across cortical layers&quot; is quite over-stated. The authors investigate whole-head MEG responses, localizing their signals to roughly the back of the head. How that would suggest segregation across cortical layers is very unclear at best.</p> <p>7) Equations 1 and 2: CC<sup>T</sup> and W<sup>T</sup>W are invertible only if C and W are full column rank. Was this evaluated (for W; I assume the columns of C are all linearly independent) or was the psuedo inverse used? MEEG data are often reduced-rank, particularly after ICA cleaning. A minor aside: It might be useful to mention that these equations are the solutions to AX = b, which would help the linear-algebra-challenged readers understand the analyses.</p> <p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p> <p>Thank you for resubmitting your work entitled &quot;Testing sensory evidence against mnemonic templates&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Eve Marder (Senior Editor), Reviewing Editor Michael Frank, and two reviewers. The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below.</p> <p>As you will see, the second reviewer is satisfied with your revised manuscript, but Reviewer 1 has some lingering concerns. Partly this reflects a difference in taste and style, but please address the specific interpretative and statistical issues that the referee raises regarding <xref ref-type="fig" rid="fig3">Figures 3</xref>, <xref ref-type="fig" rid="fig4">4</xref>, and 6. It is important that these issues are transparent.</p> <p><italic>Reviewer #1:</italic> </p> <p>The authors made some adjustments but the overall manuscript is more or less how it was in the first submission.</p> <p>The Discussion is really long, and most of it is just a rehash of the results rather than actual discussion.</p> <p>The figures are still difficult to interpret, and neither the legends nor the text is very helpful. Examples:</p> <p>I don't understand the outlines in <xref ref-type="fig" rid="fig4">Figure 4A, C</xref>. The legend suggests it's the significance of the difference between the diagonal and off-diagonal, but the outline includes dark red regions very close to the diagonal, and it doesn't include much of the plot that should also be different from the diagonal. And what is the difference between the lines and the opaque shading in panel B?</p> <p>I also don't understand <xref ref-type="fig" rid="fig6">Figure 6</xref>. The small black outline in panel A doesn't seem to match the darker areas in the figure, and is nothing statistically significant in panel B?</p> <p>Is <xref ref-type="fig" rid="fig3">Figure 3</xref> supposed to be the diagonals of <xref ref-type="fig" rid="fig4">Figure 4</xref>? The significances don't seem to map on to each other.</p> <p><italic>Reviewer #2:</italic> </p> <p>I think the authors have dealt with all the reviewers' comments in an exemplary way, and I congratulate the authors with their interesting manuscript.</p></body> </sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.09000.018</article-id><title-group><article-title>Author response</article-title></title-group></front-stub> <body><p><italic>Essential revisions (contributed by all reviewers):</italic> </p> <p><italic>1) Reviewers agreed that while the findings are informative and novel, they are somewhat oversold, with language that is and should be interpreted more conservatively. The main advances of the paper are in showing the efficacy of MEEG methods for decoding and in demonstrating that the search template is transiently re-activated, which is worthwhile. Many of the other interpretations seem to be based on overly stringent (and probably incorrect) assumptions about how a neural code must be perfectly stationary over time, in conjunction with a possible misunderstanding about how different dipole orientations would produce different topographies without reflecting different neural computations. The paper should be reframed. The authors should consider which of the elements they have direct evidence for and which are speculations.</italic></p> <p>We thank the reviewers for the positive appraisal. We have now reconsidered our use of language throughout the paper. As a result, we have toned down our language where appropriate and clarified points that lean more on speculation informed from other studies than the results of the current study. We have also added relevant theoretical discussion to address some of the issues raised by the reviewers. These are detailed below.</p> <p>We appreciate the point raised by the reviewers that neural coding is unlikely mediated by stationary activity patterns. High-temporal resolution methods provide a unique window into such dynamics, where fMRI has previously emphasised a more static patterns of activity associated with neural coding. With MEG in particular, even very subtle differences in dipole orientation can generate different patterns at the scalp surface. This property of MEG probably underpins our ability to decode stimulus orientation based on spatial patterns (see Cichy et al., 2015, NeuroImage; and relevant commentary Stokes, Wolff and Spaak, in press, Trends in Cog Sci). Formal modelling in Cichy et al. shows that even neighbouring sources can generate separable topographies within individual subjects, and this rich source of spatial information can be exploited by multivariate decoding methods. Moreover, our results suggest that these dipoles change over time, resulting in dynamic topographies that render decoding time-specific (despite stable representational structure, see RSA analyses). Such activity dynamics, evident in high-temporal resolution methods, clearly show that we need to move away from overly rigid interpretations of stable codes. In parallel, insights gained from multi-electrode recordings in animal models are helping to unravel the computational significance of dynamic coding.</p> <p>We now provide further explanation in the Discussion of the possible neurophysiological basis of dynamic coding observed in MEG, and how this relates to micro-electrode recordings. We believe this is an exciting area of study at the moment, and are confident that our paper will make a positive contribution to this unfolding story.</p> <p><italic>2) The core of the paper is in the first 4 or 5 figures; the remaining ones and analysis were less well motivated. One reviewer notes that, after <xref ref-type="fig" rid="fig3">Figure 3</xref>, no data are shown and, by <xref ref-type="fig" rid="fig7">Figure 7</xref>, interpretations are based on parameters far abstracted that it is difficult to know how those relate back to the data. As such, the manuscript is quite dense, with many highly complex analyses and results. These analyses are highly sophisticated, but it is not always clear why a particular analysis is performed. In the same vein, some details of the response-related coding analysis were also unclear. For example, why does the cosine of 'stim-template' reflect the task-relevant information, and the sine the irrelevant information?</italic> </p> <p>The second half of the Results section shows progressively more involved analyses, and we thank the reviewers for pointing out that they seemed less well motivated. We have addressed this by removing (or moving into the supplemental materials) any results that were essentially control analyses, and not directly related to advancing the main conclusions. For all results, we have rewritten sections to motivate what new aspects of the data they revealed, and why this was important. As mentioned in response to point 3, we have added a more thorough description of the representational similarity analyses in <xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig7">7A,B</xref>, and why they are an important complement to the time-limited decoding found in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p> <p>The figure illustrating the neural model (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>) is now in the supplemental materials, and the description of the model has been cut down to its essential point. We hope that by making this section more concise, the main argument comes out more clearly, with all the further details still available to the interested reader.</p> <p>Finally, we have made an effort to explain better the analyses of response-related coding in <xref ref-type="fig" rid="fig8">Figure 8</xref>. The significance of this result is that it shows a neural effect at the decision stage that is not strictly necessary for performing the task, but that provides evidence for the underlying neural mechanism. This basic result is illustrated in <xref ref-type="fig" rid="fig8">Figure 8A,B</xref>. <xref ref-type="fig" rid="fig8">Figure 8C,D</xref> showed a control analysis that is now in the supplemental materials.</p> <p>In <xref ref-type="fig" rid="fig8">Figure 8A</xref>, the two task axes measuring sensitivity to the response-related angle (i.e. the angular difference between current stimulus and template) were previously labelled ‘cosine’ and ‘sine’: we now define them as capturing the ‘magnitude’ and ‘sign’ of the response-related angle, respectively. We argue that only the magnitude axis is relevant to the task (and to behaviour). Our reasoning is as follows: participants have a noisy estimate of the template orientation, and a noisy estimate of the current stimulus orientation. If the absolute difference between these two estimates falls below a decision threshold, participants decide that a target is present. Because the decision threshold is non-zero, there will be some false alarms, and because both estimates are noisy, there will sometimes be misses. This accounts for the behavioural data shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. Importantly, this strategy relies solely on the absolute angular difference between stimulus and template (what we now term the magnitude). In our case, the magnitude of the angular distance is captured by its cosine (with a peak at 0º, when the stimulus matches exactly the template, and a trough when the stimulus differs maximally from the template). Conversely, the sign of the angular distance (i.e., whether the stimulus is clockwise or counter clockwise to the template) should be entirely irrelevant to the task. The sign of the angular distance is captured by its sine (with a peak at +90º and a trough at -90º). We have attempted to convey this in rewriting the section (Discussion).</p> <p><italic>3) Concerning the section &quot;Stimulus and task codes vary dynamically throughout the epoch&quot;, the authors have not really shown this to be the case. We would need to see how the codes differ, for example with qualitatively changing topographies. If the neural source is oscillatory, then rapid changes in the phases would limit temporal extendability to one cycle and could lead to the misinterpretation that &quot;multiple stimulus-specific codes must have been activated in sequence&quot;. Thus if the codes really vary over time, the topographies would have to be different in a way that is inconsistent with rotating or oscillating dipoles. This is a critical point, because the authors rely on the interpretation of the &quot;code&quot; being dynamic through the rest of the manuscript.</italic> </p> <p><italic>But this is more than just methodological. There is an implicit assumption here, which is that a &quot;neural code&quot; is an instantaneous spatial pattern, and so if it changes slightly from millisecond to millisecond, the &quot;code&quot; must be different. This assumption is inconsistent with neurophysiology: It is well established that neurons are sensitive to their history of inputs, and that their precise spike timing carries information about input patterns. Thus a specific representation can include time (frequency multiplexing is a simple illustration of this). Therefore, what the authors call a dynamic code could simply be a static code that has time as a dimension. If the authors were to interpret the findings more conservatively this wouldn't be problematic.</italic> </p> <p>We agree that this is a crucial point. Firstly, we completely agree that neurophysiology demonstrates that neural activity is highly dynamic, even when essentially the same information is represented. We have previously reported such dynamics in monkey prefrontal cortex (<xref ref-type="bibr" rid="bib77">Stokes et al., 2013</xref>, Neuron) and agree that diverse neurophysiological factors could supply this inherent time-dimension to patterns of neural activity (see Buonomano and Maass, 2009; Nat Rev Neurosci; Barak and Tsodyks, 2014, Trends in Neurosci; Rabinovich, Simmons and Varona, Trends in Cog Neurosci). This is exactly the purpose of our comparison between decoding and representational geometry. Together, these two complementary approaches demonstrate the reviewers’ point: dynamic changes in patterns of activity, yet stable representational structure (e.g. “a static code with a time dimension”). This <italic>is</italic> our notion of dynamic coding.</p> <p>We use ‘neural code’ to refer to the pattern of activity that systematically maps to specific information content. We argue that coding is dynamic if this mapping changes over time. However, we appreciate that a more descriptive term would be more prudent, and would therefore be more appropriate in many parts of the manuscript. In such cases, we now substitute ‘code’ for ‘pattern’. For example: “This result is consistent with multiple stimulus-specific patterns activated in sequence”. We are happy that this more agnostic term still reflects our main meaning, but with fewer assumptions. We have now also elaborated more clearly in the Results section of the paper the relationship between changing neural patterns and stable information and added to the Discussion further information of how this notion of dynamic coding could provide an information-rich form of coding that is well-suited for complex behaviour (e.g., <xref ref-type="bibr" rid="bib56">Miller and Fusi, 2013</xref>, Neuron). We believe these ideas are consistent with the reviewers’ comments.</p> <p>It is also worth noting that the cross-temporal analysis approach is sensitive to oscillatory (or reversing) structure in discriminative patterns, assuming phase-locking to the task timings (see <xref ref-type="bibr" rid="bib37">King and Dehaene, 2014</xref>). We now make closer reference to this highly relevant review paper, and note that we observe no evidence for a diagnostic pattern of event-related periodicity. We also note that lack of phase-locking would cause a break-down of such a pattern. Nevertheless, such trial-wise phase variability would also reduce evidence of time-specificity in the cross-temporal analysis. Therefore, it is unlikely that superior decoding along the diagonal axis can be accounted for by oscillatory dynamics.</p> <p><italic>4) Rather strong statements are made about sensory representations on the basis of whole-head MEG recordings. For example, they state: &quot;Since coding did not cross-generalize over time, multiple stimulus-specific codes must have been activated in sequence&quot;.</italic> </p> <p><italic>As noted above, we are a bit worried about the logic that the (in)ability to read out orientation signals in a generalizable fashion over time must mean that the neural codes are not stable. There are plenty of other reasons why a stable sensory code would nevertheless not generalize over time. For example, MEG measures a mixture of activity of many sources at any moment in time. If a stable source would be accompanied by a variety of other neural sources that change over time (which is almost certainly the case) this could also result in a (partial) break-down of generalization. More generally, the fact that a machine learning algorithm can &quot;decode&quot; a particular feature/stimulus, does not imply that the brain has &quot;encoded&quot; this feature. The 'decoder' could have picked up on any feature that has a non-zero correlation with the feature under investigation. Again it would be prudent to better qualify what can and cannot be concluded from the data.</italic> </p> <p>We appreciate the limits of M/EEG in decoding neural states, and now take more care to highlight the caveats in our interpretation. First and foremost, we soften our previously over-strong statements (e.g., using ‘could’ rather than ‘must’); but we also now discuss in more detail how extraneous factors could limit decodability. For example, systematic non-linear interactions could in principle limit decodability across different time points. Such interactions would limit cross-temporal generalisation, which also raises the question how such dynamics are handled in the brain. We agree with the reviewers that decodability does not necessarily reflect encoding in the brain (and now touch on this point in the Discussion). Until we understand exactly how activity patterns are read out by downstream areas, we cannot be sure what information is meaningfully encoded in the signal. Therefore, we must assume a more agnostic interpretation: if information is decodable in principle, then it could be neurally relevant code.</p> <p><italic>5) Differences between decoding and representational geometry:</italic></p> <p><italic>The differences in generalizability between decoding and representational similarity are intriguing. At first, it appeared that these may simply be an artifact of the method, as opposed to conceptually different characteristics of the neural activity patterns. This is better explained in the Discussion, but the reason for doing these two types of analyses, and how they provide an answer to distinct questions, could be better motivated upfront.</italic> </p> <p>As mentioned above (in response to points 2 and 3), this analysis formalises some of the important issues raised by the reviewers. In brief, the representational similarity analyses show that dynamic patterns of activity can nonetheless yield time-stable information content. We have now explained this in more detail in the Results section.</p> <p><italic>6) Discussion on predictive coding:</italic></p> <p><italic>We found the link made to predictive coding somewhat of a stretch. The authors state that the signed difference between template and stimulus is passed down, in line with predictive coding. But also the stimulus itself is encoded, see <xref ref-type="fig" rid="fig3">Figure 3A</xref>. And the prediction error in predictive coding would serve to update perception, not a decision about whether a percept is different from an internal template. This signed difference is mandated by the task in this case. Finally, the statement &quot;Our results, using non-invasive imaging, suggest that template and stimulus coding may be similarly segregated across cortical layers&quot; is quite over-stated. The authors investigate whole-head MEG responses, localizing their signals to roughly the back of the head. How that would suggest segregation across cortical layers is very unclear at best.</italic> </p> <p>The reviewers make a fair point that the link between our findings and predictive coding might be made more transparent. In the Discussion, we have now removed the comparison to prediction errors and the speculation about layer-specific responses. We do believe that our findings fit most naturally with the idea of gain modulation as articulated by Feldman and Friston. Therefore, we have left a concise reference linking our work to theirs in the text.</p> <p><italic>7) Equations 1 and 2: CC<sup>T</sup> and W<sup>T</sup>W are invertible only if C and W are full column rank. Was this evaluated (for W; I assume the columns of C are all linearly independent) or was the psuedo inverse used? MEEG data are often reduced-rank, particularly after ICA cleaning. A minor aside: It might be useful to mention that these equations are the solutions to AX = b, which would help the linear-algebra-challenged readers understand the analyses.</italic></p> <p>Matrices C and W were indeed of full rank, but analyses were done on the pseudo-inverse of W. We have amended this oversight in the Methods section, and have added a first equation (B = WC) for clarity.</p> <p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p> <p><italic>As you will see, the second reviewer is satisfied with your revised manuscript, but Reviewer 1 has some lingering concerns. Partly this reflects a difference in taste and style, but please address the specific interpretative and statistical issues that the referee raises regarding <xref ref-type="fig" rid="fig3">Figures 3</xref>, <xref ref-type="fig" rid="fig4">4</xref>, and 6. It is important that these issues are transparent.</italic></p> <p>We are grateful to the editors and reviewers for the care and time they have taken in reviewing our manuscript. We have now made an effort to articulate our results as transparently as possible. We are particularly grateful to Reviewer 1 for spotting an apparent discrepancy between <xref ref-type="fig" rid="fig3">Figures 3 and 4</xref>. This turned out to be the result of an oversight when updating figures to match precise analysis parameters between within-time and cross-time analyses. We have updated the relevant figures in the revised version of the manuscript. The results remain qualitatively and statistically the same, and the discrepancy is now resolved. Please see the response below for further details on this. The first and final authors have double checked all the uploaded data and analysis scripts to ensure that the methods and reported results are consistent and as easy as possible to follow.</p> <p><italic>Reviewer #1: The authors made some adjustments but the overall manuscript is more or less how it was in the first submission. The Discussion is really long, and most of it is just a rehash of the results rather than actual discussion.</italic> </p> <p>We agree, and have now taken greater care to avoid recapping results and have focused on points for discussion.</p> <p><italic>The figures are still difficult to interpret, and neither the legends nor the text is very helpful. Examples:</italic> </p> <p>We apologize that the text and legends still lacked clarity in places. We have made a renewed effort to explain what we show (see detailed responses below).</p> <p><italic>I don't understand the outlines in <xref ref-type="fig" rid="fig4">Figure 4A,C</xref>. The legend suggests it's the significance of the difference between the diagonal and off-diagonal, but the outline includes dark red regions very close to the diagonal, and it doesn't include much of the plot that should also be different from the diagonal. And what is the difference between the lines and the opaque shading in panel B?</italic> </p> <p>This is a crucial point, and we are grateful for an opportunity to clarify it. The color in the figure indicates the strength of decoding, with the cluster-corrected significant regions indicated by the opaque shading. Our index of dynamic coding requires both on-diagonal timepoints to be significantly higher than the corresponding cross-temporal (off-diagonal) generalization, which is why the dynamic cluster (black outline) only appears in a relatively small part of the plot where corresponding on-diagonal coding is high. Importantly, the dynamic cluster includes some points near the diagonal: while these still show significant decoding, it is nonetheless significantly lower than the corresponding on-diagonal timepoints. In this sense, we interpret the underlying pattern to have changed significantly, even though there is also some shared pattern allowing for significant cross-generalisation. Conversely, there are timepoints on the diagonal (approx. 350-400 ms, near the end of the significant cluster) that show significant decoding that is nonetheless too weak to be significantly larger than the off-diagonal – for this reason, the off-diagonal dynamic coding cluster (i.e. the black outline) might not always extend quite as far along the y-axis as the significant on-diagonal cluster.</p> <p>In <xref ref-type="fig" rid="fig4">Figure 4B</xref> (template decoding), there was no significant drop from on-diagonal to off-diagonal decoding, so there is no black outline. As with <xref ref-type="fig" rid="fig4">Figure 4A and 4C</xref>, the opaque shading showed the timepoints with significant decoding.</p> <p>We have attempted to make these points clearer in the presentation of the results (Results section) and in the legend to <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p> <p><italic>I also don't understand <xref ref-type="fig" rid="fig6">Figure 6</xref>. The small black outline in panel A doesn't seem to match the darker areas in the figure, and is nothing statistically significant in panel B?</italic> </p> <p>We used the same conventions here as in <xref ref-type="fig" rid="fig4">Figure 4</xref>: significant clusters are indicated by the opaque shading (there is one large significant cluster in each panel), and a significant off-diagonal reduction (compared to on-diagonal) is shown in the black outline. There was no significant off-diagonal reduction in panel b. We apologize that the figure legend was confusing. We have now rewritten it to improve clarity.</p> <p><italic>Is <xref ref-type="fig" rid="fig3">Figure 3</xref> supposed to be the diagonals of <xref ref-type="fig" rid="fig4">Figure 4</xref>? The significances don't seem to map on to each other.</italic> </p> <p>Yes, these two should match. That is, we should expect the significant cluster in <xref ref-type="fig" rid="fig3">Figure 3C</xref> (within-time template decoding) to match significant on-diagonal timepoints in <xref ref-type="fig" rid="fig4">Figure 4B</xref> (cross-temporal template decoding). The discrepancy was an oversight on our part: it was caused by an accidental inclusion of an old version of the within-time analysis using slightly different baselining (from -250 to -100 ms, instead of -200 to -150 ms). We have now updated the figure accordingly, and as expected, the within-time decoding (<xref ref-type="fig" rid="fig3">Figure 3C</xref>) is also significant just before stimulus onset (-72 ms onwards). A comparison of the old and new result shows almost identical curves, but the onset of template decoding is now significantly earlier than the onset of stimulus decoding. We have updated the corresponding part of the Results. However, this change in the onset of the significant window does not change our interpretation of the results, since we had already discussed the likelihood that part of the template reactivation could occur shortly before stimulus onset (end of Discussion) as the same pre-stimulus time period was near-significant (p &lt; 0.07) even in the previously reported analysis.</p> <p>As mentioned in the original version of the paper, we are committed to the principles of Open Science, and therefore make all data and analysis scripts freely available online for complete transparency. In preparing the final version to accompany this paper, we found other minor inconsistencies, none of which significantly change the qualitative or statistical aspects of our results. Specifically, the cross-time decoding analysis of the stimulus and relative angle (presented in <xref ref-type="fig" rid="fig4">Figure 4A and 4C</xref>) was based on a sliding window size of 16 ms rather than the stated 20 ms. Both yield almost identical results, and therefore we have simply updated the figure with the appropriate analysis without any other change. The cross-generalization analysis between templates and stimuli (<xref ref-type="fig" rid="fig5">Figure 5</xref>) had also used a different baseline (as stated above). Using the more consistent baseline yields the same cluster in time, although it is only a strong trend after correcting for multiple comparisons (p = 0.06). We have updated the results accordingly. The representational similarity comparison of templates and stimuli (<xref ref-type="fig" rid="fig7">Figure 7A,B,G</xref>) was calculated with a different baseline as stated (-250 to -100 ms, i.e., consistent with the old version of <xref ref-type="fig" rid="fig3">Figure 3C</xref>). We have now updated this figure using the baseline reported in the Methods (-200 to -150 ms). Again, the results remain practically unchanged, however we note that the significant cluster in <xref ref-type="fig" rid="fig7">Figure 7A</xref> has shifted slightly. Finally, the angular distance analysis (<xref ref-type="fig" rid="fig8">Figure 8</xref>) had also used a slightly different baseline window. Once again, the updated results are almost indistinguishable.</p> <p>We are now satisfied that all reported results faithfully reflect the methods as originally outlined.</p></body> </sub-article></article>