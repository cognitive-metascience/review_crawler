<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="discussion" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">02791</article-id><article-id pub-id-type="doi">10.7554/eLife.02791</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Feature Article</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Living Science</subject></subj-group></article-categories><title-group><article-title>In numbers we trust?</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-1021"><name><surname>Marder</surname><given-names>Eve</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="conf1"/><x> is a </x><role>Senior Editor</role><x>, and is in the </x><aff id="aff1"><institution content-type="dept">Department of Biology and the Volen National Center for Complex Systems</institution>, <institution>Brandeis University</institution>, <addr-line><named-content content-type="city">Waltham</named-content></addr-line>, <country>United States</country> <email>marder@brandeis.edu</email></aff></contrib></contrib-group><pub-date date-type="pub" publication-format="electronic"><day>01</day><month>04</month><year>2014</year></pub-date><pub-date pub-type="collection"><year>2014</year></pub-date><volume>3</volume><elocation-id>e02791</elocation-id><permissions><copyright-statement>© 2014, Marder</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Marder</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-02791-v1.pdf"/><abstract><p>Scientists go to great lengths to ensure that data are collected and analysed properly, so why, asks <bold>Eve Marder</bold>, do they apply different standards to data about the number of times research papers have been cited and viewed?</p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>living science</kwd><kwd>research assessment</kwd><kwd>scientific publishing</kwd><kwd>publishing</kwd><kwd>article-level metric</kwd><kwd>eLife</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Scientists go to great lengths to ensure that data are collected and analysed properly, so why do they apply different standards to data about the number of times research papers have been cited and viewed?</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p>We are all scientists and we believe in measuring, counting, and calculating. We measure the length of cilia, we monitor the reaction times of humans, and we would count the exact number of whales in the ocean, if we could. We learn how to estimate when we can’t measure and count, and a big part of becoming a scientist is learning how to evaluate the accuracy and reliability of our data. We worry about calibrating our instruments, we obsess about the resolution of our microscopes, and we stress about false positives and false negatives. We calculate statistics, either correctly or incorrectly, in hope of gaining confidence that we are capturing some truth through our measurements. As scientists, there are remarkably few instances when we would think a single measurement or count is ‘the truth’.<fig id="fig1" position="float"><caption><p>Scientists should focus on the science in research papers, not the number of times the paper has been cited or viewed.</p></caption><graphic xlink:href="elife-02791-fig1-v1.tif"/><attrib>ILLUSTRATION: BEN MARDER.</attrib></fig></p><p>Many, many editorials and opinion pieces have been published about the right and wrong metrics to use to evaluate journals, university departments or individual scientists. Journal impact factors, the numbers of citations, h-indexes, page views, downloads and various other measures have their advocates and their critics but—with a few exceptions—the conversation about these metrics has involved an innocent and blind assumption that they have been calculated in a reliable fashion. Otherwise, no sane person could imagine using any or all of these metrics to inform decisions about hiring or funding!</p><p>I recently saw a CV from an investigator who used Google Scholar as the source of his h-index and the number of citations for each of his papers. I had never looked at Google Scholar, so on a whim I checked my own h-index and the number of citations for some of my papers on both Google Scholar and Web of Science. (I was avoiding doing a particularly annoying academic task, so this was an amusing waste of time). To my surprise, my h-index was 72 on Google Scholar and 64 on Web of Science, a not inconsequential difference. Then I compared the number of times that some of my papers had been cited. I found that one paper (<xref ref-type="bibr" rid="bib1">Hooper and Marder, 1987</xref>) had 167 citations in Google Scholar and 161 in Web of Science, which was a difference of just a few per cent. But then I discovered that another paper (<xref ref-type="bibr" rid="bib2">Marder and Calabrese, 1996</xref>) had 894 citations on Google Scholar and only 636 citations on Web of Science, which was a difference of about 30%!</p><p><italic>eLife</italic>, like many journals, reports the number of times each article in the journal has been viewed (along with the number of times it has been cited according to Scopus, HighWire and Google Scholar) as another way of assessing the impact of individual articles. In the past 18 months I have published five ‘Living Science’ essays in <italic>eLife</italic>. Every few months prurient interest and vanity have caused me to look at number of views each of these pieces has attracted. Imagine my confusion when, recently, I discovered that the number of views for two of them had dropped substantially during the past several months! I still don’t know if the old values or new values are correct, or why the number of views has decreased, but it does give me pause in believing the reliability of such numbers, not only in <italic>eLife</italic>, but as posted by all journals.</p><p>But then I started thinking. Why should we have ever trusted those numbers? What kinds of systematic, or field specific, or journal specific, or author specific errors are likely in citation data? How is it possible that thousands of careful scientists who spend months and years checking and rechecking their own data, would blithely and blindly accept these numbers? Is a journal impact factor of 3.82 different from one of 4.3, even though there are some review panels who treat them as if they are different and make decisions about promotions accordingly? How many of us would not ask questions of investigators in their own fields about how the data were collected, analysed and verified? And yet how many of us have the foggiest idea how exactly the numbers of citations for publications have been collected, and what verification procedures are used?</p><disp-quote><p>How many of us have the foggiest idea how exactly the numbers of citations for publications have been collected?</p></disp-quote><p>One might argue that the actual numbers do not matter, and it is only orders of magnitude at issue. If so, why even bother with the numbers? At the limit, the numbers just validate what we already know. Someone like Linda Buck or Eric Kandel has more citations than any assistant professor. But we don’t need indexes to tell us that Nobel Prize winners have had impact and influence on their fields, and that scientists who have only been in the field for 10 years have had less cumulative effect.</p><p>When I was three years old I got my head stuck between the bars of a playground in Central Park because I was curious if my head was larger or smaller than the distance between the bars. As I stood there unable to get my head out of the bars, I felt very stupid, as I realised I could have measured the distance between the bars by holding my hands apart, without getting stuck. It was my first real experience of feeling truly self-consciously dumb. Many years later I feel equally stupid for believing that the numbers that have become so important to so many careers are reliable. I have never believed that these numbers are wisely used in assessments, but now I know there is all the more reason to forget all of these indexes and metrics and to focus on the science.</p></body><back><fn-group content-type="competing-interest"><fn fn-type="conflict" id="conf1"><label>Competing interests:</label><p>The author declares that no competing interests exist.</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hooper</surname><given-names>SL</given-names></name><name><surname>Marder</surname><given-names>E</given-names></name></person-group><year>1987</year><article-title>Modulation of the lobster pyloric rhythm by the peptide proctolin</article-title><source>Journal of Neuroscience</source><volume>7</volume><fpage>2097</fpage><lpage>2112</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marder</surname><given-names>E</given-names></name><name><surname>Calabrese</surname><given-names>RL</given-names></name></person-group><year>1996</year><article-title>Principles of rhythmic motor pattern generation</article-title><source>Physiological Reviews</source><volume>76</volume><fpage>687</fpage><lpage>717</lpage></element-citation></ref></ref-list></back></article>