<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">47027</article-id><article-id pub-id-type="doi">10.7554/eLife.47027</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Population rate-coding predicts correctly that human sound localization depends on sound intensity</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-134867"><name><surname>Ihlefeld</surname><given-names>Antje</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7185-5848</contrib-id><email>antje.ihlefeld@njit.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-136097"><name><surname>Alamatsaz</surname><given-names>Nima</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3374-3663</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-136098"><name><surname>Shapley</surname><given-names>Robert M</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>New Jersey Institute of Technology</institution><addr-line><named-content content-type="city">Newark</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Rutgers University</institution><addr-line><named-content content-type="city">Newark</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Carr</surname><given-names>Catherine Emily</given-names></name><role>Reviewing Editor</role><aff><institution>University of Maryland</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>21</day><month>10</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e47027</elocation-id><history><date date-type="received" iso-8601-date="2019-03-20"><day>20</day><month>03</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-09-20"><day>20</day><month>09</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Ihlefeld et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Ihlefeld et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-47027-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.47027.001</object-id><p>Human sound localization is an important computation performed by the brain. Models of sound localization commonly assume that sound lateralization from interaural time differences is level invariant. Here we observe that two prevalent theories of sound localization make opposing predictions. The labelled-line model encodes location through tuned representations of spatial location and predicts that perceived direction is level invariant. In contrast, the hemispheric-difference model encodes location through spike-rate and predicts that perceived direction becomes medially biased at low sound levels. Here, behavioral experiments find that softer sounds are perceived closer to midline than louder sounds, favoring rate-coding models of human sound localization. Analogously, visual depth perception, which is based on interocular disparity, depends on the contrast of the target. The similar results in hearing and vision suggest that the brain may use a canonical computation of location: encoding perceived location through population spike rate relative to baseline.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.47027.002</object-id><title>eLife digest</title><p>Being able to localize sounds helps us make sense of the world around us. The brain works out sound direction by comparing the times of when sound reaches the left versus the right ear. This cue is known as interaural time difference, or ITD for short. But how exactly the brain decodes this information is still unknown.</p><p>The brain contains nerve cells that each show maximum activity in response to one particular ITD. One idea is that these nerve cells are arranged in the brain like a map from left to right, and that the brain then uses this map to estimate sound direction. This is known as the Jeffress model, after the scientist who first proposed it. There is some evidence that birds and alligators actually use a system like this to localize sounds, but no such map of nerve cells has yet been identified in mammals. An alternative possibility is that the brain compares activity across groups of ITD-sensitive nerve cells. One of the oldest and simplest ways to measure this is to compare nerve activity in the left and right hemispheres of the brain. This readout is known as the hemispheric difference model.</p><p>By analyzing data from published studies, Ihlefeld, Alamatsaz, and Shapley discovered that these two models make opposing predictions about the effects of volume. The Jeffress model predicts that the volume of a sound will not affect a person’s ability to localize it. By contrast, the hemispheric difference model predicts that very soft sounds will lead to systematic errors, so that for the same ITD, softer sounds are perceived closer towards the front than louder sounds. To investigate this further, Ihlefeld, Alamatsaz, and Shapley asked healthy volunteers to localize sounds of different volumes. The volunteers tended to mis-localize quieter sounds, believing them to be closer to the body’s midline than they actually were, which is inconsistent with the predictions of the Jeffress model.</p><p>These new findings also reveal key parallels to processing in the visual system. Visual areas of the brain estimate how far away an object is by comparing the input that reaches the two eyes. But these estimates are also systematically less accurate for low-contrast stimuli than for high-contrast ones, just as sound localization is less accurate for softer sounds than for louder ones. The idea that the brain uses the same basic strategy to localize both sights and sounds generates a number of predictions for future studies to test.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>interaural time difference</kwd><kwd>neural coding</kwd><kwd>Jeffress model</kwd><kwd>sound localization</kwd><kwd>psychometrics</kwd><kwd>hearing</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001774</institution-id><institution>New Jersey Health Foundation</institution></institution-wrap></funding-source><award-id>PC 24-18</award-id><principal-award-recipient><name><surname>Ihlefeld</surname><given-names>Antje</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Softer sound appears closer to midline than louder sound, conflicting with a labelled-line representation of auditory space and supporting the idea that humans use rate coding when calculating sound directionality.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A fundamental question of human perception is how we perceive target locations in space. Through our eyes and skin, the activation patterns of sensory organs provide rich spatial cues. However, for other sensory dimensions, including sound localization and visual depth perception, spatial locations must be computed by the brain. For instance, interaural time differences (ITDs) of the sounds reaching the ears allow listeners to localize sound in the horizontal plane. In the ascending mammalian auditory pathway, the first neural processing stage where ITDs are encoded, on the timescale of microseconds, is the medial superior olive (MSO). Here, temporally precise binaural inputs converge, and their ITDs are converted to neural firing rate (<xref ref-type="bibr" rid="bib18">Goldberg and Brown, 1968</xref>; <xref ref-type="bibr" rid="bib64">Yin and Chan, 1990</xref>; <xref ref-type="bibr" rid="bib53">Spitzer and Semple, 1995</xref>; <xref ref-type="bibr" rid="bib39">Pecka et al., 2010</xref>; <xref ref-type="bibr" rid="bib10">Day and Semple, 2011</xref>). The shape of the MSO output firing rate curves as a function of ITD resembles that of a cross-correlation operation on the inputs to each ear (<xref ref-type="bibr" rid="bib2">Batra and Yin, 2004</xref>). How this information is interpreted downstream of the MSO has led to the development of conflicting theories on the neural mechanisms of sound localization in humans. One prominent neural model for sound localization, originally proposed by <xref ref-type="bibr" rid="bib30">Jeffress (1948)</xref>, consists of a labelled line of coincidence detector neurons that are sensitive to the binaural synchronicity of neural inputs from each ear, with each neuron maximally sensitive to a specific magnitude of ITD (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). This labelled-line model is computationally equivalent to a neural place-code based on bandlimited cross-correlations of the sounds reaching both ears (<xref ref-type="bibr" rid="bib14">Domnitz and Colburn, 1977</xref>). Several studies support the existence of labelled-line neural place-code mechanisms in the avian brain (<xref ref-type="bibr" rid="bib7">Carr and Konishi, 1988</xref>; <xref ref-type="bibr" rid="bib36">Overholt et al., 1992</xref>), and versions of it have successfully been applied in many engineering applications predicting human localization performance (e.g. <xref ref-type="bibr" rid="bib15">Durlach, 1963</xref>; <xref ref-type="bibr" rid="bib24">Hafter, 1971</xref>; <xref ref-type="bibr" rid="bib56">Stern and Trahiotis, 1995</xref>; <xref ref-type="bibr" rid="bib4">Breebaart et al., 2001</xref>; <xref ref-type="bibr" rid="bib25">Hartmann et al., 2005</xref>).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.47027.003</object-id><label>Figure 1.</label><caption><title>Modeling results.</title><p>(<bold>A</bold>) Firing rate of a simulated <italic>nucleus laminaris</italic> neuron with a preferred ITD of 375 µs, as a function of source ITD. The model predicts source laterality based on the locus of the peak of the firing rate function. (<bold>B</bold>) Hemispheric differences in firing rates, averaged across all 81 simulated <italic>inferior colliculus</italic> units. Rate models assume that source laterality is proportional to firing rate, causing ambiguities at the lowest sound intensities. Inset: Reconstructed responses of an <italic>inferior colliculus</italic> unit. The unit predominantly responds contralaterally to the direction of sound (high-contrast traces). The hemispheric difference model subtracts this activity from the average rate on the ipsilateral side (example shown with low-contrast traces). (<bold>C</bold>) Mean population response using labelled-line coding across a range of ITDs and sound intensities. Inset: The root-mean square (RMS) difference relative to estimated angle at 80 dB SPL does not change with sound intensity, predicting that sound laterality is intensity invariant. (<bold>D</bold>) Mean population response using hemispheric-difference coding. For lower sound intensities, predicted source direction is biased towards midline (compare red and orange versus blue or yellow). For higher sound intensities, predicted source direction is intensity invariant (blue on top of yellow line). Inset: RMS difference relative to estimated angle at 80 dB SPL decreases with increasing sound intensity, predicting that sound laterality is not intensity invariant. Ribbons show one standard error of the mean across 100 simulated responses. Sound intensity is denoted by color (see color key in the figure).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47027-fig1-v1.tif"/></fig><p>A growing literature proposes an alternative to the labelled-line model to explain mammalian sensitivity to ITD (<xref ref-type="bibr" rid="bib32">Lee and Groh, 2014</xref>). One reason for an alternative is that two excitatory inputs should suffice to implement the labelled-line model, but evidence from experiments on Mongolian gerbils shows that in addition to bilateral excitatory inputs, sharply tuned bilateral inhibitory inputs to the MSO play a crucial role in processing ITDs (<xref ref-type="bibr" rid="bib3">Brand et al., 2002</xref>). Moreover, to date no labelled-line type neurons encoding auditory space have been discovered in a mammalian species. Indeed, using a population rate-code, several studies proposed that mammalian sound localization can be modeled based on differences in firing rates across the two populations of neurons that are tuned to opposing hemispheres (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="bibr" rid="bib58">van Bergeijk, 1962</xref>; <xref ref-type="bibr" rid="bib34">McAlpine and Grothe, 2003</xref>; <xref ref-type="bibr" rid="bib11">Devore et al., 2009</xref>). Rate-based models generally predict that neuronal responses carry most information at the steepest slopes of neural-discharge-rate versus ITD curves, where neural discharge changes most strongly (<xref ref-type="bibr" rid="bib55">Stecker et al., 2005</xref>), consistent with the observation that the peak ITDs of rate-ITD curves often fall outside the physiologically plausible range (<xref ref-type="bibr" rid="bib34">McAlpine and Grothe, 2003</xref>; <xref ref-type="bibr" rid="bib23">Grothe et al., 2010</xref>; but see also <xref ref-type="bibr" rid="bib31">Joris et al., 2006</xref>). In addition, some authors have suggested that how mammalian sound localization adapts to stimulus history further supports a rate-based neural population code, as assessed behaviorally or via magnetoencephalography (<xref ref-type="bibr" rid="bib41">Phillips and Hall, 2005</xref>; <xref ref-type="bibr" rid="bib54">Stange et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Salminen et al., 2010</xref>).</p><p>It is unknown which of the two competing models, broadly characterized as labelled-line versus rate-code model, describes human sound localization better. Here, we observe that the two different models predict different dependencies of sound localization on sound intensity. By combining behavioral data on sound intensity dependence in normal-hearing listeners with numerical predictions of human sound lateralization from both models, we attempt to disentangle whether human auditory perception is based on a place-code, akin to the labelled-line model, or whether it is instead more closely described by a population rate-code.</p><p>An extensive physiology literature characterizes labelled-line versus population-rate type neurons and suggests that, at least from the perspective of evolution, birds and mammals use different neural mechanisms to calculate sound direction (review: <xref ref-type="bibr" rid="bib22">Grothe, 2003</xref>). Thus, we searched the avian and mammalian physiology literature and identified two studies that characterized labelled-line versus population rate-code neurons at low sound levels and as a function of both sound level and ITD (<xref ref-type="bibr" rid="bib40">Peña et al., 1996</xref>; <xref ref-type="bibr" rid="bib66">Zwiers et al., 2004</xref>). Both <xref ref-type="bibr" rid="bib40">Peña et al. (1996)</xref> and <xref ref-type="bibr" rid="bib66">Zwiers et al. (2004)</xref> report neural firing rate in response to acoustic noise stimuli and are thus suitable for predicting each model’s sensitivity to the acoustic noises we tested in the current study. Here, we ran a meta-analysis, reconstructing simulated neurons with response characteristics from each of the two studies and using maximum likelihood estimation to predict source laterality from these previous findings.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Model Predictions</title><p>To predict how lateralization depends on sound intensity from the responses of labelled-line neurons, we estimated neural firing rates from previous recordings in the nucleus laminaris in barn owl (<xref ref-type="bibr" rid="bib40">Peña et al., 1996</xref>). To estimate lateralization’s dependence on level based on a population rate-code, we used previous recordings from the <italic>inferior colliculus</italic> of rhesus macaque monkey and calculated hemispheric differences in firing rate (<xref ref-type="bibr" rid="bib66">Zwiers et al., 2004</xref>). The labelled-line neurons predicted that, as sound intensity decreases, perceived source laterality would converge towards similar means for low versus high sound intensities, with increased response variability at decreasing sound intensities (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In contrast, the hemispheric-difference model predicted that as sound intensity decreases to near threshold levels, perceived laterality would become increasingly biased toward the midline reference (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, for example note the shallower slope and thus compressed laterality percepts for red versus blue curves). At higher overall sound intensities, both models predicted that lateralization would be intensity invariant (see insets in <xref ref-type="fig" rid="fig1">Figure 1C</xref> versus D). Therefore, analyzing how sound intensity affects perceived sound direction near sensation threshold offers an opportunity to disentangle whether our human auditory system relies on a place-based or rate-based population code for localizing sound based on ITD.</p><p>A listener’s ability to discriminate ITD can vary with sound intensity (<xref ref-type="bibr" rid="bib12">Dietz et al., 2013</xref>). However, it is difficult to interpret previous findings linking sensitivity to ITD and a listener’s judgement of sound source direction as a function of sound intensity. Some reported decreased perceived source laterality near sensation threshold (<xref ref-type="bibr" rid="bib57">Teas, 1962</xref>; <xref ref-type="bibr" rid="bib49">Sabin et al., 2005</xref>), but others reported weak or no level effects on perceived lateralization (<xref ref-type="bibr" rid="bib60">Von Békésy and Wever, 1960</xref>; <xref ref-type="bibr" rid="bib35">Mickunas, 1963</xref>; <xref ref-type="bibr" rid="bib26">Hartmann and Rakerd, 1993</xref>; <xref ref-type="bibr" rid="bib33">Macpherson and Middlebrooks, 2000</xref>; <xref ref-type="bibr" rid="bib29">Inoue, 2001</xref>; <xref ref-type="bibr" rid="bib59">Vliegen and Van Opstal, 2004</xref>; <xref ref-type="bibr" rid="bib6">Brungart and Simpson, 2008</xref>; <xref ref-type="bibr" rid="bib17">Gai et al., 2013</xref>). Several factors complicate the interpretation of these previous findings in the context of the current hypothesis. For instance, assuming an approximately 30 dB dynamic range of rate-level function either at the MSO or downstream in the binaural pathway (e.g. <italic>medial superior olive</italic>: <xref ref-type="bibr" rid="bib18">Goldberg and Brown, 1968</xref>; <italic>inferior colliculus</italic>: <xref ref-type="bibr" rid="bib66">Zwiers et al., 2004</xref>), for stimuli at higher sensation levels (SL) where the rate-level functions saturate, both the labelled-line and the hemispheric difference model predict level invariance. This could explain how studies that tested for the role of sound level over a range of high intensities did not see an effect. Moreover, when presented in the free field, sounds also contain interaural level differences and spectral cues, in addition to ITD. For low-frequency sound, listeners rely dominantly on ITD when judging lateral source angle. However, for broadband sound, listeners integrate across all three types of spatial cue (<xref ref-type="bibr" rid="bib62">Wightman and Kistler, 1992</xref>; <xref ref-type="bibr" rid="bib28">Ihlefeld and Shinn-Cunningham, 2011</xref>). Unlike ITDs, interaural level differences and overall sound intensity both decrease with increasing source distance, raising the possibility that for stimuli with high-frequency content, listeners judged softer sounds to be more medial because they interpreted them to be farther away than louder sounds. Further, at low sound intensities, the sound-direction-related notches of the spectral cues at high-frequencies should have been less audible than at higher sound intensities, increasing stimulus ambiguity. A resulting increase in response variability may have obscured the effect of sound intensity on ITD coding. Finally, some historic studies used only two or three listeners, suggesting that they may have been statistically underpowered. Thus, the literature provides insufficient evidence on how ITD-based lateralization varies with sound level near sensation threshold.</p></sec><sec id="s2-2"><title>Human perception</title><p>Here, we contrasted two competing hypotheses toward the goal of disentangling whether ITD-based human sound localization relies on a labelled-line versus a population rate-place neural code. The labelled-line code hypothesis predicted that the mean perceived direction based on ITD would be intensity invariant, even at intensities close to SL. Using a psychophysical paradigm, we studied lateralization based on ITD as a function of sound intensity in a group of ten normally hearing listeners (experiment 1). Stimuli consisted of low-frequency noise tokens that were bandlimited to cover most of the frequency range where humans can discriminate ITD (<xref ref-type="bibr" rid="bib5">Brughera et al., 2013</xref>; here, corner frequencies from 300 to 1200 Hz, shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>). In each one-interval trial, listeners had to indicate perceived laterality across a range of ITDs from −375 to 375 μs. Lateralization was measured as function of SL. To examine how sound intensity affects perceived ITD coding of source direction, we modelled perceived laterality with a nonlinear mixed effect model (NLME) that included fixed effects of ITD and sound intensity as well as a random effect of listener.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.47027.004</object-id><label>Figure 2.</label><caption><title>Behavioral results.</title><p>(<bold>A</bold>) Stimuli: spectrally flat noise, used in experiment 1 (dark grey) versus A-weighted noise, tested as a control for audibility in experiment 2 (light grey). The purple line shows the magnitude of the zero-phase inverse A-weighting filter. (<bold>B</bold>) Responses from one representative listener (TCW) across two sound intensities and the corresponding NLME fits for these data. (<bold>C and D</bold>) Perceived laterality as a function of ITD for C) spectrally flat noise (experiment 1) or D) A-weighted noise (experiment 2). Error bars, where large enough to be visible, show one standard error of the mean across listeners. Colors denote sound intensity. Insets illustrate magnified section of the plots. Circles show raw data, lines and ribbons show NLME fits and one standard of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47027-fig2-v1.tif"/></fig><p><xref ref-type="fig" rid="fig2">Figure 2B</xref> depicts lateralization performance with spectrally flat noise at two sound intensities for a representative listener (<italic>TCW</italic>). <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows raw data (circles) and NMLE fits (lines) across all listeners. Error bars show one standard error of the mean across listeners, and shaded ribbons indicate one standard error of the mean fit across listeners. This model predicts 80.6% of the variance in the measured responses and is deemed an appropriate fit of the data. <xref ref-type="table" rid="table1">Table 1</xref> lists all NLME parameters. Perceived laterality scores increased with increasing ITD, as expected. With decreasing sound intensity, percepts were increasingly biased towards midline (compare order of colored lines, magnified in the inset of <xref ref-type="fig" rid="fig2">Figure 2C</xref>). These trends were supported by the NLME model, which revealed significant effects of ITD (p&lt;0.001; <inline-formula><mml:math id="inf1"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) and sound intensity (p&lt;0.001; <inline-formula><mml:math id="inf2"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) on the maximal extent of laterality, confirming the predicted trend from the hemispheric difference model and rejecting our null hypothesis. Average pure tone audiometric thresholds affect perceived laterality, albeit mildly (p&lt;0.001; <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = 0.01). Sound intensity did not significantly affect the slope of the psychometric functions (p=0.14; <inline-formula><mml:math id="inf4"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>).</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.47027.005</object-id><label>Table 1.</label><caption><title>Results of Nonlinear Mixed Effects Model for flat-spectrum noise condition.</title><p>Note that <italic>Laterality:sound intensity</italic> refers to the NLME weight attributed to acoustic sound intensity of the auditory target. In contrast, Laterality:audibility captures the NLME weight attributed to pure tone audiometric thresholds based on the listeners’ perceptual abilities (see Materials and methods for details).</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Description</th><th valign="top"/><th valign="top">Value</th><th valign="top">Std.error</th><th valign="top">t-value</th><th valign="top">p-value</th><th valign="top"/></tr></thead><tbody><tr><td valign="top"><italic>Intercept: ITD</italic></td><td><inline-formula><mml:math id="inf5"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td valign="top">0.06</td><td valign="top">0.04</td><td valign="top">1.58</td><td valign="top">0.11</td><td valign="top"/></tr><tr><td valign="top"><italic>Slope: ITD</italic></td><td><inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td valign="top">2.45</td><td valign="top">0.05</td><td valign="top">46.15</td><td valign="top">&lt;0.001</td><td valign="top">***</td></tr><tr><td valign="top"><italic>Slope: sound intensity</italic></td><td><inline-formula><mml:math id="inf7"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td valign="top">0.02</td><td valign="top">0.01</td><td valign="top">1.47</td><td valign="top">0.14</td><td valign="top"/></tr><tr><td valign="top"><italic>Laterality: sound intensity</italic></td><td><inline-formula><mml:math id="inf8"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td valign="top">0.05</td><td valign="top">0.01</td><td valign="top">7.59</td><td valign="top">&lt;0.001</td><td valign="top">***</td></tr><tr><td valign="top"><italic>Laterality: audibility</italic></td><td><inline-formula><mml:math id="inf9"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td valign="top">0.01</td><td valign="top">0.002</td><td valign="top">4.86</td><td valign="top">&lt;0.001</td><td valign="top">***</td></tr></tbody></table><table-wrap-foot><fn><p>10986 degrees of freedom.</p></fn></table-wrap-foot></table-wrap><p>In a second experiment, we examined whether these results were robust to the spectral details of the stimuli. A caveat of testing spectrally flat noise at low sound intensities is that parts of the spectrum may be inaudible, and this may contribute to the slight but significant effect of audibility on laterality (<inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). Therefore, the results of experiment 1 could potentially be confounded by the fact that the bandwidth of the audible portion of the noise tokens decreased with decreasing sound intensity. Alternatively, the effect of absolute pure tone detection thresholds that we observe in our normal-hearing listeners may reflect differences in neural function beyond audibility. As a control for perceived stimulus bandwidth, the same listeners were tested again, using inverse A-weighted noises (experiment 2). Inverse A-weighting boosts sound energy at each frequency in rough proportion to the human threshold. Resulting inverse A-weighted sensitivity thus achieves nearly constant sensation level across frequency. All of the original ten listeners from experiment 1 completed experiment 2. Methods were similar as in the first experiment, except that the stimuli consisted of inversely A-weighted noise (compare magnitude spectra in <xref ref-type="fig" rid="fig2">Figure 2A</xref>). The data and NLME model fits for the second experiment are shown in <xref ref-type="fig" rid="fig2">Figure 2D</xref> (color key identical to <xref ref-type="fig" rid="fig2">Figure 2C</xref>), and coefficients are listed in <xref ref-type="table" rid="table2">Table 2</xref>. This second model accounts for 80.4% of the variance in the data, closely fitting the measured responses. All NLME coefficients are significant (p&lt;0.001 for <inline-formula><mml:math id="inf11"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="inf12"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf14"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). The intercept coefficient (<inline-formula><mml:math id="inf15"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> estimate = −0.60, SE = 0.03, p&lt;0.001) revealed a slight leftward response bias, consistent with a slight narrowband interaural level difference in our stimuli due to precision limits of our test system. The fact that <inline-formula><mml:math id="inf16"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is significant shows that when all noise portions are approximately equally audible, as here, with inverse A-weighted noise, both perceived laterality and the slope linking the change in laterality to ITD decrease with decreasing sound intensity. This is consistent with the interpretation that by controlling for audibility across-frequency, the sensitivity of the task to sound level increases, revealing a medial bias effect not only for the most lateral but also for more medial source angles.</p><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.47027.006</object-id><label>Table 2.</label><caption><title>Results of Nonlinear Mixed Effects Model for inverse A-weighted noise condition.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">NLME weight</th><th valign="top"/><th valign="top">Value</th><th valign="top">Std.error</th><th valign="top">t-value</th><th valign="top">p-value</th><th valign="top"/></tr></thead><tbody><tr><td valign="top"><italic>Intercept: ITD</italic></td><td><inline-formula><mml:math id="inf17"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td valign="top">−0.60</td><td valign="top">0.03</td><td valign="top">−19.28</td><td valign="top">&lt;0.001</td><td valign="top">***</td></tr><tr><td valign="top"><italic>Slope: ITD</italic></td><td><inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td valign="top">2.57</td><td valign="top">0.06</td><td valign="top">46.26</td><td valign="top">&lt;0.001</td><td valign="top">***</td></tr><tr><td valign="top"><italic>Slope: sound intensity</italic></td><td><inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td valign="top">0.06</td><td valign="top">0.01</td><td valign="top">4.98</td><td valign="top">&lt;0.001</td><td valign="top">***</td></tr><tr><td valign="top"><italic>Laterality: sound intensity</italic></td><td><inline-formula><mml:math id="inf20"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td valign="top">0.04</td><td valign="top">0.01</td><td valign="top">7.10</td><td valign="top">&lt;0.001</td><td valign="top">***</td></tr><tr><td valign="top"><italic>Laterality: audibility</italic></td><td><inline-formula><mml:math id="inf21"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td valign="top">0.01</td><td valign="top">0.002</td><td valign="top">3.30</td><td valign="top">&lt;0.001</td><td valign="top">***</td></tr></tbody></table><table-wrap-foot><fn><p>10986 degrees of freedom.</p></fn></table-wrap-foot></table-wrap><p>Thus, the results confirm the effect of biasing perceived laterality toward midline with decreasing sound intensity. Therefore, for both spectrally flat noise and A-weighted noise, statistical analyses, which partialed out overall differences between listeners, are inconsistent with a labelled-line model of human sound localization.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Population rate-coding to compute sensory dimension may not be unique to the auditory system. In analogy to sound localization based on the comparison of signals from the two ears (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), visual depth is computed in the cerebral cortex based on signals from the two eyes (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; <xref ref-type="bibr" rid="bib43">Poggio, 1995</xref>; <xref ref-type="bibr" rid="bib38">Parker and Cumming, 2001</xref>; <xref ref-type="bibr" rid="bib37">Parker, 2016</xref>). Specifically, in both primary <italic>V1</italic> and extrastriate <italic>V3a</italic> cortex of rhesus macaque monkeys, three types of neurons are thought to encode binocular disparity. ‘Tuned-excitatory’ neurons respond best to zero spatial disparity between the two eyes, whereas ‘near cells’ respond more vigorously when an object approaches, increasing crossed disparity between the eyes (<xref ref-type="bibr" rid="bib38">Parker and Cumming, 2001</xref>). Finally, ‘far cells’ fire more vigorously as uncrossed disparity increases. In <italic>V1</italic>, the most frequently encountered type of binocular neurons are of the tuned-excitatory type. However, in <italic>V3a</italic> the large majority of neurons is stereo-specific (<xref ref-type="bibr" rid="bib42">Poggio et al., 1988</xref>) and most neurons are either near or far cells. Functional magnetic resonance imaging experiments on human stereoscopic vision found that unlike <italic>V1</italic> activity, the activity in cortical area <italic>V3a</italic> predicts behavioral performance on tasks involving stereoscopic depth (<xref ref-type="bibr" rid="bib1">Backus et al., 2001</xref>). These observations lead us to propose that in order to compute perceptual space from sensory input, the central nervous system has evolved a canonical computation that is common to different sensory modalities. Specifically, we propose that near and far cells encode visual distance from the fixation plane in a way similar to how <italic>inferior colliculus</italic> neurons encode auditory azimuthal angle away from midline reference: firing rate increases monotonically with distance from perceptual reference anchor or fixation.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.47027.007</object-id><label>Figure 3.</label><caption><title>Conceptual model of canonical computation of location.</title><p>(<bold>A</bold>) Computing sound direction requires analysis of the binaural difference between the signals reaching the left and right ear. (<bold>B</bold>) Estimating visual depth hinges on analysis of the binocular disparity between the signals reaching left and right eye. (<bold>C</bold>) For both hearing and vision, the proportion of the neural population that is stimulated (in the <italic>inferior colliculus</italic> or <italic>V3</italic>) depends both on the physical dimension to be estimated (source laterality or source distance) and the intensity of the stimulus (sound intensity or visual contrast). For hearing and vision, ambiguity in this putative neural code predicts D) biased responses at low stimulus intensities (sound intensity or contrast).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47027-fig3-v1.tif"/></fig><p>We observe that in both the auditory and the visual system, the same cells that are tuned to binaural ITD or binocular disparity also have intensity-response functions. A rate-code based on a population of these cells should cause ambiguities when stimulated below the saturation firing rate, either at low sound intensity or at low contrast (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Thus, based on the analogies between the stereo-depth computation and the azimuth-ITD computation, we hypothesized that low visual contrast might affect the computation of depth in a manner analogous to the effect of low sound levels in sound localization—there might be a bias to lower perceived depth at lower contrast (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). Indeed, one study found such an effect, but only in some observers (<xref ref-type="bibr" rid="bib9">Cisarik and Harwerth, 2008</xref>). A confounding factor in that earlier study is that perceived depth is a complicated neural computation, not only dependent on stereoscopic disparity but also on monocular cues including contrast (<xref ref-type="bibr" rid="bib37">Parker, 2016</xref>). Several studies on depth perception indicate that low contrast is interpreted by the brain as a cue for distance; lower contrast targets are perceived farther away (e.g. <xref ref-type="bibr" rid="bib51">Schor and Howarth, 1986</xref>; <xref ref-type="bibr" rid="bib48">Rohaly and Wilson, 1999</xref>). However, experiments that controlled for low contrast bias demonstrated that low contrast causes perceived depth to shrink, both for near and far deviations from baseline (<xref ref-type="bibr" rid="bib8">Chen et al., 2016</xref>). Thus, there is a link between population rate-coding and stimulus intensity in perceived visual depth as in perceived auditory azimuth, two perceptual spatial dimensions computed by the brain.</p><p>To illustrate how rate-based decoding of target location varies with sound intensity, we here chose a rate-coding model that compares firing rates across two populations of neurons, tuned to opposite hemifields. This read-out is a direct realization of the original canonical rate-based model for ITD decoding (<xref ref-type="bibr" rid="bib58">van Bergeijk, 1962</xref>). Alternative rate-code readouts exist (for a recent summary of binaural models, see <xref ref-type="bibr" rid="bib13">Dietz et al., 2018</xref>). Most of these rate-code models rely on subtractive comparisons between populations of neurons that are tuned to opposite hemifields, inherently sharing ambiguous readouts at low suprathreshold sound intensities. In contrast, divisive comparisons between ipsi- and contralaterally tuned neural populations are less likely to predict the observed behavioral bias due to stimulus intensity (<xref ref-type="bibr" rid="bib20">Groh, 2001</xref>). Future work will need to delineate how specific implementations of rate-based readouts shape the intensity-induced bias of sound localization. Moreover, it has been suggested that depending on perceptual task, the mammalian brain could combine place- and rate-codes (<xref ref-type="bibr" rid="bib45">Porter and Groh, 2006</xref>; <xref ref-type="bibr" rid="bib19">Goodman et al., 2013</xref>). For instance, the mammalian auditory pathway may convert place- into rate-codes and vice versa (<xref ref-type="bibr" rid="bib21">Groh et al., 2003</xref>; <xref ref-type="bibr" rid="bib45">Porter and Groh, 2006</xref>). However, downstream from the <italic>inferior colliculus</italic>, rate-coding seems to be maintained, at least in the superior colliculus of rhesus macaque (<xref ref-type="bibr" rid="bib61">Werner-Reiss and Groh, 2008</xref>; <xref ref-type="bibr" rid="bib32">Lee and Groh, 2014</xref>). Moreover, our psychophysical and computational results suggest that for sound localization based on ITD at low sound levels, cortical maps do not play a role. However, there is good evidence for spatial map-like signals in higher order auditory cortical fields when interaural level differences are present, at medium to high sound levels (<xref ref-type="bibr" rid="bib27">Higgins et al., 2010</xref>). How these interaural-level-difference-based cortical maps influence sound localization behavior is yet to be determined.</p><p>An additional factor restricting rate-based readouts is that auditory cortex units display nonlinear rate-intensity functions. For instance, excitatory-excitatory (EE) cells in auditory cortex that are tuned to sound locations near midline are also often tuned for sound intensity (<xref ref-type="bibr" rid="bib52">Semple and Kitzes, 1993</xref>; <xref ref-type="bibr" rid="bib44">Pollak et al., 2002</xref>; <xref ref-type="bibr" rid="bib65">Zhang et al., 2004</xref>; <xref ref-type="bibr" rid="bib47">Razak and Fuzessery, 2010</xref>; <xref ref-type="bibr" rid="bib27">Higgins et al., 2010</xref>). This intensity tuning may complicate rate-based decoding at higher sound intensities. However, it is not apparent at the very low sound intensities needed to explain the perceptual bias observed here. There are additional fascinating findings in the neurophysiological literature regarding frequency and intensity tuning, and interesting correlations between non-monotonicity in the azimuthal and intensity dimensions (<xref ref-type="bibr" rid="bib63">Woods et al., 2006</xref>), but a detailed discussion of these points is beyond the scope of the present behavioral-computational study.</p><p>In summary, unlike predictions from a rate-code neuronal readout, labelled-line coding predicts that sound localization is intensity invariant. Our experimental results show that for low frequency noise, where ITDs are the dominant localization cue, and at low sound intensities, sound lateralization based on ITD is not intensity invariant; it becomes increasingly medially biased with decreasing SL. The observed localization bias is overall small in magnitude, showing that the brain can robustly localize based on ITD across a large range of sound intensities. However, this bias is of theoretical importance as it confirms the prediction of a subtractive rate-based neuronal readout. Moreover, our auditory finding parallels a phenomenon of visual fixation bias when calculating visual distance from binocular disparity at low contrast. This casts doubt on the idea that the neural mechanism of ITD-based sound localization and binocular disparity-based visual distance estimation are based on place-based coding. Instead, our perceptual data on auditory localization together with previously published data on visual distance perception are parsimonious with the idea that a population rate-code underlies the brain’s computation of location.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental model and subject details</title><p>Twelve naïve normal-hearing listeners (ages 18–27, five females) were enrolled in this study and paid for their time. Their audiometric thresholds, as assessed via a calibrated GSI 39 Auto Tymp device (Grason-Stadler), were 25 dB hearing level or better at octave frequencies from 250 to 8000 Hz, and did not differ by more than 10 dB across ears at each octave frequency. This study was approved by and all testing was administered according to the guidelines of the Institutional Review Board of the New Jersey Institute of Technology, protocol F217-14. All listeners gave written informed consent both to participate in the study and to publish the results with confidential listener identity.</p></sec><sec id="s4-2"><title>Method details</title><p>Listeners were seated in a double-walled sound-attenuating booth (Industrial Acoustics Company) with a noise floor of 20.0 dB SPL (wideband LAFeq). Stimuli were digitally generated in Matlab R2016b (The MathWorks, Inc), D/A converted through an external sound card (Emotiva Stealth DC-1) at a sampling frequency of 192 kHz, with a resolution of 24 bits per sample, and presented to the listener through ER-2 insert earphones (Etymotic Research Inc). The equipment was calibrated using an acoustic mannequin (KEMAR model, G.R.A.S. Sound and Vibration) with a precision of less than ±5 μs ITD and less than ±1 dB interaural level difference. Foam eartips were inserted following guidelines provided by Etymotic Research to encourage equal representation of sounds to both ears and minimize interaural leakage. Each session lasted approximately 60 min. Listeners kept the insert earphones placed inside their ears throughout testing. Insert earphones were replaced by the experimenter after each break. Throughout this study, to generate stimuli, tokens of uniformly distributed white noise were generated and bandpassed using a zero-phase Butterworth filter with 36 dB/octave frequency roll-off, and 3 dB down points at 300 and 1200 Hz. Each noise token was 1 s in duration, including 10 ms long squared cosine ramps at the onset and offset.</p></sec><sec id="s4-3"><title>Sensation level measurements</title><p>At the beginning of each session, and, as a re-test control, mid-way through each session, each individual listener’s SL was measured for the type of sound that was later on used for training and testing, via one run of adaptive tracking. On each one-interval trial of each track, a new noise token was generated and presented diotically. Trials were spaced randomly in time (uniform distribution, inter-token intervals from 3 to 5.5 s). Listeners pressed a button when they heard a sound. No response feedback was given.</p><p>On each trial, a response was scored a ‘hit’ if a listener responded with a button push before the onset of the subsequent trial, and a ‘miss’ if the listener did not respond during the interval. If a listener’s response changed from hit to miss or from miss to hit across sequential trials, this was interpreted as a response reversal. Using one-up-one-down adaptive tracking, the noise intensity was increased or decreased after each reversal, with a step size of 5 dB (decreasing) or 2.5 dB (increasing). Each listener completed ten adaptive-track reversals, with SL threshold equaling the median of the final six reversals. Each SL was used as reference intensity for the subsequent 30 min of testing. If detection thresholds changed between initial test and re-test control by more than 5 dB, this indicated that an insert earphone moved, and the experimenter replaced the earphones. Thresholds generally did not change by more than 5 dB.</p></sec><sec id="s4-4"><title>Training</title><p>To train listeners on consistently reporting their perception of ITD, using adaptive tracking, listeners matched the perceived laterality of a variable-ITD pointer to that of a fixed-ITD target. Target token intensity was set relative to the listener’s own diotic sensation threshold, at 10 or 25 dB SL, and presented with 0 dB interaural level difference. The pointer intensity was fixed at 25 dB SL. Target ITDs spanned the range from −375 to 375 μs, in 75 μs steps. Target ITDs and SLs were randomly interleaved across runs, but held fixed throughout each adaptive run. In each two-interval trial of a run, the pointer token was presented in the first, and the target token in the second interval. The start ITD of the pointer token at the beginning of each run equaled 0 μs. Using a hand-held controller (Xbox 360 wireless controller for Windows, Microsoft Corp.), listeners adjusted the ITD of the pointer token. Specifically, listeners pushed the directional keys (D-pad) either to the left or right in order to move and match the pointer direction with that of the target sound. When a listener indicated a left- or right-ward response, the pointer ITD was decreased or increased. Initial ITD step size equaled 100 μs, then 50 ±5 μs (uniformly distributed) after the first reversal. By the end of the second reversal, ITD step size was reduced to 25 ±5 μs (uniformly distributed) and remained the same for all of the following reversals. Listeners were instructed to ‘home in’ on the target by moving the pointer initially to a position more lateral than the target, then more medial than the target with the goal of centering on the target. No response feedback was provided. A run was completed after a listener had completed a total of five adaptive-tracking reversals. For each target ITD, the matched pointer ITD was estimated by averaging the pointer ITDs of the final two reversals. Each listener performed three sessions of training: In the first session only a subset of target ITDs were presented (−375,–150, 0, 150 and 375 μs), whereas the two following sessions included all of the eleven ITDs. Per training session, each ITD was presented once at 10 and 25 dB SL, for a total of 54 adaptive tracking runs across all training sessions. To familiarize listeners with the experimental task (described below), at the end of second and third sessions of training listeners performed an additional 5 blocks of the experimental testing task, without response feedback. These task training data were not used for statistical analysis.</p><p>To assess whether listeners could reliably report their lateralization percepts, training performance was evaluated for each listener by calculating the Pearson correlation coefficient between target ITD and matched pointer ITD in the final training session. Criterion correlation equaled 0.9 (N = 11 ITDs, significance level = 0.01, power = 0.95). Ten listeners reached criterion, suggesting that they were able to consistently report where they perceived the sounds based on ITD. Two of the originally recruited twelve listeners failed to reach training criterion (<inline-formula><mml:math id="inf22"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> = &lt;0.84, 0.87&gt;) and were excluded from testing.</p></sec><sec id="s4-5"><title>Testing</title><p>Using the method of fixed stimuli, we tested lateralization in two experiments. Except for the stimuli, which consisted of spectrally flat noise tokens in experiment 1 and A-weighted noise tokens in experiment 2, the methods were similar across the two experiments. Noise tokens were generated from a statistically similar noise distribution as those presented during both SL measurements and training (see Overall Design). A touchscreen monitor (Dell P2314T) displayed the response interface at about 40 cm distance from the listener. Using a precise touch stylus (MEKO Active Fine Point Stylus 1.5 mm Tip), listeners indicated perceived laterality of noise in a one-interval task. Noise tokens were presented at 5, 10, 15, 20, and 25 dB SL. ITDs varied randomly from trial to trial, in 75 μs steps spanning the range from −375 μs to 375 μs. On each trial, a new token of noise was generated. Each listener performed 20 blocks of 55 trials each (11 ITDs at each of the five sound intensities), with SL measured both before the first and the eleventh block. ITDs and sound intensity were randomly interleaved from trial to trial such that each combination of ITD and sound intensity was presented once before all of them were repeated in a different random order.</p></sec><sec id="s4-6"><title>Models</title><p>We estimate the combined effects of ITD and sound intensity on predicted source laterality both in avian labelled-line type units and in binaurally sensitive units of a mammalian auditory system. The sound intensities where we expect to see an effect of overall sound level fall below 30 dB SPL, because only in this range would most auditory neurons fire below saturation, allowing us to disambiguate labelled-line versus hemispheric rate-difference coding. However, scant data exist for either type of unit at sound pressure levels below 30 dB SPL. We identified two prior studies that have measured neural discharge rate as a function of ITD at these very low sound intensities. Both studies used noise as acoustic stimuli, and the neural response statistics they report are thus suitable for estimating what type of information would be available to either type of coding mechanism with the type of noise stimuli that human listeners lateralized in the behavioral experiments here.</p><p>One study in barn owl shows that the output functions of <italic>nucleus laminaris</italic> neurons can be modeled through interaural cross-correlation functions, even at very low sound intensities (<xref ref-type="bibr" rid="bib40">Peña et al., 1996</xref>). That study reports Pearson correlation coefficients between the neural response function of <italic>nucleus laminaris</italic> units at 50 dB SPL versus all other tested sound levels. To reconstruct the spatial information realistically available from the output of labelled-line neurons, across both a range of −375 to 375 μs ITD in 20 μs steps, we first constructed biologically plausible interaural cross-correlation functions at 50 dB SPL and then added internal noise to the resulting curves to mimic the Pearson correlation coefficients reported by <xref ref-type="bibr" rid="bib40">Peña et al. (1996)</xref>. Our model predictions pertain to sound intensities spanning the range from 10 to 70 dB SPL, similar to previous work (<xref ref-type="bibr" rid="bib40">Peña et al., 1996</xref>). Due to overall scarcity of available data at low dB SPL, here we use firing rate characteristics for unit # 0123795–530.02 (<xref ref-type="bibr" rid="bib40">Peña et al., 1996</xref>) with a nominal best frequency of 1 kHz. To generate the acoustic inputs to the labelled-line model, we initially generated a Gaussian noise token, duplicated it and introduced a variable ITD, spanning a range from −375 to 375 μs, with 20 μs step size and 0 dB interaural level difference. To simulate ITD information available after cochlear processing, we then processed both noises with a 1/3-octave wide bandpass filter with 24 dB/octave frequency roll-off, followed by half-wave rectification and low-pass filtering at 1500 Hz. We then simulated internal noise by adding uniformly distributed dichotic noise tokens with mean spontaneous firing rates of 5% of the root mean square value of the signal, resulting in left (L) and right (R) inputs to the binaural cross-correlation neurons, called <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. To establish 50 dB SPL reference functions, at each simulated ITD, we then calculated the binaural cross-correlation function <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="inf26"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, as follows: <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>300</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>450</mml:mn><mml:mo>−</mml:mo><mml:mn>300</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf29"><mml:mi>τ</mml:mi></mml:math></inline-formula> signifying the best ITD of each neuron, and extrema scaled such that <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> spans a range from 300 to 450 spikes/sec, approximating <italic>nucleus laminaris</italic> firing rates at 50 dB SPL (<xref ref-type="bibr" rid="bib40">Peña et al., 1996</xref>). To simulate non-sound driven neural discharge, we then added uniformly distributed random noise <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, with a mean discharge of <inline-formula><mml:math id="inf32"><mml:mi>μ</mml:mi></mml:math></inline-formula> = 5 spikes/sec, (<xref ref-type="bibr" rid="bib40">Peña et al., 1996</xref>). The resulting signal is our reference cross-correlation function at 50 dB SPL, called <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref> as yellow bold line for a representative simulated neuron.</p><p>For each sound level and ITD, we then statistically reconstructed a family of interaural cross-correlation functions that match the originally reported functions (<xref ref-type="bibr" rid="bib40">Peña et al., 1996</xref>). Specifically, we added scaled dichotic uniformly distributed noise tokens <inline-formula><mml:math id="inf34"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> to the <inline-formula><mml:math id="inf36"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, such that the monaural inputs to the binaural cross-correlation functions equal <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The resulting cross-correlation function for each sound level and ITD is then <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>⋆</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, shown for a representative neuron in <xref ref-type="fig" rid="fig1">Figure 1A</xref> as blue, brown and red lines corresponding to 70, 30 and 10 dB SPL. We then searched through the space of scaling coefficients <inline-formula><mml:math id="inf40"><mml:mi>α</mml:mi></mml:math></inline-formula> until the Pearson correlation coefficient between <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> matched the coefficients originally reported by <xref ref-type="bibr" rid="bib40">Peña et al. (1996)</xref> with a precision error of less than 10%.</p><p>To estimate predicted sound laterality as a function of sound intensity for these simulated labelled-line neurons, at each intensity, we then identified the <inline-formula><mml:math id="inf43"><mml:mi>τ</mml:mi></mml:math></inline-formula> where <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. For each sound level and ITD, we calculated predicted sound laterality in 100 repetitions of these simulations. <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows mean estimated laterality across these 100 simulations, with ribbons showing one standard error of the mean across simulations.</p><p>To estimate source laterality based on rate-coding, we assayed the mammalian auditory system, where one previous study reports firing statistics for 81 <italic>inferior colliculus</italic> units in rhesus macaque as a function of ITD and over a wide range of sound intensities, including very low sound intensities (<xref ref-type="bibr" rid="bib66">Zwiers et al., 2004</xref>). From the previously published linear regression parameters, we initially reconstruct linear regression functions linking ITD, sound intensity and firing rate (<xref ref-type="bibr" rid="bib66">Zwiers et al., 2004</xref>). However, while linear regression fits afford statisticial convenience, they cannot fully capture the sigmoidally shaped firing rate functions in mammalian <italic>inferior colliculus</italic> units. Therefore, we multiplied the original linear reconstructions with sigmoid functions. Specifically, consistent with prior literature, each simulated sigmoidal output function saturates over a 30 dB dynamic range, has linear growth over the physiologically plausible range of contralateral ITDs, has a threshold between uniformly distributed between 0 and 10 dB SPL, and a spontaneous non-sound-evoked discharge of between 2 and 10 spikes/second (e.g. <xref ref-type="bibr" rid="bib46">Ramachandran et al., 1999</xref>).</p><p>The inset of <xref ref-type="fig" rid="fig1">Figure 1B</xref> shows a representative simulated <italic>inferior colliculus</italic> unit (color denotes sound intensity, dark shading shows contralateral responses), whereas <xref ref-type="fig" rid="fig1">Figure 1B</xref> shows the differences in firing rates for contra minus ipsi-lateral simulated firing rates, averaged across all 81 simulated <italic>inferior colliculus</italic> units. From these resulting differences in contra versus ipsi firing rates we calculated, collapsed across sound intensities from 0 to 80 dB SPL, the probability density of the firing rate for each <italic>inferior colliculus</italic> unit as a function of source ITD. Assuming an ideal observer, we then classified the sound azimuth as a function of sound intensity via maximum likelihood estimation. To calculate the mean and variance of predicted ITD as a function of sound intensity, we then ran a bootstrapping analysis, sampling with replacement 100 times. <xref ref-type="fig" rid="fig1">Figure 1D</xref> shows the across-simulation average predicted source laterality, with ribbons showing one standard error of the mean across simulations.</p></sec><sec id="s4-7"><title>Quantification and statistical analysis</title><p>Growth curve analysis was used to analyze perceived laterality scores as a function of ITD and sound intensity. For each of the two noise conditions, the perceived laterality scores were fitted with an NLME model. The model included fixed effects <inline-formula><mml:math id="inf45"><mml:mi>α</mml:mi></mml:math></inline-formula> and random effects <inline-formula><mml:math id="inf46"><mml:mi>β</mml:mi></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> describes a sigmoidal function linking ITD to perceived laterality, with a score from left (−1) to right (1). The effect of sound intensity on the maximal extent of lateralization is <inline-formula><mml:math id="inf47"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. To factor out across-listener differences in absolute hearing thresholds, for each listener, we calculated the pure tone average (PTA) detection threshold in quiet, averaged across ears, and across 500 and 1000 Hz. Weight <inline-formula><mml:math id="inf48"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> models the contribution of PTA. The slope terms are <inline-formula><mml:math id="inf49"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for perceived laterality changes attributed to ITD, and <inline-formula><mml:math id="inf50"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for laterality-ITD slopes attributed to sound intensity. Our stimuli were initially calibrated to have a broadband interaural level difference of 0 dB. However, because the transfer function of our sound card was not perfectly flat across frequency, fluctuations of ±1 dB interaural level difference occurred across frequency, on the same order of magnitude as the minimal threshold for human interaural level difference discrimination (<xref ref-type="bibr" rid="bib16">Francart and Wouters, 2007</xref>). Thus, parameter <inline-formula><mml:math id="inf51"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> factors out central response bias from the lateralization scores. Random effects of individual differences across listeners were used to model both the maximal extent of lateralization, <inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and the perceived midline, <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, centering the sigmoid (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>∼</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></disp-formula></p><p>To better conform with the assumptions of the NLME model, prior to fitting, ITD and sound intensity parameters were scaled by subtracting the mean stimulus value, and dividing by the standard deviation of stimulus parameters, resulting in distributions of stimulus parameters with zero-mean and a variance of one. Laterality scores were then fitted using these normalized parameters, with the nlme package, programmed in RStudio 1.1 for Windows (RStudio Inc, Boston, MA, USA).</p></sec><sec id="s4-8"><title>Data and software availability</title><p>All data and analysis code are available at Dryad (<ext-link ext-link-type="uri" xlink:href="http://doi.org/10.5061/dryad.t8c381f">http://doi.org/10.5061/dryad.t8c381f</ext-link>) .</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Investigation, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Methodology, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: This study was approved by and all testing was administered according to the guidelines of the Institutional Review Board of the New Jersey Institute of Technology, protocol F217-14. All listeners gave written informed consent both to participate in the study and to publish the results with confidential listener identity.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.47027.008</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-47027-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The computational models, coded in Matlab, and all raw data generated or analysed during this study together with the statistical model, coded in R, have been provided via Dryad (<ext-link ext-link-type="uri" xlink:href="http://doi.org/10.5061/dryad.t8c381f">http://doi.org/10.5061/dryad.t8c381f</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Ihlefeld</surname><given-names>A</given-names></name><name><surname>Alamatsaz</surname><given-names>N</given-names></name><name><surname>Shapley</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Data from: Human sound localization depends on sound intensity: implications for sensory coding</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.t8c381f</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Backus</surname> <given-names>BT</given-names></name><name><surname>Fleet</surname> <given-names>DJ</given-names></name><name><surname>Parker</surname> <given-names>AJ</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Human cortical activity correlates with stereoscopic depth perception</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>2054</fpage><lpage>2068</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.4.2054</pub-id><pub-id pub-id-type="pmid">11600661</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Batra</surname> <given-names>R</given-names></name><name><surname>Yin</surname> <given-names>TC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Cross correlation by neurons of the medial superior olive: a reexamination</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>5</volume><fpage>238</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s10162-004-4027-4</pub-id><pub-id pub-id-type="pmid">15492883</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brand</surname> <given-names>A</given-names></name><name><surname>Behrend</surname> <given-names>O</given-names></name><name><surname>Marquardt</surname> <given-names>T</given-names></name><name><surname>McAlpine</surname> <given-names>D</given-names></name><name><surname>Grothe</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Precise inhibition is essential for microsecond interaural time difference coding</article-title><source>Nature</source><volume>417</volume><fpage>543</fpage><lpage>547</lpage><pub-id pub-id-type="doi">10.1038/417543a</pub-id><pub-id pub-id-type="pmid">12037566</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breebaart</surname> <given-names>J</given-names></name><name><surname>van de Par</surname> <given-names>S</given-names></name><name><surname>Kohlrausch</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Binaural processing model based on contralateral inhibition. I. model structure</article-title><source>The Journal of the Acoustical Society of America</source><volume>110</volume><fpage>1074</fpage><lpage>1088</lpage><pub-id pub-id-type="doi">10.1121/1.1383297</pub-id><pub-id pub-id-type="pmid">11519576</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brughera</surname> <given-names>A</given-names></name><name><surname>Dunai</surname> <given-names>L</given-names></name><name><surname>Hartmann</surname> <given-names>WM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Human interaural time difference thresholds for sine tones: the high-frequency limit</article-title><source>The Journal of the Acoustical Society of America</source><volume>133</volume><fpage>2839</fpage><lpage>2855</lpage><pub-id pub-id-type="doi">10.1121/1.4795778</pub-id><pub-id pub-id-type="pmid">23654390</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Brungart</surname> <given-names>DS</given-names></name><name><surname>Simpson</surname> <given-names>BD</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Effects of temporal fine structure on the localization of broadband sounds: potential implications for the design of spatial audio displays</article-title><conf-name>Proceedings of the 14 International Conference on Auditory Display</conf-name></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname> <given-names>CE</given-names></name><name><surname>Konishi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Axonal delay lines for time measurement in the owl's brainstem</article-title><source>PNAS</source><volume>85</volume><fpage>8311</fpage><lpage>8315</lpage><pub-id pub-id-type="doi">10.1073/pnas.85.21.8311</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>PY</given-names></name><name><surname>Chen</surname> <given-names>CC</given-names></name><name><surname>Tyler</surname> <given-names>CW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The perceived depth from disparity as function of luminance contrast</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.1167/16.11.20</pub-id><pub-id pub-id-type="pmid">27690160</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisarik</surname> <given-names>PM</given-names></name><name><surname>Harwerth</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The effects of interocular correlation and contrast on stereoscopic depth magnitude estimation</article-title><source>Optometry and Vision Science</source><volume>85</volume><fpage>164</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1097/OPX.0b013e3181643e65</pub-id><pub-id pub-id-type="pmid">18317331</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Day</surname> <given-names>ML</given-names></name><name><surname>Semple</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Frequency-dependent interaural delays in the medial superior olive: implications for interaural cochlear delays</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>1985</fpage><lpage>1999</lpage><pub-id pub-id-type="doi">10.1152/jn.00131.2011</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devore</surname> <given-names>S</given-names></name><name><surname>Ihlefeld</surname> <given-names>A</given-names></name><name><surname>Hancock</surname> <given-names>K</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>B</given-names></name><name><surname>Delgutte</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate sound localization in reverberant environments is mediated by robust encoding of spatial cues in the auditory midbrain</article-title><source>Neuron</source><volume>62</volume><fpage>123</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.018</pub-id><pub-id pub-id-type="pmid">19376072</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietz</surname> <given-names>M</given-names></name><name><surname>Bernstein</surname> <given-names>LR</given-names></name><name><surname>Trahiotis</surname> <given-names>C</given-names></name><name><surname>Ewert</surname> <given-names>SD</given-names></name><name><surname>Hohmann</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The effect of overall level on sensitivity to interaural differences of time and level at high frequencies</article-title><source>The Journal of the Acoustical Society of America</source><volume>134</volume><fpage>494</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1121/1.4807827</pub-id><pub-id pub-id-type="pmid">23862824</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietz</surname> <given-names>M</given-names></name><name><surname>Lestang</surname> <given-names>JH</given-names></name><name><surname>Majdak</surname> <given-names>P</given-names></name><name><surname>Stern</surname> <given-names>RM</given-names></name><name><surname>Marquardt</surname> <given-names>T</given-names></name><name><surname>Ewert</surname> <given-names>SD</given-names></name><name><surname>Hartmann</surname> <given-names>WM</given-names></name><name><surname>Goodman</surname> <given-names>DFM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A framework for testing and comparing binaural models</article-title><source>Hearing Research</source><volume>360</volume><fpage>92</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2017.11.010</pub-id><pub-id pub-id-type="pmid">29208336</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Domnitz</surname> <given-names>RH</given-names></name><name><surname>Colburn</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Lateral position and interaural discrimination</article-title><source>The Journal of the Acoustical Society of America</source><volume>61</volume><fpage>1586</fpage><lpage>1598</lpage><pub-id pub-id-type="doi">10.1121/1.381472</pub-id><pub-id pub-id-type="pmid">893805</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durlach</surname> <given-names>NI</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Equalization and cancellation theory of binaural masking‐level differences</article-title><source>The Journal of the Acoustical Society of America</source><volume>35</volume><fpage>1206</fpage><lpage>1218</lpage><pub-id pub-id-type="doi">10.1121/1.1918675</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francart</surname> <given-names>T</given-names></name><name><surname>Wouters</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Perception of across-frequency interaural level differences</article-title><source>The Journal of the Acoustical Society of America</source><volume>122</volume><fpage>2826</fpage><lpage>2831</lpage><pub-id pub-id-type="doi">10.1121/1.2783130</pub-id><pub-id pub-id-type="pmid">18189572</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gai</surname> <given-names>Y</given-names></name><name><surname>Ruhland</surname> <given-names>JL</given-names></name><name><surname>Yin</surname> <given-names>TCT</given-names></name><name><surname>Tollin</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Behavioral and modeling studies of sound localization in cats: effects of stimulus level and duration</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>607</fpage><lpage>620</lpage><pub-id pub-id-type="doi">10.1152/jn.01019.2012</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldberg</surname> <given-names>JM</given-names></name><name><surname>Brown</surname> <given-names>PB</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Functional organization of the dog superior olivary complex: an anatomical and electrophysiological study</article-title><source>Journal of Neurophysiology</source><volume>31</volume><fpage>639</fpage><lpage>656</lpage><pub-id pub-id-type="doi">10.1152/jn.1968.31.4.639</pub-id><pub-id pub-id-type="pmid">5709877</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname> <given-names>DF</given-names></name><name><surname>Benichoux</surname> <given-names>V</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Decoding neural responses to temporal cues for sound localization</article-title><source>eLife</source><volume>2</volume><elocation-id>e01312</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.01312</pub-id><pub-id pub-id-type="pmid">24302571</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groh</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Converting neural signals from place codes to rate codes</article-title><source>Biological Cybernetics</source><volume>85</volume><fpage>159</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1007/s004220100249</pub-id><pub-id pub-id-type="pmid">11561817</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groh</surname> <given-names>JM</given-names></name><name><surname>Kelly</surname> <given-names>KA</given-names></name><name><surname>Underhill</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A monotonic code for sound azimuth in primate inferior colliculus</article-title><source>Journal of Cognitive Neuroscience</source><volume>15</volume><fpage>1217</fpage><lpage>1231</lpage><pub-id pub-id-type="doi">10.1162/089892903322598166</pub-id><pub-id pub-id-type="pmid">14709238</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grothe</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>New roles for synaptic inhibition in sound localization</article-title><source>Nature Reviews Neuroscience</source><volume>4</volume><fpage>540</fpage><lpage>550</lpage><pub-id pub-id-type="doi">10.1038/nrn1136</pub-id><pub-id pub-id-type="pmid">12838329</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grothe</surname> <given-names>B</given-names></name><name><surname>Pecka</surname> <given-names>M</given-names></name><name><surname>McAlpine</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Mechanisms of sound localization in mammals</article-title><source>Physiological Reviews</source><volume>90</volume><fpage>983</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1152/physrev.00026.2009</pub-id><pub-id pub-id-type="pmid">20664077</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafter</surname> <given-names>ER</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Quantitative evaluation of a lateralization model of masking‐level differences</article-title><source>The Journal of the Acoustical Society of America</source><volume>50</volume><fpage>1116</fpage><lpage>1122</lpage><pub-id pub-id-type="doi">10.1121/1.1912743</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartmann</surname> <given-names>WM</given-names></name><name><surname>Rakerd</surname> <given-names>B</given-names></name><name><surname>Koller</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Binaural coherence in rooms</article-title><source>Acta Acustica United with Acustica</source><volume>91</volume><fpage>451</fpage><lpage>462</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartmann</surname> <given-names>WM</given-names></name><name><surname>Rakerd</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Auditory spectral discrimination and the localization of clicks in the sagittal plane</article-title><source>The Journal of the Acoustical Society of America</source><volume>94</volume><fpage>2083</fpage><lpage>2092</lpage><pub-id pub-id-type="doi">10.1121/1.407481</pub-id><pub-id pub-id-type="pmid">8227750</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname> <given-names>NC</given-names></name><name><surname>Storace</surname> <given-names>DA</given-names></name><name><surname>Escabí</surname> <given-names>MA</given-names></name><name><surname>Read</surname> <given-names>HL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Specialization of binaural responses in ventral auditory cortices</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>14522</fpage><lpage>14532</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2561-10.2010</pub-id><pub-id pub-id-type="pmid">20980610</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ihlefeld</surname> <given-names>A</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Effect of source spectrum on sound localization in an everyday reverberant room</article-title><source>The Journal of the Acoustical Society of America</source><volume>130</volume><fpage>324</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1121/1.3596476</pub-id><pub-id pub-id-type="pmid">21786902</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inoue</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Effects of stimulus intensity on sound localization in the horizontal and upper-hemispheric median plane</article-title><source>Journal of UOEH</source><volume>23</volume><fpage>127</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.7888/juoeh.23.127</pub-id><pub-id pub-id-type="pmid">11431958</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeffress</surname> <given-names>LA</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A place theory of sound localization</article-title><source>Journal of Comparative and Physiological Psychology</source><volume>41</volume><fpage>35</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1037/h0061495</pub-id><pub-id pub-id-type="pmid">18904764</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joris</surname> <given-names>PX</given-names></name><name><surname>Van de Sande</surname> <given-names>B</given-names></name><name><surname>Louage</surname> <given-names>DH</given-names></name><name><surname>van der Heijden</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Binaural and cochlear disparities</article-title><source>PNAS</source><volume>103</volume><fpage>12917</fpage><lpage>12922</lpage><pub-id pub-id-type="doi">10.1073/pnas.0601396103</pub-id><pub-id pub-id-type="pmid">16908859</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>J</given-names></name><name><surname>Groh</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Different stimuli, different spatial codes: a visual map and an auditory rate code for oculomotor space in the primate superior colliculus</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e85017</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0085017</pub-id><pub-id pub-id-type="pmid">24454779</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macpherson</surname> <given-names>EA</given-names></name><name><surname>Middlebrooks</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Localization of brief sounds: effects of level and background noise</article-title><source>The Journal of the Acoustical Society of America</source><volume>108</volume><fpage>1834</fpage><lpage>1849</lpage><pub-id pub-id-type="doi">10.1121/1.1310196</pub-id><pub-id pub-id-type="pmid">11051510</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAlpine</surname> <given-names>D</given-names></name><name><surname>Grothe</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Sound localization and delay lines--do mammals fit the model?</article-title><source>Trends in Neurosciences</source><volume>26</volume><fpage>347</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(03)00140-1</pub-id><pub-id pub-id-type="pmid">12850430</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mickunas</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Interaural time delay and apparent direction of clicks</article-title><source>The Journal of the Acoustical Society of America</source><volume>35</volume><elocation-id>788</elocation-id><pub-id pub-id-type="doi">10.1121/1.2142443</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Overholt</surname> <given-names>EM</given-names></name><name><surname>Rubel</surname> <given-names>EW</given-names></name><name><surname>Hyson</surname> <given-names>RL</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>A circuit for coding interaural time differences in the chick brainstem</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>1698</fpage><lpage>1708</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.12-05-01698.1992</pub-id><pub-id pub-id-type="pmid">1578264</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Vision in our three-dimensional world</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>371</volume><elocation-id>20150251</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2015.0251</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Parker</surname> <given-names>A</given-names></name><name><surname>Cumming</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2001">2001</year><chapter-title>Cortical mechanisms of binocular stereoscopic vision</chapter-title><source>Progress in Brain Research</source><volume>134</volume><publisher-name>Elsevier</publisher-name><fpage>205</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(01)34015-3</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pecka</surname> <given-names>M</given-names></name><name><surname>Siveke</surname> <given-names>I</given-names></name><name><surname>Grothe</surname> <given-names>B</given-names></name><name><surname>Lesica</surname> <given-names>NA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Enhancement of ITD coding within the initial stages of the auditory pathway</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>38</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1152/jn.00628.2009</pub-id><pub-id pub-id-type="pmid">19846624</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peña</surname> <given-names>JL</given-names></name><name><surname>Viete</surname> <given-names>S</given-names></name><name><surname>Albeck</surname> <given-names>Y</given-names></name><name><surname>Konishi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Tolerance to sound intensity of binaural coincidence detection in the nucleus laminaris of the owl</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>7046</fpage><lpage>7054</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-21-07046.1996</pub-id><pub-id pub-id-type="pmid">8824340</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname> <given-names>DP</given-names></name><name><surname>Hall</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Psychophysical evidence for adaptation of central auditory processors for interaural differences in time and level</article-title><source>Hearing Research</source><volume>202</volume><fpage>188</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2004.11.001</pub-id><pub-id pub-id-type="pmid">15811711</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname> <given-names>GF</given-names></name><name><surname>Gonzalez</surname> <given-names>F</given-names></name><name><surname>Krause</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Stereoscopic mechanisms in monkey visual cortex: binocular correlation and disparity selectivity</article-title><source>The Journal of Neuroscience</source><volume>8</volume><fpage>4531</fpage><lpage>4550</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.08-12-04531.1988</pub-id><pub-id pub-id-type="pmid">3199191</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Mechanisms of Stereopsis in monkey visual cortex</article-title><source>Cerebral Cortex</source><volume>5</volume><fpage>193</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1093/cercor/5.3.193</pub-id><pub-id pub-id-type="pmid">7613075</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollak</surname> <given-names>GD</given-names></name><name><surname>Burger</surname> <given-names>RM</given-names></name><name><surname>Park</surname> <given-names>TJ</given-names></name><name><surname>Klug</surname> <given-names>A</given-names></name><name><surname>Bauer</surname> <given-names>EE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Roles of inhibition for transforming binaural properties in the brainstem auditory system</article-title><source>Hearing Research</source><volume>168</volume><fpage>60</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(02)00362-3</pub-id><pub-id pub-id-type="pmid">12117510</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Porter</surname> <given-names>KK</given-names></name><name><surname>Groh</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The &quot;other&quot; transformation required for visual-auditory integration: representational format</article-title><source>Progress in Brain Research</source><volume>155</volume><fpage>313</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(06)55018-6</pub-id><pub-id pub-id-type="pmid">17027396</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramachandran</surname> <given-names>R</given-names></name><name><surname>Davis</surname> <given-names>KA</given-names></name><name><surname>May</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Single-unit responses in the inferior colliculus of decerebrate cats. I. classification based on frequency response maps</article-title><source>Journal of Neurophysiology</source><volume>82</volume><fpage>152</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1152/jn.1999.82.1.152</pub-id><pub-id pub-id-type="pmid">10400944</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Razak</surname> <given-names>KA</given-names></name><name><surname>Fuzessery</surname> <given-names>ZM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>GABA shapes a systematic map of binaural sensitivity in the auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>517</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1152/jn.00294.2010</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohaly</surname> <given-names>AM</given-names></name><name><surname>Wilson</surname> <given-names>HR</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The effects of contrast on perceived depth and depth discrimination</article-title><source>Vision Research</source><volume>39</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(98)00034-0</pub-id><pub-id pub-id-type="pmid">10211391</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sabin</surname> <given-names>AT</given-names></name><name><surname>Macpherson</surname> <given-names>EA</given-names></name><name><surname>Middlebrooks</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Human sound localization at near-threshold levels</article-title><source>Hearing Research</source><volume>199</volume><fpage>124</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2004.08.001</pub-id><pub-id pub-id-type="pmid">15574307</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salminen</surname> <given-names>NH</given-names></name><name><surname>Tiitinen</surname> <given-names>H</given-names></name><name><surname>Yrttiaho</surname> <given-names>S</given-names></name><name><surname>May</surname> <given-names>PJC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The neural code for interaural time difference in human auditory cortex</article-title><source>The Journal of the Acoustical Society of America</source><volume>127</volume><fpage>EL60</fpage><lpage>EL65</lpage><pub-id pub-id-type="doi">10.1121/1.3290744</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schor</surname> <given-names>CM</given-names></name><name><surname>Howarth</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Suprathreshold stereo-depth matches as a function of contrast and spatial frequency</article-title><source>Perception</source><volume>15</volume><fpage>249</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1068/p150249</pub-id><pub-id pub-id-type="pmid">3797199</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Semple</surname> <given-names>MN</given-names></name><name><surname>Kitzes</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Focal selectivity for binaural sound pressure level in cat primary auditory cortex: two-way intensity network tuning</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>462</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.2.462</pub-id><pub-id pub-id-type="pmid">8459278</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitzer</surname> <given-names>MW</given-names></name><name><surname>Semple</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neurons sensitive to interaural phase disparity in gerbil superior olive: diverse monaural and temporal response properties</article-title><source>Journal of Neurophysiology</source><volume>73</volume><fpage>1668</fpage><lpage>1690</lpage><pub-id pub-id-type="doi">10.1152/jn.1995.73.4.1668</pub-id><pub-id pub-id-type="pmid">7643174</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stange</surname> <given-names>A</given-names></name><name><surname>Myoga</surname> <given-names>MH</given-names></name><name><surname>Lingner</surname> <given-names>A</given-names></name><name><surname>Ford</surname> <given-names>MC</given-names></name><name><surname>Alexandrova</surname> <given-names>O</given-names></name><name><surname>Felmy</surname> <given-names>F</given-names></name><name><surname>Pecka</surname> <given-names>M</given-names></name><name><surname>Siveke</surname> <given-names>I</given-names></name><name><surname>Grothe</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Adaptation in sound localization: from GABA(B) receptor-mediated synaptic modulation to perception</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1840</fpage><lpage>1847</lpage><pub-id pub-id-type="doi">10.1038/nn.3548</pub-id><pub-id pub-id-type="pmid">24141311</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stecker</surname> <given-names>GC</given-names></name><name><surname>Harrington</surname> <given-names>IA</given-names></name><name><surname>Middlebrooks</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Location coding by opponent neural populations in the auditory cortex</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e78</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030078</pub-id><pub-id pub-id-type="pmid">15736980</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stern</surname> <given-names>RM</given-names></name><name><surname>Trahiotis</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Models of binaural interaction</article-title><source>Handbook of Perception and Cognition</source><volume>6</volume><fpage>347</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1016/B978-012505626-7/50012-1</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teas</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Lateralization of acoustic transients</article-title><source>The Journal of the Acoustical Society of America</source><volume>34</volume><fpage>1460</fpage><lpage>1465</lpage><pub-id pub-id-type="doi">10.1121/1.1918368</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Bergeijk</surname> <given-names>WA</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Variation on a theme of békésy: a model of binaural interaction</article-title><source>The Journal of the Acoustical Society of America</source><volume>34</volume><fpage>1431</fpage><lpage>1437</lpage><pub-id pub-id-type="doi">10.1121/1.1918364</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vliegen</surname> <given-names>J</given-names></name><name><surname>Van Opstal</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The influence of duration and level on human sound localization</article-title><source>The Journal of the Acoustical Society of America</source><volume>115</volume><fpage>1705</fpage><lpage>1713</lpage><pub-id pub-id-type="doi">10.1121/1.1687423</pub-id><pub-id pub-id-type="pmid">15101649</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Von Békésy</surname> <given-names>G</given-names></name><name><surname>Wever</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1960">1960</year><source>Experiments in Hearing</source><publisher-loc>New York</publisher-loc><publisher-name>McGraw-Hill</publisher-name></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Werner-Reiss</surname> <given-names>U</given-names></name><name><surname>Groh</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A rate code for sound azimuth in monkey auditory cortex: implications for human neuroimaging studies</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>3747</fpage><lpage>3758</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5044-07.2008</pub-id><pub-id pub-id-type="pmid">18385333</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wightman</surname> <given-names>FL</given-names></name><name><surname>Kistler</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The dominant role of low-frequency interaural time differences in sound localization</article-title><source>The Journal of the Acoustical Society of America</source><volume>91</volume><fpage>1648</fpage><lpage>1661</lpage><pub-id pub-id-type="doi">10.1121/1.402445</pub-id><pub-id pub-id-type="pmid">1564201</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname> <given-names>TM</given-names></name><name><surname>Lopez</surname> <given-names>SE</given-names></name><name><surname>Long</surname> <given-names>JH</given-names></name><name><surname>Rahman</surname> <given-names>JE</given-names></name><name><surname>Recanzone</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Effects of stimulus azimuth and intensity on the single-neuron activity in the auditory cortex of the alert macaque monkey</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>3323</fpage><lpage>3337</lpage><pub-id pub-id-type="doi">10.1152/jn.00392.2006</pub-id><pub-id pub-id-type="pmid">16943318</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname> <given-names>TC</given-names></name><name><surname>Chan</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Interaural time sensitivity in medial superior olive of cat</article-title><source>Journal of Neurophysiology</source><volume>64</volume><fpage>465</fpage><lpage>488</lpage><pub-id pub-id-type="doi">10.1152/jn.1990.64.2.465</pub-id><pub-id pub-id-type="pmid">2213127</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>J</given-names></name><name><surname>Nakamoto</surname> <given-names>KT</given-names></name><name><surname>Kitzes</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Binaural interaction revisited in the cat primary auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>91</volume><fpage>101</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1152/jn.00166.2003</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zwiers</surname> <given-names>MP</given-names></name><name><surname>Versnel</surname> <given-names>H</given-names></name><name><surname>Van Opstal</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Involvement of monkey inferior colliculus in spatial hearing</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>4145</fpage><lpage>4156</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0199-04.2004</pub-id><pub-id pub-id-type="pmid">15115809</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47027.012</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Carr</surname><given-names>Catherine Emily</given-names></name><role>Reviewing Editor</role><aff><institution>University of Maryland</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Carr</surname><given-names>Catherine Emily</given-names></name><role>Reviewer</role><aff><institution>University of Maryland</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Read</surname><given-names>Heather</given-names> </name><role>Reviewer</role></contrib><contrib contrib-type="reviewer"><name><surname>Groh</surname><given-names>Jennifer M</given-names></name><role>Reviewer</role><aff><institution>Duke University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Population rate coding predicts correctly that human sound localization depends on sound intensity&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Catherine Emily Carr as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Heather Read (Reviewer #2) and Jennifer M Groh (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This is a combined modeling and behavioral study of human sound localization that supports rate-coding models. Since sound levels affect the firing rate of individual neurons, sometimes changes in intensity can be confused with changes in location. The experiments investigate psychophysical evidence for this predicted effect, with positive results.</p><p>The authors use a hemispheric difference rate coding model of a range of interaural time delays (ITD) to predict that sound localization would be biased towards midline locations at low sound levels. Though the hemispheric rate coding model is simplistic and has been explored before, it is a useful way to raise the question of whether such biases might exist perceptually in auditory systems as they do in visual systems.</p><p>Suggestions for improvement mostly concern additions and clarifications, not new experiments. In general questions from the review should be answered by changes to the text. We number major suggestions below.</p><p>1) Clarification of response types:</p><p>Towards the end, the Discussion section explains appropriately how &quot;both the auditory and the visual system, the same cells that are tuned to binaural ITD or binocular disparity also have intensity-response functions…..&quot; that should cause ambiguities (biases) in localizing sensory objects near the sensory midline. Figure 3 illustrates this principle with monotonic intensity dependence of neural response rate functions for ITD in central inferior colliculus. Thus, this part of the discussion is appropriately focused on some of the universal principles for neurometric and psychometric localization of sensory objects/stimuli in both visual and auditory systems, and their intensity/contrast dependence.</p><p>However, in the first paragraph of the Discussion section, some details are given regarding how visual cortical neurons fall into three types: (1) &quot;Tuned-excitatory&quot; neurons [that] respond best to zero spatial disparity; (2) &quot;near cells&quot; [that] respond more vigorously when [ocular] disparity[is crossed] (Parker and Cumming. (2001)); (3) and &quot;far cells&quot; when ocular disparity is uncrossed. This paragraph goes on to say that secondary visual cortices (V3) respond by and large to binocular cues (as opposed to monocular) cues. Do those binocular responses display biases towards the sensory field midline at low contrast? By analogy IT cortical neurons are biased towards central 0-6 degrees visual angle, is the IT central biased and tuning binaural and contrast dependent?</p><p>It would be useful to explain that neural responses in the auditory cortices are also distributed into regions and cortical fields according to binaural types. Auditory cortices have contralateral tuned-excitatory neurons that respond best to contralateral location disparity (e.g. level difference or ITD difference) and auditory cortices also have &quot;tuned-excitatory&quot; neurons that respond best to zero spatial disparity. The latter have been called (EE/F) or &quot;primarily binaural&quot; or &quot;two-way intensity&quot; tuned (Semple and Kitzes, 1993b; Pollak et al., 2002; Zhang et al., 2004; Razak and Fuzessery, 2010; Higgins et al., 2010). Notably, many if not all of the latter studies find that the &quot;tuned-excitatory&quot; neurons observed in primary (A1) or secondary ventral auditory cortices respond optimally at lower sound intensities. Hence, this raises an interesting twist namely, that neurons also can encode sound location/azimuth position near the midline in spite of the &quot;ambiguities&quot; at low sound intensities possibly by using spatial cues other than ITD.</p><p>2) There is some confusion about what the results actually are.a) What is the difference between &quot;audibility&quot; and &quot;intensity&quot; in the statistical assessment (subsection “Human perception”, Table 1 and Table 2) What does audibility mean in this context? If it co-varies with intensity, why is it included as a term in the model? Is it a means of incorporating the audiograms of individual subjects?</p><p>b) Aren't changes to the slope of the psychometric function as a function of intensity the key result? Yet, that was significant for the second experiment but not the first (although it was close). I think it would be reasonable to say the first experiment didn't quite reject the null hypothesis but showed a trend, so a second experiment was done that controlled an additional aspect of the stimuli in the task, and the same subjects repeated the study, at which point strongly significant results were then obtained. The authors should be commended for including both experiments in the results.</p><p>c) Re: experiment 2: the description in subsection “Human perception” is clear in explaining that in experiment 2, the A-weighting equalizes the perceived bandwidth of the sounds across intensities, but is followed by: &quot;As a control for stimulus audibility&quot;. If I have understood the lead-in correctly, this sentence would make more sense to this reader as: &quot;As a control for perceived bandwidth&quot;.</p><p>3) Computational context:</p><p>It would be useful to explain that although this study supports the intensity dependence of sound localization and potentially hemispheric models for the corresponding neural code alternative scenarios may exist. For example, is it not possible that the mammalian brain has both place-codes and rate codes in different parts that are called upon under different circumstances?</p><p>The computational underpinnings behind this experiment are perhaps more subtle than currently described. A previous study by Porter and Groh, 2006) lays out a few possible models for converting between place and rate codes and explicitly considers the dependence of neural firing patterns on both sound intensity and any other non-spatial factors (see Porter and Groh Figure 4 and page 319 for description of the models and the evidence for and against).</p><p>Factors to consider are: is the proposed readout a subtractive or divisive comparison between left and right channels? How would the predicted effect of sound intensity differ depending on that computational combination rule? Might the relatively small effect of intensity in this task reveal that the brain is doing a pretty good job of handling this under a range of conditions, but it is not perfect?</p><p>Other factors to consider are that the place codes may have effects of intensity as well, depending on how they are read out. For example, a weighted-sum readout of a place code will be sensitive to intensity signals. A weighted-average readout, however, incorporates a normalization mechanism that removes the intensity signal. A winner-take-all readout is also robust to intensity. I don't think the authors need to go into extensive detail, but a few qualifying phrases at well-placed locations in the manuscript to alert the reader that the comparison is not a matter of place codes vs rate codes per se but place-codes-and-their-readouts vs. rate-codes-and-their-readouts would go a long way.</p><p>References to include:</p><p>The Porter and Groh, (2006) study mentioned above is relevant for inclusion. In addition, Salminen and colleagues have conducted a number of studies arguing that the human brain does indeed use a rate code for sound location. In the primate brain, studies by Werner-Reiss and Groh, (2008, auditory cortex) and Groh et al., (2003, inferior colliculus) are relevant. I also recommend including citations to the literature on non-monotonic rate-level functions in the auditory pathway: this term refers to neurons whose responses go up and then <italic>down</italic> as a function of increasing sound intensity. The existence of these neurons may be part of the way in which the brain produces largely accurate sound localization <italic>despite</italic> mixed sensitivity to intensity and laterality in individual neurons.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Population rate-coding predicts correctly that human sound localization depends on sound intensity&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Barbara Shinn-Cunningham (Senior Editor), a Reviewing Editor, and two reviewers.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>After consultation among the reviewers regarding multiple details of the underlying neurophysiology as covered in the Discussion section, the consensus recommendation is that the authors add something along the lines that while their study was motivated by the notion that there's no auditory map, it is entirely possible that there are sufficient map-like signals in higher order auditory cortical fields to support perception. Citations for this latter point include: Higgins et al., 2010, and Woods et al., 2006.</p><p>There are additional fascinating findings in the neurophysiological literature regarding frequency and intensity tuning, and interesting correlations between non-monotonicity in the azimuthal and intensity dimensions, but a detailed discussion of these points is probably beyond the scope for the present behavioral/computational study.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47027.013</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary:</p><p>This is a combined modeling and behavioral study of human sound localization that supports rate-coding models. Since sound levels affect the firing rate of individual neurons, sometimes changes in intensity can be confused with changes in location. The experiments investigate psychophysical evidence for this predicted effect, with positive results.</p><p>The authors use a hemispheric difference rate coding model of a range of interaural time delays (ITD) to predict that sound localization would be biased towards midline locations at low sound levels. Though the hemispheric rate coding model is simplistic and has been explored before, it is a useful way to raise the question of whether such biases might exist perceptually in auditory systems as they do in visual systems.</p><p>Suggestions for improvement mostly concern additions and clarifications, not new experiments. In general questions from the review should be answered by changes to the text. We number major suggestions below.</p></disp-quote><p>Thank you for your thorough and constructive feedback. We now explicitly discuss rate-based readouts from the hemispheric difference model in the context of other rate-based models. We appreciate the pointers to prior decoding models of sound location by Groh and colleagues and have added those to the discussion. We now connect to the spatial adaptation work by Salminen and colleagues, which we previously missed.</p><disp-quote content-type="editor-comment"><p>1) Clarification of response types:towards the end, the Discussion section explains appropriately how &quot;both the auditory and the visual system, the same cells that are tuned to binaural ITD or binocular disparity also have intensity-response functions…..&quot; that should cause ambiguities (biases) in localizing sensory objects near the sensory midline. Figure 3 illustrates this principle with monotonic intensity dependence of neural response rate functions for ITD in central inferior colliculus. Thus, this part of the discussion is appropriately focused on some of the universal principles for neurometric and psychometric localization of sensory objects/stimuli in both visual and auditory systems, and their intensity/contrast dependence.</p></disp-quote><p>Thank you.</p><disp-quote content-type="editor-comment"><p>However, in the first paragraph of the Discussion section, some details are given regarding how visual cortical neurons fall into three types: (1) &quot;Tuned-excitatory&quot; neurons [that] respond best to zero spatial disparity; (2) &quot;near cells&quot; [that] respond more vigorously when [ocular] disparity[is crossed] (Parker and Cumming. (2001)); (3) and &quot;far cells&quot; when ocular disparity is uncrossed. This paragraph goes on to say that secondary visual cortices (V3) respond by and large to binocular cues (as opposed to monocular) cues. Do those binocular responses display biases towards the sensory field midline at low contrast? By analogy IT cortical neurons are biased towards central 0-6 degrees visual angle, is the IT central biased and tuning binaural and contrast dependent?</p></disp-quote><p>If somebody has looked at these intriguing questions, we are not aware of the work. We have searched the literature for contrast effects of this sort and could not find data that speaks to the questions.</p><disp-quote content-type="editor-comment"><p>It would be useful to explain that neural responses in the auditory cortices are also distributed into regions and cortical fields according to binaural types. Auditory cortices have contralateral tuned-excitatory neurons that respond best to contralateral location disparity (e.g. level difference or ITD difference) and auditory cortices also have &quot;tuned-excitatory&quot; neurons that respond best to zero spatial disparity. The latter have been called (EE/F) or &quot;primarily binaural&quot; or &quot;two-way intensity&quot; tuned (Semple and Kitzes, 1993b; Pollak et al., 2002; Zhang et al., 2004; Razak and Fuzessery, 2010; Higgins et al., 2010). Notably, many if not all of the latter studies find that the &quot;tuned-excitatory&quot; neurons observed in primary (A1) or secondary ventral auditory cortices respond optimally at lower sound intensities. Hence, this raises an interesting twist namely, that neurons also can encode sound location/azimuth position near the midline in spite of the &quot;ambiguities&quot; at low sound intensities possibly by using spatial cues other than ITD.</p></disp-quote><p>Thank you for raising this interesting twist. We now discuss this point explicitly (Discussion section). We agree with the reviewers that EE cells in auditory cortex that are tuned to the midline are also often tuned for sound intensity. However, this intensity tuning is not apparent at the very low sound intensities needed to explain the bias observed here. In fact the first paper by Semple and Kitzes, (1993) shows clearly a max response at 40 dB SPL and significant responses in the EE cells up to 80 dB SPL, a pattern supported by the later papers (Pollak et al., 2002; Zhang et al., 2004; Razak and Fuzessery, 2010; Higgins et al., 2010). That is, the tuned-excitatory neurons that encode the midline respond best not at the low sound levels where we observed a midline bias, but at sound levels 10X or 100X louder. These neurons would not be expected to be engaged in our task.</p><disp-quote content-type="editor-comment"><p>2) There is some confusion about what the results actually are.a) What is the difference between &quot;audibility&quot; and &quot;intensity&quot; in the statistical assessment (subsection “Human perception”, Table 1 and Table 2) What does audibility mean in this context? If it co-varies with intensity, why is it included as a term in the model? Is it a means of incorporating the audiograms of individual subjects?</p></disp-quote><p>The latter, it is a way of incorporating the audiograms of individual subjects into the statistical model. To keep readers from wondering whether overall audiograms could have caused the observed bias at our very soft sound intensities, we include the audiogram thresholds in the NLME fits. Reassuringly, the NLME parameters linking ITD and sound intensity to perceived laterality do not change appreciably when we include the subject’s pure tone audiometric thresholds. To make this point clear, we have reworded the caption of Table I.</p><disp-quote content-type="editor-comment"><p>b) Aren't changes to the slope of the psychometric function as a function of intensity the key result? Yet, that was significant for the second experiment but not the first (although it was close). I think it would be reasonable to say the first experiment didn't quite reject the null hypothesis but showed a trend, so a second experiment was done that controlled an additional aspect of the stimuli in the task, and the same subjects repeated the study, at which point strongly significant results were then obtained. The authors should be commended for including both experiments in the results.</p></disp-quote><p>Based on our modeling results, we expected to see bigger perceptual changes at lateral as compared to medial angles, and this is indeed what we saw in both experiments (maximal extent of lateralization varied with sound intensity). In addition, experiment 2 revealed a change in slope, meaning by controlling for audibility across-frequency, the effect is also visible at more medial source angles. We now point this out in the Results section.</p><disp-quote content-type="editor-comment"><p>c) Re: experiment 2: the description in subsection “Human perception” is clear in explaining that in experiment 2, the A-weighting equalizes the perceived bandwidth of the sounds across intensities, but is followed by: &quot;As a control for stimulus audibility&quot;. If I have understood the lead-in correctly, this sentence would make more sense to this reader as: &quot;As a control for perceived bandwidth&quot;.</p></disp-quote><p>Thank you for this suggestion. Reworded.</p><disp-quote content-type="editor-comment"><p>3) Computational context:</p><p>It would be useful to explain that although this study supports the intensity dependence of sound localization and potentially hemispheric models for the corresponding neural code alternative scenarios may exist. For example, is it not possible that the mammalian brain has both place-codes and rate codes in different parts that are called upon under different circumstances?</p></disp-quote><p>We now talk about this possibility in the Discussion section.</p><disp-quote content-type="editor-comment"><p>The computational underpinnings behind this experiment are perhaps more subtle than currently described. A previous study by Porter and Groh, 2006) lays out a few possible models for converting between place and rate codes and explicitly considers the dependence of neural firing patterns on both sound intensity and any other non-spatial factors (see Porter and Groh Figure 4 and page 319 for description of the models and the evidence for and against).</p></disp-quote><p>Thank you for pointing out how these prior studies connect with the current work. We have now expanded the discussion of rate- vs place-based readouts. We mention the possibility that rate-based could be converted into place-based readouts (Discussion section).</p><disp-quote content-type="editor-comment"><p>Factors to consider are: is the proposed readout a subtractive or divisive comparison between left and right channels? How would the predicted effect of sound intensity differ depending on that computational combination rule? Might the relatively small effect of intensity in this task reveal that the brain is doing a pretty good job of handling this under a range of conditions, but it is not perfect?</p></disp-quote><p>We now explain the predicted effects of sound intensity based on subtractive vs divisive readout (Discussion section). We agree with you that the small effect of intensity suggests that sound localization is largely robust across a range of conditions and now comment on this explicitly in the Discussion section.</p><disp-quote content-type="editor-comment"><p>Other factors to consider are that the place codes may have effects of intensity as well, depending on how they are read out. For example, a weighted-sum readout of a place code will be sensitive to intensity signals. A weighted-average readout, however, incorporates a normalization mechanism that removes the intensity signal. A winner-take-all readout is also robust to intensity. I don't think the authors need to go into extensive detail, but a few qualifying phrases at well-placed locations in the manuscript to alert the reader that the comparison is not a matter of place codes vs rate codes per se but place-codes-and-their-readouts vs. rate-codes-and-their-readouts would go a long way.</p><p>References to include:</p><p>The Porter and Groh, (2006) study mentioned above is relevant for inclusion. In addition, Salminen and colleagues have conducted a number of studies arguing that the human brain does indeed use a rate code for sound location. In the primate brain, studies by Werner-Reiss and Groh, (2008, auditory cortex) and Groh et al., (2003, inferior colliculus) are relevant.</p></disp-quote><p>Included.</p><disp-quote content-type="editor-comment"><p>I also recommend including citations to the literature on non-monotonic rate-level functions in the auditory pathway: this term refers to neurons whose responses go up and then down as a function of increasing sound intensity. The existence of these neurons may be part of the way in which the brain produces largely accurate sound localization despite mixed sensitivity to intensity and laterality in individual neurons.</p></disp-quote><p>We now mention that non-monotonic rate-level functions would complicate the read-out (Discussion section). We also point out the fact that these non-monotonicities mostly appear to arise at sound levels that are outside the range of sound intensities for which the hemispheric difference model predicts a behavioral bias.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>After consultation among the reviewers regarding multiple details of the underlying neurophysiology as covered in the Discussion section, the consensus recommendation is that the authors add something along the lines that while their study was motivated by the notion that there's no auditory map, it is entirely possible that there are sufficient map-like signals in higher order auditory cortical fields to support perception. Citations for this latter point include: Higgins et al., 2010, and Woods et al., 2006.</p><p>There are additional fascinating findings in the neurophysiological literature regarding frequency and intensity tuning, and interesting correlations between non-monotonicity in the azimuthal and intensity dimensions, but a detailed discussion of these points is probably beyond the scope for the present behavioral/computational study.</p></disp-quote><p>We greatly appreciate your thoughtful comments and have added two statements to this effect. In the Discussion section we have added the following sentences:</p><p>“Moreover, our psychophysical and computational results suggest that for sound localization based on ITD at low sound levels, cortical maps do not play a role. However, there is good evidence for spatial map-like signals in higher order auditory cortical fields when interaural level differences are present, at medium to high sound levels (Higgins et al., 2010). How these ILD-based cortical maps influence sound localization behavior is yet to be determined.”</p><p>In the Discussion section we now also state:</p><p>“There are additional fascinating findings in the neurophysiological literature regarding frequency and intensity tuning, and interesting correlations between non-monotonicity in the azimuthal and intensity dimensions (Woods et al., 2006), but a detailed discussion of these points is beyond the scope of the present behavioral-computational study.”</p></body></sub-article></article>