<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">10274</article-id> <article-id pub-id-type="doi">10.7554/eLife.10274</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Anxiety dissociates the adaptive functions of sensory and motor response enhancements to social threats</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-38581"><name><surname>El Zein</surname><given-names>Marwa</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-38787"><name><surname>Wyart</surname><given-names>Valentin</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6522-7837</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="other" rid="par-5"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-38788"><name><surname>Grèzes</surname><given-names>Julie</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-5"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Laboratoire de Neurosciences Cognitives, Département d’Etudes Cognitives, Ecole Normale Supérieure</institution>, <institution>PSL Research University</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff><aff id="aff2"><label>2</label><institution>Université Pierre et Marie Curie</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mason</surname><given-names>Peggy</given-names></name><role>Reviewing editor</role><aff id="aff3"><institution>University of Chicago</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>marwaelzein@gmail.com</email></corresp><fn fn-type="con" id="equal-contrib"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="pub" publication-format="electronic"><day>29</day><month>12</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>4</volume><elocation-id>e10274</elocation-id><history><date date-type="received"><day>22</day><month>07</month><year>2015</year></date><date date-type="accepted"><day>25</day><month>11</month><year>2015</year></date></history><permissions><copyright-statement>© 2015, El Zein et al</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>El Zein et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-10274-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.10274.001</object-id><p>Efficient detection and reaction to negative signals in the environment is essential for survival. In social situations, these signals are often ambiguous and can imply different levels of threat for the observer, thereby making their recognition susceptible to contextual cues – such as gaze direction when judging facial displays of emotion. However, the mechanisms underlying such contextual effects remain poorly understood. By computational modeling of human behavior and electrical brain activity, we demonstrate that gaze direction enhances the perceptual sensitivity to threat-signaling emotions – anger paired with direct gaze, and fear paired with averted gaze. This effect arises simultaneously in ventral face-selective and dorsal motor cortices at 200 ms following face presentation, dissociates across individuals as a function of anxiety, and does not reflect increased attention to threat-signaling emotions. These findings reveal that threat tunes neural processing in fast, selective, yet attention-independent fashion in sensory and motor systems, for different adaptive purposes.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10274.001">http://dx.doi.org/10.7554/eLife.10274.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.10274.002</object-id><title>eLife digest</title><p>Facial expressions can communicate important social signals, and understanding these signals can be essential for surviving threatening situations. Past studies have identified changes to brain activity and behavior in response to particular social threats, but it is not clear how the brain processes information from the facial expressions of others to identify these threats. Here, El Zein, Wyart and Grèzes aimed to identify how signals of threat are represented in the human brain.</p><p>The experiment used a technique called electroencephalography to record brain activity in healthy human volunteers as they examined angry and fearful facial expressions. El Zein, Wyart and Grèzes found that emotions that signaled a threat to the observer are better represented in particular regions of the brain – including those that control action – within a fraction of a second after the facial expression was shown to the volunteer. Moreover, the response of the brain regions that control action was greater in volunteers with higher levels of anxiety, which highlights the role of anxiety in reacting rapidly to social threats in the environment.</p><p>El Zein, Wyart and Grèzes’ findings show that social threats can alter brain activity very rapidly, and in a more selective manner than previously believed. A future challenge is to find out whether other aspects in threatening environments can stimulate similar increases in brain activity.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10274.002">http://dx.doi.org/10.7554/eLife.10274.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>anxiety</kwd><kwd>threat</kwd><kwd>decision-making</kwd><kwd>computational modeling</kwd><kwd>emotion</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-11-EMCO-00902</award-id><principal-award-recipient><name><surname>Grèzes</surname><given-names>Julie</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-11-0001-02 PSL*</award-id><principal-award-recipient><name><surname>Wyart</surname><given-names>Valentin</given-names></name><name><surname>Grèzes</surname><given-names>Julie</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-10-LABX-0087</award-id><principal-award-recipient><name><surname>Wyart</surname><given-names>Valentin</given-names></name><name><surname>Grèzes</surname><given-names>Julie</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>Fondation Roger de Spoelberch</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Grèzes</surname><given-names>Julie</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001677</institution-id><institution>Institut national de la santé et de la recherche médicale</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Wyart</surname><given-names>Valentin</given-names></name><name><surname>Grèzes</surname><given-names>Julie</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Social threats trigger enhanced neural representations within 200 milliseconds in sensory and motor systems of the human brain as a function of anxiety, highlighting its adaptive function in reacting rapidly to dangers in the environment.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Perceptual decisions rely on the combination of weak and/or ambiguous samples of sensory evidence. The accuracy of this decision process is particularly important for the interpretation of negative signals, which require rapid and adaptive responses. In the social domain, identifying the emotional state of a conspecific – e.g., is he/she angry or afraid? – rarely depends solely on facial features, which are usually ambiguous and can imply different levels of threat for the observer. Surrounding cues, such as gaze direction and body posture, are known to act as contextual information during emotion recognition (<xref ref-type="bibr" rid="bib57">Righart and de Gelder, 2008</xref>; <xref ref-type="bibr" rid="bib7">Barrett and Kensinger, 2010</xref>; <xref ref-type="bibr" rid="bib4">Aviezer et al., 2011</xref>). Specifically, the detection of anger represents an immediate threat for the observer when paired with a direct gaze; by contrast, it is when paired with an averted gaze that fear marks the presence (and possibly the localization) of a threat in the environment (<xref ref-type="bibr" rid="bib58">Sander et al., 2007</xref>). These threat-signaling combinations of gaze direction and emotion have been shown to be better recognized and rated as more intense than other combinations (<xref ref-type="bibr" rid="bib2">Adams and Kleck, 2003</xref>, <xref ref-type="bibr" rid="bib3">2005</xref>; <xref ref-type="bibr" rid="bib28">Graham and LaBar, 2007</xref>; <xref ref-type="bibr" rid="bib58">Sander et al., 2007</xref>; <xref ref-type="bibr" rid="bib10">Bindemann et al., 2008</xref>; <xref ref-type="bibr" rid="bib47">N’Diaye et al., 2009</xref>), and this as a function of anxiety level of the individuals (<xref ref-type="bibr" rid="bib25">Ewbank et al., 2010</xref>). However, the computational mechanisms underlying the prioritization of threat-signaling information remain unspecified.</p><p>Classical decision theory distinguishes two classes of mechanisms by which contextual information such as gaze direction could influence the recognition of negative emotions. Gaze direction could <italic>bias</italic> the interpretation of negative facial expressions in favor of the emotion signaling higher threat in this context – anger for direct gaze, fear for averted gaze. In signal detection theoretical terms (<xref ref-type="bibr" rid="bib30">Green and Swets, 1966</xref>; <xref ref-type="bibr" rid="bib43">Macmillan and Creelman, 2004</xref>), this effect would correspond to an additive shift of the decision criterion as a function of gaze direction. However, gaze direction could also increase the perceptual <italic>sensitivity</italic> to the facial features diagnostic of the emotion signaling higher threat. In contrast to the first account, this effect would correspond to a multiplicative boost of threat-signaling cues in the decision process. While the two accounts predict similar effects of gaze direction on the recognition of threat-signaling emotions, a bias effect would be maximal for neutral (emotionless) expressions, whereas a sensitivity effect would be maximal at low emotion strengths (<xref ref-type="fig" rid="fig1">Figure 1</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.10274.003</object-id><label>Figure 1.</label><caption><title>Model predictions for the effect of gaze direction on emotion categorization.</title><p>Left panel: prediction of an effect of gaze direction on decision bias. Upper left panel: if gaze direction biases the interpretation of negative facial expressions in favor of the emotion signaling higher threat, direct gaze would additively bias the choice selection toward anger. Lower left panel: the predicted psychometric function would accordingly be shifted to the left for direct gaze, as participants will be biased toward interpreting faces displaying a direct gaze as angry. Maximal effects would appear for neutral (emotionless) expressions as highlighted through the filled grey area on the emotion axis that represents the difference between the two psychometric functions for direct and averted gaze. Right panel: prediction of an effect of gaze direction on perceptual sensitivity. Upper right panel: if gaze direction increases the sensitivity to the facial features diagnostic of the emotion signaling higher threat, direct gaze would now multiplicatively boost the processing of an angry expression displaying a direct gaze. Lower right panel: the predicted psychometric function would now show an increased slope for threat-signaling emotions, with maximal effects at low emotion strengths (as shown in the filled grey area on the emotion axis).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10274.003">http://dx.doi.org/10.7554/eLife.10274.003</ext-link></p></caption><graphic xlink:href="elife-10274-fig1-v1.tif"/></fig></p><p>Here we arbitrated between these two possible accounts by recording human electroencephalogram (EEG) signals while participants categorized facial expressions as displaying anger or fear. We manipulated emotion strength by presenting ‘morphed’ facial expressions ranging from neutral to intense anger or fear, and contextual information by pairing facial expressions with direct or averted gaze. The parametric control over emotion strength afforded fitting decision theoretical models to the behavioral and neural data to arbitrate between bias and sensitivity accounts of threat-dependent effects on emotion recognition. At the neural level, previous studies have reported interactions between emotion and gaze direction from 200 ms following face presentation (<xref ref-type="bibr" rid="bib59">Sato et al., 2004</xref>; <xref ref-type="bibr" rid="bib47">N’Diaye et al., 2009</xref>; <xref ref-type="bibr" rid="bib1">Adams et al., 2012</xref>; <xref ref-type="bibr" rid="bib17">Conty et al., 2012</xref>), but failed to characterize the computational mechanism responsible for these effects. Here, we applied model-guided regressions of single-trial EEG signals to determine whether the neural ‘encoding’ of threat-signaling emotions is enhanced in ventral face-selective and/or dorsal motor regions (<xref ref-type="bibr" rid="bib23">El Zein et al., 2015</xref>), and whether this enhancement is mediated by increased top-down attention to threat-signaling facial features. As high-anxious individuals show increased sensitivity to threats, but also negative signals in general (<xref ref-type="bibr" rid="bib11">Bishop, 2007</xref>; <xref ref-type="bibr" rid="bib16">Cisler and Koster, 2010</xref>), we further assessed the neural mechanisms by which anxiety influences the detection of and reaction to social threats.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavior</title><p>Participants were presented at each trial with a face expressing fear or anger of varying emotion strength (7 levels of emotion strength for each emotion) and had to categorize the displayed emotion (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Crucially, direction of gaze (direct or averted) was manipulated independently of the displayed emotion in a completely implicit fashion, as it was never mentioned to the subjects nor relevant to the emotion categorization task. Nevertheless, in addition to an expected increase in categorization performance with emotion strength (F <sub>6,138</sub> = 187.3, p&lt;0.001), gaze direction strongly interacted with the displayed emotion on performance (F<sub>1,23</sub> = 21.2, p&lt;0.001). Facial displays of anger were better categorized when paired with a direct gaze (<italic>t</italic><sub>23</sub> = 4.3, p&lt;0.001), whereas expressions of fear were better categorized when paired with an averted gaze (<italic>t</italic><sub>23</sub> = -3.4, p&lt;0.01;  <xref ref-type="fig" rid="fig3">Figure 3a</xref>). These combinations of gaze and emotion, anger paired with a direct gaze and fear paired with an averted gaze, are associated with higher threat for the observer (<xref ref-type="bibr" rid="bib58">Sander et al., 2007</xref>), albeit of different natures. In the case of anger, gaze direction indicates the target of the threat, while in the case of fear gaze direction signals its source. Nevertheless, just as the combination of anger with a direct gaze is more threatening/relevant than with an averted gaze, fear is more threatening when paired with an averted gaze than with a direct gaze. These two combinations, anger direct and fear averted, will thus be labeled as THREAT+ combinations as opposed to THREAT− combinations (i.e., anger paired with averted gaze, and fear paired with direct gaze).<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.10274.004</object-id><label>Figure 2.</label><caption><title>Stimuli and experimental procedure.</title><p>(<bold>a</bold>) Examples of morphed expressions for one identity: morphs from neutral to intense fearful/angry expressions providing evidence for one or the other emotion. Stimuli displayed either an averted or a direct gaze. THREAT+ conditions (in orange) correspond to combinations of gaze and emotion that signal higher threat for the observer as compared to THREAT− conditions (in green). (<bold>b</bold>) Following fixation, a facial expression appeared for 250 ms, after which the participant had to indicate whether the face expressed anger or fear within 2 seconds. No feedback was provided after response.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10274.004">http://dx.doi.org/10.7554/eLife.10274.004</ext-link></p></caption><graphic xlink:href="elife-10274-fig2-v1.tif"/></fig><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.10274.005</object-id><label>Figure 3.</label><caption><title>Enhanced recognition accuracy and perceptual sensitivity to threat-signaling emotions.</title><p>(<bold>a</bold>) Proportion of correct responses for (from left to right) averted/anger, direct/anger, averted/fear and direct/fear. THREAT+ combinations of gaze and emotion (in orange) were associated with increased recognition accuracy. (<bold>b</bold>) Psychometric function representing the proportion of ‘anger’ responses as a function of the evidence for anger (proportion morph, 0 = neutral, negative towards fear, and positive towards anger) for THREAT+ (orange) and THREAT− (green) combinations of gaze and emotion. Dots and attached error bars indicate the human data (mean ± s.e.m.). Lines and shaded error bars indicate the predictions of the best-fitting model. (<bold>c</bold>) Parameter estimate for the slope of the psychometric curve (corresponding to emotion sensitivity) for THREAT+ and THREAT− combinations. **p &lt; 0.01, ***p &lt; 0.001.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10274.005">http://dx.doi.org/10.7554/eLife.10274.005</ext-link></p></caption><graphic xlink:href="elife-10274-fig3-v1.tif"/></fig></p><p>Moreover, a significant emotion by gaze by emotion strength interaction was observed (F<sub>6,138</sub> = 4.3, p&lt;0.01), explained by a stronger influence of gaze on emotion categorization at weak emotion strengths (gaze by emotion interaction for levels 1 to 4, F<sub>1,23</sub> = 23.8, p&lt;0.001) than at high emotion strengths (gaze by emotion interaction for levels 5 to 7, F<sub>1,23</sub> = 5.1, p&lt;0.05). Reaction time (RT) analyses revealed a decrease of correct RTs with emotion strength (repeated-measures ANOVA, F<sub>6,138</sub> = 54.5, p&lt;0.001), faster responses to angry as compared fearful faces (F<sub>1,23</sub> = 12, p&lt;0.01), and faster responses to direct as compared to averted gaze (F<sub>1,23</sub> = 7.7, p&lt;0.05). Furthermore, an emotion by gaze interaction was observed (F<sub>1,23</sub> = 8, p&lt;0.01), corresponding to faster reaction times for direct as compared to averted gaze in the anger condition only (t<sub>23</sub> = -3.9, p&lt;0.001).</p><p>To characterize the mechanism underlying the improved recognition of threat-signaling emotions, we fitted participants’ behavior using a family of nested models of choice which hypothesize that decisions are formed on the basis of a noisy comparison between the displayed emotion and a criterion, under the following formulation (see Materials and methods for details):</p><p><disp-formula id="equ1"><mml:math id="m1"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>Φ</mml:mi><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mo>·</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>]</mml:mo><mml:mo>·</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ε</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>5</mml:mn><mml:mo>·</mml:mo><mml:mi>ε</mml:mi></mml:math></disp-formula></p><p>where P(anger) corresponds to the probability of judging the face as angry, Ф[. ] to the cumulative normal function, <italic>w</italic> to the perceptual sensitivity to the displayed emotion, <bold>x</bold> to the evidence (emotion strength) in favor of anger or fear in each trial (from -7 for an intense expression of fear to +7 for an intense expression of anger), <italic>b</italic> to an additive stimulus-independent bias in favor one of the two responses/emotions, and <italic>ε</italic> to the proportion of lapses (random guesses) across trials.</p><p>We compared a ‘null’ model which did not allow for contextual influences of gaze direction on the decision process, to two additional models which instantiate two different mechanisms which could account for the observed increase in recognition accuracy for THREAT+ combinations of gaze and emotion: 1. a first variant in which gaze direction <italic>biases</italic> the decision criterion in favor of the emotion signaling higher threat, and 2. a second variant in which gaze direction enhances the <italic>sensitivity</italic> to the emotion signaling higher threat. Bayesian model selection revealed that a sensitivity enhancement for THREAT+ combinations explained substantially better the behavioral data than a criterion shift (Bayes Factor ≈ 10<sup>8</sup>, exceedance probability <italic>p</italic><sub>exc</sub> &gt; 0.74). Maximum-likelihood estimates of the perceptual sensitivity parameter <italic>w</italic> extracted from the winning model were significantly increased for THREAT+ combinations of gaze and emotion (<italic>t</italic><sub>23</sub> = 3.9, p&lt;0.001; <xref ref-type="fig" rid="fig3">Figure 3b,c</xref>). The proportion of lapses did not vary between THREAT+ and THREAT- combinations (t<sub>23</sub> = 0.4, p&gt;0.5).</p></sec><sec id="s2-2"><title>Enhanced neural encoding of threat-signaling emotions</title><p>To validate the finding of enhanced sensitivity to threat-signaling emotions, and to identify its neural substrates, we then investigated how facial expressions modulated scalp EEG activity recorded during the emotion categorization task. Instead of computing event-related averages, we relied on a parametric regression-based approach consisting in regressing single-trial EEG signals against the strength of the displayed emotion at each electrode and time point following the presentation of the face (<xref ref-type="bibr" rid="bib63">Wyart et al., 2012a</xref>, <xref ref-type="bibr" rid="bib64">2015</xref>). A general linear regression model (GLM) was fit to the EEG data where emotion strength (from 0 for a neutral/emotionless expression to 7 for an intense fear/anger expression) was introduced as a trial-per-trial predictor of broadband EEG signals at each electrode and time point following stimulus onset (from 0.2 s before to 1.0 s following stimulus onset). The resulting time course at each electrode represents the degree to which EEG activity ‘encodes’ (co-varies with) the emotion strength provided by morphed facial features.</p><p>Parameter estimates of the regression slope revealed significant correlations between emotion strength and EEG activity peaking initially around 280 ms following face presentation at temporal (t-test against zero, <italic>t</italic><sub>23</sub> = -12.7, p&lt;0.001) and frontal electrodes (<italic>t</italic><sub>23</sub> = 8.7, p&lt;0.001), and then around 500 ms and at response time at centro-parietal (<italic>t</italic><sub>23</sub> = 10.2, p&lt;0.001) and frontal electrodes (<italic>t</italic><sub>23</sub> = -7.9, p&lt;0.001) (<xref ref-type="fig" rid="fig4">Figure 4a–c</xref>). Time points and electrodes where parameter estimates diverge significantly from zero indicate neural encoding of emotion information. The strength of this neural encoding – indexed by the amplitude of the parameter estimate – provides a measure of the <italic>neural sensitivity</italic> to emotion information.<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.10274.006</object-id><label>Figure 4.</label><caption><title>Enhanced neural encoding of threat-signaling emotions.</title><p>(<bold>a</bold>) Middle panel: scalp topography of neural encoding at 280 ms, corresponding to its first peak of the encoding of emotion strength averaged across conditions (peak of deviation from zero), and expressed as mean parameter estimates in arbitrary units (a.u.). Dots indicate electrodes of interest where neural encoding was maximal. Left and right panels: encoding time course for THREAT+ and THREAT− conditions at electrodes of interest. Shaded error bars indicate s.e.m. Thick orange and green lines indicate significance against zero at a cluster-corrected p-value of 0.05. Shaded grey areas indicate significant differences between THREAT+ and THREAT− conditions at p &lt; 0.05. (<bold>b</bold>) Same conventions as (<bold>a</bold>) at the second neural encoding peak at 500 ms. (<bold>c</bold>) Same conventions as (<bold>a</bold>) at the third neural encoding peak at response time. (<bold>d</bold>) Estimated cortical sources of the encoding difference between THREAT+ and THREAT− conditions at the time of significant difference between conditions at 170 ms. (<bold>e</bold>) Same as (<bold>d</bold>) at 500 ms. (<bold>f</bold>) Same as (<bold>d</bold>) at response time. FG: fusiform gyrus, pSTS: posterior superior temporal sulcus, SMG: supramarginal gyrus, ANG: angular gyrus, STG: superior temporal gyrus, MTG: middle temporal gyrus, OCG: occipital gyrus, aINS: anterior insula, IFS: inferior frontal sulcus, TP: temporal pole, OFC: orbitofrontal cortex, OP: occipital pole, TPJ: temporo-parietal junction, dlPFC: dorsolateral prefrontal cortex.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10274.006">http://dx.doi.org/10.7554/eLife.10274.006</ext-link></p></caption><graphic xlink:href="elife-10274-fig4-v1.tif"/></fig></p><p>To test for a neural signature of the increased sensitivity to threat-signaling emotions, we compared parameter estimates extracted separately for THREAT+ (anger direct and fear averted) and THREAT− (anger averted and fear direct) combinations of gaze and emotion. This contrast revealed increased parameter estimates for THREAT+ combinations first at 170 ms at temporal (paired t-test, <italic>t</italic><sub>23</sub> = -2.5, p&lt;0.05) and frontal electrodes (<italic>t</italic><sub>23</sub> = 2.2, p&lt;0.05), and then later at 500 ms and at response time at centro-parietal (<italic>t</italic><sub>23</sub>= 2.2, p&lt;0.05) and frontal electrodes (<italic>t</italic><sub>23</sub> = -2.4, p&lt;0.05) (<xref ref-type="fig" rid="fig4">Figure 4a–c</xref>). This finding indicates that the neural gain of emotion encoding was enhanced at these time points and electrodes for threat-signaling emotions. This threat-dependent enhancement remained significant when considering only correct responses (temporal: <italic>t</italic><sub>23</sub> = -2.1, p&lt;0.05; centro-parietal <italic>t</italic><sub>23</sub> = 4.2, p&lt;0.001). Interestingly, THREAT+ combinations were not associated with increased event-related averages at classical peak latencies (P1, N170, P2, P3: all <italic>t</italic><sub>23</sub> &lt; 1.95, p&gt;0.07). To assess which brain regions generated the scalp-recorded EEG signals, we computed the cortical sources of this enhanced encoding of threat-signaling emotions by performing the same regression approach to minimum-norm current estimates distributed across the cortical surface. Parameter estimates at time points of interest (where differences between THREAT+ and THREAT− combinations were observed) were then contrasted between the two conditions (see Materials and methods). Increases in regression slopes for THREAT+ combinations shifted from ventral visual areas selective to facial expressions of emotion (fusiform gyrus and superior temporal sulcus) around 170 ms, to associative brain regions encompassing parietal, temporal and frontal cortices (superior and middle temporal, temporal pole, and orbitofrontal cortices) at 500 ms, and then to sensorimotor regions around response onset (dorsal central, parietal and frontal regions) (<xref ref-type="fig" rid="fig4">Figure 4d–f</xref>).</p><p>These neural effects converge with behavioral modeling in favor of a sustained enhancement of perceptual sensitivity to threat-signaling emotions, starting 170 ms following face presentation and lasting until after response onset. Additional evidence supports our hypothesis that enhancements in neural sensitivity to THREAT+ combinations are specifically linked to an increase in implied threat for these combinations of gaze and emotion. A separate group of participants rated the identities used in the emotion categorization task in terms of perceived threat and trustworthiness (see Materials and methods), and the group-level ratings for each identity were regressed against single-trial EEG signals as additional regressors. This regression showed that perceived threat, but not trustworthiness, correlated significantly with temporal and centro-parietal EEG activity at 500 ms following face presentation, in the same direction as the contrast between THREAT+ and THREAT− combinations (threat: <italic>t</italic><sub>23</sub> &gt; 3.6, p&lt;0.01; trustworthiness: <italic>t</italic><sub>23</sub> &lt; 0.7, p&gt;0.48).</p></sec><sec id="s2-3"><title>Attention-independent enhancement of neural processing by threat</title><p>Analyses of the neural data have so far confirmed the hypothesis that contextual gaze information affects emotion categorization by increasing the perceptual sensitivity to threat-signaling emotions. Such an effect could be mediated by increased top-down attention to threat-signaling emotions – i.e., THREAT+ combinations (anger direct and fear averted). To test this possibility, we explored whether residual fluctuations in single-trial EEG signals unexplained by variations in emotion strength (measured by the previous regressions) modulated the accuracy of the subsequent categorical decision – i.e., the perceptual sensitivity to the displayed emotion. This approach is reminiscent of ‘choice probability’ measures applied in electrophysiology to measure correlations between neural activity and choice behavior (<xref ref-type="bibr" rid="bib14">Britten et al., 1996</xref>; <xref ref-type="bibr" rid="bib60">Shadlen et al., 1996</xref>; <xref ref-type="bibr" rid="bib51">Parker and Newsome, 1998</xref>) – by estimating how much fluctuations in recorded neural signals are ‘read out’ by the subsequent decision (<xref ref-type="bibr" rid="bib63">Wyart et al., 2012a</xref>, <xref ref-type="bibr" rid="bib64">2015</xref>). Stimulus-independent improvements in neural-choice correlations have been classically interpreted as increases in ‘read-out’ weights – i.e., increased top-down attention to these neural signals (<xref ref-type="bibr" rid="bib46">Nienborg and Cumming, 2009</xref>, <xref ref-type="bibr" rid="bib45">2010</xref>). Here, an increased neural modulation of choice for THREAT+ conditions could indicate an increase in top-down attention to threat-signaling emotions, which could in turn explain the observed increase in perceptual and neural sensitivity to these combinations of gaze and emotion.</p><p>To test this hypothesis, we entered EEG residuals from the previous regression against emotion strength as an additional ‘mediation’ predictor of choice – as means to test whether these neural signals co-vary with perceptual sensitivity (see Materials and methods for details). In practice, we estimated the parameters <italic>b</italic><sub>mod</sub> and <italic>w</italic><sub>mod</sub> of these neural modulation terms at each time point following face presentation via an EEG-informed regression of choice for which the trial-by-trial neural residuals <bold>e</bold> from the regression against emotion strength were entered either alone (additive influence, parameter <italic>b</italic><sub>mod</sub>) or as their interaction with emotion strength (multiplicative influence, parameter <italic>w</italic><sub>mod</sub>) as additional predictors of the subsequent choice:</p><p><disp-formula id="equ2"><mml:math id="m2"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>[</mml:mo><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mo> </mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo><mml:mo> </mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mi>e</mml:mi><mml:mo>]</mml:mo></mml:math></disp-formula></p><p>The time course and spatial distribution of this neural modulation of perceptual sensitivity (<italic>wmod</italic>) followed qualitatively the neural encoding of emotion strength (<xref ref-type="fig" rid="fig5">Figure 5a–c</xref>), with a negative temporal component peaking at 270 ms (<italic>t</italic><sub>23</sub> = -4.2, p&lt;0.001), followed by a positive centro-parietal one peaking around 600 ms (<italic>t</italic><sub>23</sub> = 8.0, p&lt;0.001) and then at response time (<italic>t</italic><sub>23</sub> = 7.6, p&lt;0.001). We used Bayesian model selection to confirm that EEG residuals co-varied multiplicatively with the perceptual sensitivity (<italic>wmod</italic>) of the subsequent decision, not additively as a bias (<italic>bmod</italic>) in emotion strength, both at temporal (Bayes factor ≈ 10<sup>3.4</sup>, <italic>p</italic><sub>exc</sub> = 0.79) and centro-parietal electrodes (Bayes factor ≈ 10<sup>8.9</sup>, <italic>p</italic><sub>exc</sub> = 0.99). Critically, no difference in modulation strength was observed between THREAT+ (anger direct and fear averted) and THREAT− (anger averted and fear direct) combinations (temporal: <italic>t</italic><sub>23</sub> = -0.4, p&gt;0.5; centro-parietal: <italic>t</italic><sub>23</sub> = 0.1, p&gt;0.5). To determine whether this absence of significant difference is due to a genuine absence of effect (rather than a lack of statistical sensitivity), we computed Bayes factors under the same parametric assumptions as conventional statistics (see Materials and methods). We obtained Bayes factors lower than 10–4 at temporal and centro-parietal electrodes, indicative of no increase in ‘read-out’ weights for THREAT+ conditions. This null effect suggests that the observed enhancement in perceptual and neural sensitivity to these threat-signaling combinations of gaze and emotion is not triggered indirectly by an increase in top-down attention in these conditions.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.10274.007</object-id><label>Figure 5.</label><caption><title>Absence of threat-dependent enhancement of neural-choice correlations.</title><p>(<bold>a</bold>) Middle panel: scalp topography of neural-choice correlations, expressed as the modulation of perceptual sensitivity by EEG encoding residuals at 280 ms, same time point shown in <xref ref-type="fig" rid="fig4">Figure 4a</xref>. Electrodes of interest indicated with dots are the same as in <xref ref-type="fig" rid="fig4">Figure 4a</xref>. Left and right panels, time course of the modulation of perceptual sensitivity by EEG encoding residuals expressed in arbitrary units (a.u.). Same conventions as in <xref ref-type="fig" rid="fig4">Figure 4a</xref>. (<bold>b</bold>) Same conventions as (<bold>a</bold>) at 500 ms. (<bold>c</bold>) Same conventions as (<bold>a</bold>) at response time. The variation of the modulation strength over time is consistent with the variation of the encoding parameter estimate. No difference between THREAT+ and THREAT− is observed.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10274.007">http://dx.doi.org/10.7554/eLife.10274.007</ext-link></p></caption><graphic xlink:href="elife-10274-fig5-v1.tif"/></fig></p></sec><sec id="s2-4"><title>Early neural encoding of threat-signaling emotions in motor preparation</title><p>We reasoned that threat could impact not only the neural representation of the displayed emotion in visual and associative cortices, but also the preparation of the upcoming response in effector-selective structures (<xref ref-type="bibr" rid="bib17">Conty et al., 2012</xref>). To measure response-preparatory signals in the neural data, we computed spectral power in the mu and beta frequency bands (8–32 Hz) (<xref ref-type="bibr" rid="bib22">Donner et al., 2009</xref>; <xref ref-type="bibr" rid="bib19">de Lange et al., 2013</xref>). Limb movement execution and preparation coincide with suppression of low-frequency (8–32 Hz) activity that is stronger in the motor cortex contralateral as compared to ipsilateral to the movement. Thus, subtracting the contralateral from ipsilateral motor cortex activity is expected to result in a positive measure of motor preparation. The contrast between left-handed and right-handed responses at response time identified lateral central electrodes, associated with focal sources in motor cortex (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). Subtracting contralateral from ipsilateral signals relative to the hand assigned to the ‘fear’ response (counterbalanced across participants) provided a motor lateralization index whose sign predicts significantly the upcoming choice (anger or fear) from 360 ms before response onset (paired t-test, <italic>t</italic><sub>23</sub> = 4.6, p&lt;0.001; <xref ref-type="fig" rid="fig6">Figure 6b</xref>).<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.10274.008</object-id><label>Figure 6.</label><caption><title>Encoding of threat-signaling emotions in motor response lateralization measures.</title><p>(<bold>a</bold>) Top panel, scalp topography before response of the time frequency power in the 8–32 Hz band in the last 100 ms before response, for the trials where subjects responded with their left hand minus the trials where they responded with their right hand. Dots correspond to the selected electrodes, where the effect was maximal. Bottom panel: corresponding neural sources. (<bold>b</bold>) Time course of response lateralization (time frequency power activity from the contralateral electrodes minus ipsilateral electrodes to the hand used to respond ‘fear’) towards anger and fear when the choice was anger (red) or fear (blue). Shaded error bars indicate s.e.m. The shaded gray area indicates a significant difference in motor lateralization between Anger and Fear responses. (<bold>c</bold>) Encoding of emotion strength in response lateralization index for THREAT+ (orange) and THREAT− (green) conditions. Differences between conditions are observed at 200 ms after stimulus onset (stimulus-locked, upper panel) and at response time (response-locked, lower panel). Conventions are the same as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. (<bold>d</bold>) Time course of neural-choice correlations, expressed as the modulation of additive bias by motor lateralization encoding residuals in arbitrary units (a.u.) stimulus-locked (upper panel) and response locked (lower panel). Conventions are the same as in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10274.008">http://dx.doi.org/10.7554/eLife.10274.008</ext-link></p></caption><graphic xlink:href="elife-10274-fig6-v1.tif"/></fig></p><p>We applied the previous neural encoding approach by regressing this motor lateralization index against the <italic>signed</italic> emotion strength (from 0 for a neutral expression, to ±7 for an intense anger/fear expression) on a trial-by-trial basis. Parameter estimates of the regression slope diverged significantly from zero from 400 ms after stimulus onset (t-test against zero, <italic>t</italic><sub>23</sub> = 5.1, p&lt;0.001) and at response time (<italic>t</italic><sub>23</sub> = 5.2, p&lt;0.001) – reflecting stronger response preparation to stronger (i.e., more diagnostic) emotions. Computing regression slopes separately for THREAT+ (anger direct and fear averted) and THREAT- (anger averted and fear direct) combinations revealed that THREAT+ combinations produced a stronger encoding of emotion strength in motor preparation late at response onset (<italic>t</italic><sub>23</sub> = 2.9, p&lt;0.01), but also early around 200 ms following face presentation (<italic>t</italic><sub>23</sub> = 3.2, p&lt;0.01). This early threat-dependent motor enhancement remained significant when considering only correct responses (<italic>t</italic><sub>23</sub> = 3.0, p&lt;0.01). While THREAT− combinations of gaze and emotion were not associated with significant neural encoding in motor preparation until 440 ms following face presentation (<italic>t</italic><sub>23</sub> &lt; 0.8, p&gt;0.4), THREAT+ combinations resulted in significant neural encoding between 100 and 320 ms, peaking at 200 ms (<italic>t</italic><sub>23</sub> = 3.2, p&lt;0.01; <xref ref-type="fig" rid="fig6">Figure 6c</xref>).</p><p>To determine whether this early neural encoding of threat-signaling emotions in motor preparation influences the speed of subsequent responses, we recomputed and compared regression parameters estimated separately for fast and slow responses to THREAT+ combinations (anger direct and fear averted), on the basis of a median split of response times informed by emotion strength. This comparison revealed a single, gradual neural encoding of emotion strength in motor preparation preceding fast, but not slow responses, arising as early as 150 ms (at a threshold <italic>p</italic>-value of 0.05) following the presentation of the face (difference in encoding onset between fast and slow responses, jackknifed (<xref ref-type="bibr" rid="bib37">Kiesel et al., 2008</xref>, see Materials and methods) <italic>t</italic><sub>23</sub> = 5.2, p&lt;0.001; <xref ref-type="fig" rid="fig7">Figure 7a</xref>). This effect indicates that the early neural encoding of THREAT+ combinations in motor preparation is characteristic of efficient (fast) responses. We verified that this latency shift in neural encoding was selective of motor preparation signals, by performing the same comparison on the neural encoding of emotion strength at centro-parietal electrodes. This contrast revealed only a difference in peak amplitude, not onset latency, between fast and slow responses (peak amplitude: <italic>t</italic><sub>23</sub> = 5.1, p&lt;0.001; onset latency: jackknifed <italic>t</italic><sub>23</sub> = -1.3, p&gt;0.2; <xref ref-type="fig" rid="fig7">Figure 7b</xref>).<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.10274.009</object-id><label>Figure 7.</label><caption><title>Encoding of emotion strength as a function of reaction times (RT) in motor and parietal structures.</title><p>(<bold>a</bold>) Neural encoding of emotion strength for THREAT+ conditions in motor lateralization for fast and slow reaction times (RT): when RTs were fast, the encoding of emotion strength became significant at 150 ms and rose gradually until response; by contrast, when RTs were slow, the encoding of emotion strength became significant later at 540 ms. Shaded error bars indicate s.e.m. Thick dark and light grey lines indicate significance against zero at a cluster-corrected p-value of 0.05. Shaded grey bars indicate significant differences between fast and slow responses. Encoding latency is significantly different between fast and slow RTs, ***: p&lt;0.001 (<bold>b</bold>) Emotion strength encoding in parietal electrodes. Convention are the same than (<bold>a</bold>). Fast responses are associated with a stronger neural encoding of emotion strength, but without any change in encoding latency.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10274.009">http://dx.doi.org/10.7554/eLife.10274.009</ext-link></p></caption><graphic xlink:href="elife-10274-fig7-v1.tif"/></fig></p><p>Finally, we performed neural-choice correlations analyses to assess whether the early neural encoding of threat-signaling emotions in motor preparation influences not only the speed, but also the content (anger or fear) of subsequent responses. Across conditions, the neural ‘mediation’ analysis described above revealed that stimulus-independent fluctuations in motor lateralization index co-vary as an additive choice bias in the upcoming response from 400 ms following face presentation (<italic>t</italic><sub>23</sub> = 2.9, p&lt;0.01). Indeed, in contrast to fluctuations in temporal and centro-parietal activity, the impact of variability in motor lateralization on emotion categorization was better described as an additive choice <italic>bias</italic> rather than a change in perceptual <italic>sensitivity</italic> (Bayes factor ≈ 10<sup>36.4</sup>, <italic>p</italic><sub>exc</sub> = 0.98) – consistent with its hypothesized role as a motor representation of the decision variable (<xref ref-type="bibr" rid="bib22">Donner et al., 2009</xref>; <xref ref-type="bibr" rid="bib19">de Lange et al., 2013</xref>). No difference in modulation strength was observed between THREAT+ (anger direct and fear averted) and THREAT− (anger averted and fear direct) combinations (<italic>t</italic><sub>23</sub> &lt; 1.6, p&gt;0.1; <xref ref-type="fig" rid="fig6">Figure 6d</xref>). Critically, even when considering combinations alone, residual variability in motor lateralization measured between 100 and 320 ms (where the neural encoding of threat-signaling emotions was significant) did not bias significantly the upcoming choice (<italic>t</italic><sub>23</sub> &lt; 1.4, p&gt;0.17). This null effect was supported by Bayesian model selection that identified a genuine absence of neural-choice correlation as the most likely account of the data (Bayes factor ≈ 10<sup>2.3</sup>, <italic>p</italic><sub>exc</sub> = 0.96). This finding indicates that the early neural encoding of threat-signaling emotions in motor preparation occurs earlier than the formation of the upcoming choice.</p></sec><sec id="s2-5"><title>Anxiety-dependent neural encoding of threat-signaling emotions</title><p>In the general population, anxiety has been classically associated with an oversensitivity to threat signals in social conditions (<xref ref-type="bibr" rid="bib11">Bishop, 2007</xref>; <xref ref-type="bibr" rid="bib16">Cisler and Koster, 2010</xref>). Here, we assessed whether the enhanced neural processing of threat-signaling emotions in temporal and motor regions co-varied with the level of anxiety in our participants. For this purpose, we measured anxiety at the beginning of the experimental session, before data collection, using the Spielberger State-Trait Anxiety Inventory (STAI) (<xref ref-type="bibr" rid="bib61">Spielberger et al. 1983</xref>). This self-questionnaire provides a measure of vulnerability for anxiety disorders (<xref ref-type="bibr" rid="bib31">Grupe et al. 2013</xref>). Participants’ state anxiety scores ranged from 20 to 45 (mean = 30.5, SD = 6.8). Trait anxiety scores ranged from 22 to 52 (mean = 38.2, SD = 7.8). These scores are comparable to the original published norms for this age group (<xref ref-type="bibr" rid="bib61">Spielberger, 1983</xref>) and to those from French normative data (<xref ref-type="bibr" rid="bib15">Bruchon-Schweitzer and Paulhan, 1993</xref>). We analyzed the effect of anxiety on the behavioral and neural data in two complementary ways: 1. by splitting the participants in two equally-sized groups based on their measured anxiety, and 2. by correlating neural encoding parameters estimated at the level of individual participants with their measured anxiety. Surprisingly, we found no effect of anxiety on overall measures of performance (<italic>t</italic><sub>11</sub> &lt; 0.05, p&gt;0.9), nor on the difference between THREAT+ (anger direct and fear averted) and THREAT- (anger averted and fear direct) combinations of gaze and emotion (<italic>F</italic><sub>1,22</sub> &lt; 0.4, p&gt;0.5).</p><p>Nevertheless, the absence of effect of anxiety at the behavioral level was accompanied by a compensatory double dissociation in the neural data. Indeed, state anxiety influenced significantly the neural encoding of emotion strength at temporal electrodes at the peak of neural encoding, 280 ms following face presentation (median split, interaction: F<sub>1,22</sub> = 7.3, p=0.01; <xref ref-type="fig" rid="fig8">Figure 8a</xref>): high-anxious observers showed no difference in neural encoding between THREAT+ and THREAT- combinations (THREAT+ : t<sub>11</sub> = -5.8, p&lt;0.001; THREAT-: t<sub>11</sub> = -6.1, p&lt;0.001, difference: t<sub>22</sub> = 0.84, p=0.4), whereas low-anxious observers encoded exclusively THREAT+ at the same latency (THREAT+: t<sub>11</sub> = -6.5, p&lt;0.001; THREAT-: t<sub>11</sub> = -1.8, p=0.08; difference: t<sub>22</sub> = -3.0, p=0.01). A parametric assessment of the relationship between state anxiety and the difference in neural encoding between THREAT+ and THREAT- combinations proved to be significant (Pearson correlation coefficient r = 0.51, d.f. = 22, p=0.01; <xref ref-type="fig" rid="fig8">Figure 8a</xref>). In other words, high anxiety was associated with a significant and indifferent neural encoding of negative emotions, whether threat-signaling or not, in ventral face-selective regions.<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.10274.010</object-id><label>Figure 8.</label><caption><title>Modulation of threat encoding by individual anxiety.</title><p>(<bold>a</bold>) Left panel: correlation (Pearson) between state anxiety and the difference of the encoding parameter estimates between THREAT+ and THREAT− conditions in temporal electrodes at 280 ms. Right panel: encoding parameter estimates in temporal electrodes split into high and low anxious individuals for both THREAT+ and THREAT− conditions at 280 ms. T+: THREAT+, T-: THREAT-. (<bold>b</bold>) Left, correlation (Pearson) between state anxiety and the encoding parameter estimates in motor lateralization signals for THREAT+ condition at 200 ms. Right, encoding parameter estimates in motor lateralization signals split into high and low anxious individuals for both THREAT+ and THREAT− conditions at 200 ms. ***: p&lt;0.001, *p&lt;0.05.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10274.010">http://dx.doi.org/10.7554/eLife.10274.010</ext-link></p></caption><graphic xlink:href="elife-10274-fig8-v1.tif"/></fig></p><p>Interestingly, at the early time window (peak of the encoding at 200 ms) where only THREAT+ combinations (anger direct and fear averted) were encoded in motor signals, a reverse pattern was observed: only high anxious individuals showed a significant encoding at this latency (interaction between between-subject state anxiety and gaze pairing <italic>F</italic><sub>1,22</sub> = 4, p=0.05; <xref ref-type="fig" rid="fig8">Figure 8b</xref>). The more the individuals were anxious, the more they encoded observer-relevant threat signals in motor systems (correlation between parameter estimates for THREAT+ conditions and state anxiety Pearson coefficient r = 0.52, d.f. = 22, p&lt;0.01; <xref ref-type="fig" rid="fig8">Figure 8b</xref>). Moreover, the neural encoding of THREAT+ emotions in motor signals correlated with behavioral sensitivity to THREAT+ emotions for high-anxious individuals (Pearson correlation coefficient r = 0.66, d.f. = 10, p=0.01), whereas it was not the case for low-anxious individuals (Pearson correlation coefficient r = −0.42, d.f. = 10, p&gt;0.16, difference between coefficients, p&lt;0.01). To sum up, while high anxious individuals process all threat signals equivalently in face selective regions, they selectively encode threat signals that are relevant to them in motor specific systems, and this encoding reflects their behavioral sensitivity to threat-signaling emotions.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Accurate decoding of emotions in others, especially negative ones, conveys adaptive advantages in social environments. Although typical social interactions do not require an explicit categorization of the emotion expressed by others, a precise understanding of the neural mechanisms involved in emotion recognition provides important information regarding how the human brain processes socially meaningful signals. And while past work has uncovered the neural correlates of perceptual decisions (<xref ref-type="bibr" rid="bib27">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib33">Heekeren et al., 2008</xref>), only few studies have addressed the issue of how such decisions are formed on the basis of socially relevant stimuli such as facial displays of emotion. As in most perceptual categorization tasks, we manipulated the ambiguity of sensory evidence – here, using controlled morphs between angry or fearful expressions and neutral ones. But owing to the social nature of our stimuli, we could simultaneously and implicitly manipulate the contextual significance of the displayed emotion in terms of implied threat for the observer, using gaze direction, and apply a model-guided approach to characterize the neural prioritization of threat-signaling information in electrical brain signals.</p><p>Gaze direction, which acts as a contextual cue in our emotion categorization task, differs from contextual cues found in perceptual decision-making studies which are typically provided hundreds of milliseconds before the decision-relevant stimulus (<xref ref-type="bibr" rid="bib56">Rahnev et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Wyart et al., 2012b</xref>; <xref ref-type="bibr" rid="bib19">de Lange et al., 2013</xref>). Here, as in many social situations, contextual cues can co-occur with the decision-relevant stimulus – a property which strongly constrains their impact on stimulus processing. Moreover, the meaning of contextual cues (e.g., attention or expectation cues) used in perceptual decision-making studies is usually instructed explicitly, and thus processed explicitly by the participants during task execution (<xref ref-type="bibr" rid="bib39">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Wyart et al., 2012b</xref>). Here, by contrast, gaze direction is irrelevant for the emotion categorization task, and thus does not need to be processed explicitly. Despite these two differences with other contextual cues, we show that gaze direction tunes the neural processing of emotion information from 200 ms following stimulus onset until response in sensory, associative and motor circuits of the human brain.</p><p>Previous observations of increased subjective ratings and improved recognition of angry expressions paired with a direct gaze and fearful expressions paired with an averted gaze have been interpreted in terms of a contextual evaluation of the displayed emotion during its processing (<xref ref-type="bibr" rid="bib2">Adams and Kleck, 2003</xref>; <xref ref-type="bibr" rid="bib58">Sander et al., 2007</xref>; <xref ref-type="bibr" rid="bib1">Adams et al., 2012</xref>). In particular, ‘appraisal’ theories (<xref ref-type="bibr" rid="bib58">Sander et al., 2007</xref>) emphasize that an angry expression paired with a direct gaze can be interpreted as behaviorally ‘relevant’ to the observer as being the target of a verbal or physical assault, whereas a fearful expression looking aside from the observer might signal a source of danger in the immediate vicinity of the observer. However, the mechanisms which instantiate the proposed contextual evaluation of emotions as a function of their implied threat for the observer have remained unclear. Gaze direction could either bias the perceived emotion towards its most relevant (threat-signaling) interpretation – i.e., anger when paired with direct gaze, or fear when paired with averted gaze, or increase the sensitivity to the most relevant emotion. The present study answers directly this issue by showing, both behaviorally (by comparing quantitative fits of the two effects to the behavioral data) and neurally (by regressing brain signals against emotion strength), that the improved recognition accuracy for threat-signaling emotions corresponds to a selective neural enhancement of perceptual sensitivity to these combinations of gaze and emotion.</p><p>Emotion information modulated EEG signals at centro-parietal electrodes from 500 ms following face presentation until response execution, a finding in accordance with the ‘supramodal’ signature of perceptual integration reported in previous studies (<xref ref-type="bibr" rid="bib1">O’Connell et al., 2012</xref>; <xref ref-type="bibr" rid="bib63">Wyart et al., 2012a</xref>). This centro-parietal positivity has been proposed to encode a ‘domain-general’ decision variable, as it varies with the strength of sensory evidence for both visual and auditory decisions, independently from the associated response (<xref ref-type="bibr" rid="bib1">O’Connell et al., 2012</xref>). Here, the same centro-parietal positivity was found to increase with the emotion strength of facial expressions – which indexes the decision variable in our emotion categorization task. Importantly, the strength of this relationship was enhanced for threat-signaling emotions. This improved neural representation of threatening combinations of gaze and emotion cannot be explained by increased attentional or surprise responses, since the centro-parietal ‘P3’ potential, previously reported to vary as a function of attentional resources (<xref ref-type="bibr" rid="bib35">Johnson, 1988</xref>) and surprise (<xref ref-type="bibr" rid="bib44">Mars et al., 2008</xref>), was not increased in response to threat-signaling emotions. Moreover, we could also rule out the possibility that this enhanced neural encoding is triggered indirectly by an increase in selective attention, which should have been associated with an improved ‘decoding’ of participants’ decisions from their underlying neural signals (<xref ref-type="bibr" rid="bib46">Nienborg and Cumming, 2009</xref>, <xref ref-type="bibr" rid="bib45">2010</xref>; <xref ref-type="bibr" rid="bib64">Wyart et al., 2015</xref>). We therefore hypothesize that the enhanced neural processing of threat-signaling emotions proceeds in an attention-independent, bottom-up fashion.</p><p>Earlier contextual modulations of emotion processing were also observed in ventral face-selective areas from 170 ms following face presentation. While these findings contradict a ‘two-stage’ view according to which emotion and gaze information would be processed independently during the first hundreds of millisecond (<xref ref-type="bibr" rid="bib55">Pourtois et al., 2010</xref>) before being integrated as a function of their significance to the observer (<xref ref-type="bibr" rid="bib38">Klucharev and Sams, 2004</xref>), they are in agreement with recent findings (<xref ref-type="bibr" rid="bib17">Conty et al., 2012</xref>; <xref ref-type="bibr" rid="bib23">El Zein et al., 2015</xref>) of early interactions between emotion and gaze information on N170 and P200 components. At these early latencies, only threat-signaling emotions were encoded by face-selective neural signals, reflecting a faster processing of emotions signaling an immediate threat to the observer as a function of their associated gaze.</p><p>More strikingly, gaze direction also modulated the encoding of emotional expressions in effector-selective regions, in parallel with the effects observed in ventral face-selective areas: only threat-signaling emotions were encoded in response preparation signals overlying human motor cortex at 200 ms following face presentation. Recent work sheds light on the adaptive function of this early representation of threat signals in motor cortex. Disrupting this motor representation using TMS impairs the facial recognition of negative (i.e., potentially threatening) emotions, not positive ones (<xref ref-type="bibr" rid="bib5">Balconi and Bortolotti, 2012</xref>; <xref ref-type="bibr" rid="bib6">2013</xref>). Moreover, the perception of natural scenes engages the motor cortex at very early latencies only when the emotional valence of the scene is negative (<xref ref-type="bibr" rid="bib12">Borgomaneri et al., 2014</xref>). Taken together, these findings support a strong connection between emotion and motor circuits (<xref ref-type="bibr" rid="bib32">Grèzes et al., 2014</xref>) enabling the brain to react swiftly and efficiently to threat signals (<xref ref-type="bibr" rid="bib49">Ohman and Mineka, 2001</xref>; <xref ref-type="bibr" rid="bib26">Frijda, 2009</xref>). Our findings build on these earlier observations by showing that the brain encodes parametrically the strength of threat signals in motor cortex in parallel to their representation in face-selective, sensory regions.</p><p>Finally, our data reveal a clear functional dissociation between face- and effector-selective regions as a function of individual anxiety. The enhanced sensitivity to threat-signaling emotions in face-selective temporal cortex is driven by low-anxious observers, whereas the early enhancement measured in motor cortex is only found in high-anxious observers. The observation that high-anxious individuals encode all negative emotions as equally (and strongly) salient in face-selective regions is consistent with earlier reports of a ‘hyper-vigilance’ to potentially threatening signals in these individuals (<xref ref-type="bibr" rid="bib11">Bishop, 2007</xref>; <xref ref-type="bibr" rid="bib16">Cisler and Koster, 2010</xref>), and with their tendency to interpret ambiguous stimuli as threatening (<xref ref-type="bibr" rid="bib9">Beck et al., 1985</xref>) – both associated with amygdala hyperactivity (<xref ref-type="bibr" rid="bib11">Bishop, 2007</xref>; <xref ref-type="bibr" rid="bib24">Etkin and Wager, 2007</xref>). Nevertheless, our findings reveal that high-anxious individuals are capable of encoding threat signals in a selective fashion in motor cortex. Consistent with the idea of a compensatory mechanism, the distinct neural enhancements of temporal and motor activity found in low- and high-anxious individuals lead to similar behavioral improvements in terms of perceptual sensitivity to threat signals. Together, this pattern of findings suggests that anxiety increases the relative contribution of the motor pathway during the processing of negative social signals, in accordance with the adaptive function of anxiety in detecting efficiently and reacting swiftly to threats in the environment (<xref ref-type="bibr" rid="bib8">Bateson et al., 2011</xref>). It is worth noting that the present study only involved participants with anxiety scores within the range of the healthy adult population (<xref ref-type="bibr" rid="bib61">Spielberger, 1983</xref>), leaving open the question as to whether clinically anxious individuals would similarly recruit their motor cortex in response to threatening social stimuli. Moreover, further research should assess the specificity of these anxiety-dependent effects, in light of the growing evidence in favor of comorbidity between anxiety and depressive disorders.</p><p>By applying theoretical models of decision-making to socially-relevant stimuli, we were able to characterize the neural and computational mechanisms underlying the integration and interpretation of facial cues in the implicit context of threat. Evolutionary pressure might have shaped the human brain to prioritize threat signals in parallel in sensory and motor systems (<xref ref-type="bibr" rid="bib18">Darwin, 1872</xref>; <xref ref-type="bibr" rid="bib1">LeDoux, 2012</xref>). Such prioritization – found to proceed in a fast, selective, yet attention-independent fashion – could increase perceptual sensitivity to other features of the sensory environment (<xref ref-type="bibr" rid="bib54">Phelps et al., 2006</xref>) to enable rapid and adaptive responses in complex, multidimensional situations of danger.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>Twenty-four healthy subjects (12 females; mean age, 22.7 ± 0.7 years) participated in the EEG experiment. All participants were right-handed, with a normal vision and had no neurological or psychiatric history. They provided written informed consent according to institutional guidelines of the local research ethics committee (Declaration of Helsinki) and were paid for their participation.</p></sec><sec id="s4-2"><title>Stimuli</title><p>Stimuli consisted of 36 identities (18 females) adapted from the Radboud Faces Database (<xref ref-type="bibr" rid="bib41">Langner et al., 2010</xref>) that varied in emotion (neutral, angry or fearful expressions) and gaze direction (direct toward the participant or averted 45° to the left or right). Using Adobe Photoshop CS5.1 (Adobe Systems, San Jose CA), faces were modified to remove any visible hair, resized and repositioned so that eyes, nose and mouth appeared within the same circumference. All images were converted to greyscale and cropped into a 280 x 406 pixel oval centered within a 628 x 429 pixel black rectangle.</p><p>To vary the intensity of emotional expressions, faces were morphed from neutral to angry expressions and from neutral to fearful expression using FantaMorph (Abrosoft http://www.fantamorph.com/). At first, we created 7 levels of morphs from neutral to angry expressions and from neutral to fearful expressions (separately for direct and averted gaze stimuli) using a simple linear morphing transformation. This resulted in 30 conditions for each identity: 7 levels of morphs * 2 emotions * 2 gaze directions = 28 and 2 neutral stimuli with direct and averted gaze. We then calibrated the morphing between angry and fearful expressions by performing an intensity rating pre-test of the emotional expressions and adjusting the morphs based on the results. 19 subjects (9 females, mean age, 24.7 ± 0.9 years) were presented with the facial expressions for 250 ms and rated the emotional intensity perceived on a continuous scale from “not at all intense” to “very intense” using a mouse device (with a maximum of 3 seconds to respond). We adjusted for differences between emotions by linearizing the mean curves of judged intensities and creating corresponding morphs that were validated on 10 new subjects (4 females, mean age 24.1 ± 1.9).To summarize, the stimuli comprise of 36 identities with an Averted gaze condition and a Direct gaze condition, each with 7 levels of Anger and 7 levels of Fear equalized in perceived emotional intensities and a neutral condition, resulting in a total of 1080 items (see <xref ref-type="fig" rid="fig2">Figure 2a</xref> for examples of stimuli).</p></sec><sec id="s4-3"><title>Experimental procedure</title><p>Using the Psychophysics-3 Toolbox (<xref ref-type="bibr" rid="bib13">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib52">Pelli, 1997</xref>), stimuli were projected on a black screen. Each trial was initiated with a white oval delimiting the faces that was kept during all the trial. The white oval appeared for approximately 500 ms, followed by a white fixation point presented at the level of the eyes for approximately 1000 ms (to keep the fixation to the upcoming faces natural and avoid eye movements from the center of the oval to eye regions), than the stimuli appeared for 250 ms. Participants’ task was to decide whether the faces expressed Anger or Fear by pressing one of the two buttons localized on two external devices held in their right and left hands, with their right or left index correspondingly (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). An Anger/Fear mapping was used (e.g Anger: Left hand, Fear: Right hand) kept constant for each subject, counterbalanced over all subjects. All stimuli were presented once, resulting in a total of 1080 trials. The experiment was divided in 9 experimental blocks, each consisting of 120 trials, balanced in the number of emotions, directions of gaze, gender and levels of morphs. After each block, the percentage of correct responses was shown to the participants to keep them motivated.</p></sec><sec id="s4-4"><title>Behavioral data analyses</title><p>Repeated-measures ANOVA was performed on the percentage of correct responses and average reaction times, with gaze direction (direct/averted), emotion (anger/fear), and intensity (7 levels of morphs) as within-subjects factors.</p></sec><sec id="s4-5"><title>Model selection</title><p>We performed model-guided analyses of the behavioural data to characterize the observed increase in recognition accuracy for THREAT+ combinations of gaze and emotion. We used Bayesian model selection based on the model evidence (estimated by a 10-fold cross-validation estimation of model log-likelihood, which penalizes implicitly for model complexity without relying on particular approximations such as the Bayesian Information Criterion or the Akaike Information Criterion). We applied both fixed-effects and random-effects statistics previously described in the literature. The fixed-effects comparison assumes all participants to have used the same underlying model to generate their behavior, such that the overall model evidence for a given model is proportional to the product of model evidence for the model for all participants. Based on this model evidence, we compared different models by computing their Bayes factor as the ratio of model evidence of the compared model (<xref ref-type="bibr" rid="bib34">Jeffreys, 1961</xref>; <xref ref-type="bibr" rid="bib36">Kass and Raftery, 1995</xref>). The random-effects comparison is more conservative in allowing different participants to use different models to generate their behavior, and aims at inferring the distribution over models that participants draw from (<xref ref-type="bibr" rid="bib53">Penny et al., 2010</xref>). For this comparison, we computed support for the winning model by the exceedance probability (<italic>p</italic><sub>exc</sub>), which is the probability that participants were more likely to choose this model to generate behavior over any alternative model.</p><p>We started with the simplest model (model 0) that could account for each subject’s decisions using a noisy, ‘signal detection’-like psychometric model to which we included a lapse rate, thereby considering that subjects guessed randomly on a certain proportion of trials:</p><p><disp-formula id="equ3"><mml:math id="m3"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mo>*</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>]</mml:mo><mml:mo>*</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ε</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>5</mml:mn><mml:mo>*</mml:mo><mml:mi>ε</mml:mi></mml:math></disp-formula></p><p>where P(anger) corresponds to the probability of judging the face as angry, Ф[.] to the cumulative normal function, w to the perceptual sensitivity to the displayed emotion, <bold>x</bold> to a trial-wise array of evidence values in favor of anger or fear (emotion strength, from −7 for an intense expression of fear to +7 for an intense expression of anger), b to an additive, stimulus-independent bias toward one of the two emotions, and ε to the proportion of random guesses among choices. We compared a ‘null’ model which did not allow for contextual influences of gaze direction on the decision process, to two additional models which instantiate two different mechanisms which could account for the observed increase in recognition accuracy for THREAT+ combinations of gaze and emotion. A first possibility (model 1) would be that gaze direction biases emotion recognition in favor of the interpretation signaling higher threat (anger for a direct gaze, fear for an averted gaze). Alternatively (model 2), gaze direction might selectively increase sensitivity to emotions signaling higher threat in this context (modeled by a different sensitivity to emotions in THREAT+ vs. THREAT− conditions).</p></sec><sec id="s4-6"><title>EEG acquisition and pre-processing</title><p>An EEG cap of 63 sintered Ag/AgCl ring electrodes (Easycap) was used to record EEG activity. EEG activity was recorded at a sampling rate of 1000 Hz using a BRAINAMP amplifier (Brain Products, BRAINAMP MR PLUS) and low pass filtered online at 250 Hz. The reference channel was placed on their nose and a forehead ground was used. Impedances were kept under a threshold of 10 kΩ.</p><p>The raw EEG data was recalculated to average reference, down-sampled to 500 Hz, low-pass filtered at 32 Hz, and epoched from 1 s before to 4 s after the face stimulus onset using EEGLAB (<xref ref-type="bibr" rid="bib20">Delorme and Makeig, 2004</xref>). First, EEG epoched data was visually inspected to remove muscle artifacts and to identify noisy electrodes that were interpolated to the average of adjacent electrodes. Second, independent component analysis (ICA) that excluded interpolated electrodes was performed on the epoched data and ICA components capturing eye blink artifacts were manually rejected. A last, visual inspection was done on the resulting single epochs to exclude any remaining trials with artifacts. After trial rejections, an average of 999 ± 10 trials per subject remained.</p><p>Time frequency analysis was performed using the Fieldtrip toolbox for MATLAB (<xref ref-type="bibr" rid="bib50">Oostenveld et al., 2011</xref>). We were particularly interested in motor mu-bands (8–32 Hz) and thus estimated the spectral power of mu-beta band EEG oscillations using ‘multitapering’ time frequency transform (Slepian tapers, frequency range 8–32 Hz, five cycles, three tapers per window). The purpose of this multitapering approach is to obtain more precise power estimates by smoothing across frequencies. Note that this time–frequency transform uses a constant number of cycles per window across frequencies, hence a time window whose duration decreases inversely with increasing frequency.</p></sec><sec id="s4-7"><title>EEG analyses</title><sec id="s4-7-1"><title>Time frequency: motor lateralization measures</title><p>As the suppression of mu-beta activity in the hemisphere contralateral to the hand used for response is a marker of motor preparation to response (<xref ref-type="bibr" rid="bib22">Donner et al., 2009</xref>; <xref ref-type="bibr" rid="bib19">de Lange et al., 2013</xref>), spectral power from 8 to 32 Hz were calculated at each electrode and time point for all subjects. Then for each subject, to obtain the lateralization measures, the spectral power from 8 to 32 Hz for the trials where the subjects responded with their right hand was subtracted from that of the trials where the subjects responded with their left hand. After averaging on all subjects, electrodes where the motor lateralization was maximal from 200 ms before to response time were selected: 'P3,'CP3','C3' for the left hemisphere and 'P4,'CP4','C4' for the right hemisphere. Motor lateralization specific to ‘anger’ or ‘fear’ responses was obtained by taking into account the Anger/Fear mapping used and subtracting ‘Anger’ hand spectral activity to ‘Fear’ hand spectral activity (the average on 'P3,'CP3','C3' minus the average on 'P4,'CP4','C4' if participants responded ‘Anger’ with the left hand and vis versa if they responded ‘Anger’ with the right hand).</p></sec><sec id="s4-7-2"><title>Regression analysis: encoding of the emotional information</title><p>In our emotion categorization task, evidence strength corresponds to the intensity of the displayed emotion. On the basis of recent studies (<xref ref-type="bibr" rid="bib65">Wyart et al., 2012b</xref>, <xref ref-type="bibr" rid="bib64">2015</xref>), we therefore performed single-trial regressions of EEG signals against this variable. A general linear regression model (GLM) was used where emotion strength (from 0 for a neutral/emotionless expression to 7 for an intense fear/anger expression) was introduced as a trial-per-trial predictor of broadband EEG signals at each time point after stimulus onset (from 200 ms before to 1 s after stimulus onset), at each electrode. The corresponding parameter estimates of the regression, reported in arbitrary units, were measured per participant, and then averaged across participants to produced group-level averages. The time course of the parameter estimates describes the neural ‘encoding’ of the relevant (emotion) information provided by the presented facial expression. Electrodes and time points where the parameter estimates of the regression were maximal were selected to further compare between the conditions of interest: Anger Direct and Fear Averted vs Fear Direct and Anger Averted.</p><p>Similar general linear regressions were also performed on lateralized mu-beta activity. Once more, the intensity of the emotional expression was entered as a regressor to predict the trial-per-trial motor lateralization activity (calculated as described above) for each time point after stimulus onset. The only important difference is that owing to the ‘signed’ nature of the motor lateralization index (positive for a contra-lateralized activity), we expressed the intensity of the emotional expression as signed by the displayed emotion, from -7 for an intense expression of fear to +7 for an intense expression of anger.</p></sec><sec id="s4-7-3"><title>Neural-choice correlation analyses</title><p>We determined whether residual fluctuations in single-trial EEG signals unexplained by variations in emotion strength (measured by the previous neural regressions against emotion strength) modulated the recognition of the subsequent emotion. This approach is reminiscent of ‘choice probability’ measures applied in electrophysiology to measure correlations between neural activity and choice behavior (<xref ref-type="bibr" rid="bib14">Britten et al., 1996</xref>; <xref ref-type="bibr" rid="bib60">Shadlen et al., 1996</xref>; <xref ref-type="bibr" rid="bib51">Parker and Newsome, 1998</xref>) – by estimating how much fluctuations in recorded neural signals are ‘read out’ by the subsequent decision (<xref ref-type="bibr" rid="bib63">Wyart et al., 2012a</xref>, <xref ref-type="bibr" rid="bib64">2015</xref>). The advantage of measuring neural-choice correlations within the framework of our computational model is that we could not only establish <italic>whether</italic>, but also <italic>how</italic> neural fluctuations influenced the subsequent behavior – either additively as a stimulus-independent bias, or multiplicatively as a change in perceptual sensitivity.</p><p>In practice, we estimated the parameters <italic>b</italic><sub>mod</sub> and <italic>w</italic><sub>mod</sub> of these neural modulation terms at each time point following face presentation via an EEG-informed regression of choice for which the neural residuals <italic>e</italic> from the regression against emotion strength were entered either alone (additive influence, parameter <italic>b</italic><sub>mod</sub>, model 1) or as their interaction with the strength of the displayed emotion (multiplicative influence, parameter <italic>w</italic><sub>mod</sub>, model 2) as an additional predictor of the subsequent categorical choice, as follows:</p><p><disp-formula id="equ4"><mml:math id="m4"><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>·</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula></p><p><disp-formula id="equ5"><mml:math id="m5"><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>·</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mi>e</mml:mi><mml:mo>·</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>We applied Bayesian model selection to compare between these two possible modulations of the decision process by neural fluctuations using both fixed-effects and random-effects statistical procedures described above.</p></sec><sec id="s4-7-4"><title>EEG statistical procedures</title><p>All regression-based analyses of the EEG data were performed independently for each subject, and then followed by a second-level analysis at the group level to assess the significance of the observed effects across participants. Second-level analyses relied on standard parametric tests (t-tests, repeated-measures analyses of variance), with explicit control over the type-1 error rate arising from multiple comparisons across time points through non-parametric cluster-level statistics as described in (<xref ref-type="bibr" rid="bib50">Maris and Oostenveld, 2007</xref>). The pairing between experimental conditions and EEG signals was shuffled pseudo-randomly 1,000 times, and the maximal cluster-level statistics (the sum of t-values across contiguously significant time points at a threshold level of 0.05) were extracted for each shuffle to compute a ‘null’ distribution of effect size across a time window of [-200,+1000] ms around stimulus presentation, or [-1000,+200] around response onset. For each significant cluster in the original (non-shuffled) data, we computed the proportion of clusters in the null distribution whose statistics exceeded the one obtained for the cluster in question, corresponding to its ‘cluster-corrected’ p-value. We applied a second bootstrapping method to test for significant shifts in neural encoding latencies between conditions, using the ‘jackknifing’ procedure described in (<xref ref-type="bibr" rid="bib37">Kiesel et al., 2008</xref>).</p><p>Bayes factors were computed for critical absence of effects observed, to distinguish between the lack of sensitivity of tests and genuine absence of difference (<xref ref-type="bibr" rid="bib21">Dienes, 2011</xref>). A group-level random-effects Bayes factor was computed under the same assumptions as a standard <italic>t</italic> test that states that the distribution of the observed effect across individuals can be approximated by a normal distribution of the mean (µ) and standard deviation (σ). We computed the maximum log-likelihood of the model in favor of the “null” hypothesis, which assumes that µ=0 and the model in favor of the “effect” hypothesis, for which both µ and σ can be adjusted freely to the observed data. We then used the Bayesian information criterion to compare the two models and compute the corresponding Bayes factor. A Bayes factor below 1/3 provides substantial evidence in favor of the null hypothesis whereas a Bayes factor &gt; 3 provides in favor of the effect hypothesis.</p><p>We performed control analyses to confirm the robustness of the anxiety-threat correlation across individuals observed in temporal and motor regions. For this purpose, we performed a leave-one-out, cross-validation procedure in which we computed ‘cross-validated’ prediction intervals for the group-level regression line at the anxiety score of participant #n when the data for participant #n was excluded from the group-level regression. This procedure was repeated for the 24 participants for the anxiety-threat correlation in motor areas and showed that the neural effects of two participants fell slightly (&lt; 20%) outside of the cross-validated 95% prediction intervals. Recomputing the group-level correlation in motor areas after excluding the two outliers, leaving 22 participants in the analysis, led to a significant effect (<italic>r</italic> = 0.48, d.f. = 20, p=0.02). This leave-one-out analysis identified no outlier for the anxiety-threat correlation in temporal regions.</p></sec><sec id="s4-7-5"><title>Source reconstruction analysis</title><p>Source analysis was performed using Brainstorm (<xref ref-type="bibr" rid="bib62">Tadel et al., 2011</xref>). A source model consisting of 15,002 current dipoles was used to calculate Kernel inversion matrices for each subject based on all the trials of the subject. Dipole orientations were constrained to the cortical mantle of a generic brain model taken from the standard Montreal Neurological institute (MNI) template brain provided in brainstorm. Individual scalp models, recorded with a Zebris device, were used to warp this template head model to EEG sensor caps. Using the OpenMEEG BEM model (<xref ref-type="bibr" rid="bib40">Kybic et al., 2005</xref>; <xref ref-type="bibr" rid="bib29">Gramfort et al., 2010</xref>), the forward EEG model was computed for each subject. Individual inversion matrices (15002 vertices * 63 electrodes) were then extracted to perform single trial regressions at the source level.</p></sec><sec id="s4-7-6"><title>Threat and trustworthiness rating experiment</title><p>20 subjects participated to the experiment (10 females, mean age = 22.7 ± 0.6). The 36 identities used in the experiment were presented in the neutral condition only. Each identity was presented twice, once with a direct, and once with an averted gaze. Faces appeared on the screen for 2 seconds after which they disappeared and 2 continuous scales were drawn on the screen. Participants rated the identities on these scales in terms of threat and trustworthiness from “not at all” to “very much” (a text appeared at the top of the scales reminding the instructions: How much is this face threatening/trustworthy?). The order of the scales was randomized across subjects. The scales stayed on the screen until the two responses were given, however subjects were instructed to answer intuitively without spending too much time to decide.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by grants from the French National Research Agency ANR-11-EMCO-00902, ANR-11-0001-02 PSL*, ANR-10-LABX-0087, the Fondation ROGER DE SPOELBERCH and by INSERM. We wish to thank Laurent Hugueville for his useful technical help.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>MEZ, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>VW, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>JG, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants provided written informed consent according to institutional guidelines of the local research ethics committee (following the Declaration of Helsinki) and were paid for their participation. In this consent, they agreed that their data would be stored anonymously, analyzed, and that the corresponding results would be presented in conferences and published in peer-reviewed journals. Ethical approval was provided by the local ethics committee (Comite de Protection des Personnes, Ile-de-France VI, Inserm approval #C07-28, DGS approval #2007-0569, IDRCB approval #2007-A01125-48), in accordance with article L1121-1 of the French Public Health Code.</p> </fn> </fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>RB</given-names></name><name><surname>Franklin</surname><given-names>RG</given-names></name><name><surname>Kveraga</surname><given-names>K</given-names></name><name><surname>Ambady</surname><given-names>N</given-names></name><name><surname>Kleck</surname><given-names>RE</given-names></name><name><surname>Whalen</surname><given-names>PJ</given-names></name><name><surname>Hadjikhani</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Amygdala responses to averted vs direct gaze fear vary as a function of presentation speed</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>7</volume><fpage>568</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1093/scan/nsr038</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>RB</given-names></name><name><surname>Kleck</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Perceived gaze direction and the processing of facial displays of emotion</article-title><source>Psychological Science</source><volume>14</volume><fpage>644</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1046/j.0956-7976.2003.psci_1479.x</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>RB</given-names></name><name><surname>Kleck</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Effects of direct and averted gaze on the perception of facially communicated emotion</article-title><source>Emotion</source><volume>5</volume><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1037/1528-3542.5.1.3</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aviezer</surname><given-names>H</given-names></name><name><surname>Bentin</surname><given-names>S</given-names></name><name><surname>Dudarev</surname><given-names>V</given-names></name><name><surname>Hassin</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The automaticity of emotional face-context integration</article-title><source>Emotion</source><volume>11</volume><fpage>1406</fpage><lpage>1414</lpage><pub-id pub-id-type="doi">10.1037/a0023578</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balconi</surname><given-names>M</given-names></name><name><surname>Bortolotti</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Detection of the facial expression of emotion and self-report measures in empathic situations are influenced by sensorimotor circuit inhibition by low-frequency rTMS</article-title><source>Brain Stimulation</source><volume>5</volume><fpage>330</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1016/j.brs.2011.05.004</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balconi</surname><given-names>M</given-names></name><name><surname>Bortolotti</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The &quot;simulation&quot; of the facial expression of emotions in case of short and long stimulus duration. the effect of pre-motor cortex inhibition by rTMS</article-title><source>Brain and Cognition</source><volume>83</volume><fpage>114</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2013.07.003</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>LF</given-names></name><name><surname>Kensinger</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Context is routinely encoded during emotion perception</article-title><source>Psychological Science</source><volume>21</volume><fpage>595</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1177/0956797610363547</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bateson</surname><given-names>M</given-names></name><name><surname>Brilot</surname><given-names>B</given-names></name><name><surname>Nettle</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Anxiety: an evolutionary approach</article-title><source>Canadian Journal of Psychiatry</source><volume>56</volume><fpage>707</fpage><lpage>715</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>AT</given-names></name><name><surname>Emery</surname><given-names>G</given-names></name><name><surname>Greenberg</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="1985">1985</year><source>Anxiety Disorders and Phobias: A Cognitive Perspective</source><publisher-loc>New York</publisher-loc><publisher-name>Basic Books</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bindemann</surname><given-names>M</given-names></name><name><surname>Mike Burton</surname><given-names>A</given-names></name><name><surname>Langton</surname><given-names>SRH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>How do eye gaze and facial expression interact?</article-title><source>Visual Cognition</source><volume>16</volume><fpage>708</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1080/13506280701269318</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neurocognitive mechanisms of anxiety: an integrative account</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>307</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.05.008</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borgomaneri</surname><given-names>S</given-names></name><name><surname>Gazzola</surname><given-names>V</given-names></name><name><surname>Avenanti</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal dynamics of motor cortex excitability during perception of natural emotional scenes</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>9</volume><fpage>1451</fpage><lpage>1457</lpage><pub-id pub-id-type="doi">10.1093/scan/nst139</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Celebrini</surname><given-names>S</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A relationship between behavioral choice and the visual responses of neurons in macaque MT</article-title><source>Visual Neuroscience</source><volume>13</volume><fpage>87</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1017/S095252380000715X</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bruchon-Schweitzer</surname><given-names>M</given-names></name><name><surname>Paulhan</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1993">1993</year><source>Le Manuel Du STAI-Y De CD Spielberger, Adaptation Française</source><publisher-loc>Paris</publisher-loc><publisher-name>ECPA</publisher-name></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisler</surname><given-names>JM</given-names></name><name><surname>Koster</surname><given-names>EH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Mechanisms of attentional biases towards threat in anxiety disorders: an integrative review</article-title><source>Clinical Psychology Review</source><volume>30</volume><fpage>203</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1016/j.cpr.2009.11.003</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conty</surname><given-names>L</given-names></name><name><surname>Dezecache</surname><given-names>G</given-names></name><name><surname>Hugueville</surname><given-names>L</given-names></name><name><surname>Grèzes</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Early binding of gaze, gesture, and emotion: neural time course and correlates</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>4531</fpage><lpage>4539</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5636-11.2012</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Darwin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1872">1872</year><source>The Expression of the Emotions in Man and Animals</source><edition>1st edition</edition><publisher-loc>London</publisher-loc><publisher-name>John Murray</publisher-name><pub-id pub-id-type="doi">10.1037/10001-000</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Rahnev</surname><given-names>DA</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Lau</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Prestimulus oscillatory activity over motor cortex reflects perceptual expectations</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>1400</fpage><lpage>1410</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1094-12.2013</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dienes</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Bayesian versus orthodox statistics: which side are you on?</article-title><source>Perspectives on Psychological Science</source><volume>6</volume><fpage>274</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1177/1745691611406920</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Buildup of choice-predictive activity in human motor cortex during perceptual decision making</article-title><source>Current Biology</source><volume>19</volume><fpage>1581</fpage><lpage>1585</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.07.066</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El Zein</surname><given-names>M</given-names></name><name><surname>Gamond</surname><given-names>L</given-names></name><name><surname>Conty</surname><given-names>L</given-names></name><name><surname>Grèzes</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Selective attention effects on early integration of social signals: same timing, modulated neural sources</article-title><source>NeuroImage</source><volume>106</volume><fpage>182</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.10.063</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etkin</surname><given-names>A</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Functional neuroimaging of anxiety: a meta-analysis of emotional processing in PTSD, social anxiety disorder, and specific phobia</article-title><source>The American Journal of Psychiatry</source><volume>164</volume><fpage>1476</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1176/appi.ajp.2007.07030504</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ewbank</surname><given-names>MP</given-names></name><name><surname>Fox</surname><given-names>E</given-names></name><name><surname>Calder</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The interaction between gaze and facial expression in the amygdala and extended amygdala is modulated by anxiety</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><fpage>56</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2010.00056</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frijda</surname><given-names>NH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Emotion experience and its varieties</article-title><source>Emotion Review</source><volume>1</volume><fpage>264</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1177/1754073909103595</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>R</given-names></name><name><surname>LaBar</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Garner interference reveals dependencies between emotional expression and gaze in face perception</article-title><source>Emotion</source><volume>7</volume><fpage>296</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1037/1528-3542.7.2.296</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Papadopoulo</surname><given-names>T</given-names></name><name><surname>Olivi</surname><given-names>E</given-names></name><name><surname>Clerc</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>OpenMEEG: opensource software for quasistatic bioelectromagnetics</article-title><source>Biomedical Engineering Online</source><volume>9</volume><fpage>45</fpage><pub-id pub-id-type="doi">10.1186/1475-925X-9-45</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Green</surname><given-names>DM</given-names></name><name><surname>Swets</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1966">1966</year><source>Signal Detection Theory and Psychophysics</source><publisher-loc>New York</publisher-loc><publisher-name>John Wiley and Sons</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grupe</surname><given-names>DW</given-names></name><name><surname>Nitschke</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Uncertainty and anticipation in anxiety: an integrated neurobiological and psychological perspective</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>488</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1038/nrn3524</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grèzes</surname><given-names>J</given-names></name><name><surname>Valabrègue</surname><given-names>R</given-names></name><name><surname>Gholipour</surname><given-names>B</given-names></name><name><surname>Chevallier</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A direct amygdala-motor pathway for emotional displays to influence action: a diffusion tensor imaging study</article-title><source>Human Brain Mapping</source><volume>35</volume><fpage>5974</fpage><lpage>5983</lpage><pub-id pub-id-type="doi">10.1002/hbm.22598</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heekeren</surname><given-names>HR</given-names></name><name><surname>Marrett</surname><given-names>S</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The neural systems that mediate human perceptual decision making</article-title><source>Nature Reviews. Neuroscience</source><volume>9</volume><fpage>467</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1038/nrn2374</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeffreys</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1961">1961</year><source>Theory of Probability</source><publisher-loc>Oxford, UK</publisher-loc><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1988">1988</year><chapter-title>The amplitude of the P300 component of the event-related potential: Review and synthesis</chapter-title><person-group person-group-type="editor"><name><surname>Ackles</surname> <given-names>P. K</given-names></name><name><surname>Jennings</surname> <given-names>J. R</given-names></name><name><surname>Coles</surname> <given-names>M. G. H</given-names></name></person-group><source>Advances in Psychophysiology</source><publisher-loc>Greenwich, CT</publisher-loc><publisher-name>JAI Press</publisher-name><fpage>693</fpage><lpage>137</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kass</surname><given-names>RE</given-names></name><name><surname>Raftery</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Bayes factors</article-title><source>Journal of the American Statistical Association</source><volume>90</volume><fpage>773</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1080/01621459.1995.10476572</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiesel</surname><given-names>A</given-names></name><name><surname>Miller</surname><given-names>J</given-names></name><name><surname>Jolicoeur</surname><given-names>P</given-names></name><name><surname>Brisson</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Measurement of ERP latency differences: a comparison of single-participant and jackknife-based scoring methods</article-title><source>Psychophysiology</source><volume>45</volume><fpage>250</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2007.00618.x</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klucharev</surname><given-names>V</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Interaction of gaze direction and facial expressions processing: ERP study</article-title><source>NeuroReport</source><volume>15</volume><fpage>621</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1097/00001756-200403220-00010</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Jehee</surname><given-names>JF</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kybic</surname><given-names>J</given-names></name><name><surname>Clerc</surname><given-names>M</given-names></name><name><surname>Abboud</surname><given-names>T</given-names></name><name><surname>Faugeras</surname><given-names>O</given-names></name><name><surname>Keriven</surname><given-names>R</given-names></name><name><surname>Papadopoulo</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A common formalism for the integral formulations of the forward EEG problem</article-title><source>IEEE Transactions on Medical Imaging</source><volume>24</volume><fpage>12</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1109/TMI.2004.837363</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langner</surname><given-names>O</given-names></name><name><surname>Dotsch</surname><given-names>R</given-names></name><name><surname>Bijlstra</surname><given-names>G</given-names></name><name><surname>Wigboldus</surname><given-names>DHJ</given-names></name><name><surname>Hawk</surname><given-names>ST</given-names></name><name><surname>van Knippenberg</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Presentation and validation of the radboud faces database</article-title><source>Cognition &amp; Emotion</source><volume>24</volume><fpage>1377</fpage><lpage>1388</lpage><pub-id pub-id-type="doi">10.1080/02699930903485076</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeDoux</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rethinking the emotional brain</article-title><source>Neuron</source><volume>73</volume><fpage>653</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.02.004</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Macmillan</surname><given-names>NA</given-names></name><name><surname>Creelman</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Detection Theory: A User’s Guide</source><publisher-loc>New York</publisher-loc><publisher-name>Psychology Press</publisher-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Debener</surname><given-names>S</given-names></name><name><surname>Gladwin</surname><given-names>TE</given-names></name><name><surname>Harrison</surname><given-names>LM</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name><name><surname>Rothwell</surname><given-names>JC</given-names></name><name><surname>Bestmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Trial-by-trial fluctuations in the event-related electroencephalogram reflect dynamic changes in the degree of surprise</article-title><source>The Journal of Neuroscience </source><volume>28</volume><fpage>12539</fpage><lpage>12545</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2925-08.2008</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nienborg</surname><given-names>H</given-names></name><name><surname>Cumming</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Correlations between the activity of sensory neurons and behavior: how much do they tell us about a neuron's causality?</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>376</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.05.002</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nienborg</surname><given-names>H</given-names></name><name><surname>Cumming</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decision-related activity in sensory neurons reflects more than a neuron's causal effect</article-title><source>Nature</source><volume>459</volume><fpage>89</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1038/nature07821</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>N’Diaye</surname><given-names>K</given-names></name><name><surname>Sander</surname><given-names>D</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Self-relevance processing in the human amygdala: gaze direction, facial expression, and emotion intensity</article-title><source>Emotion</source><volume>9</volume><fpage>798</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1037/a0017845</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Connell</surname><given-names>RG</given-names></name><name><surname>Dockree</surname><given-names>PM</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A supramodal accumulation-to-bound signal that determines perceptual decisions in humans</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1729</fpage><lpage>1735</lpage><pub-id pub-id-type="doi">10.1038/nn.3248</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Öhman</surname><given-names>A</given-names></name><name><surname>Mineka</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Fears, phobias, and preparedness: toward an evolved module of fear and fear learning</article-title><source>Psychological Review</source><volume>108</volume><fpage>483</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.108.3.483</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>156869</fpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>AJ</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Sense and the single neuron: probing the physiology of perception</article-title><source>Annual Review of Neuroscience</source><volume>21</volume><fpage>227</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.21.1.227</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penny</surname><given-names>WD</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Rosa</surname><given-names>MJ</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Schofield</surname><given-names>TM</given-names></name><name><surname>Leff</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Comparing families of dynamic causal models</article-title><source>PLoS Computational Biology</source><volume>6</volume><elocation-id>e1000709</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000709</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phelps</surname><given-names>EA</given-names></name><name><surname>Ling</surname><given-names>S</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Emotion facilitates perception and potentiates the perceptual benefits of attention</article-title><source>Psychological Science</source><volume>17</volume><fpage>292</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2006.01701.x</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pourtois</surname><given-names>G</given-names></name><name><surname>Spinelli</surname><given-names>L</given-names></name><name><surname>Seeck</surname><given-names>M</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of face processing by emotional expression and gaze direction during intracranial recordings in right fusiform cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>2086</fpage><lpage>2107</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21404</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahnev</surname><given-names>D</given-names></name><name><surname>Lau</surname><given-names>H</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prior expectation modulates the interaction between sensory and prefrontal regions in the human brain</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>10741</fpage><lpage>10748</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1478-11.2011</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Righart R</surname></name><name><surname>de Gelder B</surname></name></person-group><year iso-8601-date="2008">2008</year><article-title>Recognition of facial expressions is influenced by emotional scene gist</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>8</volume><fpage>264</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.3758/CABN.8.3.264</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sander</surname><given-names>D</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>S</given-names></name><name><surname>Wehrle</surname><given-names>T</given-names></name><name><surname>Scherer</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Interaction effects of perceived gaze direction and dynamic facial expression: evidence for appraisal theories of emotion</article-title><source>European Journal of Cognitive Psychology</source><volume>19</volume><fpage>470</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1080/09541440600757426</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>W</given-names></name><name><surname>Yoshikawa</surname><given-names>S</given-names></name><name><surname>Kochiyama</surname><given-names>T</given-names></name><name><surname>Matsumura</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The amygdala processes the emotional significance of facial expressions: an fMRI investigation using the interaction between expression and face direction</article-title><source>NeuroImage</source><volume>22</volume><fpage>1006</fpage><lpage>1013</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.02.030</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A computational analysis of the relationship between neuronal and behavioral responses to visual motion</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>1486</fpage><lpage>1510</lpage></element-citation></ref><ref id="bib61"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Spielberger</surname><given-names>CD</given-names></name><name><surname>Gorsuch</surname><given-names>RL</given-names></name><name><surname>Lushene</surname><given-names>R</given-names></name><name><surname>Vagg</surname><given-names>PR</given-names></name><name><surname>Jacobs</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="1983">1983</year><source>Manual for the State-Trait Anxiety Inventory</source><publisher-loc>Palo Alto, CA</publisher-loc><publisher-name>Consulting Psychologists Press</publisher-name></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadel</surname><given-names>F</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Mosher</surname><given-names>JC</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Leahy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Brainstorm: a user-friendly application for MEG/EEG analysis</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>879716</fpage><pub-id pub-id-type="doi">10.1155/2011/879716</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>de Gardelle</surname><given-names>V</given-names></name><name><surname>Scholl</surname><given-names>J</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rhythmic fluctuations in evidence accumulation during decision making in the human brain</article-title><source>Neuron</source><volume>76</volume><fpage>847</fpage><lpage>858</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.015</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Myers</surname><given-names>NE</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural mechanisms of human perceptual choice under focused and divided attention</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>3485</fpage><lpage>3498</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3276-14.2015</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Dissociable prior influences of signal probability and relevance on visual contrast sensitivity</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>109</volume><fpage>3593</fpage><lpage>3598</lpage><pub-id pub-id-type="doi">10.1073/pnas.1120118109</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.10274.011</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mason</surname><given-names>Peggy</given-names></name><role>Reviewing editor</role><aff id="aff4"><institution>University of Chicago</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for submitting your work entitled &quot;Anxiety dissociates the adaptive functions of sensory and motor response enhancements to social threats&quot; for peer review at <italic>eLife</italic>. Your submission has been favorably evaluated by Timothy Behrens (Senior editor) and three reviewers, one of whom, Peggy Mason, is a member of our Board of Reviewing Editors.</p><p>The reviewers have discussed the reviews with one another and the Reviewing editor has drafted this decision to help you prepare a revised submission.</p><p>The interaction of emotional expression with gaze direction dependent on gain sensitivity is elegant and well done, and novel. The anxiety dependent analysis and related findings are intriguing. Addressing the following issues would make this a valuable contribution.</p><p>Most importantly, the Methods should be incorporated into the Results. The Methods are too integral to the understanding of the findings to be presented separately. Please write the manuscript to be understandable to the general reader. Clarity is more important than standard format and all reviewers agreed that methodological details should be interwoven into the Results presentation.</p><p>Can the authors justify not considering participant response in their EEG analysis? Or possibly analyzing the data in the standard way – by stimulus – and then additionally by classification response?</p><p>There is concern that one outlier is driving the anxiety-threat correlation for motor areas. Could the authors please address this concern?</p><p>Please explicitly state how multiple comparisons were accounted for.</p><p>Were pts screened for social anxiety or depression?</p><p>The above points are the major ones that drove the reviewers' decisions. Complete reviews are presented below.</p><p><italic>Reviewer #1:</italic> </p><p>This is an interesting study with an even more interesting answer once the anxiety bit is thrown in. I don't have many comments in part because the EEG methods are opaque to me. My one major concern is whether the middle portion of the Results – concerning all subjects together – has value. It appears that the Results are in fact driven by two different strongly significant results in two different subject cohorts – high and low anxiety. So the overall increased sensitivity to threat+ is constructed of a low anxiety - perception and high anxiety = motor pieces.</p><p><italic>Reviewer #2:</italic> </p><p>In general I think this is a nice piece of work and the authors have clearly gone to considerable effort to provide a full account of their data set and are to be applauded for the extent of the analysis conducted to better understand the neural response to threatening social stimuli. However, I found some of the analysis difficult to parse and in some cases important information was missing (or not obvious).</p><p>Particular points that bothered me were the selective presentation of the behavioural results. Where are the reaction time results (as mentioned to have been analysed in the Methods)? Was there no interaction between the categorisation performance and level of morph when exploring the eye gaze-emotion performance interaction? The plots in <xref ref-type="fig" rid="fig2">Figure 2B</xref> suggest that as one might expect, the effect of eye gaze is stronger for the weaker emotion signals (closer to neutral). As a more minor point, why are participants not 100% correct for the end points of the morphs (100% anger, 100% fear) – are some identities consistently categorised incorrectly?</p><p>Turning to the EEG analysis. The authors employ advanced and relatively specialised methods to explore the relationship of the EEG signal to the modulations in emotional strength and their results are interesting and appear strong. While I think they have done a reasonable job in explaining these methods and how they should be interpreted, I would have liked more detail on the statistical tests they employed. Surely they performed some bootstrapping to correct for the multiple comparisons issues they face. This is hinted at in the caption to <xref ref-type="fig" rid="fig3">Figure 3</xref> (cluster corrected threshold) but no detail is provided anywhere that I can see.</p><p>Furthermore, I think the authors used all trials (correct and incorrect) in their EEG analysis but it’s unclear to me why this would be justified, particularly for the weaker emotional stimuli (where participants are only correct about 70% of the time in categorising them). For example a threat+ fear averted trial could have been categorised by the participant as an anger face (and thus treated by the participant as threat-). Would the authors expect the neural encoding to be the same in both cases? If not, how can they justify not considering participant response in their EEG analysis?</p><p>Finally I enjoyed the anxiety dependent analysis and the findings are clear and interesting.</p><p><italic>Reviewer #3:</italic> </p><p>Wyart et al. examine the interaction of emotional expression and gaze direction and test two models: multiplicative threat enhancement of emotion perception, versus additive response bias. The two models are explained clearly with reference to the behavioural data, and thereafter the authors test these models against single trial neural data. The authors report the time course of sensitivity to high versus low threat conditions. They also test bias versus sensitivity models, using the residuals from the regression analysis, at both temporal partial electrodes, and find an enhancement in perceptual sensitivity and also against test the models against a motor lateralization index, and find a bias effect.</p><p>This part of the paper is elegant and well done, and to my knowledge, novel. The Methods section is really clear – my preference would be to see some of the explanation of the models moved into the main part of the paper, as this part, while 'technical', is really needed to understand the meaning of the subsequent analysis, and as it stands, the introduction is much less clear than the longer explanation.</p><p>The final section of the paper claims a relationship with anxiety levels in individual subjects, as threat+ and - are differently modulate temporal areas in low anxious individuals), and threat+ modulates motor areas in high anxious individuals.</p><p>I have reservations about the second part of the dissociation in high anxious individuals mostly to do with <xref ref-type="fig" rid="fig7">Figure 7B</xref> – it seems to me to be the main weakness.</p><p>1) Anxiety is co-morbid with depression, and social anxiety. Were participants screened for their scores with other factors that might be correlated with anxiety?</p><p>2) All participants bar one lie in the very centre of the normal range. In <xref ref-type="fig" rid="fig7">Figure 7B</xref>, it looks like that 40+ score might be an outlier. R values are computed with Pearson (I assume, though I notice that isn't on the figure; it should be). If the outlying subject is excluded, does the correlation survive?</p><p>Following on from that, it seems to me that there is an increase in variabilityin the 'more anxious' group, which could be driving the correlation, especially as it looks as though the difference between threat and no-threat in the high anxious group in 7B isn't significant.</p><p>3) The effect in <xref ref-type="fig" rid="fig7">Figure 7B</xref> seems to be a weak effect at a single time point. There is no justification/explanation of the selection of either the window in 7A, or the single point in 7B (if that is what it is).</p><p>4) The role of reaction time: the authors say that they found no difference in the behavioural responses between the more and less anxious groups. Did that include reaction times?</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Anxiety dissociates the adaptive functions of sensory and motor response enhancements to social threats&quot; for peer review at <italic>eLife</italic>. Your submission has been favorably evaluated by Timothy Behrens (Senior editor) and Peggy Mason (Reviewing editor).</p><p>This is an excellent revision. You have utilized the reviews to make this complex paper as understandable as its interesting findings deserve. The editor has three very minor suggestions for clarifications (1-2 sentences per issue should do it). The issues are:</p><p><xref ref-type="fig" rid="fig1">Figure 1</xref> is very useful <italic>except</italic> I didn't understand the emotion axis in <xref ref-type="fig" rid="fig1">Figure 1</xref> (filled in gray area) until I saw the bottom of <xref ref-type="fig" rid="fig3">Figure 3</xref> where it is labeled as emotion sensitivity. Please label in <xref ref-type="fig" rid="fig1">Figure 1</xref> and explain in the legend (in addition to the explanation in the text).</p><p>How were the times in <xref ref-type="fig" rid="fig4">Figure 4</xref> chosen? The times in B and C make some sense to me but I don't understand how 280 was chosen when it is so far from the region of significant differences. Also please explain why the maps in D-F don't look as though they came from the brains mapped in the middle of A-C (particularly B-C). Upon re-reading this, I think I understand the timing issue – you are mapping the deviations from 0 rather than the peak differences between the conditions. Perhaps make the emphasis on the latter equal to that on the former by making the gray boxes into bars up top to match the green and orange bars? All equally emphasized that way. Confusion regarding map alignments or lack thereof remains.</p><p>For the general reader, could you add a word of background about the lateralization difference illustrated in <xref ref-type="fig" rid="fig6">Figure 6B</xref> and discussed in the subsection “Early neural encoding of threat-signaling emotions in motor preparation”? It appears to be a robust difference – is it known and expected? Surprising in any way? Remember that the <italic>eLife</italic> readership is broad and not necessarily in the know about much intra-field background.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.10274.012</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>The interaction of emotional expression with gaze direction dependent on gain sensitivity is elegant and well done, and novel. The anxiety dependent analysis and related findings are intriguing. Addressing the following issues would make this a valuable contribution.</italic></p><p><italic>Most importantly, the Methods should be incorporated into the Results. The Methods are too integral to the understanding of the findings to be presented separately. Please write the manuscript to be understandable to the general reader. Clarity is more important than standard format and all reviewers agreed that methodological details should be interwoven into the Results presentation.</italic> </p><p>We followed the reviewers’ advice by blending the description of the methods into the main text. We aimed to make the findings understandable to non-specialists without requiring them to read in length the Methods section. In doing so, we chose to keep methodological details in the Methods section while providing the rationale of regression-based analyses in the main text. We believe that these changes significantly improved the readability of the manuscript to non-specialists, and we thank the reviewers for this useful suggestion.</p><p><italic>Can the authors justify not considering participant response in their EEG analysis? Or possibly analyzing the data in the standard way – by stimulus – and then additionally by classification response?</italic> </p><p>We believe this comment to be tightly linked to the previous point – describing the methods in the main text should make it clear why we did not perform straightforward contrasts based on participants’ responses (e.g., correct responses vs. errors). We did not perform such contrasts because the corresponding effects can be mediated indirectly (and trivially) by changes in the stimulus itself – not in the decision process. For example, both EEG signals and the accuracy of behavioral responses grow with the strength of the emotion displayed by the face stimulus, such that classifying EEG signals with respect to response accuracy could show a significant difference which is merely due to the fact that correct responses were on average associated with stronger emotions – i.e., by external differences in stimulation. Performing a response classification analysis not confounded with changes in stimulation would have required a ‘constant stimulus’ condition, e.g., considering emotionless/neutral stimuli in isolation (only 1/15 of all trials in our experimental design). The other approach, which we have followed throughout the manuscript and which allows to analyze all trials, is the regression-based method consisting in studying how trial-by-trial <italic>residuals</italic> from the regression of stimulus features (here, emotion strength) against EEG signals co-vary with response accuracy. The other advantage of model-based regression analyses, which do take participants’ responses into account and which we underline more explicitly in the revised manuscript, is that they allow not only to assess whether EEG signals co-vary with behavioral responses, but also how they do so: through modulations of perceptual sensitivity and/or bias.</p><p>On the theoretical side, it is worth noting that according to decision theories inspired by classical Signal Detection Theory, the decision process underlying correct responses and errors is the same – the only difference lies in the position of the noisy decision variable with respect to the decision criterion on particular trials. Identifying sensitivity and bias is therefore informed by both correct responses and errors. Following this reasoning, we chose to perform regression-based analyses of the EEG data without artificially separating correct responses from errors. The resulting pattern therefore indicates the enhancement of neural sensitivity to threat-signaling emotions <italic>irrespective of the accuracy of the subsequent response</italic>. Nevertheless, even when considering only correct trials in the ‘encoding’ analyses shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, significant differences between THREAT+ and THREAT- emotions emerge at temporal electrodes at 170 ms following stimulus onset (<italic>t</italic><sub>23</sub>= -2.1, <italic>p</italic> &lt; 0.05), parietal electrodes at 500 ms (<italic>t</italic><sub>23</sub>= 4.2, <italic>p</italic> &lt; 0.001) and motor signals at 200 ms (<italic>t</italic><sub>23</sub>= 3, <italic>p</italic> &lt; 0.01). We added these results in the Results section:</p><p>“This threat-dependent enhancement remained significant when considering only correct responses (temporal: <italic>t</italic><sub>23</sub> = -2.1, <italic>p</italic> &lt; 0.05; centro-parietal <italic>t</italic><sub>23</sub> = 4.2, <italic>p</italic> &lt; 0.001).”,</p><p>and:</p><p>“This early threat-dependent motor enhancement remained significant when considering only correct responses (<italic>t</italic><sub>23</sub> = 3.0, <italic>p</italic> &lt; 0.01).”</p><p>We hope that these clarifications, which are mirrored by additional descriptions of the regression-based methods throughout the text, will make the main findings more easily understandable to the broad readership of <italic>eLife</italic>.</p><p><italic>There is concern that one outlier is driving the anxiety-threat correlation for motor areas. Could the authors please address this concern?</italic> </p><p>We performed additional control analyses to rule out the possibility that the anxiety-threat correlation observed in motor regions is driven by one or few outliers. First, it is important to note that the anxiety scores of our participants (between 20 and 45) fall well within the range of the normal population (&lt; 60), and there is thus no a priori reason to exclude any of our participants solely based on his/her anxiety score. Besides, it is precisely the between-participant variability in anxiety scores that afforded their correlation with neural effects. As a first control analysis, we computed prediction intervals for the group-level regression line at the anxiety score of each participant. No participant fell outside of the 95% prediction intervals for both temporal (<xref ref-type="fig" rid="fig8">Figure 8A</xref>) and motor areas (<xref ref-type="fig" rid="fig8">Figure 8B</xref>) – and could thus be labeled statistically as outlier.</p><p>We performed a second, stricter control analysis to further confirm the robustness of the anxiety-threat correlation observed in temporal and motor regions. We performed a leave-one-out, cross-validation procedure in which we computed ‘cross-validated’ prediction intervals for the group-level regression line at the anxiety score of participant #n when the data for participant #n was excluded from the group-level regression. This procedure was repeated for the 24 participants for the anxiety-threat correlation in motor areas and showed that the neural effects of two participants fell slightly (&lt; 20%) outside of the <italic>cross-validated</italic> 95% prediction intervals – including participant #4 whose anxiety score was 45. Nevertheless, recomputing the group-level correlation in motor areas after excluding the two outliers, leaving 22 participants in the analysis, led to a significant effect (<italic>r</italic> = 0.48, d.f. = 20, <italic>p</italic> = 0.02). This leave-one-out analysis identified no outlier for the anxiety-threat correlation in temporal regions.</p><p>Together, we believe that these control analyses confirm the robustness of the anxiety-threat correlations for both temporal and motor regions, and we now describe them in the Methods section of the revised manuscript.</p><p><italic>Please explicitly state how multiple comparisons were accounted for.</italic> </p><p>We apologize for not providing enough statistical details about correction for multiple comparisons in the Methods section. We applied conventional corrections described in (Maris and Oostenveld, 2006), reflected in the ‘cluster-corrected’ p-values reported in the Results section. We now explicitly state how multiple corrections were accounted for in the Methods section of the revised manuscript in a new ‘statistical procedures’ subsection (“All regression-based analyses […] corresponding to its ‘cluster-corrected’ p-value.”).</p><p><italic>Were pts screened for social anxiety or depression?</italic> </p><p>This question is related to the growing evidence in favor of comorbidity between anxiety (related to six anxiety disorders among which social anxiety) and depressive disorders, which calls for the identification of both specific and shared neurocognitive markers of anxiety and depression.</p><p>Our findings put forward the adaptive and <italic>positive</italic> function of moderate (non-clinical) anxiety in response to social threats (Bateson et al., 2011). Our participants filled the STAI questionnaire, the most commonly used in experimental investigations of anxious features in non-clinical samples (see, e.g., Sylvers et al., 2011), but they were neither screened for depression nor for social anxiety. As stressed recently by Grupe and Nitschke (2013): “Despite its lack of specificity, the relevance of research using STAI is underscored by its sensitivity as a marker of risk for anxiety disorders.” Furthermore, concerning the comorbidity between depression and anxiety, it has been shown that patients with clinically defined anxiety and depression had lower mortality rates than those with depression alone (Mykletun et al., 2009). This finding thus suggests that both non-clinical and clinical anxiety may have a specific ‘protective’ function, related to the adaptive role of anxiety in survival, which is not shared with depression.</p><p>We nevertheless agree with the reviewers and acknowledge the resulting limitation as to the specificity of the observed anxiety-dependent effects. Consequently, we have added a sentence to the Discussion section:</p><p>“Further research should assess the specificity of these anxiety-dependent effects, in light of the growing evidence in favor of comorbidity between anxiety and depressive disorders.”</p><p>Reviewer #1:</p><p><italic>This is an interesting study with an even more interesting answer once the anxiety bit is thrown in. I don't have many comments in part because the EEG methods are opaque to me. My one major concern is whether the middle portion of the Results – concerning all subjects together – has value. It appears that the Results are in fact driven by two different strongly significant results in two different subject cohorts – high and low anxiety. So the overall increased sensitivity to</italic> threat<italic>+ is constructed of a low anxiety - perception and high anxiety = motor pieces.</italic> </p><p>We thank the reviewer for his/her positive comments. We agree with the reviewer that the sensory and motor response enhancements are strongly modulated, in a doubly dissociable fashion, by the anxiety score of the participants. Nevertheless, we believe the middle portion of the Results section to be of importance for the field, as it sheds light on the specific neural process responsible for threat-dependent enhancements: an increased neural sensitivity to threat-signaling emotions rather than a perceptual or decision bias, which confirms our model-based analysis of the behavioral data and which had not been tested before to our knowledge. The later dissociation between high- and low-anxious participants builds upon these first findings by highlighting the selective contributions of sensory and motor regions to the increased sensitivity to threat-signaling emotions in the two high- and low-anxious groups.</p><p>To make our EEG analyses clearer, and as indicated in the collective responses above, we followed the reviewer’s advice by blending the description of the methods into the main text. We aimed to make the findings understandable to non-specialists without requiring reading in length the Methods section. We hope that the reviewer will agree that these changes significantly improved the readability of the manuscript to non-specialists.</p><p>Reviewer #2:</p><p><italic>In general I think this is a nice piece of work and the authors have clearly gone to considerable effort to provide a full account of their data set and are to be applauded for the extent of the analysis conducted to better understand the neural response to threatening social stimuli. However, I found some of the analysis difficult to parse and in some cases important information was missing (or not obvious). Particular points that bothered me were the selective presentation of the behavioural results.</italic> </p><p>We thank the reviewer for his/her positive evaluation of our analytic framework and findings. We understand the reviewer’s concern regarding our selective description of the behavioral results. We originally decided to favor concision over exhaustiveness in the behavioral results to facilitate the transmission of the main message, but retrospectively we can see how the missing details can impair the readability of this section. We have followed the reviewer’s suggestion by providing the missing information on recognition accuracy and reaction times in the revised manuscript.</p><p><italic>Where are the reaction time results (as mentioned to have been analysed in the Methods)?</italic> </p><p>The reaction times results have been added to the Results section:</p><p>“Reaction time (RT) analyses revealed a decrease of correct RTs with emotion strength (repeated-measures ANOVA, F<sub>6,138</sub> = 54.5, p &lt; 0.001), faster responses to angry as compared fearful faces (F<sub>1,23</sub> = 12, p &lt; 0.01), and faster responses to direct as compared to averted gaze (F<sub>1,23</sub> = 7.7, p &lt; 0.05). Furthermore, an emotion by gaze interaction was observed (F<sub>1,23</sub> = 8, p &lt; 0.01), corresponding to faster reaction times for direct as compared to averted gaze in the anger condition only (t<sub>23</sub> = -3.9, p &lt; 0.001).”</p><p>Note that these reaction time results provide an independent replication of our main finding of an interaction between gaze direction and emotion on the accuracy of emotion recognition – by showing the same pattern on the speed of emotion recognition.</p><p><italic>Was there no interaction between the categorisation performance and level of morph when exploring the eye gaze-emotion performance interaction? The plots in <xref ref-type="fig" rid="fig2">Figure 2B</xref> suggest that as one might expect, the effect of eye gaze is stronger for the weaker emotion signals (closer to neutral).</italic> </p><p>There is indeed an interaction between threat+/− and the level of morph on categorization performance, and these results have been added to the Results section:</p><p>“Moreover, a significant emotion by gaze by emotion strength interaction was observed (F<sub>6,138</sub> = 4.3, p &lt; 0.01), explained by a stronger influence of gaze on emotion categorization at weak emotion strengths (gaze by emotion interaction for levels 1 to 4, F<sub>1,23</sub> = 23.8, p &lt; 0.001) than at high emotion strengths (gaze by emotion interaction for levels 5 to 7, F<sub>1,23</sub> = 5.1, p &lt; 0.05).”</p><p>Nevertheless, note that this interaction alone is not specific of either of our two models (bias vs. sensitivity difference between threat+ and threat- conditions). The difference between the two models lies in whether the effect of gaze is strongest for emotionless/neutral stimuli (bias effect) or for weak emotion strengths (sensitivity effect, see the new <xref ref-type="fig" rid="fig1">Figure 1</xref> showing model predictions for both types of effects). This observation was one of the main reasons for model-based analyses, which can arbitrate between these two alternative accounts of this interaction on emotion categorization.</p><p><italic>As a more minor point, why are participants not 100% correct for the end points of the morphs (100% anger, 100% fear) – are some identities consistently categorised incorrectly?</italic> </p><p>This is a very good point, and unfortunately we had omitted to explain how we accounted for this observation in our modeling of participants’ behavior. In every model we fitted to participants’ choices, we have added an additional ‘lapse’ parameter that corresponds to the proportion of random guesses among choices. This parameter is conventionally used in psychophysics to model the imperfect asymptotic performance noted by the reviewer. We have corrected the Methods section to describe the full model including the ‘lapse’ parameter:</p><p>“We started with the simplest model (model 0) that could account for each subject’s decisions using a noisy, ‘signal detection’-like psychometric model to which we included a lapse rate, thereby considering that subjects guessed randomly on a certain proportion of trials:</p><p>P(anger) = Ф[w*<bold>x</bold> + b]*(1− ɛ)+ 0.5*ɛ</p><p>where P(anger) corresponds to the probability of judging the face as angry, Ф[.] to the cumulative normal function, w to the perceptual sensitivity to the displayed emotion, <bold>x</bold> to a trial-wise array of evidence values in favor of anger or fear (emotion strength, from −7 for an intense expression of fear to +7 for an intense expression of anger), b to an additive, stimulus-independent bias toward one of the two emotions, and ε to the proportion of random guesses among choices.”</p><p>Note that the group-level mean for the lapse rate ɛ fitted simultaneously with the other parameters w and b is equal to 16.2 ± 1.8% (mean ± s.e.m.), a value that importantly did not differ between threat+ and threat- conditions (<italic>t</italic><sub>23</sub> = 0.4, <italic>p</italic> &gt; 0.5).</p><p>Nevertheless, we took the reviewer’s alternative hypothesis seriously and performed additional analyses to rule out the possibility that some identities were consistently categorized incorrectly by the participants. First, we verified that all of the identities used in the study were categorized significantly above chance at both extremes of the emotion morph axis. This was indeed the case (paired t-tests, all <italic>t</italic><sub>23</sub> &gt; 6.4, all <italic>p</italic> &lt; 0.001). Second, we fitted separate sigmoid functions to describe participants’ average psychometric response curve for each identity taken in isolation. This ‘fixed-effects’ analysis confirmed that all of the identities were consistently assigned to anger and fear as a function of morph level, as indicated by significantly positive slopes of the associated psychometric functions (logistic regressions, all <italic>t</italic>-values &gt; 12.8, all <italic>p</italic> &lt; 0.001). Furthermore, the distribution of the point of subjective ‘neutrality’ (at which the psychometric function crosses 0.5) across identities is centered on 0.01 (on a scale going from −1 for the strongest fear expression to +1 for the strongest anger expression), with a standard deviation of 0.20, indicating that there were no extreme biases in categorization across participants for the identities used in the study. Together, we believe that these additional results fully support our choice to model the imperfect asymptotic performance by stimulus-independent lapses rather than by misrecognized emotions displayed by certain identities.</p><p><italic>Turning to the EEG analysis. The authors employ advanced and relatively specialised methods to explore the relationship of the EEG signal to the modulations in emotional strength and their results are interesting and appear strong. While I think they have done a reasonable job in explaining these methods and how they should be interpreted, I would have liked more detail on the statistical tests they employed. Surely they performed some bootstrapping to correct for the multiple comparisons issues they face. This is hinted at in the caption to <xref ref-type="fig" rid="fig3">Figure 3</xref> (cluster corrected threshold) but no detail is provided anywhere that I can see.</italic> </p><p>As indicated in our response to collective points above, we have added a ‘statistical procedures’ paragraph in the Methods section, where we describe in more detail the statistical analyses performed on the data. Concerning the issue of multiple comparisons, we applied conventional corrections described in (Maris and Oostenveld, 2006), reflected in the ‘cluster-corrected’ p-values reported in the Results section. We now explicitly state how multiple corrections were accounted for in the Methods section of the revised manuscript in a new ‘statistical procedures’ subsection (“All regression-based analyses […] corresponding to its ‘cluster-corrected’ p-value.”).</p><p><italic>Furthermore, I think the authors used all trials (correct and incorrect) in their EEG analysis but it’s unclear to me why this would be justified, particularly for the weaker emotional stimuli (where participants are only correct about 70% of the time in categorising them). For example a</italic> threat+ <italic>fear averted trial could have been categorised by the participant as an anger face (and thus treated by the participant as</italic> threat-<italic>). Would the authors expect the neural encoding to be the same in both cases? If not, how can they justify not considering participant response in their EEG analysis?</italic> </p><p>As argued in our response to collective points above, it is important to remember that according to decision theories inspired by classical Signal Detection Theory, the decision process underlying correct responses and errors is the same – the only difference lies in the position of the noisy decision variable with respect to the decision criterion on particular trials. Identifying sensitivity and bias is therefore informed by both correct responses and errors. Following this reasoning, we decided to perform regression-based analyses of the EEG data without artificially separating correct responses from errors. The resulting pattern thus reflects the enhancement of neural sensitivity to threat-signaling emotions, irrespective of the accuracy of the subsequent response.</p><p>Nevertheless, we do not mean that the neural encoding of emotion strength should be the same when analyzing separately correct responses and errors. And as expected, the neural encoding of emotion strength by EEG signals at temporal, centro-parietal and motor electrodes did not reveal any significant cluster when considering errors in isolation (cluster-corrected <italic>p</italic> &gt; 0.1). By contrast, considering only correct trials in the encoding analyses shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> did not alter the significant differences between threat+ and threat- conditions at temporal electrodes at 170 ms following stimulus onset (<italic>t</italic><sub>23</sub>= -2.1, <italic>p</italic> &lt; 0.05), parietal electrodes at 500 ms (<italic>t</italic><sub>23</sub>= 4.2, <italic>p</italic> &lt; 0.001) and motor signals at 200 ms (<italic>t</italic><sub>23</sub>= 3, <italic>p</italic> &lt; 0.01).</p><p>Furthermore, note that the neural-choice correlation analyses performed in the Results section do take participants’ responses into account, and this is now more explicitly underlined in the revised manuscript. The important advantage of the model-based approach we applied instead of a model-free split between correct and error responses is that it allows to assess not only <italic>whether</italic> EEG signals co-vary with behavioral responses, but also how they do so: through changes of sensitivity and/or bias.</p><p><italic>Finally I enjoyed the anxiety dependent analysis and the findings are clear and interesting.</italic> </p><p>We are happy that the reviewer found our anxiety-dependent analyses of the neural effects of importance and interest.</p><p>Reviewer #3:</p><p><italic>[…] This part of the paper is elegant and well done, and to my knowledge, novel. The Methods section is really clear – my preference would be to see some of the explanation of the models moved into the main part of the paper, as this part, while 'technical', is really needed to understand the meaning of the subsequent analysis, and as it stands, the introduction is much less clear than the longer explanation.</italic> </p><p>We thank the reviewer for his/her positive comments on the study. As indicated in response to the collective points above, we have attempted to be more explicit about the regression-based, model-guided methods we have applied to the neural data throughout the Results section of the revised manuscript. We hope that the reviewer would find the revised manuscript clearer in this respect.</p><p><italic>The final section of the paper claims a relationship with anxiety levels in individual subjects, as</italic> threat+ <italic>and - are differently modulate temporal areas in low anxious individuals), and</italic> threat+ <italic>modulates motor areas in high anxious individuals.</italic> </p><p><italic>I have reservations about the second part of the dissociation in high anxious individuals mostly to do with <xref ref-type="fig" rid="fig7">Figure 7B</xref> – it seems to me to be the main weakness.</italic> </p><p>We believe to have clarified and strengthened the observed anxiety-dependent dissociation between sensory and motor enhancements in response to threat+ combinations of gaze and emotion in the revised manuscript – as indicated in response to the collective points above and the specific points raised by the reviewer below.</p><p><italic>1) Anxiety is co-morbid with depression, and social anxiety. Were participants screened for their scores with other factors that might be correlated with anxiety?</italic> </p><p>Participants were not screened for other dimensions such as social anxiety or depression, but only for the single dimension we had predictions on in relation to threat perception: anxiety. The reviewer is right that anxiety scores measured with the STAI questionnaire are typically correlated with depression and social anxiety scores across individuals, and we have updated the Discussion section to acknowledge the resulting limitation as to the specificity of the observed effects: “Further research should assess the specificity of these anxiety-dependent effects, in light of the growing evidence in favor of a comorbidity between anxiety and depressive disorders.”</p><p>Nevertheless, as indicated in response to the collective points above and concerning the comorbidity between depression and anxiety, it has been shown that patients with clinically defined anxiety and depression had lower mortality rates than those with depression alone (Mykletun et al., 2009). This finding thus suggests that clinical anxiety may have a specific ‘protective’ function, related to the adaptive role of anxiety in survival, which is not shared with depression. Despite their limitations (shared by most of existing studies of anxiety in the field), our findings provide empirical evidence in favor of such positive function of non-clinical anxiety in the general population.</p><p><italic>2) All participants bar one lie in the very centre of the normal range. In <xref ref-type="fig" rid="fig7">Figure 7B</xref>, it looks like that 40+ score might be an outlier. R values are computed with Pearson (I assume, though I notice that isn't on the figure; it should be). If the outlying subject is excluded, does the correlation survive?</italic> </p><p>The correlation strengths reported in the manuscript are indeed linear Pearson correlation values, and this is now specified in the text and figure legends of the revised manuscript. As indicated in response to the collective points above, we have performed several additional analyses of the data shown on <xref ref-type="fig" rid="fig7">Figure 7</xref> (now <xref ref-type="fig" rid="fig8">Figure 8</xref> in the revised manuscript), which should convince the reviewer of the robustness of the anxiety-dependent effects in sensory and motor regions.</p><p>First, it is important to note that the anxiety scores of our participants (between 20 and 45) fall well within the range of the normal population (&lt; 60), and there is thus no a priori reason to exclude any of our participants solely based on his/her anxiety score. Besides, it is precisely the between-participant variability in anxiety scores that afforded their correlation with neural effects. As a first control analysis, we computed prediction intervals for the group-level regression line at the anxiety score of each participant. No participant fell outside of the 95% prediction intervals for both temporal (<xref ref-type="fig" rid="fig8">Figure 8A</xref>) and motor areas (<xref ref-type="fig" rid="fig8">Figure 8B</xref>) – and could thus be labeled statistically as outlier. Nevertheless, if we exclude <italic>arbitrarily</italic> participant #4 whose anxiety score was 45, the motor correlation with anxiety remains marginally significant (<italic>r</italic> = 0.35, d.f. = 21, two-tailed <italic>p</italic> = 0.09), as well as the interaction between state anxiety and threat level (mixed-design ANOVA, F<sub>1,22</sub> = 3.1, <italic>p</italic> = 0.09). And within median-split high-anxious participants, the neural encoding of emotion strength in the threat+ condition remains significant at a two-tailed level (t-test against zero, <italic>t</italic><sub>10</sub> = 2.4, two-tailed <italic>p</italic> = 0.03), as well as the observed difference between threat+ and threat− conditions (paired t-test, <italic>t</italic><sub>10</sub> = 2.6, two-tailed <italic>p</italic> = 0.02).</p><p>Instead of excluding arbitrarily one participant, we performed a leave-one-out, cross-validation procedure in which we computed ‘cross-validated’ prediction intervals for the group-level regression line at the anxiety score of participant #n when the data for participant #n was excluded from the group-level regression. This procedure was repeated for the 24 participants for the anxiety-threat correlation in motor areas and showed that the neural effects of two participants fell slightly (&lt; 20%) outside of the <italic>cross-validated</italic> 95% prediction intervals – including participant #4. Nevertheless, recomputing the group-level correlation in motor areas after excluding the two outliers, leaving 22 participants in the analysis, led to a significant effect (<italic>r</italic> = 0.48, d.f. = 20, <italic>p</italic> = 0.02). This leave-one-out analysis identified no outlier for the anxiety-threat correlation in temporal regions.</p><p>Together, we believe that these control analyses confirm the robustness of the anxiety-threat correlations for both temporal and motor regions, and we now describe them in the Methods section of the revised manuscript.</p><p><italic>Following on from that, it seems to me that there is an increase in variabilityin the 'more anxious' group, which could be driving the correlation, especially as it looks as though the difference between threat and no-threat in the high anxious group in 7B isn't significant.</italic> </p><p>In response to this specific point, we can assure the reviewer that there is a significant difference in neural encoding in motor lateralization for the high-anxious group between threat+ and threat− conditions (paired t-test, <italic>t</italic><sub>11</sub> = 3.0, <italic>p</italic> = 0.01). Neural encoding for the threat+ condition is even significantly positive (t-test against zero, <italic>t</italic><sub>11</sub> = 2.6, <italic>p</italic> = 0.02). Note also that both tests remain significant even when participant #4 is excluded arbitrarily from the high-anxious group (see our response to the previous point). Finally, the use of a leave-one-out procedure for identifying two outliers with respect to the correlation shown on <xref ref-type="fig" rid="fig7">Figure 7B</xref> (now <xref ref-type="fig" rid="fig8">Figure 8B</xref>) left the correlation significant, thereby confirming that the correlation is not driven by outlying data points.</p><p><italic>3) The effect in <xref ref-type="fig" rid="fig7">Figure 7B</xref> seems to be a weak effect at a single time point. There is no justification/explanation of the selection of either the window in 7A, or the single point in 7B (if that is what it is).</italic> </p><p>We fully agree with the reviewer on this point. While we chose to measure the correlation with anxiety at the time point corresponding to the peak of the encoding effect in motor signals (200 ms), we chose the time window corresponding to the significant cluster for the encoding at temporal electrodes. This was indeed a discrepancy, which we have corrected by also choosing the peak of neural encoding for temporal electrodes (280 ms) to perform the correlation. We have changed accordingly the figure (previously <xref ref-type="fig" rid="fig7">Figure 7</xref>, now <xref ref-type="fig" rid="fig8">Figure 8</xref>), and the text:</p><p>“Indeed, state anxiety influenced significantly the neural encoding […] between threat+ and threat− combinations proved to be significant (Pearson correlation coefficient r = 0.51, d.f. = 22, p = 0.01; <xref ref-type="fig" rid="fig8">Figure 8A</xref>).”</p><p>Note that both correlations with anxiety remain significant if the entire time window corresponding to the significant cluster is taken, at both temporal (170-400 ms, <italic>r</italic> = 0.53, d.f. = 22, <italic>p</italic> &lt; 0.01) and motor electrodes (130-300 ms, <italic>r</italic> = 0.43, d.f. = 22, <italic>p</italic> &lt; 0.05). We believe that these additional analyses further strengthen the anxiety-dependent correlations reported in the manuscript.</p><p><italic>4) The role of reaction time: the authors say that they found no difference in the behavioural responses between the more and less anxious groups. Did that include reaction times?</italic> </p><p>The analyses indeed included reaction times – which do not differ between high- and low-anxious groups nor correlate with anxiety. There was no measurable difference between the two anxiety groups in terms of behavior alone. The single anxiety-dependent effect that significantly impacts behavior in our study is reported in the Results section. The strength of neural encoding of threat+ emotions at motor electrodes correlated significantly with behavioral sensitivity for high-anxious participants but not for low-anxious participants, and significantly more for high-anxious than low-anxious participants:</p><p>“Moreover, the neural encoding of threat+ emotions in motor signals correlated with behavioral sensitivity to threat+ emotions for high-anxious individuals (Pearson correlation coefficient r = 0.66, d.f. = 10, p = 0.01; <xref ref-type="fig" rid="fig8">Figure 8B</xref>), whereas it was not the case for low-anxious individuals (Pearson correlation coefficient r = −0.42, d.f. = 10, p &gt; 0.16, difference between coefficients, p &lt; 0.01).”</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p><italic><xref ref-type="fig" rid="fig1">Figure 1</xref> is very useful</italic> except <italic>I didn't understand the emotion axis in <xref ref-type="fig" rid="fig1">Figure 1</xref> (filled in gray area) until I saw the bottom of <xref ref-type="fig" rid="fig3">Figure 3</xref> where it is labeled as emotion sensitivity. Please label in <xref ref-type="fig" rid="fig1">Figure 1</xref> and explain in the legend (in addition to the explanation in the text).</italic> </p><p>We are sorry for not clarifying what the filled gray area is on <xref ref-type="fig" rid="fig1">Figure 1</xref>. The emotion axis represents the evidence to emotional expression from the most fearful face on the left, to the angriest face on the right. The filled gray area represents the difference between the two curves plotted (one for direct gaze, one for averted gaze). We added this gray area to emphasize the fact that differences between direct and averted gaze (i.e., the difference between the two curves) would be maximal for the neutral level for a change in bias between direct and averted gaze conditions, and maximal for low emotion strengths for a change in sensitivity between threat+ and threat- conditions. We now added to the figure a gray box that describes the filled gray area: 'difference direct/averted', and we added to the legend:</p><p>&quot;Maximal effects would appear for neutral (emotionless) expressions as highlighted through the filled gray area on the emotion axis that represents the difference between the two psychometric functions for direct and averted gaze.&quot;</p><p><italic>How were the times in <xref ref-type="fig" rid="fig4">Figure 4</xref> chosen? The times in B and C make some sense to me but I don't understand how 280 was chosen when it is so far from the region of significant differences. Also please explain why the maps in D-F don't look as though they came from the brains mapped in the middle of A-C (particularly B-C). Upon re-reading this, I think I understand the timing issue – you are mapping the deviations from 0 rather than the peak differences between the conditions. Perhaps make the emphasis on the latter equal to that on the former by making the gray boxes into bars up top to match the green and orange bars? All equally emphasized that way. Confusion regarding map alignments or lack thereof remains.</italic> </p><p>The EEG topographies plotted in the middle of <xref ref-type="fig" rid="fig4">Figure 4a-c</xref> are taken at the latency of peak deviations from zero, and not of peak differences between conditions. While the first peak of encoding independently of conditions emerges at 280 ms, the difference between our two conditions of interest (threat+ and threat-) emerged earlier (170 ms). This is not the case for the two other peaks (500 ms and response time) as the difference between conditions corresponded to the peak deviations from zero. We plot these topographies to justify how we selected the electrodes on which we compare between the two conditions of interest – i.e., using a contrast (t-test against zero) orthogonal to the contrast between the two conditions. We now added to <xref ref-type="fig" rid="fig4">Figure 4</xref> on top of these topographies: “Peak encoding across conditions”.</p><p>Moreover, as suggested by the editor, we changed the gray boxes to gray bars on top of the green and orange bars to equally emphasize the deviations from zero and the differences between conditions (revised <xref ref-type="fig" rid="fig4">Figure 4</xref>). Yet, our main goal was to address the cognitive and neural differences between our conditions of interest (threat+ and threat-) rather than studying the encoding of emotion per se. We therefore still believe that it might be useful to differentiate visually these two types of statistical differences with a stronger emphasis on the difference between threat+ and threat- conditions.</p><p>The brain maps plotted in <xref ref-type="fig" rid="fig4">Figure 4d-f</xref> represent the difference in encoding between threat+ and threat- conditions at the source level, at the times where there was a significant difference between these conditions (gray bar). We now added on <xref ref-type="fig" rid="fig4">Figure 4</xref> on top of these brain maps: “Source of encoding difference T+/T-”. Source reconstruction analyses localize the regions of the brain that generate the scalp-recorded EEG signals. Similar averaged scalp-recorded EEG signals can therefore stem from different brain sources. Accordingly, temporal EEG components (N170) were shown to have different brain sources, including the fusiform gyrus (e.g. Itier and Taylor, 2002) and the superior temporal sulcus (e.g. Batty and Taylor, 2003). Here we report both these regions as sources of the difference between threat+ and threat- encoding. Also, brain sources of parietal EEG activity (P300) include widely separated brain regions within which frontal and temporal regions (Nieuwenhuis et al., 2005), similarly to those reported in the present study when computing the sources of difference between threat+ and threat- related to parietal scalp-recorded EEG. Finally, revealing dorsal central motor-related regions in f (related to the topography in c at response time), and not e (related to the topography in b at 500 ms) is not surprising at response time.</p><p>We also added this sentence to the Results section:</p><p>“To assess which brain regions generated the scalp-recorded EEG signals, we computed the cortical sources of this enhanced encoding of threat-signaling emotions by performing the same regression approach to minimum-norm current estimates distributed across the cortical surface.”</p><p><italic>For the general reader, could you add a word of background about the lateralization difference illustrated in <xref ref-type="fig" rid="fig6">Figure 6B</xref> and discussed in the subsection “Early neural encoding of threat-signaling emotions in motor preparation”? It appears to be a robust difference – is it known and expected? Surprising in any way? Remember that the</italic> eLife <italic>readership is broad and not necessarily in the know about much intra-field background.</italic> </p><p>Following the editor's advice, we added some background about the lateralization difference illustrated in <xref ref-type="fig" rid="fig6">Figure 6B</xref>, in the subsection “Early neural encoding of threat-signaling emotions in motor preparation”:</p><p>&quot;Limb movement execution and preparation coincide with suppression of low-frequency (8-32 Hz) activity that is stronger in the motor cortex contralateral as compared to ipsilateral to the movement. Thus, subtracting the contralateral from ipsilateral motor cortex activity is expected to result in a positive measure of motor preparation&quot;.</p></body></sub-article></article>