<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">01312</article-id><article-id pub-id-type="doi">10.7554/eLife.01312</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Decoding neural responses to temporal cues for sound localization</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-6781"><name><surname>Goodman</surname><given-names>Dan FM</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="aff" rid="aff2"/><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="pa1">†a</xref><xref ref-type="fn" rid="pa2">†b</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-6824"><name><surname>Benichoux</surname><given-names>Victor</given-names></name><xref ref-type="aff" rid="aff3"/><xref ref-type="aff" rid="aff4"/><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-6825"><name><surname>Brette</surname><given-names>Romain</given-names></name><xref ref-type="aff" rid="aff3"/><xref ref-type="aff" rid="aff4"/><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Laboratoire de Psychologie de la Perception</institution>, <institution>CNRS and Université Paris Descartes</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff><aff id="aff2"><institution content-type="dept">Département d’Etudes Cognitives</institution>, <institution>Ecole Normale Supérieure</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff><aff id="aff3"><institution content-type="dept">Laboratoire de Psychologie de la Perception</institution>, <institution>CNRS and Université Paris Descartes</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff><aff id="aff4"><institution content-type="dept">Département d’Etudes Cognitives</institution>, <institution>Ecole Normale Supérieure</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Angelaki</surname><given-names>Dora</given-names></name><role>Reviewing editor</role><aff><institution>Baylor College of Medicine</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>dan_goodman@meei.harvard.edu</email> (DFMG);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>romain.brette@ens.fr</email> (RB)</corresp><fn fn-type="present-address" id="pa1"><label>a</label><p>Eaton Peabody Laboratory, Massachusetts Eye and Ear Infirmary, Boston, United States</p></fn><fn fn-type="present-address" id="pa2"><label>b</label><p>Department of Otology and Otolaryngology, Harvard Medical School, Boston, United States</p></fn></author-notes><pub-date date-type="pub" publication-format="electronic"><day>03</day><month>12</month><year>2013</year></pub-date><pub-date pub-type="collection"><year>2013</year></pub-date><volume>2</volume><elocation-id>e01312</elocation-id><history><date date-type="received"><day>30</day><month>07</month><year>2013</year></date><date date-type="accepted"><day>25</day><month>10</month><year>2013</year></date></history><permissions><copyright-statement>© 2013, Goodman et al</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Goodman et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-01312-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.01312.001</object-id><p>The activity of sensory neural populations carries information about the environment. This may be extracted from neural activity using different strategies. In the auditory brainstem, a recent theory proposes that sound location in the horizontal plane is decoded from the relative summed activity of two populations in each hemisphere, whereas earlier theories hypothesized that the location was decoded from the identity of the most active cells. We tested the performance of various decoders of neural responses in increasingly complex acoustical situations, including spectrum variations, noise, and sound diffraction. We demonstrate that there is insufficient information in the pooled activity of each hemisphere to estimate sound direction in a reliable way consistent with behavior, whereas robust estimates can be obtained from neural activity by taking into account the heterogeneous tuning of cells. These estimates can still be obtained when only contralateral neural responses are used, consistently with unilateral lesion studies.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.001">http://dx.doi.org/10.7554/eLife.01312.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.01312.002</object-id><title>eLife digest</title><p>Having two ears allows animals to localize the source of a sound. For example, barn owls can snatch their prey in complete darkness by relying on sound alone. It has been known for a long time that this ability depends on tiny differences in the sounds that arrive at each ear, including differences in the time of arrival: in humans, for example, sound will arrive at the ear closer to the source up to half a millisecond earlier than it arrives at the other ear. These differences are called interaural time differences. However, the way that the brain processes this information to figure out where the sound came from has been the source of much debate.</p><p>Several theories have been proposed for how the brain calculates position from interaural time differences. According to the hemispheric theory, the activities of particular binaurally sensitive neurons in each of side of the brain are added together: adding signals in this way has been shown to maximize sensitivity to time differences under simple, controlled circumstances. The peak decoding theory proposes that the brain can work out the location of a sound on the basis of which neurons responded most strongly to the sound.</p><p>Both theories have their potential advantages, and there is evidence in support of each. Now, Goodman et al<italic>.</italic> have used computational simulations to compare the models under ecologically relevant circumstances. The simulations show that the results predicted by both models are inconsistent with those observed in real animals, and they propose that the brain must use the full pattern of neural responses to calculate the location of a sound.</p><p>One of the parts of the brain that is responsible for locating sounds is the inferior colliculus. Studies in cats and humans have shown that damage to the inferior colliculus on one side of the brain prevents accurate localization of sounds on the opposite side of the body, but the animals are still able to locate sounds on the same side. This finding is difficult to explain using the hemispheric model, but Goodman et al. show that it can be explained with pattern-based models.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.002">http://dx.doi.org/10.7554/eLife.01312.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>sound localization</kwd><kwd>neural coding</kwd><kwd>audition</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>European Research Council</institution></institution-wrap></funding-source><award-id>ERC StG 240132</award-id><principal-award-recipient><name><surname>Goodman</surname><given-names>Dan FM</given-names></name><name><surname>Benichoux</surname><given-names>Victor</given-names></name><name><surname>Brette</surname><given-names>Romain</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-11-BSH2-0004, ANR-11-0001-02 PSL* and ANR-10-LABX-0087</award-id><principal-award-recipient><name><surname>Goodman</surname><given-names>Dan FM</given-names></name><name><surname>Benichoux</surname><given-names>Victor</given-names></name><name><surname>Brette</surname><given-names>Romain</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Computer simulations of interaural time difference decoders show that heterogeneous tuning of binaural neurons leads to accurate sound localization in natural environments.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>To localize sound sources in the horizontal plane, humans and many other species use submillisecond timing differences in the signals arriving at the two ears (<xref ref-type="bibr" rid="bib2">Ashida and Carr, 2011</xref>). The ear closer to the source receives the sound earlier than the other. These interaural time differences (ITDs) are encoded in the auditory brainstem by binaural neurons, which are tuned to both frequency and ITD. An influential theory proposes that ITD is represented by the activity pattern of cells with heterogeneous tunings, a pattern code for sound location (<xref ref-type="bibr" rid="bib31">Jeffress, 1948</xref>). In a stronger version, ITD is represented by the identity of the most active cell in each frequency band, a labeled line code for sound location. Although this theory has proved successful in barn owls (<xref ref-type="bibr" rid="bib35">Konishi, 2003</xref>), discrepancies have been observed in mammals. In particular, at low frequencies, many cells have best delays (BDs) larger than the physiological range of ITDs experienced by the animal (<xref ref-type="bibr" rid="bib47">McAlpine et al., 2001</xref>). In a labeled line code, these cells would not have any function. An alternative theory was proposed, in which ITD is coded not by the firing of individual cells, but by the relative summed activity of each hemisphere, a summation code for sound location (<xref ref-type="bibr" rid="bib56">Stecker et al., 2005</xref>; <xref ref-type="bibr" rid="bib24">Grothe et al., 2010</xref>).</p><p>The nature of the neural code for ITD in mammals is still contentious because it is not known whether the auditory system sums activity or uses cell identity in decoding responses. In favor of the summation code hypothesis, cells with large BDs maximize ITD sensitivity of firing rate in the physiological range, whereas they are useless in a labeled line code. However, most of the cells with BDs inside the physiological range (most cells in cats; <xref ref-type="bibr" rid="bib39">Kuwada and Yin, 1983</xref>; <xref ref-type="bibr" rid="bib67">Yin and Chan, 1990a</xref>) actually degrade a summation code because their rates do not vary monotonically with ITD.</p><p>In simple situations where there is a single acoustical stimulus (e.g., tone) with unknown ITD, theoretical arguments show that a summation code is optimal at low frequencies (<xref ref-type="bibr" rid="bib26">Harper and McAlpine, 2004</xref>). Previous studies have also shown that with simple stimuli, taking into account cell identity rather than simply summing all responses does not improve performance (<xref ref-type="bibr" rid="bib41">Lesica et al., 2010</xref>; <xref ref-type="bibr" rid="bib43">Lüling et al., 2011</xref>). However, what is optimal in a simple world may not be optimal in an ecological environment. In a simple situation where only the ITD varies, the optimal code is the most sensitive one. In complex situations where other dimensions also vary, there is a trade-off between sensitivity and robustness, so the optimal code is not the most sensitive one (<xref ref-type="bibr" rid="bib4">Brette, 2010</xref>). In fact, theory predicts that in complex situations, the heterogeneity of ITD tunings is critical to produce robust estimates.</p><p>To address this, we studied the performance of different decoders in increasingly complex situations, including variations in spectrum, background noise, and head-related acoustic filtering. We found that summing cell responses is strongly suboptimal and that heterogeneity in tunings is information rather than noise.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Decoding the sound’s ITD from cell responses</title><p>Previous studies have tested the performance of simple decoders based on single-unit cell responses to acoustical stimuli (<xref ref-type="bibr" rid="bib17">Fitzpatrick et al., 1997</xref>; <xref ref-type="bibr" rid="bib25">Hancock and Delgutte, 2004</xref>; <xref ref-type="bibr" rid="bib56">Stecker et al., 2005</xref>; <xref ref-type="bibr" rid="bib14">Devore et al., 2009</xref>; <xref ref-type="bibr" rid="bib48">Miller and Recanzone, 2009</xref>; <xref ref-type="bibr" rid="bib41">Lesica et al., 2010</xref>; <xref ref-type="bibr" rid="bib43">Lüling et al., 2011</xref>). However, this approach is limited to a small number of acoustical stimuli and cells. Here, we wanted to test the performance of different decoders based on the response of a large population (up to 480 cells) to a large variety of sounds totaling 11 hr of sound per cell. Obtaining this amount of data from electrophysiological recordings is not feasible because it would correspond to more than 7 months of single-unit recordings. We therefore decided to base our comparison on responses generated by a standard computational model, fitted with empirical data, which has been shown to produce realistic responses (‘Materials and methods’).</p><p>First, we sampled many cells from a distribution of BD vs best frequency (BF) (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, left). For guinea pigs, the distribution was defined by the measured mean and variance of BD as a function of BF (<xref ref-type="bibr" rid="bib47">McAlpine et al., 2001</xref>). For cats, we fitted a distribution to a set of measurements of BDs and BFs in 192 cells (the source data was in terms of characteristic frequency rather than BF, but these are equivalent for the linear model used here) (<xref ref-type="bibr" rid="bib33">Joris et al., 2006</xref>). The cells are then modeled as generalized cross-correlators with an internal delay BD (<xref ref-type="bibr" rid="bib69">Yin et al., 1987</xref>) (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, right). <xref ref-type="fig" rid="fig1">Figure 1B</xref> illustrates the details of the model. We first model the acoustical propagation of the sound to the two ears. In the first part of this study, we consider only fixed ITDs, ignoring diffraction effects. In the second part, we move toward more realistic cues, using measured head-related transfer functions (HRTFs). The signals are then band-pass filtered around the cell’s BF using gammatone filters and normalized (representing the saturation seen in bushy cells; <xref ref-type="bibr" rid="bib37">Kuenzel et al., 2011</xref>) and then crosscorrelated with an internal delay equal to the cell’s BD (‘Materials and methods’). The result is the firing rate of the cell, and we generate spikes with Poisson statistics. <xref ref-type="fig" rid="fig1">Figure 1C</xref> displays the responses of 480 cells of the guinea pig model to white noise (left column) and to a natural sound (right column) at two different ITDs (top and bottom). We will then estimate the sound’s ITD from these population responses, using various decoders.<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.01312.003</object-id><label>Figure 1.</label><caption><title>Overview of model.</title><p>(<bold>A</bold>) The distribution of best delay vs best frequency for cells in the guinea pig model (left), with the physiological range of ITDs shown in gray, and a sample tuning curve (right). (<bold>B</bold>) Illustration of the model: a sound source is filtered via position-dependent HRTFs to give left and right channels. For each best frequency on each channel, the signal undergoes cochlear filtering using gammatone filters. An internal delay is added, and the two channels are combined and sent to a binaural neuron model that produces Poisson distributed spikes. (<bold>C</bold>) The response of the cells to sounds at two different ITDs (rows) for white noise (left column) and a natural sound (right column). The ITD is indicated by the black dashed line. Each cell is surrounded by a disc with a color indicating the response of that neuron (hot colors corresponding to strong responses). When two or more discs overlap, each point is colored according to the closest cell. The strongest responses lie along the line of the ITD.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.003">http://dx.doi.org/10.7554/eLife.01312.003</ext-link></p></caption><graphic xlink:href="elife-01312-fig1-v1.tif"/></fig></p><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> illustrates the peak and hemispheric decoders. A 100-ms sound is presented at 200 µs ITD. The peak decoder picks the most active cell and reports its BD as the estimated ITD. We observe already that although we chose cells with BFs in a narrow frequency band (640–760 Hz), the peak decoder performs poorly because of the noise in spiking. Therefore, we introduce a smoothed peak decoder. We first discard the information about BF, and simply consider the spike count of cells as a function of their BD. This relationship is smoothed to reduce noise, and we take the BD at the peak of the smoothed curve (one of the possible variations of crosscorrelation models; <xref ref-type="bibr" rid="bib58">Stern and Trahiotis, 1995</xref>). Smoothing could be neurally implemented by pooling the activity of cells with similar tuning. This decoder is less noisy. Finally, we consider the hemispheric decoder, in which we also discard information about BD in each hemisphere. To simplify, we define each hemisphere as the set of cells with BDs of the same sign. We calculate the total spike count for each hemisphere (yellow and orange rectangles) and compute the difference, normalized by the total activity. This gives a value between −1 and 1, the hemispheric difference, which varies systematically with ITD (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right). Therefore, from the observation of the difference, one can invert the relationship and infer the ITD. Note, however, that this relationship depends on the frequency band in which the hemispheric difference is computed. In blue, cells are picked with BFs around 700 Hz and the hemispheric difference varies almost linearly with ITD. In purple, cells are picked with BFs around 1300 Hz and the curve is a sigmoid. More importantly, ambiguities start to occur at these high BFs: for example, a hemispheric difference of −0.8 is consistent with ITDs of both 100 and 300 µs. This occurs when the physiological range of ITD represents more than one period of the auditory filter’s center frequency.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.01312.004</object-id><label>Figure 2.</label><caption><title>Decoders in single frequency bands.</title><p>(<bold>A</bold>) Peak and hemispheric decoders. Left: response of binaural neurons to sound at ITD = 0.2 ms (dashed line), in a narrow frequency band. The size of points is proportionate to spike count, and the crossed point corresponds to the highest spike count. Middle: the same cell responses are displayed as best delay vs spike count (note the different horizontal axis). The solid black line is the Gaussian smoothed spike count, whose peak (circle) is the ITD estimate. The maximally responsive neuron is also indicated with a circle for comparison. The yellow and orange bars give the mean response of neurons with positive and negative best delays, respectively, from which the normalized hemispheric difference is computed. Right: the hemispheric difference as a function of ITD at 700 Hz (blue) and 1.3 kHz (purple). At 1.3 kHz, the difference shown by the dashed line gives an ambiguous estimate of the ITD. (<bold>B</bold>) Mean error for the guinea pig and cat, for the peak (blue, dashed), smoothed peak (blue, solid), hemispheric (red), and pattern match (green) decoders. The distribution of BD vs BF is shown in the inset. (<bold>C</bold>) Illustration of the pattern match decoder and a neural circuit that implements it. The response (left) is compared to two patterns A and B, corresponding to two different ITDs (right). Each response neuron is connected to a pattern-sensitive neuron with weights proportional to the stored response of each pattern. When the weights match the responses, the output of the pattern-sensitive neuron is strongest.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.004">http://dx.doi.org/10.7554/eLife.01312.004</ext-link></p></caption><graphic xlink:href="elife-01312-fig2-v1.tif"/></fig></p><p>We now systematically test the estimation error of these three decoders for cells whose BFs are in narrow frequency bands within the range 100–1500 Hz (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Stimuli are white noise bursts lasting 100 ms. For the hemispheric decoder, we use the hemispheric difference curve calculated in the same frequency band in which it is tested. Thus, there is a specific decoder for each frequency band tested, which is the most favorable scenario. As expected, the peak decoder performs very poorly, both for the guinea pig and the cat models. The two animal models differed by the BD distributions and by the physiological range of ITDs (300 µs for the guinea pig model, <xref ref-type="bibr" rid="bib57">Sterbing et al., 2003</xref>; 450 µs for the cat model, <xref ref-type="bibr" rid="bib61">Tollin and Koka, 2009a</xref>). Using the smoothed peak decoder improves substantially on these results. The hemispheric decoder performs better than both decoders for the guinea pig model at all frequencies, but for the cat, it is only better than the smoothed peak decoder for frequencies below 600 Hz. Thus, it appears that even for this simple scenario, the hemispheric decoder is a very poor decoder of ITD for the cat model, except at very low frequency. The fact that the estimation error of the hemispheric decoder starts increasing at a lower frequency for the cat than for the guinea pig model was expected based on the larger head size of the cat (<xref ref-type="bibr" rid="bib26">Harper and McAlpine, 2004</xref>).</p><p>The reasons for the limitations of the different decoders are simple. Because the peak decoder selects a single cell, its estimation error reflects the level of noise in individual responses, which is high. The smoothed decoder improves on this matter, but still mostly uses the responses of cells with similar tuning. In addition, at low frequencies, both estimators rely on the responses of a small pool of cells with BDs inside the physiological range. The hemispheric decoder sums all responses, which reduces noise but also discards all information about BF and BD.</p><p>We introduce the pattern match decoder, a simple decoder that addresses both problems (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We calculate the average response of cells to sounds presented at each ITD to be identified. This population response, which we call a pattern, is stored in a vector (w<sub>1</sub>, …, w<sub>n</sub>), normalized to have length 1. When a sound is presented, the cell responses are compared with the patterns by computing a normalized dot product between the responses and the patterns, varying between 0 (perfectly dissimilar) and 1 (perfectly similar) (‘Materials and methods’ for formulae). This can be implemented by a single-layer neural network in which the output neurons encode the preferred ITD and the synaptic weights represent the patterns. The reported ITD is the ITD associated to the most similar pattern, that is, with the highest dot product.</p><p><xref ref-type="fig" rid="fig2">Figure 2B</xref> also shows the performance of the pattern matching decoder. As for the hemispheric decoder, patterns were computed in the same frequency band in which the decoder is tested. The pattern match decoder performs better than both the other decoders, both for the guinea pig and the cat models. The difference with the hemispheric decoder is very large for the cat model, but for the guinea pig model, it only starts being substantial above 1 kHz. The pattern match decoder combines the advantages of the hemispheric and peak decoders: it averages spiking noise over all cells, but it still uses individual information about BF and BD. The purpose of introducing this decoder is not to suggest that the auditory system extracts information about sound location in this exact way, but rather to estimate how much information can be obtained from the heterogeneous responses of these neurons. We also tested several other standard decoders from machine learning, including optimal linear decoding, maximum likelihood estimation, and nearest neighbor regression, but the pattern match decoder outperformed them in all cases and so we do not present the results of these decoders here (although see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for a sample of these results).</p></sec><sec id="s2-2"><title>Integrating information across frequency</title><p>The estimation task in <xref ref-type="fig" rid="fig2">Figure 2</xref> was very simple because we trained and tested the decoders in the same narrow frequency bands. In <xref ref-type="fig" rid="fig3">Figure 3</xref>, we investigate the issue of frequency integration. All decoders are now trained with white noise at various ITDs, considering all cells with BFs between 100 Hz and 1.5 kHz. For the hemispheric decoder, this means that we pool the responses of all cells in the same hemisphere, for all BFs, and we use a single broadband hemispheric difference curve to estimate ITDs. Decoder performance is then tested with white noise. For this more realistic task, it appears that the error made by the pattern match decoder is about half the error of the hemispheric decoder for the guinea pig model. For the cat models, this difference is even larger. In fact, it turns out that for the cat, the smoothed peak decoder performs better than the hemispheric decoder. To understand why, we now test the decoders on band-pass noises, as a function of the center frequency, while the decoders are still trained with broadband noise (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). This test addresses the robustness of these decoders to changes in sound spectrum. We make two observations. First, all decoders perform much worse than when decoders are trained and tested in the same frequency bands (compare with <xref ref-type="fig" rid="fig2">Figure 2B</xref>; the unsmoothed peak decoder performs very poorly and is not shown). This means that frequency integration is indeed an issue. Second, the hemispheric decoder performs worse than the two other decoders above 700 Hz for the guinea pig models and above 500 Hz for the cat. This was expected for two reasons: (1) the hemispheric difference is ambiguous at high frequency (above about 1200 Hz for both animals), and (2) the hemispheric difference depends not only on ITD but also on frequency (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right). We attempt to solve the first problem by discarding all cells with BF higher than a specified cutoff frequency (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Performance is tested again with white noise. Both for the guinea pig and the cat models, the error of the hemispheric decoder starts increasing when cells with BF above 1.2 kHz are included. For this reason and because the hemispheric difference becomes ambiguous above 1.2 kHz in both models, we restrict to cells with BF &lt;1.2 kHz in the rest of this study. We note, however, that the error of the pattern match decoder continues decreasing as cells with high frequency are added.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.01312.005</object-id><label>Figure 3.</label><caption><title>Integration across frequencies.</title><p>(<bold>A</bold>) Mean error in estimating ITD for white noise using the smoothed peak (blue), hemispheric (red), and pattern match (green) decoders, as a function of the number of binaural cells. Training and testing of the decoders are both performed using white noise. (<bold>B</bold>) Mean error as a function of frequency band when decoders are trained on white noise but tested on band-pass noise centered at the given frequency. Notice the different vertical scale between (<bold>A</bold> and <bold>B</bold>). (<bold>C</bold>) Performance when cells with a frequency above the cutoff are discarded. (<bold>D</bold>) Mean error and bias to center in the decoders for guinea pig (with a maximum frequency of 1.2 kHz) when trained on white noise and tested on colored noise.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.005">http://dx.doi.org/10.7554/eLife.01312.005</ext-link></p></caption><graphic xlink:href="elife-01312-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.01312.006</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Comparison with standard machine learning decoders.</title><p>As for <xref ref-type="fig" rid="fig3">Figure 3</xref>, except that the main decoders are shown in light colors, and there are two additional decoders: ridge regression (magenta) and nearest neighbor regression (black).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.006">http://dx.doi.org/10.7554/eLife.01312.006</ext-link></p></caption><graphic xlink:href="elife-01312-fig3-figsupp1-v1.tif"/></fig></fig-group></p><p>We have seen that estimation error depends on the frequency band of the presented sound (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). This observation extends to broadband sounds that vary in spectrum. We tested the estimation performance for the guinea pig models when the decoders are trained on white noise and tested with 1/<italic>f</italic><sup>α</sup> noise: from white noise (α = 0) to pink (α = 1) and brown (α = 2) (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). The hemispheric decoder is not robust to changes in spectrum, as the error increases with noise color α. The pattern match decoder shows the same trend, but the error remains constant on a larger range. The error of the smoothed peak decoder does not depend on sound spectrum. The main reason for the lack of robustness of the hemispheric decoder is shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref> (bottom). As α increases, the estimate of the ITD becomes more and more biased to the center. This is because with high α, every cell receives more low frequencies than high frequencies compared to the white noise case, and therefore the hemispheric difference curve changes and becomes flatter (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right). Note that this happens not by the recruitment of more low-frequency cells but also by the change in the hemispheric difference for all cells.</p><p>We now attempt to improve frequency integration in the hemispheric decoder by taking into account the change in hemispheric difference with frequency (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The ITD tuning curves have different shapes, depending on the cell’s BF (left). As a result, the hemispheric difference in each frequency band varies with the center frequency (middle). The curves are shallower in low frequency and sharper in high frequency (right). In fact, the slope is expected to be proportional to frequency: the hemispheric difference is determined by the interaural phase difference, which is the product of frequency and ITD. Therefore, we fix this issue by normalizing the cell responses by their BF in the calculation of the hemispheric difference (‘Materials and methods’). This produces hemispheric differences with similar slopes in all frequency bands. Note that there are constant biases due to the fact that the cells’ BDs are not exactly symmetrical between the two hemispheres (only their distribution is).<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.01312.007</object-id><label>Figure 4.</label><caption><title>Frequency-dependent improvements.</title><p>(<bold>A</bold>) Comparing hemispheric differences across frequency channels. In each plot, color indicates frequency with red being high frequency and blue being low frequency. Left: tuning curves for a few binaural neurons. Middle: hemispheric difference (L − R)/(L + R). Right: frequency-dependent hemispheric difference (1/<italic>f</italic>) (L − R)/(L + R). (<bold>B</bold>) Mean error as a function of frequency band in the guinea pig model, for hemispheric (red) and pattern match (green) decoders (dashed lines), and frequency-dependent hemispheric and pattern match (solid) decoders. The shaded regions show the difference between the simple and frequency-dependent versions. The dotted lines show the mean error for band-pass noise if the decoder is trained and tested on the same frequency band, as shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref> (guinea pig). This represents the lower bound for the error.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.007">http://dx.doi.org/10.7554/eLife.01312.007</ext-link></p></caption><graphic xlink:href="elife-01312-fig4-v1.tif"/></fig></p><p>This frequency-dependent correction indeed improves ITD estimation when the decoder is trained on broadband noise and on band-passed noise (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). However, the error still remains higher than in the simple case when the decoder is trained and tested in the same frequency band. In the same way, we improved frequency integration for the pattern match decoder by calculating intermediate estimates in each frequency band and combining the results (‘Materials and methods’). This correction improves the performance above 600 Hz, where it is close to the performance obtained in the simple case. In the remainder of this study, we only consider these two frequency-corrected decoders.</p></sec><sec id="s2-3"><title>Background noise and sound diffraction</title><p>We then test the decoders in increasingly realistic situations. First, we consider the effect of background noise on performance (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Interaural correlation is decreased by adding dichotic background noise to the binaural signals, and the estimation error is computed as a function of signal-to-noise ratio (SNR). All decoders were trained in quiet. In all cases, the pattern match decoder performs best, but for the guinea pig models, it substantially outperforms the hemispheric decoder at low SNR, whereas it showed similar performance in quiet. Interestingly, the smoothed peak decoder also outperforms the hemispheric decoder at SNR below 10 dB, both for the guinea pig and the cat models. Indeed, although this decoder performs worst in quiet, it proves more robust than the hemispheric decoder. The poor performance of the hemispheric decoder can again be accounted for by a bias problem. At high SNR, the hemispheric difference curves become shallower, which implies that ITD estimation is biased toward the center. The problem is less present for the pattern match decoder. Note that the smoothed peak decoder tends to be biased away from the center. This is simply because BDs are more represented away from the center.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.01312.008</object-id><label>Figure 5.</label><caption><title>Background acoustic noise.</title><p>(<bold>A</bold>) Illustration of protocol: a binaural sound is presented with a given ITD, with independent white noise added to the two channels. (<bold>B</bold>) Mean error (left column) and central bias (right column) at different signal to noise levels. Decoders are smoothed peak (blue), hemispheric (red), and pattern match (green).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.008">http://dx.doi.org/10.7554/eLife.01312.008</ext-link></p></caption><graphic xlink:href="elife-01312-fig5-v1.tif"/></fig></p><p>Previously, we considered a simplistic model of sound propagation, in which sounds are simply delayed. In reality, sounds are diffracted by the head. A better description of this process is that sounds arriving at the two ears are two filtered versions of the original sound, with filters depending on source direction. These are called HRTFs and can be measured in anechoic chambers. We measured high-resolution HRTFs of a stuffed guinea pig in a natural posture (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). It is known that diffraction produces ITDs that depend on frequency for the same source direction, with larger ITDs in low frequency (<xref ref-type="bibr" rid="bib38">Kuhn, 1977</xref>). We find the same pattern in our measurements (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), and the range of ITDs is similar to previously reported measurements in live guinea pigs (<xref ref-type="bibr" rid="bib57">Sterbing et al., 2003</xref>). For the cat model, we used HRTFs measured in an anesthetized cat (<xref ref-type="bibr" rid="bib61">Tollin and Koka, 2009a</xref>).<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.01312.009</object-id><label>Figure 6.</label><caption><title>Realistic head-related transfer functions.</title><p>(<bold>A</bold>) Photograph of stuffed guinea pig used for HRTF recordings, and three pairs of left/right ear impulse responses corresponding to the directions marked on the photograph. (<bold>B</bold>) Frequency dependence of ITD for the three azimuths shown in panel (<bold>A</bold>), in guinea pig and cat HRTFs. (<bold>C</bold>) Mean response of the model to white noise stimuli at the same azimuth (90°) for both animals, the frequency-dependent ITD curve is shown for this azimuth (dashed). (<bold>D</bold>) Performance of the model as a function of the number of cells for hemispheric (red) and pattern match (green) decoders.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.009">http://dx.doi.org/10.7554/eLife.01312.009</ext-link></p></caption><graphic xlink:href="elife-01312-fig6-v1.tif"/></fig></p><p><xref ref-type="fig" rid="fig6">Figure 6C</xref> displays the cell responses for sounds presented at 90° azimuth, where we used the HRTFs to filter the sound in an acoustically realistic way. We then test the estimation error in azimuth, rather than in ITD, for white noise presented in quiet (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). For both animals, the pattern match decoder is substantially better than the hemispheric decoder. Indeed, since the hemispheric decoder discards all information about BF and BD, it cannot take advantage of the frequency variation of ITDs, whereas the pattern match decoder does. The difference is particularly striking for the cat due to its larger head size.</p></sec><sec id="s2-4"><title>Tuning heterogeneity as information</title><p>We have argued that the hemispheric decoder performs poorly because it discards the information present in the heterogeneity of ITD tunings of the cells. We demonstrate this point in <xref ref-type="fig" rid="fig7">Figure 7A</xref> by varying the amount of heterogeneity in the BDs of the guinea pig model. The standard deviation of the BD is multiplied by a ‘spread factor’: below 1, the BD distribution is less heterogeneous than in the original distribution; above 1, it is more heterogeneous. We then test the estimation error for white noise as a function of spread factor. When the BDs are less heterogeneous, there is little difference between the performance of the hemispheric and pattern match decoder. But as heterogeneity increases, the pattern match decoder performs better, whereas the hemispheric decoder shows little change in performance. Therefore, heterogeneity of tunings is indeed useful to estimate the ITD, and pooling the responses discards this information. Performance remains stable when the distribution is made more heterogeneous than the actual distribution (spread factor &gt;1).<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.01312.010</object-id><label>Figure 7.</label><caption><title>Effect of heterogeneity and lesions.</title><p>(<bold>A</bold>) Mean error for the hemispheric (red) and pattern match (green) decoders in the guinea pig model, depending on the spread of the best delays, for white noise presented with acoustic noise (SNR between −5 and 5 dB, no HRTF filtering). For every frequency, the standard deviation of BDs is multiplied by the spread factor: lower than 1 denotes less heterogeneous than the original distribution (top left), greater than 1 denotes more heterogeneous (top right). Dashed lines represent the estimation error for the original distribution. (<bold>B</bold>) Mean error for the pattern match (green) and smooth peak (blue) decoders before (dashed) and after (solid) lesioning one hemisphere in the guinea pig, as a function of presented ITD. The model is retrained after lesioning. The error curves are Gaussian smoothed to reduce noise and improve readability.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.010">http://dx.doi.org/10.7554/eLife.01312.010</ext-link></p></caption><graphic xlink:href="elife-01312-fig7-v1.tif"/></fig></p></sec><sec id="s2-5"><title>Effect of lesions</title><p>Lesion studies in cats (<xref ref-type="bibr" rid="bib32">Jenkins and Masterton, 1982</xref>) and in humans (<xref ref-type="bibr" rid="bib42">Litovsky et al., 2002</xref>) show that when one inferior colliculus is removed, the sound localization performance in the contralateral field drops but remains almost intact in the ipsilateral field. This is not compatible with an ITD estimation based on the comparison between the activity of the two hemispheres. We simulated a hemispheric lesion in the pattern match and smoothed peak decoders (<xref ref-type="fig" rid="fig7">Figure 7B</xref>), by removing all cells with negative BDs. The performance for positive ITDs is essentially unchanged, whereas it is highly degraded for negative ITDs, especially for the smoothed peak decoder. Lesion data indicate that sound localization performance is greatly degraded in the contralateral hemifield, but not completely abolished, which would discard the smoothed peak decoder—although lesions might not have been complete, and those were free-field experiments involving other cues than ITD.</p></sec><sec id="s2-6"><title>Owls and humans</title><p>Finally, we test the estimation performance in barn owls and humans, for white noise filtered through measured HRTFs (‘Materials and methods’) (<xref ref-type="fig" rid="fig8">Figure 8</xref>). For barn owls, we used a previously measured distribution of BD vs BF (<xref ref-type="bibr" rid="bib64">Wagner et al., 2007</xref>). Barn owls are sensitive to ITDs in very high frequency (about 2–8 kHz), and therefore, as expected, the hemispheric decoder performs very badly compared to the pattern match decoder (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). For humans, the distribution of BD vs BF is unknown. Some indirect evidence from MRI and EEG studies suggests that BD is not uniformly distributed (<xref ref-type="bibr" rid="bib60">Thompson et al., 2006</xref>; <xref ref-type="bibr" rid="bib5">Briley et al., 2013</xref>). We tested three possibilities: uniformly distributed BD within the pi-limit, similar to what is observed in birds (<xref ref-type="fig" rid="fig8">Figure 8B</xref>), BD distribution of the guinea pig model (<xref ref-type="fig" rid="fig8">Figure 8C</xref>), and BD distribution of the cat model (<xref ref-type="fig" rid="fig8">Figure 8D</xref>). In all cases, the estimation error of the hemispheric decoder is very high, an order of magnitude larger than human sound localization acuity (on the order of 3°) (<xref ref-type="bibr" rid="bib7">Carlile et al., 1997</xref>).<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.01312.011</object-id><label>Figure 8.</label><caption><title>Humans and owls.</title><p>(<bold>A</bold>) Mean error for the pattern match (green) and hemispheric (red) decoders for the barn owl model, with sounds presented through measured HRTFs. (<bold>B</bold>) Performance in the human model with uniformly distributed best interaural phase differences. (<bold>C</bold>) Performance in the human model with best delays distributed as in the guinea pig model. (<bold>D</bold>) Performance in the human model with best delays distributed as in the cat model.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.011">http://dx.doi.org/10.7554/eLife.01312.011</ext-link></p></caption><graphic xlink:href="elife-01312-fig8-v1.tif"/></fig></p></sec><sec id="s2-7"><title>Comparison with behavioral performance</title><p>Our results can be compared with behavioral performance measured in psychophysical experiments. One may immediately object that the pattern decoder is in fact too accurate compared to natural performance, in particular for humans (<xref ref-type="fig" rid="fig8">Figure 8</xref>). Therefore, we must stress again that the performance obtained by a decoder in a given context is always overestimated, compared to the same decoder adapted to a more general context, ‘even when tested with the same sounds’. To give an example, all decoders are much more accurate when trained and tested on a single narrow frequency band (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) than when trained with broadband sounds and tested with exactly the same narrowband sounds (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Additional imprecision is introduced by other uncontrolled sources of variability in the sounds, many of which we have not considered:<list list-type="simple"><list-item><p>1. Sound locations differ not only by azimuth but also elevation and distance, both of which impact binaural cues</p></list-item><list-item><p>2. Reflections on the ground and objects impact ITDs (<xref ref-type="bibr" rid="bib23">Gourévitch and Brette, 2012</xref>)</p></list-item><list-item><p>3. There are often multiple sound sources</p></list-item><list-item><p>4. Sound sources are generally not point sources and are directional</p></list-item><list-item><p>5. Natural sounds have widely diverse spectrotemporal properties</p></list-item></list></p><p>A sound localization system adapted for the full range of ecological variability necessarily performs worse in a narrower range of conditions than a system specifically optimized for that narrow range. In addition, acoustical cues must be associated with sound location through some feedback mechanism, and therefore sound localization acuity is constrained by the precision of this feedback. Indeed a comparative study across 23 mammalian species shows that sound localization acuity is best explained by the width of the field of best vision (<xref ref-type="bibr" rid="bib30">Heffner and Heffner, 1992</xref>). Therefore, our model results should be understood as a lower bound for the accuracy of these decoders. In particular, the poor performance of the hemispheric decoder is actually optimistic, all the more so as we made specific efforts to enhance it by taking into account nonlinearities (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) and by applying frequency-dependent corrections (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>Most psychophysical studies in animals have focused on the measurement of the minimum audible angle (MAA), which is a discrimination threshold for sources near the midline. In cats, the MAA is about 5° for broadband noises longer than 40 ms (defined as the speaker separation given 75% correct responses) (<xref ref-type="bibr" rid="bib8">Casseday and Neff, 1973</xref>; <xref ref-type="bibr" rid="bib28">Heffner and Heffner, 1988a</xref>). <xref ref-type="bibr" rid="bib63">Tollin et al. (2005)</xref> measured accuracy in an absolute localization study in the −25° to 25° range. When the cat’s head is unrestrained, direction estimates show little bias and the standard deviation is 3–4°, which corresponds to a mean unsigned error (the measure used in this article) of 2.4–3.2° (assuming normally distributed responses). In <xref ref-type="bibr" rid="bib50">Moore et al. (2008)</xref>, the mean unsigned error was directly reported (although only for directions near the midline) and was in the 2–4° range. In a behavioral study in which cats were trained to walk to a target speaker in the −90° to 90° range, the animals could do the task with nearly 100% accuracy, with no apparent dependence on speaker azimuth (<xref ref-type="bibr" rid="bib46">Malhotra et al., 2004</xref>)—but speakers were spaced by 15°. In <xref ref-type="fig" rid="fig6">Figure 6D</xref>, we report a mean unsigned error of 5° for the optimized hemispheric model (including frequency-dependent and nonlinear corrections; error for the pattern decoder was nearly 0°). The model was trained and tested in quiet with broadband noises, with sources constrained to the horizontal plane. Therefore, it is a very optimistic estimate, especially given that the sound localization tasks mentioned above were two-dimensional (i.e., the elevation had to be estimated as well). It could be argued that the behavioral task included additional cues, in particular interaural intensity differences, because the sounds were broadband. However, <xref ref-type="bibr" rid="bib50">Moore et al. (2008)</xref> showed for sources near the midline that sound localization accuracy in the horizontal plane is very similar for broadband and low-pass filtered noises (&lt;5 kHz), which do not include these cues.</p><p>In cats, the just noticeable difference in ITD is similar for tones of 500 Hz and 1 kHz, about 25 µs (<xref ref-type="bibr" rid="bib65">Wakeford and Robinson, 1974</xref>). The performance of the pattern decoder is generally not strongly dependent on frequency in the 500–1200 Hz range (<xref ref-type="fig" rid="fig2 fig3 fig4">Figures 2–4</xref>), whereas the performance of the hemispheric decoder consistently increases with frequency (between about 500 and 1 kHz in <xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p>Unfortunately, there are no behavioral studies in guinea pigs. In the gerbil, another small mammal with low-frequency hearing, the MAA is 27° (<xref ref-type="bibr" rid="bib29">Heffner and Heffner, 1988b</xref>). This makes sound localization acuity in gerbils one of the worst of all mammalian species in which it has been measured (<xref ref-type="bibr" rid="bib30">Heffner and Heffner, 1992</xref>). Given that the maximum ITD is about 120 µs (<xref ref-type="bibr" rid="bib44">Maki and Furukawa, 2005</xref>), the threshold ITD should be about 54 µs (using Kuhn’s formula; <xref ref-type="bibr" rid="bib38">Kuhn, 1977</xref>). Given that this threshold is so high, and in the absence of absolute localization studies in these two species, it is difficult to discard any model on the basis of the existing behavioral data alone. We note however that, for a given accuracy, the hemispheric decoder requires many more neurons than the pattern match decoder (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><p>In owls, ITD is a cue to azimuth, whereas interaural level difference is a cue to elevation (<xref ref-type="bibr" rid="bib59">Takahashi et al., 1984</xref>; <xref ref-type="bibr" rid="bib49">Moiseff, 1989</xref>). We found that the mean unsigned error with the hemispheric decoder was greater than 30°, when trained and tested in quiet with HRTFs (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). Behaviorally, barn owls localize broadband sounds with azimuthal error smaller than about 10° at all azimuths (<xref ref-type="bibr" rid="bib34">Knudsen et al., 1979</xref>), and a large part of this error is due to an underestimate of eccentric azimuths that can be accounted for by a prior for frontal directions (<xref ref-type="bibr" rid="bib16">Fischer and Peña, 2011</xref>). In humans, behavioral estimates of azimuth are largely dominated by low-frequency ITDs (<xref ref-type="bibr" rid="bib66">Wightman and Kistler, 1992</xref>), and the mean unsigned error with broadband noise bursts (open-loop condition) is about 5° in the frontal hemifield (2–10° depending on azimuth) in a two-dimensional absolute localization task (<xref ref-type="bibr" rid="bib45">Makous and Middlebrooks, 1990</xref>). In contrast, the hemispheric decoder has an average error of about 10° in the most favorable scenario (<xref ref-type="fig" rid="fig8">Figure 8B</xref>), although sources are constrained to the horizontal plane.</p><p>In summary, our results with the hemispheric decoder appear inconsistent with behavioral data for cats, humans, and owls. In contrast, the results obtained by the pattern match decoder imply that there is enough information in the activity of binaural neurons to account for the sound localization accuracy of these species. There is insufficient behavioral data in the guinea pig model to distinguish between different decoders.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>There are two major theories of ITD processing in mammals. One theory, initially proposed by <xref ref-type="bibr" rid="bib31">Jeffress (1948)</xref>, asserts that ITD is represented in the activity pattern of neurons with heterogeneous tunings to ITD. A more recent theory claims that ITD is represented by the relative activity of the two MSOs, irrespective of the tunings of the cells (<xref ref-type="bibr" rid="bib24">Grothe et al., 2010</xref>). We compared different ways of extracting information about sound location from the responses of a model population of binaural neurons constrained by electrophysiological data and acoustical recordings, to a large variety of sounds—which would be infeasible with single-unit recordings in animals and impossible in humans. Our results demonstrate that, although a labeled line code for ITD—the most literal interpretation of the Jeffress model—is too inefficient, summing the activity in each hemisphere discards too much of the information that is present in neural activity patterns. In addition, the heterogeneity of ITD tunings is important for decoding performance, rather than being meaningless variability (<xref ref-type="fig" rid="fig7">Figure 7</xref>). This loss of information is large enough that the hemispheric decoder cannot account for behavioral performance measured in cats and humans, although we improved it by taking into account nonlinearities (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) and by applying frequency-dependent corrections (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The critical flaw lies in the fact that an estimate of ITD based on global hemispheric activity is not robust to changes in sound properties other than ITD.</p><sec id="s3-1"><title>Optimal coding of ITD</title><p>Our results appear to contradict the previous studies showing that hemispheric codes for ITD are optimal (<xref ref-type="bibr" rid="bib26">Harper and McAlpine, 2004</xref>) and that response patterns do not provide more information than simply summing (<xref ref-type="bibr" rid="bib41">Lesica et al., 2010</xref>; <xref ref-type="bibr" rid="bib43">Lüling et al., 2011</xref>). However, these studies focused on a simple task, in which only the ITD was allowed to vary. This is an elementary task for a decoder because any variation in the pattern of responses can be attributed to a change in sound location. It is much more difficult to estimate location independently of irrelevant dimensions found in ecological situations, such as level, spectrum, and background noise. This point is related to the concept of ‘overfitting’ in statistical learning theory: an estimator may be very accurate when trained and tested with the same data, while in fact very poor when tested on new data. This is precisely what happens with the hemispheric decoder. When tested with the same sounds used to calibrate the decoder, its performance is indeed very good for the guinea pig model (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), consistently with previous results. However, when the decoder is calibrated for broadband sounds and tested with the same sounds as before, performance degrades drastically (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Thus, our results directly demonstrate that indiscriminate pooling of the activity in each hemisphere is a poor way to decode information about sound location. <xref ref-type="fig" rid="fig7">Figure 7A</xref> also directly contradicts the claim that the optimal code for ITD consists of two populations of identically tuned neurons (<xref ref-type="bibr" rid="bib26">Harper and McAlpine, 2004</xref>). On the contrary, heterogeneity of tunings is critical for robust estimation, consistently with theoretical arguments (<xref ref-type="bibr" rid="bib4">Brette, 2010</xref>).</p><p>The discrepancy with previous arguments in favor of hemispheric or ‘slope’ coding seems to stem from a confusion between accuracy and acuity (<xref ref-type="bibr" rid="bib27">Heffner and Heffner, 2005</xref>; <xref ref-type="bibr" rid="bib63">Tollin et al., 2005</xref>): accuracy measures how well one can estimate the correct value (absolute localization); acuity measures how easily one can distinguish between two values (discrimination). Acuity can be directly related to ITD sensitivity of neural responses (favoring a ‘slope code’), but accuracy is in fact the relevant ecological concept for the animal.</p><p>It may be objected that the focus on optimality may be irrelevant because animals only need to be accurate enough, given the ecologically relevant tasks. However, our results also imply that for a given level of accuracy, the hemispheric decoder requires many more neurons than the pattern match decoder (<xref ref-type="fig" rid="fig3 fig6">Figures 3A and 6D</xref>), and therefore it is energetically inefficient. Although there may be little evolutionary pressure for very accurate sound localization in some species, the same argument does not apply to energy consumption in the brain (<xref ref-type="bibr" rid="bib3">Attwell and Laughlin, 2001</xref>). Nevertheless, there are also physiological constraints on the way information can be extracted, which we examine in the section ‘Physiological mechanisms’ below.</p></sec><sec id="s3-2"><title>Correlations and coding</title><p>It is known that the structure of neural correlations can be critical to optimal decoding. There are different sources of correlations: anatomical divergence (overlap in the sets of presynaptic neurons), shared variability due to feedback or lateral connections, and stimulus-dependent variability (e.g., changes in level). In the medial superior olive (MSO), the earliest nucleus with ITD-sensitive neurons, correlations due to anatomical divergence are probably limited because frequencies are narrowly tuned in these binaural neurons and receive inputs from few monaural neurons (<xref ref-type="bibr" rid="bib10">Couchman et al., 2010</xref>). The second source of correlations is also likely to be weak because there are no identified lateral connections within the MSO and little evidence of feedback connections, although GABAergic receptors have been recently characterized (<xref ref-type="bibr" rid="bib11">Couchman et al., 2012</xref>). Thus, neural correlations in this early sound localization circuit are presumably mainly due to the shared acoustic stimulus.</p><p>When these correlations are neglected and neurons are assumed to fire independently, conditionally to the ITD, then the optimal code is the most sensitive one (<xref ref-type="bibr" rid="bib26">Harper and McAlpine, 2004</xref>), and reliable estimates can be obtained by simply pooling the estimates obtained from individual responses. This conclusion is wrong in general when there are stimulus-dependent correlations (<xref ref-type="bibr" rid="bib4">Brette, 2010</xref>). In this case, as we have shown, the structure of neural correlations contains useful information that can be exploited by simple decoders. For example, changes in various aspects of the sound induce shared variability in individual responses, which results in the same variability in pooled responses, but little variability in the relative activity of neurons (used by the pattern decoder).</p><p>Another mechanism to reduce the impact of shared stimulus-dependent variability is divisive normalization (<xref ref-type="bibr" rid="bib6">Carandini and Heeger, 2011</xref>). Level normalization was in fact included in all the models we tested, so as to focus on ITD cues (rather than interaural level differences).</p></sec><sec id="s3-3"><title>Pattern decoders</title><p>Previous studies have assessed the performance of pattern decoders in estimating sound location from the responses of ensembles of cortical neurons. These decoders included artificial neural networks using spike counts or relative spike timing (<xref ref-type="bibr" rid="bib19">Furukawa et al., 2000</xref>; <xref ref-type="bibr" rid="bib56">Stecker et al., 2005</xref>; <xref ref-type="bibr" rid="bib40">Lee and Middlebrooks, 2013</xref>) and maximum likelihood estimation (<xref ref-type="bibr" rid="bib48">Miller and Recanzone, 2009</xref>), which is close to the pattern decoder used in this study. It is generally found that good performance can be achieved provided that the set of neurons is large enough. A previous study also found good performance with an opponent channel model, similar to the hemispheric model we tested in our study (<xref ref-type="bibr" rid="bib56">Stecker et al., 2005</xref>). However, these studies tested the decoders on responses to a single type of sound (white noise), although with several levels, and as we have shown, substantial differences between the performances of decoding mechanisms only arise when sounds are allowed to vary in dimensions other than the dimension being estimated (e.g., spectrum).</p><p>In a recent experimental study, a maximum likelihood decoder was found to outperform a hemispheric decoder in estimating sound location from responses of neurons in the inferior colliculus (<xref ref-type="bibr" rid="bib12">Day and Delgutte, 2013</xref>), which is consistent with our study. However, as in previous studies, only responses to a single type of sound were used, which implies that performance in more realistic scenarios was overestimated.</p></sec><sec id="s3-4"><title>Physiological mechanisms</title><p>The pattern decoder is essentially a perceptron (<xref ref-type="bibr" rid="bib13">Dayan and Abbott, 2001</xref>): spatially tuned neurons are formed by simply pooling neural responses with different weights, and then the maximally active neuron indicates source location. These weights reflect the average activity pattern for the preferred location, and thus could be learned by standard Hebbian plasticity mechanisms. The smoothed peak decoder is essentially the same, except the weights are not learned. In the cat model, most low-frequency neurons in the central nucleus of the inferior colliculus are spatially tuned, with preferred azimuth homogeneously distributed in the contralateral hemifield (<xref ref-type="bibr" rid="bib1">Aitkin et al., 1985</xref>). These neurons receive excitatory inputs from ITD-sensitive neurons in the MSO. In addition, when one inferior colliculus is removed, sound localization performance drops only in the contralateral field. Both the pattern and smooth peak decoders are in line with these findings.</p><p>The hemispheric decoder pools neural activity from each hemisphere, and then calculates the normalized difference, which are both simple operations. However, two remarks are in order. First, contrary to the other decoders, the presence of both functional hemispheres is required to estimate sound direction in either hemifield. Second, these operations produce a graded estimate of sound direction, not a spatially tuned response. Therefore, producing spatially tuned responses requires an additional step, with neurons tuned to a specific ratio of hemispheric activity. The firing rate of these neurons must then depend nonmonotonically on the activity of each side. Thus, the hemispheric decoder appears more complex, in terms of neural circuitry, than any of the other decoders. It could be argued that creating spatially tuned responses is in fact not necessary for sound localization behavior. For example, movements toward the sound source could be generated with activity in the two hemispheres controlling opposite muscles (<xref ref-type="bibr" rid="bib25">Hancock and Delgutte, 2004</xref>). However, in addition to the fact that there are spatially tuned neurons in the inferior colliculus (<xref ref-type="bibr" rid="bib1">Aitkin et al., 1985</xref>), this idea does not fit with what is known of the physiology of eye movements. Cats orient their gaze toward a briefly presented sound, a behavioral response that has been used to measure sound localization accuracy (<xref ref-type="bibr" rid="bib63">Tollin et al., 2005</xref>). Eye movements are controlled by neurons in the superior colliculus (SC), which form a map (a ‘place code’): stimulation of neurons in the SC produces saccades whose amplitude and direction depend on the site of stimulation, but not on intensity or frequency of stimulation (<xref ref-type="bibr" rid="bib55">Sparks and Nelson, 1987</xref>). Some of these neurons are tuned to sound location (<xref ref-type="bibr" rid="bib52">Populin et al., 2004</xref>).</p><p>The anatomy and physiology of the ITD processing pathway are very similar across mammalian species (<xref ref-type="bibr" rid="bib24">Grothe et al., 2010</xref>). However, while hemispheric decoding might be consistent with behavioral data in small mammals, it is not with data in cats and humans. Therefore, if there is a common mechanism for ITD processing in mammals, it cannot be based on pooling neural activity on each hemisphere. A traditional argument in favor of the hemispheric model of ITD processing or ‘slope coding’ is that in small mammals, there are many binaural neurons with large BDs, which is contradictory with a labeled line code. However, it should be noted that the exact symmetrical argument applies as well: there are many binaural neurons with small BDs (within the ecological range), both in small and large mammals, which is contradictory with the slope coding hypothesis.</p></sec><sec id="s3-5"><title>Experimental predictions</title><p>Traditionally, physiological studies of ITD processing have focused on the question of sensitivity: how responses vary along the dimension to be estimated (ITD), which is typically measured by recording ITD selectivity curves. Indeed, there can be no information about ITD in responses that are insensitive to ITD. But sensitivity is only a necessary condition. To understand how ITD is extracted, one must identify those aspects of neural responses that are specific to ITD. In other words, one must analyze not only what varies with ITD but also what is invariant when properties other than ITD vary.</p><p>This point is related to the difference in behavioral studies between acuity, a measure of discriminability between stimuli, and accuracy, a more ecologically relevant measure of how well the animal can reach a target (<xref ref-type="bibr" rid="bib27">Heffner and Heffner, 2005</xref>; <xref ref-type="bibr" rid="bib63">Tollin et al., 2005</xref>). Indeed, the computationally challenging task for a sensory system is not to discriminate between two signals, but to extract meaningful information in face of the tremendous diversity of sensory inputs in ecological environments.</p><p>Such an analysis requires recording neural responses to a large variety of sounds. For practical reasons, we based our study on model responses. In principle, the same analysis could be done experimentally by recording the responses of a large number of cells to a broad set of sounds and levels presented at different locations. Given the large number of stimuli, such a study might require imaging or multielectrode recordings. An initial approach could be to look for invariant properties in the response of a subset of neurons to natural sounds presented at the same spatial location.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Response model</title><p>The basic model consists of the following stages, illustrated in <xref ref-type="fig" rid="fig1">Figure 1B</xref>.</p><p>A sound S has a location θ, which could be an ITD or azimuth. Sounds are either (i) white noise, (ii) band-passed white noise, or (iii) colored noise with a <inline-formula><mml:math id="inf1"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> spectrum with color parameter α between 0 and 2 (0 = white noise, 1 = pink noise, 2 = brown noise).</p><p>The signal received at the two ears is this sound transformed by a pair of HRTFs for that location. We consider two HRTF models: (i) no diffraction, that is, frequency-independent ITDs, and (ii) HRTFs measured in an anechoic chamber (<xref ref-type="fig" rid="fig6 fig8">Figures 6 and 8</xref>). In addition to the target sound, each ear can receive an acoustic noise (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><p>The signal received at each ear is then monaurally filtered by a gammatone filter bank (<xref ref-type="bibr" rid="bib20">Glasberg and Moore, 1990</xref>; <xref ref-type="bibr" rid="bib54">Slaney, 1993</xref>) with center frequencies and bandwidths defined by the animal model (see below). The equivalent rectangular bandwidth (ERB) Q factor of a filter is defined as <inline-formula><mml:math id="inf2"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi mathvariant="normal">ERB</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">ERB</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> where ERB(<italic>f</italic>) is the width of a rectangular filter that would pass as much power as the filter (for white noise). Following <xref ref-type="bibr" rid="bib53">Shera et al. (2002)</xref>, we use the formula <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi mathvariant="normal">ERB</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">kHz</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where α and β are parameters specific to the animal model (see below).</p><p>Each binaural neuron receives two monaurally filtered inputs, one from each side, with an internal delay defined by the animal model. The firing rate response of the binaural neuron is given by the formula <inline-formula><mml:math id="inf4"><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext>k</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula>, with a constant k defined by the animal model, and L(t) and R(t) are delayed and normalized versions of the gammatone filtered signals at the left and right ears at time t. The normalization factor is proportional to <inline-formula><mml:math id="inf5"><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext>k</mml:mtext></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mtext>k</mml:mtext></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and chosen for a target maximal firing rate F of the binaural neuron. This binaural model is a generalization of two previous models that were found to produce good fits for the delay–response curves of the guinea pig (<xref ref-type="bibr" rid="bib26">Harper and McAlpine, 2004</xref>) and owl models (<xref ref-type="bibr" rid="bib15">Fischer et al., 2008</xref>). Here, we generalized it to include cats and humans, and checked that the delay–response curves give good fits to published data for the cat (<xref ref-type="bibr" rid="bib33">Joris et al., 2006</xref>).</p><p>The result is the output response r of the binaural neuron, and the spike count is drawn from a Poisson distribution with mean r (so that r/T is the firing rate of the neuron for duration T).</p><p>All simulations were performed using the ‘Brian’ simulator (<xref ref-type="bibr" rid="bib21">Goodman and Brette, 2008</xref>, <xref ref-type="bibr" rid="bib22">2009</xref>) with the ‘Brian hears’ auditory periphery library (<xref ref-type="bibr" rid="bib18">Fontaine et al., 2011</xref>).</p></sec><sec id="s4-2"><title>Animal models</title><p>Each binaural neuron then is specified by a BF and a BD so that the left channel is delayed by BD/2 and the right channel by −BD/2. The distribution of these parameters, as well as the bandwidths for the monaural filters, is defined separately for each animal model.</p><p>For all models, we used a range of 480 BFs ERB spaced between 100 Hz and 1.5 kHz, with the exception of the cat model with HRTFs (as the recorded HRTFs were not reliable below 400 Hz) and the owl (which uses higher frequencies for ITD processing). The firing rate of the binaural neurons was calibrated to have a peak of F = 200 Hz. As firing is Poisson, smaller or larger values would only increase or decrease neuronal noise. Parameters for all models are summarised in <xref ref-type="table" rid="tbl1">Table 1</xref>.<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.01312.012</object-id><label>Table 1.</label><caption><p>Summary of animal models</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.01312.012">http://dx.doi.org/10.7554/eLife.01312.012</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Name</th><th>ITD source</th><th>ITD range, μs</th><th>Best delays (BD)</th><th>Best frequencies (BF)</th><th>α</th><th>β</th><th>k</th></tr></thead><tbody><tr><td>Guinea pig</td><td>Artificial</td><td>± 300</td><td>Measured</td><td>100–1500 Hz</td><td align="char" char=".">0.35</td><td align="char" char=".">4.0</td><td align="char" char=".">8</td></tr><tr><td>Guinea pig</td><td>HRTF</td><td>± 250</td><td>Measured</td><td>100–1500 Hz</td><td align="char" char=".">0.35</td><td align="char" char=".">4.0</td><td align="char" char=".">8</td></tr><tr><td>Cat</td><td>Artificial</td><td>± 400</td><td>Measured</td><td>100–1500 Hz</td><td align="char" char=".">0.37</td><td align="char" char=".">5.0</td><td align="char" char=".">4</td></tr><tr><td>Cat</td><td>HRTF</td><td>± 450</td><td>Measured</td><td>400–1500 Hz</td><td align="char" char=".">0.37</td><td align="char" char=".">5.0</td><td align="char" char=".">4</td></tr><tr><td>Human</td><td>HRTF</td><td>± 950</td><td>Uniform within π-limit</td><td>100–1500 Hz</td><td align="char" char=".">0.37</td><td align="char" char=".">5.0</td><td align="char" char=".">4</td></tr><tr><td>Human</td><td>HRTF</td><td>± 950</td><td>Guinea pig distribution</td><td>100–1500 Hz</td><td align="char" char=".">0.37</td><td align="char" char=".">5.0</td><td align="char" char=".">4</td></tr><tr><td>Human</td><td>HRTF</td><td>± 950</td><td>Cat distribution</td><td>100–1500 Hz</td><td align="char" char=".">0.37</td><td align="char" char=".">5.0</td><td align="char" char=".">4</td></tr><tr><td>Owl</td><td>HRTF</td><td>± 260</td><td>Measured</td><td>2–8 kHz</td><td align="char" char=".">0.50</td><td align="char" char=".">4.3</td><td align="char" char=".">2</td></tr></tbody></table></table-wrap></p><sec id="s4-2-1"><title>Guinea pig</title><p>For the artificially induced ITD model (no diffraction), we use a maximal ITD of 300 μs (<xref ref-type="bibr" rid="bib57">Sterbing et al., 2003</xref>) and a range of BDs measured from inferior colliculus of guinea pigs (<xref ref-type="bibr" rid="bib47">McAlpine et al., 2001</xref>). Given a BF, we selected a BD from a normal distribution with the measured mean and variance for that BF. The bandwidth parameters α and β were as given in <xref ref-type="bibr" rid="bib53">Shera et al. (2002)</xref>. The binaural power k was selected to match the curves in <xref ref-type="bibr" rid="bib26">Harper and McAlpine (2004)</xref>. We measured high-resolution guinea pig HRTFs from a taxidermist model from the Museum of Natural History (Paris), in an anechoic chamber covered with glass wool wedges, using the same protocol and equipment as for the LISTEN HRTF database (<ext-link ext-link-type="uri" xlink:href="http://www.ircam.fr/equipes/salles/listen/">http://www.ircam.fr/equipes/salles/listen/</ext-link>). Because of the impedance mismatch between the skin and the air, acoustical properties are essentially determined by the shape, not by the material inside the body. The taxidermist model is both still and in a natural posture, which makes it very convenient to measure reliable HRTFs. ITDs were found to be frequency dependent, a maximal ITD of 250 μs, consistent with previously reported measurements in live guinea pigs (<xref ref-type="bibr" rid="bib57">Sterbing et al., 2003</xref>).</p></sec><sec id="s4-2-2"><title>Cat</title><p>For the artificially induced ITD model, we used a maximal ITD of 400 μs (<xref ref-type="bibr" rid="bib68">Yin and Chan, 1990b</xref>) and a range of BDs measured from cat IC (<xref ref-type="bibr" rid="bib33">Joris et al., 2006</xref>). We generated a kernel density estimate (KDE) probability distribution of BD and BF from the measured set, and then for each BF, we chose a BD from the conditional KDE distribution of BD, given the BF. The measured data used characteristic frequency (CF) rather than BF; however, in a linear model such as the one used here, these two measures are equivalent. The bandwidth parameters α and β were as given in <xref ref-type="bibr" rid="bib53">Shera et al. (2002)</xref>. The binaural power k = 4 was chosen to fit the data of <xref ref-type="bibr" rid="bib33">Joris et al. (2006)</xref>, although note selecting a power of k = 2 to match the guinea pig model did not significantly alter the results. For the HRTF model, we used the HRTFs recorded by <xref ref-type="bibr" rid="bib62">Tollin and Koka (2009b)</xref>, which had a maximal ITD of 450 μs. These HRTFs were unreliable below 400 Hz, and so this model was restricted to be used between 400 Hz and 1.5 kHz.</p></sec><sec id="s4-2-3"><title>Owl</title><p>We used HRTFs and a distribution of BDs measured from barn owl IC from <xref ref-type="bibr" rid="bib64">Wagner et al. (2007)</xref>. BDs were chosen using the same procedure as in cats, with KDE estimates. The HRTFs had a maximal ITD of 260 μs. The bandwidth parameters α and β were as given in <xref ref-type="bibr" rid="bib36">Köppl (1997)</xref>. The binaural power k from <xref ref-type="bibr" rid="bib15">Fischer et al. (2008)</xref> was used. BFs from 2–8 kHz were used, as the owl is known to be ITD sensitive above 2 kHz (<xref ref-type="bibr" rid="bib9">Coles and Guppy, 1988</xref>), and the HRTFs were only accurate up to 8 kHz.</p></sec><sec id="s4-2-4"><title>Human</title><p>HRTFs from the IRCAM LISTEN database (<ext-link ext-link-type="uri" xlink:href="http://www.ircam.fr/equipes/salles/listen/">http://www.ircam.fr/equipes/salles/listen/</ext-link>) were used. These had a maximal ITD of approximately 950 μs. As the distribution of BDs in human is unknown, we used three hypothetical distributions: (i) a uniform distribution of BDs within the pi-limit, (ii) the distribution used in the guinea pig model, and (iii) the distribution used in the cat model. Bandwidth parameters and the binaural power k were as used in the cat. We tested other binaural powers and bandwidths (including the much sharper bandwidth estimates from <xref ref-type="bibr" rid="bib53">Shera et al. (2002)</xref>), but these did not significantly alter our results.</p></sec></sec><sec id="s4-3"><title>Decoders</title><p>The decoding problem is to compute an estimate <inline-formula><mml:math id="inf6"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of θ, given the vector of responses <bold>r</bold> of the binaural neurons. We define a training set and a testing set of data. The acoustical inputs can be different between the two sets, for example, training with white noise and testing with colored noise (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). The training set is used to set the parameters of the decoder, and the testing set is used to compute the errors and biases of the decoder (‘Analysis’). We consider the following decoders, all of which can be straightforwardly implemented with a simple neural circuit:</p><sec id="s4-3-1"><title>Peak decoder</title><p>The naive form of the peak decoder takes <inline-formula><mml:math id="inf7"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to be the BD of the maximally responsive neuron. We also define a smoothed form, in which the maximum is taken with respect to a Gaussian smoothed response of <bold>r</bold> defined by <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>σ</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf9"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mtext>i</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mtext>j</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <italic>w</italic> is the smoothing window width.</p></sec><sec id="s4-3-2"><title>Hemispheric decoder</title><p>A normalized hemispheric difference λ is computed as the difference between the sum of the responses of neurons with positive BDs and the sum of the responses of neurons with negative BDs divided by the sum of the responses of all the neurons. Mathematically, <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>∈</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>i</mml:mtext></mml:msub></mml:mrow></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>∉</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>i</mml:mtext></mml:msub></mml:mrow></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>i</mml:mtext></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where I is the set of neurons with positive BD. The estimation <inline-formula><mml:math id="inf11"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined by inverting the average hemispheric difference <inline-formula><mml:math id="inf12"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtext>θ</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mtext>θ</mml:mtext><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where the expectation is taken over the training data. In practice, a polynomial p(θ) is fitted to the data (θ<sub>i</sub>, λ<sub>i</sub>) where θ<sub>i</sub> is the location of training datum i and λ<sub>i</sub> is the corresponding hemispheric difference, and this polynomial is inverted to give <inline-formula><mml:math id="inf13"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mtext>p</mml:mtext><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The degree of the polynomial was chosen to maximize performance (lower degrees fit poorly but higher degrees overfit). We also consider an enhanced version of the hemispheric model able to integrate information across frequencies, the frequency-dependent hemispheric model, where the ratio is given by <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>∈</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>i</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mtext>i</mml:mtext></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>∉</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>i</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mtext>i</mml:mtext></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>i</mml:mtext></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <italic>f</italic><sub>i</sub> is the BF of neuron i. Most papers studying the hemispheric difference model do not take varying levels into account and therefore use an un-normalized hemispheric difference. <xref ref-type="bibr" rid="bib56">Stecker et al. (2005)</xref> use the maximum rather than the sum as a normalizing factor, but this is essentially equivalent.</p></sec><sec id="s4-3-3"><title>Pattern match decoder</title><p>Each training datum forms a response pattern we write as <bold>ρ</bold> to distinguish from the testing response <bold>r</bold>. We compute a similarity index for each training datum<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ρ</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ρ</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt><mml:mo>.</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which varies between 0 (totally dissimilar) and 1 (totally similar). This is the standard cosine similarity measure from machine learning theory (the value of <inline-formula><mml:math id="inf15"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the cosine of the angle between the two vectors). The estimate <inline-formula><mml:math id="inf16"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the location θ<sub>j</sub> for the index j that maximizes <inline-formula><mml:math id="inf17"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. We also consider a frequency-dependent pattern matching decoder in which the pattern <inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ρ</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is broken into subvectors corresponding to frequency bands, and each subvector is normalized separately. That is, each neural response is divided by the norm of all the neural responses in the same frequency band. More precisely, assuming that the neuron indices are sorted by increasing BF, and the bands are of equal size consisting of B neurons each (i.e., the first band is neurons 0 to B − 1, the second from B to 2B − 1, etc.; we used B = 40), we compute the dot product<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mtext>bandnorm</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ρ</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi>r</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>.</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>⌊</mml:mo><mml:mrow><mml:mfrac><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:mfrac></mml:mrow><mml:mo>⌋</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>⌊</mml:mo><mml:mrow><mml:mfrac><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:mfrac></mml:mrow><mml:mo>⌋</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where for a vector <bold>x</bold>, <inline-formula><mml:math id="inf19"><mml:mrow><mml:mtext>bandnorm</mml:mtext><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>⌊</mml:mo><mml:mrow><mml:mfrac><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:mfrac></mml:mrow><mml:mo>⌋</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>⌊</mml:mo><mml:mrow><mml:mfrac><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:mfrac></mml:mrow><mml:mo>⌋</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where ⌊<italic>z</italic>⌋ is the floor function, the greatest integer less than or equal to z. Note that this banded <inline-formula><mml:math id="inf20"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> does not vary between 0 and 1, but the value of j that maximizes it is still used to find the best estimate of the location.</p><p>In addition to these three decoders, we tested several standard decoders from machine learning and theoretical neuroscience including linear/ridge regression, nearest neighbor regression, maximum likelihood estimators, and support vector classifiers. Data for some of these are shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. Detailed results and analysis of these decoders are not presented here, as in all cases they were outperformed by the pattern match decoder. The best of these decoders was nearest neighbor regression, which performed almost as well as the pattern match decoder. The machine learning algorithms were implemented using the scikits-learn package (<xref ref-type="bibr" rid="bib51">Pedregosa et al., 2011</xref>).</p></sec></sec><sec id="s4-4"><title>Analysis</title><p>We analyze the decoders based on their errors and biases. The error is computed as <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mtext>θ</mml:mtext><mml:mo>|</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where the expectation is taken over the testing data. The bias is computed by taking a linear regression through the points <inline-formula><mml:math id="inf22"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>θ</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> with the restriction that the line must pass through (0, 0). The bias <italic>b</italic> is given as a percentage bias toward the center from the slope <italic>g</italic> of the best fit line via <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To get a better estimate, we compute multiple values of the error and bias over 25 different shuffles of the data, and compute the mean and standard deviation of these values over the multiple shuffles. We generate 6400 total data, and to form each shuffled set of data, we take the following steps: (i) choose a subset of the full set of cells to consider (in those analyses where the number of cells was varied), (ii) choose a random subset of the data as training data, usually 400 data, (iii) choose a nonoverlapping random subset of the data as testing data, usually 800 data. This procedure was chosen to minimize biases introduced by the random sampling while keeping total computation times to a reasonable level (total computation time on an 8-core Intel i7 desktop was approximately 1 week).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank the Museum of Natural History for providing stuffed animals, Hermann Wagner for sharing measured HRTFs and electrophysiological measurements of BD and BF in barn owls, Daniel Tollin for sharing measured HRTFs of a cat, and Philip Joris for sharing electrophysiological measurements of BD and BF in cat’s IC. We also thank Mitchell Day, Marcel Stimberg, Agnès Léger, and Christian Lorenzi for additional comments.</p></ack><sec sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>DFMG, Wrote and carried out simulations, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>VB, Recorded HRTFs, Acquisition of data</p></fn><fn fn-type="con" id="con3"><p>RB, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitkin</surname><given-names>LM</given-names></name><name><surname>Pettigrew</surname><given-names>JD</given-names></name><name><surname>Calford</surname><given-names>MB</given-names></name><name><surname>Phillips</surname><given-names>SC</given-names></name><name><surname>Wise</surname><given-names>LZ</given-names></name></person-group><year>1985</year><article-title>Representation of stimulus azimuth by low-frequency neurons in inferior colliculus of the cat</article-title><source>J Neurophysiol</source><volume>53</volume><fpage>43</fpage><lpage>59</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashida</surname><given-names>G</given-names></name><name><surname>Carr</surname><given-names>CE</given-names></name></person-group><year>2011</year><article-title>Sound localization: jeffress and beyond</article-title><source>Curr Opin Neurobiol</source><volume>21</volume><fpage>745</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.05.008</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attwell</surname><given-names>D</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><year>2001</year><article-title>An energy budget for signaling in the grey matter of the brain</article-title><source>J Cereb Blood Flow Metab</source><volume>21</volume><fpage>1133</fpage><lpage>1145</lpage><pub-id pub-id-type="doi">10.1097/00004647-200110000-00001</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname><given-names>R</given-names></name></person-group><year>2010</year><article-title>On the interpretation of sensitivity analyses of neural responses</article-title><source>J Acoust Soc Am</source><volume>128</volume><fpage>2965</fpage><lpage>2972</lpage><pub-id pub-id-type="doi">10.1121/1.3488311</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Briley</surname><given-names>PM</given-names></name><name><surname>Kitterick</surname><given-names>PT</given-names></name><name><surname>Summerfield</surname><given-names>AQ</given-names></name></person-group><year>2013</year><article-title>Evidence for opponent process analysis of sound source location in humans</article-title><source>J Assoc Res Otolaryngol</source><volume>14</volume><fpage>83</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1007/s10162-012-0356-x</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year>2011</year><article-title>Normalization as a canonical neural computation</article-title><source>Nat Rev Neurosci</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlile</surname><given-names>S</given-names></name><name><surname>Leong</surname><given-names>P</given-names></name><name><surname>Hyams</surname><given-names>S</given-names></name></person-group><year>1997</year><article-title>The nature and distribution of errors in sound localization by human listeners</article-title><source>Hear Res</source><volume>114</volume><fpage>179</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(97)00161-5</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Casseday</surname><given-names>JH</given-names></name><name><surname>Neff</surname><given-names>WD</given-names></name></person-group><year>1973</year><article-title>Localization of pure tones</article-title><source>J Acoust Soc Am</source><volume>54</volume><fpage>365</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1121/1.1913586</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coles</surname><given-names>RB</given-names></name><name><surname>Guppy</surname><given-names>A</given-names></name></person-group><year>1988</year><article-title>Directional hearing in the barn owl (Tyto alba)</article-title><source>J Comp Physiol A</source><volume>163</volume><fpage>117</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1007/BF00612002</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Couchman</surname><given-names>K</given-names></name><name><surname>Grothe</surname><given-names>B</given-names></name><name><surname>Felmy</surname><given-names>F</given-names></name></person-group><year>2010</year><article-title>Medial superior olivary neurons receive surprisingly few excitatory and inhibitory inputs with balanced strength and short-term dynamics</article-title><source>J Neurosci</source><volume>30</volume><fpage>17111</fpage><lpage>17121</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1760-10.2010</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Couchman</surname><given-names>K</given-names></name><name><surname>Grothe</surname><given-names>B</given-names></name><name><surname>Felmy</surname><given-names>F</given-names></name></person-group><year>2012</year><article-title>Functional localization of neurotransmitter receptors and synaptic inputs to mature neurons of the medial superior olive</article-title><source>J Neurophysiol</source><volume>107</volume><fpage>1186</fpage><lpage>1198</lpage><pub-id pub-id-type="doi">10.1152/jn.00586.2011</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Day</surname><given-names>ML</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name></person-group><year>2013</year><article-title>Decoding sound source location and separation using neural population activity patterns</article-title><source>J Neurosci</source><volume>33</volume><fpage>15837</fpage><lpage>15847</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2034-13.2013</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Abbott</surname><given-names>L</given-names></name></person-group><year>2001</year><source>Theoretical neuroscience: computational and mathematical modeling of neural systems</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devore</surname><given-names>S</given-names></name><name><surname>Ihlefeld</surname><given-names>A</given-names></name><name><surname>Hancock</surname><given-names>K</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>B</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name></person-group><year>2009</year><article-title>Accurate sound localization in reverberant environments is mediated by robust encoding of spatial cues in the auditory midbrain</article-title><source>Neuron</source><volume>62</volume><fpage>123</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.018</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>BJ</given-names></name><name><surname>Christianson</surname><given-names>GB</given-names></name><name><surname>Pena</surname><given-names>JL</given-names></name></person-group><year>2008</year><article-title>Cross-correlation in the auditory coincidence detectors of owls</article-title><source>J Neurosci</source><volume>28</volume><fpage>8107</fpage><lpage>8115</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1969-08.2008</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>BJ</given-names></name><name><surname>Peña</surname><given-names>JL</given-names></name></person-group><year>2011</year><article-title>Owl’s behavior and neural representation predicted by Bayesian inference</article-title><source>Nat Neurosci</source><volume>14</volume><fpage>1061</fpage><lpage>1066</lpage><pub-id pub-id-type="doi">10.1038/nn.2872</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzpatrick</surname><given-names>DC</given-names></name><name><surname>Batra</surname><given-names>R</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Kuwada</surname><given-names>S</given-names></name></person-group><year>1997</year><article-title>A neuronal population code for sound localization</article-title><source>Nature</source><volume>388</volume><fpage>871</fpage><lpage>874</lpage><pub-id pub-id-type="doi">10.1038/42246</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontaine</surname><given-names>B</given-names></name><name><surname>Goodman</surname><given-names>DFM</given-names></name><name><surname>Benichoux</surname><given-names>V</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name></person-group><year>2011</year><article-title>Brian hears: online auditory processing using vectorization over channels</article-title><source>Front Neuroinform</source><volume>5</volume><fpage>9</fpage><pub-id pub-id-type="doi">10.3389/fninf.2011.00009</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furukawa</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>L</given-names></name><name><surname>Middlebrooks</surname><given-names>JC</given-names></name></person-group><year>2000</year><article-title>Coding of sound-source location by ensembles of cortical neurons</article-title><source>J Neurosci</source><volume>20</volume><fpage>1216</fpage><lpage>1228</lpage></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasberg</surname><given-names>BR</given-names></name><name><surname>Moore</surname><given-names>BC</given-names></name></person-group><year>1990</year><article-title>Derivation of auditory filter shapes from notched-noise data</article-title><source>Hear Res</source><volume>47</volume><fpage>103</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/0378-5955(90)90170-T</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>D</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name></person-group><year>2008</year><article-title>Brian: a simulator for spiking neural networks in python</article-title><source>Front Neuroinform</source><volume>2</volume><fpage>5</fpage><pub-id pub-id-type="doi">10.3389/neuro.11.005.2008</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>DFM</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name></person-group><year>2009</year><article-title>The Brian simulator</article-title><source>Front Neurosci</source><volume>3</volume><fpage>192</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.026.2009</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gourévitch</surname><given-names>B</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name></person-group><year>2012</year><article-title>The impact of early reflections on binaural cues</article-title><source>J Acoust Soc Am</source><volume>132</volume><fpage>9</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1121/1.4726052</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grothe</surname><given-names>B</given-names></name><name><surname>Pecka</surname><given-names>M</given-names></name><name><surname>McAlpine</surname><given-names>D</given-names></name></person-group><year>2010</year><article-title>Mechanisms of sound localization in mammals</article-title><source>Physiol Rev</source><volume>90</volume><fpage>983</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1152/physrev.00026.2009</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hancock</surname><given-names>KE</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name></person-group><year>2004</year><article-title>A Physiologically based model of interaural time difference discrimination</article-title><source>J Neurosci</source><volume>24</volume><fpage>7110</fpage><lpage>7117</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0762-04.2004</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>McAlpine</surname><given-names>D</given-names></name></person-group><year>2004</year><article-title>Optimal neural population coding of an auditory spatial cue</article-title><source>Nature</source><volume>430</volume><fpage>682</fpage><lpage>686</lpage><pub-id pub-id-type="doi">10.1038/nature02768</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffner</surname><given-names>HE</given-names></name><name><surname>Heffner</surname><given-names>RS</given-names></name></person-group><year>2005</year><article-title>The sound-localization ability of cats</article-title><source>J Neurophysiol</source><volume>94</volume><fpage>3653</fpage><lpage>3655</lpage><pub-id pub-id-type="doi">10.1152/jn.00720.2005</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffner</surname><given-names>RS</given-names></name><name><surname>Heffner</surname><given-names>HE</given-names></name></person-group><year>1988a</year><article-title>Sound localization acuity in the cat: effect of azimuth, signal duration, and test procedure</article-title><source>Hear Res</source><volume>36</volume><fpage>221</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/0378-5955(88)90064-0</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffner</surname><given-names>RS</given-names></name><name><surname>Heffner</surname><given-names>HE</given-names></name></person-group><year>1988b</year><article-title>Sound localization and use of binaural cues by the gerbil (<italic>Meriones unguiculatus</italic>)</article-title><source>Behav Neurosci</source><volume>102</volume><fpage>422</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1037/0735-7044.102.3.422</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffner</surname><given-names>RS</given-names></name><name><surname>Heffner</surname><given-names>HE</given-names></name></person-group><year>1992</year><article-title>Visual factors in sound localization in mammals</article-title><source>J Comp Neurol</source><volume>317</volume><fpage>219</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1002/cne.903170302</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeffress</surname><given-names>LA</given-names></name></person-group><year>1948</year><article-title>A place theory of sound localization</article-title><source>J Comp Physiol Psychol</source><volume>41</volume><fpage>35</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1037/h0061495</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkins</surname><given-names>WM</given-names></name><name><surname>Masterton</surname><given-names>RB</given-names></name></person-group><year>1982</year><article-title>Sound localization: effects of unilateral lesions in central auditory system</article-title><source>J Neurophysiol</source><volume>47</volume><fpage>987</fpage><lpage>1016</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joris</surname><given-names>PX</given-names></name><name><surname>Van de Sande</surname><given-names>B</given-names></name><name><surname>Louage</surname><given-names>DH</given-names></name><name><surname>van der Heijden</surname><given-names>M</given-names></name></person-group><year>2006</year><article-title>Binaural and cochlear disparities</article-title><source>Proc Natl Acad Sci USA</source><volume>103</volume><fpage>12917</fpage><pub-id pub-id-type="doi">10.1073/pnas.0601396103</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knudsen</surname><given-names>EI</given-names></name><name><surname>Blasdel</surname><given-names>GG</given-names></name><name><surname>Konishi</surname><given-names>M</given-names></name></person-group><year>1979</year><article-title>Sound localization by the barn owl (Tyto alba) measured with the search coil technique</article-title><source>J Comp Physiol</source><volume>133</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1007/BF00663105</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konishi</surname><given-names>M</given-names></name></person-group><year>2003</year><article-title>Coding of auditory space</article-title><source>Annu Rev Neurosci</source><volume>26</volume><fpage>31</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.26.041002.131123</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Köppl</surname><given-names>C</given-names></name></person-group><year>1997</year><article-title>Frequency tuning and spontaneous activity in the auditory nerve and cochlear nucleus magnocellularis of the barn owl Tyto alba</article-title><source>J Neurophysiol</source><volume>77</volume><fpage>364</fpage><lpage>377</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuenzel</surname><given-names>T</given-names></name><name><surname>Borst</surname><given-names>JGG</given-names></name><name><surname>van der Heijden</surname><given-names>M</given-names></name></person-group><year>2011</year><article-title>Factors controlling the input-output relationship of spherical bushy cells in the gerbil cochlear nucleus</article-title><source>J Neurosci</source><volume>31</volume><fpage>4260</fpage><lpage>4273</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5433-10.2011</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhn</surname><given-names>GF</given-names></name></person-group><year>1977</year><article-title>Model for the interaural time differences in the azimuthal plane</article-title><source>J Acoust Soc Am</source><volume>62</volume><fpage>157</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1121/1.381498</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuwada</surname><given-names>S</given-names></name><name><surname>Yin</surname><given-names>TC</given-names></name></person-group><year>1983</year><article-title>Binaural interaction in low-frequency neurons in inferior colliculus of the cat. I. Effects of long interaural delays, intensity, and repetition rate on interaural delay function</article-title><source>J Neurophysiol</source><volume>50</volume><fpage>981</fpage><lpage>999</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>CC</given-names></name><name><surname>Middlebrooks</surname><given-names>JC</given-names></name></person-group><year>2013</year><article-title>Specialization for sound localization in fields A1, DZ, and PAF of cat auditory cortex</article-title><source>J Assoc Res Otolaryngol</source><volume>14</volume><fpage>61</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1007/s10162-012-0357-9</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lesica</surname><given-names>NA</given-names></name><name><surname>Lingner</surname><given-names>A</given-names></name><name><surname>Grothe</surname><given-names>B</given-names></name></person-group><year>2010</year><article-title>Population coding of interaural time differences in gerbils and barn owls</article-title><source>J Neurosci</source><volume>30</volume><fpage>11696</fpage><lpage>11702</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0846-10.2010</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litovsky</surname><given-names>RY</given-names></name><name><surname>Fligor</surname><given-names>BJ</given-names></name><name><surname>Tramo</surname><given-names>MJ</given-names></name></person-group><year>2002</year><article-title>Functional role of the human inferior colliculus in binaural hearing</article-title><source>Hear Res</source><volume>165</volume><fpage>177</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(02)00304-0</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lüling</surname><given-names>H</given-names></name><name><surname>Siveke</surname><given-names>I</given-names></name><name><surname>Grothe</surname><given-names>B</given-names></name><name><surname>Leibold</surname><given-names>C</given-names></name></person-group><year>2011</year><article-title>Frequency-invariant representation of interaural time differences in mammals</article-title><source>PLOS Comput Biol</source><volume>7</volume><fpage>e1002013</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002013</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maki</surname><given-names>K</given-names></name><name><surname>Furukawa</surname><given-names>S</given-names></name></person-group><year>2005</year><article-title>Acoustical cues for sound localization by the Mongolian gerbil, <italic>Meriones unguiculatus</italic></article-title><source>J Acoust Soc Am</source><volume>118</volume><fpage>872</fpage><lpage>886</lpage><pub-id pub-id-type="doi">10.1121/1.1944647</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makous</surname><given-names>JC</given-names></name><name><surname>Middlebrooks</surname><given-names>JC</given-names></name></person-group><year>1990</year><article-title>Two-dimensional sound localization by human listeners</article-title><source>J Acoust Soc Am</source><volume>87</volume><fpage>2188</fpage><lpage>2200</lpage><pub-id pub-id-type="doi">10.1121/1.399186</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malhotra</surname><given-names>S</given-names></name><name><surname>Hall</surname><given-names>AJ</given-names></name><name><surname>Lomber</surname><given-names>SG</given-names></name></person-group><year>2004</year><article-title>Cortical control of sound localization in the cat: unilateral cooling deactivation of 19 cerebral areas</article-title><source>J Neurophysiol</source><volume>92</volume><fpage>1625</fpage><lpage>1643</lpage><pub-id pub-id-type="doi">10.1152/jn.01205.2003</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAlpine</surname><given-names>D</given-names></name><name><surname>Jiang</surname><given-names>D</given-names></name><name><surname>Palmer</surname><given-names>AR</given-names></name></person-group><year>2001</year><article-title>A neural code for low-frequency sound localization in mammals</article-title><source>Nat Neurosci</source><volume>4</volume><fpage>396</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1038/86049</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>LM</given-names></name><name><surname>Recanzone</surname><given-names>GH</given-names></name></person-group><year>2009</year><article-title>Populations of auditory cortical neurons can accurately encode acoustic space across stimulus intensity</article-title><source>Proc Natl Acad Sci USA</source><volume>106</volume><fpage>5931</fpage><lpage>5935</lpage><pub-id pub-id-type="doi">10.1073/pnas.0901023106</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moiseff</surname><given-names>A</given-names></name></person-group><year>1989</year><article-title>Binaural disparity cues available to the barn owl for sound localization</article-title><source>J Comp Physiol</source><volume>164</volume><fpage>629</fpage><lpage>636</lpage><pub-id pub-id-type="doi">10.1007/BF00614505</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>JM</given-names></name><name><surname>Tollin</surname><given-names>DJ</given-names></name><name><surname>Yin</surname><given-names>TCT</given-names></name></person-group><year>2008</year><article-title>Can measures of sound localization acuity be related to the precision of absolute location estimates?</article-title><source>Hear Res</source><volume>238</volume><fpage>94</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2007.11.006</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><etal/></person-group><year>2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>J Mach Learn Res</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Populin</surname><given-names>LC</given-names></name><name><surname>Tollin</surname><given-names>DJ</given-names></name><name><surname>Yin</surname><given-names>TCT</given-names></name></person-group><year>2004</year><article-title>Effect of eye position on saccades and neuronal responses to acoustic stimuli in the superior colliculus of the behaving cat</article-title><source>J Neurophysiol</source><volume>92</volume><fpage>2151</fpage><lpage>2167</lpage><pub-id pub-id-type="doi">10.1152/jn.00453.2004</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shera</surname><given-names>CA</given-names></name><name><surname>Guinan</surname><given-names>JJ</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year>2002</year><article-title>Revised estimates of human cochlear tuning from otoacoustic and behavioral measurements</article-title><source>Proc Natl Acad Sci USA</source><volume>99</volume><fpage>3318</fpage><lpage>3323</lpage><pub-id pub-id-type="doi">10.1073/pnas.032675099</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Slaney</surname><given-names>M</given-names></name></person-group><year>1993</year><source>Auditory toolbox, apple technical report #45</source><publisher-name>Apple Computer, Inc</publisher-name><ext-link ext-link-type="uri" xlink:href="https://engineering.purdue.edu/~malcolm/interval/1998-010/">https://engineering.purdue.edu/∼malcolm/interval/1998-010/</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sparks</surname><given-names>DL</given-names></name><name><surname>Nelson</surname><given-names>IS</given-names></name></person-group><year>1987</year><article-title>Sensory and motor maps in the mammalian superior colliculus</article-title><source>Trend Neurosci</source><volume>10</volume><fpage>312</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(87)90085-3</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stecker</surname><given-names>GC</given-names></name><name><surname>Harrington</surname><given-names>IA</given-names></name><name><surname>Middlebrooks</surname><given-names>JC</given-names></name></person-group><year>2005</year><article-title>Location coding by opponent neural populations in the auditory cortex</article-title><source>PLOS Biol</source><volume>3</volume><fpage>e78</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.0030078</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sterbing</surname><given-names>SJ</given-names></name><name><surname>Hartung</surname><given-names>K</given-names></name><name><surname>Hoffmann</surname><given-names>K-P</given-names></name></person-group><year>2003</year><article-title>Spatial tuning to virtual sounds in the inferior colliculus of the guinea pig</article-title><source>J Neurophysiol</source><volume>90</volume><fpage>2648</fpage><lpage>2659</lpage><pub-id pub-id-type="doi">10.1152/jn.00348.2003</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stern</surname><given-names>RM</given-names></name><name><surname>Trahiotis</surname><given-names>C</given-names></name></person-group><year>1995</year><article-title>Models of binaural interaction</article-title><person-group person-group-type="editor"><name><surname>Moore</surname><given-names>Brian CJ</given-names></name></person-group><source>Handbook of perception and cognition</source><comment>Volume 6: Hearing</comment><publisher-loc>New York</publisher-loc><publisher-name>Academic Press</publisher-name><fpage>347</fpage><lpage>387</lpage></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>T</given-names></name><name><surname>Moiseff</surname><given-names>A</given-names></name><name><surname>Konishi</surname><given-names>M</given-names></name></person-group><year>1984</year><article-title>Time and intensity cues are processed independently in the auditory system of the owl</article-title><source>J Neurosci</source><volume>4</volume><fpage>1781</fpage><lpage>1786</lpage></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>SK</given-names></name><name><surname>von Kriegstein</surname><given-names>K</given-names></name><name><surname>Deane-Pratt</surname><given-names>A</given-names></name><name><surname>Marquardt</surname><given-names>T</given-names></name><name><surname>Deichmann</surname><given-names>R</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><etal/></person-group><year>2006</year><article-title>Representation of interaural time delay in the human auditory midbrain</article-title><source>Nat Neurosci</source><volume>9</volume><fpage>1096</fpage><lpage>1098</lpage><pub-id pub-id-type="doi">10.1038/nn1755</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tollin</surname><given-names>DJ</given-names></name><name><surname>Koka</surname><given-names>K</given-names></name></person-group><year>2009a</year><article-title>Postnatal development of sound pressure transformations by the head and pinnae of the cat: binaural characteristics</article-title><source>J Acoust Soc Am</source><volume>126</volume><fpage>3125</fpage><lpage>3136</lpage><pub-id pub-id-type="doi">10.1121/1.3257234</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tollin</surname><given-names>DJ</given-names></name><name><surname>Koka</surname><given-names>K</given-names></name></person-group><year>2009b</year><article-title>Postnatal development of sound pressure transformations by the head and pinnae of the cat: monaural characteristics</article-title><source>J Acoust Soc Am</source><volume>125</volume><fpage>980</fpage><lpage>994</lpage><pub-id pub-id-type="doi">10.1121/1.3058630</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tollin</surname><given-names>DJ</given-names></name><name><surname>Populin</surname><given-names>LC</given-names></name><name><surname>Moore</surname><given-names>JM</given-names></name><name><surname>Ruhland</surname><given-names>JL</given-names></name><name><surname>Yin</surname><given-names>TCT</given-names></name></person-group><year>2005</year><article-title>Sound-localization performance in the cat: the effect of restraining the head</article-title><source>J Neurophysiol</source><volume>93</volume><fpage>1223</fpage><lpage>1234</lpage><pub-id pub-id-type="doi">10.1152/jn.00747.2004</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagner</surname><given-names>H</given-names></name><name><surname>Asadollahi</surname><given-names>A</given-names></name><name><surname>Bremen</surname><given-names>P</given-names></name><name><surname>Endler</surname><given-names>F</given-names></name><name><surname>Vonderschen</surname><given-names>K</given-names></name><name><surname>von Campenhausen</surname><given-names>M</given-names></name></person-group><year>2007</year><article-title>Distribution of interaural time difference in the barn owl’s inferior colliculus in the low- and high-frequency ranges</article-title><source>J Neurosci</source><volume>27</volume><fpage>4191</fpage><lpage>4200</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5250-06.2007</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wakeford</surname><given-names>OS</given-names></name><name><surname>Robinson</surname><given-names>DE</given-names></name></person-group><year>1974</year><article-title>Lateralization of tonal stimuli by the cat</article-title><source>J Acoust Soc Am</source><volume>55</volume><fpage>649</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1121/1.1914577</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wightman</surname><given-names>FL</given-names></name><name><surname>Kistler</surname><given-names>DJ</given-names></name></person-group><year>1992</year><article-title>The dominant role of low-frequency interaural time differences in sound localization</article-title><source>J Acoust Soc Am</source><volume>91</volume><fpage>1648</fpage><lpage>1661</lpage><pub-id pub-id-type="doi">10.1121/1.402445</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>TC</given-names></name><name><surname>Chan</surname><given-names>JC</given-names></name></person-group><year>1990a</year><article-title>Interaural time sensitivity in medial superior olive of cat</article-title><source>J Neurophysiol</source><volume>64</volume><fpage>465</fpage><lpage>488</lpage></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>TC</given-names></name><name><surname>Chan</surname><given-names>JC</given-names></name></person-group><year>1990b</year><article-title>Interaural time sensitivity in medial superior olive of cat</article-title><source>J Neurophysiol</source><volume>64</volume><fpage>465</fpage><lpage>488</lpage></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>TC</given-names></name><name><surname>Chan</surname><given-names>JC</given-names></name><name><surname>Carney</surname><given-names>LH</given-names></name></person-group><year>1987</year><article-title>Effects of interaural time delays of noise stimuli on low-frequency cells in the cat’s inferior colliculus. III. Evidence for cross-correlation</article-title><source>J Neurophysiol</source><volume>58</volume><fpage>562</fpage><lpage>583</lpage></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.01312.013</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Angelaki</surname><given-names>Dora</given-names></name><role>Reviewing editor</role><aff><institution>Baylor College of Medicine</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elife.elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for sending your work entitled “Decoding neural responses to sound location” for consideration at <italic>eLife</italic>. Your article has been favorably evaluated by a Senior editor and 3 reviewers, one of whom is a member of our Board of Reviewing Editors.</p><p>The Reviewing editor and the other reviewers discussed their comments before we reached this decision, and the Reviewing editor has assembled the following comments to help you prepare a revised submission.</p><p>This is a modeling study comparing different ways of extracting information about sound location from the responses of a model population of binaural neurons. The conclusion of the manuscript is that, while a labeled line code is too inefficient, summing the activity in each hemisphere discards too much of the information that is present in neural activity patterns.</p><p>This is an interesting and important topic that could be of general interest. However, one of the reviewers felt that it would be nice to expand on the population decoding, and discuss the fact that they have not considered population models that do not simply sum activities (e.g.,, those with optimal weighting of the contribution of each cell) and network nonlinearities useful for marginalization of task-irrelevant information like divisive normalization. Other properties known to be critical for information loss and decoding optimality, like shared variability and interneuronal correlations, have also not been considered. The reviewers would like to see a broader coverage/discussion, such that this work can appeal to the broader neuroscience community.</p><p>Specific comments:</p><p>One of the reviewers particularly liked that the authors model the effects of a unilateral lesion. Such a lesion of their pattern-match model predicts strictly contralesional localization deficits, which is what is seen in all the animal lesion studies. In contrast, a unilateral lesion of their hemispheric model predicts bilateral deficits, which are never seen in animal studies. The reviewers would like to see that result given a little more prominence, such as mention in the Abstract.</p><p>The title should be revised to more specifically indicate that the study examines “sound location based on interaural time differences”.</p><p>The results in <xref ref-type="fig" rid="fig7">Figure 7</xref> for the simulated lesions should be compared, at least qualitatively, to results for the human listeners with lesions. The two models shown in <xref ref-type="fig" rid="fig7">Figure 7B</xref> have very different results, so the comparison may be interesting.</p><p>In the section on “Comparison to Behavioral Performance”, the argument is made that the hemispheric difference model predicts errors that are larger than those observed in behavioral studies. In the preamble to this section, it is stated that the pattern match model has very small errors, but the numerical errors for the pattern-match model are not provided for comparison to specific species and cases, as they are for the hemispheric difference model. Despite the fact that the pattern-match model's errors will appear to be quite small, they are useful and should be provided for the reader. Although the absolute errors will be smaller than behavioral thresholds, the trends in the errors across conditions are interesting to present. One can argue that the actual system will have errors larger than this very detailed model, but the (fractional) difference between model and actual performance should be comparable across conditions. (That is, it would be bothersome if the model were 1 order of magnitude too accurate in some cases, and 3 orders of magnitude too accurate in others, such that the degradation in performance required to match behavior had to change dramatically across conditions.)</p><p>The model results presented here illustrate an interesting difference in the trend of the errors between the hemispheric model and both the smoothed peak and pattern-match models at high frequencies (e.g., <xref ref-type="fig" rid="fig2 fig3 fig4">Figures 2, 3, 4</xref>). The comparison of this prediction to behavioral results deserves inclusion in the section that compares model to behavioral performance. Again, this is a consistent trend in the results that can be compared qualitatively to trends in behavioral results.</p><p>In the Discussion, the argument is made that additional processing would be required to generate spatially tuned neurons from the hemispheric model. This is an interesting point, but it is not clear what spatially tuned neurons the authors are thinking about. It would help to be more specific as to what neurons (presumably at a level higher than the IC) are being considered here (indeed, there are precious few spatially tuned neurons at higher levels, and the hemispheric model doesn't require such neurons, does it?). For example, it might be helpful to consider decoding schemes proposed for the cortex. <xref ref-type="bibr" rid="bib56">Stecker, Harrington, and Middlebrooks, 2005</xref> (cited) tested a hemispheric model as a sort of worst-case scenario, although several authors have seized on the hemispheric model as reality. Others have looked at decoding based on spatial tuning of individual neurons, compiled as ensembles (anesthetized cat: <xref ref-type="bibr" rid="bib19">Furukawa, Xu, and Middlebrooks, 2000</xref>; behaving cat: <xref ref-type="bibr" rid="bib40">Lee and Middlebrooks, 2013</xref>; awake monkey: <xref ref-type="bibr" rid="bib48">Miller and Recanzone, 2009</xref>). These experimental studies should be discussed because they do empirically what the authors are doing with their simulations.</p><p>It would also be useful for the authors to include comment on the recent paper by Briley et al., JARO 2013, Vol 14: 83-101, which supports the hemispheric model, based on EEG recordings in human.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.01312.014</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>This is an interesting and important topic that could be of general interest. However, one of the reviewers felt that it would be nice to expand on the population decoding, and discuss the fact that they have not considered population models that do not simply sum activities (e.g., those with optimal weighting of the contribution of each cell) and network nonlinearities useful for marginalization of task-irrelevant information like divisive normalization. Other properties known to be critical for information loss and decoding optimality, like shared variability and interneuronal correlations, have also not been considered. The reviewers would like to see a broader coverage/discussion, such that this work can appeal to the broader neuroscience community</italic>.</p><p>The target of this research was the ongoing debate between two major competing models of sound localization, and so in the paper we focused on these models. However, we also tested a large number of other decoders, including a large number of standard decoders from machine learning. We added a figure supplement to <xref ref-type="fig" rid="fig3">Figure 3</xref> showing the results of two of these decoders, ridge regression and nearest neighbor regression. Ridge regression is similar to linear regression but has an additional penalty to large coefficients. Because of the signal correlations between cells with overlapping frequency bands, the linear regression decoder was highly subject to noise, and ridge regression resolves this issue and finds the optimal weights for each cell. As can be seen in that figure, the linear/ridge regression performed similarly to the hemispheric decoder: performing slightly better in some aspects and worse in others. We also show the nearest neighbor regression decoder performs quite similarly to the pattern match decoder, which is not surprising as it is based on a similar idea (comparison of test result to stored patterns). To keep the paper to a manageable length, we only presented results of the three decoders, which we feel makes sense as two are the major current models of ITD decoding, and the third (pattern match) performed the best of all the decoders we tried. The pattern match estimator also has the virtue that it is straightforward to implement in a neural circuit, unlike many of the methods from machine learning. We also added new paragraphs to the Results and Methods sections mentioning that we tried other decoders.</p><p>Some normalization is already present in the responses of binaural neurons (see Methods, Response Model), where level is normalized, and therefore is included in all decoders. It indeed removes some task-independent sources of variability, namely overall level and ILDs. However, the differences between decoders are not due to this normalization, since it is included in all of them. The pattern match decoder also includes divisive normalization in the calculation of similarity between responses and templates, however it has no influence on the decoder’s output because it uses a winner-take-all operation and the normalization factor is identical for all candidate templates. The hemispheric decoder also uses a normalized difference, so it is not the decisive factor in the performance differences.</p><p>We added a section about shared variability and correlations in the Discussion. The bottom line is that, because the MSO (first binaural nucleus) is an essentially anatomically feedforward and tonotopic circuit, the main source of correlations should be stimulus variability. The topic of correlations is indeed very relevant to this study, because the previous conclusion that the hemispheric code (or “slope coding”) is supposedly optimal relied on the implicit assumption that neural responses are independent, conditionally to the ITD (<xref ref-type="bibr" rid="bib26">Harper and McAlpine, 2004</xref>). But the variability of the stimulus, which is shared, implies that a better decoder must use the structure of correlations. For example, in a Jeffress-like model, changes in the sound (e.g., level, background noise) results in changes in the activity of all neurons, but the identity of the most active neuron does not change (a theoretical argument made in <xref ref-type="bibr" rid="bib4">Brette, 2010</xref>).</p><p><italic>One of the reviewers particularly liked that the authors model the effects of a unilateral lesion. Such a lesion of their pattern-match model predicts strictly contralesional localization deficits, which is what is seen in all the animal lesion studies. In contrast, a unilateral lesion of their hemispheric model predicts bilateral deficits, which are never seen in animal studies. The reviewers would like to see that result given a little more prominence, such as mention in the Abstract</italic>.</p><p>We added this to the abstract.</p><p><italic>The title should be revised to more specifically indicate that the study examines “sound location based on interaural time differences”</italic>.</p><p>We used the reviewers’ suggestion of “Decoding neural responses to temporal cues for sound localization”.</p><p><italic>The results in</italic> <xref ref-type="fig" rid="fig7"><italic>Figure 7</italic></xref> <italic>for the simulated lesions should be compared, at least qualitatively, to results for the human listeners with lesions. The two models shown in</italic> <xref ref-type="fig" rid="fig7"><italic>Figure 7B</italic></xref> <italic>have very different results, so the comparison may be interesting</italic>.</p><p>We added the following sentence: “Lesion data indicate that sound localization performance is greatly degraded in the contralateral hemifield, but not completely abolished, which would discard the smoothed peak decoder – although lesions might not have been complete, and those were free-field experiments involving other cues than ITD.”</p><p><italic>In the section on “Comparison to Behavioral Performance”, the argument is made that the hemispheric difference model predicts errors that are larger than those observed in behavioral studies. In the preamble to this section, it is stated that the pattern match model has very small errors, but the numerical errors for the pattern-match model are not provided for comparison to specific species and cases, as they are for the hemispheric difference model. Despite the fact that the pattern-match model's errors will appear to be quite small, they are useful and should be provided for the reader. Although the absolute errors will be smaller than behavioral thresholds, the trends in the errors across conditions are interesting to present. One can argue that the actual system will have errors larger than this very detailed model, but the (fractional) difference between model and actual performance should be comparable across conditions. (That is, it would be bothersome if the model were 1 order of magnitude too accurate in some cases, and 3 orders of magnitude too accurate in others, such that the degradation in performance required to match behavior had to change dramatically across conditions.</italic>)</p><p><italic>The model results presented here illustrate an interesting difference in the trend of the errors between the hemispheric model and both the smoothed peak and pattern-match models at high frequencies (e.g.,</italic> <xref ref-type="fig" rid="fig2 fig3 fig4"><italic>Figures 2, 3, 4</italic></xref><italic>). The comparison of this prediction to behavioral results deserves inclusion in the section that compares model to behavioral performance. Again, this is a consistent trend in the results that can be compared qualitatively to trends in behavioral results</italic>.</p><p>We understand the reasoning and we had considered including a discussion of these trends. However, we had decided not to include them for two reasons, even though they are generally in line with our conclusions.</p><p>First, it relies on an assumption that the degradation of error between the “optimal” model and behavior is a frequency-independent factor. But this assumption may not be true. In that section, we argued that there are additional sources of errors, in particular the fact that the estimation of sound location relies not only on accurate acoustical cues but also on accurate feedback, such as visual feedback. If the limiting factor is the accuracy of this feedback (which is supported by the Heffner &amp; Heffner study), then it should not be frequency-dependent, even if the coding accuracy of acoustical cues is frequency-dependent. Therefore, we think the extrapolation is a bit speculative. What is more informative, however, is when behavior is more accurate than a model (such as the hemispheric model), which does not have this additional source of error. In this case it is fair to conclude to the model is insufficiently accurate.</p><p>The second reason is that many studies use free-field experiments, and acoustical cues depend on frequency, in amount and in reliability (both ITDs and ILDs). It would bring a potentially confounding factor to the comparisons.</p><p>However, we added a paragraph in which we compared our results with a behavioral experiment using controlled binaural tones presented through earphones (no ILD), but we feel that this aspect should not be over-emphasized in this comparison. We also added a mention of the error of the pattern decoder in the first paragraph.</p><p><italic>In the Discussion, the argument is made that additional processing would be required to generate spatially tuned neurons from the hemispheric model. This is an interesting point, but it is not clear what spatially tuned neurons the authors are thinking about. It would help to be more specific as to what neurons (presumably at a level higher than the IC) are being considered here (indeed, there are precious few spatially tuned neurons at higher levels, and the hemispheric model doesn't require such neurons, does it?). For example, it might be helpful to consider decoding schemes proposed for the cortex.</italic> <xref ref-type="bibr" rid="bib56"><italic>Stecker, Harrington, and Middlebrooks, 2005</italic></xref> <italic>(cited) tested a hemispheric model as a sort of worst-case scenario, although several authors have seized on the hemispheric model as reality. Others have looked at decoding based on spatial tuning of individual neurons, compiled as ensembles (anesthetized cat:</italic> <xref ref-type="bibr" rid="bib19"><italic>Furukawa, Xu, and Middlebrooks, 2000</italic></xref><italic>; behaving cat:</italic> <xref ref-type="bibr" rid="bib40"><italic>Lee and Middlebrooks, 2013</italic></xref><italic>; awake monkey:</italic> <xref ref-type="bibr" rid="bib48"><italic>Miller and Recanzone, 2009</italic></xref><italic>). These experimental studies should be discussed because they do empirically what the authors are doing with their simulations</italic>.</p><p>We wrote in the previous paragraph: “In the cat, most low frequency neurons in the central nucleus of the inferior colliculus are spatially tuned, with preferred azimuth homogeneously distributed in the contralateral hemifield (<xref ref-type="bibr" rid="bib1">Aitkin et al., 1985</xref>).” Perhaps more importantly, sound localization behavior does require that the graded code assumed in the hemispheric model be converted to spatially tuned responses, at least for eye movements, which have been used to measure sound localization accuracy in cats. We now discuss this point in the Discussion (Physiological mechanisms). In the superior colliculus (to which IC projects), there are spatially tuned auditory neurons, whose stimulation produces movements of fixed amplitude and direction, independently of stimulation strength, and they are arranged topographically.</p><p>We did not address the question of how cortical responses might be decoded, because the controversy we address arose from measurements of responses in MSO and IC, and because these are earlier in the auditory pathway. However, we agree that there are a number of papers about estimating sound location from cortical responses, which would be relevant to discuss. We added a paragraph about them in the Discussion (Pattern decoders). Some of the decoders used in these studies are, indeed, similar to the pattern decoder we used (the closest one being the maximum likelihood decoder in <xref ref-type="bibr" rid="bib48">Miller and Recanzone, 2009</xref>), however we do not agree that these studies do empirically what we are doing with simulations. Our key point is that decoders should be tested in a situation when the stimulus is allowed to be variable (different spectrum, etc.); otherwise a large part of the problem is neglected (stimulus-dependent variability). The difference between hemispheric decoder and pattern decoder only appears in this more general situation. The studies cited above generally find that decoders based on patterns (either spike timing or spike counts) perform well, provided there are enough neurons, but because the stimulus is fixed (except for changes in level), they do not show that such decoders are robust (i.e., would work with other sounds). Conversely, the opponent channel model shows good results in one cortical study by Stecker et al., but as in other previous studies in subcortical areas, it may not be robust to stimulus-dependent variability other than level. Our analysis strongly suggests that it is not, because the hemispheric difference is likely also sensitive to sound spectrum and other features that cortical neurons are tuned to, and because adding more neurons in the decoder does not remove stimulus-dependent variability.</p><p><italic>It would also be useful for the authors to include comment on the recent paper by Briley et al., JARO 2013, Vol 14: 83-101, which supports the hemispheric model, based on EEG recordings in human</italic>.</p><p>As far as we understand it, this paper is about cortical responses to different sound locations, not about how these responses might be “decoded” into an estimate of sound location. It provides some indirect evidence that the distribution of BDs in humans is similar to what was found with single-unit electrophysiology in small mammals, i.e., more neurons with large BDs than expected from a uniform distribution. This observation does not by itself imply that sound location is estimated from the average response (note that the behavioral part in that study is in fact an analysis of the sensitivity of the hemispheric decoder, not of its accuracy as we do in our study). In fact, in our study, we start precisely from such a distribution of BD and show that the hemispheric is suboptimal. Therefore, that paper cannot be taken as evidence in favor of the hemispheric model. The approach is also indirect, as individual neural responses are not measured, and therefore a number of factors other than BD could contribute to EEG responses (e.g., peak firing rate, tuning width, if these properties have a non-zero correlation with BD). We added a mention of that paper in the paragraph about humans.</p></body></sub-article></article>