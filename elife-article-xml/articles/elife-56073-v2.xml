<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">56073</article-id><article-id pub-id-type="doi">10.7554/eLife.56073</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Long-term implicit memory for sequential auditory patterns in humans</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-158783"><name><surname>Bianco</surname><given-names>Roberta</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9613-8933</contrib-id><email>r.bianco@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-176483"><name><surname>Harrison</surname><given-names>Peter MC</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9851-9462</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-176484"><name><surname>Hu</surname><given-names>Mingyue</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-176485"><name><surname>Bolger</surname><given-names>Cora</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-176486"><name><surname>Picken</surname><given-names>Samantha</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-176487"><name><surname>Pearce</surname><given-names>Marcus T</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-4307"><name><surname>Chait</surname><given-names>Maria</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7808-3593</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>UCL Ear Institute, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>School of Electronic Engineering and Computer Science, Queen Mary University of London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Department of Clinical Medicine, Aarhus University</institution><addr-line><named-content content-type="city">Aarhus</named-content></addr-line><country>Denmark</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Obleser</surname><given-names>Jonas</given-names></name><role>Reviewing Editor</role><aff><institution>University of Lübeck</institution><country>Germany</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>18</day><month>05</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e56073</elocation-id><history><date date-type="received" iso-8601-date="2020-02-16"><day>16</day><month>02</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-05-18"><day>18</day><month>05</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Bianco et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Bianco et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-56073-v2.pdf"/><abstract><p>Memory, on multiple timescales, is critical to our ability to discover the structure of our surroundings, and efficiently interact with the environment. We combined behavioural manipulation and modelling to investigate the dynamics of memory formation for rarely reoccurring acoustic patterns. In a series of experiments, participants detected the emergence of regularly repeating patterns within rapid tone-pip sequences. Unbeknownst to them, a few patterns reoccurred every ~3 min. All sequences consisted of the same 20 frequencies and were distinguishable only by the order of tone-pips. Despite this, reoccurring patterns were associated with a rapidly growing detection-time advantage over novel patterns. This effect was implicit, robust to interference, and persisted for 7 weeks. The results implicate an interplay between short (a few seconds) and long-term (over many minutes) integration in memory formation and demonstrate the remarkable sensitivity of the human auditory system to sporadically reoccurring structure within the acoustic environment.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Patterns of sound – such as the noise of footsteps approaching or a person speaking – often provide valuable information. To recognize these patterns, our memory must hold each part of the sound sequence long enough to perceive how they fit together. This ability is necessary in many situations: from discriminating between random noises in the woods to understanding language and appreciating music. Memory traces left by each sound are crucial for discovering new patterns and recognizing patterns we have previously encountered. However, it remained unclear whether sounds that reoccur sporadically can stick in our memory, and under what conditions this happens.</p><p>To answer this question, Bianco et al. conducted a series of experiments where human volunteers listened to rapid sequences of 20 random tones interspersed with repeated patterns. Participants were asked to press a button as soon as they detected a repeating pattern. Most of the patterns were new but some reoccurred every three minutes or so unbeknownst to the listener.</p><p>Bianco et al. found that participants became progressively faster at recognizing a repeated pattern each time it reoccurred, gradually forming an enduring memory which lasted at least seven weeks after the initial training. The volunteers did not recognize these retained patterns in other tests suggesting they were unaware of these memories. This suggests that as well as remembering meaningful sounds, like the melody of a song, people can also unknowingly memorize the complex pattern of arbitrary sounds, including ones they rarely encounter.</p><p>These findings provide new insights into how humans discover and recognize sound patterns which could help treat diseases associated with impaired memory and hearing. More studies are needed to understand what exactly happens in the brain as these memories of sound patterns are created, and whether this also happens for other senses and in other species.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>memory</kwd><kwd>sequential pattern</kwd><kwd>perception</kwd><kwd>auditory scene analysis</kwd><kwd>PPM</kwd><kwd>predictive coding</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/P003745/1</award-id><principal-award-recipient><name><surname>Chait</surname><given-names>Maria</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008721</institution-id><institution>University College London Hospitals NHS Foundation Trust</institution></institution-wrap></funding-source><award-id>Biomedical Research Centre Deafness and Hearing Problems Theme</award-id><principal-award-recipient><name><surname>Chait</surname><given-names>Maria</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Human listeners rapidly form robust, long lasting (up to 7 weeks) memories of rarely encountered, featureless sound sequences presented among many similar stimuli.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Memory is a crucial component of sensory perception, on multiple processing levels (<xref ref-type="bibr" rid="bib8">Bale et al., 2017</xref>; <xref ref-type="bibr" rid="bib63">Muckli and Petro, 2017</xref>). In the auditory modality, the ability to identify essentially any sound source, from footsteps to musical melody, requires the capacity to hold consecutive events in memory so as to link past and incoming information into a coherent emerging representation (<xref ref-type="bibr" rid="bib46">Koelsch et al., 2019</xref>; <xref ref-type="bibr" rid="bib58">McDermott et al., 2013</xref>; <xref ref-type="bibr" rid="bib93">Winkler et al., 2009</xref>). Whilst traditional models of sensory memory (e.g. <xref ref-type="bibr" rid="bib25">Cowan, 1998</xref>) argued that such sensory traces are characterized by short retention times and computational encapsulation, a large body of work has since revealed that observers can retain detailed sensory information implicitly, over long periods (<xref ref-type="bibr" rid="bib6">Arciuli and Simpson, 2012</xref>; <xref ref-type="bibr" rid="bib21">Chun, 2000</xref>; <xref ref-type="bibr" rid="bib41">Jiang et al., 2005</xref>; <xref ref-type="bibr" rid="bib45">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib91">Vogt and Magnussen, 2007</xref>; <xref ref-type="bibr" rid="bib94">Winkler and Cowan, 2005</xref>). A compelling instance was demonstrated by <xref ref-type="bibr" rid="bib2">Agus et al., 2010</xref>; (see also <xref ref-type="bibr" rid="bib3">Agus and Pressnitzer, 2013</xref>; <xref ref-type="bibr" rid="bib42">Kang et al., 2017</xref> who showed that naive listeners readily remembered certain spectro-temporal features of random noise bursts, such that reoccurring snippets were recognized weeks after initial exposure.</p><p>Here, we focus on long-term memory formation for arbitrary frequency patterns within rapidly unfolding sequences of discrete sounds. We ask whether naïve listeners can become sensitized to sparsely reoccurring tone sequences and investigate the conditions under which such memories are formed. To formalize the underlying psychological mechanisms, we simulate human performance with a probabilistic model of sequential memory (<xref ref-type="bibr" rid="bib36">Harrison et al., 2020</xref>; <xref ref-type="bibr" rid="bib67">Pearce, 2018</xref>).</p><p>The experimental design (<xref ref-type="fig" rid="fig1">Figure 1</xref>) capitalizes on a paradigm developed by <xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref> for measuring listeners’ sensitivity to complex acoustic patterns. Using fast sequences of short tones, they showed that listeners can rapidly detect the transition to a regularly repeating pattern (REG) from a sequence of random tones (RAN). Sequences were novel and too rapid to allow for conscious tracking, but on most trials, participants were able to respond soon after the onset of the second cycle of regularity, implicating an efficient memory for the immediate sequence context. Here, we ask how this memory is affected if the tone pattern was already experienced in the past.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Example stimuli.</title><p>Sequences were generated anew on each trial from a pool of 20 tone-pips of 50 ms duration each. RAN sequences were generated by randomly sampling from the full pool with replacement; RANREG sequences contained a transition from a random (RAN) to a regularly repeating cycles of 20 tone-pips (REG, cycles are marked with dashed lines). Therefore, the transition was manifested as a change in pattern only, whilst maintaining the same long-term first-order statistics. The transition (randomized between 3 and 4 s post onset) is indicated by a red line; the red dashed line marks the ‘effective’ transition – the point at which the pattern starts repeating and hence becomes statistically detectable. Participants were instructed to respond to such transitions (50% of trials) as soon as possible. STEP stimuli, containing a step change in frequency, (and their ‘no change’ control, CONT) were also included in the stimulus set for the purpose of estimating simple reaction time. Three (six in Exp. 4 and Exp. S1 in Appendix 1) particular regular patterns (REGr) were presented identically across three trials within a block (RANREGr). Reoccurrences were spaced ~3 min apart. Different REGr were used for each participant. A schematic representation of outputs from the observer model is provided to illustrate how pattern reoccurrence might affect reaction time. For each tone in a sequence, the model outputs information content (IC) as a measure of its unexpectedness, given the preceding context. After the transition from a RAN to REG pattern, the IC drops over a few consecutive tones, reflecting the discovery of the REG. The brain is hypothesized to be sensitive to this change in IC, and once sufficient evidence has been accumulated, the emergent regularity ‘pops out’ perceptually. Therefore, RTs to onset of regularities can be used to quantify the amount of sensory information (number of tone-pips), required to detect the increasing predictability within the unfolding sequence. The black solid lines indicate the crossing of this putative evidence threshold (when the information content becomes clearly distinguishable from the RAN baseline). For novel patterns (blue line), this typically occurs within the second cycle. For reoccurring patterns (yellow line), IC is expected to show an earlier drop, and therefore lead to faster RT (‘RT advantage’).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig1-v2.tif"/></fig><p>Reaction times in <xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref> were consistent with those obtained from an ideal-observer model based on prediction by partial matching (PPM; <xref ref-type="bibr" rid="bib65">Pearce, 2005</xref>; <xref ref-type="bibr" rid="bib67">Pearce, 2018</xref>). Shown to be an effective model of human auditory sequence learning on multiple time scales (<xref ref-type="bibr" rid="bib1">Agres et al., 2018</xref>; <xref ref-type="bibr" rid="bib27">Di Liberto et al., 2020</xref>; <xref ref-type="bibr" rid="bib38">Harrison and Pearce, 2018</xref>; <xref ref-type="bibr" rid="bib67">Pearce, 2018</xref>; <xref ref-type="bibr" rid="bib69">Pearce and Wiggins, 2006</xref>), this model proposes that listeners acquire an internal representation of the sound input by keeping track of multiple-order Markovian transition probabilities. This context is then used to evaluate the (un)expectedness of ensuing sounds by deriving a measure of surprisal (information content – IC; negative log probability). RAN and REG sequences differ in unexpectedness (high for RAN, low for REG). The transition from a random to a regular pattern (RANREG stimulus) can therefore be detected as a salient drop in information content in the model output (<xref ref-type="fig" rid="fig1">Figure 1</xref>) which reflects increasing compatibility between the incoming sounds and the stored context. The pattern of behavioural reaction times as well as brain response latencies recorded from naive, passively listening participants (<xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="bib82">Southwell et al., 2017</xref>; <xref ref-type="bibr" rid="bib83">Southwell and Chait, 2018</xref>) suggest that listeners indeed identify the emergence of regularity by detecting the associated drop in information content and that such tracking of instantaneous expectedness constitutes an automatic, inherent aspect of auditory sequence processing.</p><p>We used a combination of behavioural manipulation and modelling to examine the durations over which these memory representations are maintained by introducing rare pattern reoccurrences. One might expect that detection of regularities benefits not only from immediate sequence context, but also from traces accumulated over a longer period. Participants listened to RAN and RANREG sequences (as shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, see stimulus examples: 'Sound - RAN', and 'Sound - RANREG'), and were instructed to press a keyboard button as soon as possible when a transition to REG was detected. New sequences were generated on each trial, but unbeknownst to participants, a few different regular patterns reoccurred very sparsely (every ~3 min) across trials (RANREGr).</p><p>We hypothesized that, if the stored representation of a pattern strengthens through repetition, the information content associated with a transition to a familiar regularity will dip earlier than that associated with a novel regular pattern (<xref ref-type="fig" rid="fig1">Figure 1</xref>, yellow line in the cartoon model), reaching the putative detection threshold more quickly. Behaviourally, this should be revealed as faster reaction times to reoccurring patterns (‘RT advantage’ in <xref ref-type="fig" rid="fig1">Figure 1</xref>). The size of this effect may provide a window into the latent variables associated with the retention of sensory information in memory.</p><p>Several properties render this paradigm attractive. First, all sequences consist of the same 20 frequency ‘building blocks’. This simplifies parametrization and modelling of the task, while retaining sufficient pattern complexity (there are more than a trillion permutations of 20 frequencies). Second, these 20 frequencies are isochronous and occur with equal probability and roughly equal temporal density in all conditions: stimuli are thus matched in terms of long-term spectrum, average statistics and time patterning. The only difference between RAN and REG patterns and, importantly, between REG and REGr patterns, is the specific arrangement of these tone-pips over time. To distinguish a familiar regularity from a novel one, the specific tone-pip permutation must be remembered (we confirm this explicitly in Experiment 1B). Third, the task does not require listeners to memorize sounds <italic>explicitly</italic>: the emergence of the regularity readily pops out perceptually (see stimulus examples in supplementary materials). The task thus taps the process by which we automatically glean acoustic information from an ongoing sound-stream. Lastly, the sporadic presentation of REGr prevents them from becoming apparent to the listener, thereby allowing us to focus on putative implicit processes which underlie memory formation.</p><p>Across the experiments presented here, we ask whether human listeners form implicit long-term memories of sparsely reoccurring regular patterns (yes), whether this memory is robust to interference (yes), and whether it can be formed through passive exposure (partially). Through a combination of behavioural manipulation and modelling, we also demonstrate the interplay between short (a few seconds) and long (over many minutes) integration in the process of long-term memory formation. Overall, the results highlight the remarkable attunement of the auditory system to exceedingly sparse repeating patterns within the unfolding acoustic environment.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Participants listened to RAN, RANREG, RANREGr, CONT and STEP sequences as illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref> and were instructed to monitor for transitions. For each participant, different regularities were designated as reoccurring patterns (REGr). Critically, the RAN portion of RANREGr trials remained novel. Stimuli were presented in blocks of approximately 8 min each. Within each block, each REGr reoccurred three times (about 5% of the trials within a block) and was flanked by many novel patterns (RAN and RANREG).</p><p>The reaction time (RT) to STEP was subtracted from the RT to RANREG and RANREGr to estimate a lower bound measure of the time required to detect the emergence of regularity. RT values reported below are all baselined RTs (the raw RTs from which the RT to the STEP condition was subtracted).</p><p>Compared with RTs to the emergence of novel regularities (RANREG), we expected progressively faster RTs as regularities reoccur across the experiment (RANREGr), indicating that their representations have become retrievable from memory. We assess overall memory formation of REGr based on RTs averaged over all three reoccurrences within each block. However, we focus on RTs in each intra-block presentation to assess persistence of memory effects across experimental manipulations.</p><sec id="s2-1"><title>Experiment 1A: implicit long-lasting memory for three reoccurring patterns</title><p><xref ref-type="fig" rid="fig2">Figure 2A-D</xref> plots the mean and individual results of the regularity detection task performed in three sessions: five blocks on day 1, one block after 24 hr (‘24 hr’) and one block after 7 weeks, (‘7 w’). Participants were highly accurate in detecting regularities (<xref ref-type="fig" rid="fig2">Figure 2A</xref>): d’ plateaued at near ceiling performance after the first block. No difference was observed between hit rates for RANREG and RANREGr [no main effect of condition: F(1, 19) = .39, p = 0.539, η<sub>p</sub><sup>2</sup> = .02; no main effect of block: F(5, 90) = 0.46, p = 0.804, η<sub>p</sub><sup>2</sup> = .02; no interaction between condition and block: F(5, 90) = 1.10, p = 0.367, η<sub>p</sub><sup>2</sup> = .06].</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Experiment 1A (N = 19), 1B (N = 20): implicit long-lasting memory for three reoccurring patterns and specificity to sequential structure.</title><p>(<bold>A–D</bold>) Exp. 1A (three reoccurring targets). (<bold>A</bold>) Sensitivity to emergence of regularity (d') across blocks during the first session, as well as after 24 hr and after 7 weeks. Error bars indicate 1 s.e.m. (<bold>B</bold>) RT to the transition from random to regular pattern in RANREG and RANREGr conditions, across blocks. Error bars indicate 1 s.e.m. ‘<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> plots the RT advantage for each intra-block presentation. (<bold>C</bold>) Correlations between RT advantage at the end of the first day – block 5 – and after 24 hr (upper plot) and after 7 weeks (lower plot). Each data point represents an individual. Note N = 14 in the 7W data due to attrition. (<bold>D</bold>) The relationship between RTs for the RANREG and RANREGr conditions. Each data point represents an individual participant. Dots below the diagonal reveal faster detection of RANREGr compared with RANREG. These implicit memory effects were not linked to explicit memory. See <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> for explicit recognition estimates. (<bold>E–G</bold>) Exp. 1B (time reversal): (<bold>E</bold>) Sensitivity to emergence of regularity (<bold>d’</bold>) across blocks. (<bold>F</bold>) RT to the transition from random to regular pattern in RANREG and RANREGr conditions, across blocks. The block containing time-reversed REGr is shaded in yellow. The RT advantage dropped when REGr were time reversed, and restored in block 5. <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> plots the RT advantage for each intra-block presentation. ﻿(<bold>G</bold>) The relationship between RTs to the RANREG and RANREGr conditions in block 5.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Experiment 1A. RT advantage for each intra-block presentation.</title><p>The plot depicts the progressive emergence of an RT advantage with each presentation of REGr. Plotted values correspond to the RT advantage of REGr for each intra-block presentation. RTs of 1 st, 2nd or 3rd intra-block presentations were averaged across the different REGr, and RTs to novel REG were averaged across trials which occurred at the beginning (first third), middle or end of each block. There was no significant difference between the last presentation in block 5, and the first presentation after 24 hr, or between the last presentation after 24 hr and the first presentation after 7 weeks, indicating that the formed memory trace was preserved long term. Error bars indicate 1 s.e.m. Note that the RT for REGr is computed based on three trials and the effects are therefore rather noisy.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Experiment 1A. Explicit recognition estimates.</title><p>MCC coefficient (refer to Materials and methods) computed for the familiarity task performed after the regularity detection task in Exp. 1A. Each dot represents an individual participant. MCC was low overall, indicating low explicit recognition and did not correlate with the RT advantage (refer to main text).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Experiment 1B. RT advantage for each intra-block presentation.</title><p>Plotted values correspond to the RT advantage of REGr for each intra-block presentation. RTs of 1 st, 2nd or 3rd intra-block presentations were averaged across the different REGr, and RTs to novel REG were averaged across trials which occurred at the beginning (first third), middle or end of each block. The RT advantage dropped when REGr were time reversed and was restored when the original REGr were re-introduced (block 5). Error bars indicate 1 s.e.m. Note that the RT for REGr is computed based on three trials and the effects are therefore rather noisy.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig2-figsupp3-v2.tif"/></fig></fig-group><p>Despite the ceiling effects associated with pattern detection (mean hit rate = 97.3%), faster RTs in RANREGr than in RANREG (‘RT advantage’) were observed in all participants by the end of the first session (block 5; <xref ref-type="fig" rid="fig2">Figure 2D</xref>), indicating a clear implicit memory for the reoccurring patterns. A repeated measures ANOVA on RTs with condition (RANREG and RANREGr) and block as factors yielded a main effect of condition [F(1, 18) = 34.09, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .65], main effect of block [F(5, 90) = 9.24, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .3] and an interaction between condition and block [F(5,90) = 6.88, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .28]. Specifically, in the first block of the first session, performance did not differ between RANREG and RANREGr [t(18) = 0.794, p = 1]. By the end of the second block (after 6 REGr reoccurrences), a significant difference (~140 ms; 2.8 tones) between RTs was observed [REG – REGr: t(18) = 3.964, p = 0.006]. This difference grew over the following blocks (all ps &lt; 0.001), plateauing after block 3 (233 ± 0.17 ms; 4.7 tones). The RT advantage on the third block did not differ from the fourth [t(18) = −0.907, p = 1] nor from the fifth block [t(18) = −0.0003, p = 1]). In Experiment S1 (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>), we demonstrate that similar effects are obtained when doubling the number of REGr patterns to be memorised (six different patterns per participant). In Experiments S2A and S2B (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>), we further demonstrate that the memory trace is not abolished by introducing ‘interrupting blocks’ (in which REGr were not presented) between ‘standard blocks’ (in which REGr patterns reoccurred every ~3 min).</p><p>Critically, implicit memory for reoccurring regularities persisted after 24 hr and after 7 weeks: the RT difference between novel and reoccurring sequences remained constant between the last block of day 1 (block 5) and after 24 hr [t(18) = 0.139, p = 0.891], as well as between 24 hr and 7 weeks later [t(13) = −0.668, p = 0.515]. An inspection of intra-block reoccurrences (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) revealed that the RT advantage for REGr was similar between the third (last) intra-block presentation of day 1 and the first intra-block presentation after 24 hr [t(18) = 0.123, p = 0.903]; similarly, in the session conducted after 7 weeks, the RT advantage measured after the first intra-block presentation did not differ from the third (last) presentation in the session conducted after 24 hr [t(13) = 0 .958, p = 0.356; (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>)]. This suggests that the effect observed after 24 hr and 7 weeks reflects the presence of a lasting memory trace of reoccurring regularities rather than rapid within-block re-learning.</p><p>Further, we examined the correlation of individual participants’ RT advantage across the three sessions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). A robust correlation was found between the end of the first day (block 5) and the measurement taken after 24 hr (spearman’s rho = 0.635, p = 0.004) – participants who exhibited a larger RT advantage at the end of the first day were also those showing a larger advantage 24 hr later. A similar correlation was found with performance after 7 weeks (spearman’s rho = 0.740, p = 0.003). This confirms strong reliability of individual effects.</p></sec><sec id="s2-2"><title>The memory effects are not driven by explicit recognition of reoccurring patterns</title><p>Explicit memory for reoccurring regularities was examined at the end of each session by means of a familiarity task. Only regular sequences were presented: REGr (one presentation of each pattern) were intermixed with previously unheard REG patterns. Participants were instructed to indicate which patterns sounded ‘familiar’. Classification was evaluated using the MCC score (see Materials and methods) which ranges between 1 (perfect classification) to −1 (total misclassification). Whilst low overall, the mean MCC on each testing session indicated above chance performance [day 1: mean = 0.231; t(18) = 4.214, p &lt; 0.001; 24 hr: mean = 0.44, t(18) = 7.044, p &lt; 0.001; 7 w: mean = 0.360, t(13) = 5.204, p &lt; 0.001] (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). An improvement in MCC scores was observed between day 1 and 24 hr later [t(18) = −3.635, p = 0.004], suggesting potential consolidation. There was no change in MCC scores between the 24 hr session and 7 weeks later [t(13) = 0.348, p = 1].</p><p>Importantly, MCC scores did not correlate with the RT advantage: MCC on day 1 did not correlate with the RT advantage observed in block 5 (spearman’s Rho = 0.307; p = 0.201; a similar result was also obtained when pooling across participants from Exp. 1A and Exp. S1 (which used 6 REGr patterns, see Appendix 1) (Spearman’s Rho = 0.114; p = 0.493; N = 38). Though a weak correlation between RT advantage and MCC was measured after 24 hr (uncorrected; Spearman’s Rho = 0.459, p=0.048, N = 19), it disappeared after 7 weeks (Spearman’s Rho = −0.024, p = 0.934, N = 14). Therefore, implicit memory for reoccurring patterns, observed in nearly all participants, is not linked to explicit awareness of reoccurrence.</p></sec><sec id="s2-3"><title>Experiment 1B: Implicit memory is specific to sequential structure</title><p>﻿To confirm that the RT advantage effects are driven by memory of sequential structure, we tested whether implicit memory for reoccurring patterns is tolerant to time reversal of the originally learned patterns (<xref ref-type="fig" rid="fig2">Figure 2E–G</xref>). Participants performed the regularity detection task as in Exp. 1A over six experimental blocks. The first four were identical to those in Exp. 1A. In the fifth block, REGr sequences were replaced by time-reversed versions. In block 6, the original REGr were introduced again. Participants were naive to the experimental manipulation. It was expected that, if implicit memory is specific to the sequential structure of regularity, the RT advantage should disappear in the time-reversed block (see also <xref ref-type="bibr" rid="bib42">Kang et al., 2017</xref>).</p><p>Blocks 1–4 revealed the same effects as in Exp. 1A (<xref ref-type="fig" rid="fig2">Figure 2F</xref>) [main effect of condition: F(1, 19) = 71.96, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .79; main effect of block: F(3, 5) = 9.90, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .34; interaction condition by block: F(3, 57) = 5.67, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .23]. Specifically, in the first block RTs in the RANREGr condition were similar to those in RANREG [t(19) = 0.725, p = 1], but became progressively faster (114 ms; 2.27 tones) in the second block [t(19) = 3.56, p = .01], and across the remaining blocks (all ps &lt; 0.001) (203 ms; 4.1 tones in the 4th block).</p><p>Importantly, this RT advantage was abolished in the time-reversed block, but restored in the subsequent block containing the originally learned REGr: a repeated measures ANOVA with condition (RANREG and RANREGr) and the last two blocks as factors yielded a main effect of condition (F(1, 19) = 25.57, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .57), a main effect of block (F(1, 19) = 18.09, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .49), and an interaction condition by block (F(1, 19) = 40.03, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .68), demonstrating the significantly greater RT advantage (RANREG novel – RANREGr) in the last than in the time-reversed block [t(19) = 6.33, p &lt; 0.001]. The RT advantage for REGr in the third intra-block presentation of block 4 (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>) was greater than in the first intra-block presentation of the time-reversed block [t(19) = −2.261, p = 0.035], but similar to the first intra-block presentation of the last block reintroducing the original REGr [t(19) = 0.788, p = 0.440].</p><p>﻿ These results constrain the nature of the observed memory effect to sequential information.</p></sec><sec id="s2-4"><title>Experiment 2: Limited formation of memory traces of non-adjacent patterns</title><p>We tested whether <italic>adjacent</italic> repetition of patterns (as is inherently the case for REG sequences) is required for implicit memory to be formed (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Experiment 2 (N = 30): Limited formation of memory traces of non-adjacent patterns.</title><p>(<bold>A</bold>) In blocks 1 to 4, listeners were exposed to RAN, RANREG, RANREGr, PATinRAN and PATinRANr trials. An example spectrogram for a PATinRAN stimulus is provided. The non-adjacent repetitions of the 20-tones pattern (PAT) are indicated by dashed rectangles. In block 5 (‘test' block) PATinRANr sequences were replaced by versions where the two cycles were set adjacent at the end of the trial (RANREGr*). (<bold>B</bold>) Accuracy (block 1 to 4): hit rates are computed separately for adjacent (RANREG and RANREGr) and non-adjacent (PATinRAN and PATinRANr) trials. (<bold>C</bold>) Hit rates in block 4, separately for novel and reoccurring adjacent and non-adjacent conditions. ‘*’ indicates a significant difference between conditions. (<bold>D</bold>) RT (measured relative to the onset of the second cycle; see red line in A) across blocks 1 to 4 for RANREG, RANREGr, PATinRAN and PATinRANr.﻿ Error bars indicate 1 s.e.m. Note that since RT here is computed relative to the onset of the REG repetition, to compare RANREG RT with those reported in figures aboveadd 1 s. (<bold>E</bold>) Test block: RT advantage for RANREGr (yellow) and RANREGr* (green) in each intra-block presentation. Error bars indicate 1 s.e.m. To determine the presence of a memory trace to REGr* we specifically focus on the first intra-block presentation. ‘*’ indicates a significant RT advantage, ‘ns’ indicates an RT advantage not significantly different from 0.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig3-v2.tif"/></fig><p>Over four blocks, listeners were exposed to RAN, RANREG and RANREGr trials as in previous experiments. We also introduced a new condition, PATinRAN (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), which consisted of two identical <italic>non-adjacent</italic> 20-tone patterns (PAT) embedded within a random sequence of tone-pips. The second appearance always occurred at the end of the sequence. The first appearance was embedded partway through the sequence at an average distance of 1.7 s (range 0.5–2.9 s). To understand whether memories of non-adjacent patterns (PAT) can be formed during listening, three different PAT reoccurred three times within block (PATinRANr; the random parts of the sequences as well as the separation between the two PAT patterns remained random on each trial).</p><p>Both non-adjacent (PATinRAN, PATinRANr) and adjacent (RANREG, RANREGr) trials included two repetitions of each pattern with the only difference being that they were contiguous in the latter and separated by random tones in the former. Participants were instructed to respond if they detected two identical, not necessarily contiguous, 20-tone patterns within a trial; 50% of the trials consisted of fully random patterns. In order to make sure that participants paid equal attention to the (harder) PATinRAN sequences, accuracy was emphasized over response speed.</p><p>In the last block (block 5; ‘test' block), we tested whether, following a comparable amount of exposure through block 1 to 4, PATinRANr and RANREGr patterns were similarly remembered. To equate difficulty of pattern detection in this block, PATinRANr sequences were replaced by versions where the two cycles were set adjacent. We refer to these conditions as RANREGr*. Participants were instructed to respond as quickly as possible. We compared the magnitude of the RT advantage associated with RANREGr* to that associated with RANREGr.</p><p><xref ref-type="fig" rid="fig3">Figure 3B</xref> shows the detection performance during the exposure blocks (1 to 4). Despite having practised the PATinRAN condition, detection performance was overall worse, and substantially more variable in PATinRAN (mean over blocks 1–4: 47.36 ± 16.5%) relative to RANREG (88.47 ± 11.6%), and improved less across blocks [main effect of condition: F(1, 29) = 419.01, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .94; main effect of block: F(3, 87) = 9.24, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .24; interaction of condition per block: F(3, 87) = 4.83, p = 0.004, η<sub>p</sub><sup>2</sup> = .14]. Thus, whilst a pattern is highly detectable when contiguously repeated, performance drops substantially when the repetition is not adjacent, presumably due to limits on short-term memory.</p><p>Focusing on the 4th block (<xref ref-type="fig" rid="fig3">Figure 3C</xref>): a repeated measures ANOVA with the factors reoccurrence (novel/reoccurring patterns) and adjacency (adjacent/non-adjacent patterns) yielded a significant main effect of adjacency [F(1, 29) = 205.99, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .88]. As expected, whilst participants were very apt at detecting RANREG patterns, performance on PATinRAN was substantially more variable and lower overall. Interestingly a main effect of reoccurrence [F(1, 29) = 21.74, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .43], was also observed, with no interaction between the two factors [F(1, 29) = 3.95, p = 0.056, η<sub>p</sub><sup>2 </sup>= .12]. Therefore, detection data showed an increase in accuracy for reoccurring patterns in both adjacent and non-adjacent conditions. The emergence of this effect for RANREGr, despite its absence in Exp. 1A, is presumably driven by the below ceiling performance observed here (mean hit rate = 93% relative to 97.5% in Exp. 1A) – likely a consequence of the extra behavioural strain introduced by the PATinRAN stimuli. Critically, the finding of increased hit rates for PATinRANr (a mean increase of 15%) demonstrates that, through repeated exposure, listeners formed a memory trace for the non-adjacent patterns.</p><p>RT results across block 1 to 4 are shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref>. To allow for a comparison across conditions, RTs here are measured relative to the onset of the second regularity cycle (indicated with a red line in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Since participants were encouraged to prioritise accuracy over speed in these blocks, the RT data in blocks 1–4 were not statistically analysed. However, an RT advantage (reaching 131 ms, 2.63 tones in block 4) is clearly visible for RANREGr relative to RANREG stimuli.</p><p>Test block: as a critical test for the formation of memory traces, we assessed the presence of an RT advantage in the 1st intra-block presentation of RANREGr and RANREGr* (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). The RT advantage was significantly different from zero in RANREGr [one-sample t-test: t(29) = 3.724, p = 0.001], but not in the RANREGr* condition [one-sample t-test: t(29) = .419, p = 0.678]. A paired t-test further confirmed a greater RT advantage in the RANREGr than in the RANREGr* condition [t(29) = 3.169, p = 0.003]. This indicates that, as a group, participants did not demonstrate an immediate RT advantage to RANREGr* patterns. As seen in <xref ref-type="fig" rid="fig3">Figure 3E</xref>, an RT advantage in RANREGr* emerged following the second intra-block presentation. This effect may be associated with learning within the test block. A repeated measures ANOVA on RT advantage in the test block with the factors condition (REGr / REGr*) and intra-block presentation (1st / 2nd / 3rd) revealed a main effect of condition [F(1, 29) = 9.09, p = 0.005, η<sub>p</sub><sup>2</sup> = .24] but no main effect of intra-block presentation [F(2, 58) = 0.67, p = 0.515, η<sub>p</sub><sup>2</sup> = .02], or interaction [F(2, 58) = 1.27, p = 0.287, η<sub>p</sub><sup>2</sup> = .04], consistent with an overall smaller RT advantage to RANREGr*.</p><p>As an exploratory analysis, we tested whether higher detection accuracy for non-adjacent patterns (hit rates for PATinRANr / PATinRAN in block four) predicted a greater RT advantage when the patterns were set adjacently in the test block (REGr*). We observed a significant moderate correlation between the detection accuracy of PATinRANr in block four and the RT advantage in the 1st intra-block presentation of REGr* (spearman’s rho = 0.429, p = 0.018) such that those participants who exhibited a higher detection accuracy for PATinRANr in block 4, also demonstrated a higher RT advantage for REGr* in the test block. This correlation with RT advantage was specific to PATinRANr, in that it did not extend to PATinRAN (spearman’s rho = 0.017 p = 0.927) and held when the effect of detection accuracy for PATinRAN was accounted for (spearman’s rho = 0.465, p = 0.011). The specificity to PATinRANr suggests that the link is not simply related to some property of short-term memory (in which case we would have expected a correlation with PATinRAN as well), but it is specific to the memory advantage for PATinRANr stimuli which developed over the first four blocks.</p><p>Overall, these results suggest the presence of measurable (though small) memory traces for reoccurring, non-adjacent patterns (PATinRANr). However, it is clear that the formation of robust implicit memory traces for sound sequences depends on short-term memory (and hence benefits from immediate repetition of patterns) such that introducing a gap of even 2 s results in substantially weakened storage in memory.</p></sec><sec id="s2-5"><title>Modelling</title><p>We constructed a ‘memory constrained’ computational model, based on ‘prediction by partial matching’ (PPM; see Materials and methods) to provide a formal simulation of the psychological mechanisms underlying the process of memory trace formation, as observed in Experiments 1A (<xref ref-type="fig" rid="fig2">Figure 2</xref>), 2 (Figure 3) and S2A (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2K</xref>). These experiments reflect critical manipulations of the effect of long- and short-term memory decay. Although the existence of memory decay in humans is in general well established, ways of incorporating memory decay into probabilistic computational models of sequences processing is very much an active topic of research. Our PPM model implemented a single set of values (<xref ref-type="table" rid="table1">Table 1</xref>) that fully accounted for the dynamics of memory formation observed across experiments. As a benchmark, we also report the results for an equivalent unconstrained model (i.e. with perfect memory), as employed in previous research using the same paradigm (<xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Parameters for the memory-decay PPM model as manually optimized for Experiments 1A, 2, and S2A.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Parameter</th><th valign="top">Value</th></tr></thead><tbody><tr><td valign="top">Buffer capacity</td><td valign="top">15 items</td></tr><tr><td valign="top">Buffer weight</td><td valign="top">1</td></tr><tr><td valign="top">Short-term memory weight<sup>*</sup></td><td valign="top">1</td></tr><tr><td valign="top">Short-term memory duration<sup>*</sup></td><td valign="top">15 s</td></tr><tr><td valign="top">Long-term memory weight<sup>*</sup></td><td valign="top">0.02</td></tr><tr><td valign="top">Long-term memory half life</td><td valign="top">500 s</td></tr><tr><td valign="top">Long-term memory asymptote</td><td valign="top">0</td></tr><tr><td valign="top">Noise</td><td valign="top">1.3</td></tr><tr><td valign="top">Order bound</td><td valign="top">4</td></tr></tbody></table><table-wrap-foot><fn><p><sup>*</sup>The combination of STM weight, STM duration and LTM weight yields a STM half-life of 3.06 s.</p></fn></table-wrap-foot></table-wrap><p>The following cognitive hypotheses were instantiated:</p><list list-type="order"><list-item><p>Listeners learn sequence transition probabilities throughout the experiment. This approach is similar to other models of statistical learning (<xref ref-type="bibr" rid="bib15">Bröker et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Harrison et al., 2011</xref>; <xref ref-type="bibr" rid="bib60">Meyniel et al., 2016</xref>; <xref ref-type="bibr" rid="bib86">Takahasi et al., 2010</xref>) except the present model extends beyond first-order transition probabilities. Learning of sequence statistics is accomplished through partitioning the unfolding stimulus into sub-sequences of increasing order (n-grams) that are thereon stored in memory, such that the more a listener is exposed to a given <italic>n</italic>-gram, the stronger its salience (‘weight’). Here, we allow <italic>n</italic> to range between 1 and 5, corresponding to Markovian transition probabilities of orders 0 to 4.</p></list-item><list-item><p>The listener uses these <italic>n</italic>-gram statistics to quantify the predictability (IC, where high IC corresponds to low probability and low IC corresponds to high probability) of incoming tones based on the preceding portion of the sequence and other information stored in memory as a generative probabilistic model (represented by PPM, see Materials and methods).</p></list-item><list-item><p>Sudden changes in IC are indicative of potential changes in the environment. In the present case, a sudden drop in IC reflects the onset of repetitive structure in the stimulus corresponding to a transition from RAN to REG. Once the model is sufficiently confident that a reliable drop has occurred, it registers a ‘change detected’ response analogous to the participant’s button press.</p></list-item><list-item><p>The memory weight of a given n-gram observation decays over time, with this decay profile reflecting the dynamics of human auditory memory. In particular, we adopt the memory-weighting scheme recently presented in <xref ref-type="bibr" rid="bib36">Harrison et al., 2020</xref>, and implement the following decay profile for the memory salience of an n-gram observation: a) an initial short and high-fidelity steady-state phase, representing an echoic memory buffer; b) a fast exponential-decay phase, representing short-term memory; c) a slow exponential-decay phase, representing longer-term memory (see <xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="table" rid="table1">Table 1</xref> for more details). The model also adds noise to the memory retrieval stage, simulating inaccuracies in human memory retrieval.</p></list-item></list><p>Overall, the memory constrained model shows close qualitative correspondence to the pattern of RTs observed in Experiments 1 and 2, and specifically to the dynamics of the emergence of the RT advantage.</p><p><xref ref-type="fig" rid="fig5">Figure 5A</xref> shows model outputs for experiment 1A using an unconstrained (left) and constrained (right) PPM model. The imposed memory constraints are able to reproduce the slow dynamics of REGr memory formation: like the human participants, the constrained PPM model experiences a moderate facilitation effect that grows over successive presentations of identical regular patterns. <xref ref-type="fig" rid="fig4">Figure 4B</xref> illustrates this effect in more detail, plotting average information content profiles for RANREGr trials in block five as compared to RANREGr trials in block 1.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Memory constrained PPM model.</title><p>(<bold>A</bold>) Memory decay profile for the constrained PPM model. The curve describes the weight of a given n-gram observation in memory as a function of the number of consequent tones that have been presented, assuming a constant presentation rate of 20 Hz. The two dotted lines indicate transitions between the different phases of memory decay: the first, between the memory buffer and short-term memory, and the second, between short-term memory and long-term memory. The inset shows the transition from the memory buffer (of 15 tones capacity) to the fast exponential-decay phase. See <xref ref-type="table" rid="table1">Table 1</xref> for model parameters. (<bold>B</bold>) Information content as a function of tone number for RANREGr trials in blocks 1 and 5 of Exp. 1A. Mean Information content is computed from the memory-decay PPM model, expressed in bits, and averaged over all trials. The shaded ribbons correspond to 1 STDEV. Trials are aligned such that a tone number of 0 corresponds to the first REG tone after the transition. The transition between RAN and REG phases becomes clearest after about 24 tones; however, the model detects the transition faster in block five than in block 1, because it partially recognises the REGr cycle from its previous occurrences, yielding a lower information content that is more clearly distinguishable from the RAN baseline and therefore requires less evidence accumulation time (=faster detection). However, it is obvious from the large error bars that the effects are subtle.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig4-v2.tif"/></fig><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Model simulations for Experiments 1A and 2 for the unconstrained (left) vs. constrained (right) PPM model.</title><p>Overall, we demonstrate a qualitative similarity between the formal simulation of constrained memory and observed human responses. (<bold>A</bold>) Exp. 1A: the estimated RTs to the transition from random to regular patterns in RANREG and RANREGr conditions across five consecutive blocks. For RANREG trials, the REG patterns are novel for each trial and the unconstrained PPM model detects transitions after one complete cycle plus eight tones (about 1.4 s; Note that the model change point detection algorithm was configured with a strict threshold in order to achieve an appropriate Type I error rate , see Materials and methods). For RANREGr trials after the first block, the regular patterns are already familiar from previous trials. The unconstrained PPM model remembers these previous patterns perfectly and hence demonstrates an immediate drop in RT. In contrast, the constrained model readily captures human performance, whereby the RT advantage for RANREGr trials slowly grows over successive presentations of the REGr patterns. (<bold>B</bold>) Exp. 2: RT advantage in RANREGr and RANREGr* conditions for each intra-block presentation within the test block. Data are presented in the same way as in <xref ref-type="fig" rid="fig3">Figure 3E</xref>. The unconstrained model reveals an equal RT advantage in both conditions. In contrast, as exhibited by the human listeners, the constrained memory model does not learn the reoccurring non-adjacent patterns across blocks 1 to 4, as shown by the null RT advantage in the first intra-block presentation in the RANREGr* condition. Error bars indicate 1 s.e.m.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig5-v2.tif"/></fig><p>It is important to note that the steady long-term decay, which is a key feature of the memory constrained model predicts that the performance facilitation should disappear after 24 hr, and certainly after 7 weeks. After such time periods, the memory traces for the reoccurring patterns should decay to zero, and the corresponding facilitation effect should disappear. Remarkably, the participants exhibited unaltered performance facilitation. This suggests that the memory traces of these reoccurring patterns are somehow ‘fixed’ at a certain point during testing. One way of simulating this effect would be to change the asymptote of the exponential memory decay, such that the memory trace asymptotically approaches a small but non-zero value as time tends to infinity. However, we found that incorporating such an asymptote caused the performance facilitation for RANREGr trials to increase constantly from block to block, in contrast to the slow plateau shown in the behavioural data. It seems likely, therefore, that there remains a non-trivial ‘fixing’ effect that may reflect consolidation processes, not accounted for by the current model (to our knowledge there is no other statistical learning model that accounts both for learning dynamics and long-term fixed effects).</p><p>Experiment 2 investigated the effect of pattern adjacency on pattern detection and memory formation. We trained unconstrained and constrained models on blocks 1–4, and report their performance for the ‘test’ block (block 5). As expected, the unconstrained PPM model is unaffected by adjacency (<xref ref-type="fig" rid="fig5">Figure 5B</xref> left). The memory-decay PPM model (<xref ref-type="fig" rid="fig5">Figure 5B</xref> right) fully reproduces the behavioural data (<xref ref-type="fig" rid="fig3">Figure 3E</xref>).</p><p>Overall, the modelling successfully replicated the slow dynamics of memory formation exhibited by human listeners demonstrating that memory constrained transition-probability learning is a plausible computational underpinning of sequential pattern acquisition.</p></sec><sec id="s2-6"><title>Experiment 3: Memories of a set of reoccurring regularities are not overwritten by subsequent memorization of another set</title><p>Does memorization of a new set of REGr interfere with the representation of a previously memorized set? Participants performed the same transition detection task as in Exp. 1A. They were exposed to a set of three reoccurring patterns (REGr1) in the first three blocks, followed by three blocks in which another set of patterns (REGr2) reoccurred. Blocks 7 and 8 then re-tested memory for the reoccurring regularities of set 1 and set 2, respectively. After 24 hr, memory for the two sets was tested again.</p><p>Clear implicit memory for the first set of targets (REGr1), as indicated by an RT advantage, was observed after the 3rd block (<xref ref-type="fig" rid="fig6">Figure 6B</xref>) [main effect of condition: F(1, 28) = 41.01, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .59; main effect of block: F(3, 84) = 15.69, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .36; condition by block interaction: F(3, 84) = 6.83, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .20]. As expected, after three blocks of exposure the RT advantage in the RANREGr1 condition (163 ms – 3.3 tones) was similar to that observed in Exp. 1A above. Critically, this RT advantage for RANREGr1 was not perturbed after the presentation of the second set of regularities (REGr2) [RT advantage in block three vs. block 7: t(28) = .877, p = 0.387]. It also lasted after 24 hr [RT advantage in block seven vs. after 24 hr: t(28) = −0.553, p = 0.584], and was similar to the 24 hr RT advantage observed in Exp. 1A [no main effect of experiment: F(1, 50) = .33, p = 0.567, η<sub>p</sub><sup>2 </sup>= .01]. These results indicate that, once formed, memory traces are neither overwritten nor weakened by ‘interfering’ new sets of reoccurring patterns.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Experiment 3 (N = 29): memories of a set of reoccurring regularities are not overwritten by subsequent memorization of another set.</title><p>Participants were exposed to a set of three reoccurring patterns in the first three blocks (REGr1, yellow shading), followed by three blocks in which another set of patterns was reoccurring (REGr2, grey shading). The final blocks (7 and 8) tested memory for set 1 and 2, respectively. After 24 hr, memory for the two sets was tested again. (<bold>A</bold>) d’ across all blocks on day 1 and after 24 hr. Error bars indicate 1 s.e.m. (<bold>B</bold>) RT to the transition from random to regular pattern across blocks for RANREG, RANREGr1 and RANREGr2 on day 1 and after 24 hr.﻿ Error bars indicate 1 s.e.m. <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref> plots the RT advantage for each intra-block presentation. <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref> shows the RT data with N = 19.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Experiment 3.</title><p>(<bold>A</bold>) RT advantage for each intra-block presentation. Plotted values correspond to the RT advantage of REGr for each intra-block presentation. RTs to the 1st, 2nd or 3rd intra-block presentations were averaged across the different REGr, and RTs to novel REG were averaged across trials which occurred at the beginning (first third), middle or end of each block. The RT advantage for a set of reoccurring patterns (REG1; yellow traces) was not affected by the presentation of another set of REGr (REGr2) in blocks 4–6. Error bars indicate 1 s.e.m. Note that the RT for REGr is computed based on three trials and the effects are therefore rather noisy. (<bold>B</bold>) RT across blocks with N = 19. The overall pattern was identical to that observed with N = 30 participants (reported in the main text). The RT advantage for the first set of REGr1 observed across the first three blocks [main effect of condition: F(1, 18) = 24.16, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.57; main effect of block: F(3, 54) = 11.47, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.39; condition by block interaction: F(3, 54) = 3.08, p = 0.035, η<sub>p</sub><sup>2</sup> = 0.15] was not perturbed after the presentation of the second set of reoccurring sequences [RT advantage for RANREGr1 in block three vs. block 7: t(18) = .403, p = 0.691].</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig6-figsupp1-v2.tif"/></fig></fig-group><p>In blocks 4–6 presenting the second set of reoccurring regularities (REGr2) also showed an RT advantage, as demonstrated by the emerging separation between the RT to novel and reoccurring regularities. A repeated measures ANOVA on the RT advantage with ‘experimental stage’ (blocks 1–3, blocks 4–6) and block number (1st, 2nd or 3rd) showed a main effect of block number [F(2, 56) = 20.13, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.42; consistent with a growing RT advantage across blocks], and stage [F(1, 28) = 15.70, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.36] with no interactions. The main effect of stage suggests an overall larger RT advantage for the first set (REGr1). The noisier RT pattern observed in blocks 4–6 may be indicative of an order / fatigue effect. Importantly, at the end of day 1 the RT advantage for the two sets of reoccurring regularities did not differ (block seven vs. block 8: t(28) = 1.721, p = 0.096]. The RT advantage for the second set was maintained when tested after 24 hr (RT advantage of last block of day one vs. after 24 hr: t(28) = −0.277, p = 0.784), and did not differ from that of the first set [RT advantage after 24 hr for RANREGr1 vs. RANREGr2 t(28) = 1.848, p = 0.075].</p></sec><sec id="s2-7"><title>Experiment 4: Implicit memory is robust to pattern phase shifts</title><p>In all the previous experiments reoccurring regularities were always presented at the same phase of the REG cycle. Here we asked whether the resulting memory trace was anchored to this fixed boundary – that is, whether listeners remembered the pattern as a specific ‘chunk’ (<xref ref-type="bibr" rid="bib26">Dehaene et al., 2015</xref>; <xref ref-type="bibr" rid="bib89">Thiessen, 2017</xref>). If so, the RT advantage should reduce when REGr are phase shifted.</p><p>Listeners were presented with six reoccurring regularities (REGr) over three blocks. In block 4, identical REGr were presented but each presentation was with a shifted onset relative to the originally presented pattern (see <xref ref-type="fig" rid="fig7">Figure 7A</xref>, and Materials and methods).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Experiment 4 (N = 20): Implicit memory is robust to pattern phase shifts.</title><p>(<bold>A</bold>) In this experiment, six different reoccurring regularities (REGr) per participant were presented. In block 4 (yellow shading in C), these patterns were replaced by versions with shifted onset relative to the originally learned REGr. Two examples of phase shifted REGr and their original REGr version are depicted. The solid red line indicates the transition between RAN and REG (the onset of the regular pattern); the dashed red line denotes one cycle (20 tones) (<bold>B</bold>) d’ across all blocks. Error bars indicate 1 s.e.m. (<bold>C</bold>) RT to the transition from RAN to REG pattern across blocks for RANREG and RANREGr.﻿ The bottom plot represents the RT advantage observed in blocks 3 and 4. Error bars indicate 1 s.e.m. <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> plots the RT advantage for each intra-block presentation. (<bold>D</bold>) The individual RT advantage in block three compared with block 4. Each circle represents an individual participant. (<bold>E</bold>) Plotted is the relationship between RTs to RANREG and RANREGr in block 4. Each circle represents a unique REGr pattern (six per participant), plotted against the mean RT to RANREG for that participant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>RT advantage for each intra-block presentation.</title><p>Plotted values correspond to the RT advantage of the 6 REGr for each intra-block presentation. RTs of 1st, 2nd or 3rd intra-block presentations were averaged across the different 6 REGr, and RTs to novel REG were averaged across trials which occurred at the beginning (first third), middle or end of each block. The RT advantage was preserved after the introduction of a REGr phase shift. Note that this analysis is based on a small number of trials per ‘intra-block’ presentation condition, and effects are therefore noisy. Error bars indicate 1 s.e.m.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig7-figsupp1-v2.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig7">Figure 7C</xref> shows the progressive emergence of the RT advantage associated with the memorization of the reoccurring patterns [main effect of condition: F(1, 19) = 21.12, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .53; main effect of block: F(3, 57) = 18.52, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .49; condition by block interaction: F(3, 57) = 10.64, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .36]. Specifically, whilst in the first block performance did not differ between RANREG and RANREGr [t(19) = −0.876, p = 1], a faster RT to the RANREGr condition developed across ensuing blocks. This effect continued into block 4, where phase-shifting was introduced (<xref ref-type="fig" rid="fig7">Figure 7C</xref> bottom plot). The RT advantage for phase-shifted RANREGr (167 ms – 3.35 tones) in block 4 was greater than the RT advantage in block 3 (100 ms; 2 tones) [block three vs. block 4: t(19) = −13.111, p &lt; 0.001] in the majority of participants (<xref ref-type="fig" rid="fig7">Figure 7D</xref>), demonstrating a strengthening (rather than disappearing) memory effect. The immediate robustness to phase shifting was confirmed by comparing the RT advantage in the first intra-block presentation in block 4, to that in the third (last) intra-block presentation in block 3 (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). No significant difference was observed [t(19) = 1.069, p = 0.298], supporting the conclusion that the RT advantage persisted despite phase shifting.</p><p>Further tests confirmed that the RT advantage for REGr in block 4 was similar across small and large phase shifts: a repeated measures ANOVA with factor phase shift (small / large, namely 1–5 and 16–19 vs. 6–15 tones from the original onset) yielded no significant effect of phase shift on the RT advantage [F(1, 19) = 0.74, p = 0.400].</p><p>These results suggest that sequences are not represented as a fixed chunk of sequential items which is retrieved as a single unit, but more likely as a collection of sequential predictions that are flexibly retrieved from memory according to the available sensory information.</p><p>As a further probe into the nature of the representation of the pattern in memory, in Experiment S3 (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>) we investigated listeners’ tolerance to small frequency transpositions. We reveal a transfer of the RT advantage to the transposed pattern, suggesting that the formed representation is not of an exact echoic nature. It is possible that tolerance to frequency transposition reflects a ‘fuzzy’ spectral representation, although we note that the spacing in the present pool – 12% – is generally larger than the just noticeable difference (JND) for frequency typically exhibited by non-musically trained listeners (<xref ref-type="bibr" rid="bib88">Tervaniemi et al., 2005</xref>). Alternatively, the tolerance to transposition may suggest that instead of the specific frequency pattern, the auditory system maintains a representation of the contour, or inter-tone interval within the pattern.</p></sec><sec id="s2-8"><title>Experiment 5: Implicit memory can form when sounds are behaviourally irrelevant, but does not immediately transfer to behaviour</title><p>We asked whether memories for reoccurring patterns are formed when sequences are not behaviourally relevant. Naïve participants were exposed to three blocks of the same kind as in Exp. 1A, but instructed to detect the STEP changes only, and ignore the other sounds. In the fourth block (‘test’ block), they were instructed to also detect the RANREG transitions.</p><p>We analysed the performance in the test block of the pre-exposed group in comparison to the performance of a non pre-exposed ‘control’ group, formed by pooling block one data from several other experiments (<italic>Pooled data-block<sub>1</sub></italic>, N = 147, see Materials and methods). Sensitivity to transitions in the test block (<xref ref-type="fig" rid="fig8">Figure 8A</xref>) was high overall (mean d’ = 2.77 ± .73), but lower than in the first block of the control group [independent sample t(163) = −2.028, p = 0.044]. This is likely because, in order to keep them naive, participants did not receive training on RANREG detection.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Experiment 5 (N = 18): implicit memory can form when sounds are behaviourally irrelevant, but does not immediately transfer to behaviour.</title><p>During three initial blocks, participants were asked to respond only to the STEP trials and ignore the other sounds. In the following test block, they were instructed to also detect the RANREG transitions. (<bold>A</bold>) Sensitivity to emergence of regularity (<bold>d’</bold>) in the test block. Error bars indicate 1 s.e.m. (<bold>B</bold>) The relationship between RTs to the RANREG and RANREGr conditions in the test block. Each data point represents an individual participant. Dots below the diagonal indicate faster detection of RANREGr compared with RANREG. (<bold>C</bold>) RT advantage in the pre-exposed and the control group (participants without previous exposure; see Materials and methods). ﻿Error bars indicate 1 s.e.m. ‘*’ indicates a significant difference. (<bold>D</bold>) Bootstrap resampling-based distributions of the RT advantage for the 1st, 2nd and 3rd intra-block presentation from the control group. The mean of the distribution is indicated by blue dashed lines. Red dots indicate the data from the present experiment (pre-exposed group). One-tailed p-values are reported with each graph.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig8-v2.tif"/></fig><p>In the test block (<xref ref-type="fig" rid="fig8">Figure 8B</xref>), the mean RT to RANREGr was significantly faster than that to novel RANREG [t(17) = 3.1, p = 0.006], consistent with the presence of an RT advantage. The RT advantage in the pre-exposed group (~157 ms, 3.14 tones) was substantially greater than in the control group (~30 ms, 0.6 tones) [independent sample t(163) = 3.023, p = 0.003], indicating a beneficial effect of pre-exposure.</p><p>As a critical test for the presence of a memory trace after pre-exposure, we examined RT in each intra-block presentation of REGr. If memories for reoccurring patterns are formed during pre-exposure, an RT advantage should be exhibited immediately - at the first presentation of REGr in the test block. One sample t-tests demonstrated that an RT advantage was absent at the first and second intra-block presentations [t(16) = 0.377, p = 0.711; t(17) = 1.691, p = 0.109], but emerged at the third presentation of REGr [t(17) = 3.954, p = 0.001]. We also compared the RT advantage, across intra-block presentations, between the pre-exposed and control groups. A bootstrap approach (see Materials and methods) was used to generate a distribution of performance over subsets of 20 participants drawn from the control group and to compare with the actually observed performance in the pre-exposed group (<xref ref-type="fig" rid="fig8">Figure 8D</xref>). The plots in <xref ref-type="fig" rid="fig8">Figure 8D</xref> show distributions of the RT advantage for the 1st, 2nd and 3rd REGr presentation in the control group. The mean RT advantage of the ‘pre-exposed’ group is shown by the red dots. This analysis revealed that the RT advantage to the 1st presentation did not differ from the control group. However, a difference emerged after the 2nd presentation. This suggests that, by the 2nd appearance of REGr in the ‘test’ block, the passively pre-exposed group exhibited substantially faster responses than non pre-exposed participants. The difference between the passively pre-exposed group and the control group grew further by the 3rd presentation.</p><p>Overall, these results demonstrate that implicit memory was not present at the onset of the test block (as evidenced by the lack of an RT advantage); however, learning occurred more rapidly in the pre-exposed listeners such that by the end of the test block, they exhibited a substantially higher RT advantage than that shown by the control group.</p><p>Explicit memory was poor (mean MCC = 0.064) and did not correlate with the RT advantage measured in the test block [Spearman’s Rho = 0.235; p = 0.347].</p></sec><sec id="s2-9"><title>Across-experiment analysis reveals that most patterns are remembered and most participants exhibit implicit memory</title><p>We quantified the robustness of the memory effect for reoccurring patterns across the different experiments reported here. <xref ref-type="fig" rid="fig9">Figure 9A</xref> shows the distribution of RTs for RANREG vs. RANREGr pooled from block three data, (i.e. after nine presentations of each REGr; approx. 25 min of listening) where most data from different experiments were available (the pilot experiment, Experiment 1A, 1B, 3, 4, S1, and S3). In <xref ref-type="fig" rid="fig9">Figure 9B</xref> each dot represents the mean RT for RANREG vs. RANREGr of an individual participant (N = 147). 88.4% of participants exhibited an RT advantage, which we interpret as revealing implicit memory for REGr.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Individual variability in implicit memory.</title><p>(<bold>A</bold>) Cumulative distribution function (left) and distribution (right) of RTs to RANREG and RANREGr pooled from block 3 of several experiments (see Materials and methods). A two sample Kolmogorov-Smirnov test confirmed a significant difference in cumulative probability (D = 0.232, p &lt; 0.001) (<bold>B</bold>) The relationship between RTs to the RANREG and RANREGr conditions in block 3. Each circle represents an individual participant. (<bold>C</bold>) Distribution of RT advantages across 558 different REGr patterns as measured after three blocks (9 presentations of REGr). Values &gt; 0 indicate faster RTs to REGr relative to novel REG. (<bold>D</bold>) Distributions of the RT advantage in each block. To estimate the distribution of the RT advantage across the population (of young, healthy participants) we pooled data from several experiments (see Materials and methods) in which participants performed the standard regularity detection task. Pooled data-block<sub>1</sub> reflects the distribution of RT advantage after one block (3 presentations of REGr), Pooled data-block<sub>2</sub> reflects the distribution of the RT advantage after two blocks (6 presentations of REGr), etc. The distributions are computed via a bootstrapping process whereby on each iteration (1000 overall), data from 20 participants are chosen randomly (with replacement), to obtain an average RT advantage. The mean of each distribution is indicated by blue dashed lines. Overall these distributions demonstrate a robust emergence of an RT advantage after the first block.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-fig9-v2.tif"/></fig><p>We also tested the generality, across patterns, of the observed memory effect. It is important to note that all REGr were similar in the sense that all are composed from the same set of tones and only differed in the specific permutation of their order. <xref ref-type="fig" rid="fig9">Figure 9C</xref> plots a distribution of the RT advantage per unique REGr (558 overall). Though the data are inherently noisy (RT is quantified as an average over only three presentations in block 3), RT advantage appears to be normally distributed with 75.6% of patterns exhibiting a memory effect. This demonstrates that the observed effects are not driven by particularly ‘memorable’ REGr sequences. The same analysis run over block five data (not shown; N unique REGr = 165) showed that 84.4% of REGr were associated with an RT advantage after 15 reoccurrences. <xref ref-type="fig" rid="fig9">Figure 9D</xref> plots the distributions of group RT advantage per block, based on performance observed across all of the experiments reported (see Materials and methods). A gradual build-up of RT advantage is seen across blocks reaching a mean of 5.5 tones by the end of block 5.</p><p>Overall the results demonstrate that the memory effect generalizes to most (healthy, young) listeners and is not driven by particular memorable stimuli.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We used rapid sequences of discrete sounds (<xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="bib82">Southwell et al., 2017</xref>; <xref ref-type="bibr" rid="bib98">Zhao et al., 2019</xref>) specifically structured to allow for detailed behavioural and model-based investigation of memory formation. All sequences were generated from a fixed set of 20 frequencies, with the only difference being the order in which these were presented. Participants performed a regularity detection task and were oblivious to rare reoccurrences of certain patterns throughout the session. However, reaction times to new vs. previously encountered regularities demonstrated that following limited exposure to reoccurring patterns listeners retained sequential information in long-term memory. Statistics of pattern learning across experiments revealed that most patterns were remembered, and most participants exhibited a memory effect, although the size of this effect varied across individuals. Memory was implicit, resistant to interference, and preserved over remarkably long durations (over 7 weeks). Importantly, we also demonstrate that local pattern repetition was critical for long-term memory formation. This finding highlights a key role for immediate reinforcement and implicates an interplay between rapid and slow memory decay in supporting the formation of enduring memories of arbitrary sound sequences.</p><p>Overall the results reveal the brain’s remarkable capacity to implicitly preserve arbitrary sequential information in long-term memory.</p><sec id="s3-1"><title>Relationship to ‘noise memory’</title><p>The general behavioural pattern revealed here is reminiscent of the ‘noise memory’ effect first shown by <xref ref-type="bibr" rid="bib2">Agus et al., 2010</xref> (see also <xref ref-type="bibr" rid="bib3">Agus and Pressnitzer, 2013</xref>; <xref ref-type="bibr" rid="bib5">Andrillon et al., 2015</xref>; <xref ref-type="bibr" rid="bib33">Gold et al., 2014</xref>; <xref ref-type="bibr" rid="bib44">Keller and Sekuler, 2015</xref>; <xref ref-type="bibr" rid="bib53">Luo et al., 2013</xref>). In that study naïve listeners readily remembered reoccurring white-noise snippets presented amongst novel noise bursts. The learning was unsupervised, rapid, implicit and lasted upwards of 2 weeks.</p><p>Inspections of the nature of this memory revealed that it was robust to time reversal and even to scrambling into bins as small as 10–20 ms, indicating that the remembered features reflect local spectro-temporal idiosyncrasies within the reoccurring noise snippet (<xref ref-type="bibr" rid="bib2">Agus et al., 2010</xref>; <xref ref-type="bibr" rid="bib90">Viswanathan et al., 2016</xref>). The apparent dependence of this memory on certain local features of the noise signal may also explain the high inter-sample variability often seen with this paradigm (e.g., the distinction between ‘memorable’ and ‘not memorable’ patterns; <xref ref-type="bibr" rid="bib2">Agus et al., 2010</xref>; <xref ref-type="bibr" rid="bib90">Viswanathan et al., 2016</xref>; <xref ref-type="bibr" rid="bib42">Kang et al., 2017</xref>).</p><p>In contrast, here we focus on fast memory formation for <italic>sequences</italic> of discrete tones, distinguishable only by their specific order, and presented in a surrounding context of highly similar patterns (all sequences consisted of the same 20 ‘building blocks’). We showed that the vast majority of patterns were learned, revealing high sensitivity to reoccurring arbitrary frequency patterns despite the exceedingly rare reoccurrence rate (every ~3 min; 5% of trials; in contrast to the much more frequent reoccurrence, &lt;~15 s in <xref ref-type="bibr" rid="bib2">Agus et al., 2010</xref> and <xref ref-type="bibr" rid="bib42">Kang et al., 2017</xref>).</p><p>An important question for future work will be to determine whether these effects draw on similar or distinct neural systems (discussed further below).</p></sec><sec id="s3-2"><title>Memory for auditory sequences</title><p>Signals based on tone-pip patterns have long been used to understand the effect of auditory memory on listeners’ perception of sound sequences (e.g. <xref ref-type="bibr" rid="bib92">Watson et al., 1975</xref>; <xref ref-type="bibr" rid="bib7">Atienza and Cantero, 2001</xref>; <xref ref-type="bibr" rid="bib64">Näätänen et al., 1993</xref>; <xref ref-type="bibr" rid="bib81">Schröger et al., 1992</xref>; <xref ref-type="bibr" rid="bib87">Tervaniemi et al., 2001</xref>; <xref ref-type="bibr" rid="bib62">Moldwin et al., 2017</xref>). However, these paradigms are predominantly based on extensive exposure (in the order of hundreds of consecutive repetitions) to a single pattern.</p><p>Of particular relevance is a large body of work, broadly referred to as ‘statistical learning’, which has demonstrated the brain’s capacity to discover repeating structure in random stimulus sequences (<xref ref-type="bibr" rid="bib24">Conway and Christiansen, 2005</xref>; <xref ref-type="bibr" rid="bib32">Frost et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib78">Saffran et al., 1999</xref>; <xref ref-type="bibr" rid="bib79">Saffran and Kirkham, 2018</xref>). The classic paradigm (<xref ref-type="bibr" rid="bib77">Saffran et al., 1996</xref>; <xref ref-type="bibr" rid="bib80">Santolin and Saffran, 2018</xref>) involves a small set of syllables arranged into short ‘words’ (e.g., three syllables each). A few minutes’ exposure to such structured streams leads to learning of the statistical structure of the unfolding sequence such that subjects can distinguish the repeatedly occurring ‘words’ from a random arrangement of syllables.</p><p>Our results can be interpreted as reflecting similar implicit learning processes. However, in contrast to the demonstrations above which usually involved one or a small number of stimuli that are repeated many times, we show that a very sparse presentation of long patterns, which are intermixed with many highly similar sequences, is sufficient for robust memories to be formed.</p><p>Note that to focus on implicit memory formation, we placed our listeners in rather extreme conditions, both in terms of presentation rate of reoccurring targets and their complexity. It is possible that relaxing these constraints would result in stronger (but perhaps more explicit) memories.</p><p>We showed that listeners can learn at least six concurrently presented REGr patterns (Exp. 4 and Exp. S1 in Appendix 1). Important questions for future work involve understanding the capacity limits on this memory and the factors which might affect subsequent forgetting.</p><p>Overall, we demonstrate that the brain is tuned to retain repeated structure in our acoustic environments, even when such reoccurrences are exceedingly infrequent and the signals are highly similar.</p><p>Preserving as much information as possible from the unfolding sensory input is important for an organism because the relevance of any single event is not always immediately apparent, but is rather inferred post-hoc, e.g., through repetition (“I’ve heard this exact pattern twice within 3 min, therefore it might reflect an individual sound source rather than random noise in the forest&quot;; e.g., <xref ref-type="bibr" rid="bib57">McDermott et al., 2011</xref>; <xref ref-type="bibr" rid="bib95">Woods and McDermott, 2018</xref>). Our results hint at the heuristics utilized by the brain in determining how representations of statistical structure in the sensory environment are converted from transient to stable forms of memories (<xref ref-type="bibr" rid="bib48">Leimer et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Li and van Rossum, 2020</xref>).</p></sec><sec id="s3-3"><title>Reaction time as a measure of memory formation</title><p>We used reaction time (RT) as a proxy for memory formation. RT allowed us to determine how much information was required for listeners to detect repeating (REG) structure and to compare these measures with formal models of sequence processing. We hypothesized that reoccurrence would increase the weight of sequence components in memory resulting in faster detection of regularity. RT thus provided a sensitive means for tracking the formation and maintenance of such memories over time.</p><p>We observed that the RT to REGr steadily shortened with increasing number of reoccurrences, allowing us to measure the dynamics of memory trace establishment. The ‘RT advantage’, defined as the difference in RT between novel and reoccurring REG patterns, grew rapidly over the first three blocks (9 reoccurrences) and then stabilized, though evidence from <xref ref-type="fig" rid="fig9">Figure 9D</xref> suggests a continuous slow growth throughout the experimental session. The absence of correlation between the familiarity test and the RT advantage suggests a dissociation between implicit memory and explicit recall abilities.</p></sec><sec id="s3-4"><title>Time scales of memory for sequences</title><p>The basic behavioural task required participants to detect the transitions from RAN to REG – namely the emergence of repeating structure. As such it fundamentally relied on auditory short-term memory: in order to detect REG patterns, the listener must compare incoming tones to those that occurred at least a cycle ago. The effect of reoccurrence suggested that listeners also draw on much longer-term memory whereby information about previously encountered sounds is retained over minutes between successive REGr presentations.</p><p>Due to practical issues related to providing breaks, all of the reported experiments were subject to fixed presentation parameters: the experimental session was divided into blocks of roughly 8 min and REGr were presented three times per block. We therefore only have a relatively coarse estimate of the properties of the long-term memory store. Lengthening of inter-reoccurrence intervals was probed by introducing interrupting blocks where only novel patterns were presented (see Exp. S2A-B in Appendix 1). Memory was largely maintained over ~10 min intervals indicating a very slow long-term decay. In conjunction, the results of Exp. 2 suggested that the short-term memory store is critical for long-term memory formation. Participants were markedly impaired at detecting pattern repetition when the two cycles were separated by a brief series of random tones (about 2 s). Those conditions were also associated with substantially reduced long-term memories for the reoccurring patterns, indicating that immediate reinforcement is critical for the formation of lasting memory traces. These observations point to an integral interplay between a short (few seconds) and much longer (at least a few minutes) integration in the process of formation of robust, implicit memories for reoccurring arbitrary sound sequences.</p><p>The persistence of a stable RT advantage 24 hr and 7 weeks after initial exposure demonstrates the establishment of a long-term memory representation, possibly through a process of consolidation involving long-lasting synaptic changes (<xref ref-type="bibr" rid="bib71">Phan et al., 2017</xref>; <xref ref-type="bibr" rid="bib75">Redondo and Morris, 2011</xref>). It may also be tempting to interpret the resistance to interruption, observed in early stages of memory formation (Exp. 3, Exp. S2 in Appendix 1), as a hint that a form of consolidation might have occurred already after a few initial presentations.</p><p>In animal models, repetitive exposure to sound tokens (though, notably at a much higher rate than that used here) has been shown to evoke a process of long-lasting adaptation manifested as sparser firing and increased response specificity. These effects, persisting for hours to days after the initial stimulation, have been observed in primary and secondary auditory areas in song birds (﻿Caudal Medial Nidopallium; <xref ref-type="bibr" rid="bib18">Cazala et al., 2019</xref>; <xref ref-type="bibr" rid="bib40">Honda and Okanoya, 1999</xref>; <xref ref-type="bibr" rid="bib52">Lu and Vicario, 2014</xref>; <xref ref-type="bibr" rid="bib59">Menyhart et al., 2015</xref>; <xref ref-type="bibr" rid="bib86">Takahasi et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Chew et al., 1996</xref>; <xref ref-type="bibr" rid="bib84">Soyman and Vicario, 2019</xref>) and in secondary auditory cortex in ferrets (<xref ref-type="bibr" rid="bib51">Lu et al., 2018</xref>). The hypothesis that similar processes might back the behavioural effects we report is appealing.</p><p>Agus et al. proposed that mechanisms based on spike-timing-dependent plasticity (STDP; <xref ref-type="bibr" rid="bib54">Markram et al., 1997</xref>; <xref ref-type="bibr" rid="bib55">Masquelier et al., 2008</xref>; <xref ref-type="bibr" rid="bib56">Masquelier et al., 2009</xref>) may be possible neural underpinnings for rapid noise memory formation: repeatedly presented, but relatively temporally confined, spectro-temporal ‘constellations’ within the noise snippets may evoke coincident firing among auditory afferents leading to rapidly emerging selectivity for this feature in subsequent presentations of the same noise burst. <xref ref-type="bibr" rid="bib42">Kang et al., 2017</xref> suggested that including a degree of temporal integration can also account for similar effects observed with temporal patterns (<xref ref-type="bibr" rid="bib42">Kang et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Karmarkar and Buonomano, 2007</xref>; <xref ref-type="bibr" rid="bib50">Lim et al., 2017</xref>; see also <xref ref-type="bibr" rid="bib11">Bi and Poo, 2001</xref>). As will be discussed below, the behavioural pattern observed here is consistent with sequential information being stored as short sub-sequences (n-grams), that is without retaining the full 20-item sequence. Therefore, a form of STDP, incorporating an integration time of several hundred milliseconds, may underpin the representation of n-grams and implement their increased weight through reoccurance, thus supporting memory for discrete tone sequences.</p><p>On a systems level, accumulating evidence suggests that an interaction between auditory cortex and the hippocampus may play a role in memory formation. Previous work has implicated the hippocampus in sensitivity to sensory patterns across rapid time scales (<xref ref-type="bibr" rid="bib4">Aly et al., 2013</xref>; <xref ref-type="bibr" rid="bib85">Stachenfeld et al., 2017</xref>; <xref ref-type="bibr" rid="bib97">Yonelinas, 2013</xref>) and specifically in the process of discovering RAN-REG transitions (<xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref>). There is also some evidence that hints at its possible role in supporting long-term memory for acoustic patterns (<xref ref-type="bibr" rid="bib47">Kumar et al., 2014</xref>).</p></sec><sec id="s3-5"><title>What is being remembered?</title><p>The RT advantage to REGr reflects an implicit memory of sequential structure (Exp. 1B). But what, specifically, is remembered? Clearly participants did not perfectly memorize the full pattern, in that this would have been associated with much faster RTs (e.g. as exhibited by the observer with unconstrained memory, <xref ref-type="fig" rid="fig5">Figure 5A</xref>). Instead, by the end of block 3, the distribution of RT to REGr shifted leftwards by about four tones, without otherwise changing (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). Modelling suggests that this performance is consistent with a statistical-learning effect whereby the participants retained imperfect memory of patterns presented earlier in the experiment. These memories are not strong enough to prompt immediate recognition of a pattern heard in a past trial, but they are sufficiently strong to speed the recognition of that pattern once it begins repeating in the new trial.</p><p>Similar to other models of statistical learning (<xref ref-type="bibr" rid="bib15">Bröker et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Harrison et al., 2011</xref>; <xref ref-type="bibr" rid="bib60">Meyniel et al., 2016</xref>), our memory-constrained PPM model explicitly assumes that listeners represent the unfolding sequences in the form of n-gram sub-sequences of variable length, from which transition probabilities are computed. Previous computational, behavioural and neuroimaging studies <xref ref-type="bibr" rid="bib12">Bianco et al., 2020</xref>; <xref ref-type="bibr" rid="bib23">Conklin and Witten, 1995</xref>; <xref ref-type="bibr" rid="bib27">Di Liberto et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Egermann et al., 2013</xref>; <xref ref-type="bibr" rid="bib66">Pearce et al., 2010</xref>; <xref ref-type="bibr" rid="bib68">Pearce and Wiggins, 2004</xref>; <xref ref-type="bibr" rid="bib69">Pearce and Wiggins, 2006</xref> demonstrated that PPM successfully generalizes to prediction of musical sequences and effectively accounts for psychophysiological responses to melodies. In particular, PPM provided a good match to brain response latencies evoked by transitions between RAN and REG patterns (<xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="bib83">Southwell and Chait, 2018</xref>), suggesting that listeners may rely on similar memory representations as those proposed by the model. Here, the memory constrained version of PPM was able to successfully simulate human performance - concretizing how the interplay between short- and long-term decay might give rise to the progressive emergence of a memory trace across presentations. Whether listeners do indeed represent auditory patterns in this way is a matter of ongoing debate (e.g. <xref ref-type="bibr" rid="bib89">Thiessen, 2017</xref>). Additional support for an n-gram-like representation is provided in Exp. 4, which demonstrated that the REGr RT advantage is robust to pattern phase shifts. This finding indicates that REG patterns are not encoded in memory as rigid chunks of sequential items (<xref ref-type="bibr" rid="bib70">Perruchet and Pacton, 2006</xref>; <xref ref-type="bibr" rid="bib89">Thiessen, 2017</xref>), but are instead represented as a transition rule which allows for flexible retrieval. Whilst further empirical evidence is essential to determine the nature of the memory representation, the insight into single-trial level dynamics derived from the present modelling (<xref ref-type="fig" rid="fig4">Figure 4</xref>) may be useful for constraining the search for the physiological underpinnings of these phenomena. Furthermore, the model can readily be applied to statistical learning in other modalities (reviewed by <xref ref-type="bibr" rid="bib32">Frost et al., 2019</xref>) and even in other species, including songbirds such as finches, known to be capable of statistical learning (<xref ref-type="bibr" rid="bib59">Menyhart et al., 2015</xref>; <xref ref-type="bibr" rid="bib86">Takahasi et al., 2010</xref>).</p><p>A related question pertains to the generalizability of the present model to natural sounds beyond quantized sequences, such as those used here. In order to relate listeners’ performance to a measure of statistical information within unfolding signals, simplifying assumptions are necessary. This includes the presence of a prior stage of category formation which converts a continuous sound into discrete units that form the model’s ‘alphabet’. Accumulating evidence is indeed revealing that unsupervised segmentation of unfolding sounds into basic elements, perhaps using envelope-based cues, may be an inherent feature of listening (<xref ref-type="bibr" rid="bib28">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Doelling et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Hickok and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib72">Poeppel, 2003</xref>).</p></sec><sec id="s3-6"><title>Does sequence memory require attention?</title><p>The short-term memory mechanisms which allow listeners to discover the emergence of repeating structure (RANREG) in rapid tone sequences have been demonstrated to operate automatically, even when listeners’ attention is directed away from sound: brain activity recorded from naïve, distracted listeners reveals robust responses to RAN-REG transitions with latencies consistent with those expected from an ideal observer (<xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="bib82">Southwell et al., 2017</xref>; <xref ref-type="bibr" rid="bib83">Southwell and Chait, 2018</xref>).</p><p>In contrast, in Exp. 5, we demonstrated that longer term memory trace formation appears to require attentive processing in that there was no evidence for an immediate RT advantage when listeners moved from the exposure blocks, in which patterns were behaviourally irrelevant, to the active detection (‘test’) block. This suggests that the formation of lasting memories for sound patterns is not fully automatic, or does not immediately translate to behaviour. Whether this is driven by absence of attention per se or other factors is difficult to determine. For example, it is possible that the reduced memory effect when sounds are not behaviourally relevant is driven by decreased arousal or reward, known to substantially modulate learning (<xref ref-type="bibr" rid="bib10">Beste and Dinse, 2013</xref>; <xref ref-type="bibr" rid="bib14">Braun et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Polley, 2006</xref>; <xref ref-type="bibr" rid="bib96">Yebra et al., 2019</xref>), and which likely distinguish active detection (where feedback was provided after each trial) from passive listening.</p><p>Importantly, we showed that though implicit memory was not present at the onset of the test block, learning occurred more rapidly in the pre-exposed listeners, hinting at the presence of pre-exposure-related latent traces that may contribute to faster instantiation of representations in memory once the sequences become behaviourally relevant (<xref ref-type="bibr" rid="bib31">Frankland et al., 2019</xref>).</p></sec><sec id="s3-7"><title>Conclusion</title><p>Uncovering how memory traces are encoded and preserved by the brain is crucial for understanding subsequent learning operations which drive pattern recognition and generalization. We showed that representations of sporadically reoccurring rapid sound patterns are retained in memory, thus facilitating detection when previously encountered patterns reoccur. In spite of the fact that the patterns were relatively featureless and undistinctive compared to real-world stimuli, this memory was robust, implicit, remarkably resistant to interruption, and persisted over long durations, revealing human listeners’ astonishing sensitivity to reoccurring structure in the auditory environment. Important questions for future work include understanding the neurobiological foundations of these behavioural effects, the limits on the capacity of the memory store(s) involved and the factors which might affect subsequent forgetting.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Stimuli</title><p>Stimuli were sequences of 50 ms tone-pips of different frequencies generated at a sampling rate of 44.1 kHz and gated on and off with 5 ms raised cosine ramps; the total sequence duration was 7 s (140 tones). Frequencies were drawn from a pool of twenty values logarithmically spaced between 222 and 2000 Hz; 12% steps. The order in which these frequencies were successively distributed defined different conditions that were otherwise identical in their spectral and timing profiles (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). RAN sequences consisted of tone-pips arranged in random order, with the constraint that adjacent tones were not of the same frequency. Each frequency occurred equiprobably across the sequence duration. The RANREG condition contained a transition between a random (RAN), and a regularly repeating pattern (REG): sequences with initially randomly ordered tones changed into regularly repeating cycles of 20 frequencies (an overall cycle duration of 1000 ms; new on each trial). The change occurred between 3000 and 4000 ms after sequence onset such that each RANREG sequence contained between 3 to 4 REG cycles (only two in Exp. 2, see below). RAN and RANREG conditions were generated anew for each trial and occurred equiprobably. Thus, each trial contained exactly the same frequency ‘building blocks’, with the same overall distribution, and only varied in the specific order of tone-pips. The inter-trial interval was jittered between 1400 and 1800 ms.</p><p>Unbeknown to participants, a few different REG patterns (different for each participant) reoccurred identically several times within the session (RANREGr condition). Reoccurrences happened three times per block (every ~3 min), and 9–15 times per session, depending on the number of blocks in the specific experiment. Note that the RAN part preceding each REGr was always novel. Reoccurrences were distributed within each block such that they occurred at the beginning (first third), middle and end of each block.</p><p>Two control conditions were also included within each block: sequences of tones of a fixed frequency (CONT), and sequences with a step change in frequency partway through the trial (STEP). The STEP condition served as a measure of individuals’ reaction time to simple acoustic changes. The RT to STEP was subtracted from the RT to RANREG sequences to obtain a lower bound measure of computation time required to detect the transition.</p><p>Participants were instructed to respond, by pressing a keyboard button, as soon as possible after detecting a RANREG transition. Feedback about response accuracy and speed was delivered at the end of each trial. Since RT is a key performance measure in these experiments, it was important to motivate the participants to respond as quickly as possible. The feedback was given based on our previous work (<xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref>), and consisted of a green circle if the response fell within 2200 ms from the regularity onset in the RANREG conditions, or within 300 ms from the change of tone in the STEP condition. For slower RTs, an orange circle (between 2200 and 2600 ms in the RANREG conditions, and between 300 and 600 ms in the STEP condition) or a red circle were displayed. It was explained to participants that they should strive to obtain as much ‘green’ or ‘orange’ feedback as possible. The experimental session was delivered in ~8 min blocks, separated by brief breaks. Stimuli were presented with PsychToolBox in MATLAB (9.2.0, R2017a) in an acoustically shielded room and at a comfortable listening level (self-adjusted by each listener).</p></sec><sec id="s4-2"><title>Participant numbers</title><p>We initially ran a pilot experiment (N = 20, 16 females, age 23.5 ± 2.95 years) which consisted of five consecutive blocks as in Exp. 1A. The effect size for the main effect of condition (RANREG vs. RANREGr) was η<sub>p</sub><sup>2</sup> = .48 and η<sub>p</sub><sup>2</sup> = .79 after the first 3 and 5 blocks respectively. Using η<sub>p</sub><sup>2 </sup>= 0.48 for a prospective power calculation (beta = 0.8; alpha = 0.05) yielded a required sample size of N = 13. We decided to increase our sample size up to N = 20 to account for possible drop outs due to low accuracy. The research ethics committee of University College London approved the experiment [Project ID Number]: 1490/009, and written informed consent was obtained from each participant.</p></sec><sec id="s4-3"><title>Experiment 1a</title><p>The transition detection task was performed in three sessions: five blocks on day one (‘day1’), one block after 24 hr (‘24 hr’) and another block after 7 weeks (‘7 w’). Each block consisted of 60 stimuli (~8 min duration; 3 RANREGr x three reoccurrences per block, 18 RANREG, 27 RAN, 3 STEP, and 3 CONT). Feedback about the response accuracy and speed was delivered after each trial. Before starting, a short training block of 12 trials (with the same conditions as in the main experiment, but no RANREGr) was performed to acquaint participants with the task.</p><p>The familiarity task was performed at the end of each session (day1, 24 hr, 7 w). In these tests the three REGr patterns were randomly intermixed with 18 novel REG sequences. Participants were informed that a ‘handful’ of patterns reoccurred during the just completed session and asked to indicate, by means of a button press, if each presented pattern sounded ‘familiar’.</p><p>Participants. Twenty paid individuals (ten females; average age, 24.4 ± 3.03 years) took part in the experiment. Because of poor accuracy (d’ &lt; 2 after the first block), one participant was excluded from the analysis. We were able to test only 14 participants after 7 weeks (eight females; average age, 24.7 ± 3.02 years). No participant reported hearing difficulties.</p></sec><sec id="s4-4"><title>Experiment 1b</title><p>Participants performed the transition detection task for six consecutive blocks consisting of the same set of stimuli described for Exp. 1A. In the 5th block, each REGr was time reversed.</p><p>Participants. Twenty paid individuals (13 females; average age = 24.25 ± 3.58 years) took part in the experiment. No participant reported hearing difficulties.</p></sec><sec id="s4-5"><title>Experiment 2</title><p>The stimulus set in the initial four blocks contained RANREG and RANREGr stimuli, as before, except they contained only two repeating cycles after the transition. To understand whether immediate repetition is necessary for memory to be formed two further conditions were used: PATinRAN stimuli contained two identical 20 tone patterns embedded amongst random tones (mean separation of 1.7 s; drawn randomly from a range. 5–2.9; the second appearance always occurred at the end of the sequence as shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Similar to REGr, three different PAT were designated as reoccurring across trials (different for each participant; three reoccurrences per block). The embedding RAN sequence and the spacing between the two PAT patterns were randomly set for each reoccurrence. Overall each block contained 82 stimuli (36 RAN, 9 RANREG, 9 RANREGr, 9 PATinRAN, 9 PATinRANr, 5 STEP, 5 CONT), with ISI between 2.4 and 2.8 s. Reoccurrences of RANREGr and PATinRANr occurred approximately every 3.6 min.</p><p>Participants were informed of the presence of PATinRAN and RANREG stimuli (but were naïve about RANREGr and PATinRANr) and were instructed to indicate, by button press, if they detected the presence of a repeating pattern within the just-heard sequence. Feedback was provided at the end of each trial as in the above experiments, except that in the PATinRAN conditions we delivered a green circle if the response fell within 1200 ms from the second cycle onset, a red circle if the response was slower that 1600 ms, and an orange one if it fell in between. It was explained to participants that they should be fast but prioritise accuracy, given the generally difficult level of the task.</p><p>In order to quantify any memory effects, in the 5th block (‘test’ block) each of PATinRANr sequences were replaced by sequences with the two cycles set adjacently. We will refer to this condition as RANREGr*. The test block contained 36 RAN, 18 RANREG, 9 RANREGr, 9 RANREGr*, 5 STEP, 5 CONT Stimuli were about 5.45 ms long (~109 tones).</p><p>Participants. Given the task complexity and expectation for a reduced SNR, we increased the number of participants, a-priori, by 50% relative to the previous experiment. Thirty paid individuals (twenty females; average age, 24.26 ± 3.8 years) took part in the experiment. No participants reported hearing difficulties.</p></sec><sec id="s4-6"><title>Experiment 3</title><p>This experiment consisted of two days of testing. On the first day participants performed a transition detection task as in Exp. 1A, but two different sets of reoccurring patterns (REGr1 and REGr2; three different patterns each) were presented. REGr1 was presented over the first three blocks, and REGr2 over the subsequent three blocks. On day 2 (after 24 hr), participants returned to the lab to perform two test blocks for the two sets of reoccurring regularities, REGr1 and REGr2 (order counterbalanced across participants).</p><p>Participants. We initially ran 20 participants (one excluded from analysis), but decided to run an additional 10 participants (+two excluded), to increase the SNR for the memory effects observed for the RANREGr1 and RANREGr2 conditions on day two. The results with N = 19 yielded qualitatively similar results (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref>). Thirty-two paid individuals (twenty females; average age, 24.5 ± 3.8 years) took part in the experiment. No participant reported hearing difficulties. ﻿Because of poor accuracy (d’ &lt; 2 after the first block), three participants were excluded from the analysis.</p></sec><sec id="s4-7"><title>Experiment 4</title><p>Participants performed the detection task through four consecutive blocks of 82 stimuli each. The stimulus set included the same conditions as described for Exp. 1A, but with six, instead of three, REGr sequences, each presented three times within a block (6 RANREGr x three reoccurrences per block, 18 RANREG, 36 RAN, 5 STEP, and 5 CONT). In block 4, REGr were phase shifted (see examples in <xref ref-type="fig" rid="fig7">Figure 7A</xref>). To ensure uniform sampling of possible phase shifts, for each REGr in block 4, each of the three intra-block presentations was subject to pattern phase shift of 2 to 7, 8 to 13, or 14 to 19 tones from the onset of the original pattern. The phase shift was determined independently for each REGr and each intra-block presentation. Stimulus duration was 6.5 s, and the transition time was between 3 and 3.5 s from the sequence onset. Different REGr patterns reoccurred sparsely (every ~3.4 min) across trials and blocks.</p><p>Participants. Twenty paid individuals (fourteen females; average age, 23.5 ± 3.2 years) took part in the experiment. No participant reported hearing difficulties.</p></sec><sec id="s4-8"><title>Experiment 5</title><p>The experiment consisted of four blocks. The stimulus structure was as in Exp. 1A, except that for the first three blocks participants were instructed to respond to STEP changes only. They received no explanation about the regularity structure of the stimuli, and performed no practice. On the fourth block, they were instructed to detect RANREG transitions in addition to STEP transitions. Each block contained 72 stimuli (3 RANREGr x three reoccurrences per block, 18 RANREG, 27 RAN, 9 STEP, and 9 CONT; ISI between 900 and 1300); the number of STEP and CONT trials was increased relative to that in Experiment 1A due to the task change. As in Exp. 1A, participants performed the familiarity task at the end of the session.</p><p>Participants. Nineteen paid individuals (14 females; average age, 23.4 ± 3.1 years) took part in the experiment. No participant reported hearing difficulties. One participant was excluded from the analysis ﻿because of poor accuracy (d’ &lt; 1).</p></sec><sec id="s4-9"><title>Statistical analysis</title><p>In the transition detection task, two indexes of performance were computed: sensitivity (d’) and reaction time (RT).</p><p>For each participant and each block, d’ was quantified over trials (collapsed over RANREG and RANREGr) to give a general measure of sensitivity to the presence of regularities. Responses to RANREG and RANREGr, which occurred after the nominal transition were considered hits; Responses to RAN trials were considered false alarms. Participants who showed d’ &lt; 2 after the first block of the transition detection task were excluded from the analysis. Because Exp. five had only one ‘active’ block and no previous training, we adopted a more lenient exclusion criterion of d’ &lt; 1. Note that d’ was not available in Exp. 2 because of the intermixed nature of the presentation of RANREG and PATinRAN stimuli. To quantify performance, we therefore focus on hit rates and false alarms. For the purpose of participant exclusion, we computed an overall d’ (collapsing across conditions) and set the threshold at d’ &lt; 1.5.</p><p>Only RTs of correct trials (hits) were analysed. In all experiments, RT was defined as the time difference between the onset of the regular pattern (‘nominal transition’ in <xref ref-type="fig" rid="fig1">Figure 1</xref>) and the participant’s button press. However, Exp. 2 contained conditions with non-contiguous pattern presentations. RT was therefore computed from the onset of the second cycle (as indicated in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Across all experiments, RTs which occurred before the transition to the regularity (see <xref ref-type="fig" rid="fig1">Figure 1</xref>; ~1.3% of the trials) were considered to indicate a false positive and excluded from the analysis. To control for individual latency of motor response to a simple acoustic change, RTs were then ‘baselined’ by subtracting the individual mean RT to the STEP transition. Moreover, for each participant and block, the RTs beyond 2 SD from the mean were discarded.</p><p>To quantify the formation of a memory trace over REGr presentations, RT were averaged to yield a mean RT per condition per subject per block. Therefore, RT to RANREGr were based on nine trials (3 REGr x three presentations per block). However, to evaluate the immediate presence of a memory trace following certain experimental manipulations (e.g. in Exp. 2 and 5) or when re-testing after 24 hr or 7 weeks (as in Exp. 1A) we also analysed RT for each intra-block presentation (the first, second and third intra-block instance of a REGr pattern; see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>; <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>; <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>; <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>; <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1D</xref>; <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2D-J</xref>; <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3D</xref>). To calculate the ‘RT advantage’ for each intra-block presentation, mean RTs of 1st, 2nd or 3rd intra-block presentation (averaged across the different REGr) were subtracted from the mean RTs of novel REG which occurred at the beginning (first third), middle or end of each block.</p><p>Performance was statistically tested with linear analyses of variance (ANOVA) implemented in the R environment (version 0.99.320) using the ‘ezANOVA’ function (<xref ref-type="bibr" rid="bib61">Michael Lawrence, 2016</xref>). The analysis of d’ modelled the repeated measures factor block (1: N blocks). The analysis on RTs modelled the repeated measures factors: condition (RANREG / RANREGr), block (1: N blocks), and their interaction term. P-values were Greenhouse-Geisser adjusted when sphericity assumptions were violated. Post hoc t-tests were used to test for differences in performance between conditions across blocks or experiments. A Bonferroni correction was applied by multiplying p values by the number of comparisons (resulting values were capped at 1.0).</p><p>As a benchmark (see <xref ref-type="fig" rid="fig9">Figure 9D</xref>) across which to compare the effect of various manipulations on the RT advantage (i.e., <xref ref-type="fig" rid="fig8">Figure 8D</xref>, <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2C-G</xref>), we pooled data from several experiments to obtain a distribution of RT advantage values after each block: <italic>Pooled data-block<sub>1</sub></italic>, <italic>Pooled data-block<sub>2</sub></italic>, <italic>Pooled data-block<sub>3</sub></italic> were formed by pooling block 1, 2 or 3 data, respectively, from Experiments 1A, 1B, 3, 4, S1, S3, and pilot experiment identical to Exp. 1 (total N = 147). <italic>Pooled data-block<sub>4</sub></italic> was formed by pooling block four data from Experiments 1A, 1B, S1, S3 and the pilot (total N = 98), and <italic>Pooled data-block<sub>5</sub></italic> by pooling block 5 from Experiments 1A, S1 and the pilot (total N = 58). To obtain distributions of expected RT advantage values, data in each set were subjected to bootstrap resampling (1000 iterations) where, on each iteration, N random participants (N = number of participants in the experiment under examination) were drawn from the full pool, and their mean RT advantage (RANREG- RANREGr) was computed. This procedure yielded a distribution to which the actual data from the experiment under examination were compared. The p values provided (i.e., <xref ref-type="fig" rid="fig8">Figure 8D</xref>, <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2C-G</xref>) reflect the probability of the measured RT advantage (red dots in the relevant figures) relative to the benchmark distribution.</p></sec><sec id="s4-10"><title>Analysis of the familiarity task</title><p>The familiarity measurement required participants to categorize the presented patterns into ‘familiar’ (REGr) or ‘new’ (REG). Each REGr was presented once only, to avoid learning during the testing session and hence the ‘familiar’ category included only three items (six in Exp. S1, see Appendix1). These were presented among a larger set of foils (18 in Exp. 1A and Exp. 5, 36 in Exp. S1). Due to the small number of REGr, standard signal detection approaches are not useable. Instead, we computed the MCC score, which is a measure of the quality of a binary classification, applicable even when classes are of different sizes (<xref ref-type="bibr" rid="bib13">Boughorbel et al., 2017</xref>; <xref ref-type="bibr" rid="bib74">Powers, 2007</xref>). The coefficient ranges between 1 (perfect classification) to −1 (total misclassification) and is calculated using the following formula: <inline-formula><mml:math id="inf1"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mroot><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced><mml:mfenced separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced><mml:mfenced separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced><mml:mfenced separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mroot></mml:mrow></mml:mfrac></mml:math></inline-formula>. Where TP = number of true positives; TN = number of true negatives; FP = number of false positives; FN = number of false negatives. The MCC scores obtained for each participant in Exp. 1A are shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>.</p></sec><sec id="s4-11"><title>PPM-decay model</title><p>Prediction by Partial Matching (PPM) is a Markov modelling technique (<xref ref-type="bibr" rid="bib22">Cleary and Witten, 1984</xref>) that models statistical structure within symbolic sequences by tabulating occurrences of <italic>n</italic>-grams within a training dataset. PPM is a variable-order Markov model, meaning that it generates predictions by combining <italic>n</italic>-gram models of different orders; here we use a model combination technique called ‘interpolated smoothing’ (<xref ref-type="bibr" rid="bib16">Bunton, 1996</xref>; <xref ref-type="bibr" rid="bib17">Bunton, 1997</xref>; see also <xref ref-type="bibr" rid="bib68">Pearce and Wiggins, 2004</xref>; <xref ref-type="bibr" rid="bib36">Harrison et al., 2020</xref>; for more details). This approach combines the advantages of both the structural specificity afforded by high-order n-gram predictions and the statistical reliability afforded by low-order n-gram predictions.</p><p>The PPM models used in prior cognitive research <xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Cheung et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Gold et al., 2019</xref> have a ‘perfect’ memory, in that historic <italic>n</italic>-gram observations are preserved with the same fidelity as recent events, and are weighted the same in prediction generation. Noting that human memory exhibits clear capacity limitations and recency effects, <xref ref-type="bibr" rid="bib36">Harrison et al., 2020</xref> modified PPM to incorporate a customizable decay kernel, whereby historic <italic>n</italic>-gram observations are down-weighted as a function of the time elapsed and the consequent <italic>n</italic>-grams observed since the initial observation. Modelling reaction-time data from a RANREG paradigm similar to <xref ref-type="bibr" rid="bib9">Barascud et al., 2016</xref>, Harrison et al. concluded in favour of a capacity-limited high-fidelity echoic memory buffer followed by a lower-fidelity short-term memory phase with exponential decay. We likewise use an echoic-memory phase and a short-term memory phase in the present work, but add a slower-decaying long-term memory phase in order to capture the long-term learning observed in the present experiment.</p><p>The modelling aimed to reproduce behavioural performance qualitatively rather quantitatively. Many simplifications are made including that inter-sequence intervals, and breaks between experimental blocks are modelled at a fixed rate of 1 s. We explored various parameter settings for the model, and retained the configuration that best reproduced the observed behavioural patterns in Experiments 1A, 2, and S2A (<xref ref-type="fig" rid="fig5">Figure 5</xref>, and <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2K</xref>), which represent the key manipulations of memory duration. The resulting parameters are listed in <xref ref-type="table" rid="table1">Table 1</xref>; the decay kernel is plotted in <xref ref-type="fig" rid="fig4">Figure 4A</xref>. Further implementation details are described in <xref ref-type="bibr" rid="bib36">Harrison et al., 2020</xref>. The model outputs a conditional probability estimate for each tone in each sequence experienced throughout an experiment, which we convert to information content (the negative log probability in base 2). An implementation of this model is freely available in our open-source R package ‘ppm’ (<ext-link ext-link-type="uri" xlink:href="https://github.com/pmcharrison/ppm">https://github.com/pmcharrison/ppm</ext-link>; <xref ref-type="bibr" rid="bib37">Harrison, 2020</xref>).</p><p>To identify changes in the information content profile corresponding to the RANDREG transition on a given trial, we use the nonparametric changepoint detection algorithm of <xref ref-type="bibr" rid="bib76">Ross et al., 2011</xref>, which sequentially applies the Mann-Whitney test to identify changes in a time series’ location while controlling for Type I error. Here, the target Type I error rate was set to 1 in 10000 tones. Note that, for simplicity, the change point detection algorithm is free of memory constraints. Human listeners likely use a rougher (less detailed) statistical representation for transition detection.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Barathy Ganeshakumara and Hera Ahmad for help with the behavioural data collection and Alain de Cheveigne for comments and discussion. This research was funded by a BBSRC grant (BB/P003745/1) to MC and supported by the NIHR UCLH BRC Deafness and Hearing Problems Theme.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Supervision, Validation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Data curation, Formal analysis, Supervision, Funding acquisition, Validation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: The research ethics committee of University College London approved the experiment, and written informed consent was obtained from each participant. Project ID Number: 1490/009.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Sound - RAN sequence.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-56073-supp1-v2.mp4"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Sound - RANREG sequence.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-56073-supp2-v2.mp4"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-56073-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The datasets for this study can be found in the OSF repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/DTZS3">https://doi.org/10.17605/OSF.IO/DTZS3</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Bianco</surname><given-names>R</given-names></name><name><surname>Harrison</surname><given-names>P</given-names></name><name><surname>Pearce</surname><given-names>M</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Long-term implicit memory for sequential auditory patterns in humans</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/OSF.IO/DTZS3</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agres</surname> <given-names>K</given-names></name><name><surname>Abdallah</surname> <given-names>S</given-names></name><name><surname>Pearce</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Information-Theoretic properties of auditory sequences dynamically influence expectation and memory</article-title><source>Cognitive Science</source><volume>42</volume><fpage>43</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1111/cogs.12477</pub-id><pub-id pub-id-type="pmid">28121017</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agus</surname> <given-names>TR</given-names></name><name><surname>Thorpe</surname> <given-names>SJ</given-names></name><name><surname>Pressnitzer</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Rapid formation of robust auditory memories: insights from noise</article-title><source>Neuron</source><volume>66</volume><fpage>610</fpage><lpage>618</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.014</pub-id><pub-id pub-id-type="pmid">20510864</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agus</surname> <given-names>TR</given-names></name><name><surname>Pressnitzer</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The detection of repetitions in noise before and after perceptual learning</article-title><source>The Journal of the Acoustical Society of America</source><volume>134</volume><fpage>464</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1121/1.4807641</pub-id><pub-id pub-id-type="pmid">23862821</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aly</surname> <given-names>M</given-names></name><name><surname>Ranganath</surname> <given-names>C</given-names></name><name><surname>Yonelinas</surname> <given-names>AP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Detecting changes in scenes: the Hippocampus is critical for strength-based perception</article-title><source>Neuron</source><volume>78</volume><fpage>1127</fpage><lpage>1137</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.018</pub-id><pub-id pub-id-type="pmid">23791201</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrillon</surname> <given-names>T</given-names></name><name><surname>Kouider</surname> <given-names>S</given-names></name><name><surname>Agus</surname> <given-names>T</given-names></name><name><surname>Pressnitzer</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Perceptual learning of acoustic noise generates memory-evoked potentials</article-title><source>Current Biology</source><volume>25</volume><fpage>2823</fpage><lpage>2829</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.09.027</pub-id><pub-id pub-id-type="pmid">26455302</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arciuli</surname> <given-names>J</given-names></name><name><surname>Simpson</surname> <given-names>IC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Statistical learning is related to reading ability in children and adults</article-title><source>Cognitive Science</source><volume>36</volume><fpage>286</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1111/j.1551-6709.2011.01200.x</pub-id><pub-id pub-id-type="pmid">21974775</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atienza</surname> <given-names>M</given-names></name><name><surname>Cantero</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Complex sound processing during human REM sleep by recovering information from long-term memory as revealed by the mismatch negativity (MMN)</article-title><source>Brain Research</source><volume>901</volume><fpage>151</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1016/S0006-8993(01)02340-X</pub-id><pub-id pub-id-type="pmid">11368962</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bale</surname> <given-names>MR</given-names></name><name><surname>Bitzidou</surname> <given-names>M</given-names></name><name><surname>Pitas</surname> <given-names>A</given-names></name><name><surname>Brebner</surname> <given-names>LS</given-names></name><name><surname>Khazim</surname> <given-names>L</given-names></name><name><surname>Anagnou</surname> <given-names>ST</given-names></name><name><surname>Stevenson</surname> <given-names>CD</given-names></name><name><surname>Maravall</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning and recognition of tactile temporal sequences by mice and humans</article-title><source>eLife</source><volume>6</volume><elocation-id>e27333</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.27333</pub-id><pub-id pub-id-type="pmid">28812976</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barascud</surname> <given-names>N</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns</article-title><source>PNAS</source><volume>113</volume><fpage>E616</fpage><lpage>E625</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508523113</pub-id><pub-id pub-id-type="pmid">26787854</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beste</surname> <given-names>C</given-names></name><name><surname>Dinse</surname> <given-names>HR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Learning without training</article-title><source>Current Biology</source><volume>23</volume><fpage>R489</fpage><lpage>R499</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.04.044</pub-id><pub-id pub-id-type="pmid">23743417</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname> <given-names>G</given-names></name><name><surname>Poo</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Synaptic modification by correlated activity: hebb's postulate revisited</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>139</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.139</pub-id><pub-id pub-id-type="pmid">11283308</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname> <given-names>R</given-names></name><name><surname>Ptasczynski</surname> <given-names>LE</given-names></name><name><surname>Omigie</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pupil responses to pitch deviants reflect predictability of melodic sequences</article-title><source>Brain and Cognition</source><volume>138</volume><elocation-id>103621</elocation-id><pub-id pub-id-type="doi">10.1016/j.bandc.2019.103621</pub-id><pub-id pub-id-type="pmid">31862512</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boughorbel</surname> <given-names>S</given-names></name><name><surname>Jarray</surname> <given-names>F</given-names></name><name><surname>El-Anbari</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimal classifier for imbalanced data using matthews correlation coefficient metric</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0177678</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0177678</pub-id><pub-id pub-id-type="pmid">28574989</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braun</surname> <given-names>EK</given-names></name><name><surname>Wimmer</surname> <given-names>GE</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Retroactive and graded prioritization of memory by reward</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4886</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-07280-0</pub-id><pub-id pub-id-type="pmid">30459310</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bröker</surname> <given-names>F</given-names></name><name><surname>Marshall</surname> <given-names>L</given-names></name><name><surname>Bestmann</surname> <given-names>S</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Forget-me-some: general versus special purpose models in a hierarchical probabilistic task</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0205974</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0205974</pub-id><pub-id pub-id-type="pmid">30346977</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bunton</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>On-Line Stochastic Processes in Data Compression</source><publisher-name>University of Washington</publisher-name></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bunton</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Semantically motivated improvements for PPM variants</article-title><source>The Computer Journal</source><volume>40</volume><fpage>76</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1093/comjnl/40.2_and_3.76</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cazala</surname> <given-names>A</given-names></name><name><surname>Giret</surname> <given-names>N</given-names></name><name><surname>Edeline</surname> <given-names>JM</given-names></name><name><surname>Del Negro</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neuronal encoding in a High-Level auditory area: from sequential order of elements to grammatical structure</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>6150</fpage><lpage>6161</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2767-18.2019</pub-id><pub-id pub-id-type="pmid">31147525</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname> <given-names>VKM</given-names></name><name><surname>Harrison</surname> <given-names>PMC</given-names></name><name><surname>Meyer</surname> <given-names>L</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Haynes</surname> <given-names>J-D</given-names></name><name><surname>Koelsch</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Uncertainty and surprise jointly predict musical pleasure and Amygdala, Hippocampus, and auditory cortex activity</article-title><source>Current Biology</source><volume>29</volume><fpage>4084</fpage><lpage>4092</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.09.067</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chew</surname> <given-names>SJ</given-names></name><name><surname>Vicario</surname> <given-names>DS</given-names></name><name><surname>Nottebohm</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A large-capacity memory system that recognizes the calls and songs of individual birds</article-title><source>PNAS</source><volume>93</volume><fpage>1950</fpage><lpage>1955</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.5.1950</pub-id><pub-id pub-id-type="pmid">8700865</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chun</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Contextual cueing of visual attention</article-title><source>Trends in Cognitive Sciences</source><volume>4</volume><fpage>170</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01476-5</pub-id><pub-id pub-id-type="pmid">10782102</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cleary</surname> <given-names>J</given-names></name><name><surname>Witten</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Data compression using adaptive coding and partial string matching</article-title><source>IEEE Transactions on Communications</source><volume>32</volume><fpage>396</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1109/TCOM.1984.1096090</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conklin</surname> <given-names>D</given-names></name><name><surname>Witten</surname> <given-names>IH</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Multiple viewpoint systems for music prediction</article-title><source>Journal of New Music Research</source><volume>24</volume><fpage>51</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1080/09298219508570672</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conway</surname> <given-names>CM</given-names></name><name><surname>Christiansen</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Modality-Constrained statistical learning of tactile, visual, and auditory sequences</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>31</volume><fpage>24</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.31.1.24</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowan</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Visual and auditory working memory capacity</article-title><source>Trends in Cognitive Sciences</source><volume>2</volume><fpage>77</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(98)01144-9</pub-id><pub-id pub-id-type="pmid">21227076</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Meyniel</surname> <given-names>F</given-names></name><name><surname>Wacongne</surname> <given-names>C</given-names></name><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Pallier</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The neural representation of sequences: from transition probabilities to algebraic patterns and linguistic trees</article-title><source>Neuron</source><volume>88</volume><fpage>2</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.019</pub-id><pub-id pub-id-type="pmid">26447569</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>Pelofi</surname> <given-names>C</given-names></name><name><surname>Bianco</surname> <given-names>R</given-names></name><name><surname>Patel</surname> <given-names>P</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Herrero</surname> <given-names>JL</given-names></name><name><surname>de Cheveigné</surname> <given-names>A</given-names></name><name><surname>Shamma</surname> <given-names>S</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title><source>eLife</source><volume>9</volume><elocation-id>e51784</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.51784</pub-id><pub-id pub-id-type="pmid">32122465</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Melloni</surname> <given-names>L</given-names></name><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Rule-based and Word-level Statistics-based processing of language: insights from neuroscience</article-title><source>Language, Cognition and Neuroscience</source><volume>32</volume><fpage>570</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1080/23273798.2016.1215477</pub-id><pub-id pub-id-type="pmid">29399592</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname> <given-names>KB</given-names></name><name><surname>Arnal</surname> <given-names>LH</given-names></name><name><surname>Ghitza</surname> <given-names>O</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing</article-title><source>NeuroImage</source><volume>85</volume><fpage>761</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.06.035</pub-id><pub-id pub-id-type="pmid">23791839</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egermann</surname> <given-names>H</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Wiggins</surname> <given-names>GA</given-names></name><name><surname>McAdams</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Probabilistic models of expectation violation predict psychophysiological emotional responses to live concert music</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>13</volume><fpage>533</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.3758/s13415-013-0161-y</pub-id><pub-id pub-id-type="pmid">23605956</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frankland</surname> <given-names>PW</given-names></name><name><surname>Josselyn</surname> <given-names>SA</given-names></name><name><surname>Köhler</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The neurobiological foundation of memory retrieval</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1576</fpage><lpage>1585</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0493-1</pub-id><pub-id pub-id-type="pmid">31551594</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frost</surname> <given-names>R</given-names></name><name><surname>Armstrong</surname> <given-names>BC</given-names></name><name><surname>Christiansen</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Statistical learning research: a critical review and possible new directions</article-title><source>Psychological Bulletin</source><volume>145</volume><fpage>1128</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1037/bul0000210</pub-id><pub-id pub-id-type="pmid">31580089</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname> <given-names>JM</given-names></name><name><surname>Aizenman</surname> <given-names>A</given-names></name><name><surname>Bond</surname> <given-names>SM</given-names></name><name><surname>Sekuler</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Memory and incidental learning for visual frozen noise sequences</article-title><source>Vision Research</source><volume>99</volume><fpage>19</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2013.09.005</pub-id><pub-id pub-id-type="pmid">24075900</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname> <given-names>BP</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Mas-Herrero</surname> <given-names>E</given-names></name><name><surname>Dagher</surname> <given-names>A</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predictability and uncertainty in the pleasure of music: a reward for learning?</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>9397</fpage><lpage>9409</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0428-19.2019</pub-id><pub-id pub-id-type="pmid">31636112</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname> <given-names>LM</given-names></name><name><surname>Bestmann</surname> <given-names>S</given-names></name><name><surname>Rosa</surname> <given-names>MJ</given-names></name><name><surname>Penny</surname> <given-names>W</given-names></name><name><surname>Green</surname> <given-names>GG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Time scales of representation in the human brain: weighing past information to predict future events</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.3389/fnhum.2011.00037</pub-id><pub-id pub-id-type="pmid">21629858</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Harrison</surname> <given-names>PMC</given-names></name><name><surname>Bianco</surname> <given-names>R</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>PPM-Decay: a computational model of auditory prediction with memory decay</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.09.900266</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Harrison</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>ppm (R package) - Implements variants of the PPM statistical learning algorithm</data-title><source>GitHub</source><version designator="0.2.0">v0.2.0</version><ext-link ext-link-type="uri" xlink:href="https://github.com/pmcharrison/ppm">https://github.com/pmcharrison/ppm</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Harrison</surname> <given-names>PMC</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dissociating sensory and cognitive theories of harmony perception through computational modeling</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/wgjyv</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The cortical organization of speech processing</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1038/nrn2113</pub-id><pub-id pub-id-type="pmid">17431404</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honda</surname> <given-names>E</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Acoustical and syntactical comparisons between songs of the White-backed munia (Lonchura striata) and its domesticated strain, the bengalese finch (Lonchura striata var Domestica)</article-title><source>Zoological Science</source><volume>16</volume><fpage>319</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.2108/zsj.16.319</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>Y</given-names></name><name><surname>Song</surname> <given-names>JH</given-names></name><name><surname>Rigas</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>High-capacity spatial contextual memory</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>12</volume><fpage>524</fpage><lpage>529</lpage><pub-id pub-id-type="doi">10.3758/BF03193799</pub-id><pub-id pub-id-type="pmid">16235640</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kang</surname> <given-names>H</given-names></name><name><surname>Agus</surname> <given-names>TR</given-names></name><name><surname>Pressnitzer</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Auditory memory for random time patterns</article-title><source>The Journal of the Acoustical Society of America</source><volume>142</volume><fpage>2219</fpage><lpage>2232</lpage><pub-id pub-id-type="doi">10.1121/1.5007730</pub-id><pub-id pub-id-type="pmid">29092589</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karmarkar</surname> <given-names>UR</given-names></name><name><surname>Buonomano</surname> <given-names>DV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Timing in the absence of clocks: encoding time in neural network states</article-title><source>Neuron</source><volume>53</volume><fpage>427</fpage><lpage>438</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.01.006</pub-id><pub-id pub-id-type="pmid">17270738</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname> <given-names>AS</given-names></name><name><surname>Sekuler</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Memory and learning with rapid audiovisual sequences</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.1167/15.15.7</pub-id><pub-id pub-id-type="pmid">26575193</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>R</given-names></name><name><surname>Seitz</surname> <given-names>A</given-names></name><name><surname>Feenstra</surname> <given-names>H</given-names></name><name><surname>Shams</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Testing assumptions of statistical learning: is it long-term and implicit?</article-title><source>Neuroscience Letters</source><volume>461</volume><fpage>145</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2009.06.030</pub-id><pub-id pub-id-type="pmid">19539701</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Vuust</surname> <given-names>P</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predictive processes and the peculiar case of music</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>63</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.10.006</pub-id><pub-id pub-id-type="pmid">30471869</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname> <given-names>S</given-names></name><name><surname>Bonnici</surname> <given-names>HM</given-names></name><name><surname>Teki</surname> <given-names>S</given-names></name><name><surname>Agus</surname> <given-names>TR</given-names></name><name><surname>Pressnitzer</surname> <given-names>D</given-names></name><name><surname>Maguire</surname> <given-names>EA</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Representations of specific acoustic patterns in the auditory cortex and Hippocampus</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>281</volume><elocation-id>20141000</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2014.1000</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Leimer</surname> <given-names>P</given-names></name><name><surname>Herzog</surname> <given-names>M</given-names></name><name><surname>Senn</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Synaptic weight decay with selective consolidation enables fast learning without catastrophic forgetting results reward maximizing learning rule with selective consolidation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/613265</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>HL</given-names></name><name><surname>van Rossum</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Energy efficient synaptic plasticity</article-title><source>eLife</source><volume>9</volume><elocation-id>e50804</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.50804</pub-id><pub-id pub-id-type="pmid">32053106</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname> <given-names>Y</given-names></name><name><surname>Lagoy</surname> <given-names>R</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name><name><surname>Gardner</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Transformation of temporal sequences in the zebra finch auditory system</article-title><source>eLife</source><volume>5</volume><elocation-id>e18205</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.18205</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname> <given-names>K</given-names></name><name><surname>Liu</surname> <given-names>W</given-names></name><name><surname>Zan</surname> <given-names>P</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Fritz</surname> <given-names>JB</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Implicit memory for complex sounds in higher auditory cortex of the ferret</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>9955</fpage><lpage>9966</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2118-18.2018</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname> <given-names>K</given-names></name><name><surname>Vicario</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Statistical learning of recurring sound patterns encodes auditory objects in songbird forebrain</article-title><source>PNAS</source><volume>111</volume><fpage>14553</fpage><lpage>14558</lpage><pub-id pub-id-type="doi">10.1073/pnas.1412109111</pub-id><pub-id pub-id-type="pmid">25246563</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>H</given-names></name><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Song</surname> <given-names>K</given-names></name><name><surname>Zhou</surname> <given-names>K</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural response phase tracks how listeners learn new acoustic representations</article-title><source>Current Biology</source><volume>23</volume><fpage>968</fpage><lpage>974</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.04.031</pub-id><pub-id pub-id-type="pmid">23664974</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname> <given-names>H</given-names></name><name><surname>Lübke</surname> <given-names>J</given-names></name><name><surname>Frotscher</surname> <given-names>M</given-names></name><name><surname>Sakmann</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</article-title><source>Science</source><volume>275</volume><fpage>213</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1126/science.275.5297.213</pub-id><pub-id pub-id-type="pmid">8985014</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masquelier</surname> <given-names>T</given-names></name><name><surname>Guyonneau</surname> <given-names>R</given-names></name><name><surname>Thorpe</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spike timing dependent plasticity finds the start of repeating patterns in continuous spike trains</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e1377</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0001377</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masquelier</surname> <given-names>T</given-names></name><name><surname>Hugues</surname> <given-names>E</given-names></name><name><surname>Deco</surname> <given-names>G</given-names></name><name><surname>Thorpe</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Oscillations, phase-of-firing coding, and spike timing-dependent plasticity: an efficient learning scheme</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>13484</fpage><lpage>13493</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2207-09.2009</pub-id><pub-id pub-id-type="pmid">19864561</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname> <given-names>JH</given-names></name><name><surname>Wrobleski</surname> <given-names>D</given-names></name><name><surname>Oxenham</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Recovering sound sources from embedded repetition</article-title><source>PNAS</source><volume>108</volume><fpage>1188</fpage><lpage>1193</lpage><pub-id pub-id-type="doi">10.1073/pnas.1004765108</pub-id><pub-id pub-id-type="pmid">21199948</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname> <given-names>JH</given-names></name><name><surname>Schemitsch</surname> <given-names>M</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Summary statistics in auditory perception</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>493</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1038/nn.3347</pub-id><pub-id pub-id-type="pmid">23434915</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menyhart</surname> <given-names>O</given-names></name><name><surname>Kolodny</surname> <given-names>O</given-names></name><name><surname>Goldstein</surname> <given-names>MH</given-names></name><name><surname>DeVoogd</surname> <given-names>TJ</given-names></name><name><surname>Edelman</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Juvenile zebra finches learn the underlying structural regularities of their fathers' song</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>571</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00571</pub-id><pub-id pub-id-type="pmid">26005428</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyniel</surname> <given-names>F</given-names></name><name><surname>Maheu</surname> <given-names>M</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Human inferences about sequences: a minimal transition probability model</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005260</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005260</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Michael Lawrence</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>ez: Easy Analysis and Visualization of Factorial Experiments</source></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moldwin</surname> <given-names>T</given-names></name><name><surname>Schwartz</surname> <given-names>O</given-names></name><name><surname>Sussman</surname> <given-names>ES</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Statistical learning of melodic patterns influences the brain's Response to Wrong Notes</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>2114</fpage><lpage>2122</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01181</pub-id><pub-id pub-id-type="pmid">28850296</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muckli</surname> <given-names>L</given-names></name><name><surname>Petro</surname> <given-names>LS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The significance of memory in sensory cortex</article-title><source>Trends in Neurosciences</source><volume>40</volume><fpage>255</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2017.03.004</pub-id><pub-id pub-id-type="pmid">28363477</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Näätänen</surname> <given-names>R</given-names></name><name><surname>Schröger</surname> <given-names>E</given-names></name><name><surname>Karakas</surname> <given-names>S</given-names></name><name><surname>Tervaniemi</surname> <given-names>M</given-names></name><name><surname>Paavilainen</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Development of a memory trace for a complex sound in the human brain</article-title><source>NeuroReport</source><volume>4</volume><fpage>503</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1097/00001756-199305000-00010</pub-id><pub-id pub-id-type="pmid">8513127</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>MT</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition</article-title><publisher-name>Dissertation</publisher-name><ext-link ext-link-type="uri" xlink:href="http://webprojects.eecs.qmul.ac.uk/marcusp/papers/Pearce2005.pdf">http://webprojects.eecs.qmul.ac.uk/marcusp/papers/Pearce2005.pdf</ext-link></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Ruiz</surname> <given-names>MH</given-names></name><name><surname>Kapasi</surname> <given-names>S</given-names></name><name><surname>Wiggins</surname> <given-names>GA</given-names></name><name><surname>Bhattacharya</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Unsupervised statistical learning underpins computational, behavioural, and neural manifestations of musical expectation</article-title><source>NeuroImage</source><volume>50</volume><fpage>302</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.019</pub-id><pub-id pub-id-type="pmid">20005297</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>MT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Statistical learning and probabilistic prediction in music cognition: mechanisms of stylistic enculturation</article-title><source>Annals of the New York Academy of Sciences</source><volume>1423</volume><fpage>378</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1111/nyas.13654</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>M</given-names></name><name><surname>Wiggins</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Improved methods for statistical modelling of monophonic music</article-title><source>Journal of New Music Research</source><volume>33</volume><fpage>367</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1080/0929821052000343840</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Wiggins</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Expectation in melody: the influence of context and learning</article-title><source>Music Perception</source><volume>23</volume><fpage>377</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1525/mp.2006.23.5.377</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perruchet</surname> <given-names>P</given-names></name><name><surname>Pacton</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Implicit learning and statistical learning: one phenomenon, two approaches</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>233</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.03.006</pub-id><pub-id pub-id-type="pmid">16616590</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phan</surname> <given-names>ML</given-names></name><name><surname>Gergues</surname> <given-names>MM</given-names></name><name><surname>Mahidadia</surname> <given-names>S</given-names></name><name><surname>Jimenez-Castillo</surname> <given-names>J</given-names></name><name><surname>Vicario</surname> <given-names>DS</given-names></name><name><surname>Bieszczad</surname> <given-names>KM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>HDAC3 inhibitor RGFP966 modulates neuronal memory for vocal communication signals in a songbird model</article-title><source>Frontiers in Systems Neuroscience</source><volume>11</volume><elocation-id>65</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2017.00065</pub-id><pub-id pub-id-type="pmid">28928640</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The analysis of speech in different temporal integration windows: cerebral lateralization as ‘asymmetric sampling in time’</article-title><source>Speech Communication</source><volume>41</volume><fpage>245</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/S0167-6393(02)00107-3</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polley</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Perceptual learning directs auditory cortical map reorganization through Top-Down influences</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>4970</fpage><lpage>4982</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3771-05.2006</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Powers</surname> <given-names>DMW</given-names></name></person-group><year iso-8601-date="2007">2007</year><data-title>Evaluation: from precision, recall and F-Factor to ROC, informedness, markedness &amp; correlation</data-title><source>Technical Report SIE-07-001</source><ext-link ext-link-type="uri" xlink:href="http://david.wardpowers.info/BM/index.htm">http://david.wardpowers.info/BM/index.htm</ext-link></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redondo</surname> <given-names>RL</given-names></name><name><surname>Morris</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Making memories last: the synaptic tagging and capture hypothesis</article-title><source>Nature Reviews Neuroscience</source><volume>12</volume><fpage>17</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1038/nrn2963</pub-id><pub-id pub-id-type="pmid">21170072</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname> <given-names>GJ</given-names></name><name><surname>Tasoulis</surname> <given-names>DK</given-names></name><name><surname>Adams</surname> <given-names>NM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nonparametric monitoring of data streams for changes in location and scale</article-title><source>Technometrics</source><volume>53</volume><fpage>379</fpage><lpage>389</lpage><pub-id pub-id-type="doi">10.1198/TECH.2011.10069</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname> <given-names>JR</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name><name><surname>Newport</surname> <given-names>EL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Statistical learning by 8-month-old infants</article-title><source>Science</source><volume>274</volume><fpage>1926</fpage><lpage>1928</lpage><pub-id pub-id-type="doi">10.1126/science.274.5294.1926</pub-id><pub-id pub-id-type="pmid">8943209</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname> <given-names>JR</given-names></name><name><surname>Johnson</surname> <given-names>EK</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name><name><surname>Newport</surname> <given-names>EL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Statistical learning of tone sequences by human infants and adults</article-title><source>Cognition</source><volume>70</volume><fpage>27</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(98)00075-4</pub-id><pub-id pub-id-type="pmid">10193055</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname> <given-names>JR</given-names></name><name><surname>Kirkham</surname> <given-names>NZ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Infant statistical learning</article-title><source>Annual Review of Psychology</source><volume>69</volume><fpage>181</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122216-011805</pub-id><pub-id pub-id-type="pmid">28793812</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santolin</surname> <given-names>C</given-names></name><name><surname>Saffran</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Constraints on statistical learning across species</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>52</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.10.003</pub-id><pub-id pub-id-type="pmid">29150414</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schröger</surname> <given-names>E</given-names></name><name><surname>Näätänen</surname> <given-names>R</given-names></name><name><surname>Paavilainen</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Event-related potentials reveal how non-attended complex sound patterns are represented by the human brain</article-title><source>Neuroscience Letters</source><volume>146</volume><fpage>183</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/0304-3940(92)90073-G</pub-id><pub-id pub-id-type="pmid">1491786</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Southwell</surname> <given-names>R</given-names></name><name><surname>Baumann</surname> <given-names>A</given-names></name><name><surname>Gal</surname> <given-names>C</given-names></name><name><surname>Barascud</surname> <given-names>N</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Is predictability salient? A study of attentional capture by auditory patterns</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>20160105</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0105</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Southwell</surname> <given-names>R</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Enhanced deviant responses in patterned relative to random sound sequences</article-title><source>Cortex</source><volume>109</volume><fpage>92</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2018.08.032</pub-id><pub-id pub-id-type="pmid">30312781</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soyman</surname> <given-names>E</given-names></name><name><surname>Vicario</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Rapid and long-lasting improvements in neural discrimination of acoustic signals with passive familiarization</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0221819</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0221819</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname> <given-names>KL</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Hippocampus as a predictive map</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1038/nn.4650</pub-id><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahasi</surname> <given-names>M</given-names></name><name><surname>Yamada</surname> <given-names>H</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Statistical and prosodic cues for song segmentation learning by bengalese finches (Lonchura striata var Domestica)</article-title><source>Ethology</source><volume>116</volume><fpage>481</fpage><lpage>489</lpage><pub-id pub-id-type="doi">10.1111/j.1439-0310.2010.01772.x</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tervaniemi</surname> <given-names>M</given-names></name><name><surname>Rytkönen</surname> <given-names>M</given-names></name><name><surname>Schröger</surname> <given-names>E</given-names></name><name><surname>Ilmoniemi</surname> <given-names>RJ</given-names></name><name><surname>Näätänen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Superior formation of cortical memory traces for melodic patterns in musicians</article-title><source>Learning &amp; Memory</source><volume>8</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1101/lm.39501</pub-id><pub-id pub-id-type="pmid">11584077</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tervaniemi</surname> <given-names>M</given-names></name><name><surname>Just</surname> <given-names>V</given-names></name><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Widmann</surname> <given-names>A</given-names></name><name><surname>Schröger</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Pitch discrimination accuracy in musicians vs nonmusicians: an event-related potential and behavioral study</article-title><source>Experimental Brain Research</source><volume>161</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1007/s00221-004-2044-5</pub-id><pub-id pub-id-type="pmid">15551089</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thiessen</surname> <given-names>ED</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>What's statistical about learning? insights from modelling statistical learning as a set of memory processes</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>20160056</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0056</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viswanathan</surname> <given-names>J</given-names></name><name><surname>Rémy</surname> <given-names>F</given-names></name><name><surname>Bacon-Macé</surname> <given-names>N</given-names></name><name><surname>Thorpe</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Long term memory for noise: evidence of robust encoding of very short temporal acoustic patterns</article-title><source>Frontiers in Neuroscience</source><volume>10</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2016.00490</pub-id><pub-id pub-id-type="pmid">27932941</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogt</surname> <given-names>S</given-names></name><name><surname>Magnussen</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Long-term memory for 400 pictures on a common theme</article-title><source>Experimental Psychology</source><volume>54</volume><fpage>298</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1027/1618-3169.54.4.298</pub-id><pub-id pub-id-type="pmid">17953150</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname> <given-names>CS</given-names></name><name><surname>Wroton</surname> <given-names>HW</given-names></name><name><surname>Kelly</surname> <given-names>WJ</given-names></name><name><surname>Benbassat</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Factors in the discrimination of tonal patterns. I. component frequency, temporal position, and silent intervals</article-title><source>The Journal of the Acoustical Society of America</source><volume>57</volume><fpage>1175</fpage><lpage>1185</lpage><pub-id pub-id-type="doi">10.1121/1.380576</pub-id><pub-id pub-id-type="pmid">1127172</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname> <given-names>I</given-names></name><name><surname>Denham</surname> <given-names>SL</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Modeling the auditory scene: predictive regularity representations and perceptual objects</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>532</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.09.003</pub-id><pub-id pub-id-type="pmid">19828357</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname> <given-names>I</given-names></name><name><surname>Cowan</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>From sensory to long-term memory: evidence from auditory memory reactivation studies</article-title><source>Experimental Psychology</source><volume>52</volume><fpage>3</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1027/1618-3169.52.1.3</pub-id><pub-id pub-id-type="pmid">15779526</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname> <given-names>KJP</given-names></name><name><surname>McDermott</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Schema learning for the cocktail party problem</article-title><source>PNAS</source><volume>115</volume><fpage>E3313</fpage><lpage>E3322</lpage><pub-id pub-id-type="doi">10.1073/pnas.1801614115</pub-id><pub-id pub-id-type="pmid">29563229</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yebra</surname> <given-names>M</given-names></name><name><surname>Galarza-Vallejo</surname> <given-names>A</given-names></name><name><surname>Soto-Leon</surname> <given-names>V</given-names></name><name><surname>Gonzalez-Rosa</surname> <given-names>JJ</given-names></name><name><surname>de Berker</surname> <given-names>AO</given-names></name><name><surname>Bestmann</surname> <given-names>S</given-names></name><name><surname>Oliviero</surname> <given-names>A</given-names></name><name><surname>Kroes</surname> <given-names>MCW</given-names></name><name><surname>Strange</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Action boosts episodic memory encoding in humans via engagement of a noradrenergic system</article-title><source>Nature Communications</source><volume>10</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-11358-8</pub-id><pub-id pub-id-type="pmid">31388000</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yonelinas</surname> <given-names>AP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The Hippocampus supports high-resolution binding in the service of perception, working memory and long-term memory</article-title><source>Behavioural Brain Research</source><volume>254</volume><fpage>34</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2013.05.030</pub-id><pub-id pub-id-type="pmid">23721964</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname> <given-names>S</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name><name><surname>Dick</surname> <given-names>F</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Furukawa</surname> <given-names>S</given-names></name><name><surname>Liao</surname> <given-names>HI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pupil-linked phasic arousal evoked by violation but not emergence of regularity within rapid sound sequences</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>4030</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-12048-1</pub-id><pub-id pub-id-type="pmid">31492881</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s8" sec-type="appendix"><title>Supplementary Experiments</title><boxed-text><sec id="s9"><title>Experiment S1: Implicit memory for six concurrent patterns</title><p>In this experiment (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>) we probed implicit memory capacity by doubling the number of regularities to be memorised (six different REGr per participant).</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Experiment S1 (N = 19): implicit memory for six concurrent patterns.</title><p>(<bold>A</bold>) Sensitivity to emergence of regularity (d') across blocks. (<bold>B</bold>) RT to the RAN to REG transition in RANREG and RANREGr conditions across blocks. ﻿(<bold>C</bold>) The relationship between RTs to the RANREG and RANREGr conditions in block 5. Each dot represents an individual participant. All participants exhibited implicit memory of reoccurring patterns by the end of the 5th block. (<bold>D</bold>) RT advantage for each intra-block presentation. A progressive RT advantage emergeed even when six different REGr were presented. Plotted values correspond to the RT advantage of REGr for each intra-block presentation. RTs of 1st, 2nd or 3rd intra-block presentations were averaged across the different REGr, and RTs to novel REG were averaged across trials which occurred at the beginning (first third), middle or end of each block. Note that this analysis is based on a small number of trials per ‘intra-block’ presentation condition, and effects are therefore somewhat noisy. Error bars indicate 1 s.e.m.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-app1-fig1-v2.tif"/></fig><p>Methods: The transition detection task was identical to Exp. 1A, but six different REGr were presented per participant. Similar to Exp. 1A, participants performed the familiarity task after the transition detection task, in which the 6 REGr trials were randomly intermixed with 36 novel REG sequences.</p><p>Participants. Twenty paid individuals (seventeen females; average age, 24.5 ± 3.8 years) took part in the study. No participant reported hearing difficulties. ﻿Because of poor accuracy (d’ &lt; 2 after the first block), one participant was excluded from the analysis.</p><p>Results: Overall, the same pattern of performance as in Exp. 1A was demonstrated.</p><p><xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B and D</xref> reveals a progressively larger RT advantage for RANREGr [main effect of condition: F(1, 18) = 71.76, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .80; main effect of block: F(4, 72) = 4.19, p = 0.045, η<sub>p</sub><sup>2</sup> = .22; interaction condition by block: F(4, 72) = 7.26, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .29]. A significantly faster response (80 ms; 1.6 tones) for RANREGr relative to RANREG was observed already by the end of the first block [t(18) = 3.512, p = 0.012]. It grew across the following blocks (all ps &lt; 0.001), and reached 244 ms (4.9 tones) in the fifth block, consistent with Exp. 1A [RT advantage in Exp 1A vs. Exp.2: independent sample t(36) = .515, p = 0.609].</p><p><xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C</xref> shows the mean RT advantage for RANREGr for each individual in block 5. Implicit memory was exhibited by all participants by the end of the session.</p><p>Explicit memory (probed in the same way as described for Exp. 1A) was poor (mean MCC = 0.178) and did not correlate with the RT advantage in block 5 (spearman’s Rho = 0.091; p = 0.710).</p></sec><sec id="s10"><title>Experiment S2A, B: The memory trace is weakened, but not abolished by interrupting blocks</title><p>Although reoccurrence of regularities was quite sparse in Exp. 1A (every ~2.7 min), they were presented regularly over five blocks. Here, we asked whether memory formation can be interrupted by introducing a delay of 10 min (‘interrupting blocks’ in which REGr were not presented) between ‘standard blocks’.</p><p>Methods: These experiments involved the same transition detection task as in Exp. 1A, but ‘interrupting blocks’, in which RANREGr condition was not presented, were introduced between ‘standard blocks’. The ‘interrupting blocks’ were block 2 and 4 in experiment S2A, block 3 and 5 in experiment S2B. Across five blocks, in experiment S2A participants were presented with 27 RANREGr, 108 RANREG, 135 RAN, 15 STEP, and 15 CONT. Across six blocks, in experiment S2B participants were presented with 36 RANREGr, 126 RANREG, 162 RAN, 18 STEP, and 18 CONT.</p><p>Participants of experiment S2A. Nineteen paid individuals (13 females; average age, 23.8 ± 4.7 years) took part in the study. No participant reported hearing difficulties. Because of poor accuracy (d’ &lt; 2 after the first block), one participant was excluded from the analysis.</p><p>Participants of experiment S2B. Twenty paid individuals (10 females; average age, 23.8 ± 4.00 years) took part in the study. No participant reported hearing difficulties. Because of poor accuracy (d’ &lt; 2 after the first block), one participant was excluded from the analysis.</p><p>Results: In Exp. S2A, an interrupting block was inserted after each standard block (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2B</xref>). The RT data demonstrated an RT advantage to reoccurring vs. novel regularities (~130 ms – 2.6 tones by the end of the third standard block), which did not improve substantially between the second and third standard blocks [main effect of condition: F(1, 17) = 35.03, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .67; main effect of block: F(2, 34) = 10.67, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .39; no interaction: F(2, 34) = 3.03, p = 0.061, η<sub>p</sub><sup>2</sup> = .15]. The RT advantage here was smaller than that typically observed after three consecutive blocks (~180 ms – 3.7 tones in <italic>Pooled data-block<sub>3</sub></italic>; <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2C</xref>; difference significant at p = 0.027 based on bootstrap resampling; see Materials and methods in the main document).</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Experiment S2A (N = 18) and S2B (N = 19): the memory trace is weakened, but not abolished, by interrupting blocks.</title><p>(<bold>A–D</bold>) Exp. S2A: (<bold>A</bold>) Sensitivity to emergence of regularity (d') across blocks in experiment S2A. Error bars indicate 1 s.e.m. (<bold>B</bold>) RTs to transition in RANREG and RANREGr across blocks. Error bars indicate 1 s.e.m. Yellow shading indicates blocks where REGr were present. (<bold>C</bold>) Bootstrap resampling-based distributions of RT advantage after three uninterrupted blocks (<italic>Pooled data-block<sub>3</sub></italic>; see Materials and methods). The red dot indicates the RT advantage measured after block three in the present experiment. (<bold>D</bold>) RT advantage for each intra-block presentation. The RT advantage was preserved over ‘interrupting’ blocks. Plotted values correspond to the RT advantage of REGr for each intra-block presentation. RTs of 1st, 2nd or 3rd intra-block presentations were averaged across the different REGr, and RTs to novel REG were averaged across trials which occurred at the beginning (first third), middle or end of each block. Note that the RT for REGr is computed based on three trials and the effects are therefore rather noisy. Error bars indicate 1 s.e.m. (<bold>E–J</bold>) Exp. S2B: (<bold>F</bold>) Sensitivity to emergence of regularity (d') across blocks for experiment S2B Error bars indicate 1 s.e.m. (<bold>F</bold>) RTs to the transition in RANREG and RANREGr across blocks. Error bars indicate 1 s.e.m. Yellow shading indicates blocks where REGr were present. (<bold>G</bold>) Bootstrap resampling-based distributions of RT advantage after 4th blocks (<italic>Pooled data-block<sub>4</sub></italic>; see Materials and methods). The red dot indicates the RT advantage measured after block four in the present experiment. (<bold>J</bold>) The RT advantage was preserved over ‘interrupting’ blocks. (<bold>K</bold>) Unconstrained vs. Constrained memory model results for Exp. S2A. Error bars indicate 1 s.e.m.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-app1-fig2-v2.tif"/></fig><p>In Experiment S2B, we introduced the first interrupting block after block two in order to allow for the memory trace to emerge (see <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2F</xref>). The RT advantage in the 2nd block was similar to that observed in the control (<italic>Pooled data-block<sub>2</sub></italic>: p=0.48), but no considerable improvement was observed across blocks thereafter [main effect of condition: F(1, 18) = 74.93, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .81; main effect of block: F(3, 54) = 11.19, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .38; no interaction: F(2, 54) = 2.56, p = 0.064 η<sub>p</sub><sup>2</sup> = .12]. The RT advantage in the blocks thereafter was indeed smaller than under ‘uninterrupted’ control conditions (block three vs. <italic>Pooled data-block<sub>3</sub></italic>: p = 0.071; block four vs. <italic>Pooled data-block<sub>4</sub></italic>: p = 0.013, see <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2G</xref>).</p><p>These results suggest that the memory trace for REGr can withstand quite substantial interruptions: suspending the regular reoccurrences of REGr (by introducing ‘interrupting blocks’) resulted in a largely maintained memory, though there was evidence for a somewhat stagnated RT advantage.</p><p>Modelling Exp. S2A. The performance of the unconstrained PPM model (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2K</xref>), was not affected by the interruptions (also compare this figure with <xref ref-type="fig" rid="fig5">Figure 5</xref>-A in the main text). In contrast, in the memory-decay PPM model inserting ‘interrupting’ blocks has the effect of reducing the memory traces of previously heard regularities. The constrained model shows somewhat worse performance relative to the constrained model in Exp. 1A, consistent with human effects.</p></sec><sec id="s11"><title>Experiment S3: Implicit memory is robust to pattern transposition</title><p>We tested whether the implicit memory for reoccurring sequences generalises to versions in which relative relationships within the stimulus (pitch intervals) are preserved, while absolute information (the frequency values themselves) are manipulated.</p><p>Methods: The stimulus set included the same conditions as described for Exp. 1A, but with the following differences: RAN sequences were generated from a pool of twenty-six frequencies (logarithmically-spaced values between 222 and 4004 Hz; 12% steps). REG patterns consisted of 20 frequencies randomly selected from the pool. To allow for the transposition, REGr patterns were drawn from a subset of 24 frequencies (i.e., not including the highest and lowest frequency in the pool). In the 5<sup>th</sup> block, each REGr was randomly transposed up or down by one tone (12%; shifted one place higher or lower in the frequency pool than the original, see 3-A).</p><p>Participants. Twenty paid individuals (twelve females; average age, 24.75 ± 6.8 years) took part in the study. No participant reported hearing difficulties. ﻿</p><p>Results: Overall, the same pattern of performance as in Exp. 1A was seen. <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3C</xref> demonstrates progressively stronger implicit memory for REGr, as revealed by a growing RT advantage over novel REG across blocks [main effect of condition: F(1, 19) = 47.31, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .71; main effect of block: F(4, 76) = 7.95, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = .29; interaction condition per block: F(4, 76) = 5.35, p = 0.003, η<sub>p</sub><sup>2</sup> = .22]. Specifically, whilst in the first block performance did not differ between RANREG and RANREGr conditions [t(19) = 1.635, p = 0.59], a significantly faster response (186 ms; 3.7 tones) for RANREGr was observed in the second block [t(19) = 4.302 p = . 001], and grew across the remaining blocks (all ps &lt; 0.004).</p><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Experiment S3 (N = 20): implicit memory is robust to pattern transposition.</title><p>(<bold>A</bold>) Schematic example of the transposition. Yellow squares indicate tones in a REGr sequence; grey squares indicate the transposed version (in this example, the tones were shifted downwards by one step in the frequency pool; 12%). The red line indicates the transition from RAN (light blue area) to REGr. (<bold>B</bold>) d’ across all blocks. Error bars indicate 1 s.e.m. (<bold>C</bold>) RT to the transition in RANREG and RANREGr across blocks. In block 5 (yellow shading) the originally learned REGr were replaced by transposed versions. Error bars indicate 1 s.e.m. (<bold>D</bold>) RT advantage for each intra-block presentation. ﻿The RT advantage was preserved following frequency transposition of the REGr pattern. Plotted values correspond to the RT advantage of REGr for each intra-block presentation. RTs of 1st, 2nd or 3rd intra-block presentations were averaged across the different REGr, and RTs to novel REG were averaged across trials which occurred at the beginning (first third), middle or end of each block. Note that the RT for REGr is computed based on three trials and the effects are therefore rather noisy. Error bars indicate 1 s.e.m.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56073-app1-fig3-v2.tif"/></fig><p>Importantly, this RT advantage (205 ms – 4.1 tones) in block 5 (transposed REGr) did not differ from the RT advantage on block 4 (272 ms; 5.4 tones) [t(19) = 1.541, p = 0.14]. To confirm the immediacy of the transfer we compared the RT advantage in the first intra-block presentation in block 5, where the transposition was introduced, with the third (last) intra-block presentation in block 4 (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3D</xref>). No difference was observed [t(19) = 1.26, p = 0.223], suggesting that the generalization to the transposed pattern was instantaneous.</p><p>The observation of a transfer of RT advantage to the transposed sequences may suggest that the formed representation is not precisely echoic: instead of the specific frequency pattern, the auditory system might be maintaining a representation of the contour, or inter-tone interval within the REGr pattern. Another possibility is that the tolerance reflects a noisy frequency representation, though we note that the frequency steps here (12%) are large enough to be discriminable by most listeners.</p></sec></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56073.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Obleser</surname><given-names>Jonas</given-names></name><role>Reviewing Editor</role><aff><institution>University of Lübeck</institution><country>Germany</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Schroger</surname><given-names>Erich</given-names> </name><role>Reviewer</role></contrib><contrib contrib-type="reviewer"><name><surname>Bieszczad</surname><given-names>Kasia</given-names> </name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper shows, in our view very convincingly, that human listeners rapidly form robust and surpirisingly long-lasting memories of rarely encountered, featureless sound sequences presented among many similar stimuli. Using a previously established listening paradigm of such sound sequences and employing a compelling set of control conditions and analyses, including an ideal-observer mode, these findings are surprising and exciting. The experiments are well controlled and the analysis considers alternate hypotheses. Subsequent experiment answer additional questions to fully characterize the effect and demonstrate that it is acoustic-frequency invariant, long-lasting, and resistant to interference.</p><p>The paper connects to a long tradition in psychoacoustics using such tone patterns as stimuli. However, the huge majority of those studies focused on sensory memory or short-term learning. We think that the current research opens a new avenue by shifting the focus towards long-term memory for tonal patterns. While the main finding is sufficiently robust and compelling to be published, the results are also likely to provoke rich follow-up research. With its elegantly designed experiments using behavioral measures (accuracy, reaction times) as dependent variables and including computational modeling, the paper establishes a foundation for future neuroscientific studies.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Long-term implicit memory for sequential auditory patterns in humans&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by Barbara Shinn-Cunningham as the Senior Editor, a Reviewing Editor, and three reviewers. The following individuals involved in review of your submission have agreed to reveal their identity: Erich Schroger (Reviewer #2); Kasia Bieszczad (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The reviews yielded a surprisingly large degree of overlap: This manuscript describes an unusually long-lasting implicit memory of sound sequence based on repetitive exposure alone. Within it, the authors describe time constants that constrain the formation of this memory that are modeled to replicate the human data. Overall, these findings are surprising and exciting.</p><p>The usage of such tonal patterns has a long tradition in psychoacoustics (since the 1970s, e.g. Watson.et al., 1975) and cognitive psychophysiology (since the 1990s, e.g. Schröger, Näätänenand Paavilainen, 1992) to study auditory processing in humans (and animals) still is a very timely topic resulting in hundreds of papers. However, the huge majority of those studies focused on sensory memory or short-term learning (with some exceptions also studying long-term learning of tonal patterns such as Atienza and Cantero, 2001; Näätänen et al., 1993). The current research by Biancoand et al., opens a new avenue by shifting the focus towards long-term memory for tonal patterns. Bianco and colleagues present a series of elegantly designed experiments using behavioral measures (accuracy, reaction times) as dependent variables.</p><p>The experiments are well-controlled and with excellent consideration of alternate hypotheses. We found answers to arising questions in subsequent experiments throughout the manuscript that fully characterized the effect to be acoustic-frequency invariant, long-lasting and resistant to interference.</p><p>The work is well done, well written, and interesting. As a grain of salt, some of us were missing the bigger conclusion that can be drawn from the work beyond the individual experiments. We identified three revision-worthy issues: conceptual (all three reviewers); modeling (reviewers 1 and 2); and methods in general (see below for details).</p><p>Essential revisions:</p><p>1) All reviewers and editors agreed that the findings must be couched better in the existing learning/memory literature w.r.t. MMN and exposure-only learning models in adult brains (e.g., songbirds). As for the modeling, we agreed agree that the modeling could be better integrated with the behavioral findings in a way that is additive rather than somewhat circular. In other words, what does the modeling experiment actually contribute to the discovery? Does it suggest mechanisms, constraints?</p><p>2) Abstract and conclusions: The abstract could tell the reader more clearly what we learn from this paper. The abstract describes what was done and what the results are, but why this question is important and what to conclude from the work remains not that clear (the last sentence feels opaque and not fully related to the data; or at least this could be clarified more). A conclusion section is missing entirely. We urge the authors to add such conclusion, in order to wrap up with clear statements (but clearly related to the data) what the reader may take from this study. This will help increase the lasting impact of this work.</p><p>3) A relatively big concern relates to the modeling. The authors may want to emphasize more strongly what new the reader learns from the modeling. We do know that memory decays (so the unconstrained model is not very interesting). The exact time course of decay is not very interesting as this will dependent on the stimulus and paradigm, and will likely be different for different stimuli. The model does not explain the longer-term effects well (e.g., 7w). The modeling section in the results reflects largely a re-description of the experimental data. The model was not fit to the individual participant data, nor was it compared to an alternative model. It is not necessarily a problem that a descriptive model is presented to explain some hypothetical mechanism, but at the same time, the model (n-grams) seems to work particularly with discrete countable events (such as the tones used here). We wondered how much it generalizes. The impression occured that the model was &quot;made to look like&quot; the data, which ends up being circular. One might even consider removing it, since the experimental data speak for themselves, but we assume the authors are more married to the model. We thus recommend making clear what we learn anew from the model that is not known from the literature and that is generalizable beyond the experimental design utilized here.</p><p>4) Analysis and plotting related to Figure 8C/D: We recommend the authors include the data from the first block (pooled data) in Figure 8C (i.e., RANREG and RANREGr for each intra-block). This would provide an additional useful visualization of the data (this could also be accomplished using their bootstrapping approach: for each iteration, the mean and variance could be stored; mean and variance are then averaged across iterations and the SEM is calculated based on the variance [with N=36]). Moreover, the authors may also want to provide results from more a traditional analysis: an ANOVA with RT advantage for 1-3 intra-blocks (within-subjects) and group (between-subjects). It is not a problem that there are differences in the number of the available participants per group.</p><p>5) We strongly recommend removing the separation into 25% (low performers) and 75% (high performers). This seems unmotivated conceptually, the data distribution is normal (or at least uni-modal) and thus does not justify dividing the distribution arbitrarily, leads to circular wording (e.g., that low performers do worse [put simply]), and may lead to speculations (e.g., &quot;what cognitive abilities distinguish the low- from the high-performers&quot;, subsection “Across-experiment analysis reveals that most patterns are remembered and most participants exhibit implicit memory”; it could also be differences in motivation, sensory abilities, or any number of variables; moreover, statistically speaking, there will always be a 25% bottom group for a uni-modal distribution).</p><p>6) The authors may want to acknowledge explicitly that their passive condition is actually active with respect to the auditory stimulation. The data are not fully conclusive, in my view, whether passive (as in not actively listening to the sounds) leads to similar behavioral patterns. I think this distinction needs to be addressed more clearly in the results and discussion.</p><p>7) Authors should consider contrasting effects to the relevant birdsong literature that also describes statistical learning and its underlying neurobiological mechanisms</p><p>8) Have the authors considered memory consolidation effects to explain time-dependent processes that oppose memory decay? These are known to improve performance after an incubation period (time passing) without continued exposure or training. This may be relevant to explain the maintenance of memory after &gt;7 weeks.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56073.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) All reviewers and editors agreed that the findings must be couched better in the existing learning/memory literature w.r.t. MMN and exposure-only learning models in adult brains (e.g., songbirds). As for the modeling, we agreed agree that the modeling could be better integrated with the behavioral findings in a way that is additive rather than somewhat circular. In other words, what does the modeling experiment actually contribute to the discovery? Does it suggest mechanisms, constraints?</p></disp-quote><p>Thank you for these suggestions. We now expanded the Discussion section to refer to previous MMN work (Subsection “Memory for auditory sequences”) and findings from the song bird literature (subsection “Time scales of memory for sequences”). We also emphasize the usefulness of the model (for details see reply to question 3 below).</p><p>Subsection “Memory for auditory sequences” (MMN and exposure only learning models):</p><p>‘Signals based on tone-pip patterns have long been used to understand how auditory memory affects human listeners’ perception of sound sequences (e.g. Watson, Wroton, Kelly, and Benbassat, 1975; Atienza and Cantero, 2001; Näätänen, Schröger, Karakas, Tervaniemi, and Paavilainen, 1993; Schröger, Näätänen, and Paavilainen, 1992; Tervaniemi, Rytkönen, Schröger, Ilmoniemi, and Näätänen, 2001; Moldwin, Schwartz, and Sussman, 2017). However, these paradigms are predominantly based on extensive exposure (in the order of hundreds of consecutive repetitions) to a single pattern.’</p><p>Subsection “Time scales of memory for sequences” (Songbirds literature):</p><p>‘In animal models, repetitive exposure to sound tokens (though, notably at a much higher rate than that used here) has been shown to evoke a process of long-lasting adaptation manifested as sparser firing and increased response specificity. These effects, persisting for hours to days after the initial stimulation, have been observed in primary and secondary auditory areas in song birds (Caudal Medial Nidopallium; Cazala, Giret, Edeline, and Del Negro, 2019; Honda and Okanoya, 1999; Lu and Vicario, 2014; Menyhart, Kolodny, Goldstein, DeVoogd, and Edelman, 2015; Takahasi et al., 2010; Chew, Vicario, and Nottebohm, 1996; Soyman and Vicario, 2019) and in secondary auditory cortex in ferrets (Lu et al., 2018). The hypothesis that similar processes might back the behavioural effects we report is appealing.’</p><disp-quote content-type="editor-comment"><p>2) Abstract and conclusions: The abstract could tell the reader more clearly what we learn from this paper. The abstract describes what was done and what the results are, but why this question is important and what to conclude from the work remains not that clear (the last sentence feels opaque and not fully related to the data; or at least this could be clarified more). A conclusion section is missing entirely. We urge the authors to add such conclusion, in order to wrap up with clear statements (but clearly related to the data) what the reader may take from this study. This will help increase the lasting impact of this work.</p></disp-quote><p>We changed the Abstract as follows:“Memory, on multiple timescales, is critical to our ability to discover the structure of our surroundings, and efficiently interact with the environment. We combined behavioural manipulation and modelling to investigate the dynamics of memory formation for rarely reoccurring acoustic patterns. In a series of experiments, participants detected the emergence of regularly repeating patterns within rapid tone-pip sequences. Unbeknownst to them, a few patterns reoccurred every ~3 minutes. All sequences consisted of the same 20 frequencies and were distinguishable only by the order of tone-pips. Despite this, reoccurring patterns were associated with a rapidly growing detection-time advantage over novel patterns. This effect was implicit, robust to interference, and persisted up to 7 weeks. The results implicate an interplay between short (a few seconds) and long-term (over many minutes) integration in memory formation and demonstrate the remarkable sensitivity of the human auditory system to sporadically reoccurring structure within the acoustic environment.”</p><p>We added a conclusion paragraph that reads:</p><p>“‘Uncovering how memory traces are encoded and preserved by the brain is crucial for understanding subsequent learning operations which drive pattern recognition and generalization. […] Important questions for future work include understanding the neurobiological foundations of these behavioural effects, the limits on the capacity of the memory store(s) involved and the factors which might affect subsequent forgetting.”</p><disp-quote content-type="editor-comment"><p>3) A relatively big concern relates to the modeling. The authors may want to emphasize more strongly what new the reader learns from the modeling. We do know that memory decays (so the unconstrained model is not very interesting). The exact time course of decay is not very interesting as this will dependent on the stimulus and paradigm, and will likely be different for different stimuli. The model does not explain the longer-term effects well (e.g., 7w). The modeling section in the results reflects largely a re-description of the experimental data. The model was not fit to the individual participant data, nor was it compared to an alternative model. It is not necessarily a problem that a descriptive model is presented to explain some hypothetical mechanism, but at the same time, the model (n-grams) seems to work particularly with discrete countable events (such as the tones used here). We wondered how much it generalizes. The impression occured that the model was &quot;made to look like&quot; the data, which ends up being circular. One might even consider removing it, since the experimental data speak for themselves, but we assume the authors are more married to the model. We thus recommend making clear what we learn anew from the model that is not known from the literature and that is generalizable beyond the experimental design utilized here.</p></disp-quote><p>We rewrote the relevant sections so as to emphasize the usefulness of the model. In particular, we highlight the following:</p><p>- Although the existence of memory decay in humans is in general well established, ways of incorporating memory decay into probabilistic computational models of sequences processing is very much an active topic of research.</p><p>- Though it has many constraints and simplifications, the model is useful in concretizing the effect of interplay between short- and longer- time scales in the formation of enduring memories for sequences.</p><p>- In particular, the biologically plausible set of parameters fully account for the dynamics of memory formation over reoccurrences.</p><p>- The insight into possible single-trial level dynamics afforded by the modelling can be useful for constraining the search for the neural underpinnings of the observed effects.</p><p>- We also acknowledge (subsection “Modelling”) that although our model does not explain our long-term effects, to our knowledge there is no other statistical learning model that accounts both for learning dynamics and long-term fixed effects.</p><p>- The unconstrained model has been used successfully in previous research (Barascud et al., 2016) to help understand the responses of listeners. In the present paper, the comparison with the non-constrained model is useful for providing a benchmark – i.e. demonstrating the effect the change in parameter has on model output.</p><p>Subsection “Modelling”: ‘We constructed a ‘memory constrained’ computational model, based on ‘prediction by partial matching’ (PPM; see Materials and methods section) to provide a formal simulation of the psychological mechanisms underlying the process of memory trace formation, as observed in Experiments 1A (Figure 2), 2 (Figure 5) and S2A (Appendix1—figure 2.Figure D). These experiments reflect critical manipulations of the effect of long- and short- term memory decay. Although the existence of memory decay in humans is in general well established, ways of incorporating memory decay into probabilistic computational models of sequences processing is very much an active topic of research. Our PPM model implemented a single set of values (Table 1) that fully accounted for the dynamics of memory formation observed across experiments. As a benchmark, we also report the results for an equivalent unconstrained model (i.e., with perfect memory), as employed in previous research using the same paradigm (Barascud et al., 2016). The following cognitive hypotheses were instantiated: (1) Listeners learn sequence transition probabilities throughout the experiment. This approach is similar to other models of statistical learning (Bröker, Bestmann, Dayan, and Marshall, 2018; Harrison, Bestmann, Rosa, Penny, and Green, 2011; Meyniel, Maheu, and Dehaene, 2016a; Takahasi, Yamada, and Okanoya, 2010) except the present model extends beyond first-order transition probabilities. Learning of sequence statistics is accomplished though partitioning the unfolding stimulus into sub-sequences of increasing order (n-grams) that are thereon stored in memory, such that the more a listener is exposed to a given n-gram, the stronger its salience (‘weight’). Here, we allow n to range between 1 and 5, corresponding to Markovian transition probabilities of orders 0 to 4. ….’</p><p>Subsection “Modelling”: “Overall, the modelling successfully replicated the slow dynamics of memory formation exhibited by human listeners demonstrating that a memory constrained transition-probability learning is a plausible computational underpinning of sequential pattern acquisition.”</p><p>We also discuss model limitations and possible generalizability to natural sounds. The subsection “What is being remembered?”Discussion section now reads:</p><p>“Similar to other models of statistical learning (Bröker et al., 2018; Harrison et al., 2011; Meyniel, Maheu, and Dehaene, 2016b), our memory-constrained PPM model explicitly assumes that listeners represent the unfolding sequences in the form of n-gram sub-sequences of variable length, from which transition probabilities are computed. Previous computational, behavioural and neuroimaging studies (Bianco, Ptasczynski, and Omigie, 2020; Conklin and Witten, 1995; Di Liberto et al., 2020; Egermann, Pearce, Wiggins, and McAdams, 2013; Pearce, Ruiz, Kapasi, Wiggins, and Bhattacharya, 2010; Pearce and Wiggins, 2004, 2006) demonstrated that PPM successfully generalizes to prediction of musical sequences and effectively accounts for psychophysiological responses to melodies. In particular, PPM provided a good match to brain response latencies evoked by transitions between RAN and REG patterns (Barascud et al., 2016; Southwell and Chait, 2018), suggesting that listeners may rely on similar memory representations as those proposed by the model. Here, the memory constrained version of PPM was able to successfully simulate human performance – concretizing how the interplay between short- and long- term decay might give rise to the progressive emergence of a memory trace across presentations. Whether listeners do indeed represent auditory patterns in this way is a matter of ongoing debate (e.g. Thiessen, 2017). Additional support for an n-gram-like representation is provided in Exp. 4, which demonstrated that the REGr RT advantage is robust to pattern phase shifts. This finding indicates that REG patterns are not encoded in memory as rigid chunks of sequential items (Perruchet and Pacton, 2006; Thiessen, 2017), but are instead represented as a transition rule which allows for flexible retrieval. Whilst further empirical evidence is essential to determine the nature of the memory representation, the insight into single-trial level dynamics derived from the present modelling (Figure 4) may be useful for constraining the search for the physiological underpinnings of these phenomena. Furthermore, the model can readily be applied to statistical learning in other modalities (reviewed by Frost et al., 2019) and even in other species, including songbirds such as finches, known to be capable of statistical learning (Menyhart et al., 2015; Takahasi et al., 2010).</p><p>A related question pertains to the generalizability of the present model to natural sounds beyond quantized sequences, such as those used here. In order to relate listeners’ performance to a measure of statistical information within unfolding signals, simplifying assumptions are necessary. This includes the presence of a prior stage of category formation which converts a continuous sound into discrete units that form the model’s ‘alphabet’. Accumulating evidence is indeed revealing that unsupervised segmentation of unfolding sounds into basic elements, perhaps using envelope-based cues, may be an inherent feature of listening (Ding, Melloni, Tian, and Poeppel, 2017; Doelling, Arnal, Ghitza, and Poeppel, 2014; Hickok and Poeppel, 2007; Poeppel, 2003).”</p><p>We have also tightened the subsection “Modelling” to avoid redundancy with the behavioural data section. Unfortunately, we cannot model individual participants because the implicit reaction-time paradigm is rather noisy and can only deliver a good signal when we average over participants.</p><disp-quote content-type="editor-comment"><p>4) Analysis and plotting related to Figure 8C/D: We recommend the authors include the data from the first block (pooled data) in Figure 8C (i.e., RANREG and RANREGr for each intra-block). This would provide an additional useful visualization of the data (this could also be accomplished using their bootstrapping approach: for each iteration, the mean and variance could be stored; mean and variance are then averaged across iterations and the SEM is calculated based on the variance [with N=36]). Moreover, the authors may also want to provide results from more a traditional analysis: an ANOVA with RT advantage for 1-3 intra-blocks (within-subjects) and group (between-subjects). It is not a problem that there are differences in the number of the available participants per group.</p></disp-quote><p>Thank you for the suggestions. We have:</p><p>1) Included a direct comparison with the control group (N=147) using traditional analysis (t-test).</p><p>2) Within the pre-exposed group we quantified RT advantage for each intra-block presentation and compared to 0 as a measure of the presence of a memory effect.</p><p>We also ran a between-group ANOVA with intra-block repetition as within factor on the RT advantage, as suggested by the reviewer. We found a main effect of group [F(1,160) = 8.60, p = .004, ges = .021], a main effect of intra-block presentation [F(2,308) = 7.45, p &lt;.001, ges = .027, but not in interaction with group [F(2,308) = 1.11, p = .329, ges = .004] (ges = generalised eta square). This indicates that RT advantage grows across intra-blocks presentation in both groups, but the overall RT advantage achieved by the pre-exposed group is greater than that of the ‘control’ group. These results are entirely consistent with the bootstrap analysis reported originally and are therefore not included in the text to reduce redundancy. We believe the non-parametric bootstrap analysis is the more robust approach for this sort of analysis.</p><p>The relevant part in subsection “Experiment 5: Implicit memory can form when sounds are behaviourally irrelevant, but does not immediately transfer to behaviour” now reads:</p><p>“We analysed the performance in the test block of the pre-exposed group in comparison to the performance of a non pre-exposed ‘control’ group, formed by pooling block 1 data from several other experiments (Pooled data-block1, N = 147, see Materials and methods section). ….</p><p>In the test block (Figure 8B), the mean RT to RANREGr was significantly faster than that to novel RANREG [t(17) = 3.1, p = 0.006], consistent with the presence of an RT advantage. The RT advantage in the pre-exposed group (~157 ms, 3.14 tones) was substantially greater than in the control group (~30 ms, 0.6 tones) [independent sample t(163) = 3.023, p = .003], indicating a beneficial effect of pre-exposure.</p><p>As a critical test for the presence of a memory trace after pre-exposure, we examined RT in each intra-block presentation of REGr. If memories for reoccurring patterns are formed during pre-exposure, an RT advantage should be exhibited immediately – at the first presentation of REGr in the test block. One sample t-tests demonstrated that an RT advantage was absent at the first and second intra-block presentations [t(16) = .377, p = .711; t(17) = 1.691, p = .109], but emerged at third presentation of REGr [t(17) = 3.954, p = .001]. We also compared the RT advantage, across intra-block presentations between the pre-exposed and control groups. A bootstrap approach (see Materials and methods section) was used to generate a distribution of performance over subsets of 20 participants drawn from the control group and to compare with the actually observed performance in the pre-exposed group (Figure 8D).”</p><disp-quote content-type="editor-comment"><p>5) We strongly recommend removing the separation into 25% (low performers) and 75% (high performers). This seems unmotivated conceptually, the data distribution is normal (or at least uni-modal) and thus does not justify dividing the distribution arbitrarily, leads to circular wording (e.g., that low performers do worse [put simply]), and may lead to speculations (e.g., &quot;what cognitive abilities distinguish the low- from the high-performers&quot;, subsection “Across-experiment analysis reveals that most patterns are remembered and most participants exhibit implicit memory”; it could also be differences in motivation, sensory abilities, or any number of variables; moreover, statistically speaking, there will always be a 25% bottom group for a uni-modal distribution).</p></disp-quote><p>Done (Figure 9). In line with the suggestion, we also removed that separation from Figure 7.</p><disp-quote content-type="editor-comment"><p>6) The authors may want to acknowledge explicitly that their passive condition is actually active with respect to the auditory stimulation. The data are not fully conclusive, in my view, whether passive (as in not actively listening to the sounds) leads to similar behavioral patterns. I think this distinction needs to be addressed more clearly in the results and discussion.</p></disp-quote><p>This is a very good point, that we already explain in the text, but the title is misleading. We changed the wording to ‘behaviourally irrelevant’ instead of ‘passive’ (subsection “Experiment 5: Implicit memory can form when sounds are behaviourally irrelevant, but does not immediately transfer to behaviour”Figure 8).</p><disp-quote content-type="editor-comment"><p>7) Authors should consider contrasting effects to the relevant birdsong literature that also describes statistical learning and its underlying neurobiological mechanisms</p></disp-quote><p>Done (Figure 9). In line with the suggestion, we also removed that separation from Figure 7.</p><p>Thank you for this suggestion. We have now added an explicit discussion of relevant findings from the birdsong literature (subsection “Time scales of memory for sequences”). which now reads:</p><p>“In animal models, repetitive exposure to sound tokens (though, notably at a much higher rate than that used here) has been shown to evoke a process of long-lasting adaptation manifested as sparser firing and increased response specificity. These effects, persisting for hours to days after the initial stimulation, have been observed in primary and secondary auditory areas in song birds (Caudal Medial Nidopallium; Cazala, Giret, Edeline, and Del Negro, 2019; Honda and Okanoya, 1999; Lu and Vicario, 2014; Menyhart, Kolodny, Goldstein, DeVoogd, and Edelman, 2015; Takahasi et al., 2010; Chew, Vicario, and Nottebohm, 1996; Soyman and Vicario, 2019) and in secondary auditory cortex in ferrets (Lu et al., 2018). The hypothesis that similar processes might back the behavioural effects we report is appealing.”</p><disp-quote content-type="editor-comment"><p>8) Have the authors considered memory consolidation effects to explain time-dependent processes that oppose memory decay? These are known to improve performance after an incubation period (time passing) without continued exposure or training. This may be relevant to explain the maintenance of memory after &gt;7 weeks.</p></disp-quote><p>Thank you for this interesting suggestion. We agree that the account provided by the reviewers is a possible one, but since we are dealing with ‘coarse’ behavioural data it is difficult to strongly claim memory consolidation processes over very slow decay. We refer to this point in the sections below:</p><p>“It is important to note that the steady long-term decay, which is a key feature of the memory constrained model predicts that the performance facilitation should disappear after 24 hours, and certainly after 7 weeks. After such time periods, the memory traces for the reoccurring patterns should decay to zero, and the corresponding facilitation effect should disappear. Remarkably, the participants exhibited unaltered performance facilitation. This suggests that the memory traces of these reoccurring patterns are somehow ‘fixed’ at a certain point during testing. One way of simulating this effect would be to change the asymptote of the exponential memory decay, such that the memory trace asymptotically approaches a small but non-zero value as time tends to infinity. However, we found that incorporating such an asymptote caused the performance facilitation for RANREGr trials to increase constantly from block to block, in contrast to the slow plateau shown in the behavioural data. It seems likely, therefore, that there remains a non-trivial ‘fixing’ effect that may reflect consolidation processes, not accounted for by the current model (to our knowledge there is no other statistical learning model that accounts both for learning dynamics and long-term fixed effects).” (subsection “Modelling”).</p><p>“The persistence of a stable RT advantage 24 hours and 7 weeks after initial exposure demonstrates the establishment of a long-term memory representation, possibly through a process of consolidation involving long-lasting synaptic changes (Phan et al., 2017; Redondo and Morris, 2011). It may also be tempting to interpret the resistance to interruption, observed in early stages of memory formation (Exp. 3, Exp. S2), as a hint that a form of consolidation might have occurred already after a few initial presentations.” (subsection “Time scales of memory for sequences”).</p></body></sub-article></article>