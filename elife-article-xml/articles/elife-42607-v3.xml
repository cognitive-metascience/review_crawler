<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">42607</article-id><article-id pub-id-type="doi">10.7554/eLife.42607</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Leave-One-Trial-Out, LOTO, a general approach to link single-trial parameters of cognitive models to neural data</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-66814"><name><surname>Gluth</surname><given-names>Sebastian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2241-5103</contrib-id><email>sebastian.gluth@unibas.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-123096"><name><surname>Meiran</surname><given-names>Nachshon</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Psychology</institution><institution>University of Basel</institution><addr-line><named-content content-type="city">Basel</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychology</institution><institution>Ben-Gurion University of the Negev</institution><addr-line><named-content content-type="city">Beer-Sheva</named-content></addr-line><country>Israel</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Zlotowski Center for Neuroscience</institution><institution>Ben-Gurion University of the Negev</institution><addr-line><named-content content-type="city">Beer-Sheva</named-content></addr-line><country>Israel</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Van Maanen</surname><given-names>Leendert</given-names></name><role>Reviewing Editor</role><aff><institution>University of Amsterdam</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>08</day><month>02</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e42607</elocation-id><history><date date-type="received" iso-8601-date="2018-10-05"><day>05</day><month>10</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-02-07"><day>07</day><month>02</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Gluth and Meiran</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Gluth and Meiran</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-42607-v3.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.42607.001</object-id><p>A key goal of model-based cognitive neuroscience is to estimate the trial-by-trial fluctuations of cognitive model parameters in order to link these fluctuations to brain signals. However, previously developed methods are limited by being difficult to implement, time-consuming, or model-specific. Here, we propose an easy, efficient and general approach to estimating trial-wise changes in parameters: Leave-One-Trial-Out (LOTO). The rationale behind LOTO is that the difference between parameter estimates for the complete dataset and for the dataset with one omitted trial reflects the parameter value in the omitted trial. We show that LOTO is superior to estimating parameter values from single trials and compare it to previously proposed approaches. Furthermore, the method makes it possible to distinguish true variability in a parameter from noise and from other sources of variability. In our view, the practicability and generality of LOTO will advance research on tracking fluctuations in latent cognitive variables and linking them to neural data.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cognitive modeling</kwd><kwd>intra-individual variability</kwd><kwd>leave-one-out</kwd><kwd>jackknife</kwd><kwd>model-based cognitive neuroscience</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>100014_172761</award-id><principal-award-recipient><name><surname>Gluth</surname><given-names>Sebastian</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>381-15</award-id><principal-award-recipient><name><surname>Meiran</surname><given-names>Nachshon</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Leave-One-Trial-Out (LOTO) is a general, efficient and easily implementable approach for inferring trial-by-trial measures of computational model parameters in order to link these measures to neural mechanisms.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Model-based cognitive neuroscience attempts to link mathematical models of cognitive processes to neural data in order to advance our knowledge of the mind and brain (<xref ref-type="bibr" rid="bib10">Forstmann et al., 2011</xref>). A particularly promising but challenging approach in this regard is to derive trial-specific values for parameters of cognitive models and to relate these values to trial-specific brain data, which offers insights into cognitive and neural principles at a highly detailed level of analysis (<xref ref-type="bibr" rid="bib18">Gluth and Rieskamp, 2017</xref>; <xref ref-type="bibr" rid="bib47">van Maanen et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Turner et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Wiecki et al., 2013</xref>). In the present work, we introduce a novel technique for capturing trial-specific values of model parameters that is efficient, widely applicable and easy to apply. We briefly summarize related work and methods before turning to our current proposal.</p><p>Primarily, the difficulty in linking momentary fluctuations in cognitive states to neural measures lies in specifying the variability in the cognitive model. Often, cognitive models specify the distribution from which this variability is assumed to originate (e.g., a normal distribution), but remain silent about its direction and extent in single trials. Previous attempts to capture parameter variability have often been specific to a single model. For example, <xref ref-type="bibr" rid="bib47">van Maanen et al. (2011)</xref> derived maximum likelihood estimates for two single-trial parameters of the Linear Ballistic Accumulator (LBA) model (<xref ref-type="bibr" rid="bib4">Brown and Heathcote, 2008</xref>). Because the LBA model is of special interest for the present paper, we describe it in some detail here. The LBA belongs to the class of sequential sampling models of decision making, which assume that decisions arise from an evidence-accumulation process that continues until a threshold has been reached, indicating that the required amount of evidence for a decision has been gathered. Three critical parameters of the LBA are the rate of evidence accumulation (drift rate), the amount of evidence required to reach a decision (decision threshold), and the point from which the accumulation begins (start point). Importantly, sequential sampling models such as LBA predict accuracy rates and the shape of response times (RT) distributions conjointly. <xref ref-type="bibr" rid="bib50">Wiecki et al. (2013)</xref> introduced a hierarchical Bayesian modeling tool for another sequential sampling model, the Diffusion Decision Model (DDM) (<xref ref-type="bibr" rid="bib37">Ratcliff, 1978</xref>). Their tool allows the regresssion of single-trial parameters of the DDM onto trial-by-trial neural measures such as electroencephalography (EEG) or event-related functional magnetic resonance imaging (fMRI) recordings (similar regression-based approaches were proposed by <xref ref-type="bibr" rid="bib19">Hawkins et al. (2017)</xref> and by <xref ref-type="bibr" rid="bib27">Nunez et al. (2017)</xref>). Turner and colleagues proposed a related method, whose rationale is to model behavioral and neuroimaging data jointly under a common (hierarchical Bayesian) umbrella (<xref ref-type="bibr" rid="bib29">Palestro et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Turner et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Turner et al., 2015</xref>). Although the approach was again tested within the DDM framework, it may be generalized to any other cognitive model. As we have argued before, however, the complexity of this approach might discourage many cognitive neuroscientists from applying it (<xref ref-type="bibr" rid="bib18">Gluth and Rieskamp, 2017</xref>). Another, but less general, approach of modeling behavioral and brain data jointly was proposed by <xref ref-type="bibr" rid="bib48">van Ravenzwaaij et al. (2017)</xref>.</p><p>We recently proposed a comparatively simple and very general approach to capture trial-by-trial variability in cognitive model parameters (<xref ref-type="bibr" rid="bib18">Gluth and Rieskamp, 2017</xref>). Again, the approach is based on Bayesian principles: it specifies the posterior probability of a parameter value in a specific trial, using the ‘average’ parameter value across all trials and the behavior in the specific trial as the prior and likelihood, respectively. Thus, the method basically answers the question of how the difference between a person’s behavior in specific trials vs. that in all trials can be mapped (in a Bayesian optimal way) onto changes in a specific parameter of interest. For example, a surprisingly fast decision made by an otherwise very cautious decision maker might be attributed to a reduced decision threshold (in the context of sequential sampling models such as LBA or DDM), which then might be linked to altered neural activation in a specific region of the brain such as the pre-supplementary motor area (<xref ref-type="bibr" rid="bib15">Gluth et al., 2012</xref>; <xref ref-type="bibr" rid="bib47">van Maanen et al., 2011</xref>). While this approach is both general and comparatively simple, it can require high amounts of computation time, in particular when one seeks to estimate variability in more than one parameter simultaneously.</p><p>In the current work, we propose a novel approach to capturing trial-by-trial variability in cognitive model parameters that tries to overcome the shortcomings of specificity, complexity, and inefficiency: Leave-One-Trial-Out (LOTO). Briefly, the idea of LOTO is that the difference in the parameter estimates that are based on all trials vs. those that are based on all trials except a single trial provides a reflection of the ‘true’ parameter value in the left-out trial. The rest of the article is structured as follows: first, we introduce the rationale behind LOTO and exemplify this rationale with a simple toy model. Second, we show the circumstances under which LOTO can be expected to capture trial-by-trial variability appropriately, and explain how LOTO’s performance can be improved by adapting its application, the underlying cognitive model or the experimental design. Third, we illustrate how one can test the assumption of the presence of systematic parameter variability. Fourth, we compare LOTO to previously proposed methods. Fifth, we present a simulation-based power analysis that aims to test whether LOTO can be expected to identify the neural correlates of trial-by-trial fluctuations of model parameters with sufficient power and specificity. Finally, we provide an example analysis, in which we apply LOTO to link variability in a cognitive process (encoding of episodic memory) to brain signals (fMRI activation in the hippocampus). Although we believe that it is essential for the potential user to read the entire article (in order to understand when and why LOTO can be expected to provide acceptable results), we note that some parts of the article are rather technical and may be difficult to understand. Therefore, our suggestion for readers with limited statistical and mathematical knowledge is to read 'Results: II. The LOTO principle', and 'Results: XI.: An example of using LOTO for model-based fMRI', which exemplifies the application of LOTO, and then to proceed directly to the discussion, in which we provide a ‘LOTO recipe’ that provides a step-by-step summary of how LOTO should be applied. The recipe then refers the reader back to relevant specific sections of the article.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>I. The LOTO principle</title><p>The rationale behind LOTO is simple and intuitive and can be summarized as follows:</p><list list-type="order"><list-item><p>When fitting a (cognitive) model to data, all <italic>n</italic> trials contribute to the model’s goodness-of-fit statistic (e.g., log-likelihood) and thus to the estimation of parameter values.</p></list-item><list-item><p>A single trial, <italic>t</italic>, shifts the parameter estimation to the direction that accords with the behavior in trial <italic>t</italic>.</p></list-item><list-item><p>Therefore, if trial <italic>t</italic> is taken out and the model is fitted again to the <italic>n</italic>–1 dataset, the new parameter estimate will shift (slightly) to the opposite direction.</p></list-item><list-item><p>Therefore, the (true and unknown) parameter value in <italic>t</italic> is reflected in the difference between the parameter estimate of all trials and the parameter estimate of all trials except <italic>t</italic>.</p></list-item></list><p>Accordingly, the application of LOTO works as follows:</p><list list-type="order"><list-item><p>Estimate all parameters of your model by fitting the model to the entire dataset (e.g., all <italic>n</italic> trials of a subject).</p></list-item><list-item><p>Re-estimate the parameter(s) of interest by fitting the model to the dataset without trial <italic>t</italic>; keep all other parameters fixed to their estimates derived in step 1.</p></list-item><list-item><p>Take the difference between the parameter estimate(s) of step 1 and step 2, which will provide a reflection of the parameter value in <italic>t</italic> (i.e., the difference should be correlated positively with the true parameter value).</p></list-item><list-item><p>Repeat the Steps 2 and 3 for all <italic>n</italic> trials.</p></list-item><list-item><p>You may then link the obtained vector of trial-by-trial parameter values to any external measure that was recorded at the same time and that has an appropriate temporal resolution (e.g., event-related fMRI data, EEG data, single-unit recordings, eye-tracking data, skin conductance responses; see also <xref ref-type="bibr" rid="bib18">Gluth and Rieskamp, 2017</xref>).</p></list-item></list><p>Note that instead of taking the difference between the estimates for <italic>n</italic> and <italic>n</italic>–1 trials in step 3, one could simply multiply the <italic>n</italic>-1 trials estimates by −1 to obtain LOTO estimates that are positively correlated with the true parameter values. However, we prefer taking the difference, as this nicely separates above-average from below-average parameter values, which will be positive and negative, respectively. In general, it should be obvious that LOTO does not provide single-trial parameter estimates in an absolute sense. Rather, the method provides information about the direction and (relative) amount of deviation of single-trial parameter values from the average. Note that, in most cases, cognitive neuroscientists will be interested in the correlation between trial-by-trial parameter values and neural data, for which absolute values do not matter (<xref ref-type="bibr" rid="bib18">Gluth and Rieskamp, 2017</xref>). In the rest of the article, we sometimes refer to ‘LOTO estimates of a parameter’, but it should be clear that this refers to estimates in a relative sense only.</p></sec><sec id="s2-2"><title>II. A ‘toy’ model example</title><p>In the following, we illustrate the LOTO principle using the binomial distribution as a ‘toy’ model example. In general, the binomial distributions specifies the probability of observing <italic>k</italic> successes out of <italic>n</italic> trials given probability <italic>θ</italic>, which specifies the probability of a success in each trial. Let us assume that we seek to find the estimate of <italic>θ</italic> that provides the best account of the observation of <italic>k</italic> successes in <italic>n</italic> trials. The Maximum Likelihood Estimate (MLE) of this is simply (e.g., <xref ref-type="bibr" rid="bib8">Farrell and Lewandowsky, 2018</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Let us now assume that we believe that <italic>θ</italic> is not stable but may vary from trial to trial, and that our goal is to estimate this trial-by-trial variability using LOTO. (Note that for this and the following sections, we simply assume that such trial-by-trial variability exists; we address the issue of testing for the presence of trial-by-trial variability in 'Results: Section V'. Also note that for this ‘toy’ model example, LOTO cannot provide more information about parameter <italic>θ</italic> than what is already given by the observations <italic>k</italic>; the goal of this section is to exemplify the workflow of LOTO rather than its capabilities.) As explained above, LOTO’s estimate for trial <italic>t</italic> is the difference between the <italic>n</italic> and the <italic>n</italic>–1 estimates of <italic>θ</italic>. For the binomial model, we thus obtain:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">¬</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>k<sub>t</sub></italic> indicates whether a success was observed in trial <italic>t</italic> (<italic>k<sub>t</sub></italic> = 1) or not (<italic>k<sub>t</sub></italic> = 0). The critical question is whether we can expect the LOTO approach to capture changes in observations meaningfully. To answer this question, we take the derivative of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> with respect to <italic>k<sub>t</sub></italic> to see how LOTO’s estimate changes as a function of <italic>k<sub>t</sub></italic>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ3">Equation 3</xref> implies that changes in single-trial observations map linearly and positively onto LOTO’s estimates, which is what we desired. To give an example, let us assume <italic>k</italic> = 5, <italic>n</italic> = 10, <italic>k<sub>t=1</sub></italic> = 1, and <italic>k<sub>t=2</sub></italic> = 0. Then LOTO’s estimates for <italic>t</italic> = 1 and <italic>t</italic> = 2 are 1/18 and −1/18, respectively, with the difference being 1/(<italic>n</italic>–1) = 1/9. <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> also reveals a general property of LOTO, namely that the change in LOTO is inversely related to the number of trials: naturally, the influence of a single trial on the estimation of a parameter decreases as the number of trials increases. We will further elaborate on this issue in 'Results: Section VII'. Also, we see that LOTO<italic><sub>t</sub></italic> is not an estimate of <italic>θ</italic> for trial <italic>t</italic> in an absolute sense (e.g., a <italic>θ</italic> of −1/18 is impossible), but that positive and negative LOTO<italic><sub>t</sub></italic> values indicate above- and below-average single-trial values, respectively.</p><p>So far, we have only established a sensible relationship between LOTO’s estimate and the observation <italic>k<sub>t</sub></italic>, but ultimately we are interested in whether LOTO is also related to the underlying trial-wise (data-generating) parameter <italic>θ<sub>t</sub></italic>. This depends on how a change in <italic>θ<sub>t</sub></italic> is mapped onto a change in <italic>k<sub>t</sub></italic>, which is given by the Fisher information, the variance of the first derivative of the log-likelihood function with respect to <italic>θ<sub>t</sub></italic>. For the binomial distribution with <italic>n</italic> = 1 (i.e., the Bernoulli distribution), the Fisher information is (e.g., <xref ref-type="bibr" rid="bib23">Ly et al., 2017</xref>):<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>As can be seen in the left panel of <xref ref-type="fig" rid="fig1">Figure 1A</xref>, the Fisher information of the Bernoulli distribution indicates that observations are more informative about the underlying parameter at the extremes. Accordingly, LOTO’s performance (i.e., the correlation between the true <italic>θ<sub>t</sub></italic> and LOTO’s estimates of it) will also depend on the range of <italic>θ<sub>t</sub></italic>. To illustrate this, we ran simulations by drawing 300 values of <italic>θ<sub>t</sub></italic> from a uniform distribution with a range of .2 (e.g., between .4 and .6) and repeated this for every range from [0, .2] to [.8, 1] in steps of .01 (detailed specifications of all simulations are provided in 'Materials and methods'). LOTO was then used to recover <italic>θ<sub>t</sub></italic>. The middle panel of <xref ref-type="fig" rid="fig1">Figure 1A</xref> depicts the average correlation between <italic>θ<sub>t</sub></italic> and its LOTO estimate as a function of the average <italic>θ</italic>. As predicted by the Fisher information, the correlation is higher when <italic>θ</italic> is close to 0 or close to 1. Even though we do not further discuss the Fisher information in the context of the more complex models in the following sections, the principles of Fisher information also apply there (including the fact that more extreme parameter values are most informative).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.002</object-id><label>Figure 1.</label><caption><title>Fisher information and performance of LOTO for the binomial distribution.</title><p>(<bold>A</bold>) Fisher information (left panel), average correlation between and LOTO’s estimate (middle panel), and example correlation for values of <italic>θ<sub>t</sub></italic> in the range [.4,.6] (right panel) for the Bernoulli distribution. (<bold>B</bold>) The same as in panel <bold>A</bold> but generalized to the binomial distribution, that is, assuming that <italic>θ</italic> is stable over <italic>m</italic> trials, for different levels of <italic>m</italic> (the example correlation in the right panel uses <italic>m</italic> = 20).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig1-v3.tif"/></fig><p>Notably, the (average) correlations between <italic>θ<sub>t</sub></italic> and LOTO’s estimate are quite low (i.e., &lt; .2). The reason for this can be seen in the right panel of <xref ref-type="fig" rid="fig1">Figure 1A</xref>, which shows the correlation for an example of 300 trials in the range [.4, .6]: the estimate of LOTO can only take two values, depending in whether a success was observed in <italic>t</italic> or not. Fortunately, the performance can be improved by assuming that <italic>θ</italic> does not change from trial to trial but remains constant over a certain number of trials, which we denote <italic>m</italic>. The larger <italic>m</italic>, the more different values LOTO can take (and at the same time, our confidence in the estimate of <italic>θ</italic> increases). LOTO’s estimate, its derivative with respect to <italic>m</italic>, and the Fisher information for the Bernoulli distribution generalized to <italic>m</italic> ≥ 1 (i.e., the binomial distribution) are:<disp-formula id="equ5"><label>(5a)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">¬</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ6"><label>(5b)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ7"><label>(5c)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>m</mml:mi><mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p><xref ref-type="fig" rid="fig1">Figure 1B</xref> illustrates the increases in Fisher information and LOTO’s performance as <italic>m</italic> increases. This is true, even though we kept the number of draws to 300 (implying that when <italic>m</italic> = 20, there are only <italic>n</italic>/<italic>m</italic> = 300/20 = 15 values estimated by LOTO; see <xref ref-type="fig" rid="fig1">Figure 1B</xref>, right panel). Thus, it might be more promising to track fluctuations over multiple trials (e.g., short blocks or sessions) than to try to capture parameter values in each and every trial. For example, <xref ref-type="bibr" rid="bib16">Gluth et al. (2015)</xref> investigated memory-based decisions and inferred whether an item had been remembered on the basis of three trials that included the item as a choice option (see also 'Results: Section XI', in which we use LOTO to reproduce this analysis).</p><p>We see that LOTO can be applied to the Bernoulli and binomial distributions, but it should be (again) noted that LOTO is not particularly helpful in these cases. This is because LOTO will not provide us with any novel information over and above what is already known from the observations. In other words, LOTO’s estimates are perfectly correlated with <italic>k<sub>t</sub></italic> or <italic>k<sub>m</sub></italic>, which is mirrored in the derivatives (<xref ref-type="disp-formula" rid="equ3 equ6">Equations 3 and 5b</xref>) which depend only on constants <italic>n</italic> or <italic>m</italic>. In principle, there are two ways to make LOTO (and any other method for tracking trial-by-trial variability in parameters) more useful. First, one could introduce variations in the task, with these variations being directly linked to the predictions of the cognitive model. Second, one could apply cognitive models that predict a ‘richer’ set of data (e.g., sequential sampling models that predict accuracy rates and RT distributions conjointly; see above). In the next section, we first turn to introducing variations in the task.</p></sec><sec id="s2-3"><title>III. Task variations and comparison with ‘single-trial fitting’</title><p>We demonstrate the importance of using variations in the experimental design that are systematically linked to the prediction of the cognitive model within the context of studying intertemporal choices. Intertemporal choices are decisions between a smaller reward available now or in near future vs. a larger reward available only in the more distant future (an example trial of a typical intertemporal choice task could be: <italic>‘Do you want to receive $20 now or $50 in 20 days?</italic>'). Traditionally, these decisions have been modeled using temporal discounting models that describe the decrease in utility <italic>u<sub>i</sub></italic> of an option <italic>i</italic> as a function of the delay <italic>d<sub>i</sub></italic> of its pay-out (<xref ref-type="bibr" rid="bib11">Frederick et al., 2002</xref>). We apply the hyperbolic discounting (HD) model (<xref ref-type="bibr" rid="bib24">Mazur, 1987</xref>), according to which:<disp-formula id="equ8"><label>(6)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>x<sub>i</sub></italic> refers to the amount of reward and κ is the discount rate (i.e., a free parameter modeling the degree to which utilities are lowered by delays; 0 ≤ κ ≤ 1). To derive choice probabilities (that can then be subjected to MLE for the estimation of κ), the HD model is usually combined with the logistic (or soft-max) choice function (e.g., <xref ref-type="bibr" rid="bib17">Gluth et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Peters et al., 2012</xref>), according to which the probability <italic>p<sub>i</sub></italic> of choosing option <italic>i</italic> from a set of options including <italic>i</italic> and <italic>j</italic> is:<disp-formula id="equ9"><label>(7)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>β</italic> is a second free parameter modeling the sensitivity of the decision maker to utility differences (<italic>β</italic> ≥ 0).</p><p>Our goal here is to show that by applying LOTO, we can extract information about parameter κ (which we assume to vary from trial to trial) that goes beyond the information already provided by the observations. We do this by looking at the first derivatives of the log-likelihood of the HD model with respect to κ. Let us assume that option <italic>i</italic> is an ‘immediate’ option, offering amount <italic>x<sub>i</sub></italic> at delay <italic>d<sub>i</sub></italic> = 0, so that <italic>u<sub>i</sub></italic> = <italic>x<sub>i</sub></italic>, and that option <italic>j</italic> is a ‘delayed’ option, offering amount <italic>x<sub>j</sub></italic> (<italic>x<sub>j</sub></italic> &gt; <italic>x<sub>i</sub></italic>) at delay <italic>d<sub>j</sub></italic> (<italic>d<sub>j</sub></italic> &gt; 0). It turns out that the first derivative of the log-likelihood for choosing the immediate option is:<disp-formula id="equ10"><label>(8a)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Detailed derivations are provided in the 'Materials and methods'. Similarly, the first derivative of the log-likelihood for choosing the delayed option is:<disp-formula id="equ11"><label>(8b)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mo>∗</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that the MLE can be obtained by setting these derivatives to 0. Importantly, we see that in contrast to the derivatives of the binomial distribution, the derivatives of the HD model depend on features of the task, namely on the amount and delay of the delayed option (i.e., <italic>x<sub>j</sub></italic> and <italic>d<sub>j</sub></italic>) and on the amount of the immediate option (i.e., <italic>x<sub>i</sub></italic>), which is hidden in <italic>p<sub>i</sub></italic>. At first glance, this already looks as if the estimation of single-trial values of κ<italic><sub>t</sub></italic> will vary with the task’s features, which is what we desired. Note, however, that both of the terms on the right-hand side of the derivatives are ≥ 0. Thus, by setting the derivatives in <xref ref-type="disp-formula" rid="equ10">Equation 8a</xref> and <xref ref-type="disp-formula" rid="equ11">Equation 8b</xref> to 0 to obtain the MLEs, parameter κ<italic><sub>t</sub></italic> will take the highest possible value (i.e., κ<italic><sub>t</sub></italic> = 1) whenever the immediate option is chosen and the lowest possible value (i.e., κ<italic><sub>t</sub></italic> = 0) whenever the delayed option is chosen. (Intuitively speaking: the best account for choosing the immediate option is to assume maximal impatience, the best account for choosing the delayed option is to assume is maximal patience.) This is illustrated in <xref ref-type="fig" rid="fig2">Figure 2A and B</xref>, which depict the log-likelihoods of choosing <italic>i</italic> (left panels) or <italic>j</italic> (middle panels) and their derivatives as a function of κ for two different trials with <italic>x<sub>i</sub>,</italic><sub>1</sub> = 20, <italic>x<sub>j</sub>,</italic><sub>1</sub> = 50, and <italic>d<sub>j</sub>,</italic><sub>1</sub> = 20 in trial 1 (bright colors) and <italic>x<sub>i</sub>,</italic><sub>2</sub> = 18, <italic>x<sub>j</sub>,</italic><sub>2</sub> = 100, and <italic>d<sub>j</sub>,</italic><sub>2</sub> = 25 in trial 2 (dark colors). Independent of the features of the task (i.e., the trial-specific amount and delays), the log-likelihoods of choosing <italic>i</italic> are monotonically increasing, and their derivatives are always positive, whereas the log-likelihoods of choosing <italic>j</italic> are monotonically decreasing, and their derivatives are always negative.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.003</object-id><label>Figure 2.</label><caption><title>Log-likelihoods and derivatives for the hyperbolic discounting model.</title><p>(<bold>A</bold>) Log-likelihoods for choosing the immediate option (left panel), choosing the delayed option (middle panel), and choosing the immediate option once and the delayed option once (right panel) as a function of parameter κ. The bright and dark colors refer to two different trials with different amounts and delays. (<bold>B</bold>) First derivatives of the log-likelihoods shown in panel <bold>A</bold>. Note that the estimation of κ will depend on the features of the task only when both options are chosen at least once.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig2-v3.tif"/></fig><p>Critically, however, this caveat disappears as soon as each option (<italic>i</italic> and <italic>j</italic>) is chosen at least once. For instance, when keeping <italic>x<sub>i</sub></italic>, <italic>x<sub>j</sub></italic>, and <italic>d<sub>j</sub></italic> constant over two trials in which both <italic>i</italic> and <italic>j</italic> are chosen once, the derivative of the log-likelihood becomes:<disp-formula id="equ12"><label>(8c)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>∗</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>which is positive if <italic>p<sub>i</sub></italic> &lt; .5 but negative if <italic>p<sub>i</sub></italic> &gt; .5. The right panels of <xref ref-type="fig" rid="fig2">Figure 2A and B</xref> show the log-likelihoods and their derivatives when assuming that trial 1 or trial 2 were presented twice, and <italic>i</italic> was chosen in one occasion and <italic>j</italic> was chosen in the other occasion. Here, the derivatives cross the 0-line, and even more importantly, they do so at different values of κ for the different trials (the MLE of κ for trial 2 is higher because the immediate option is comparatively less attractive than in trial 1).</p><p>Why does this allow LOTO to extract more information about κ<italic><sub>t</sub></italic> than that given by the observations? Recall that the estimate of LOTO is the difference between the MLE of κ for all trials and the MLE for all trials except <italic>t</italic>. As explicated above, these two MLEs will depend on the task features in the <italic>n</italic>/<italic>n</italic>–1 trials as long as both choice options (e.g., immediate, delayed) have been chosen at least once in the <italic>n</italic>/<italic>n</italic>–1 trials (so that we get non-extreme estimates of κ from the <italic>n</italic>/<italic>n</italic>–1 trials). In principle, LOTO can therefore be applied as soon as both options have been chosen at least twice over the course of an experiment (if one option has been chosen in only one trial, then the estimate of κ when excluding this particular trial will be extreme as in the single-trial case). We illustrate this using the two example trials from <xref ref-type="fig" rid="fig2">Figure 2</xref> at the end of the 'Materials and methods'.</p><p>It follows that LOTO should outperform the arguably most elementary method of capturing trial-by-trial parameter variability, which we denote ‘single-trial fitting’. With single-trial fitting, we mean that after estimating all parameters of a model once for all trials, a specific parameter of interest (e.g., κ of the HD model) is fitted again to each trial individually (while all other parameters are fixed to their ‘all-trial’ estimates). Here, we show that LOTO but not single-trial fitting of parameter κ of the HD model is able to extract additional information about the true κ<italic><sub>t</sub>,</italic> over and above what is already provided by the observations. Thereto, we generated 160 trials with a constant immediate choice option and variable delayed choice options, and simulated decisions of a participant with κ<italic><sub>t</sub></italic> varying from trial to trial. Then, we applied LOTO and the single-trial fitting method to recover κ<italic><sub>t</sub></italic>. As can be seen in <xref ref-type="fig" rid="fig3">Figure 3</xref>, the estimates of LOTO (left panel) but not of single-trial fitting (right panel) are correlated with the true κ<italic><sub>t</sub></italic> after accounting for the observations (i.e., whether the immediate or the delayed option was chosen). In other words, the positive correlation between κ<italic><sub>t</sub></italic> and the single-trial estimates (dashed magenta line) can be attributed entirely to the fact that higher values of κ<italic><sub>t</sub></italic> lead to more choices of the immediate option, but the positive correlation between κ<italic><sub>t</sub></italic> and the LOTO estimates is further driven by how expectable or surprising a decision is (given the trial features <italic>x<sub>i,t</sub></italic>, <italic>x<sub>j,t</sub></italic>, and <italic>d<sub>j,t</sub></italic>).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.004</object-id><label>Figure 3.</label><caption><title>Comparison of LOTO and single-trial fitting.</title><p>The left panel shows the correlation of the trial-specific parameter κ<italic><sub>t</sub></italic> of the HD model with LOTO’s estimates, separately for choosing the immediate (red) and for choosing the delayed (blue) option. The dashed line (magenta) shows the regression slope when collapsing over all trials. The right panel shows the same for the single-trial-fitting approach.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig3-v3.tif"/></fig><p>To summarize, one approach to extract information about trial-by-trial changes in a parameter (over and above what is already provided by the observations) is to introduce variations in the experiment that are linked in a systematic manner to the cognitive model. Because its estimates are not solely dependent on the behavior in a single trial but also take all remaining trials into account, LOTO allows to capture this additional information. Note, however, that for specific models, such as sequential sampling models of decision making, LOTO can be useful even without introducing task variations. In the next section, we will look at further improvements in LOTO’s ability to recover trial-specific parameter values by extending the HD model to allow the prediction of choice and RT conjointly.</p></sec><sec id="s2-4"><title>IV. LOTO benefits from modeling an enriched set of data</title><p>Using LOTO to recover trial-specific parameter values of models that predict only binary outcomes, such as choices or accuracy rates, is possible but somewhat limited. This can be seen when looking at the left panel of <xref ref-type="fig" rid="fig3">Figure 3</xref>, which suggests that LOTO’s estimates are always higher when the immediate option was chosen. The reason for this is that taking one trial out in which the immediate/delayed option was chosen will always de-/increase the estimation of the discount factor κ, because (as stated above) the single-trial log-likelihood is monotonically in-/decreasing given the choice. (The magnitude of this change varies as a function of the trial-specific choice options, but the sign of this change is determined by the choice.) However, this strict choice-dependent separation might not be plausible; for example, it could very well be that the delayed option is chosen in a trial in which this option offered a very high reward <italic>x<sub>j</sub></italic> with a very low delay <italic>d<sub>j</sub>,</italic> even though the actual discount factor in that trial (i.e., κ<italic><sub>t</sub></italic>) was higher than on average. To overcome this limitation, we suggest that the model is extended so that it can take RT data into account. Going back to the example, the intuition is that even if the highly attractive delayed option is chosen, a high discount factor κ<italic><sub>t</sub></italic> will make this decision more difficult, which will result in a comparatively long RT. Applying LOTO to the extended model then allows to capture this RT-specific information.</p><p>We extended the HD model by adding the LBA model to it (using an approach that is similar but not equivalent to that used by <xref ref-type="bibr" rid="bib39">Rodriguez et al., 2014</xref> and <xref ref-type="bibr" rid="bib40">Rodriguez et al., 2015</xref>). Specifically, we assume that the difference in utility between the immediate and delayed options is mapped onto their LBA’s expected drift rates as follows:<disp-formula id="equ13"><label>(9a)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>ν</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ14"><label>(9b)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>ν</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where λ is a free parameter that scales the relationship between utility and expected drift rate. Note that these definitions ensure that the two expected drift rates sum up to 1, following the original specification of the LBA (<xref ref-type="bibr" rid="bib4">Brown and Heathcote, 2008</xref>). More specifically, <italic>ν<sub>i</sub></italic>, and <italic>ν<sub>j</sub></italic> represent the means of two independent normal distributions (with standard deviation s), from which the actual drift rates in a trial are assumed to be drawn (see 'Materials and methods'). Choices and RT were simulated for 160 trials of 30 participants and LOTO was applied to the HD and the HD-LBA models. Single-trial fitting was also applied to the HD-LBA model. <xref ref-type="fig" rid="fig4">Figure 4A</xref> depicts correlations of an example participant between the true κ<italic><sub>t</sub></italic> and LOTO’s estimate for HD (left panel) and for HD-LBA (right panel). For HD-LBA, the correlation is much improved, and LOTO’s estimates do not strictly separate choices of the immediate and delayed options anymore. The improved performance of LOTO is confirmed when comparing the correlations of all 30 simulated participants (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; immediate chosen: <italic>t</italic>(29) = 10.79, <italic>p </italic>&lt; .001, Cohen’s <italic>d</italic> = 1.97, Bayes Factor in favor of the alternative hypothesis BF<sub>10</sub> &gt; 1,000; delayed chosen: <italic>t</italic>(29) = 8.79, <italic>p </italic>&lt; .001, <italic>d</italic> = 1.61, BF<sub>10</sub> &gt; 1,000).</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.005</object-id><label>Figure 4.</label><caption><title>Comparison of LOTO for the HD and the HD-LBA models.</title><p>(<bold>A</bold>) Correlation of the trial-specific parameter κ<italic><sub>t</sub></italic> with LOTO’s estimates for an example participant. The left panel shows the results when using the HD model, the right panel when using the HD-LBA model. (<bold>B</bold>) Same as panel (<bold>A</bold>) but for the single-trial fitting method when using the HD-LBA model. (<bold>C</bold>) Average correlation for all 30 simulated participants. Individual values (gray dots) are shown together with the 95% confidence intervals of the mean (black error bars).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig4-v3.tif"/></fig><p>Notably, the single-trial fitting method applied to the HD-LBA model is also able to capture some variability over and above the information that is already contained in the decisions (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This is because RT (and the interplay between RT and decisions) add further information about κ<italic><sub>t</sub></italic> that can be picked up by single-trial fitting. Nonetheless, applying LOTO to HD-LBA yields significantly better recovery of the trial-wise parameter values (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; immediate chosen: <italic>t</italic>(29) = 7.24, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.32, BF<sub>10</sub> &gt; 1,000; delayed chosen: <italic>t</italic>(29) = 7.71, <italic>p </italic>&lt; .001, <italic>d</italic> = 1.41, BF<sub>10</sub> &gt; 1,000). Taken together, recovering trial-by-trial changes in a parameter of a cognitive model with LOTO (but also with other techniques) is much more efficient when the model is able to predict a rich set of single-trial observations, such as RT (and possibly even continuous ratings of choice confidence as in <xref ref-type="bibr" rid="bib33">Pleskac and Busemeyer, 2010</xref>) in addition to choices or accuracy rates.</p></sec><sec id="s2-5"><title>V. A non-parametric test for the presence of systematic trial-by-trial variability</title><p>So far, our derivations and simulations have rested on the assumption that systematic variability in the parameter of interest exists. As we have argued before, however, this assumption does not have to be correct — not even when the overt behavior appears to be stochastic (<xref ref-type="bibr" rid="bib18">Gluth and Rieskamp, 2017</xref>). It could be that stochasticity is induced by random fluctuations that are not accounted for by the cognitive model (e.g., randomness in motor execution). When assuming variability in a specific parameter, the risk is that such unsystematic variability is taken up by LOTO even in the absence of any systematic variability in the parameter. Hence, the question is whether one can separate situations in which LOTO captures systematic vs. unsystematic trial-by-trial variability.</p><p>To test for the presence of systematic trial-by-trial variability, we propose a non-parametric and simulation-based approach that exploits the improvement in model fit by LOTO: if a model is fitted to <italic>n</italic>–1 trials, the fit statistic (e.g., the deviance) will improve for two reasons: i) the statistic for the left-out trial is excluded; and ii) the parameter estimate is adjusted so that it optimally fits the <italic>n</italic>–1 dataset. The rationale of our proposed approach is that the improvement in fit that is attributable to the second reason (i.e., the adjustment) will be lower when there is no systematic trial-by-trial variability in the parameter of interest compared to when there is systematic variability. Practically, we suggest conducting this test in five steps:</p><list list-type="order"><list-item><p>For each participant, estimate parameters of the cognitive model for all trials and compute the goodness-of-fit per trial.</p></list-item><list-item><p>For each participant, generate many (e.g., 1,000) simulations of the model for all trials and all participants using the participants’ estimated parameters and assuming no variability in the parameter of interest (i.e., the null hypothesis).</p></list-item><list-item><p>Estimate parameters for these simulated data again, apply LOTO, and compute the LOTO-based improvement in goodness-of-fit for the <italic>n</italic>–1 trials.</p></list-item><list-item><p>Do the same as in step 3, but for the empirical data.</p></list-item><list-item><p>Test whether the LOTO-based improvement in fit for the empirical data is greater than 95% of the improvements in the simulations that were generated under the null hypothesis.</p></list-item></list><p>Note that one advantage of this non-parametric approach is that it does not rely on any assumptions about how the improvement of model fit is distributed under the null hypothesis.</p><p><xref ref-type="fig" rid="fig5">Figure 5A</xref> shows the results of this test for the HD model introduced in 'Results: Section III'.</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.006</object-id><label>Figure 5.</label><caption><title>A test for the presence of parameter variability.</title><p>(<bold>A</bold>) Improvement of the HD model’s deviance (in the <italic>n</italic>–1 trials included per LOTO step) from the ‘all-trials’ fit to the LOTO fit when variability in κ is absent (histogram) and when variability in κ of increasing amount is present (blue to red vertical lines). The black dashed line indicates the 5% of simulations under the null hypothesis with the largest improvement and serves as the statistical threshold. (<bold>B</bold>) Same as in panel <bold>A</bold> but with variability in <italic>β</italic> instead of κ.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.42607.007</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Testing for misattribution of variability in input and output variables to variability in a parameter.</title><p>Depicted are tests for systematic trial-by-trial variability of κ, but with true variability in input variables (amounts or delays; upper and middle panel) or output variables (utilities; lower panel). As in <xref ref-type="fig" rid="fig5">Figure 5</xref>, the histogram represents the distribution of improvement in model fit under the null hypothesis, the dashed black line indicates the significance criterion, and the colors of the vertical lines indicate the degree of variability induced in the respective variables.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig5-figsupp1-v3.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.42607.008</object-id><label>Figure 5—figure supplement 2.</label><caption><title>Testing for the presence of parameter variability and for misattributions in an adapted task.</title><p>In the intertemporal choice task with three choice options, true variability in discount factor κ still leads to improvements in model fit that exceed the improvement under the null hypothesis (first panel). At the same time, variability in input or output variables is not misattributed to κ anymore (lower panels). As in <xref ref-type="fig" rid="fig5">Figure 5</xref>, the histogram represents the distribution of improvement in model fit under the null hypothesis, the dashed black line indicates the significance criterion, and the colors of the vertical lines indicate the degree of variability induced in the respective variables.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig5-figsupp2-v3.tif"/></fig></fig-group><p>We generated 1,000 simulations per participant under the null hypothesis and 10 additional simulations with increasing levels of variability in parameter κ (for details, see 'Materials and methods'). Importantly, the figure shows the average difference in deviance between the ‘all-trial’ fit and the LOTO fits only for the <italic>n</italic>–1 trials that were included in each LOTO step. This means that the trivial improvement in fit resulting from the fact that LOTO has to explain one trial less is taken out. Thus, the histogram in <xref ref-type="fig" rid="fig5">Figure 5A</xref>, which shows the results for the 1,000 simulations under the null hypothesis, confirms our suspicion that LOTO will improve the fit even in the absence of true trial-by-trial variability in parameter κ. However, the continuous vertical lines, which show the results for the simulations with true variability of increasing amount (from blue to red), indicate significantly higher improvements in fit when variability is actually present — except for the lowest level of variability (the dashed black line indicates the 95% threshold).</p><p>What happens if there is no systematic variability in the parameter of interest (here: κ), but there is systematic variability in another parameter (here: <italic>β</italic>)? We can repeat the previous analysis but now simulate variability (of increasing amount) in <italic>β</italic> instead of κ. (Note that variability in <italic>β</italic> would mean that the utility difference between choice options is mapped onto the decision in a more/less deterministic way, depending on whether <italic>β<sub>t</sub></italic> is high or low in a trial; for instance, choosing the immediate option in a trial in which the delayed option is very attractive could be attributed to a low <italic>β<sub>t</sub></italic> value instead of a high κ<italic><sub>t</sub></italic> value.) As can be seen in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, the improvement in goodness-of-fit when trying to explain variability in <italic>β</italic> by modeling variability in κ using LOTO does not exceed the improvement under the null hypothesis, and there is no systematic relationship with the amount of variability in <italic>β</italic>. Thus, we can conclude that finding a significantly higher improvement in fit in our data compared to simulated data under the null hypothesis cannot be due to a misattribution of variability in <italic>β</italic> to variability in κ. (Of course, this conclusion does not have to hold for other models, parameter sets, and tasks; the test has to be conducted anew for every different situation.)</p><p>Variability in behavior (and neural activity) may derive not only from variability in model parameters but also from the processes used to infer the input and output signals of a computation (<xref ref-type="bibr" rid="bib6">Drugowitsch et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Findling et al., 2018</xref>; <xref ref-type="bibr" rid="bib22">Loomes et al., 2002</xref>; <xref ref-type="bibr" rid="bib34">Polanía et al., 2019</xref>). For example, choosing the immediate or delayed option in the intertemporal choice task might depend not only on the current impulsiveness (i.e., κ) but also on whether the utility of the delayed option (i.e., <italic>u<sub>j</sub></italic>) is computed correctly or whether the amount <italic>x<sub>j</sub></italic> and the delay <italic>d<sub>j</sub></italic> are perceived accurately. Thus, it might be desirable to examine whether LOTO can dissociate potential variability in these features from variability in the parameter of interest.</p><p>Accordingly, we simulated data of the HD model without trial-by-trial variability in κ but with variability in either the amounts <italic>x<sub>j</sub></italic>, the delays <italic>d<sub>j</sub></italic> or the computation of utilities <italic>u<sub>j</sub></italic> of the delayed option. (Note that we ensured that the degree of variability in these variables was similar to that of κ in the previous simulations; see 'Materials and methods'.) Then, we applied LOTO to these simulated data to test whether LOTO misattributes the variability to κ. As can be seen in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, LOTO does not misattribute variability in amounts, but for 3 out of 10 simulations of delay-based variability and for 4 of 10 simulations of utility-based variability, a misattribution could not be ruled out. This appears reasonable given the tight connection of the parameter to both delays and utilities (see <xref ref-type="disp-formula" rid="equ8">Equation 6</xref>).</p><p>How can the separability of the discount factor κ from the input and output variables <italic>x</italic>, <italic>d</italic>, and <italic>u </italic>be improved? We reasoned that changing the intertemporal choice task by increasing the number of options should help: whereas the discount factor can be assumed to be stable across options (and to vary only from trial to trial), the variability in the input and output variables can be assumed to differ between options (i.e., they are drawn from independent distributions for each option). Accordingly, we extended the simulated task to three options, which can still be considered a reasonable experimental design (e.g., <xref ref-type="bibr" rid="bib17">Gluth et al., 2017</xref>). We then performed the test for systematic variability using simulated data for this extended task that exhibited true variability in either κ, <italic>x</italic>, <italic>d</italic>, or <italic>u</italic>. As predicted, a significant improvement in model fit over and above the improvement under the null hypothesis of no variability was only found for κ but not for any of the input or output variables (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). In conclusion, LOTO might be an efficient tool to dissociate parameter variability from variability in the input and output signals of computational processes, assuming an appropriate experimental design and careful simulations.</p></sec><sec id="s2-6"><title>VI. Inferring trial-by-trial variability in more than one parameter</title><p>An advantageous feature of LOTO is that the duration of applying the method is almost independent of the number of parameters for which trial-by-trial variability is inferred. The full model is still estimated once for all <italic>n</italic> trials, and LOTO is still applied <italic>n</italic> times (but the estimation might take a bit longer for multiple parameters). In this section, we illustrate the application of LOTO to inferring trial-by-trial variability in two parameters of the HD-LBA model: the discount factor κ and the decision threshold <italic>b</italic> (for details, see 'Materials and methods').</p><p>First, we used the non-parametric test from the previous section to check for the presence of systematic variability. In doing this, we simulated 1,000 sets of 30 participants under the null hypothesis and tested whether the simulation with true variability in κ and <italic>b</italic> provides a significantly improved deviance than those simulations. This was confirmed as only 19 of the 1,000 simulations under the null hypothesis provided a better fit. It should be noted that the explanatory power of this test is somewhat limited, because we can only conclude that there is systematic variability in at least one of the two parameters but not necessarily in both of them.</p><p>Next, we looked at the correlations between the true κ<italic><sub>t</sub></italic> and <italic>b<sub>t</sub></italic> parameter values and their LOTO estimates for an example participant. As shown in the upper left panel of <xref ref-type="fig" rid="fig6">Figure 6A</xref>, LOTO is still capable of recovering the variability in parameter κ from trial to trial for both types of decisions (i.e., immediate and delayed). Similarly, LOTO captures the variability in parameter <italic>b</italic> (lower right panel). The lower left and upper right panels in <xref ref-type="fig" rid="fig6">Figure 6A</xref> show the correlations between the true κ<italic><sub>t</sub></italic> and <italic>b<sub>t</sub></italic> with the ‘wrong’ LOTO estimates of <italic>b<sub>t</sub></italic>and κ<italic><sub>t</sub></italic>, respectively. Ideally, one would desire correlations close to zero here, that is, LOTO should not attribute variability in one parameter to variability in another parameter. However, we see that there are modest correlations whose signs depend on the choice. When the immediate option was chosen, the correlations are negative; when the delayed option was chosen, the correlations are positive. These results are confirmed when looking at the average correlations across all 30 simulated participants (<xref ref-type="fig" rid="fig6">Figure 6B</xref>): LOTO attributes variability in the ‘correct’ parameters independent of choice and variability in the ‘wrong’ parameters depending on which option was chosen. (Intuitively, a high discount factor κ<italic><sub>t</sub></italic> competes with the decision threshold <italic>b<sub>t</sub></italic> as an alternative explanation for fast choices of the immediate option or slow choices of the delayed option. Similarly, a high decision threshold <italic>b<sub>t</sub></italic> competes with the discount factor κ<italic><sub>t</sub></italic> as an alternative explanation for choices of the option with highest utility.)</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.009</object-id><label>Figure 6.</label><caption><title>Capturing trial-by-trial variability in two parameters.</title><p>(<bold>A</bold>) Correlations of true parameter values κ<italic><sub>t</sub></italic> and <italic>b<sub>t</sub></italic> with their LOTO estimates for an example participant; the on-diagonal elements show the ‘correct’ inferences, the off-diagonal elements show the ‘wrong’ inferences. (<bold>B</bold>) Average correlations for all simulated participants (left panel: correlation with κ<italic><sub>t</sub></italic>; right panel: correlation with <italic>b<sub>t</sub></italic>). (<bold>C</bold>) Average standardized regression coefficients of LOTO estimates, choices, and RT when explaining variance in κ<italic><sub>t</sub></italic> (left panel) and <italic>b<sub>t</sub></italic> (right panel). Individual values (gray dots) are shown together with 95% confidence intervals of the mean (black error bars).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig6-v3.tif"/></fig><p>Importantly, these results indicate that the observations (i.e., choices and RT) could explain away the undesired correlations while not affecting the desired correlations. To test this, we conducted random-effects linear regression analyses with the true parameter (κ<italic><sub>t</sub></italic>, <italic>b<sub>t</sub></italic>) as the dependent variable and with the LOTO estimates of these parameters together with choices and RT as independent variables (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). The standardized regression coefficient (averaged across simulated participants) for LOTO’s estimate of κ<italic><sub>t</sub></italic> when explaining variance in true κ<italic><sub>t</sub></italic> was indeed significantly positive with a very high effect size (<italic>t</italic>(29) = 28.48, <italic>p </italic>&lt; .001, <italic>d</italic> = 5.20, BF<sub>10</sub> &gt; 1,000). The coefficient for the ‘wrong’ estimate of <italic>b<sub>t</sub></italic> when explaining variance in κ<italic><sub>t</sub></italic> also reached significance, but the effect was much weaker (<italic>t</italic>(29) = 3.01, <italic>p </italic>= .005, <italic>d</italic> = 0.55, BF<sub>10</sub> = 7.74). With respect to explaining variance in true <italic>b<sub>t</sub></italic>, only LOTO’s estimate of the ‘correct’ estimate of <italic>b<sub>t</sub></italic> was significant and strong (<italic>t</italic>(29) = 30.40, <italic>p </italic>&lt; .001, <italic>d</italic> = 5.55, BF<sub>10</sub> &gt; 1,000), whereas the effect for the ‘wrong’ estimate of κ<italic><sub>t</sub></italic> did not reach significance (<italic>t</italic>(29) = −1.93, <italic>p </italic>= .063, <italic>d</italic> = −0.45, BF<sub>10</sub> = 0.99).</p><p>Taken together, LOTO makes it possible to infer trial-by-trial variability in more than one parameter within the same model, revealing information over and above that provided by the observations. At the same time, LOTO’s misattributions of variability to other parameters are largely explained away by the observations. (As before, these conclusions are restricted to the current setting of task, model, and parameters, and have to be tested anew for different settings.) Notably, the random-effects linear regression analysis conducted above is very similar to the conventional approach of analyzing fMRI data on the basis of the General Linear Model (<xref ref-type="bibr" rid="bib12">Friston et al., 1994</xref>). The ultimate question is not whether LOTO induces some significant misattribution or not, but whether this misattribution can in turn be expected to lead to wrong inferences about the relationship between brain activity and variability in cognitive parameters. We further elaborate on this issue in 'Results: Section X'. </p></sec><sec id="s2-7"><title>VII. The performance of LOTO as a function of the number of trials</title><p>Trial numbers in neuroimaging studies are often low. fMRI studies are particularly limited because scanning hours are expensive, and because the low temporal resolution of fMRI requires long inter-trial intervals in the case of event-related fMRI (which is most suitable for trial-by-trial analyses). Therefore, it is important to understand the extent to which LOTO’s accuracy depends on the number of trials. To test this, we repeated the simulations of the HD and the HD-LBA models for 20, 40, 80, 160, 320, and 640 trials. As shown in <xref ref-type="fig" rid="fig7">Figure 7A</xref>, the average extent of the correlations between the true parameter values and the respective estimates of LOTO are largely unaffected by the number of trials. The recovery performance is slightly reduced for the threshold parameter <italic>b</italic> of the HD-LBA model in the case of 20 trials. Note, however, that the variance of the correlations decreases as the number of trials increases. Therefore, LOTO still benefits from increasing the number of trials in terms of achieving higher statistical power, as we show in 'Results: Section X'.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.010</object-id><label>Figure 7.</label><caption><title>Relationship between the performance of LOTO and the number of trials.</title><p>(<bold>A</bold>) Average correlations between true parameter values and LOTO estimates for parameters of the HD and the HD-LBA models as a function of the number of trials per simulated participant. (<bold>B</bold>) Example correlations between true values and LOTO estimates for 160 trials (upper panels) and for 1,600 trials (lower panels), separately for a strict tolerance criterion for convergence of the minimization algorithm (left panels) and for a more lenient, default tolerance criterion (right panels). Error bars indicate 95% confidence intervals of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig7-v3.tif"/></fig><p>In 'Results: Section II', we mentioned that the impact of a single trial on the overall model fit, as well as on the LOTO estimate, decreases as the number of trials increases. If the number of trials is very high, then the improvement in model fit that results from a change in the parameter value might become smaller than the default tolerance criterion set by the minimization algorithm. In other words, the algorithm assumes convergence because the change in model fit caused by taking one trial out is insignificantly small. <xref ref-type="fig" rid="fig7">Figure 7B</xref> illustrates this phenomenon. The upper panels show the results for parameter κ of the HD model for 160 trials, the lower panels for 1,600 trials. In the left panels, a strict tolerance criterion was used; in the right panels, the default tolerance criterion in MATLAB was used (for details, see 'Materials and methods'). In the case of 1,600 trials, the tolerance criterion matters: if the default criterion is used, the LOTO estimates exhibit discrete jumps, that is, the same parameter value is estimated for trials with relatively similar observations, because a more fine-graded estimation does not improve the fit beyond the default tolerance criterion. This is not the case for the strict tolerance criterion.</p><p>According to the example in <xref ref-type="fig" rid="fig7">Figure 7B</xref>, the default tolerance criterion appears to lower the correlation between the true values and the LOTO estimates in the case of 1,600 trials, but only to a minor extent. This observation is confirmed when comparing the correlations across all 30 simulated participants. In the case of 160 trials, the correlations obtained using the strict and the default tolerance criterion are not significantly different (<italic>t</italic>(29) = 1.27, <italic>p </italic>= .213, <italic>d</italic> = 0.23, BF<sub>10</sub> = 0.40). In the case of 1,600 trials, there is a significant difference (<italic>t</italic>(29) = 4.59, <italic>p </italic>&lt; .001, <italic>d</italic> = 0.84, BF<sub>10</sub> = 323.03), but the difference is small in absolute terms (i.e., on average the correlation is reduced by only .003). In general, we recommend checking the potential impact of the tolerance criterion on LOTO’s performance by running simulations, in particular when LOTO is applied to a task with many trials (i.e., ≥ 1,000).</p></sec><sec id="s2-8"><title>VIII. Invariance of LOTO with respect to the underlying parameter distribution</title><p>In contrast to some other methods for inferring trial-by-trial variability in model parameters (<xref ref-type="bibr" rid="bib47">van Maanen et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Turner et al., 2015</xref>), LOTO does not require the assumption that parameters are drawn from specific distributions. Even though it is not the purpose of LOTO, one might wonder whether the distributions of parameters and of LOTO’s estimates can be expected to be similar. To answer this question, we reverted to the initial ‘toy’ model example of the binomial distribution ('Results: Section II'). For this purpose, we sampled 10,000 Bernoulli events, which we split into 500 ‘blocks’ of 20 trials each (see 'Materials and methods'). For each block, parameter <italic>θ<sub>m</sub></italic> was drawn from either a normal or a uniform distribution and observations <italic>k<sub>m</sub></italic> were then drawn from binomial distributions given <italic>θ<sub>m</sub></italic>. Finally, LOTO was applied to infer variability in <italic>θ</italic>.</p><p>The left and right panels of <xref ref-type="fig" rid="fig8">Figure 8A</xref> depict the normally and uniformly distributed parameter values of θ<italic><sub>m</sub></italic>, respectively. Similarly, the left and right panels of <xref ref-type="fig" rid="fig8">Figure 8B</xref> show the distributions of the corresponding LOTO estimates. The distributions of the true parameter values are markedly different, whereas the distributions of LOTO estimates are very similar. The reason for this can be seen in <xref ref-type="fig" rid="fig8">Figure 8C</xref>, which depicts the distribution of observations <italic>k<sub>m</sub></italic>, which perfectly match the LOTO distributions (because the observations and LOTO estimates are perfectly correlated for this ‘toy’ model; see 'Results: Section II'). By definition, the observations are distributed binomially and so are the LOTO estimates. Generally speaking, the distribution of LOTO estimates is mostly invariant to the distribution of the underlying parameters but depends much more on how the parameter values map onto the observations via the model and its likelihood function.</p><fig-group><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.011</object-id><label>Figure 8.</label><caption><title>Invariance of LOTO with respect to the underlying parameter distribution.</title><p>(<bold>A</bold>) Distribution of the data-generating parameter values <italic>θ<sub>m</sub></italic> for two simulations that sample from a normal distribution (left, blue) or from a uniform distribution (right, red). (<bold>B</bold>) The corresponding distributions of LOTO estimates. (<bold>C</bold>) The corresponding distributions of observations <italic>k<sub>m</sub></italic>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig8-v3.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.42607.012</object-id><label>Figure 8—figure supplement 1.</label><caption><title>Comparison of LOTO distributions for parameter κ of the HD-LBA model.</title><p>The first and second rows depict the distributions of true parameters and LOTO estimates, respectively, for thenormally (left, blue) and uniformly (right, red) distributed true parameters. The third row depicts correlations between true parameters and LOTO estimates for an example participant. The lowest panel depicts the average correlations for all 30 participants for the normally and uniformly distributed true parameters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig8-figsupp1-v3.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.42607.013</object-id><label>Figure 8—figure supplement 2.</label><caption><title>Comparison of LOTO distributions for parameter <italic>b</italic> of the HD-LBA model.</title><p>The first and second rows depict the distributions of true parameters and LOTO estimates, respectively, fornormally (left, blue) and uniformly (right, red) distributed true parameters. The third row depicts correlations between true parameters and LOTO estimates for an example participant. The lowest panel depicts average correlations for all 30 participants for the normally and uniformly distributed true parameters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig8-figsupp2-v3.tif"/></fig></fig-group><p>To generalize these conclusions to the more interesting case of a cognitive model, we sampled single-trial parameter values of κ (i.e., the discount factor) and <italic>b</italic> (i.e., the decision threshold) of the HD-LBA model either from a normal distribution or from a uniform distribution (details are provided in 'Materials and methods'). The results for parameter κ and for parameter <italic>b</italic> are presented in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> and <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>, respectively. Again, the distributions of LOTO estimates appear to be invariant to the shape of the underlying parameter distributions. In this case of the HD-LBA model, the parameter values and LOTO’s estimates are linked via the LBA likelihood function. Most of all, this function depends on the drift rates that are drawn from normal distributions. Expectedly, the LOTO estimates appear to be roughly normally distributed. Importantly, our simulations of the HD-LBA model also suggest that the performance of LOTO (in terms of the correlation between true parameter values and LOTO estimates) does not seem to depend much on the distribution of the underlying parameter values. Hence, we conclude that LOTO cannot be used to infer the shape of the underlying parameter distribution. Despite this limitation, LOTO is highly efficient in detecting significant relationships between cognitive model parameters and neural data, regardless of the underlying parameter distribution.</p></sec><sec id="s2-9"><title>IX. Comparison of LOTO with related approaches</title><p>In this section, we compare LOTO’s ability to recover the underlying trial-by-trial variability of cognitive model parameters with alternative techniques. More specifically, we generated synthetic data (i.e., choices and RT) from the LBA model (<xref ref-type="bibr" rid="bib4">Brown and Heathcote, 2008</xref>), which assumes that the trial-wise drift rates <italic>ν<sub>t</sub></italic> and start points <italic>A<sub>t</sub></italic> are drawn from normal and uniform distributions, respectively (details are provided in the 'Materials and methods'). We compare LOTO with three other methods: i) with our previously proposed technique (<xref ref-type="bibr" rid="bib18">Gluth and Rieskamp, 2017</xref>; henceforth ‘GR17’), ii) with ‘single-trial fitting’, and iii) with the ‘Single-Trial LBA’ approach (<xref ref-type="bibr" rid="bib47">van Maanen et al., 2011</xref>; henceforth ‘STLBA’), which was specifically developed for retrieving maximum-likelihood estimates of trial-wise drift rates and start points of the LBA.</p><p><xref ref-type="fig" rid="fig9">Figure 9</xref> shows correlations between the true drift rates and start points and the values recovered by each method for an example simulation (upper panels), together with average correlations for multiple simulated experiments (lowest panels). From the example correlations, we see that LOTO and GR17 yield very similar results. This is to be expected because the two approaches employ similar principles. In both cases, parameters are estimated for all trials, and those parameters for which trial-by-trial variability is not assumed are fixed to their all-trial estimates; then, both approaches use the discrepancy between the observation in each trial and the observations over all trials to infer plausible trial-specific values. However, the comparison across multiple simulations reveals that LOTO performs significantly better than GR17 for both parameters (drift rate: <italic>t</italic>(19) = 3.76, <italic>p </italic>= .001, <italic>d</italic> = 0.84, BF<sub>10</sub> = 28.75; start point: <italic>t</italic>(19) = 4.86, <italic>p </italic>&lt; .001, <italic>d</italic> = 1.09, BF<sub>10</sub> = 259.17). The reason for this is that the performance of GR17 depends on the step size with which the range of possible parameter values is scanned, and we were forced to choose an immediate step size to avoid excessive computation times. As a consequence, the GR17 estimates exhibit discrete jumps (similar to those mentioned in the previous section when applying LOTO with a too lenient tolerance criterion), which lower the correlation with the true parameters. In other words, the advantage of LOTO over GR17 is its higher processing speed, which is particularly relevant when modeling variability in multiple parameters.</p><fig-group><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.014</object-id><label>Figure 9.</label><caption><title>Comparison of methods for capturing trial-wise parameter values.</title><p>(<bold>A</bold>) Results for the LBA drift rate parameter. The first four panels show correlations for an example participant for the methods LOTO (blue), GR17 (cyan), single-trial fitting (ST; red), and STLBA (green). The lowest panel shows correlations averaged over all simulations. (<bold>B</bold>) Same as panel <bold>A</bold> but for the LBA start point parameter. Individual values (gray dots) are shown together with 95% confidence intervals of the mean (black error bars).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig9-v3.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.42607.015</object-id><label>Figure 9—figure supplement 1.</label><caption><title>Comparison of LOTO with Bayesian modeling and joint modeling.</title><p>(<bold>A</bold>) Results for the HD model with respect to capturing variability in the discount factor κ. The three upper panels show correlations of an example participant for LOTO (blue), Bayesian modeling (red), and joint modeling (magenta). The lowest panel shows correlations averaged over all 30 simulated participants. (<bold>B</bold>) Same as panel <bold>A</bold> but with respect to capturing the variability in a (correlated) neural signal <italic>F</italic>(κ)<italic><sub>t</sub></italic>. Individual values (gray dots) are shown together with 95% confidence intervals of the mean (black error bars).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig9-figsupp1-v3.tif"/></fig></fig-group><p>The ‘single-trial fitting’ approach yields acceptable results for the drift rate, but is still outperformed by LOTO (<italic>t</italic>(19) = 19.72, <italic>p </italic>&lt; .001, <italic>d</italic> = 4.41, BF<sub>10</sub> &gt; 1,000). More dramatically, the method fails completely with respect to recovering the start point values. In our view, this is because adjusting the drift rate is sufficient to account for the observations in a single trial (i.e., whether the choice was correct or not and how fast the choice was), so that there is no information left to be captured by the start point. By contrast, LOTO (and GR17) estimates depend on how the likelihood space of the LBA model changes when one trial is taken out, and this likelihood space depends on both parameters.</p><p>Finally, the STLBA method performs worse than LOTO with respect to the drift rate (<italic>t</italic>(19) = 52.60, <italic>p </italic>&lt; .001, <italic>d</italic> = 11.76, BF<sub>10</sub> &gt; 1,000) but better with respect to the start point (<italic>t</italic>(19) = −30.08, <italic>p </italic>&lt; .001, <italic>d</italic> = −6.73, BF<sub>10</sub> &gt; 1,000). Looking at the example simulations in <xref ref-type="fig" rid="fig9">Figure 9</xref>, we see that this is because many single-trial drift rates (but not start points) of the STLBA are equal to the ‘all-trial estimate’. This is a consequence of LBA’s assumption that drift rates are drawn from a normal distribution whereas start points are drawn from a uniform distribution, which induces a ‘ridge’ in the joint density distribution on which one drift rate value (namely the mean of the normal distribution) together with many start points are the maximum-likelihood estimates (L. van Maanen; personal communication, February 2018). Overall, LOTO appears to provide comparatively good, and in fact mostly superior, results when compared to alternative approaches for capturing trial-by-trial variability in cognitive model parameters.</p><p>The previous comparison does not include arguably the most powerful approach for linking cognitive models with neural data, which is the joint modeling approach (<xref ref-type="bibr" rid="bib44">Turner et al., 2015</xref>). This method employs hierarchical Bayesian modeling (<xref ref-type="bibr" rid="bib8">Farrell and Lewandowsky, 2018</xref>; <xref ref-type="bibr" rid="bib21">Lee and Wagenmakers, 2013</xref>) to draw both behavioral and neural parameters from a joint, higher-order probability distribution and to fit models to both behavioral and neural data. To compare LOTO to the joint modeling approach, we first implemented Bayesian modeling to obtain single-trial estimates of parameter κ of the HD model (details are provided in the 'Materials and methods'). Then, we extended this to the joint modeling approach by simulating neural data <italic>F</italic>(κ)<italic><sub>t</sub></italic>, which we assumed to correlate with κ<italic><sub>t</sub></italic>, and included these data when estimating the model. Thus, the following comparison comprises three methods: LOTO, Bayesian modeling (without joint modeling), and the joint modeling approach.</p><p>We compared the three methods with respect to the correlation between the true values of κ<italic><sub>t</sub></italic> and the methods’ estimates (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1A</xref>) and with respect to the correlation between the neural data <italic>F</italic>(κ)<italic><sub>t</sub></italic> and the methods’ estimates (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1B</xref>). First of all, all three methods provided significant relationships with the true model parameters and with the neural data (all <italic>p </italic>&lt; .001, all BF<sub>10</sub> &gt; 1,000). Expectedly, the correlations illustrate that the Bayesian modeling and the joint modeling approach yielded similar estimates. Averaged over the simulated participants, however, the joint modeling approach outperformed (non-joint) Bayesian modeling with respect to both κ<italic><sub>t</sub></italic> (<italic>t</italic>(29) = 3.61, <italic>p </italic>= .001, <italic>d</italic> = 0.66, BF<sub>10</sub> = 21.96) and <italic>F</italic>(κ)<italic><sub>t</sub></italic> (<italic>t</italic>(29) = 4.32, <italic>p </italic>&lt; .001, <italic>d</italic> = 0.79, BF<sub>10</sub> = 131.84). Remarkably, LOTO outperformed Bayesian modeling (<italic>t</italic>(29) = 2.82, <italic>p </italic>= .009, <italic>d</italic> = 0.51, BF<sub>10</sub> = 6.07) as well as the joint modeling approach (<italic>t</italic>(29) = 2.51, <italic>p </italic>= .018, <italic>d</italic> = 0.46, BF<sub>10</sub> = 3.33) with respect to κ<italic><sub>t</sub></italic>. With respect to the neural signal <italic>F</italic>(κ)<italic><sub>t</sub></italic>, however, LOTO yielded similar correlations compared to Bayesian modeling (<italic>t</italic>(29) = −0.62, <italic>p </italic>= .540, <italic>d</italic> = −0.11, BF<sub>10</sub> = 0.26) and was outperformed by the joint modeling approach (<italic>t</italic>(29) = −3.03, <italic>p </italic>= .005, <italic>d</italic> = −0.55, BF<sub>10</sub> = 13.78). Overall, the joint modeling approach did indeed provide the best resultsin terms of linking the single-trial neural data to the cognitive model, but the differences between the methods appear to be comparatively small. Taking flexibility and difficulty of implementation into account, LOTO appears to offer an expedient alternative to joint modeling (see also 'Discussion').</p></sec><sec id="s2-10"><title>X. A power analysis for linking LOTO estimates to neural data</title><p>The ultimate goal of LOTO (and related approaches) is to establish a link between the inferred trial-by-trial variability in parameters of cognitive models and the brain activity measured with tools such as fMRI or EEG. Therefore, the question of whether LOTO is an appropriate tool for a given experiment with a specific design (including task, number of trials, number of participants etc.), cognitive model, and parameter(s) of interest should be answered by testing whether LOTO can be expected to recover the (truly existing) correlation between a neural signal and a parameter. In the case of modeling variability in more than one parameter, it is also important to test whether the potential misattribution of variability by LOTO may lead to incorrect inferences regarding the brain–parameter relationship (see 'Results: Sections V and VI'). We emphasize here that these requirements are not unique to LOTO. Any approach for investigating the neural correlates of trial-by-trial parameter variability should be accompanied by such analyses of statistical power (i.e., sensitivity) and false-positive rates (i.e., specificity).</p><p>To link LOTO estimates to neural data, we reverted to the HD-LBA model for which we have inferred variability in two parameters (i.e., discount factor κ and decision threshold <italic>b</italic>) in 'Results: Section VI'. When simulating trial-specific parameter values κ<italic><sub>t</sub></italic> and <italic>b<sub>t</sub>,</italic> as well as choices and RT, we also generated two hypothetical neural signals <italic>F</italic>(κ)<italic><sub>t</sub></italic> and <italic>F</italic>(<italic>b</italic>)<italic><sub>t</sub></italic> that were positively correlated with κ<italic><sub>t</sub></italic> and <italic>b<sub>t</sub></italic>, respectively (for details, see 'Materials and methods'). Then, we generated 1,000 datasets of 30 participants with 160 trials each and tested how often the LOTO estimate for a parameter (e.g., κ) correctly explains a significant amount of variability in the associated neural signal (e.g., <italic>F</italic>[κ]), and how often it incorrectly explains a significant amount of variability in the neural signal associated with the other parameter (e.g., <italic>F</italic>[<italic>b</italic>]) over and above that explained by the observations. This was realized by a random-effects linear regression analysis – similar to the General Linear Model approach in fMRI analysis (<xref ref-type="bibr" rid="bib12">Friston et al., 1994</xref>). Stated differently, we tested i) the sensitivity of LOTO by means of a power analysis, asking how likely it is that LOTO identifies the correct neural signal, and ii) the specificity of LOTO by means of a false-positive analysis, asking how likely it is that LOTO misattributes a neural signal to a different parameter.</p><p><xref ref-type="fig" rid="fig10">Figure 10A</xref> depicts the histograms of <italic>p</italic>-values for tests of whether LOTO’s estimates for κ and <italic>b</italic> are significantly related to the brain signals <italic>F</italic>(κ)<italic><sub>t</sub></italic> (left panel) and <italic>F</italic>(<italic>b</italic>)<italic><sub>t</sub></italic> (right panel), using the setting with 30 participants. According to these results, LOTO achieves a statistical power (or sensitivity) of 67% for parameter κ and of 76% for parameter <italic>b</italic>, values that are slightly below the threshold of 80% that is usually deemed acceptable. On the other hand, the false-positive rate (or specificity) of LOTO is good, with values lying just slightly above the 5% error rate. This means that the small misattribution of parameter variability seen in 'Results: Section VI' does not carry over much to a misattribution of the neural signal. In <xref ref-type="fig" rid="fig10">Figure 10B</xref>, we repeated the analysis for a sample size that is increased by two-thirds (i.e., 50 instead of 30 simulated participants). Expectedly, the statistical power of LOTO improves and lies within acceptable ranges. An alternative solution is to increase the number of trials per participant. <xref ref-type="fig" rid="fig10">Figure 10C</xref> depicts the results when the number of participants is kept to 30 but the number of trials is increased by two-thirds (i.e., 267 instead of 160 trials). The improvement in statistical power is comparable to that resulting from the increased sample size. Accordingly, one would conclude from these analyses that investigating the neural basis of trial-by-trial variability in parameters κ and <italic>b</italic> of the HD-LBA model might require testing more than 30 participants or testing more than 160 trials per participant, whatever is more suitable in a given context (e.g., adding trials might be difficult for event-related fMRI designs that require long inter-trial intervals but less problematic for EEG designs).</p><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.016</object-id><label>Figure 10.</label><caption><title>Power analysis of LOTO for detecting brain–parameter correlations.</title><p>(<bold>A</bold>) Histogram of the <italic>p</italic>-values from tests of the significant relationships between κ and <italic>b</italic> and brain signals <italic>F</italic>(κ) (left panel) and <italic>F</italic>(<italic>b</italic>) (right panel). (<bold>B</bold>) The same as in panel <bold>A</bold> but for 50 instead of 30 participants per simulated experiment. (<bold>C</bold>) The same as in panel <bold>A</bold> but for 267 instead of 160 trials per participant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig10-v3.tif"/></fig></sec><sec id="s2-11"><title>XI. An example of using LOTO for model-based fMRI</title><p>In this section, we apply LOTO to a real neuroimaging dataset to further illustrate the method’s use and usability in model-based cognitive neuroscience. The dataset is taken from a previously published fMRI study (<xref ref-type="bibr" rid="bib16">Gluth et al., 2015</xref>), in which participants were asked to make preferential choices from memory. More specifically, participants first encoded associations between food snacks and screen positions and then decided between pairs of food snacks while seeing only the snacks’ screen positions (<xref ref-type="fig" rid="fig11">Figure 11A</xref>). Participants had either one or five seconds in which to retrieve the item–location associations and to deliberate upon the preferred snack option.</p><fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.017</object-id><label>Figure 11.</label><caption><title>The remember-and-decide task (<xref ref-type="bibr" rid="bib16">Gluth et al., 2015</xref>).</title><p>This figure shows one example trial from the encoding and decision-making phases of the task. During an encoding trial, participants learned the association between one out of six different snacks with a specific screen position. During a decision-making trial, two screen positions were highlighted, and participants had to retrieve the two associated snacks from memory in order to choose their preferred option. Here, we apply LOTO to a decision-making model in order to infer memory-related activity during encoding. For a more detailed illustration of the study design, see Figure 1 in <xref ref-type="bibr" rid="bib16">Gluth et al. (2015)</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig11-v3.tif"/></fig><p>Choice behavior was modeled using an unbounded sequential sampling model that predicted more accurate decisions (i.e., decisions that are more consistent with evaluations of the food snacks in a separate task) with more deliberation time (details on the model and the fMRI analyses are provided in 'Materials and methods'; see also Figure 3 in <xref ref-type="bibr" rid="bib16">Gluth et al. (2015)</xref>). Importantly, the model estimates the average memory performance of a participant, that is, the probability of accurately remembering a snack when making decisions. In the original study (<xref ref-type="bibr" rid="bib16">Gluth et al., 2015</xref>), we employed our previously proposed GR2017 method for estimating trial-by-trial variability in order to infer the likelihood of successful memory retrieval for each snack given the choice behavior (intuitively, choosing unattractive snacks is attributed to poor memory performance). Here, we repeat this analysis but use LOTO to make the inference. Notably, LOTO takes advantage of the fact that each snack is a choice option in multiple (i.e., three) choice trials (see 'Results: Section II'). Ultimately, LOTO’s item-specific estimate of memory performance is applied to the fMRI analysis of the encoding phase. In other words, a model-based analysis of the well-established <italic>difference in subsequent memory effect</italic> (<xref ref-type="bibr" rid="bib30">Paller and Wagner, 2002</xref>) is conducted with the prediction that variations in memory performance should be positively correlated with activation of the hippocampus (as has been found in our original publication).</p><p>Before using LOTO to inform the fMRI analysis, we checked whether the variability estimates could be linked to other behavioral indicators of memory performance and whether the assumption of systematic variability in memory could be justified. To address the first question, we conducted a logistic regression testing whether the LOTO-based estimate predicted correct responses in the cued recall phase (i.e., a phase following the decision-making phase in which participants were asked to recall each snack; see 'Materials and methods' and <xref ref-type="bibr" rid="bib16">Gluth et al., 2015</xref>). As additional independent variables, this regression analysis also included whether the snack was encoded once or twice, how often a snack was chosen, and the subjective value of the snack. As can be seen in <xref ref-type="fig" rid="fig12">Figure 12A</xref>, the LOTO-based estimate of memory explained variance in cued recall performance over and above the other predictor variables (<italic>t</italic>(29) = 7.44, <italic>p </italic>&lt; .001, <italic>d</italic> = 1.36, BF<sub>10</sub> &gt; 1,000). To address the question of systematic variability, we followed the procedures described in 'Results: Section V' and generated 1,000 datasets under the null hypothesis of no variability in memory performance. We then compared the LOTO-based improvement in model fit in these datasets with the improvement in the real data. Indeed, the improvement in the real data was much larger than in any of the datasets generated under the null hypothesis (<xref ref-type="fig" rid="fig12">Figure 12B</xref>). Furthermore, we checked whether potential variability in parameter σ, which represents the standard deviation of the drift rate (i.e., noise in the decision process), could have been misattributed to the memory-related parameters. (It seems less plausible to assume that variability in the decision process, in contrast to variability in memory, is item-specific. Nevertheless, we included this control analysis for instructional purposes.) Thereto, we repeated the test for the presence of systematic variability with simulated data that included different levels of true variability in σ (for details, see 'Materials and methods'). As can be seen in <xref ref-type="fig" rid="fig12s1">Figure 12—figure supplement 1</xref>, LOTO does not appear to misattribute variability in σ to the memory-related parameters.</p><fig-group><fig id="fig12" position="float"><object-id pub-id-type="doi">10.7554/eLife.42607.018</object-id><label>Figure 12.</label><caption><title>Results of the example LOTO application.</title><p>(<bold>A</bold>) Average standardized regression coefficients for predicting memory performance in cued recall (1x/2x refers to how often an item was encoded). (<bold>B</bold>) Testing for systematic variability in memory; the green line shows the average trial-wise improvement of fit for the real dataset. The vertical dashed line represents the significance threshold based on the simulated data under the null hypothesis of no variability (gray histogram). (<bold>C</bold>) fMRI signals linked to the LOTO-based estimate of item-specific memory; the red outline depicts the anatomical mask of the hippocampus. (<bold>D</bold>) ROI analysis with anatomical masks of left and right hippocampus (HC) and average fMRI signals in these ROIs (the analyses in panels <bold>C</bold> and <bold>D</bold> are analogous to those in Figure 5 of <xref ref-type="bibr" rid="bib16">Gluth et al., 2015</xref>). Individual values (gray dots) are shown together with 95% confidence intervals of the mean (black error bars).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig12-v3.tif"/></fig><fig id="fig12s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.42607.019</object-id><label>Figure 12—figure supplement 1.</label><caption><title>Testing for potential misattribution of variability in ‘decision noise’ parameter σ.</title><p>The analysis shown in <xref ref-type="fig" rid="fig12">Figure 12B</xref> was repeated for simulated data with true trial-by-trial variability in parameter σ (vertical lines in red to yellow colors). In contrast to the real data (green vertical line), the improvements of deviance for these simulations did not exceed those generated under the null hypothesis of no variability (gray histogram), suggesting that any potential variability in parameter σ is unlikely to be misattributed to the memory-related parameters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42607-fig12-figsupp1-v3.tif"/></fig></fig-group><p>Having established that LOTO has the potential to identify meaningful neural correlates of item-specific memory performance, we turned to the fMRI analysis. Here, we employed a frequent approach to model-based fMRI (<xref ref-type="bibr" rid="bib28">O'Doherty et al., 2007</xref>), which is to add the predictions of a cognitive model as a so-called parametric modulator (PM) of the hemodynamic response during a time window of interest. In our case, this time window is the period during which snack food items are presented during encoding, with the LOTO-based variability estimates serving as the PM (in order to test for a model-based subsequent memory effect). As can be seen in <xref ref-type="fig" rid="fig12">Figure 12C</xref>, this analysis revealed fMRI signals in bilateral hippocampus (MNI coordinates of peak voxel in left hippocampus: x = −18, y = −10, z = −12; right hippocampus: 38,–12, −26) that surpassed the statistical threshold corrected for multiple comparisons (left: <italic>Z</italic> = 3.73; <italic>p </italic>= .033; right: <italic>Z</italic> = 4.46, <italic>p </italic>= .002). These effects are in line with those reported in the original publication, which were based on the GR2017 method and also showed a significant relationship between item-specific memory strength and hippocampal activation. (Note that, as in the original publication, we ruled out alternative explanations for these results by including cued recall performance and subject values of snacks as additional PMs in the analysis; see 'Materials and methods'.)</p><p>To illustrate a second analysis approach, we also conducted a region-of-interest (ROI) analysis by extracting the average fMRI signal from anatomically defined masks of the left and right hippocampi (<xref ref-type="fig" rid="fig12">Figure 12D</xref>). The average signals in these ROIs were significantly related to the LOTO-based PM (left: <italic>t</italic>(29) = 2.93, <italic>p </italic>= .007, <italic>d</italic> = 0.53, BF<sub>10</sub> = 6.42; right: <italic>t</italic>(29) = 4.36, <italic>p </italic>&lt; .001, <italic>d</italic> = 0.80, BF<sub>10</sub> = 182.52). Taken together, these findings demonstrate that we were able to reproduce the analysis of a model-based subsequent memory effect reported by <xref ref-type="bibr" rid="bib16">Gluth et al. (2015)</xref> using LOTO.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we introduce LOTO, a simple yet efficient technique to capture trial-by-trial measures of parameters of cognitive models so that these measures can be related to neural or physiological data. The core principle is to infer single-trial values from the difference in the parameter estimates when fitting a model with all trials vs. a model with one trial being left out. The application of LOTO is simple: first, the full set of parameters of a model is estimated for all <italic>n</italic> trials; then, the parameters of interest are estimated again <italic>n</italic> times with each trial being left out once, while the remaining parameters are fixed to their all-trial estimates. In addition to its simplicity, LOTO is also comparatively fast: if fitting a model once to all trials takes <italic>u</italic> time units, then LOTO’s runtime will be <italic>u</italic> × <italic>n</italic> at most (in fact, it can be expected to be even faster because not all parameters are estimated again). And in contrast to the GR2017 approach, the LOTO runtime will not increase (much) if variability in more than one parameter is estimated. Compared to related methods, LOTO appears to provide mostly comparable or even superior results. Finally, it is a very general approach for capturing variability in parameters. In principle, it can be applied to any cognitive model, although we stress the importance of testing the feasibility of LOTO for each model and study design anew.</p><sec id="s3-1"><title>The LOTO recipe</title><p>In this section, we provide a ‘LOTO recipe’ that summarizes the critical steps, tests and checks that one should follow when seeking to apply the method. We link each point to the relevant section(s) in the 'Results'. Thus, despite our recommendation that readers with limited statistical and mathematical knowledge should first consult this recipe, it is indispensable for interested cognitive neuroscientists to read the relevant sections before trying to apply LOTO to their own data. Notably, Steps 1 and 2 of this recipe are to be performed before conducting the experiment. Therefore, LOTO is not suitable for post-hoc analyses of data that have already been acquired with different study goals in mind. Also note that all of the computer codes associated with this article and with LOTO are freely available on the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/du85f/">https://osf.io/du85f/</ext-link>).</p><list list-type="order"><list-item><p>Specify the cognitive model and the parameters for which trial-by-trial variability should be estimated; similarly, specify the design of the study (number of trials and participants, experimental manipulations, neuroscientific method).</p><list list-type="alpha-lower"><list-item><p>What type of observations does the model explain? If it explains only choices, try to extend the model to allow the prediction of RT ('Results: Section IV').</p></list-item><list-item><p>How are the experimental manipulations or conditions related to the model (i.e., which manipulation is predicted to selectively influence a given parameter)? If the model has to be fit to each condition separately, try to adjust the model so that its predictions change directly as a function of the condition ('Results: Section III').</p></list-item><list-item><p>Should variability be captured in each trial or across multiple trials (e.g., small blocks of trials)? If simulations of LOTO (see Step 2 below) do not yield acceptable results, consider pooling multiple trials into small blocks and capturing parameter variability from block to block ('Results: Section II').</p></list-item><list-item><p>Does the neuroscientific method allow the measurement of signals that can be related to trial-by-trial variability? Block-designs for fMRI and ERPs for EEG are less suitable than event-related designs for fMRI and time-frequency analyses for EEG.</p></list-item></list></list-item><list-item><p>Test the feasibility of the LOTO approach for the chosen model and study design.</p><list list-type="alpha-lower"><list-item><p>Test whether LOTO can (in principle) provide sensible estimates of the true trial-specific values of the targeted model parameter(s). Specification of the first derivative of the log-likelihood and the Fisher information (as described in 'Results: Sections II and III') is desirable but often cumbersome or even impossible. Instead, we suggest running simulations ('Results: Sections II–IV').</p></list-item><list-item><p>Check whether LOTO might misattribute random noise by conducting the non-parametric test for the presence of systematic variability ('Results: Section V'). Similarly, check whether LOTO might misattribute variability in other parameters to the parameters of interest by conducting the regression analyses outlined in 'Results: Section VI'.</p></list-item><list-item><p>Estimate the statistical power and if applicable the specificity of LOTO given the model and study design ('Results: Section X'). This might require running a pilot study to obtain a rough estimate of the amount of trial-by-trial variability in model parameters that can be expected (alternatively, existing datasets could be inspected for this purpose).</p></list-item><list-item><p>For experiments with many trials per participant (i.e.,≥1,000), check whether the default tolerance criterion of the parameter estimation method is sufficient or whether there are discrete ‘jumps’ in the LOTO estimates. If necessary, choose a stricter criterion ('Results: Section VII').</p></list-item><list-item><p>Always use the actually planned number of trials, number of participants and manipulations when doing simulations. Always include the observations that are used to estimate parameters (e.g., choices and RT) when testing the potential of LOTO to recover parameter variability. Applying LOTO is meaningless when the method does not yield any additional information over and above that provided by the observations themselves ('Results: Section II').</p></list-item><list-item><p>If the simulations (see Steps 2a,b) do not yield acceptable results, consider increasing the number of participants or the number of trials per participant ('Results: Section X'), or consider adapting the model or experimental paradigm ('Results: Sections II–IV').</p></list-item></list></list-item><list-item><p>After conducting the study, apply LOTO ('Results: Section I'), conduct the non-parametric test for the presence of systematic variability ('Results: Section V'), and use LOTO’s estimate for analyzing the recorded neural data (e.g., 'Results: Section XI').</p></list-item></list><p>Of note, the test for the presence of variability ('Results: Section V'; Steps 2b and 3) and the power analyses ('Results: Section X'; Step 2c) are very time-consuming because they require the simulation of a large number (e.g., 1,000) of entire experiments and the application of LOTO to the resulting data. For example, for the first power analysis reported in 'Results: Section X', we ran 1,000 simulations of 30 participants with 160 trials each, meaning that the HD-LBA model had to be estimated ~5 million times. Therefore, it may not be possible to conduct an extensive search for optimal sample size and trial number to meet a desired statistical power such as 80%. Instead, trying out a limited set of realistic scenarios appears most appropriate. Note, however, that other approaches such as GR17 or hierarchical Bayesian modeling can be much more time-consuming than LOTO, so that even the simplest simulation-based power analyses would become infeasible using these approaches.</p></sec><sec id="s3-2"><title>The principle of leaving one out</title><p>The idea of leaving one data point out, also termed the ‘jackknife’ principle, is not new. In fact, the approach was proposed by statisticians in the 1950s (<xref ref-type="bibr" rid="bib36">Quenouille, 1956</xref>; <xref ref-type="bibr" rid="bib42">Tukey, 1958</xref>) as a way to estimate the variance and bias of estimators when dealing with small samples. From a technical point of view, LOTO bears obvious similarities to statistical methods such as (Bayesian) leave-one-out cross-validation, which is employed in classification-based analyses (<xref ref-type="bibr" rid="bib3">Bishop, 2006</xref>), Bayesian model evaluation (<xref ref-type="bibr" rid="bib49">Vehtari et al., 2017</xref>), or the leave-one-subject-out approach, which is used to circumvent the non-independence error in statistical analyses of fMRI data (<xref ref-type="bibr" rid="bib7">Esterman et al., 2010</xref>). Notably, the jackknife principle has also been adopted for inferring trial-specific onset latencies of ERPs (<xref ref-type="bibr" rid="bib26">Miller et al., 1998</xref>; <xref ref-type="bibr" rid="bib41">Stahl and Gibbons, 2004</xref>) and for quantifying trial-specific functional connectivity between different brain regions (<xref ref-type="bibr" rid="bib38">Richter et al., 2015</xref>). Thus, a combination of LOTO for inferring variability in cognitive model parameters with the same principle used for identifying trial-specific brain activation patterns could be a promising approach for future research in model-based cognitive neuroscience (e.g., <xref ref-type="bibr" rid="bib2">Benwell et al., 2018</xref>).</p></sec><sec id="s3-3"><title>LOTO as a two-stage approach to linking behavioral and neural data</title><p>As mentioned in the introduction, LOTO is one of several approaches that have been put forward to link computational models of cognition and behavior with neural data. <xref ref-type="bibr" rid="bib45">Turner et al. (2017)</xref> proposed a taxonomy of these approaches based on whether a method: i) uses neural data to constrain the (analysis of the) cognitive model, ii) uses the cognitive model to constrain the (analysis of the) neural data, or iii) uses both types of data to constrain each other in a reciprocal manner. Within this taxonomy, LOTO is best classified as belonging to the second class, that is, it infers trial-wise values of the cognitive model to then inform the analysis of neural data. In other words, LOTO is a ‘two-stage approach’ to model-based cognitive neuroscience, in which the modeling of behavior precedes the analysis of brain data (as exemplified in 'Results: Section XI'). This approach misses out on the advantage of reciprocity exhibited by the joint modeling approach (<xref ref-type="bibr" rid="bib44">Turner et al., 2015</xref>). However, LOTO’s strength lies in its ease, speed and generality of application. Moreover, it retains the flexibility of the two-stage approach with respect to identifying the neural correlates of trial-by-trial parameters: it does not require an a priori hypothesis of which brain region or EEG component might be related to variability in a model parameter. Instead, the variability estimate captured by LOTO can simply be plugged into, for instance, a GLM-based fMRI whole-brain analysis (<xref ref-type="bibr" rid="bib18">Gluth and Rieskamp, 2017</xref>; <xref ref-type="bibr" rid="bib28">O'Doherty et al., 2007</xref>; see also <xref ref-type="fig" rid="fig12">Figure 12C</xref>). Notably, LOTO also retains the flexibility of testing different functional relationships between brain and behavior (e.g., linear, quadratic, exponential) and thus lends itself to exploring different linking functions for subsequent, confirmatory studies.</p></sec><sec id="s3-4"><title>Concluding remarks</title><p>Today, neuroimaging and psychophysiological techniques such as fMRI, EEG and eye-tracking allow us to shed light on the process of single actions. However, the development of cognitive models has only partially kept up with these advances because the central tenet of cognitive modeling is still the identification of invariant features of behavior by averaging over repeated observations. LOTO has the potential to fill this gap, in particular because it will not be restricted to a handful of specific models or to a handful of experts in the field who are highly proficient in computational modeling.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th valign="top">Reagent type <break/>(species) or resource</th><th valign="top">Designation</th><th valign="top">Source or reference</th><th valign="top">Identifiers</th></tr></thead><tbody><tr><td valign="top">Software</td><td valign="top">MATLAB</td><td valign="top">MathWorks</td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001622">SCR_001622</ext-link></td></tr><tr><td valign="top">Software</td><td valign="top">R Project for Statistical Computing</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/">https://cran.r-project.org/</ext-link></td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001905">SCR_001905</ext-link></td></tr><tr><td valign="top">Software</td><td valign="top">Just Another Gibbs Sampler (JAGS)</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="http://mcmc-jags.sourceforge.net/">http://mcmc-jags.sourceforge.net/</ext-link></td><td valign="top">-</td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Simulations of Bernoulli and binomial distributions</title><p>We drew <italic>n</italic> = 300 values of <italic>θ<sub>t</sub></italic> from a uniform distribution over a range of .2 (e.g., [.4, .6]), and then generated one Bernoulli event per trial <italic>t</italic> using <italic>θ<sub>t</sub></italic> as the underlying probability. LOTO was then used to recover <italic>θ<sub>t</sub></italic> by estimating and subtracting <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mrow><mml:mo>¬</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, for every <italic>t</italic> according to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. This was done for every range from [0, .2] to [.8, 1] in steps of .01 (i.e., 80 times) and repeated 1,000 times. Pearson product-moment correlations between <italic>θ<sub>t</sub></italic> and LOTO<italic><sub>t</sub></italic> were calculated and averaged over the 1,000 repetitions.</p><p>In a second step, we generalized to the binomial distribution by assuming that <italic>θ</italic> remains stable over a limited number of trials <italic>m</italic> (<italic>m</italic> &lt; <italic>n</italic>). The goal was then to recover <italic>θ<sub>m</sub></italic> with LOTO. We chose four different levels of <italic>m</italic> (i.e., <italic>L</italic>(<italic>m</italic>) = {1, 5, 10, 20}), but kept <italic>n</italic> constant (i.e., <italic>n</italic> = 300). Thus, the number of different values of <italic>θ<sub>m</sub></italic> decreased by <italic>n/m</italic> as <italic>m</italic> increased. Again, simulations were run for every range from [0, .2] to [.8, 1] in steps of .01 and repeated 1,000 times, before LOTO was applied (<xref ref-type="disp-formula" rid="equ5">Equation 5a</xref>) and correlations between θ<italic><sub>m</sub></italic> and LOTO<italic><sub>m</sub></italic> were calculated and averaged over the 1,000 repetitions (for each level of <italic>m</italic> separately).</p><p>For 'Results: Section IX', which discusses the shape of the distribution of LOTO estimates, we conducted two simulations of <italic>n</italic> = 10,000 trials each, sampling 500 values of <italic>θ<sub>m</sub></italic> for blocks of 20 trials. The 500 values were drawn from a normal distribution (with mean = 0.5 and SD = 0.05) in the first simulation and from a uniform distribution (in the range [.4, .6]) in the second simulation. We then generated the corresponding Bernoulli events and applied LOTO.</p></sec><sec id="s4-2"><title>Simulations of intertemporal choice sets and the HD model</title><p>For the sake of plausibility, choice sets for the intertemporal choice task were created in rough correspondence to typical settings in fMRI studies (<xref ref-type="bibr" rid="bib31">Peters et al., 2012</xref>). We chose to simulate 160 trials per ‘participant’, which would result in a realistic total duration of 40 min of an fMRI experiment, in which one trial lasts ~15 s (e.g., <xref ref-type="bibr" rid="bib32">Peters and Büchel, 2009</xref>). As is often the case in fMRI studies on intertemporal choice, the immediate choice option was identical in all trials (i.e., <italic>d<sub>i</sub></italic> = 0; <italic>x<sub>i</sub></italic> = 20), but the delay and the number of the delayed choice options varied from trial to trial. In a first step, delays <italic>d<sub>j</sub></italic> were drawn uniformly from the set <italic>S</italic>(<italic>d<sub>j</sub></italic>) = {1, 7, 14, 30, 90, 180, 270, 360}, and amounts <italic>x<sub>j</sub></italic> were drawn from a uniform distribution with limits [20.5; 80], discretized to steps of 0.5. In a second step, this procedure was repeated until a choice set was found for which the utility of the immediate option <italic>u<sub>i</sub></italic> was higher than the utility of the delayed option <italic>u<sub>j</sub></italic> in ~50% of the trials, given the simulated participant’s baseline parameter κ (see below). Specifically, a choice set was accepted when <italic>u<sub>i</sub> – u<sub>j</sub></italic> &gt; 0 in more than 45% but less than 55% of trials. Otherwise, a new choice set was created. This was repeated up to 1,000 times. If no choice set could be found, the criterion was relaxed by 10% (i.e., a choice set was now accepted when <italic>u<sub>i</sub> – u<sub>j</sub></italic> &gt;0 in more than 40% but less than 60% of trials). This procedure was repeated until an appropriate choice set was found. Note that using such adaptive designs is very common and recommended for intertemporal choice tasks in order to avoid having participants who choose only immediate or only delayed options (<xref ref-type="bibr" rid="bib17">Gluth et al., 2017</xref>; <xref ref-type="bibr" rid="bib20">Koffarnus et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Peters et al., 2012</xref>). In practice, adaptive designs require a pre-test to infer the individual discount rate κ.</p><p>Simulations of the HD model were also based on empirical data (<xref ref-type="bibr" rid="bib31">Peters et al., 2012</xref>). For each simulated participant, we first determined ‘baseline’ parameters κ and <italic>β</italic> (for the sake of simplicity, we omit the indicator variable for participants here and for all following notations) by drawing from log-normal distributions truncated to ranges [0.001, 0.07] and [0.01, 10], respectively. Means (µ) and standard deviations (σ) of these log-normal distributions were {µ<sub>κ</sub> = –4.8, σ<sub>κ</sub> = 1} and {µ<sub>κ</sub> = –0.77, σ<sub>κ</sub> = 0.71}, respectively, and were chosen so that the medians and interquartile ranges of the generated values would match those reported by <xref ref-type="bibr" rid="bib31">Peters et al. (2012)</xref>. Log-normal distributions were used to account for the fact that distributions of empirical discount rates and sensitivity parameters are often right-skewed. If trial-by-trial variability in κ and <italic>β</italic> was assumed, then this was realized by drawing trial-specific values of κ<italic><sub>t</sub></italic> and <italic>β<sub>t</sub></italic> from a normal distribution (truncated to values ≥ 0) with means κ and <italic>β</italic>, and standard deviations 0.01 and 1, respectively. Otherwise, we assumed that κ<italic><sub>t</sub></italic> = κ, <italic>β<sub>t</sub></italic> = <italic>β</italic>.</p><p>After specifying choice sets and trial-specific parameter values, choices were simulated and the parameters of the HD model were estimated for all trials. This was done in a two-step procedure starting with a grid search (with 25 different values per parameter) to find suitable starting values for the parameters, which were then used in a constrained simplex minimization. Then, LOTO was applied by leaving each trial out once and estimating the parameter(s) of interest (i.e., either κ<italic><sub>t</sub></italic> or <italic>β<sub>t</sub></italic> or both), again using the ‘all-trials’ parameter estimates as starting values. Similarly, single-trial fitting was conducted whenever necessary by fitting each trial individually, also using the ‘all-trials’ estimates as starting values.</p><p>Unless stated otherwise, this procedure was conducted for 30 participants and repeated multiple times whenever necessary. The example participant shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> was taken from such a set of simulations. This participant had baseline parameters κ = 0.0076 and <italic>β</italic> = 2.11, but only κ<italic><sub>t</sub></italic> varied from trial to trial. When testing for the presence of systematic trial-by-trial variability in 'Results: Section V', we generated 1,000 simulations of 30 participants under the null hypothesis and 10 simulations of 30 participants assuming 10, 20, 30, 40, 50, 60, 70, 80, 90, 100% variability in κ or <italic>β</italic> compared to the amount of variability used in our initial simulations (see above). When testing for potential misattribution of variability in amounts <italic>x<sub>j</sub></italic>, delays <italic>d<sub>j</sub></italic>, or utilities <italic>u<sub>j</sub></italic> of the delayed option, we simulated 10 different levels of variability (i.e., Gaussian noise) in these input or output variables. Importantly, we ensured comparability with effects of κ by adjusting the largest amount of Gaussian noise in <italic>x<sub>j</sub></italic> (i.e., SD<italic><sub>x</sub></italic> = 9), <italic>d<sub>j</sub></italic> (i.e., SD<italic><sub>d</sub></italic> = 10), or <italic>u<sub>j</sub></italic> (i.e., SD<italic><sub>u</sub></italic> = 6), so that the variance in the output variable <italic>u<sub>j</sub></italic> would be similar to that under the largest amount of Gaussian noise in κ (i.e., SD<sub>κ</sub>=0.01; see above). In the context of these simulations (see 'Results: Section V'), we also extended the intertemporal choice task to three options. For every trial of this task, three amounts and three delays were randomly drawn from the range of values used in the standard version of the task (see above) and were always combined so that one option had the smallest amount and delay, one option had the largest amount and delay, and one option had medium amount and delay. As for the standard version, trial generation was adapted to the sampled discount factor, so that choice probabilities would not be too extreme. Thereto, the choice probability of each option was calculated using the (generalized) logistic or soft-max choice function:<disp-formula id="equ15"><label>(10)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and amount and delays were sampled until choice probabilities were as similar as possible. (More specifically, the first set of options was accepted only if the highest average choice probability for one option was ≤ .34; for each subsequent set of options, this criterion was relaxed by an increase of .001 until a set of options was accepted.) <xref ref-type="disp-formula" rid="equ15">Equation 10</xref> was also used to generate simulated choices, to estimate parameters, and to apply LOTO. Note that a three-option design could induce specific choice phenomena, so-called ‘context effects’, which are incompatible with the logistic or soft-max choice rule (<xref ref-type="bibr" rid="bib5">Busemeyer et al., 2019</xref>; <xref ref-type="bibr" rid="bib17">Gluth et al., 2017</xref>). For the purposes of the present work, however, these effects can be neglected.</p></sec><sec id="s4-3"><title>Simulations of the HD-LBA model</title><p>Procedures for simulating the HD-LBA model were similar to those for the HD model. Like the HD model, the HD-LBA model includes parameter κ, and values for κ and κ<italic><sub>t</sub></italic> were generated in the same way as before. However, HD-LBA does not use the soft-max choice function (see <xref ref-type="disp-formula" rid="equ9">Equation 7</xref>) to specify choice probabilities but instead uses the LBA model (<xref ref-type="bibr" rid="bib4">Brown and Heathcote, 2008</xref>) to specify the joint choice and response time (RT) distributions. Thus, instead of including the soft-max choice sensitivity parameter β, the HD-LBA model includes parameter λ, which scales the relationship between utility and drift rate (see <xref ref-type="disp-formula" rid="equ13">Equations 9a and 9</xref>b). HD-LBA also includes four additional parameters from the LBA model: parameter <italic>s</italic>, which models (unsystematic) trial-by-trial variability in the drift rates (i.e., drift rates for options <italic>i</italic>, <italic>j</italic> are drawn from independent normal distributions with means <italic>ν<sub>i</sub></italic>, <italic>ν<sub>j</sub></italic> and standard deviation <italic>s</italic>); parameter <italic>b</italic>, which represents the decision threshold; parameter <italic>A</italic>, which represents the height of the uninform distribution of the start point; and parameter <italic>t</italic><sub>0</sub>, which accounts for the non-decision time of the RT. Most of these parameters were kept identical for all simulated participants (i.e., λ = 0.1, <italic>s</italic> = 0.3, <italic>A</italic> = 1, <italic>t</italic><sub>0</sub> = 0.5). Besides κ, only the decision threshold <italic>b</italic> varied across participants (because it was used to test how well LOTO captures trial-by-trial variability in two parameters simultaneously) by drawing individual values from a uniform distribution with limits [2, 3]. When testing LOTO on two parameters simultaneously, trial-specific values of κ<italic><sub>t</sub></italic> were drawn as stated above, and trial-specific values of <italic>b<sub>t</sub></italic> were drawn from a normal distribution with mean <italic>b</italic> and standard deviation 0.04 (with the constraint that <italic>b<sub>t</sub></italic> ≥ A).</p><p>Parameter estimation again started with an initial grid search. To reduce computation time, grid search was extensive (i.e., 15 grid search values) for parameters κ and <italic>b</italic> but limited (i.e., three grid search values) for the remaining parameters that were identical across simulated participants. As for the HD model, the best parameter estimates from the grid search were used as starting values for the constrained simplex minimization, followed by applying LOTO and single-trial fitting.</p><p>For 'Results: Section IX', which discusses the shape of the distribution of LOTO estimates, the HD-LBA model was simulated with a few modifications. First of all, the number of trials was increased to 480 to improve the interpretability of the shape of distributions. Second, the parameters of the log-normal distribution from which individual values of κ were drawn were changed to {µ<sub>κ</sub> = -3.5, σ<sub>κ</sub> = 0.2}, so that single-trial parameter values were unlikely to be extreme (i.e., κ<italic><sub>t</sub></italic> = 0)–which would have affected the shape of the distribution. Normally distributed single-trial values of κ<italic><sub>t</sub></italic> and <italic>b<sub>t</sub></italic> were drawn as specified above, whereas uniformly distributed values of κ<italic><sub>t</sub></italic> were drawn from a range [κ – <italic>K</italic>/2; κ + <italic>K</italic>/2] with <italic>K</italic> = 0.35, and uniformly distributed values of <italic>b<sub>t</sub></italic> were drawn from a range [<italic>b – B</italic>/2; <italic>b + B</italic>/2] with <italic>B</italic> = 1.4.</p></sec><sec id="s4-4"><title>Simulations of the LBA model</title><p>We compared four different methods for capturing trial-by-trial variability with respect to how well these methods recover the trial-wise drift rate and the start point values of the LBA model (<xref ref-type="bibr" rid="bib4">Brown and Heathcote, 2008</xref>). We chose these two LBA parameters because one of the tested methods, the single-trial LBA (STLBA) approach (<xref ref-type="bibr" rid="bib47">van Maanen et al., 2011</xref>), has been developed specifically for this set of parameters. The other three methods were LOTO, the ‘single-trial fitting’ method (see 'Results: Section III'), and an approach that we proposed in a previous publication (<xref ref-type="bibr" rid="bib18">Gluth and Rieskamp, 2017</xref>), ‘GR17’.</p><p>In total, we generated synthetic data from 20 experiments with 20 simulated participants each, and with 200 trials per participant. The decision threshold and non-decision time parameters were fixed across participants (i.e., <italic>b</italic> = 1.5; <italic>t</italic><sub>0</sub> = 0.2). The remaining three parameters of the LBA model were sampled individually for each participant: i) a value representing the average drift rate parameter ν was drawn from a normal distribution with mean μ<italic><sub>ν</sub></italic> = 0.6 and standard deviation σ<italic><sub>ν</sub></italic> = 0.02; ii) a value representing the standard deviation parameter <italic>s</italic> (for trial-by-trial variability of the drift rate) was drawn from a normal distribution with μ<italic><sub>s</sub></italic> = 0.2 and σ<italic><sub>s</sub></italic> = 0.02; and iii) a value representing the height of the start point distribution <italic>A</italic> was drawn from a normal distribution with μ<italic><sub>A</sub></italic> = 1 and σ<italic><sub>A</sub></italic> = 0.1 (with the constraint that <italic>A</italic> ≤ <italic>b</italic>). Trial-specific drift rates <italic>ν<sub>t</sub></italic> were drawn from a normal distribution with μ(<italic>ν<sub>t</sub></italic>) = <italic>ν</italic> and σ(<italic>ν<sub>t</sub></italic>) = <italic>s</italic>. Trial-specific start points <italic>A<sub>t</sub></italic> were drawn from a uniform distribution with limits [0, <italic>A</italic>]. Note that these values refer to the (trial-specific) drift rates and start points for correct responses (as, for instance, in a two-alternative forced-choice perceptual or inferential choice task). Similarly, trial-specific drift rates and start points for incorrect responses were drawn from distributions with the same specifications, except that the mean of the drift rate was 1–<italic>ν</italic>. We chose the above-mentioned values so that accuracy rates and RTs for simulated participants were kept in realistic ranges (i.e., mean accuracy rates and RTs per simulated participant ranged from 53 to 89% and from 1.5 to 2.1 s, respectively).</p><p>Parameter estimation procedures were very similar to those of the HD-LBA model. An initial grid search with an extensive search (i.e., 15 grid search values) for parameters <italic>ν</italic> and <italic>A</italic> and a limited search (i.e., three grid search values) for the remaining parameters was used to find good starting values for the constrained simplex minimization, followed by the application of the four methods for capturing trial-by-trial variability. The STLBA method was applied according to the equations provided by <xref ref-type="bibr" rid="bib47">van Maanen et al. (2011)</xref>. The GR17 method required the specification of a range of possible parameter values for <italic>ν</italic> and <italic>A</italic> as well as a step size with which this range of values was scanned (i.e., prior, likelihood and posterior values were calculated for each step within the range of possible values; for more details, see <xref ref-type="bibr" rid="bib18">Gluth and Rieskamp, 2017</xref>). Possible values for both parameters were restricted to be within three standard deviations below and above the parameters’ group means, with 1,000 steps per parameter. This implied 1 million calculations for both prior and likelihood distributions because the joint distributions of both parameters needed to be specified. (With these specifications, the GR17 method required about three times the computation time of the three other methods combined; this extensive computation time was the reason for reducing the number of participants in these simulations from 30 to 20.) To test the methods’ abilities to recover trial-specific drift rates and start points, we restricted the analysis to correct trials only (as is often the case in perceptual or inferential choice tasks).</p></sec><sec id="s4-5"><title>Bayesian modeling and joint modeling</title><p>For the second comparison of LOTO with related approaches in 'Results: Section IX', Bayesian modeling (<xref ref-type="bibr" rid="bib8">Farrell and Lewandowsky, 2018</xref>; <xref ref-type="bibr" rid="bib21">Lee and Wagenmakers, 2013</xref>) and the joint modeling approach (<xref ref-type="bibr" rid="bib29">Palestro et al., 2018</xref>; <xref ref-type="bibr" rid="bib44">Turner et al., 2015</xref>) were implemented for the HD model using the Just Another Gibbs Sampler (JAGS) software. Our specifications of the joint modeling approach closely followed the tutorial by <xref ref-type="bibr" rid="bib29">Palestro et al. (2018)</xref>: the ‘behavioral’ parameter δ and the ‘neural’ parameter θ were drawn from a joint, multivariate normal distribution with means μ<sub>δ</sub> and μ<sub>θ</sub> and precision matrix Ω. The means μ<sub>δ</sub> and μ<sub>θ</sub> were drawn from a multivariate normal distribution with means [0, 0] and precision matrix [1, 0; 0, 1], and Ω was drawn from a Wishart distribution with dispersion matrix [1, 0; 0, 1] and df = 2. The neural data were modeled as being sampled from a normal distribution with mean θ and standard deviation τ = 2 (the simulation of neural data is explained in the next paragraph). The behavioral data were sampled from the HD model. Within this behavioral model, the discount factor κ was the probit-transformed value of δ (because κ is restricted to values between 0 and 1, but δ is drawn from a normal distribution). To reduce the joint modeling approach to the (non-joint) Bayesian modeling, the sampling of neural data was simply omitted, so that parameter estimates were solely dependent on the behavioral data. For the sake of simplicity, we conducted parameter estimation within each simulated participant and did not draw individual parameters from higher-level group distributions. Sampling settings were as follows: five chains, 20,000 burn-in samples per chain, 2,000 recorded samples, thinning of 20 samples. Model convergence was checked by computing the Gelman-Rubin statistic <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib13">Gelman and Rubin, 1992</xref>), which was ensured to be below 1.01.</p></sec><sec id="s4-6"><title>Simulations of neural signals</title><p>To apply the joint modeling approach (<xref ref-type="bibr" rid="bib44">Turner et al., 2015</xref>) and to test whether LOTO can capture neural signals that systematically vary with variability in a cognitive model parameter, we simulated neural signals in a very basic form. We assumed that the neural signal <italic>F</italic>(π) in trial <italic>t</italic> (which could, for instance, represent the amplitude of the fMRI blood-oxygen-level-dependent signal in a circumscribed region of the brain or the average EEG spectral power in a specific frequency band during a circumscribed time period of the trial) that is systematically linked to variability in a parameter π is drawn from a normal distribution with mean π<italic><sub>t</sub></italic> and standard deviation σ<italic><sub>F</sub></italic><sub>(π)</sub> (see <xref ref-type="bibr" rid="bib29">Palestro et al., 2018</xref> for a similar assumption of normally distributed neural data in related work). The value for the standard deviation σ<italic><sub>F</sub></italic><sub>(π)</sub> was specified as follows: we simulated <italic>F</italic>(π)<italic><sub>t</sub></italic> together with π<italic><sub>t</sub></italic> and the observations (choices and RT) for the desired number of trials (160) and participants (30), and then conducted linear regressions with <italic>F</italic>(π)<italic><sub>t</sub></italic> as the dependent variable and with π<italic><sub>t</sub></italic> and the observations as independent variables for each simulated participant (independent variables were standardized). Then, we estimated the effect size of the regression coefficient of π<italic><sub>t</sub></italic> at the group level. The goal was to obtain an effect size of <italic>d</italic> = 1 by trying out different values for σ<italic><sub>F</sub></italic><sub>(π)</sub> (the choice of the effect size is justified in the next paragraph). This iterative procedure was stopped as soon as a value of σ<italic><sub>F</sub></italic><sub>(π)</sub> was found, for which the 95% confidence interval of the effect size overlapped with 1 when generating 1,000 simulations of the desired number of participants and trials (i.e., 20/30 participants with 200/160 trials in 'Results: Sections IX and X'). For parameter κ of the HD model, we obtained σ<italic><sub>F</sub></italic><sub>(κ) </sub>= 7.5. For parameter κ of the HD-LBA model, we obtained σ<italic><sub>F</sub></italic><sub>(κ) </sub>= 0.93; for parameter <italic>b</italic> of the HD-LBA model, we obtained σ<italic><sub>F</sub></italic><sub>(<italic>b</italic>) </sub>= 4.8. With respect to the HD-LBA model, this implies lower variability in the simulated neural signal for parameter κ than in the signal for parameter <italic>b</italic>, because the trial-by-trial variability in the former parameter is much lower than that in the latter (see above). This might be implausible from a neurophysiological point of view. However, this implausibility does not matter in the context of the current work because we are only interested in the correlation between <italic>F</italic>(π)<italic><sub>t</sub></italic> and π<italic><sub>t</sub></italic> (and the LOTO estimate associated with π<italic><sub>t</sub></italic>), which is unaffected by a linear transformation of <italic>F</italic>(π)<italic><sub>t</sub></italic>.</p><p>The rationale of this approach is that in an idealized world, in which we would know the true trial-specific values of π<italic><sub>t</sub></italic>, we could expect a very high effect size (such as <italic>d</italic> = 1) for detecting a relationship between π<italic><sub>t</sub></italic> and <italic>F</italic>(π)<italic><sub>t</sub></italic>. This effect size is slightly higher than, for instance, current estimates of realistic effect sizes for fMRI studies, which are based on large-scale empirical studies (<xref ref-type="bibr" rid="bib14">Geuter et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Poldrack et al., 2017</xref>). But again, the chosen effect size refers to a hypothetical world, in which the true trial-by-trial value of a cognitive function is known and then related to brain activity. For such a fortunate scenario, we can expect to obtain a slightly higher effect size than in reality.</p><p>We generated 1,000 datasets of 30 simulated participants with 160 trials for the HD-LBA model. For each of these datasets, we conducted random-effects linear regression analyses, that is, we ran two linear regression analyses with <italic>F</italic>(κ)<italic><sub>t</sub></italic>/<italic>F</italic>(b)<italic><sub>t</sub></italic> as the dependent variable and the LOTO estimates for π<italic><sub>t</sub></italic> and b<italic><sub>t</sub></italic> together with the observations (i.e., choices and RT) as independent variables in each simulated participant, and then conducted two-sided one-sample <italic>t</italic>-tests on the regression coefficients for π<italic><sub>t</sub></italic> and <italic>b<sub>t</sub></italic> at the group level. The proportion of significant effects for ‘correct’/‘wrong’ pairings of <italic>F</italic> and π determine the statistical power/false positive rate of LOTO.</p></sec><sec id="s4-7"><title>A note on the statistical analyses for simulations</title><p>The employment of frequentist statistical analyses for simulations is controversial because the results depend on the degrees of freedom of the tests, which can easily be adjusted to change the significance of a result. Our motivation in providing these statistical tests is that they offer an intuition of whether specific settings or features can be expected to be relevant when applying LOTO ‘in the field’. Therefore, we chose to conduct statistical tests with their degrees of freedom usually being in the range of common and realistic sample sizes in cognitive neuroscience (e.g., studies of 20 or 30 participants). To complement the frequentist statistics, we also report effect sizes (i.e., Cohen’s <italic>d</italic>, correlation coefficients) and the results of Bayesian hypothesis tests (i.e., the Bayes Factor BF<sub>10</sub> refers to evidence in favor of the alternative hypothesis). Bayesian hypothesis tests were conducted in R using the BayesFactor package.</p></sec><sec id="s4-8"><title>fMRI example: essential study information</title><p>Three groups with a total of 84 participants (<xref ref-type="bibr" rid="bib16">Gluth et al., 2015</xref>) took part in the study (one group of participants that underwent fMRI and two groups that conducted behavioral tasks only). The final sample of the fMRI group included 30 participants, and the analyses in the current article are restricted to this sample. All participants gave written informed consent and the study was approved by the Ärztekammer Hamburg, a local ethics committee in Hamburg, Germany (case # PV4290), where the study was conducted.</p><p>Participants were asked to not to eat for 3 hr prior to the experiment. They were familiarized with a set of 48 common food snacks from German supermarkets. To infer the subjective values of food snacks, participants conducted a Becker-DeGroot-Marschak (BDM) auction (<xref ref-type="bibr" rid="bib1">Becker et al., 1964</xref>), in which they stated their willingness to pay money for each snack. To incentivize accurate bids, one randomly selected bid trial was realized at the end of the experiment. Inside the MR scanner, participants conducted the remember-and-decide task, which consisted of multiple runs of encoding, distraction, decision-making and recall phases. During encoding, participants learned associations between six different food snacks and six different screen positions. To manipulate memory performance, half of the snacks were encoded once, and the other half twice. During distraction, participants worked on a 2-back working memory task for 30 s. Afterwards, participants made nine decisions between pairs of snacks. Snacks were not shown directly, but the respective screen positions were highlighted and participants had to use their memory to make accurate decisions. To manipulate deliberation time, participants were given either a 1 s or a 5 s time window before they had to make a choice. Finally, the ability of the participant to remember snacks correctly was assessed via cued recall (during this phase, MR scanning was paused to enable communication between the participant and the experimenter). Participants were incentivized to make accurate choices by realizing one choice from a randomly selected decision-making trial. Further information can be found in <xref ref-type="bibr" rid="bib16">Gluth et al. (2015)</xref>.</p></sec><sec id="s4-9"><title>fMRI example: cognitive model</title><p>To predict preferential choices and to inform the LOTO-based fMRI analysis, we used a slightly modified version of the cognitive model proposed in the original study (each modification is explained below). This model assumes that decisions are based on a noisy evidence accumulation (diffusion) process that is informed by the subjective values of the available choice options. The model also accounts for the possibility that options are not recalled and for participants’ tendency to prefer remembered over forgotten options (the <italic>memory bias</italic>; see also <xref ref-type="bibr" rid="bib16">Gluth et al., 2015</xref> and <xref ref-type="bibr" rid="bib25">Mechera-Ostrovsky and Gluth, 2018</xref>). Because participants were forced to wait for a specified amount of time (i.e., either 1 s or 5 s) and then had to indicate their decision promptly, the model does not assume a decision threshold but an unbounded diffusion process that is terminated at the specified amount of time. The probability that an option is chosen is thus given by the probability that the diffusion process ends with a positive or negative value. More specifically, the probability of choosing the left option (<italic>l</italic>) over the right option (<italic>r</italic>) is:<disp-formula id="equ16"><label>(11)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where Φ(<italic>x</italic>) refers to the value of the cumulative distribution function of the standard normal distribution at <italic>x</italic>, <italic>t</italic> refers to the deliberation time (i.e., <italic>t</italic> = 1 or <italic>t</italic> = 5), σ accounts for the amount of noise in the diffusion process (σ ≥ 0), and Δ<italic><sub>V</sub></italic> refers to the value difference between <italic>l</italic> and <italic>r</italic>, which is given by:<disp-formula id="equ17"><label>(12)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>M<sub>l</sub></italic> is the probability that option <italic>l</italic> is remembered, <italic>V<sub>r</sub></italic> is the standardized subjective value of option <italic>r</italic>, and <italic>g</italic> represents the memory bias. The memory probability <italic>M</italic> is either α or β, depending on whether an item has been shown once or twice during encoding, and α and β are two free parameters (0 ≤ α, β ≤ 1). This definition differs from the model used in <xref ref-type="bibr" rid="bib16">Gluth et al. (2015)</xref> in which β represented the improvement of memory performance for items encoded twice compared to items encoded once. We changed this definition because here we are not interested in testing whether there is a significant improvement in memory for items encoded twice. The second adaption of the model is that the memory bias <italic>g</italic> is not a free parameter. Instead, we took each participant’s intercept coefficient from the logistic regression that tested for the presence of a memory bias (see Equation 1 in <xref ref-type="bibr" rid="bib16">Gluth et al., 2015</xref>). This adaption was made because, contrary to the original study, we did not estimate parameters with hierarchical Bayesian modeling here, and because treating the memory bias as a free parameter led to severe misspecifications of the other free parameters in some participants when using maximum likelihood estimation.</p><p>The three free parameters of the model (i.e., σ, α and β) were estimated via maximum likelihood estimation using the same minimization techniques as were used for the simulations. A grid search was not performed, but the minimization was repeated ten times with randomly generated starting values. After estimating the full model, LOTO was applied to extract trial-by-trial (or more exactly, item-by-item) variability in memory probability <italic>M</italic> by fixing σ to the full-model estimate and re-estimating either α or β with each item taken out once (e.g., for an item <italic>i</italic> that was encoded twice, α was also fixed to the full-model estimate and β was re-estimated by taking out the three decision-making trials that included <italic>i</italic> as a choice option). The difference between the <italic>n</italic> item and the <italic>n</italic>–1 item parameter estimates were then computed and used for the fMRI analysis. To test for the presence of systematic variability in memory probability (see 'Results: Section V'), we generated 1,000 datasets of each participant using the full-model parameter estimates and assuming no variability in α or β. LOTO was then applied to these datasets and the improvement in model fit was compared to that in the real data. To check whether potential trial-by-trial variability in parameter σ could have been misattributed to the memory-related parameters, we simulated data with true trial-wise Gaussian noise in σ (but not in α or β) and compared the LOTO-based improvement in model fit with that of the distribution under the null hypothesis of no variability. We tried 10 different levels of noise in σ; for the highest level, the trial-by-trial variability within a simulated participant was as high as the across-subject variability in the real data (i.e., SD<sub>σ </sub>= 2.2). The lower nine levels exhibited 10% to 90% of this variability.</p></sec><sec id="s4-10"><title>fMRI example: analysis of fMRI data</title><p>Because the fMRI analysis closely followed the procedures described in <xref ref-type="bibr" rid="bib16">Gluth et al. (2015)</xref>, we provide only a brief overview here. Details on fMRI data acquisition, pre-processing and statistical analyses can be found in the original publication. Statistical analysis was based on the General Linear Model (GLM) approach as implemented in SPM12. As in the original study, twelve onset vectors were defined, including two vectors for the encoding phase: one for items shown once and one for items shown twice. Both of these two onset vectors were accompanied by three parametric modulators (PM) that modeled the predicted change of the hemodynamic response as a function of the following variables of interest: i) the subjective value of the encoded snack; ii) whether the snack was correctly remembered or not in the subsequent cued recall phase; and iii) the LOTO-based estimate of item-by-item variability in memory probability (see above). Importantly, the LOTO-based PM was entered into the GLM last, so that correlated fMRI signals could be uniquely attributed to this PM. For each participant, one contrast image was generated that combined the effect of the LOTO-based PM for both onset vectors of the encoding phase.</p><p>At the group level, individual contrast images were subjected to a one-sample <italic>t</italic>-test. For illustration purposes, we performed two different statistical analyses. In the first analysis, we tested for significant voxels within the bilateral hippocampus, using a family-wise error correction for small volume at <italic>p </italic>&lt; .05. The small volume was defined by a bilateral anatomical mask of the hippocampus taken from the AAL brain atlas (<xref ref-type="bibr" rid="bib46">Tzourio-Mazoyer et al., 2002</xref>). The results were displayed on the mean anatomical (T1) image of all participants using an uncorrected threshold of <italic>p </italic>&lt; .001 with at least 10 contiguous voxels. The second analysis was a region-of-interest (ROI) analysis for which we averaged the fMRI signal for left and right hippocampus separately (using the same anatomical mask) and tested whether the average signal in the ROI was significant (at <italic>p </italic>&lt; .05, two-sided).</p></sec><sec id="s4-11"><title>General settings</title><p>All simulations and analyses were conducted in MATLAB. Function minimization via a (constrained) simplex algorithm was conducted using the function <italic>fminsearchcon</italic>. For determining the convergence of parameter estimation and the termination of search, a very strict tolerance criterion of 10<sup>−13</sup> was chosen, because LOTO needs to work with very small differences in deviance when trying to adjust a parameter value from the ‘all-trial’ estimate to the <italic>n</italic>–1 estimate, in particular when <italic>n</italic> is high. To illustrate the effect of the tolerance criterion, in 'Results: Section VII' the performance of LOTO for the HD model was compared between the strict tolerance criterion and the default criterion in MATLAB, which is 10<sup>−4</sup>. Bayesian and joint modeling were conducted with JAGS (see above), which was called from within MATLAB via the function matjags.m.</p></sec><sec id="s4-12"><title>Code availability</title><p>The data and computer codes associated with this article and with the LOTO method are freely available on the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/du85f/">https://osf.io/du85f/</ext-link>). Although running these codes requires access to the proprietary MATLAB software, we stress that LOTO is a technique rather than a software package and can therefore be realized in various programing languages (including open source software such as R or Python).</p></sec><sec id="s4-13"><title>Derivatives of the hyperbolic discounting (HD) model</title><p>The utilities <italic>u<sub>i</sub></italic> and <italic>u<sub>j</sub></italic> of the immediate and delayed choice options, as well as the corresponding choice probabilities <italic>p<sub>i</sub></italic> and <italic>p<sub>j</sub></italic>, according to the HD model are provided in <xref ref-type="disp-formula" rid="equ8 equ9">Equations 6 and 7</xref>, respectively. The log-likelihood for a single choice of the immediate option is given by the natural logarithm of the choice probability <italic>p<sub>i</sub></italic> and thus:<disp-formula id="equ18"><label>(13)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The derivative of <italic>LL<sub>i</sub></italic> with respect to κ (which is hidden in <italic>u<sub>j</sub></italic> in <xref ref-type="disp-formula" rid="equ18">Equation 13</xref>) is therefore:<disp-formula id="equ19"><label>(14)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Similarly, the log-likelihood for a single choice of the delayed option is:<disp-formula id="equ20"><label>(15)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>And the derivative of <italic>LL<sub>j</sub></italic> with respect to κ is therefore:<disp-formula id="equ21"><label>(16)</label><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-14"><title>Example derivatives of LOTO for the HD model</title><p>Here, we show that LOTO will produce estimates of a parameter that are related to both the choices and task features and that are not extreme, as soon as each option has been chosen at least two times (in the case of a two-alternative choice task). We will use the examples of the HD model mentioned in 'Results: Section III'. Assume that there are two trials with identical features (i.e., identical amount <italic>x<sub>i</sub></italic> of the immediate option, identical amount <italic>x<sub>j</sub></italic> of the delayed option, identical delay <italic>d<sub>j</sub></italic> of the delayed option) and that the immediate option is chosen in one trial but the delayed option is chosen in the other trial. (We assume identical features only to simplify the derivatives. As stated above, it is important to vary the features across trials.) In this case, the derivative of the log-likelihood with respect to the discount factor κ is given by <xref ref-type="disp-formula" rid="equ12">Equation 8c</xref>. Setting this equation to 0 yields the MLE of the parameter:<disp-formula id="equ22"><label>(17)</label><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mo>∗</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We can easily generalize this to the case of <italic>n</italic> trials in which the immediate option was chosen <italic>n<sub>i</sub></italic> times and the delayed option was chosen <italic>n<sub>j</sub></italic> times (i.e., <italic>n<sub>i</sub> + n<sub>j</sub></italic> = n):<disp-formula id="equ23"><label>(18a)</label><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mo>∗</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This equation has to be solved for κ, which requires some steps. By putting the second part of the right-hand side to the left hand side, we get:<disp-formula id="equ24"><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mo>∗</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This simplifies to:<disp-formula id="equ25"><mml:math id="m25"><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="normal">κ</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="normal">κ</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">κ</mml:mi><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">*</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">κ</mml:mi><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>We re-arrange this to:<disp-formula id="equ26"><mml:math id="m26"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">κ</mml:mi><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">*</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">κ</mml:mi><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Note that <italic>p<sub>i</sub></italic> is specified in <xref ref-type="disp-formula" rid="equ9">Equation 7</xref>. Therefore:<disp-formula id="equ27"><mml:math id="m27"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Taking the logarithm on both sides yields:<disp-formula id="equ28"><mml:math id="m28"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></disp-formula></p><p>We re-arrange this to:<disp-formula id="equ29"><mml:math id="m29"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac> <mml:mi mathvariant="normal"/> <mml:mi mathvariant="normal"/></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Note that <italic>u<sub>i</sub></italic> and <italic>u<sub>j</sub></italic> are given by <xref ref-type="disp-formula" rid="equ8">Equation 6</xref>. Therefore:<disp-formula id="equ30"><mml:math id="m30"><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Finally, we re-arrange this to isolate κ:<disp-formula id="equ31"><mml:math id="m31"><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ32"><mml:math id="m32"><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mi>*</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">κ</mml:mi><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ33"><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ34"><label>(18b)</label><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>For instance, in the case of <italic>n<sub>i</sub></italic> = <italic>n<sub>j</sub></italic>, this equation simplifies to:<disp-formula id="equ35"><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>which yields a κ of 0.075 and 0.182 for the two example trials mentioned in 'Results: Section III' (compare with the right panels of <xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p>Importantly, Equation 18b can be expected to yield reasonable results as long as <italic>n<sub>i</sub></italic> and <italic>n<sub>j</sub></italic> are ≥ 1. If either <italic>n<sub>i</sub></italic> or <italic>n<sub>j</sub></italic> is 0, then the first term in the parentheses on the right-hand side of Equation 18b will be 0, because we have <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>±</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the denominator, and the estimation of κ will simply be –1/<italic>d<sub>j</sub></italic>, which is negative and thus lies outside of the range of values allowed for κ. When applying LOTO, we therefore require <italic>n<sub>i</sub></italic> and <italic>n<sub>j</sub></italic> to be ≥ 2, because the parameter is estimated once for <italic>n</italic> and once for <italic>n</italic>–1 trials, and in the <italic>n</italic>–1 case, LOTO will yield an unreasonable estimate in one trial (i.e., if <italic>n<sub>i</sub></italic> = 1, then the error will be in that one trial in which the immediate option was chosen; if <italic>n<sub>j</sub></italic> = 1, then the error will be in that one trial in which the delayed option was chosen).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank the members of the Decision Neuroscience and Economic Psychology groups at the University of Basel for helpful discussions, and Gilad Zlotkin, Yoav Kessler and Gideon Rosenthal for suggesting related work in other domains. This work was supported by a grant from the Swiss National Science Foundation (#100014_172761) to SG and by a grant from the Israel Science Foundation (#381–15) to NM.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Visualization, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Methodology, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants gave written informed consent, and the study was approved by the Aerztekammer Hamburg, Germany (case # PV4290). All experiments were performed in accordance with the relevant guidelines and regulations.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.42607.020</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-42607-transrepform-v3.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The relevant data and computer codes are uploaded on the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/du85f/">https://osf.io/du85f/</ext-link>) and are freely available.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Gluth</surname><given-names>S</given-names></name><name><surname>Meiran</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Leave-one-trial-out (LOTO)</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/OSF.IO/DU85F</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname> <given-names>GM</given-names></name><name><surname>DeGroot</surname> <given-names>MH</given-names></name><name><surname>Marschak</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Measuring utility by a single-response sequential method</article-title><source>Behavioral Science</source><volume>9</volume><fpage>226</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1002/bs.3830090304</pub-id><pub-id pub-id-type="pmid">5888778</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benwell</surname> <given-names>CSY</given-names></name><name><surname>Keitel</surname> <given-names>C</given-names></name><name><surname>Harvey</surname> <given-names>M</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Thut</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Trial-by-trial co-variation of pre-stimulus EEG alpha power and visuospatial bias reflects a mixture of stochastic and deterministic effects</article-title><source>European Journal of Neuroscience</source><volume>48</volume><fpage>2566</fpage><lpage>2584</lpage><pub-id pub-id-type="doi">10.1111/ejn.13688</pub-id><pub-id pub-id-type="pmid">28887893</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Pattern Recognition and Machine Learning</source><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The simplest complete model of choice response time: linear ballistic accumulation</article-title><source>Cognitive Psychology</source><volume>57</volume><fpage>153</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2007.12.002</pub-id><pub-id pub-id-type="pmid">18243170</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busemeyer</surname> <given-names>JR</given-names></name><name><surname>Gluth</surname> <given-names>S</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name><name><surname>Turner</surname> <given-names>BM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cognitive and neural bases of Multi-Attribute, Multi-Alternative, Value-based decisions</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>251</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.12.003</pub-id><pub-id pub-id-type="pmid">30630672</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>Wyart</surname> <given-names>V</given-names></name><name><surname>Devauchelle</surname> <given-names>AD</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational precision of mental inference as critical source of human choice suboptimality</article-title><source>Neuron</source><volume>92</volume><fpage>1398</fpage><lpage>1411</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.11.005</pub-id><pub-id pub-id-type="pmid">27916454</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esterman</surname> <given-names>M</given-names></name><name><surname>Tamber-Rosenau</surname> <given-names>BJ</given-names></name><name><surname>Chiu</surname> <given-names>YC</given-names></name><name><surname>Yantis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Avoiding non-independence in fMRI data analysis: leave one subject out</article-title><source>NeuroImage</source><volume>50</volume><fpage>572</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.10.092</pub-id><pub-id pub-id-type="pmid">20006712</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Farrell</surname> <given-names>S</given-names></name><name><surname>Lewandowsky</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Computational Modeling of Cognition and Behavior</source><publisher-loc>New York</publisher-loc><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9781316272503</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Findling</surname> <given-names>C</given-names></name><name><surname>Skvortsova</surname> <given-names>V</given-names></name><name><surname>Dromnelle</surname> <given-names>R</given-names></name><name><surname>Palminteri</surname> <given-names>S</given-names></name><name><surname>Wyart</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Computational noise in reward-guided learning drives behavioral variability in volatile environments</article-title><source>BioRxiv</source><pub-id pub-id-type="doi">10.1101/439885</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Eichele</surname> <given-names>T</given-names></name><name><surname>Brown</surname> <given-names>S</given-names></name><name><surname>Serences</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reciprocal relations between cognitive neuroscience and formal cognitive models: opposites attract?</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>272</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.04.002</pub-id><pub-id pub-id-type="pmid">21612972</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frederick</surname> <given-names>S</given-names></name><name><surname>Loewenstein</surname> <given-names>G</given-names></name><name><surname>O'Donoghue</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Time discounting and time preference: a critical review</article-title><source>Journal of Economic Literature</source><volume>40</volume><fpage>351</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1257/jel.40.2.351</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Holmes</surname> <given-names>AP</given-names></name><name><surname>Worsley</surname> <given-names>KJ</given-names></name><name><surname>Poline</surname> <given-names>J-P</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name><name><surname>Frackowiak</surname> <given-names>RSJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Statistical parametric maps in functional imaging: a general linear approach</article-title><source>Human Brain Mapping</source><volume>2</volume><fpage>189</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1002/hbm.460020402</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Rubin</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Inference from iterative simulation using multiple sequences</article-title><source>Statistical Science</source><volume>7</volume><fpage>457</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1214/ss/1177011136</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Geuter</surname> <given-names>S</given-names></name><name><surname>Qi</surname> <given-names>G</given-names></name><name><surname>Welsh</surname> <given-names>RC</given-names></name><name><surname>Wager</surname> <given-names>TD</given-names></name><name><surname>Lindquist</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Effect size and power in fMRI group analysis</article-title><source>Biorxiv</source><pub-id pub-id-type="doi">10.1101/295048</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gluth</surname> <given-names>S</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Deciding when to decide: time-variant sequential sampling models explain the emergence of value-based decisions in the human brain</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>10686</fpage><lpage>10698</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0727-12.2012</pub-id><pub-id pub-id-type="pmid">22855817</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gluth</surname> <given-names>S</given-names></name><name><surname>Sommer</surname> <given-names>T</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Effective connectivity between hippocampus and ventromedial prefrontal cortex controls preferential choices from memory</article-title><source>Neuron</source><volume>86</volume><fpage>1078</fpage><lpage>1090</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.04.023</pub-id><pub-id pub-id-type="pmid">25996135</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gluth</surname> <given-names>S</given-names></name><name><surname>Hotaling</surname> <given-names>JM</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The attraction effect modulates reward prediction errors and intertemporal choices</article-title><source>Journal of Neuroscience</source><volume>37</volume><fpage>371</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2532-16.2016</pub-id><pub-id pub-id-type="pmid">28077716</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gluth</surname> <given-names>S</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Variability in behavior that cognitive models do not explain can be linked to neuroimaging data</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>104</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.04.012</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>Mittner</surname> <given-names>M</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>On the efficiency of neurally-informed cognitive models to identify latent cognitive states</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>142</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.06.007</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koffarnus</surname> <given-names>MN</given-names></name><name><surname>Deshpande</surname> <given-names>HU</given-names></name><name><surname>Lisinski</surname> <given-names>JM</given-names></name><name><surname>Eklund</surname> <given-names>A</given-names></name><name><surname>Bickel</surname> <given-names>WK</given-names></name><name><surname>LaConte</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An adaptive, individualized fMRI delay discounting procedure to increase flexibility and optimize scanner time</article-title><source>NeuroImage</source><volume>161</volume><fpage>56</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.024</pub-id><pub-id pub-id-type="pmid">28803942</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>MD</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Bayesian Cognitive Modeling: A Practical Course</source><publisher-loc>Cambridge </publisher-loc><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9781139087759</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loomes</surname> <given-names>G</given-names></name><name><surname>Moffatt</surname> <given-names>PG</given-names></name><name><surname>Sugden</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A microeconometric test of alternative stochastic theories of risky choice</article-title><source>Journal of Risk and Uncertainty</source><volume>24</volume><fpage>103</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1023/A:1014094209265</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ly</surname> <given-names>A</given-names></name><name><surname>Marsman</surname> <given-names>M</given-names></name><name><surname>Verhagen</surname> <given-names>J</given-names></name><name><surname>Grasman</surname> <given-names>R</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A tutorial on Fisher information</article-title><source>ArXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.01064">https://arxiv.org/abs/1705.01064</ext-link></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mazur</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="1987">1987</year><chapter-title>An adjusting procedure for studying delayed reinforcement. In Quantitative Analyses of Behavior</chapter-title><source>The Effect of Delay and of Intervening Events on Reinforcement Value</source><volume>5</volume><publisher-loc>Hillsdale</publisher-loc><publisher-name>Erlbaum</publisher-name><fpage>55</fpage><lpage>73</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mechera-Ostrovsky</surname> <given-names>T</given-names></name><name><surname>Gluth</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Memory beliefs drive the memory bias on Value-based decisions</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>10592</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-28728-9</pub-id><pub-id pub-id-type="pmid">30002496</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>J</given-names></name><name><surname>Patterson</surname> <given-names>T</given-names></name><name><surname>Ulrich</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Jackknife-based method for measuring LRP onset latency differences</article-title><source>Psychophysiology</source><volume>35</volume><fpage>99</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1111/1469-8986.3510099</pub-id><pub-id pub-id-type="pmid">9499711</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nunez</surname> <given-names>MD</given-names></name><name><surname>Vandekerckhove</surname> <given-names>J</given-names></name><name><surname>Srinivasan</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How attention influences perceptual decision making: Single-trial EEG correlates of drift-diffusion model parameters</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>117</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.03.003</pub-id><pub-id pub-id-type="pmid">28435173</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Doherty</surname> <given-names>JP</given-names></name><name><surname>Hampton</surname> <given-names>A</given-names></name><name><surname>Kim</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Model-based fMRI and its application to reward learning and decision making</article-title><source>Annals of the New York Academy of Sciences</source><volume>1104</volume><fpage>35</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1196/annals.1390.022</pub-id><pub-id pub-id-type="pmid">17416921</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palestro</surname> <given-names>JJ</given-names></name><name><surname>Bahg</surname> <given-names>G</given-names></name><name><surname>Sederberg</surname> <given-names>PB</given-names></name><name><surname>Lu</surname> <given-names>Z-L</given-names></name><name><surname>Steyvers</surname> <given-names>M</given-names></name><name><surname>Turner</surname> <given-names>BM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A tutorial on joint models of neural and behavioral measures of cognition</article-title><source>Journal of Mathematical Psychology</source><volume>84</volume><fpage>20</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2018.03.003</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paller</surname> <given-names>KA</given-names></name><name><surname>Wagner</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Observing the transformation of experience into memory</article-title><source>Trends in Cognitive Sciences</source><volume>6</volume><fpage>93</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01845-3</pub-id><pub-id pub-id-type="pmid">15866193</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname> <given-names>J</given-names></name><name><surname>Miedl</surname> <given-names>SF</given-names></name><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Formal comparison of Dual-Parameter temporal discounting models in controls and pathological gamblers</article-title><source>PLoS ONE</source><volume>7</volume><elocation-id>e47225</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0047225</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname> <given-names>J</given-names></name><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Overlapping and distinct neural systems code for subjective value during intertemporal and risky decision making</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>15727</fpage><lpage>15734</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3489-09.2009</pub-id><pub-id pub-id-type="pmid">20016088</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pleskac</surname> <given-names>TJ</given-names></name><name><surname>Busemeyer</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Two-stage dynamic signal detection: a theory of choice, decision time, and confidence</article-title><source>Psychological Review</source><volume>117</volume><fpage>864</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1037/a0019737</pub-id><pub-id pub-id-type="pmid">20658856</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polanía</surname> <given-names>R</given-names></name><name><surname>Woodford</surname> <given-names>M</given-names></name><name><surname>Ruff</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Efficient coding of subjective value</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>134</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0292-0</pub-id><pub-id pub-id-type="pmid">30559477</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname> <given-names>RA</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name><name><surname>Durnez</surname> <given-names>J</given-names></name><name><surname>Gorgolewski</surname> <given-names>KJ</given-names></name><name><surname>Matthews</surname> <given-names>PM</given-names></name><name><surname>Munafò</surname> <given-names>MR</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name><name><surname>Poline</surname> <given-names>JB</given-names></name><name><surname>Vul</surname> <given-names>E</given-names></name><name><surname>Yarkoni</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Scanning the horizon: towards transparent and reproducible neuroimaging research</article-title><source>Nature Reviews Neuroscience</source><volume>18</volume><fpage>115</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.167</pub-id><pub-id pub-id-type="pmid">28053326</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quenouille</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="1956">1956</year><article-title>Notes on bias in estimation</article-title><source>Biometrika</source><volume>43</volume><fpage>353</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1093/biomet/43.3-4.353</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richter</surname> <given-names>CG</given-names></name><name><surname>Thompson</surname> <given-names>WH</given-names></name><name><surname>Bosman</surname> <given-names>CA</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A jackknife approach to quantifying single-trial correlation between covariance-based metrics undefined on a single-trial basis</article-title><source>NeuroImage</source><volume>114</volume><fpage>57</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.04.040</pub-id><pub-id pub-id-type="pmid">25917516</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez</surname> <given-names>CA</given-names></name><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>McClure</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Intertemporal choice as discounted value accumulation</article-title><source>PLoS ONE</source><volume>9</volume><elocation-id>e90138</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0090138</pub-id><pub-id pub-id-type="pmid">24587243</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez</surname> <given-names>CA</given-names></name><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Van Zandt</surname> <given-names>T</given-names></name><name><surname>McClure</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The neural basis of value accumulation in intertemporal choice</article-title><source>European Journal of Neuroscience</source><volume>42</volume><fpage>2179</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1111/ejn.12997</pub-id><pub-id pub-id-type="pmid">26179826</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stahl</surname> <given-names>J</given-names></name><name><surname>Gibbons</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The application of jackknife-based onset detection of lateralized readiness potential in correlative approaches</article-title><source>Psychophysiology</source><volume>41</volume><fpage>845</fpage><lpage>860</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2004.00243.x</pub-id><pub-id pub-id-type="pmid">15563338</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tukey</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="1958">1958</year><article-title>Bias and confidence in not quite large samples</article-title><source>Ann. Math. Stat</source><volume>29</volume><fpage>614</fpage><lpage>623</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Sederberg</surname> <given-names>PB</given-names></name><name><surname>Steyvers</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A Bayesian framework for simultaneously modeling neural and behavioral data</article-title><source>NeuroImage</source><volume>72</volume><fpage>193</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.01.048</pub-id><pub-id pub-id-type="pmid">23370060</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>van Maanen</surname> <given-names>L</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Informing cognitive abstractions through neuroimaging: the neural drift diffusion model</article-title><source>Psychological Review</source><volume>122</volume><fpage>312</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1037/a0038894</pub-id><pub-id pub-id-type="pmid">25844875</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Love</surname> <given-names>BC</given-names></name><name><surname>Palmeri</surname> <given-names>TJ</given-names></name><name><surname>Van Maanen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Approaches to analysis in model-based cognitive neuroscience</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>65</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.01.001</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzourio-Mazoyer</surname> <given-names>N</given-names></name><name><surname>Landeau</surname> <given-names>B</given-names></name><name><surname>Papathanassiou</surname> <given-names>D</given-names></name><name><surname>Crivello</surname> <given-names>F</given-names></name><name><surname>Etard</surname> <given-names>O</given-names></name><name><surname>Delcroix</surname> <given-names>N</given-names></name><name><surname>Mazoyer</surname> <given-names>B</given-names></name><name><surname>Joliot</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title><source>NeuroImage</source><volume>15</volume><fpage>273</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0978</pub-id><pub-id pub-id-type="pmid">11771995</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Maanen</surname> <given-names>L</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Eichele</surname> <given-names>T</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Ho</surname> <given-names>T</given-names></name><name><surname>Serences</surname> <given-names>J</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural correlates of trial-to-trial fluctuations in response caution</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>17488</fpage><lpage>17495</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2924-11.2011</pub-id><pub-id pub-id-type="pmid">22131410</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ravenzwaaij</surname> <given-names>D</given-names></name><name><surname>Provost</surname> <given-names>A</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A confirmatory approach for integrating neural and behavioral data into a single model</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>131</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.04.005</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vehtari</surname> <given-names>A</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Gabry</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical bayesian model evaluation using leave-one-out cross-validation and WAIC</article-title><source>Statistics and Computing</source><volume>27</volume><fpage>1413</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1007/s11222-016-9696-4</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Sofer</surname> <given-names>I</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>HDDM: hierarchical bayesian estimation of the Drift-Diffusion model in python</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00014</pub-id><pub-id pub-id-type="pmid">23935581</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.42607.024</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Van Maanen</surname><given-names>Leendert</given-names></name><role>Reviewing Editor</role><aff><institution>University of Amsterdam</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Van Maanen</surname><given-names>Leendert</given-names> </name><role>Reviewer</role><aff><institution>University of Amsterdam</institution><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Leave-One-Trial-Out (LOTO): A general approach to link single-trial parameters of cognitive models to neural data&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by Michael Frank as the Senior Editor, a Reviewing Editor, and two reviewers.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Both of the reviewers appreciate the importance of readily available methods to estimate single-trial values of computational model parameters. We feel that there is much to like about the current approach, but there are some concerns, that I have compressed into a single list of points below.</p><p>Summary:</p><p>The authors discuss a leave-one-out method to estimate trial-by-trial variability in parameters of computational models (LOTO). The rationale is that the deviation between a model parameter that is estimated using all data and a model parameter that is estimated using all data minus one data point, reflects the influence of that one data point. The authors demonstrate that LOTO is relatively easy to implement and faster with respect to previous approaches, and importantly it can be easily demonstrated via simulations whether the adopted approach is falsifiable.</p><p>Essential revisions:</p><p>1) An important issue with the current version of the paper is that in subsection “Comparison of LOTO with related approaches”, a comparison with the more complex, but presumably superior method of joint modeling (Turner et al.) is not performed. It would be good to see how much information about the single trial parameter estimates is really lost when not accounting for the covariance structure as Turner proposes.</p><p>2) A substantial concern about the utility of the approach and the interpretation of the results obtained by LOTO comes from the fact that recent evidence indicates that a good proportion of trial to trial fluctuations comes from inference processes of the incoming signals or subsequent computations (for emerging evidence see e.g., Drugowitsch et al., 2016; Findling et al., 2018; Polania et al., 2018). This suggests, that for instance, in the case of the HD example given by the authors, fluctuations can come directly from encoding d<sub>i</sub> or x<sub>i</sub> or even from performing the computation (x<sub>i</sub> /(1+κ*d<sub>i</sub>)) but not necessarily from κ.. This means that variability in behavior can be misattributed to a given parameter (and therefore misattributed to a brain region when applied to neural data), but the sources of fluctuations may potentially come from encoding/decoding processes of input-signals/computations as indicated by recent research. Is there a way that the authors can rule out or study this possibility? For the case of the HD model, I assume that this might be difficult as the denominator in Equation 6 contains κ*d<sub>i</sub>, but it could be that this is dissociable as u<sub>j</sub> does not depend on κ (at least in some way as this implies d=0). Is there a way that the authors can come up with a paradigm, experiment or analyses to resolve this issue? If this is the case, this is an approach that could significantly advance the current state of the art in the field. The authors would need to convincingly demonstrate that this is the case.</p><p>3) The &quot;toy&quot; example based on the binomial model can be illustrative, it can also be confusing if some clarifications are not done from the beginning. From the moment the authors started to argue that LOTO can be used to detect trial to trial fluctuations on θ, it was clear to me that it was impossible to dissociate trial to trial fluctuations in <italic>k</italic> and θ. The explanation that this is the case comes only in the last paragraph of the subsection. I am not exactly sure what would be the best way to deal with this, but perhaps this issue about LOTO can be mentioned before the authors dive into applying LOTO to this example. Moreover, this is an issue that the authors might want to emphasize in the Discussion by explicitly describing what is or what is not possible with LOTO and giving some recommendations in this respect.</p><p>4) The usual approach when modelling trial-to-trial fluctuations for a given parameter is to assume some parametrization (e.g. uniform distribution for the starting evidence or Gaussian distribution for drift rate, or sometimes a Gamma distribution for precision parameters). While the correlations between LOTO recovered and true values is rather high in general, it would be interesting to see whether the distribution of the parameters recovered by LOTO match the shape of the generating distribution. If yes, great; if not, why not?</p><p>5) In subsection “An example of using LOTO for model-based fMRI”, the authors argue that the neuroimaging effects were &quot;even stronger than those reported in the original publication&quot;. These statements have to be backed up via quantitative analyses. Given that the authors are performing their analysis based on the SPM framework, a direct way to test this is via the Bayesian model comparison module. In this way one could understand how much stronger a given activation pattern is better explained by one model or the other. Please provide the statistical maps of these analyses.</p><p>6) In the LOTO analyses for subsection “An example of using LOTO for model-based fMRI” (reported in Appendix D), σ could have been included as a potential source of trial-to-trial fluctuations. Why was this not the case? If σ does not change from trial to trial, then one should see this from the null distribution analyses on potential improvement of deviance. If this is the case, then how are the neuroimaging results affected?</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.42607.025</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary:</p><p>The authors discuss a leave-one-out method to estimate trial-by-trial variability in parameters of computational models (LOTO). The rationale is that the deviation between a model parameter that is estimated using all data and a model parameter that is estimated using all data minus one data point, reflects the influence of that one data point. The authors demonstrate that LOTO is relatively easy to implement and faster with respect to previous approaches, and importantly it can be easily demonstrated via simulations whether the adopted approach is falsifiable.</p></disp-quote><p>We thank the reviewers for this overall very positive evaluation of our work. In the following, we address the issues raised by the reviewers point by point. Note that the new scripts which were required to run the revision analyses are uploaded on our OSF project (https://osf.io/du85f/).</p><p>On a general note, we re-organized the manuscript to better match the structure of <italic>eLife</italic> articles in the following way: the previous chapter I is the “Introduction” now; the previous chapters II–XI belong to the “Results section” which contains 11 chapters (Results sections I–XI; including a new chapter; see below); the previous Appendices A–D constitute the “Materials and methods section” now. There are no supplementary files (except for the figure supplements).</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) An important issue with the current version of the paper is that in subsection “Comparison of LOTO with related approaches”, a comparison with the more complex, but presumably superior method of joint modeling (Turner et al.) is not performed. It would be good to see how much information about the single trial parameter estimates is really lost when not accounting for the covariance structure as Turner proposes.</p></disp-quote><p>We agree that a comparison of LOTO with the joint modeling approach (Turner et al., 2015) is desirable. In the revised manuscript, we added a comparison between LOTO and the joint modeling approach for estimating variability in parameter κ of the hyperbolic discounting (HD) model (we had problems implementing the joint modeling approach for the LBA model, which is not surprising given the high complexity of the approach).</p><p>Note that the joint modeling approach requires fitting the model via hierarchical Bayesian modeling (Farrell and Lewandowsky, 2018; Lee and Wagenmakers, 2013), because the behavioral and neural parameters of the joint model are assumed to be drawn from a (joint) higher-order (multivariate normal) distribution. Hence, implementing this approach allowed us to compare LOTO also with the (non-joint) Bayesian modeling approach itself, which was not included in the original manuscript either. Adding this method to the comparison also helped to quantify the contribution of accounting for the covariance structure more exactly.</p><p>The results are described at the end of subsection “Comparison of LOTO with related approaches” and are illustrated in Figure 9—figure supplement 1. Interestingly, at least for this example (i.e., parameter κ of the HD model) the correlation between the true trial-by-trial parameter values and LOTO’s estimates are slightly higher than those for the Bayesian modeling and the joint modeling approaches, while the joint modeling approach outperforms the Bayesian approach. On the other hand, the correlation between the neural signal and LOTO’s estimates is slightly lower than the correlation of the joint modeling approach. Again, the joint modeling approach outperforms the non-joint Bayesian approach with respect to the neural data. LOTO and the non-joint Bayesian approach yield similar results. Overall, we deem the differences between the three approaches rather small (for our simulations with 30 participants and 160 trials).</p><disp-quote content-type="editor-comment"><p>2) A substantial concern about the utility of the approach and the interpretation of the results obtained by LOTO comes from the fact that recent evidence indicates that a good proportion of trial to trial fluctuations comes from inference processes of the incoming signals or subsequent computations (for emerging evidence see e.g., Drugowitsch et al., 2016; Findling et al., 2018; Polania et al., 2018). This suggests, that for instance, in the case of the HD example given by the authors, fluctuations can come directly from encoding d<sub>i</sub> or x<sub>i</sub> or even from performing the computation (x<sub>i</sub> /(1+κ*d<sub>i</sub>)) but not necessarily from κ. This means that variability in behavior can be misattributed to a given parameter (and therefore misattributed to a brain region when applied to neural data), but the sources of fluctuations may potentially come from encoding/decoding processes of input-signals/computations as indicated by recent research. Is there a way that the authors can rule out or study this possibility? For the case of the HD model, I assume that this might be difficult as the denominator in Equation 6 contains κ*d<sub>i</sub>, but it could be that this is dissociable as u<sub>j</sub> does not depend on κ (at least in some way as this implies d=0). Is there a way that the authors can come up with a paradigm, experiment or analyses to resolve this issue? If this is the case, this is an approach that could significantly advance the current state of the art in the field. The authors would need to convincingly demonstrate that this is the case.</p></disp-quote><p>Indeed, dissociating trial-by-trial variability of input or output signals of computational processes from variability in parameters of these processes is a very interesting potential application of LOTO. To see whether LOTO could be useful in this regard, we tested whether inducing variability in the input variables of the HD model (i.e., the amount <italic>x</italic> and delay <italic>d</italic> of the delayed option) or in its output variable (i.e., the utility <italic>u</italic>) would be misattributed to parameter κ, when applying LOTO. Mostly in line with the reviewer’s intuitions, this worked well for the amount <italic>x</italic>, but for high levels of variability in delay <italic>d</italic> or in utility <italic>u</italic>, a potential misattribution could not be ruled out (see Figure 5—figure supplement 1).</p><p>Next, we reasoned that a better dissociation between the parameter and the input/output signals might be achieved when adding more choice options. The intuition is that whereas parameter κ could be assumed to be stable across different options (and to vary only from trial to trial), variability in the input/output variables could be assumed to vary across options. Hence, variability in the input/output variables should be less likely to be taken up by κ to vary from trial to trial via LOTO.</p><p>Therefore, we set up simulations of an adapted paradigm of the intertemporal choice task with three options (see also Gluth et al., 2017) and applied the “non-parametric test for systematic variability” for the cases of true variability in κ, <italic>x, d</italic>, or <italic>u</italic> (for details, see subsection “An example of using LOTO for model-based fMRI”). As predicted, a significant improvement in model fit over and above the improvement under the null hypothesis of no variability was only found for κ, but not for any of the input/output variables (see Figure 5—figure supplement 2).</p><p>Taken together, LOTO might indeed be a powerful tool to dissociate parameter variability from variability of input and output signals of a neurocognitive process. As with many other potential applications of LOTO, however, this requires appropriate experimental designs and careful simulations. We discuss these issues at the end of subsection “A non-parametric test for the presence of systematic trial-by-trial variability”.</p><disp-quote content-type="editor-comment"><p>3) The &quot;toy&quot; example based on the binomial model can be illustrative, it can also be confusing if some clarifications are not done from the beginning. From the moment the authors started to argue that LOTO can be used to detect trial to trial fluctuations on θ, it was clear to me that it was impossible to dissociate trial to trial fluctuations in k and θ. The explanation that this is the case comes only in the last paragraph of the subsection. I am not exactly sure what would be the best way to deal with this, but perhaps this issue about LOTO can be mentioned before the authors dive into applying LOTO to this example. Moreover, this is an issue that the authors might want to emphasize in the Discussion by explicitly describing what is or what is not possible with LOTO and giving some recommendations in this respect.</p></disp-quote><p>We followed the reviewer’s suggestion and now state up front that one reason why we dub the binomial distribution a “toy” model is because LOTO cannot infer more information about parameter θ than what is given by the observations <italic>k</italic> (subsection “A “toy” model example”). Our recommendation in this regard, which we re-iterate in the “LOTO recipe” (subsection “The LOTO recipe”), is to include the behavioral data themselves (i.e., choices, RT) when running simulations and regressing the true parameter values onto LOTO estimates (as we have done in our own analyses; see Figure 6C and Figure 12A). If LOTO does not provide any additional information beyond the observations themselves, this will become evident in these regression analyses.</p><disp-quote content-type="editor-comment"><p>4) The usual approach when modelling trial-to-trial fluctuations for a given parameter is to assume some parametrization (e.g. uniform distribution for the starting evidence or Gaussian distribution for drift rate, or sometimes a Gamma distribution for precision parameters). While the correlations between LOTO recovered and true values is rather high in general, it would be interesting to see whether the distribution of the parameters recovered by LOTO match the shape of the generating distribution. If yes, great; if not, why not?</p></disp-quote><p>This is indeed an interesting question, and we denoted a new subsection “Comparison of LOTO with related approaches” to this topic. In one sentence: LOTO does not recover the distribution of the underlying parameters.</p><p>This becomes most apparent when going back to the initial “toy” model of the binomial distribution. For this model, we sampled 500 blocks of 20 trials each with probability parameter θ being fixed over each 20 trials but varying across the 500 blocks (similar to what we did in subsection “A “toy” model example”). Across these 500 blocks, we once sampled values of θ from a uniform distribution and once from a normal distribution. Independent of the underlying distribution, the distribution of LOTO estimates resembled a binomial distribution (see the new Figure 8). This is because the LOTO estimates are perfectly correlated with the observations <italic>k</italic> (see subsection “A “toy” model example”), which are binomially distributed by definition.</p><p>Stated differently, the parameter and LOTO are linked via the model, and the likelihood function of the model determines whether and how much a specific parameter value exerts an influence on the observations (and thus an influence on LOTO’s estimates). Hence, the distribution of LOTO’s estimates do not depend so much on the distributions of the parameter values but rather on the “error function” of the model (e.g., the binomial distribution, the soft-max function for a choice model, the Wiener distribution of a diffusion model, etc.).</p><p>Because the “toy” model of the binomial distribution is neither particularly interesting nor suitable for LOTO-based inferences (see our reply to the previous point), we also looked at the distribution of LOTO values for the two parameters κ (i.e., discount factor) and <italic>b</italic> (i.e., decision threshold) of the HD-LBA model (i.e., the combination of hyperbolic discounting with the LBA). Again, we drew single-trial parameter values of κ and <italic>b</italic> either from normal or from uniform distributions. Here, the LOTO values are distributed (more or less) normally (see Figure 8—figure supplements 1 and 2, for κ and <italic>b</italic>, respectively). Again, the reason for this is that the parameter values are connected with LOTO and the observations (i.e., choices and RT) via the model which exhibits a roughly normally distributed error term (due to the drift rates which are drawn from normal distributions). Importantly, the shape of the underlying distribution did not affect the strength of the correlation between true parameters and LOTO’s estimates much (see the lowest panels in Figure 8—figure supplements 1 and 2). Thus, we conclude that on the one hand LOTO is not suitable to infer the underlying distribution from which fluctuating parameter values are possibly drawn. On the other hand, however, cognitive neuroscientists who are primarily interested in using LOTO to detect significant relationships between cognitive models and neural data (i.e., the audience we have in mind) do not need to be concerned (much) about these underlying distributions.</p><disp-quote content-type="editor-comment"><p>5) In subsection “An example of using LOTO for model-based fMRI”, the authors argue that the neuroimaging effects were &quot;even stronger than those reported in the original publication&quot;. These statements have to be backed up via quantitative analyses. Given that the authors are performing their analysis based on the SPM framework, a direct way to test this is via the Bayesian model comparison module. In this way one could understand how much stronger a given activation pattern is better explained by one model or the other. Please provide the statistical maps of these analyses.</p></disp-quote><p>We agree that such a statement would require statistical confirmation. However, we decided to refrain from performing the proposed Bayesian model comparison, because the comparability of the current analysis and that of the original publication is limited anyway. This is because the analyses do not only differ with respect to the method of capturing trial-by-trial variability (LOTO vs. Gluth and Rieskamp, 2017), but also with respect to the way the model is fitted in the first place (maximum likelihood vs. Bayesian modeling) and with respect to the fMRI software package (SPM8 vs. SPM12). Hence, a potential significant difference between the analyses could not be exclusively attributed to the different methods of capturing trial-by-trial variability.</p><p>Therefore, we decided to take out the claim that the current results are stronger than those of the original publication. Instead, we only mention that the results are mostly in line with the original study, in which a relationship between trial-by-trial memory and hippocampal activation has been found as well subsection “An example of using LOTO for model-based fMRI”.</p><disp-quote content-type="editor-comment"><p>6) In the LOTO analyses for subsection “An example of using LOTO for model-based fMRI” (reported in Appendix D), σ could have been included as a potential source of trial-to-trial fluctuations. Why was this not the case? If σ does not change from trial to trial, then one should see this from the null distribution analyses on potential improvement of deviance. If this is the case, then how are the neuroimaging results affected?</p></disp-quote><p>We did not include parameter σ as a source of trial-by-trial variability for theoretical reasons: note that in this example (memory-based decisions), we do not really track trial-by-trial but rather item-by-item variability. Each snack item is presented three times in the decision-making task, and we assume the same ability to remember the snack across the three trials (which is sensible, given that it is always the same snack). On the other hand, parameter σ can be seen as quantifying “noise in the decision process”, and it seems to be more plausible to assume that this noise differs in every trial.</p><p>Despite these conceptual objections, we decided to test whether true trial-by-trial variability in σ would be misattributed to the memory-related parameters (α and β) when applying LOTO. The results suggest that such a misattribution is unlikely to have occurred: The LOTO-based improvement of model fit in the presence of true variability in σ does not exceed the null distribution and there is also no systematic relationship between the amount of variability in σ and the model fit. We report the results in subsection “An example of using LOTO for model-based fMRI” and Figure 12—figure supplement 1.</p></body></sub-article></article>