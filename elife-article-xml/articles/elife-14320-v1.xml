<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">14320</article-id><article-id pub-id-type="doi">10.7554/eLife.14320</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Serial grouping of 2D-image regions with object-based attention in humans</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-50549"><name><surname>Jeurissen</surname><given-names>Danique</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3835-5977</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-50550"><name><surname>Self</surname><given-names>Matthew W</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5731-579X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-33135"><name><surname>Roelfsema</surname><given-names>Pieter R</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1625-0034</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="other" rid="par-6"/><xref ref-type="other" rid="par-7"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Vision and Cognition, Netherlands Institute for Neuroscience</institution>, <institution>Royal Netherlands Academy of Arts and Sciences</institution>, <addr-line><named-content content-type="city">Amsterdam</named-content></addr-line>, <country>The Netherlands</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychiatry</institution>, <institution>Academic Medical Center</institution>, <addr-line><named-content content-type="city">Amsterdam</named-content></addr-line>, <country>The Netherlands</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Integrative Neurophysiology, Centre for Neurogenomics and Cognitive Research</institution>, <institution>Vrije Universiteit Amsterdam</institution>, <addr-line><named-content content-type="city">Amsterdam</named-content></addr-line>, <country>The Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kastner</surname><given-names>Sabine</given-names></name><role>Reviewing editor</role><aff id="aff4"><institution>Princeton University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>d.jeurissen@nin.knaw.nl</email> (DJ);</corresp><corresp id="cor2"><email>p.roelfsema@nin.knaw.nl</email> (PRR)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>13</day><month>06</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e14320</elocation-id><history><date date-type="received"><day>12</day><month>01</month><year>2016</year></date><date date-type="accepted"><day>17</day><month>05</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Jeurissen et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Jeurissen et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-14320-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.14320.001</object-id><p>After an initial stage of local analysis within the retina and early visual pathways, the human visual system creates a structured representation of the visual scene by co-selecting image elements that are part of behaviorally relevant objects. The mechanisms underlying this perceptual organization process are only partially understood. We here investigate the time-course of perceptual grouping of two-dimensional image-regions by measuring the reaction times of human participants and report that it is associated with the gradual spread of object-based attention. Attention spreads fastest over large and homogeneous areas and is slowed down at locations that require small-scale processing. We find that the time-course of the object-based selection process is well explained by a 'growth-cone' model, which selects surface elements in an incremental, scale-dependent manner. We discuss how the visual cortical hierarchy can implement this scale-dependent spread of object-based attention, leveraging the different receptive field sizes in distinct cortical areas.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.001">http://dx.doi.org/10.7554/eLife.14320.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.14320.002</object-id><title>eLife digest</title><p>When we look at an object, we perceive it as a whole. However, this is not how the brain processes objects. Instead, cells at early stages of the visual system respond selectively to single features of the object, such as edges. Moreover, each cell responds to its target feature in only a small region of space known as its receptive field. At higher levels of the visual system, cells respond to more complex features: angles rather than edges, for example. The receptive fields of the cells are also larger. For us to see an object, the brain must therefore 'stitch' together diverse features into a unified impression.</p><p>This process is termed perceptual grouping. But how does it work? Jeurissen et al. hypothesized that this process depends on the visual system’s attention spreading over a region in the image occupied by an object, and that the speed of the process will depend on the size of the receptive fields involved. If an image region is narrow, the visual system must recruit cells with small receptive fields to process the individual features. Grouping will therefore be slow. By contrast, if the object consists of large uniform areas lacking in detail, grouping should be fast. These assumptions give rise to a model called the “growth-conemodel”, which makes a number of specific predictions about reaction times during perceptual grouping.</p><p>Jeurissen et al. tested the growth-cone model’s predictions by measuring the speed of perceptual grouping in 160 human volunteers. These volunteers looked at an image made up of two simple shapes, and reported whether two dots fell on the same or different shapes. The results supported the growth-cone model. People were able to group large and uniform areas quickly, but were slower for narrow areas. Grouping also took more time when the distance between the dots increased. Hence, perceptual grouping of everyday objects calls on a step-by-step process that resembles solving a small maze.</p><p>The results also revealed that perceptual grouping of simple shapes relies on the spreading of visual attention over the relevant object. Furthermore, the data support the hypothesis that perceptual grouping makes use of the different sizes of receptive fields at various levels of the visual system. Further research will be needed to translate these findings to the more complex natural scenes we encounter in our daily lives.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.002">http://dx.doi.org/10.7554/eLife.14320.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>object-based attention</kwd><kwd>perceptual grouping</kwd><kwd>growth-cone model</kwd><kwd>surface perception</kwd><kwd>receptive field size</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>VICI scheme, MaGW grant, 400-09-198</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter R</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>Brain and Cognition grant, 433-09-208</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter R</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>ALW 823-02-010</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter R</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>PITN-GA-2011-290011, ABC</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter R</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>ERC advanced grant, 39490, Cortic_al_gorithms</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter R</given-names></name></principal-award-recipient></award-group><award-group id="par-7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>604102, Human Brain Project</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter R</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The human visual system groups the individual features of objects together into a coherent whole via a serial, attention-demanding process.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neurophysiological studies over the past 40 years have revealed that the neuronal representation of an object in low-level areas of the visual cortex consists of a set of simple features such as colors and edge orientations. However, this is not how we perceive a visual scene. Our perception is much more structured, because our visual system groups the features into objects. Introspectively, this grouping process appears to be effortless because we hardly ever perceive features in isolation. Yet, the processes for perceptual organization are only partially understood.</p><p>One influential theory suggested that perceptual grouping occurs instantaneously and in parallel if features are connected to each other, i.e. 'uniformly connected' (<xref ref-type="bibr" rid="bib39">Palmer and Rock, 1994</xref>). Such a rapid grouping process would be in line with studies demonstrating that object recognition can be extremely fast and pre-attentive (<xref ref-type="bibr" rid="bib56">Thorpe et al., 1996</xref>; <xref ref-type="bibr" rid="bib35">Moore and Egeth, 1997</xref>; <xref ref-type="bibr" rid="bib57">Treisman, 1985</xref>). High-speed object recognition presumably relies on feedforward processing, leveraging the hierarchy of features represented in the visual cortex. Neurons in lower areas coding for elementary features rapidly propagate activity to shape selective neurons in higher visual areas (<xref ref-type="bibr" rid="bib21">Hung et al., 2005</xref>; <xref ref-type="bibr" rid="bib3">DiCarlo et al., 2012</xref>), grouping features into more complex constellations (<xref ref-type="bibr" rid="bib52">Roelfsema, 2006</xref>) so that perceptual grouping coincides with object recognition (<xref ref-type="bibr" rid="bib1">Biederman et al., 1987</xref>; <xref ref-type="bibr" rid="bib36">Nakayama et al., 1989</xref>; <xref ref-type="bibr" rid="bib5">Driver and Baylis, 1996</xref>; <xref ref-type="bibr" rid="bib41">Pelli et al., 2009</xref>). However, there also exist conditions where perceptual grouping requires a slow and serial process (<xref ref-type="bibr" rid="bib52">Roelfsema, 2006</xref>).</p><p>Curve tracing is an example of a task that invokes such a serial, incremental grouping operation (<xref ref-type="bibr" rid="bib23">Jolicoeur et al., 1986</xref>; <xref ref-type="bibr" rid="bib22">Jolicoeur and Ingleton, 1991</xref>; <xref ref-type="bibr" rid="bib45">Pringle and Egeth, 1988</xref>; <xref ref-type="bibr" rid="bib49">Roelfsema and Houtkamp, 2011</xref>) when participants group contour elements of an elongated curve. In the example display of <xref ref-type="fig" rid="fig1">Figure 1a</xref> subjects judge if the two colored circles fall on the same curve. In this task, reaction times increase linearly with the length of the curve. Subjects first direct their attention to the red fixation point and attention then gradually spreads over the curve until it reaches the green circle (<xref ref-type="bibr" rid="bib54">Scholte et al., 2001</xref>; <xref ref-type="bibr" rid="bib19">Houtkamp et al., 2003</xref>). This process is implemented in the visual cortex as the propagation of enhanced neuronal activity over the curve’s representation (<xref ref-type="bibr" rid="bib44">Pooresmaeili and Roelfsema, 2014</xref>) (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Curve-tracing is size invariant (<xref ref-type="bibr" rid="bib22">Jolicoeur and Ingleton, 1991</xref>) so that the reaction time of observers depends little on the viewing distance. This is remarkable, because the length of the curve in degrees of visual angle increases when subjects view the stimulus from nearby. However, now the distance between the curves also increases and this enhances tracing speed (in degree/s) compensates for the longer curves so that the total reaction time remains the same. Size invariance can be explained if perceptual grouping occurs at multiple levels of the visual cortical hierarchy (<xref ref-type="bibr" rid="bib49">Roelfsema and Houtkamp, 2011</xref>; <xref ref-type="bibr" rid="bib44">Pooresmaeili and Roelfsema, 2014</xref>). When curves are nearby, perceptual grouping requires the high spatial resolution provided by low-level areas where neurons have small receptive fields (RFs) and horizontal connections interconnect neurons with RFs that are nearby in visual space so that progress is slow. If curves are farther apart, however, neurons in higher areas can take over and their larger RFs could speed up the grouping process (<xref ref-type="bibr" rid="bib44">Pooresmaeili and Roelfsema, 2014</xref>). Size invariance also occurs when subjects solve a maze, because paths are followed at a higher speed if the distance between the walls is larger (<xref ref-type="bibr" rid="bib2">Crowe et al., 2000</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.14320.003</object-id><label>Figure 1.</label><caption><title>Mechanism of perceptual grouping.</title><p>(<bold>a</bold>) Perceptual grouping of contour elements calls on a serial process as illustrated with a curve-tracing task. The actual stimulus contains one cue (green, dashed circles show other possible cue locations) and the participant indicates whether it falls on the same curve or on the other curve (points labeled ‘d’) as the fixation point. Reactions times increase linearly with the distance between the fixation point and the second cue on the same object (here 4, 8, 12, or 16 degrees) (<xref ref-type="bibr" rid="bib23">Jolicoeur et al., 1986</xref>). (<bold>b</bold>) Perceptual grouping corresponds to spreading object-based attention over the curve. Cortical neurons propagate an enhanced firing rate of cells over the representation of the relevant curve in the visual cortex (<xref ref-type="bibr" rid="bib52">Roelfsema, 2006</xref>). (<bold>c</bold>) An example stimulus of a 2D shape for which we measure the time course of perceptual grouping. (<bold>d</bold>) We tested the hypothesis that grouping of 2D shapes also requires a serial grouping operation.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.003">http://dx.doi.org/10.7554/eLife.14320.003</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-14320-fig1-v1"/></fig></p><p>Curve tracing and maze-solving might be special cases, however, and it is unclear whether perceptual grouping is serial in more typical visual scenes. We therefore investigated the time course of grouping for line drawings of relatively simple 2D shapes (<xref ref-type="fig" rid="fig1">Figure 1c,d</xref>). Specifically, we addressed the following questions: (1) Does perceptual grouping of simple 2D stimuli rely on a serial, incremental process? (2) What is the influence of image scale on the speed of grouping and the spread of object-based attention? (3) Is there an effect of object-recognition on the speed of the grouping process?</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We examined the time-course of perceptual grouping in line drawings with a new task where subjects judged whether two cues were placed on the same object or two different objects (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Before we describe the psychophysical results, we will first outline possible models for grouping image regions, which predict different patterns of reaction times.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.14320.004</object-id><label>Figure 2.</label><caption><title>Model predictions.</title><p>(<bold>a</bold>) An example stimulus. The participant indicates if the cues (black dots) fall on the same or different shapes. (<bold>b</bold>) The pixel-by-pixel model predicts that the RT depends on the shortest path within the interior of the image region. (<bold>c</bold>) The growth-cone model holds that the speed of the grouping signal also depends on the size of homogeneous image regions. (<bold>d</bold>) The filling-in model predicts that the grouping signal spreads inwards from the boundaries. (<bold>e</bold>) The Euclidean model holds that reaction time depends on the distance between cues (i.e., the eccentricity of the second cue).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.004">http://dx.doi.org/10.7554/eLife.14320.004</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-14320-fig2-v1"/></fig></p><sec id="s2-1"><title>Models of perceptual grouping</title><p>The uniform connectedness hypothesis by <xref ref-type="bibr" rid="bib39">Palmer and Rock, 1994</xref> will serve as baseline. These authors suggested that image regions with homogenous surface properties are grouped instantaneously (<xref ref-type="bibr" rid="bib39">Palmer and Rock, 1994</xref>) so that the reaction time should not depend on the placement of cues within a white region enclosed by a black contour (as in <xref ref-type="fig" rid="fig2">Figure 2a</xref>). The second model has been called 'pixel-by-pixel' in the context of curve-tracing (<xref ref-type="bibr" rid="bib33">McCormick and Jolicoeur, 1994</xref>; <xref ref-type="bibr" rid="bib22">Jolicoeur and Ingleton, 1991</xref>). Grouping is realized by spreading attention across pixels of the same color, at a fixed speed (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Spreading starts at one of the cues and processing time is proportional to the length of the shortest path through the object. At a neuronal level, enhanced activity could spread among neurons tuned to the same color, at a single spatial scale (<xref ref-type="bibr" rid="bib11">Grossberg and Mingolla, 1985</xref>). The third model is the growth-cone model (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), inspired by models for curve tracing (<xref ref-type="bibr" rid="bib44">Pooresmaeili and Roelfsema, 2014</xref>; <xref ref-type="bibr" rid="bib33">McCormick and Jolicoeur, 1994</xref>; <xref ref-type="bibr" rid="bib23">Jolicoeur et al., 1986</xref>) and visual routines (<xref ref-type="bibr" rid="bib58">Ullman, 1984</xref>; <xref ref-type="bibr" rid="bib50">Roelfsema et al., 2000</xref>). The growth-cone model predicts that grouping speed depends on the size of homogeneous image regions. Spreading is fast within large regions and slows down in narrow regions. In our implementation, we determined for every pixel the size of the largest circle centered at that pixel that did not touch the boundaries and assumed that speed was proportional to this size. This model is scale invariant because the total spreading time does not depend on variations in the overall size of the picture, caused e.g. by changes in viewing distance. In the visual cortex, this model could be implemented if neurons with various receptive field (RF) sizes tuned to the same feature spread the enhanced activity. In large, homogenous image regions, neurons with large RFs would make fast progress, but in narrow regions grouping speed decreases because neurons with small RFs take over. The fourth model is one where the grouping signal spreads inwards from the boundaries of the cued object so that the response time increases with the distance between the cue and the boundary (<xref ref-type="fig" rid="fig2">Figure 2d</xref>) (<xref ref-type="bibr" rid="bib26">Komatsu, 2006</xref>). Such a 'filling-in' process has been proposed for brightness perception (<xref ref-type="bibr" rid="bib40">Paradiso and Nakayama, 1991</xref>) and texture segregation (<xref ref-type="bibr" rid="bib29">Lamme et al., 1999</xref>). The fifth and final model is a Euclidean model, which will serve as reference because it is simple. It holds that the reaction time depends on the distance between cues, irrespective of the shape of the objects (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). It differs from the pixel-by-pixel model where distance is measured as the shortest path within the object.</p></sec><sec id="s2-2"><title>Experiment 1 – Perceptual grouping of wedge-shaped objects</title><p>Experiment 1 examined the time-course of perceptual grouping with wedge-shaped stimuli (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). Stimuli contained two adjacent wedges and a red fixation point that served as first cue. Twenty participants judged whether a second red cue (one of the dots shown in blue/grey in <xref ref-type="fig" rid="fig3">Figure 3b</xref> per trial) fell on the same wedge as the fixation point, by pressing a button. We measured eye-position to ensure that the participants maintained gaze on the fixation point. The screen coordinates of the cues were uninformative, because we mirrored and rotated the stimulus across trials. In Experiment 1A the second cue was at a fixed distance from the fixation point. <xref ref-type="fig" rid="fig3">Figure 3c</xref> illustrates the model predictions. The filling-in model predicts short RTs for cues near the edges and long RTs in the center. The uniform connectedness model, the Euclidean model, and the pixel-by-pixel model predict that RTs are constant, because all cues are at the same distance from fixation. In contrast, the growth-cone model predicts that RTs are short in the center and longer near the edges.<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.14320.005</object-id><label>Figure 3.</label><caption><title>Parsing a wedge-shaped object (Experiment 1A).</title><p>(<bold>a</bold>) Trial structure. After a fixation period of 500 ms, we presented two wedges and a cue. The subjects reported whether the cue fell on the same wedge as the fixation point. (<bold>b</bold>) The second cue could fall on the same (blue dots) or different wedge as fixation point (grey dots). The cues in the actual experiment were red. (<bold>c</bold>) RT predictions of the filling-in model, Euclidean model, and growth-cone model. (<bold>d</bold>) Average RTs of 20 observers. Blue bars, RTs for the same object; grey bars, RTs for the different object. Error bars, s.e.m. across participants (after correction for baseline differences in RT).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.005">http://dx.doi.org/10.7554/eLife.14320.005</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-14320-fig3-v1"/></fig></p><p>The average accuracy was 94%, without signs of a speed-accuracy trade-off (correlation between RT and accuracy; r=0.17, t(22)=0.80, p&gt;0.4). <xref ref-type="fig" rid="fig3">Figure 3d</xref> shows the average RTs. If the two cues fell on the 'same' wedge, RTs were shortest in the center of the wedge and longer near the edges (repeated-measures ANOVA, Greenhuise-Geisser corrected, F(4.5, 85.7)=10.9, p&lt;0.001). RTs in the 'different' condition were longer than in the 'same' condition (F(1,19)=19.5, p&lt;0.001). A regression analysis revealed that the growth-cone model accounted for 86% of the variance of the RTs, which was significantly better than models predicting that RT is constant (p&lt;0.001 bootstrap statistic, see Materials and methods). The Filling-in model predicted that RT is shortest near the edges, opposite to the RT data. Thus, these results support the growth-cone model and are incompatible with the other models described above.</p><p>We considered the possibility that the longer RTs near the edges were a masking effect caused by the edges. We therefore carried out a control experiment in which participants viewed the same stimulus and performed a color discrimination task. Eleven participants reported the cue’s color, which could now be red or green. We did not observe a significant correlation between RT and the distance between cue and edge (r=−0.2, t(30)=−1.31, p&gt;0.1). Thus, the edges did not act as a mask.</p><p>Experiment 1B measured the influence of the distance between the fixation point and the cue on RT in thirty new participants (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). The average accuracy was 93% and we did not obtain evidence for a speed-accuracy trade off (correlation between RT and accuracy, r=0.02; t(62)=0.14, p&gt;0.8). The average RT was higher when the cue was farther from the fixation point and a further delay was observed near the edges (warm colors in <xref ref-type="fig" rid="fig4">Figure 4b</xref>), in line with the predictions of the growth-cone model (compare <xref ref-type="fig" rid="fig4">Figure 4b</xref> to the lower left panel of <xref ref-type="fig" rid="fig3">Figure 3c</xref>). We used a regression analysis to compare models (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). The filling-in model erroneously predicted that RTs are shortest near the edges. The Euclidean model (and the pixel-by-pixel model, which made the same prediction in this experiment) accounted for 49% of the variance but failed to explain the influence of edge vicinity. The growth-cone model did capture this effect and it accounted for 72% of the variance. The predictions of the growth-cone model were significantly better than those of the other models (all ps&lt;0.01 bootstrap statistic) and the regression revealed that the average speed of perceptual grouping was 22 ms/growth cone (95% confidence interval 17–27 ms, bootstrap analysis). In other words, the RT increased by ~22 ms for every image patch (with the size of one growth cone) that was added to the perceptual group. Thus, grouping of surfaces invokes a serial perceptual grouping process that proceeds fast in homogeneous image regions and slows down near edges, in accordance with the growth-cone model.<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.14320.006</object-id><label>Figure 4.</label><caption><title>Time course of grouping a simple, wedge-shaped object (Experiment 1B).</title><p>(<bold>a</bold>) In Experiment 1B cue locations were at multiple eccentricities on the same (blue) or different object (grey). (<bold>b</bold>) The pattern of RTs. Cold and warm colors show short and long RTs, respectively. (<bold>c</bold>) Regression analysis showing the fit of the models to the RT data. The growth-cone model explains 72% of the variance (r=0.85) and is superior to the other models.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.006">http://dx.doi.org/10.7554/eLife.14320.006</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-14320-fig4-v1"/></fig></p></sec><sec id="s2-3"><title>Experiment 2 – Perceptual grouping of complex shapes</title><p>We next measured the time-course of perceptual grouping with more complex images to address a number of questions. First, does the shape of the object determine growth cone size? Does grouping speed decrease in narrow parts due to small growth cones and increase in broader parts? Second, do internal features such as textures or color changes influence grouping speed? Third, does object recognition affect perceptual grouping, because it can provide larger chunks to be grouped at once, as suggested by previous work (<xref ref-type="bibr" rid="bib32">Mahoney and Ullman, 1988</xref>; <xref ref-type="bibr" rid="bib42">Peterson and Gibson, 1994</xref>; <xref ref-type="bibr" rid="bib59">Vecera and Farah, 1997</xref>; <xref ref-type="bibr" rid="bib27">Korjoukov et al., 2012</xref>)?</p><p>We created four stimulus sets with 24 stimuli each; color pictures, detailed cartoons, cartoon outlines, and scrambled cartoons (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), based on 12 pictures with two animals and 12 pictures with two vehicles (<xref ref-type="bibr" rid="bib27">Korjoukov et al., 2012</xref>) where animals and vehicles were easy to recognize. However, we made image recognition difficult for the scrambled cartoons by repositioning a few line segments but we left the region where the two objects intersected unchanged (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, lower). Object area, perimeter, and cue locations were similar across the stimulus sets.</p><p>To measure grouping speed, we asked participants to report whether a cue appeared on the same object as the fixation point or on a different object (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). When the subjects had maintained gaze on the fixation point for 500 ms, we presented the cue for 1000 ms and then the two objects. The cue was at a distance of 4.7° (1/3 of the trials) or 9.4° (2/3 of the trials) from the fixation point and these distances were matched if cues fell on the other object. With 24 images per condition and 3 cue positions per object, we had 72 data points for the same object condition, and an equal number for the different object condition. We assigned a total of 88 participants to the four image categories (between-subject condition), with 22 participants per condition. Sixty-four participants maintained gaze on the fixation point until the response, but we found that eye-movements had little influence on the pattern of RTs (Materials and methods).<fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.14320.007</object-id><label>Figure 5.</label><caption><title>Time course of parsing scrambled cartoons (Experiment 2) (the actual stimuli were white outlines on a black background).</title><p>(<bold>a</bold>) After a fixation epoch of 500 ms, the subjects saw the cue for 1000 ms, and then also two objects. They reported whether the cue fell on the same object as the fixation point. (<bold>b</bold>) Left, Example stimulus. FP, fixation point. Numbers indicate the estimated number of growth cones between the cues and the FP. Right, RTs averaged across participants for the different cue locations. Error bars represent s.e.m. (<bold>c-f</bold>) Regression of the RT on the predictions of the filling-in (<bold>c</bold>), Euclidean (<bold>d</bold>), pixel-by-pixel (<bold>e</bold>), and growth-cone model (<bold>f</bold>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.007">http://dx.doi.org/10.7554/eLife.14320.007</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-14320-fig5-v1"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.14320.008</object-id><label>Figure 5—figure supplement 1.</label><caption><title>The full stimulus set for Experiment 2.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.008">http://dx.doi.org/10.7554/eLife.14320.008</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-14320-fig5-figsupp1-v1"/></fig></fig-group></p><p><xref ref-type="fig" rid="fig5">Figure 5b</xref> shows an example scrambled cartoon picture and the RTs averaged across 22 participants. The RT was shortest for the nearest cue (blue in <xref ref-type="fig" rid="fig5">Figure 5b</xref>, 627 ms) and it increased by 24/30 ms (green/red in <xref ref-type="fig" rid="fig5">Figure 5b</xref>) for the more distant cues. We investigated how well the different grouping models account for the pattern of RTs across all scrambled cartoon pictures with a regression analysis (<xref ref-type="fig" rid="fig5">Figure 5c–f</xref>). As in experiment 1, the filling-in model made the wrong prediction that RT is shortest near boundaries (r=−0.30). The Euclidean model predicts that RT depends on the distance between fixation point and cue (i.e. the eccentricity of the cue) but this relationship was weak in the data (r=0.29). The predictions of the pixel-by-pixel model were better (r=0.36) but those of the growth-cone model were best (r=0.76). The growth-cone model explained 58% of the variance, which was significantly better than the other three models (<xref ref-type="fig" rid="fig6">Figure 6a</xref>) (bootstrap statistic: all ps&lt;0.01) and the regression yielded an estimated time per growth cone shift of 24 ms (95% confidence interval; 18–29 ms), in accordance with the estimate of Experiment 1. Thus, perceptual grouping of 2D image regions invokes a serial grouping process with a speed that depends on their scale: wide regions are grouped quickly and narrow regions more slowly.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.14320.009</object-id><label>Figure 6.</label><caption><title>Model fits for the different types of images (Experiment 2).</title><p>The bars show the variance in RT explained by the different models for (<bold>a</bold>) scrambled cartoons; (<bold>b</bold>) cartoon outlines; (<bold>c</bold>) detailed cartoons; and (<bold>d</bold>) color pictures. <bold>G</bold>, growth-cone; <bold>F</bold>, filling-in; <bold>E</bold>, Euclidean; <bold>P</bold>, pixel-by-pixel model. Asterisks (***) represent p&lt;0.01 (bootstrap test).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.009">http://dx.doi.org/10.7554/eLife.14320.009</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-14320-fig6-v1"/></fig></p><p>We constructed the scrambled cartoons with the aim to make the objects difficult to recognize, as object-recognition can influence perceptual grouping. We did, however, also test the cartoon images in 22 subjects (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). The growth cone model explained 41% of the variance, which is more than any other model (bootstrap: all ps&lt;0.01). However, the growth cone model was not better than the other models for the detailed cartoons with interior contours and for the color pictures (<xref ref-type="fig" rid="fig6">Figure 6c,d</xref>).</p><p>These results reveal that the growth-cone model provides an excellent account for the pattern of RTs for scrambled cartoons and cartoon outlines, but that the predictions for the detailed cartoons (with internal contours) and colored pictures are worse. It follows that the interior lines of the detailed cartoons and the colors and textures of the picture stimuli influence grouping speed. We only used the outlines of the shapes to estimate growth-cone size, which may explain why predictions worsened. Future grouping models might explain additional variance by taking interior features, texture gradients and color transitions into account, but these new models would require additional experimental and theoretical work.</p></sec><sec id="s2-4"><title>Experiment 3 – The role of object-based attention in perceptual grouping</title><p>Previous studies suggested that grouping depends on the spread of object-based attention across the features that need to be bound in perception (<xref ref-type="bibr" rid="bib46">Rensink, 2000</xref>; <xref ref-type="bibr" rid="bib52">Roelfsema, 2006</xref>; <xref ref-type="bibr" rid="bib51">Roelfsema et al., 2007</xref>). So far, our experiments measured reaction times but we did not test the spread of 'attention'. Our third experiment measured the spread of object-based attention, capitalizing on the Egly cueing paradigm (<xref ref-type="bibr" rid="bib7">Egly et al., 1994</xref>) to measure the speed. <xref ref-type="bibr" rid="bib7">Egly et al., 1994</xref> used displays similar to the ones in <xref ref-type="fig" rid="fig7">Figure 7a</xref>. Their participants saw two bars, then one bar was cued at one of its ends and finally a target was presented to which the participants responded with a button press. If the cue was valid because the target appeared at the same location as the cue, the participants’ responses were faster than if it appeared at another location, because spatial attention was summoned by the cue. Their main finding, however, was that if the cue was invalid and the target appeared at the other end of the cued bar (invalid same-object trials), the response was faster than if it appeared at the non-cued bar (invalid different-object trials), even if the distance between the target and the invalid cue was constant. The reaction-time difference between invalid same- and different object trials is a measure for object-based attention, which is hypothesized to select the entire cued bar (<xref ref-type="bibr" rid="bib7">Egly et al., 1994</xref>; <xref ref-type="bibr" rid="bib60">Xu and Chun, 2007</xref>; <xref ref-type="bibr" rid="bib30">Lamy and Egeth, 2002</xref>; <xref ref-type="bibr" rid="bib6">Drummond &amp; Shomstein 2010</xref>).<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.14320.010</object-id><label>Figure 7.</label><caption><title>The time-course of spreading object-based attention (Experiment 3; actual stimuli were white on a black background).</title><p>(<bold>a</bold>) We presented two broad (left) or narrow bars (right) and one bar was cued at one of its ends for 100 ms. After a variable delay, we presented a target dot at the cued location (valid trials, see inset) or at one of two locations that were equidistant from the cue, on the same (invalid same) or on the other bar (invalid different), i.e. only one target dot per trial. In catch trials (not shown), the target did not appear. (<bold>b</bold>) Reaction times for the validly cued trials for the broad (red) and narrow bars (blue) did not differ (t-test, p&gt;0.1). Reaction times for the validly (black) cued locations were faster than those for the invalidly (grey) cued locations (*, p&lt;0.01 in t-test). (<bold>c</bold>) The object-based advantage (RT<sub>invalid different</sub> – RT<sub>invalid same</sub>) as a function of cue-target onset asynchrony. The curves show the fit of Gaussian functions to the object-based advantage for the broad (red) and narrow bars (blue), respectively. The red and blue horizontal bars on the x-axis indicate the 95% confidence interval of the peak of the Gaussian function as measured with a bootstrap method. Error bars represent s.e.m. of the data points.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.010">http://dx.doi.org/10.7554/eLife.14320.010</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-14320-fig7-v1"/></fig></p><p>In our version of the experiment, eleven participants maintained gaze at a fixation point (controlled with an eye-tracker) and they saw two horizontal or vertical bars. We cued a corner of one of the bars (<xref ref-type="fig" rid="fig7">Figure 7a</xref>) and in 80% of trials the participants reported the appearance of a red target dot at one of three positions by pressing a button. The remaining 20% were catch trials that did not require a button press. In target trials, we presented the target dot at the cued location (60%, valid trials), at the opposite side of the same bar (20%, invalid same object), or in the other bar (20%, invalid different object). We varied the time between cue and target onset (between 200 and 600 ms) to measure the time-course of the object-based advantage (invalid same trials vs. invalid different trials). To examine if the speed of object-based attention depends on the size of image regions, we presented either broad bars (3.9° wide, red panels in <xref ref-type="fig" rid="fig7">Figure 7a</xref>) or narrow bars (2°, blue panels). The growth-cone model predicts that the spread of object-based attention is faster for broader bars because growth-cones are larger; we estimated that there were 3.3 and 7.2 growth-cones between cue and target for the broad and narrow bar, respectively (see Materials and methods).</p><p>The average accuracy across participants was 99%. On validly cued trials, bar width did not influence RT (t(10)=−1.450, p&gt;0.1; <xref ref-type="fig" rid="fig7">Figure 7b</xref>). As expected, RTs were shorter on validly cued than invalidly cued trials (t(10)=−3.468, p&lt;0.01). We computed the object-based advantage for the broad and narrow bars and found that RTs were shorter with invalid cues on the same object than on a different object, an object-based advantage that is in accordance with previous studies (<xref ref-type="bibr" rid="bib7">Egly et al., 1994</xref>; <xref ref-type="bibr" rid="bib60">Xu and Chun, 2007</xref>; <xref ref-type="bibr" rid="bib30">Lamy and Egeth, 2002</xref>; <xref ref-type="bibr" rid="bib6">Drummond &amp; Shomstein 2010</xref>). The maximal object-based advantage occurred earlier for the broad bar than for the narrow bar (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). We fitted a Gaussian function to the object-based benefit and found that it peaked after 344 ms (95% confidence interval: 311–376 ms) for the broad bar and after 463 ms (434–492 ms) for the narrow bar (p&lt;0.001; bootstrap analysis). We estimated the speed of attention spreading by dividing the time difference of 119 ms by the difference in the estimated number of growth cones (7.2–3.3=3.9) and obtained a speed of 31 ms per growth cone, compatible with the speed estimates of Experiments 1 and 2.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The new task allowed us to measure the time course of grouping operations in simple 2D line drawings and revealed that perceptual grouping relies on a serial, incremental process, refuting the uniform connectedness rule proposed by <xref ref-type="bibr" rid="bib39">Palmer and Rock, 1994</xref>. A growth-cone model where the speed of the grouping operation depends on the size of homogeneous image regions best describes the results. At first sight, such a slow, serial grouping process seems to be at odds with previous findings that some forms of perceptual grouping, related to the recognition of objects or image categories like animals and vehicles, is very rapid and pre-attentive (<xref ref-type="bibr" rid="bib56">Thorpe et al., 1996</xref>; <xref ref-type="bibr" rid="bib25">Kirchner and Thorpe, 2006</xref>; <xref ref-type="bibr" rid="bib35">Moore and Egeth, 1997</xref>; <xref ref-type="bibr" rid="bib57">Treisman, 1985</xref>). The pre-attentive formation of perceptual groupings appears to rely on rapid feedforward computations that take place when visual information propagates from early to higher visual areas (<xref ref-type="bibr" rid="bib61">Yamins et al., 2014</xref>), a strategy that is also used by state-of-the-art convolutional networks for object recognition (<xref ref-type="bibr" rid="bib55">Szegedy et al., 2015</xref>). However, the recognition of objects in the scene does not imply that visual segmentation processes have completed (<xref ref-type="bibr" rid="bib8">Epshtein et al., 2008</xref>; <xref ref-type="bibr" rid="bib18">Houtkamp and Roelfsema, 2010</xref>). For example, connectedness is one of the cues for perceptual grouping, and it is difficult to compute connectedness in a purely feedforward manner (<xref ref-type="bibr" rid="bib34">Minsky and Papert, 1990</xref>; <xref ref-type="bibr" rid="bib48">Roelfsema et al., 1999</xref>). Furthermore, if a scene contains multiple animals, the recognition of the species does not imply that all parts of every animal have been grouped as coherent percepts (<xref ref-type="bibr" rid="bib27">Korjoukov et al., 2012</xref>) because there are also cases where recognition precedes image parsing (<xref ref-type="bibr" rid="bib59">Vecera and Farah, 1997</xref>; <xref ref-type="bibr" rid="bib43">Peterson et al., 1991</xref>). Experiments 1 and 2 specifically probed this grouping process, asking participants to report whether cues fell on parts of the same object, and we obtained unequivocal evidence for a serial grouping operation. Our results suggest that the visual system processes simple 2D image regions as miniature mazes, and for mazes serial processing is less surprising (<xref ref-type="bibr" rid="bib2">Crowe et al., 2000</xref>).</p><p>Serial processing has also been observed previously in contour grouping tasks (<xref ref-type="bibr" rid="bib52">Roelfsema, 2006</xref>), where grouping is associated with the gradual spread of object-based attention over the perceptual group (<xref ref-type="fig" rid="fig1">Figure 1a,b</xref>) (<xref ref-type="bibr" rid="bib19">Houtkamp et al., 2003</xref>), a process that can only occur for one curve at a time (<xref ref-type="bibr" rid="bib18">Houtkamp and Roelfsema, 2010</xref>). In the visual cortex, the spread of attention is associated with the propagation of enhanced neuronal activity over the representation of the relevant curve (<xref ref-type="bibr" rid="bib44">Pooresmaeili and Roelfsema, 2014</xref>). Spreading is fast if curves are far apart but it slows down if curves are in each other’s vicinity. The curve-tracing operation has also been modeled as a growth-cone process, similar to the one we now used to model grouping of 2D image regions (<xref ref-type="bibr" rid="bib44">Pooresmaeili and Roelfsema, 2014</xref>). In daily life, this serial grouping operation may only be necessary for specific tasks. During object recognition, image parsing may be unnecessary, but during grasping one needs to determine whether surfaces belong to the same object to avoid placing fingers on surfaces of different ones.</p><sec id="s3-1"><title>A growth-cone model for parsing 2D-image regions</title><p>The new growth-cone model holds that grouping speed depends on the size of homogenous image regions. The variation in the scale of processing could rely on multiple stages of the visual cortical hierarchy where neurons have different RF sizes (<xref ref-type="fig" rid="fig8">Figure 8</xref>). Near boundaries and in narrow parts of an object, the perceptual grouping process requires a high spatial resolution, which can be supported by small receptive fields in lower areas (purple and blue in <xref ref-type="fig" rid="fig8">Figure 8</xref>). In these areas the horizontal connections, responsible for the spread of enhanced activity, link neurons that represent nearby locations in visual space. If the homogeneous image regions are larger, the propagation may take place in higher areas with larger RFs so that the grouping speed is higher (red in <xref ref-type="fig" rid="fig8">Figure 8</xref>). This scale dependence of the spread of object-based attention is in accordance with neuroimaging studies revealing the stronger engagement of lower visual areas when the relevant spatial scale is finer (<xref ref-type="bibr" rid="bib47">Rijpkema et al., 2008</xref>; <xref ref-type="bibr" rid="bib17">Hopf et al., 2006</xref>). Future studies could aim to unravel the interactions between neurons at different levels of the visual cortical hierarchy during image parsing.<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.14320.011</object-id><label>Figure 8.</label><caption><title>Schematic representation of the contribution of different visual areas to perceptual grouping.</title><p>Horizontal rows illustrate low-, mid-, and high-level visual areas with larger receptive fields in higher areas. The four columns illustrate different time steps during the grouping process; after the presentation of the cue, and after 1, 3 and 7 growth-cone shifts. The labeling process begins at the cued location. Higher cortical areas with large receptive fields make great strides in the propagation of enhanced neuronal activity and this fast progress also impacts on lower areas through feedback connections (downward pointing arrows). However, the higher visual areas cannot resolve fine-scale details and the grouping of narrower image regions therefore relies on the propagation of enhanced neuronal activity in lower visual areas with smaller receptive fields. Darker colors represent image regions that have been recently reached by the grouping process and lighter colors denote image regions that were labeled at an earlier point in time. White circles represent receptive fields that have not been reached by the grouping process. Note that the labeling process is serial and that the speed of grouping depends on the size of the receptive fields that contribute to the grouping process.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.011">http://dx.doi.org/10.7554/eLife.14320.011</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-14320-fig8-v1"/></fig></p><p>Although the growth-cone model capitalizes on the presence of receptive fields with different sizes at distinct levels of the visual cortical processing hierarchy, it does not require the receptive field size to be constant within a visual cortical area. Indeed, it is well known that the size of receptive fields increases with eccentricity. Suppose that the task is to group an elongated bar with a width of 2° (as in <xref ref-type="fig" rid="fig7">Figure 7a</xref>). The propagation of enhanced neuronal activity would take place in area V1 for the more eccentric parts of the bar (e.g. at 10° eccentricity) (<xref ref-type="bibr" rid="bib9">Gattass et al., 1981</xref>) but area V4 would make fastest progress where the bar is nearer to the fovea (e.g. at 2° eccentricity) (<xref ref-type="bibr" rid="bib10">Gattass et al., 1988</xref>). The horizontal distance between neurons with abutting, non-overlapping RFs is relatively constant across visual areas (<xref ref-type="bibr" rid="bib13">Harvey and Dumoulin, 2011</xref>). Thus, if we assume that the propagation of enhanced neuronal activity through horizontal connections is also similar, it follows that the propagation speed (in °/s or growth-cones/s) should also be constant across eccentricities for a bar with a constant width. The growth-cone mechanism is therefore compatible with the fact that the receptive field size in visual cortical areas increases with eccentricity.</p></sec><sec id="s3-2"><title>Influence of object recognition and interior contours on image parsing</title><p>The current implementation of the growth-cone model is based on the outlines of the shapes only and, accordingly, the interior features of detailed cartoons and color pictures decreased fit quality. Consider, for example, the picture with lemurs of <xref ref-type="fig" rid="fig6">Figure 6c</xref>. The interior contours in the tails form closed, disconnected compartments that cannot be grouped by the growth-cone model. The overall influence of the interior contours is difficult to predict, however. On the one hand, they might slow down parsing by presenting barriers. On the other hand, they might facilitate object recognition and thereby speed up the grouping. Hence, the interior contours, the influence of color and texture, as well as the influence of object-recognition go beyond the simple models considered by us so that variance had to remain unexplained.</p><p>Importantly, perceptual grouping with the same color pictures also relies on a serial process (<xref ref-type="bibr" rid="bib27">Korjoukov et al., 2012</xref>). It would therefore be of interest to generalize the growth-cone model to account for perceptual grouping of objects with interior contours, colors, and textures. We expect that these generalizations necessitate a mechanism for object recognition. In the picture of the lemur (<xref ref-type="fig" rid="fig6">Figure 6c,d</xref>), for example, we see that the left tail belongs to the right lemur, because we know what a lemur looks like. Segmentation based on the characteristic shape of objects is known as 'semantic segmentation'. Models of segmentation can take advantage of the recent breakthroughs in hierarchical convolutional neural networks for object recognition (e.g. <xref ref-type="bibr" rid="bib28">Krizhevsky et al. (2012)</xref>; <xref ref-type="bibr" rid="bib63">Zeiler and Fergus (2013)</xref>; reviewed by <xref ref-type="bibr" rid="bib31">LeCun et al., 2015</xref>). These networks consist of multiple hierarchically organized layers, which transform simple features at the lower layers, resembling lower visual cortical areas, to representations for shape recognition in higher layers, resembling the inferotemporal cortex (<xref ref-type="bibr" rid="bib12">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib62">Yamins and DiCarlo, 2016</xref>). Recent studies have started to apply hierarchical convolutional networks to achieve semantic segmentation (<xref ref-type="bibr" rid="bib16">Hong et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Noh et al., 2015</xref>). The resulting networks can segment pictures of everyday visual scenes. They first use a convolutional network to determine the semantic categories present in the visual scene, which is followed by a 'deconvolutional network' that retrieves the lower-level features that are part of a particular semantic category. Hence, these models explain why object-recognition facilitates image parsing in human perception (<xref ref-type="bibr" rid="bib59">Vecera and Farah, 1997</xref>; <xref ref-type="bibr" rid="bib43">Peterson et al., 1991</xref>; <xref ref-type="bibr" rid="bib27">Korjoukov et al., 2012</xref>). The deconvolutional network could be implemented in the visual cortex as a top-down influence from neurons in shape selective cortical areas to neurons in lower visual areas (i.e. generalizing the feedback connections in <xref ref-type="fig" rid="fig8">Figure 8</xref>) (<xref ref-type="bibr" rid="bib15">Hochstein and Ahissar, 2002</xref>). We expect that semantic segmentation networks may need to also include horizontal connections in order to account for the serial patterns of reaction times in grouping tasks with natural images (as in <xref ref-type="bibr" rid="bib27">Korjoukov et al., 2012</xref>). According to this view, the parsing of a natural scene would invoke an incremental grouping process that gradually adds simple features and more complex shape fragments to an evolving perceptual group (<xref ref-type="bibr" rid="bib52">Roelfsema, 2006</xref>; <xref ref-type="bibr" rid="bib49">Roelfsema and Houtkamp, 2011</xref>).</p></sec><sec id="s3-3"><title>The spread of object-based attention</title><p>We conjectured that grouping relies on the spread of object-based attention within homogenous image regions, and this spread should therefore depend on the size of these regions. Our third experiment confirmed this prediction by showing that the spread of object-based attention is slower for narrow regions. The last experiment yielded an estimate of the attentional propagation speed of ~30 ms per growth cone, which is similar to the estimates of 22 and 24 ms/growth cone of the other two experiments. In the cortex, a shift time of ~25 ms would correspond to the propagation of activity between neurons with abutting, non-overlapping RFs. In V1, the distance between neurons with adjacent RFs is 2–4 mm, and it is only slightly larger in higher visual areas (<xref ref-type="bibr" rid="bib20">Hubel and Wiesel, 1974</xref>; <xref ref-type="bibr" rid="bib13">Harvey and Dumoulin, 2011</xref>). Our results therefore suggest that the propagation speed is 8–16 cm/s (2–4 mm/ 25 ms), which falls neatly in the range of previous neurophysiological measures for the horizontal propagation of neuronal activity in visual cortex (<xref ref-type="bibr" rid="bib14">Hirsch and Gilbert, 1991</xref>; <xref ref-type="bibr" rid="bib37">Nauhaus et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Sato and Carandini, 2012</xref>).</p></sec><sec id="s3-4"><title>Conclusion</title><p>Our results suggest that perceptual grouping of 2D image regions calls on a serial process that takes tens to hundreds of milliseconds for naturalistic pictures. Subjects apparently need this time to spread object-based attention within in the interior of image regions that need to be grouped in perception. The typical time between two saccadic eye movements is ~200–300 ms and it is therefore tempting to speculate that the duration of visual fixations is optimized to allot time for both object recognition and image parsing before the next saccade is made.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Model implementation</title><sec id="s4-1-1"><title>Euclidean model</title><p>The Euclidean model holds that RT depends linearly on the distance between cues, p<sub>1</sub> and p<sub>2</sub>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mtext> </mml:mtext><mml:mo>⋅</mml:mo><mml:mtext> </mml:mtext><mml:mo>∥</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∥</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <italic>RT<sub>Offset</sub></italic> is the intercept and <italic>C</italic> the slope (in ms/deg).</p></sec><sec id="s4-1-2"><title>Filling-in model</title><p>The filling-in model assumes that the grouping signal spreads inwards from the edges of the object towards the interior. RT therefore depends on the distance of the cues to the object boundary:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mtext> </mml:mtext><mml:mo>⋅</mml:mo><mml:mtext> </mml:mtext><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∥</mml:mo><mml:mo>,</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∥</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <italic>RT<sub>Fill</sub></italic> is the predicted reaction time, <italic>RT<sub>Offset</sub></italic> is the intercept and <italic>C</italic> the slope (in ms/deg), and <italic>p<sub>edge1/2</sub></italic> is the point on the object’s edge closest to the respective cue. Thus, <italic>RT<sub>Fill</sub></italic> depends on the largest cue-edge distance because the grouping signal has to reach both cues.</p></sec><sec id="s4-1-3"><title>Pixel-by-pixel model</title><p>In the pixel-by-pixel model the object is grouped incrementally by spreading object-based attention at a constant speed over the object’s surface. The RT depends on the distance between cues, but now measured within the object’s interior. To compute the shortest path between the cues, we defined an 8-connected lattice <italic>G=(V,E)</italic> with vertices (pixels) <italic>V</italic> connected by edges <italic>E</italic> where two neighboring pixels are only connected if they do not cross the object boundary. A path is a set of connected edges {e<inline-formula><mml:math id="inf1"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>E}. We defined P as the set of all paths that emanate from <italic>p<sub>1</sub></italic><inline-formula><mml:math id="inf2"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>V and terminate at <italic>p<sub>2</sub></italic><inline-formula><mml:math id="inf3"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>V. An edge has length 1 for two horizontally and two vertically aligned pixels and <inline-formula><mml:math id="inf4"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> for diagonally aligned pixels. RT was estimated as<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>C</mml:mi><mml:mtext> </mml:mtext><mml:mo>⋅</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mo form="prefix" movablelimits="true">min</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>∈</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf5"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for horizontally and vertically aligned pixels and <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> for diagonally aligned pixels. The pixel values are transformed into visual degrees. The shortest path in P was computed with Dijkstra’s algorithm (<xref ref-type="bibr" rid="bib4">Dijkstra, 1959</xref>).</p></sec><sec id="s4-1-4"><title>Growth-cone model</title><p>The growth-cone model is similar to the pixel-by-pixel model, but it holds that grouping occurs at multiple spatial scales, in visual areas with different RF sizes. The model selects a scale for the growth cone (i.e. RF size) at every pixel on the surface, which corresponds to the diameter <inline-formula><mml:math id="inf7"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo></mml:mrow></mml:math></inline-formula> of the largest inscribed circle centered on that pixel which fits within the boundaries of the object. The growth-cone size determines the speed of the spreading processes at that location; larger growth cones correspond to higher speeds. Accordingly, RT is estimated as follows:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>C</mml:mi><mml:mtext> </mml:mtext><mml:mo>⋅</mml:mo><mml:mtext> </mml:mtext><mml:munder><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>∈</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <italic>RT<sub>Offset</sub></italic> is the intercept and <italic>C</italic> the slope (in ms/growth-cone) of the regression, <inline-formula><mml:math id="inf8"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mo>+</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> for horizontally and vertically aligned pixels and <inline-formula><mml:math id="inf9"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mo>+</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> for pixels aligned diagonally and <italic>RT<sub>GC</sub></italic> is the reaction time. d<italic><sub>inscr</sub>(pix<sub>1</sub></italic>) and d<italic><sub>inscr</sub>(pix<sub>2</sub></italic>) are the maximal diameters of inscribed circles centered at two adjacent pixels. We used a minimum d<italic><sub>inscr</sub></italic> of 1 degree, roughly corresponding to the smallest RF sizes in V1. <italic>RT<sub>GC</sub></italic> is scale invariant. If the image is magnified by a factor, the distance between two cues increases, but the size of the growth cones increases by the same proportion such that <italic>RT<sub>GC</sub></italic> stays the same.</p></sec></sec><sec id="s4-2"><title>Experiments</title><p>The participants reported normal or corrected-to-normal acuity and were paid for their participation. The Ethics Committee at the University of Amsterdam approved the experiments. Informed consent was obtained before the start of the experiment. Eye position was sampled at 1000 Hz with an EyeLink eye-tracking system (SR Research Ltd). The experiments were performed in a dimly lit room. We used a chinrest and the participants sat at a distance of 57 cm from a CRT screen. They practiced the task before data collection started.</p></sec><sec id="s4-3"><title>Experiment 1</title><sec id="s4-3-1"><title>Participants</title><p>Twenty participants enrolled in Experiment 1A and 30 participants in Experiment 1B. Their average age was 24 years (range: 18–55; 35 females, 48 right-handed).</p></sec><sec id="s4-3-2"><title>Stimuli and procedure</title><p>The stimulus consisted of two wedge shapes, drawn in black on a white background (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). When the subject had maintained gaze at the red fixation (size, 0.3°) point for 500 ms, the wedges and the second red cue appeared. The two wedges were always adjacent and the participant’s task was to indicate whether the cues were on the same or different wedges by pressing a button. The stimulus was presented until a response was given or maximally for 3000 ms. We gave feedback ('correct', 'incorrect', or 'too late' – for responses after 3000 ms) after each trial. We instructed participants to complete the task as accurately and quickly as possible and we repeated incorrect trials later in the block.</p><p>The two wedges only differed near the fixation point, at the narrowest part of the wedge (size, 0.6°). The wedges had a radius of 9° (measured from the fixation point) and a width of 10°. In Experiment 1A, the second cue (0.3°) could be placed at one of 24 equidistant positions on the same or different wedge (50% same and 50% different), at a distance of 7.2° from fixation (<xref ref-type="fig" rid="fig3">Figure 3a,b</xref>). The minimal distance between cue and wedge outline was 0.3°. The wedges appeared at one of 9 rotation angles around the fixation point and the wedge with the fixation point was adjacent to the other wedge in the clockwise or counterclockwise direction. We presented these conditions (9 rotations, 2 mirror images, 24 cue locations) in a pseudo-randomized order with 4 repetitions per condition, resulting in a total of 1728 trials per participant, presented in 12 blocks of 144 trials each.</p><p>In Experiment 1B, the second cue could be placed in 64 positions. Within each wedge, three of these locations had an eccentricity of 1.7° from fixation, five locations of 2.9°, five of 4.3°, six of 5.7°, six of 7.2°, and seven of 8.4°. Cue locations at each eccentricity were equidistant from each other and at a minimal distance of 0.5° from the wedge outline (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). The wedges appeared in one of 8 rotations. We presented the conditions (8 orientations, 2 mirror images, 64 cue locations, 2 repetitions) in a pseudo-randomized order with 2048 trials per participant, in 16 blocks of 128 trials.</p><p>Whenever fixation was lost (gaze deviated from fixation by more than 1.4 degrees), the trial was aborted and repeated later within the same block. The buttons (left and right) for the response (same or different) were counterbalanced across participants.</p></sec><sec id="s4-3-3"><title>Data analysis</title><p>We analyzed RTs of correct trials after removing outliers separately for each cue condition if they deviated from the mean of the 1/RT distribution by more than 2.5 s.d. (&lt;0.5% of the data). We tested for a possible speed-accuracy trade-off by computing the correlation between the average RT and accuracy across cue conditions. We used a repeated-measures ANOVA to test RT predictions in Experiment 1A. We applied the Greenhouse-Geisser correction if appropriate.</p><p>For Experiments 1A and 1B, we performed a regression analysis to test how well <italic>RT<sub>Fill</sub>, RT<sub>Eucl</sub>/RT<sub>Pix</sub></italic>, and <italic>RT<sub>GC</sub></italic> (defined in equations above) predicted RT. All models have two free parameters, the baseline reaction time <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and the slope <italic>C</italic> and we used a bootstrapping procedure to test whether the quality of the fits differed between the models. We resampled the data 10,000 times by drawing a random sample of 30 participants with replacement, maintaining the within-subject cue-condition assignment. We computed the distribution of differences in explained variance between models (R-squared) and the confidence interval of the slope of the regression line.</p></sec><sec id="s4-3-4"><title>Control experiment</title><p>Eleven subjects (average age: 23 years; range 20–31; 8 females, all right handed) were enrolled in a control experiment to test the influence of the object boundary on cue visibility. Stimuli and procedures were similar to Experiment 1B with the following exceptions: Part of the wedge shape around the black fixation point was removed and a circular window of 1.4° diameter around fixation was white to remove the difference between the two wedges. The participants indicated whether the cue was red or green and we computed the correlation between RT and the distance to the boundary. We did not observe a significant correlation, which indicates that the boundaries did not mask the colored cues in the main experiments.</p></sec></sec><sec id="s4-4"><title>Experiment 2</title><sec id="s4-4-1"><title>Participants</title><p>Eighty-nine participants (average age: 21 years; range: 18–29; 62 females, 75 right-handed) enrolled in Experiment 2. We removed one participant from the dataset because eye tracking failed when she fell asleep (the pattern of RTs was similar to other participants) so that 88 participants remained.</p></sec><sec id="s4-4-2"><title>Stimuli and procedure</title><p>Participants judged whether two cues were placed on the same or different objects. Trials started with the presentation of a white fixation point on a black background (<xref ref-type="fig" rid="fig5">Figure 5Fig. 5a</xref>). When the participants had maintained gaze at the fixation point for 500 ms, the second white cue (0.5°) appeared for 1000 ms so that subjects could register its location before stimulus onset. Then an image (maximal size: 27° x 20°) of two objects was presented with the fixation point and second cue superimposed. The pictures were presented in full color; the cartoon versions of the stimulus were presented as white lines on a black background (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). At the onset of the image, the cues started flickering at 10 Hz to ensure their continuous visibility. Participants indicated their same/different answer by a left/right button press (the assignment of buttons was counterbalanced across subjects). The stimulus disappeared after a response was given or after 5000 ms. Feedback ('correct', 'incorrect', or 'too late') was given after every trial.</p><p>For a given picture, the first cue was the fixation point, which was always at the same position for that particular image and the second cue was shown in one of six positions (within-subject factor). Three cues were on the same object (50% of trials) and three on a different object (the other 50% of trials) (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). The distance between the second cue and fixation was 4.7° (1/3 of trials) or 9.4°. (2/3 of trials; 1/3 per cue with this distance). We matched the distances of the cues presented on the ‘same’ and ‘different’ object to ensure that they were not informative about the required response.</p><p>The four image conditions were a between-subject factor: color pictures, detailed cartoons, cartoon outlines, or scrambled cartoons (<xref ref-type="fig" rid="fig6">Figure 6</xref>, left panels). There were twenty-four images per condition (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>); twelve pictures with two animals and twelve pictures with two vehicles, adapted from <xref ref-type="bibr" rid="bib27">Korjoukov et al., 2012</xref>. The 88 participants were distributed equally over the four image conditions. We constructed the scrambled cartoon images by scrambling a few line segments of the cartoon outline images so that they became difficult to recognize, but we did not change the image region where the two objects intersected.</p><p>We instructed the participants to perform the task as accurately and quickly as possible. During the explanation of the task we showed all the images once and participants performed 24 practice trials before data collection started to ensure that they understood the task. The participants completed six blocks of 144 trials each, resulting in a total of 864 trials. Within blocks, we randomized the trials of the six cue-conditions and pictures. We required 64 participants (equally distributed across image conditions) to maintain fixation throughout the trial. Whenever fixation was lost (i.e., deviated from the fixation point by more than 1.5°), the trial was aborted and repeated again later within the same block. The other 24 participants directed their gaze to the fixation point to start the trial, but they were then allowed to make eye movements during the remainder of the trial. Previous studies using related tasks (<xref ref-type="bibr" rid="bib22">Jolicoeur and Ingleton, 1991</xref>) and stimulus material (<xref ref-type="bibr" rid="bib27">Korjoukov et al., 2012</xref>) demonstrated that eye movements do not have a strong influence on RT. Indeed, the RTs of subjects that only fixated at the beginning of the trial were highly correlated with those of subjects who maintained fixation (r=0.83, t(70)=12.63, p&lt;0.0001), indicating that eye movements did not have a large influence on RTs. Our main analyses were therefore based on the combined dataset from both experiments. However, if we evaluated the data of the 64 participants that maintained fixation separately, we reproduced the same conclusions: the correlation of the growth-cone model prediction with the RTs to the scrambled cartoons was 0.67, which was significantly better than all other models (p&lt;0.01 in a bootstrap test); the correlation with the data obtained with the cartoon outlines was 0.59, which was significantly better than the other models (p&lt;0.05 in a bootstrap test). In the data set of the 24 participants that were allowed to make saccades we found the same pattern of results: the correlation of the growth-cone model in the scrambled cartoons was 0.81, superior to all other models (p&lt;0.01, bootstrap test); and the correlation of the model prediction with the cartoon outline data-set was 0.55, again, better than the other models (p&lt;0.01, bootstrap test).</p></sec><sec id="s4-4-3"><title>Data analysis</title><p>We analyzed the pattern of RTs of correct trials after removing outliers from each of the six cue conditions that deviated from the mean of the 1/RT distribution by more than 2.5 s.d. (&lt;2% of the data). We tested for the presence of a speed-accuracy trade-off in the four image-conditions by correlating the average RT and accuracy across cue conditions, but did not obtain evidence for such a trade-off in any condition. We used a regression analysis to test how well <italic>RT<sub>Fill</sub>, RT<sub>Eucl</sub>, RT<sub>Pix</sub></italic>, and <italic>RT<sub>GC</sub></italic> (based on equations 1–4) predict the observed RT in the various cue conditions. We tested differences in the quality of the fits between models with the bootstrapping procedure that has been described for Experiment 1.</p></sec></sec><sec id="s4-5"><title>Experiment 3</title><sec id="s4-5-1"><title>Participants</title><p>Eleven participants (average age: 24 years; range: 20–36; 9 females, 10 right-handed) enrolled in Experiment 3.</p></sec><sec id="s4-5-2"><title>Stimuli and procedure</title><p>A trial (see <xref ref-type="fig" rid="fig7">Figure 7a</xref>) started when the participant fixated on a red fixation point (0.4° in diameter) on a black background for 500 ms. White outlines of two horizontal or vertical bars (broad bars: both bars 3.9° wide and 15.6° long; or narrow bars: 2.0° wide and 15.6° long) were presented with one white cue on one end of one of the bars for 100 ms (line thickness of the bars: 0.2°; line thickness of the cue: 1.2°). The cue extended over the full width of the bar and for 3.9° on the long side of the bar. After cue offset, the bars remained on the screen and participants pressed a button when they detected a red target dot (present in 80% of trials), which appeared at a cue-target onset asynchrony of 200, 300, 400, 500, or 600 ms measured relative to the offset of the cue. The red target dot (0.8° diameter) was presented at one of three locations (distance to fixation: 8.3°) until the button press. The remaining 20 percent of trials were catch trials and the participant had to maintain fixation for 1000 ms. Feedback was given at the end of every trial for correct (for 500 ms) or incorrect (for 3000 ms) responses.</p><p>A target trial could be 1) valid (60% of non-catch trials): the target was presented at the cued location; 2) invalid on the same object (20%): the cue and target are presented on opposite sides of the same bar; or 3) invalid on the different object (20%): the cue and target are presented on different bars (inset in <xref ref-type="fig" rid="fig7">Figure 7a</xref>). Note that the distance between the cue and the target was identical for the invalid same and invalid different conditions. On each trial, both bars were either broad or narrow. All trial types (3 validity types at a ratio of 3:1:1, horizontal/vertical orientation of the bars, 4 corners, 2 bar widths, 5 cue-target onset asynchronies) were presented twice, resulting in a total of 800 target trials and we interleaved 200 catch trials with a random orientation, cued corner, and bar width. The trial was aborted and repeated later if fixation was lost (gaze deviated by more than 2° from fixation) or an error was made. The participants completed 10 blocks of 100 trials. They could proceed to the next block whenever they were ready by pressing a button, but after even-numbered blocks, participants had to take a break of at least 1 min. We instructed the participants to perform the task as accurate and fast as possible.</p></sec><sec id="s4-5-3"><title>Data analysis</title><p>We analyzed the RTs of correct trials after removing outliers for each condition (&lt;2.5% of the data) by excluding data points that deviated from the mean of the 1/RT distribution by more than 2.5 s.d. We used a paired t-test to assess the difference between RTs on valid trials with broad and narrow bars and we used an additional paired t-test to test the RT difference between valid and invalid trials (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). We computed the object-based advantage by subtracting RTs to the invalidly cued targets on the same bar from those on the other bar, separately for broad and narrow bars and for each of the five cue-target onset asynchronies. We fitted a Gaussian function to the object-based RT advantage as function of onset asynchrony to determine the optimal cue-target onset asynchrony for the two bar widths (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). We applied a bootstrapping procedure to assess the reliability of the peaks of the fitted Gaussian functions. We selected a random sample of subjects (with replacement) and fitted Gaussian functions to the time course of the object-based advantage. We repeated this procedure 10,000 times to compute the 95% confidence interval of the peak time for broad and narrow bars. We used the growth-cone model to determine the number of growth cones between the cue and invalid target position on the same object (3.3 growth-cones for the broad bar; 7.2 for the narrow bar) and computed the speed of the attentional spreading process by dividing the time-difference between the peak same object-advantages for the broad and narrow bar by the difference in the number of growth-cones.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors thank Heiko Neumann for helpful comments on a earlier draft of the manuscript, Marita Rokx, Kaushik Lakshminarasimhan, Klaudia Ambroziak, and Sasa Kozelj for help with collecting the data, and Laurens van der Maaten for help with the model implementation.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>DJ, Designed the experiment, Made the stimuli, Implemented the models, Performed the experiments, Analyzed the data, Did the statistical analysis, Wrote the manuscript</p></fn><fn fn-type="con" id="con2"><p>MWS, Designed the experiment, Did the statistical analysis, Wrote the manuscript</p></fn><fn fn-type="con" id="con3"><p>PRR, Designed the experiment, Wrote the manuscript</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The Ethics Committee at the University of Amsterdam approved the experiments. Informed consent was obtained from the participant before the start of the experiment.</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman I</surname></name></person-group><year iso-8601-date="1987">1987</year><article-title>Recognition-by-components: a theory of human image understanding</article-title><source>Psychological Review</source><volume>94</volume><fpage>115</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.94.2.115</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowe</surname><given-names>DA</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Chafee</surname><given-names>MV</given-names></name><name><surname>Anderson</surname><given-names>JH</given-names></name><name><surname>Georgopoulos</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Mental maze solving</article-title><source>Journal of Cognitive Neuroscience</source><volume>12</volume><fpage>813</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.1162/089892900562426</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>EW</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>A note on two problems in connexion with graphs</article-title><source>Numerische Mathematik</source><volume>1</volume><fpage>269</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1007/BF01386390</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driver</surname><given-names>J</given-names></name><name><surname>Baylis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Edge-assignment and figure-ground segmentation in short-term visual matching</article-title><source>Cognitive Psychology</source><volume>31</volume><fpage>248</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1006/cogp.1996.0018</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drummond</surname><given-names>L</given-names></name><name><surname>Shomstein</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Object-based attention: shifting or uncertainty?</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>72</volume><fpage>1743</fpage><lpage>1755</lpage><pub-id pub-id-type="doi">10.3758/APP.72.7.1743</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egly</surname><given-names>R</given-names></name><name><surname>Driver</surname><given-names>J</given-names></name><name><surname>Rafal</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Shifting visual attention between objects and locations: evidence from normal and parietal lesion subjects</article-title><source>Journal of Experimental Psychology. General</source><volume>123</volume><fpage>161</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.123.2.161</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epshtein</surname><given-names>B</given-names></name><name><surname>Lifshitz I</surname></name><name><surname>Ullman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Image interpretation by a single bottom-up top-down cycle</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>105</volume><fpage>14298</fpage><lpage>14303</lpage><pub-id pub-id-type="doi">10.1073/pnas.0800968105</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gattass</surname><given-names>R</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name><name><surname>Sandell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Visual topography of V2 in the macaque</article-title><source>The Journal of Comparative Neurology</source><volume>201</volume><fpage>519</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1002/cne.902010405</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gattass</surname><given-names>R</given-names></name><name><surname>Sousa</surname><given-names>AP</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Visuotopic organization and extent of V3 and V4 of the macaque</article-title><source>Journal of Neuroscience</source><volume>8</volume><fpage>1831</fpage><lpage>1845</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossberg</surname><given-names>S</given-names></name><name><surname>Mingolla</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Neural dynamics of form perception: boundary completion, illusory figures, and neon color spreading</article-title><source>Psychological Review</source><volume>92</volume><fpage>173</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.92.2.173</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>BM</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The relationship between cortical magnification factor and population receptive field size in human visual cortex: constancies in cortical architecture</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>13604</fpage><lpage>13612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2572-11.2011</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirsch</surname><given-names>JA</given-names></name><name><surname>Gilbert</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Synaptic physiology of horizontal connections in the cat's visual cortex</article-title><source>Journal of Neuroscience</source><volume>11</volume><fpage>1800</fpage><lpage>1809</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochstein</surname><given-names>S</given-names></name><name><surname>Ahissar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>View from the top: hierarchies and reverse hierarchies in the visual system</article-title><source>Neuron</source><volume>36</volume><fpage>791</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01091-7</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>S</given-names></name><name><surname>Oh</surname><given-names>J</given-names></name><name><surname>Han</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning transferrable knowledge for semantic segmentation with deep convolutional neural network</article-title><source>ArXiv</source><elocation-id>1512.07928</elocation-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopf</surname><given-names>JM</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Boelmans</surname><given-names>K</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Boehler</surname><given-names>CN</given-names></name><name><surname>Rieger</surname><given-names>J</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The neural site of attention matches the spatial scale of perception</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>3532</fpage><lpage>3540</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4510-05.2006</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houtkamp</surname><given-names>R</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Parallel and serial grouping of image elements in visual perception</article-title><source>Journal of Experimental Psychology</source><volume>36</volume><fpage>1443</fpage><lpage>1459</lpage><pub-id pub-id-type="doi">10.1037/a0020248</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houtkamp</surname><given-names>R</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A gradual spread of attention during mental curve tracing</article-title><source>Perception &amp; Psychophysics</source><volume>65</volume><fpage>1136</fpage><lpage>1144</lpage><pub-id pub-id-type="doi">10.3758/BF03194840</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Uniformity of monkey striate cortex: a parallel relationship between field size, scatter, and magnification factor</article-title><source>The Journal of Comparative Neurology</source><volume>158</volume><fpage>295</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1002/cne.901580305</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>CP</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Fast readout of object identity from macaque inferior temporal cortex</article-title><source>Science</source><volume>310</volume><fpage>863</fpage><lpage>866</lpage><pub-id pub-id-type="doi">10.1126/science.1117593</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jolicoeur</surname><given-names>P</given-names></name><name><surname>Ingleton</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Size invariance in curve tracing</article-title><source>Memory &amp; Cognition</source><volume>19</volume><fpage>21</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.3758/BF03198493</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jolicoeur</surname><given-names>P</given-names></name><name><surname>Ullman</surname><given-names>S</given-names></name><name><surname>Mackay</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Curve tracing: a possible basic operation in the perception of spatial relations</article-title><source>Memory &amp; Cognition</source><volume>14</volume><fpage>129</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.3758/BF03198373</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jolicoeur</surname><given-names>P</given-names></name><name><surname>Ullman</surname><given-names>S</given-names></name><name><surname>Mackay</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Visual curve tracing properties</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>17</volume><fpage>997</fpage><lpage>1022</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.17.4.997</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirchner</surname><given-names>H</given-names></name><name><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Ultra-rapid object detection with saccadic eye movements: visual processing speed revisited</article-title><source>Vision Research</source><volume>46</volume><fpage>1762</fpage><lpage>1776</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.10.002</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Komatsu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The neural mechanisms of perceptual filling-in</article-title><source>Nature Reviews. Neuroscience</source><volume>7</volume><fpage>220</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1038/nrn1869</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korjoukov I</surname></name><name><surname>Jeurissen</surname><given-names>D</given-names></name><name><surname>Kloosterman</surname><given-names>NA</given-names></name><name><surname>Verhoeven</surname><given-names>JE</given-names></name><name><surname>Scholte</surname><given-names>HS</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The time course of perceptual grouping in natural scenes</article-title><source>Psychological Science</source><volume>23</volume><fpage>1482</fpage><lpage>1489</lpage><pub-id pub-id-type="doi">10.1177/0956797612443832</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Advances in Neural Information Processing Systems</source><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VA</given-names></name><name><surname>Rodriguez-Rodriguez</surname><given-names>V</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names> </name></person-group><year iso-8601-date="1999">1999</year><article-title>Separate processing dynamics for texture elements, boundaries and surfaces in primary visual cortex of the macaque monkey</article-title><source>Cerebral Cortex</source><volume>9</volume><fpage>406</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1093/cercor/9.4.406</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamy</surname><given-names>D</given-names></name><name><surname>Egeth</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Object-based selection: the role of attentional shifts</article-title><source>Perception &amp; Psychophysics</source><volume>64</volume><fpage>52</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.3758/BF03194557</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mahoney</surname><given-names>JV</given-names></name><name><surname>Ullman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1988">1988</year><person-group person-group-type="editor"><name><surname>Pylyshyn</surname> <given-names>Z</given-names></name></person-group><source>Image Chunking Defining Spatial Building Blocks for Scene Analysis</source><publisher-loc>Norwood, NJ</publisher-loc><publisher-name>Ablex</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCormick</surname><given-names>PA</given-names></name><name><surname>Jolicoeur</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Manipulating the shape of distance effects in visual curve tracing: further evidence for the zoom lens model</article-title><source>Canadian Journal of Experimental Psychology = Revue Canadienne De Psychologie ExpéRimentale</source><volume>48</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1037/1196-1961.48.1.1</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Minsky</surname><given-names>ML</given-names></name><name><surname>Papert</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="1990">1990</year><source>Perceptrons (Vol4)</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>CM</given-names></name><name><surname>Egeth</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Perception without attention: Evidence of grouping under conditions of inattention</article-title><source>Journal of Experimental Psychology</source><volume>23</volume><fpage>339</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.23.2.339</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakayama</surname><given-names>K</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name><name><surname>Silverman</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Stereoscopic depth: its relation to image segmentation, grouping, and the recognition of occluded objects</article-title><source>Perception</source><volume>18</volume><fpage>55</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1068/p180055</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nauhaus I</surname></name><name><surname>Busse</surname><given-names>L</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Ringach</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Stimulus contrast modulates functional connectivity in visual cortex</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>70</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1038/nn.2232</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noh</surname><given-names>H</given-names></name><name><surname>Hong</surname><given-names>S</given-names></name><name><surname>Han</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning deconvolution network for semantic segmentation</article-title><source>ArXiv</source><pub-id pub-id-type="doi">10.1109/iccv.2015.178</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>S</given-names></name><name><surname>Rock I</surname></name></person-group><year iso-8601-date="1994">1994</year><article-title>Rethinking perceptual organization: The role of uniform connectedness</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>1</volume><fpage>29</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.3758/BF03200760</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paradiso</surname><given-names>MA</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Brightness perception and filling-in</article-title><source>Vision Research</source><volume>31</volume><fpage>1221</fpage><lpage>1236</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(91)90047-9</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Raizman</surname><given-names>N</given-names></name><name><surname>Christian</surname><given-names>CJ</given-names></name><name><surname>Kim</surname><given-names>E</given-names></name><name><surname>Palomares</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Grouping in object recognition: the role of a Gestalt law in letter identification</article-title><source>Cognitive Neuropsychology</source><volume>26</volume><fpage>36</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1080/13546800802550134</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>MA</given-names></name><name><surname>Gibson</surname><given-names>BS</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Object recognition contributions to figure-ground organization: operations on outlines and subjective contours</article-title><source>Perception &amp; Psychophysics</source><volume>56</volume><fpage>551</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.3758/BF03206951</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>MA</given-names></name><name><surname>Harvey</surname><given-names>EM</given-names></name><name><surname>Weidenbacher</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Shape recognition contributions to figure-ground reversal: Which route counts?</article-title><source>Journal of Experimental Psychology</source><volume>17</volume><fpage>1075</fpage><lpage>1089</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.17.4.1075</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pooresmaeili</surname><given-names>A</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A growth-cone model for the spread of object-based attention during contour grouping</article-title><source>Current Biology</source><volume>24</volume><fpage>2869</fpage><lpage>2877</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.10.007</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pringle</surname><given-names>R</given-names></name><name><surname>Egeth</surname><given-names>HE</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Mental curve tracing with elementary stimuli</article-title><source>Journal of Experimental Psychology</source><volume>14</volume><fpage>716</fpage><lpage>728</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.14.4.716</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rensink</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The Dynamic Representation of Scenes</article-title><source>Visual Cognition</source><volume>7</volume><fpage>17</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1080/135062800394667</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rijpkema</surname><given-names>M</given-names></name><name><surname>van Aalderen</surname><given-names>SI</given-names></name><name><surname>Schwarzbach</surname><given-names>JV</given-names></name><name><surname>Verstraten</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Activation patterns in visual cortex reveal receptive field size-dependent attentional modulation</article-title><source>Brain Research</source><volume>1189</volume><fpage>90</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2007.10.100</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Bothe</surname><given-names>S</given-names></name><name><surname>Spekreijse</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1999">1999</year><chapter-title>Algorithms for the detection of connectedness and their neural implementation</chapter-title><person-group person-group-type="editor"><name><surname>Burdet</surname> <given-names>P</given-names></name><name><surname>Combe</surname> <given-names>P</given-names></name><name><surname>Parodi</surname> <given-names>O</given-names></name></person-group><source>Neuronal Information Processing</source><publisher-loc>Singapore</publisher-loc><publisher-name>World Scientific</publisher-name><month>05</month><fpage>81</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1142/9789812818041_0004</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Houtkamp</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Incremental grouping of image elements in vision</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>73</volume><fpage>2542</fpage><lpage>2572</lpage><pub-id pub-id-type="doi">10.3758/s13414-011-0200-0</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Lamme</surname><given-names>VA</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The implementation of visual routines</article-title><source>Vision Research</source><volume>40</volume><fpage>1385</fpage><lpage>1411</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(00)00004-3</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Tolboom</surname><given-names>M</given-names></name><name><surname>Khayat</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Different processing phases for features, figures, and selective attention in the primary visual cortex</article-title><source>Neuron</source><volume>56</volume><fpage>785</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.006</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical algorithms for perceptual grouping</article-title><source>Annual Review of Neuroscience</source><volume>29</volume><fpage>203</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.112939</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>TK</given-names></name><name><surname>Nauhaus I</surname></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Traveling waves in visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>218</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.06.029</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholte</surname><given-names>HS</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The spatial profile of visual attention in mental curve tracing</article-title><source>Vision Research</source><volume>41</volume><fpage>2569</fpage><lpage>2580</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(01)00148-1</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>Sermanet</surname><given-names>P</given-names></name><name><surname>Reed</surname><given-names>S</given-names></name><name><surname>Anguelov</surname><given-names>D</given-names></name><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Rabinovich</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Going deeper with convolutions</article-title><source>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname><given-names>S</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Marlot</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><volume>381</volume><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/381520a0</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treisman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Preattentive processing in vision</article-title><source>Computer Vision, Graphics, and Image Processing</source><volume>31</volume><fpage>156</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/S0734-189X(85)80004-9</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Visual routines</article-title><source>Cognition</source><volume>18</volume><fpage>97</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(84)90023-4</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vecera</surname><given-names>SP</given-names></name><name><surname>Farah</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Is visual image segmentation a bottom-up or an interactive process?</article-title><source>Perception &amp; Psychophysics</source><volume>59</volume><fpage>1280</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.3758/BF03214214</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual grouping in human parietal cortex</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>104</volume><fpage>18766</fpage><lpage>18771</lpage><pub-id pub-id-type="doi">10.1073/pnas.0705618104</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>111</volume><fpage>8619</fpage><lpage>8724</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>MD</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visualizing and understanding convolutional networks</article-title><source>ArXiv</source><elocation-id>1311.2901</elocation-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.14320.013</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kastner</surname><given-names>Sabine</given-names></name><role>Reviewing editor</role><aff id="aff5"><institution>Princeton University</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Serial grouping of 2D-image regions with object-based attention in humans&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, including Ruediger von der Heydt, and the evaluation has been overseen by Sabine Kastner as the Reviewing Editor and David Van Essen as the Senior Editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The mechanisms underlying the process of perceptual organization of multiple object displays or scenes are poorly understood. The present paper uses behavioral data to derive a 'growth-cone' model that describes the propagation of visual signals across the processing hierarchy with the help of object-based attentional spreading.</p><p>Essential revisions:</p><p>We received two complementary reviews that were supportive and enthusiastic about your study. Three major issues deserve further consideration: (i) The non-linearity of the mapping of retinal to cortical space and the consequences for signal propagation need to be discussed (please make your assumptions regarding linearity explicit.); (ii) Details of the growth-cone model need to be explained. (iii) Difficulties of the model regarding texture and internal contours need to be addressed.</p><p><italic>Reviewer #1:</italic> </p><p>This is a wonderful study of perceptual grouping in the human visual system. By measuring the reaction time of subjects' decisions whether two point probes are on the same object or not, the authors provide strong support for a &quot;growth-cone model&quot; compared to several other relevant models. The study also measures the time course of attention spreading with an adaptation of the Egly et al. paradigm, again in support of their model. The study and presentation of the results seems fine to me, except for one major concern that is outlined below.</p><p>My concern is that the way the authors link the model to the structure of the visual cortex assumes that retinal space is mapped linearly onto cortical space. Under this assumption, visual propagation velocity measured in °/s corresponds to neural propagation velocity measured in growth-cones/s, and cm/s in each cortical area. With this assumption, the data suggest a fixed neural propagation velocity the estimate of which is in reasonable agreement with known estimates of horizontal fiber conduction velocity. However, as is well known, cortical mapping is not linear, but follows a logarithmic law. It seems to me that, taking the real physiology into account, the model would not predict a linear relationship as shown, for example, in <xref ref-type="fig" rid="fig4">Figure 4C</xref> (right). While the authors take pains to discuss their somewhat lower estimate of propagation velocity compared to estimates of horizontal fiber conduction, I could not find any discussion of the mapping assumption in this paper, not even in the supplementary information. This issue needs to be addressed in Results (if appropriate by additional computations) and certainly in the Discussion.</p><p><italic>Reviewer #2:</italic> </p><p>I think that this is a nice study with interesting results. I recommend acceptance if the issues below are adequately addressed.</p><p>A general issue that requires a better, more coherent treatment has to do with the roles of bounding contour, internal contours, and surface properties in the process of spreading attention. Without an adequate discussion the models are not sufficiently clear, and it will improve the discussion of the experimental results.</p><p>For example, in presenting the 'growth-cone' model (subsection “Models of perceptual grouping”), it is stated that cone 'should not touch the boundaries'. This immediately raises a difficulty in the reader's mind: do we have to know the object external boundaries in order to apply the process? This brings about a potential chicken-and-egg problem, where some sort of object segmentation is performed prior to the application of the process.</p><p>A similar difficulty arises with the notion of surface properties. For example, the model of uniform connectedness (subsection “Models of perceptual grouping”) talks about 'homogenous surface properties', but it remains unclear whether the two different shapes have the same or distinct surface properties. The two cases give naturally rise to different predictions; in the case of two uniform, but different shapes, the task would be in principle much easier.</p><p>A clear understanding of the models will therefore require a discussion of the possible effects of internal contours and surface properties. This will also be useful for discussion the experimental results of experiment 2, where the growth-cone model runs into difficulties with shapes that have internal contours, and with colored shapes.</p><p>In the discussion of experiment 2 (subsection “Influence of object recognition on image parsing”, last paragraph) the discussion concludes that 'interior contours, as well the influence of color and texture go beyond the present growth-cone model so that variance had to remain unexplained.' I think that the discussion could benefit here from some conceptual aspects. For example, explain why the growth-cone process as used in the model runs into difficulties with internal boundaries and discuss potential implications, does it mean for instance that some form of distinguishing internal from external boundaries is implied prior to the application of the attention-spread process?</p><p>In the subsection “Models of perceptual grouping”: Uniform connectedness – not sufficiently clear. I think, if the two shapes have different colors, e.g. on is red and the other is green, you can check if the two marked locations are on the same shape. But the shapes are defined here by the bounding contours, and the inside surface properties are the same.</p><p>In the subsection “Models of perceptual grouping”, Growth-cone model: it will useful to add a reference to Visual Routines (Ullman 1984) for discussion of 'attention spread' including scale-independent attention spread.</p><p>The Filling-in model: do we have to know which contours are boundaries or which side is the inside?</p><p>The wedge stimulus, experiment 1. The wedges are example of difficulty with possible internal contours. It is not entirely clear if the line separating the two wedges is an internal or external contour.</p><p>In the fourth paragraph of the subsection “Experiment 2 – Perceptual grouping of complex shapes”. 'Thus, perceptual grouping of 2D image regions invokes a serial grouping process with a speed that depends on their scale.' It is not sufficiently clear, what the 'scale' here refers to (scale of what?).</p><p>In the last paragraph of the subsection “Experiment 2 – Perceptual grouping of complex shapes”. The 'growth-cone' had problems with 'detailed' cartoons – does 'detailed' here mean the presence of internal contours? Will be useful to state this here explicitly.</p><p>In the first paragraph of the subsection “Experiment 3 – The role of object-based attention in perceptual grouping”. Give a brief description of the Egly method.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.14320.014</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>Essential revisions:</p><p><italic>We received two complementary reviews that were supportive and enthusiastic about your study. Three major issues deserve further consideration: (i) The non-linearity of the mapping of retinal to cortical space and the consequences for signal propagation need to be discussed (please make your assumptions regarding linearity explicit.); (ii) Details of the growth-cone model need to be explained. (iii) Difficulties of the model regarding texture and internal contours need to be addressed.</italic></p><p><italic>Reviewer #1:</italic></p><p><italic>This is a wonderful study of perceptual grouping in the human visual system. By measuring the reaction time of subjects' decisions whether two point probes are on the same object or not, the authors provide strong support for a &quot;growth-cone model&quot; compared to several other relevant models. The study also measures the time course of attention spreading with an adaptation of the Egly et al. paradigm, again in support of their model. The study and presentation of the results seems fine to me, except for one major concern that is outlined below.</italic></p><p><italic>My concern is that the way the authors link the model to the structure of the visual cortex assumes that retinal space is mapped linearly onto cortical space. Under this assumption, visual propagation velocity measured in °/s corresponds to neural propagation velocity measured in growth-cones/s, and cm/s in each cortical area. With this assumption, the data suggest a fixed neural propagation velocity the estimate of which is in reasonable agreement with known estimates of horizontal fiber conduction velocity. However, as is well known, cortical mapping is not linear, but follows a logarithmic law. It seems to me that, taking the real physiology into account, the model would not predict a linear relationship as shown, for example, in <xref ref-type="fig" rid="fig4">Figure 4C</xref> (right). While the authors take pains to discuss their somewhat lower estimate of propagation velocity compared to estimates of horizontal fiber conduction, I could not find any discussion of the mapping assumption in this paper, not even in the supplementary information. This issue needs to be addressed in Results (if appropriate by additional computations) and certainly in the Discussion.</italic></p><p>The reviewer raises an interesting point about the dependence of receptive field size on eccentricity: more eccentric RFs in a single visual cortical area are larger. What are the implications of this dependence for the growth-cone model?</p><p>It is perhaps remarkable, but the implications turn out to be minimal. The reason is that the RF sizes of the growth-cone model are not mapped onto specific visual cortical areas. If grouping demands a specific growth-cone size (as measured in °), the required RFs near the fovea will be in higher visual areas than those at more peripheral locations. Suppose the task demands grouping image patches with a size of ~2°: Depending on eccentricity, grouping can occur at a low level (e.g. V1 if eccentricity is ~10°) (Gattass et al. 1981) or at a higher level (e.g. V4 if eccentricity is ~2°) (Gattass et al. 1988). Thus, if a single elongated bar with a width of 2° needs to be grouped (as in our variant of the Egly et al. paradigm, <xref ref-type="fig" rid="fig7">Figure 7</xref>) the same grouping process involves V4 neurons at 2° eccentricity and V1 neurons at 10° eccentricity. In V1, neurons with abutting, non-overlapping RFs are separated by 2-4 mm (Hubel &amp; Wiesel 1974; Harvey &amp; Dumoulin 2011), and the distance between neurons with non-overlapping RFs (the “population point image”) is only slightly larger in higher visual areas (Harvey &amp; Dumoulin 2011). It seems likely that the propagation of enhanced neuronal activity through horizontal connections occurs at similar speeds (in m/s) in different visual areas, and it therefore follows that the grouping speed (in °/s or in growth-cone/s) is constant across eccentricities. The growth-cone mechanism is therefore fully compatible with receptive field sizes increasing with eccentricity.</p><p>We clarified this issue in a new paragraph (subsection “A growth-cone model for parsing 2D-image regions”, last paragraph) of the Discussion.</p><p><italic>Reviewer #2:</italic></p><p><italic>I think that this is a nice study with interesting results. I recommend acceptance if the issues below are adequately addressed.</italic></p><p><italic>A general issue that requires a better, more coherent treatment has to do with the roles of bounding contour, internal contours, and surface properties in the process of spreading attention. Without an adequate discussion the models are not sufficiently clear, and it will improve the discussion of the experimental results.</italic></p><p><italic>For example, in presenting the 'growth-cone' model (subsection “Models of perceptual grouping”), it is stated that cone 'should not touch the boundaries'. This immediately raises a difficulty in the reader's mind: do we have to know the object external boundaries in order to apply the process? This brings about a potential chicken-and-egg problem, where some sort of object segmentation is performed prior to the application of the process.</italic></p><p>We thank the reviewer for pointing out this important issue. We observed that the growth-cone model is only superior to the other models for pictures without interior contours (<xref ref-type="fig" rid="fig6">Figure 6</xref>) and we agree that it does not solve the chicken-and-egg problem of distinguishing between internal and external contours. It only accounts for reaction time data within a more limited domain: line drawings with homogenous surfaces and bounding (external) contours.</p><p>Yet, perceptual grouping in natural images (Korjoukov et al., 2012) also relies on a serial grouping process, just as is the case for the line drawings without internal contours (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig5">5</xref>). Human observers are able to distinguish interior from exterior contours in the pictures of the present study (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Consider, for example, the line drawing of the two trucks in <xref ref-type="fig" rid="fig9">Author response image 1</xref>. We can see that the contour indicated by the arrow is an external contour of the left truck. Note, however, that this percept relies on our knowledge about the shape of trucks. Segmentation that is based on the characteristic shapes of objects is known as “semantic segmentation”.<fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.14320.012</object-id><label>Author response image 1.</label><caption><title>We see that the contour indicated by the red arrow is part of the left truck.</title><p>The correct assignment of this contour depends on knowledge about the shape of a truck. Image parsing based on shape knowledge is called “semantic segmentation”.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.14320.012">http://dx.doi.org/10.7554/eLife.14320.012</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-14320-resp-fig1-v1"/></fig></p><p>Semantic segmentation is beyond the capabilities of all the simple models that we have considered in the present manuscript. However, in the revision we now also indicate how future incremental grouping models could take advantage of the new insights derived from convolutional neural networks for object recognition that are composed of multiple hierarchically organized layers (e.g. Krizhevsky et al. (2012); Zeiler &amp; Fergus (2012); see also the review by LeCun et al. (2015)). These convolutional networks provide a useful description of the computations that implement object recognition in the ventral stream, from early visual cortical areas toward the inferotemporal cortex (Güçlü &amp; van Gerven, 2015; Yamins &amp; DiCarlo, 2016). Recent extensions of hierarchical convolutional networks have been used for semantic segmentation. They can label the image elements that belong to one of the image categories in pictures of everyday visual scenes (Hong et al., 2015; Noh et al., 2015). They solve the chicken-and-egg problem by first determining the semantic categories present in the visual scene with a convolutional network and they then use a “deconvolutional network” to go from one of the categories back to the low-level features that are part of the objects of this category. It is conceivable that these convolutional/deconvolutional networks, in combination with a horizontal grouping process, might account for the serial patterns of reaction times in grouping tasks with colorful pictures, textures and interior contours (as in Korjoukov et al., 2012). Although this is a promising avenue for future research, these more complex models remain to be built, as we now indicate in a new section of the Discussion (“Influence of object recognition and interior contours on image parsing”).</p><p><italic>A similar difficulty arises with the notion of surface properties. For example, the model of uniform connectedness (subsection “Models of perceptual grouping”) talks about 'homogenous surface properties', but it remains unclear whether the two different shapes have the same or distinct surface properties. The two cases give naturally rise to different predictions; in the case of two uniform, but different shapes, the task would be in principle much easier.</italic></p><p>We agree with the reviewer that grouping is easier if each shape has its own unique surface properties (e.g. a unique color). We note, however, that Palmer and Rock (1994) claimed that multiple regions are automatically parsed in separate uniform connected regions, irrespective of whether they have the same or different surface properties. This rule has been rather influential (the paper received 556 citations according to Google Scholar), which is why we used it as a baseline for our results.</p><p>In a recent study, Watson et al. (2013) investigated the effect of line properties in a curve-tracing task, presenting curves with the same or different contrast polarities. As anticipated by the reviewer, grouping was fastest if the target curve had a unique contrast polarity, a finding that provides additional evidence against the “uniform connectedness” rule. We have now more carefully described the “uniform connectedness rule”, clearly indicating that this was a previous proposal by Palmer &amp; Rock (1994). See subsection “Models of perceptual grouping” and the first paragraph of the Discussion.</p><p><italic>A clear understanding of the models will therefore require a discussion of the possible effects of internal contours and surface properties. This will also be useful for discussion the experimental results of experiment 2, where the growth-cone model runs into difficulties with shapes that have internal contours, and with colored shapes.</italic></p><p><italic>In the discussion of experiment 2 (subsection “Influence of object recognition on image parsing”, last paragraph) the discussion concludes that 'interior contours, as well the influence of color and texture go beyond the present growth-cone model so that variance had to remain unexplained.' I think that the discussion could benefit here from some conceptual aspects. For example, explain why the growth-cone process as used in the model runs into difficulties with internal boundaries and discuss potential implications, does it mean for instance that some form of distinguishing internal from external boundaries is implied prior to the application of the attention-spread process?</italic></p><p>We hope to have addressed this issue satisfactorily in our reply to one of the reviewers’ previous issues.</p><p><italic>In the subsection “Models of perceptual grouping”: Uniform connectedness – not sufficiently clear. I think, if the two shapes have different colors, e.g. on is red and the other is green, you can check if the two marked locations are on the same shape. But the shapes are defined here by the bounding contours, and the inside surface properties are the same.</italic></p><p>We also hope to have addressed this point in the above because we have now clarified that the uniform connectedness rule was proposed by Palmer and Rock (1994).</p><p><italic>In the subsection “Models of perceptual grouping”, Growth-cone model: it will useful to add a reference to Visual Routines (Ullman 1984) for discussion of 'attention spread' including scale-independent attention spread.</italic></p><p>We have added the reference to Ullman (1984) in this section and we now indicate that previous models for curve-tracing were inspired by the visual routines hypothesis.</p><p><italic>The Filling-in model: do we have to know which contours are boundaries or which side is the inside?</italic></p><p><italic>The wedge stimulus, experiment 1. The wedges are example of difficulty with possible internal contours. It is not entirely clear if the line separating the two wedges is an internal or external contour.</italic></p><p>The simple models in the manuscript assume that all contours are external and they are unable to segment images with internal contours. Thus, these models form the correct groupings in the wedge experiment by simply assuming that the line separating the wedges is external. Furthermore, in the experiment participants were instructed to select individual wedges as separate objects.</p><p>We do appreciate the reviewer’s concern because most objects in realistic images do contain internal contours. In the revised Discussion we explain that parsing of these more realistic images will require feedback from shape representations. This process is known as “semantic segmentation” as has been outlined in our response to one of the earlier remarks of the reviewer.</p><p><italic>In the fourth paragraph of the subsection “Experiment 2 – Perceptual grouping of complex shapes”. 'Thus, perceptual grouping of 2D image regions invokes a serial grouping process with a speed that depends on their scale.' It is not sufficiently clear, what the 'scale' here refers to (scale of what?).</italic></p><p>We have clarified what we mean by 'scale': grouping is faster for wide than for narrow image regions (subsection “Experiment 2 – Perceptual grouping of complex shapes”, fourth paragraph).</p><p><italic>In the last paragraph of the subsection “Experiment 2 – Perceptual grouping of complex shapes”. The 'growth-cone' had problems with 'detailed' cartoons – does 'detailed' here mean the presence of internal contours? Will be useful to state this here explicitly.</italic></p><p>Yes, the 'detailed cartoons' refers to the image set with internal contours. We now explicitly state this in the fifth paragraph of the subsection “Experiment 2 – Perceptual grouping of complex shapes”.</p><p><italic>In the first paragraph of the subsection “Experiment 3 – The role of object-based attention in perceptual grouping”. Give a brief description of the Egly method.</italic></p><p>We added a brief description of the paradigm and main findings of Egly et al. (1994) on page 12.</p></body></sub-article></article>